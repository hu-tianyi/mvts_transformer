{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40db25f3",
   "metadata": {},
   "source": [
    "# Pose Error Project\n",
    "### (Transformer-Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d884a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:19:37,719 | INFO : Loading packages ...\n",
      "2023-05-24 10:19:39,041 | INFO : Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-05-24 10:19:39,042 | INFO : NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Loading packages ...\")\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# 3rd party packages\n",
    "\n",
    "#from tqdm import tqdm\n",
    "# since we are using it in jupyter notebook\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Project modules\n",
    "from options import Options\n",
    "from running import setup, pipeline_factory, validate, check_progress, NEG_METRICS\n",
    "from utils import utils\n",
    "from datasets.data import data_factory, Normalizer\n",
    "from datasets.datasplit import split_dataset\n",
    "from models.ts_transformer import model_factory\n",
    "from models.loss import get_loss_module\n",
    "from optimizers import get_optimizer\n",
    "\n",
    "import parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871bc365",
   "metadata": {},
   "source": [
    "# Setup Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e17d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting 1 - Single Stage\n",
    "# Training From Scratch\n",
    "text = \"--output_dir ../experiments/ --comment 'poseErrorPred_from_Scratch_smooth24' \\\n",
    "        --name poseErrorPred_fromScratch_Regression_Selective_3_64 --records_file Regression_records.xls \\\n",
    "        --data_dir ../data/SenseTimeV4_Selective/ --data_class pose \\\n",
    "        --epochs 25 --lr 0.0001 --optimizer RAdam --batch_size 128 \\\n",
    "        --pos_encoding learnable --task regression --print_interval 1\\\n",
    "        --num_layers 3  --num_heads 8 --d_model 64 --dim_feedforward 256 --gpu 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036bf287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntext = \"--output_dir ../experiments --comment \\'poseErrorPred_finetune\\'         --name poseErrorPred_finetuned --records_file Regression_records.xls         --data_dir ../data/SenseTimeV4_Selective/ --data_class pose         --epochs 50 --lr 0.0001 --optimizer RAdam         --pos_encoding learnable --d_model 64         --load_model ../experiments/poseErrorPred_preTrain_2023-05-15_15-03-04_vvg/checkpoints/model_best.pth         --task regression --change_output --batch_size 128\"\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting 2 - Two Stages\n",
    "# Pretrain\n",
    "'''\n",
    "text = \"--output_dir ../experiments/ --comment 'poseErrorPred_pretrain' \\\n",
    "        --name poseErrorPred_preTrain --records_file Regression_records.xls \\\n",
    "        --data_dir ../data/SenseTimeV4_Selective/ --data_class pose \\\n",
    "        --val_ratio 0.2 --epochs 50 --lr 0.0001 --optimizer RAdam --batch_size 128 \\\n",
    "        --pos_encoding learnable --task regression --print_interval 1\"\n",
    "'''\n",
    "# Finetune\n",
    "\n",
    "'''\n",
    "text = \"--output_dir ../experiments --comment 'poseErrorPred_finetune' \\\n",
    "        --name poseErrorPred_finetuned --records_file Regression_records.xls \\\n",
    "        --data_dir ../data/SenseTimeV4_Selective/ --data_class pose \\\n",
    "        --epochs 50 --lr 0.0001 --optimizer RAdam \\\n",
    "        --pos_encoding learnable --d_model 64 \\\n",
    "        --load_model ../experiments/poseErrorPred_preTrain_2023-05-15_15-03-04_vvg/checkpoints/model_best.pth \\\n",
    "        --task regression --change_output --batch_size 128\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e99d546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:19:39,184 | INFO : Stored configuration file in '../experiments/poseErrorPred_fromScratch_Regression_Selective_3_64_2023-05-24_10-19-39_TGV'\n"
     ]
    }
   ],
   "source": [
    "# Process the setting string\n",
    "# Generate the config variable\n",
    "input_text = text.split()\n",
    "args = Options().parse(input_text)\n",
    "config = setup(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f6877ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_filepath': None,\n",
       " 'output_dir': '../experiments/poseErrorPred_fromScratch_Regression_Selective_3_64_2023-05-24_10-19-39_TGV',\n",
       " 'data_dir': '../data/SenseTimeV4_Selective/',\n",
       " 'load_model': None,\n",
       " 'resume': False,\n",
       " 'change_output': False,\n",
       " 'save_all': False,\n",
       " 'experiment_name': 'poseErrorPred_fromScratch_Regression_Selective_3_64',\n",
       " 'comment': \"'poseErrorPred_from_Scratch_smooth24'\",\n",
       " 'no_timestamp': False,\n",
       " 'records_file': 'Regression_records.xls',\n",
       " 'console': False,\n",
       " 'print_interval': 1,\n",
       " 'gpu': '2',\n",
       " 'n_proc': -1,\n",
       " 'num_workers': 0,\n",
       " 'seed': None,\n",
       " 'limit_size': None,\n",
       " 'test_only': None,\n",
       " 'data_class': 'pose',\n",
       " 'labels': None,\n",
       " 'test_from': None,\n",
       " 'test_ratio': 0,\n",
       " 'val_ratio': 0.2,\n",
       " 'pattern': None,\n",
       " 'val_pattern': None,\n",
       " 'test_pattern': None,\n",
       " 'normalization': 'standardization',\n",
       " 'norm_from': None,\n",
       " 'subsample_factor': None,\n",
       " 'task': 'regression',\n",
       " 'masking_ratio': 0.15,\n",
       " 'mean_mask_length': 3,\n",
       " 'mask_mode': 'separate',\n",
       " 'mask_distribution': 'geometric',\n",
       " 'exclude_feats': None,\n",
       " 'mask_feats': [0, 1],\n",
       " 'start_hint': 0.0,\n",
       " 'end_hint': 0.0,\n",
       " 'harden': False,\n",
       " 'epochs': 25,\n",
       " 'val_interval': 2,\n",
       " 'optimizer': 'RAdam',\n",
       " 'lr': 0.0001,\n",
       " 'lr_step': [1000000],\n",
       " 'lr_factor': [0.1],\n",
       " 'batch_size': 128,\n",
       " 'l2_reg': 0,\n",
       " 'global_reg': False,\n",
       " 'key_metric': 'loss',\n",
       " 'freeze': False,\n",
       " 'model': 'transformer',\n",
       " 'max_seq_len': None,\n",
       " 'data_window_len': None,\n",
       " 'd_model': 64,\n",
       " 'dim_feedforward': 256,\n",
       " 'num_heads': 8,\n",
       " 'num_layers': 3,\n",
       " 'dropout': 0.1,\n",
       " 'pos_encoding': 'learnable',\n",
       " 'activation': 'gelu',\n",
       " 'normalization_layer': 'BatchNorm',\n",
       " 'initial_timestamp': '2023-05-24_10-19-39',\n",
       " 'save_dir': '../experiments/poseErrorPred_fromScratch_Regression_Selective_3_64_2023-05-24_10-19-39_TGV/checkpoints',\n",
       " 'pred_dir': '../experiments/poseErrorPred_fromScratch_Regression_Selective_3_64_2023-05-24_10-19-39_TGV/predictions',\n",
       " 'tensorboard_dir': '../experiments/poseErrorPred_fromScratch_Regression_Selective_3_64_2023-05-24_10-19-39_TGV/tb_summaries'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639d146",
   "metadata": {},
   "source": [
    "# Setup Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "552d0b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch_time = 0\n",
    "total_eval_time = 0\n",
    "\n",
    "total_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386c3baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:19:39,195 | INFO : Running:\n",
      "/home/tianyi/anaconda3/envs/transformer/lib/python3.8/site-packages/ipykernel_launcher.py -f /home/tianyi/.local/share/jupyter/runtime/kernel-2e4d2906-4879-4630-ba4d-46f28c4ec180.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add file logging besides stdout\n",
    "file_handler = logging.FileHandler(os.path.join(config['output_dir'], 'output.log'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.info('Running:\\n{}\\n'.format(' '.join(sys.argv)))  # command used to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd3027",
   "metadata": {},
   "source": [
    "# Setup Training Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07459b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:19:39,229 | INFO : Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if config['seed'] is not None:\n",
    "    torch.manual_seed(config['seed'])\n",
    "\n",
    "device = torch.device('cuda' if (torch.cuda.is_available() and config['gpu'] != '-1') else 'cpu')\n",
    "logger.info(\"Using device: {}\".format(device))\n",
    "if device == 'cuda':\n",
    "    logger.info(\"Device index: {}\".format(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d796b",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e8acbf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:19:39,234 | INFO : Loading and preprocessing data ...\n",
      "2023-05-24 10:19:39,236 | INFO : Loading 69 datasets files using 32 parallel processes ...\n"
     ]
    }
   ],
   "source": [
    " # Build data\n",
    "logger.info(\"Loading and preprocessing data ...\")\n",
    "data_class = data_factory[config['data_class']]\n",
    "my_data = data_class(config['data_dir'], \n",
    "                     pattern=config['pattern'], \n",
    "                     n_proc=config['n_proc'], \n",
    "                     limit_size=config['limit_size'], \n",
    "                     config=config)\n",
    "feat_dim = my_data.feature_df.shape[1]  # dimensionality of data features\n",
    "if config['task'] == 'classification':\n",
    "    validation_method = 'StratifiedShuffleSplit'\n",
    "    labels = my_data.labels_df.values.flatten()\n",
    "else:\n",
    "    validation_method = 'ShuffleSplit'\n",
    "    labels = None\n",
    "    \n",
    "# Modify for the pose error pred\n",
    "validation_method = 'PoseErrorTimeSplit'\n",
    "validation_method = 'PoseErrorSequenceSplit'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706bd0fa",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "329d7c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "test_data = my_data\n",
    "test_indices = None  # will be converted to empty list in `split_dataset`, if also test_set_ratio == 0\n",
    "val_data = my_data\n",
    "val_indices = []\n",
    "if config['test_pattern']:  # used if test data come from different files / file patterns\n",
    "    test_data = data_class(config['data_dir'], pattern=config['test_pattern'], n_proc=-1, config=config)\n",
    "    test_indices = test_data.all_IDs\n",
    "if config['test_from']:  # load test IDs directly from file, if available, otherwise use `test_set_ratio`. Can work together with `test_pattern`\n",
    "    test_indices = list(set([line.rstrip() for line in open(config['test_from']).readlines()]))\n",
    "    try:\n",
    "        test_indices = [int(ind) for ind in test_indices]  # integer indices\n",
    "    except ValueError:\n",
    "        pass  # in case indices are non-integers\n",
    "    logger.info(\"Loaded {} test IDs from file: '{}'\".format(len(test_indices), config['test_from']))\n",
    "if config['val_pattern']:  # used if val data come from different files / file patterns\n",
    "    val_data = data_class(config['data_dir'], pattern=config['val_pattern'], n_proc=-1, config=config)\n",
    "    val_indices = val_data.all_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1fa9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: currently a validation set must exist, either with `val_pattern` or `val_ratio`\n",
    "# Using a `val_pattern` means that `val_ratio` == 0 and `test_ratio` == 0\n",
    "if config['val_ratio'] > 0:\n",
    "    train_indices, val_indices, test_indices = split_dataset(data_indices=my_data.all_IDs,\n",
    "                                                             validation_method=validation_method,\n",
    "                                                             n_splits=1,\n",
    "                                                             validation_ratio=config['val_ratio'],\n",
    "                                                             test_set_ratio=config['test_ratio'],  # used only if test_indices not explicitly specified\n",
    "                                                             test_indices=test_indices,\n",
    "                                                             random_seed=1337,\n",
    "                                                             labels=labels)\n",
    "    train_indices = train_indices[0]  # `split_dataset` returns a list of indices *per fold/split*\n",
    "    val_indices = val_indices[0]  # `split_dataset` returns a list of indices *per fold/split*\n",
    "else:\n",
    "    train_indices = my_data.all_IDs\n",
    "    if test_indices is None:\n",
    "        test_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0abe5fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[100100200000,\n",
       "  100100200001,\n",
       "  100100200002,\n",
       "  100100200003,\n",
       "  100100200004,\n",
       "  100100200005,\n",
       "  100100200006,\n",
       "  100100200007,\n",
       "  100100200008,\n",
       "  100100200009,\n",
       "  100100200010,\n",
       "  100100200011,\n",
       "  100100200012,\n",
       "  100100200013,\n",
       "  100100200014,\n",
       "  100100200015,\n",
       "  100100200016,\n",
       "  100100200017,\n",
       "  100100200018,\n",
       "  100100200019,\n",
       "  100100200020,\n",
       "  100100200021,\n",
       "  100100200022,\n",
       "  100100200023,\n",
       "  100100200024,\n",
       "  100100200025,\n",
       "  100100200026,\n",
       "  100100200027,\n",
       "  100100200028,\n",
       "  100100200029,\n",
       "  100100200030,\n",
       "  100100200031,\n",
       "  100100200032,\n",
       "  100100200033,\n",
       "  100100200034,\n",
       "  100100200035,\n",
       "  100100200036,\n",
       "  100100200037,\n",
       "  100100200038,\n",
       "  100100200039,\n",
       "  100100200040,\n",
       "  100100200041,\n",
       "  100100200042,\n",
       "  100100200043,\n",
       "  100100200044,\n",
       "  100100200045,\n",
       "  100100200046,\n",
       "  100100200047,\n",
       "  100100200048,\n",
       "  100100200049,\n",
       "  100100200050,\n",
       "  100100200051,\n",
       "  100100200052,\n",
       "  100100200053,\n",
       "  100100200054,\n",
       "  100100200055,\n",
       "  100100200056,\n",
       "  100100200057,\n",
       "  100100200058,\n",
       "  100100200059,\n",
       "  100100200060,\n",
       "  100100200061,\n",
       "  100100200062,\n",
       "  100100200063,\n",
       "  100100200064,\n",
       "  100100200065,\n",
       "  100100200066,\n",
       "  100100200067,\n",
       "  100100200068,\n",
       "  100100200069,\n",
       "  100100200070,\n",
       "  100100200071,\n",
       "  100100200072,\n",
       "  100100200073,\n",
       "  100100200074,\n",
       "  100100200075,\n",
       "  100100200076,\n",
       "  100100200077,\n",
       "  100100200078,\n",
       "  100100200079,\n",
       "  100100200080,\n",
       "  100100200081,\n",
       "  100100200082,\n",
       "  100100200083,\n",
       "  100100200084,\n",
       "  100100200085,\n",
       "  100100200086,\n",
       "  100100200087,\n",
       "  100100200088,\n",
       "  100100200089,\n",
       "  100100200090,\n",
       "  100100200091,\n",
       "  100100200092,\n",
       "  100100200093,\n",
       "  100100200094,\n",
       "  100100200095,\n",
       "  100100200096,\n",
       "  100100200097,\n",
       "  100100200098,\n",
       "  100100200099,\n",
       "  100100200100,\n",
       "  100100200101,\n",
       "  100100200102,\n",
       "  100100200103,\n",
       "  100100200104,\n",
       "  100100200105,\n",
       "  100100200106,\n",
       "  100100200107,\n",
       "  100100200108,\n",
       "  100100200109,\n",
       "  100100200110,\n",
       "  100100200111,\n",
       "  100100200112,\n",
       "  100100200113,\n",
       "  100100200114,\n",
       "  100100200115,\n",
       "  100100200116,\n",
       "  100100200117,\n",
       "  100100200118,\n",
       "  100100200119,\n",
       "  100100200120,\n",
       "  100100200121,\n",
       "  100100200122,\n",
       "  100100200123,\n",
       "  100100200124,\n",
       "  100100200125,\n",
       "  100100200126,\n",
       "  100100200127,\n",
       "  100100200128,\n",
       "  100100200129,\n",
       "  100100200130,\n",
       "  100100200131,\n",
       "  100100200132,\n",
       "  100100200133,\n",
       "  100100200134,\n",
       "  100100200135,\n",
       "  100100200136,\n",
       "  100100200137,\n",
       "  100100200138,\n",
       "  100100200139,\n",
       "  100100200140,\n",
       "  100100200141,\n",
       "  100100200142,\n",
       "  100100200143,\n",
       "  100100200144,\n",
       "  100100200145,\n",
       "  100100200146,\n",
       "  100100200147,\n",
       "  100100200148,\n",
       "  100100200149,\n",
       "  100100200150,\n",
       "  100100200151,\n",
       "  100100200152,\n",
       "  100100200153,\n",
       "  100100200154,\n",
       "  100100200155,\n",
       "  100100200156,\n",
       "  100100200157,\n",
       "  100100200158,\n",
       "  100100200159,\n",
       "  100100200160,\n",
       "  100100200161,\n",
       "  100100200162,\n",
       "  100100200163,\n",
       "  100100200164,\n",
       "  100100200165,\n",
       "  100100200166,\n",
       "  100100200167,\n",
       "  100100200168,\n",
       "  100100200169,\n",
       "  100100200170,\n",
       "  100100200171,\n",
       "  100100200172,\n",
       "  100100200173,\n",
       "  100100200174,\n",
       "  100100200175,\n",
       "  100100200176,\n",
       "  100100200177,\n",
       "  100100200178,\n",
       "  100100200179,\n",
       "  100100200180,\n",
       "  100100200181,\n",
       "  100100200182,\n",
       "  100100200183,\n",
       "  100100200184,\n",
       "  100100200185,\n",
       "  100100200186,\n",
       "  100100200187,\n",
       "  100100200188,\n",
       "  100100200189,\n",
       "  100100200190,\n",
       "  100100200191,\n",
       "  100100200192,\n",
       "  100100200193,\n",
       "  100100200194,\n",
       "  100100200195,\n",
       "  100100200196,\n",
       "  100100200197,\n",
       "  100100200198,\n",
       "  100100200199,\n",
       "  100100200200,\n",
       "  100100200201,\n",
       "  100100200202,\n",
       "  100100200203,\n",
       "  100100200204,\n",
       "  100100200205,\n",
       "  100100200206,\n",
       "  100100200207,\n",
       "  100100200208,\n",
       "  100100200209,\n",
       "  100100200210,\n",
       "  100100200211,\n",
       "  100100200212,\n",
       "  100100200213,\n",
       "  100100200214,\n",
       "  100100200215,\n",
       "  100100200216,\n",
       "  100100200217,\n",
       "  100100200218,\n",
       "  100100200219,\n",
       "  100100200220,\n",
       "  100100200221,\n",
       "  100100200222,\n",
       "  100100200223,\n",
       "  100100200224,\n",
       "  100100200225,\n",
       "  100100200226,\n",
       "  100100200227,\n",
       "  100100200228,\n",
       "  100100200229,\n",
       "  100100200230,\n",
       "  100100200231,\n",
       "  100100200232,\n",
       "  100100200233,\n",
       "  100100200234,\n",
       "  100100200235,\n",
       "  100100200236,\n",
       "  100100200237,\n",
       "  100100200238,\n",
       "  100100200239,\n",
       "  100100200240,\n",
       "  100100200241,\n",
       "  100100200242,\n",
       "  100100200243,\n",
       "  100100200244,\n",
       "  100100200245,\n",
       "  100100200246,\n",
       "  100100200247,\n",
       "  100100200248,\n",
       "  100100200249,\n",
       "  100100200250,\n",
       "  100100200251,\n",
       "  100100200252,\n",
       "  100100200253,\n",
       "  100100200254,\n",
       "  100100200255,\n",
       "  100100200256,\n",
       "  100100200257,\n",
       "  100100200258,\n",
       "  100100200259,\n",
       "  100100200260,\n",
       "  100100200261,\n",
       "  100100200262,\n",
       "  100100200263,\n",
       "  100100200264,\n",
       "  100100200265,\n",
       "  100100200266,\n",
       "  100100200267,\n",
       "  100100200268,\n",
       "  100100200269,\n",
       "  100100200270,\n",
       "  100100200271,\n",
       "  100100200272,\n",
       "  100100200273,\n",
       "  100100200274,\n",
       "  100100200275,\n",
       "  100100200276,\n",
       "  100100200277,\n",
       "  100100200278,\n",
       "  100100200279,\n",
       "  100100200280,\n",
       "  100100200281,\n",
       "  100100200282,\n",
       "  100100200283,\n",
       "  100100200284,\n",
       "  100100200285,\n",
       "  100100200286,\n",
       "  100100200287,\n",
       "  100100200288,\n",
       "  100100200289,\n",
       "  100100200290,\n",
       "  100100200291,\n",
       "  100100200292,\n",
       "  100100200293,\n",
       "  100100200294,\n",
       "  100100200295,\n",
       "  100100200296,\n",
       "  100100200297,\n",
       "  100100200298,\n",
       "  100100200299,\n",
       "  100100200300,\n",
       "  100100200301,\n",
       "  100100200302,\n",
       "  100100200303,\n",
       "  100100200304,\n",
       "  100100200305,\n",
       "  100100200306,\n",
       "  100100200307,\n",
       "  100100200308,\n",
       "  100100200309,\n",
       "  100100200310,\n",
       "  100100200311,\n",
       "  100100200312,\n",
       "  100100200313,\n",
       "  100100200314,\n",
       "  100100200315,\n",
       "  100100200316,\n",
       "  100100200317,\n",
       "  100100200318,\n",
       "  100100200319,\n",
       "  100100200320,\n",
       "  100100200321,\n",
       "  100100200322,\n",
       "  100100200323,\n",
       "  100100200324,\n",
       "  100100200325,\n",
       "  100100200326,\n",
       "  100100200327,\n",
       "  100100200328,\n",
       "  100100200329,\n",
       "  100100200330,\n",
       "  100100200331,\n",
       "  100100200332,\n",
       "  100100200333,\n",
       "  100100200334,\n",
       "  100100200335,\n",
       "  100100200336,\n",
       "  100100200337,\n",
       "  100100200338,\n",
       "  100100200339,\n",
       "  100100200340,\n",
       "  100100200341,\n",
       "  100100200342,\n",
       "  100100200343,\n",
       "  100100200344,\n",
       "  100100200345,\n",
       "  100100200346,\n",
       "  100100200347,\n",
       "  100100200348,\n",
       "  100100200349,\n",
       "  100100200350,\n",
       "  100100200351,\n",
       "  100100200352,\n",
       "  100100200353,\n",
       "  100100200354,\n",
       "  100100200355,\n",
       "  100100200356,\n",
       "  100100200357,\n",
       "  100100200358,\n",
       "  100100200359,\n",
       "  100100200360,\n",
       "  100100200361,\n",
       "  100100200362,\n",
       "  100100200363,\n",
       "  100100200364,\n",
       "  100100200365,\n",
       "  100100200366,\n",
       "  100100200367,\n",
       "  100100200368,\n",
       "  100100200369,\n",
       "  100100200370,\n",
       "  100100200371,\n",
       "  100100200372,\n",
       "  100100200373,\n",
       "  100100200374,\n",
       "  100100200375,\n",
       "  100100200376,\n",
       "  100100200377,\n",
       "  100100200378,\n",
       "  100100200379,\n",
       "  100100200380,\n",
       "  100100200381,\n",
       "  100100200382,\n",
       "  100100200383,\n",
       "  100100200384,\n",
       "  100100200385,\n",
       "  100100200386,\n",
       "  100100200387,\n",
       "  100100200388,\n",
       "  100100200389,\n",
       "  100100200390,\n",
       "  100100200391,\n",
       "  100100200392,\n",
       "  100100200393,\n",
       "  100100200394,\n",
       "  100100200395,\n",
       "  100100200396,\n",
       "  100100200397,\n",
       "  100100200398,\n",
       "  100100200399,\n",
       "  100100200400,\n",
       "  100100200401,\n",
       "  100100200402,\n",
       "  100100200403,\n",
       "  100100200404,\n",
       "  100100200405,\n",
       "  100100200406,\n",
       "  100100200407,\n",
       "  100100200408,\n",
       "  100100200409,\n",
       "  100100200410,\n",
       "  100100200411,\n",
       "  100100200412,\n",
       "  100100200413,\n",
       "  100100200414,\n",
       "  100100200415,\n",
       "  100100200416,\n",
       "  100100200417,\n",
       "  100100200418,\n",
       "  100100200419,\n",
       "  100100200420,\n",
       "  100100200421,\n",
       "  100100200422,\n",
       "  100100200423,\n",
       "  100100200424,\n",
       "  100100200425,\n",
       "  100100200426,\n",
       "  100100200427,\n",
       "  100100200428,\n",
       "  100100200429,\n",
       "  100100200430,\n",
       "  100100200431,\n",
       "  100100200432,\n",
       "  100100200433,\n",
       "  100100200434,\n",
       "  100100200435,\n",
       "  100100200436,\n",
       "  100100200437,\n",
       "  100100200438,\n",
       "  100100200439,\n",
       "  100100200440,\n",
       "  100100200441,\n",
       "  100100200442,\n",
       "  100100200443,\n",
       "  100100200444,\n",
       "  100100200445,\n",
       "  100100200446,\n",
       "  100100200447,\n",
       "  100100200448,\n",
       "  100100200449,\n",
       "  100100200450,\n",
       "  100100200451,\n",
       "  100100200452,\n",
       "  100100200453,\n",
       "  100100200454,\n",
       "  100100200455,\n",
       "  100100200456,\n",
       "  100100200457,\n",
       "  100100200458,\n",
       "  100100200459,\n",
       "  100100200460,\n",
       "  100100200461,\n",
       "  100100200462,\n",
       "  100100200463,\n",
       "  100100200464,\n",
       "  100100200465,\n",
       "  100100200466,\n",
       "  100100200467,\n",
       "  100100200468,\n",
       "  100100200469,\n",
       "  100100200470,\n",
       "  100100200471,\n",
       "  100100200472,\n",
       "  100100200473,\n",
       "  100100200474,\n",
       "  100100200475,\n",
       "  100100200476,\n",
       "  100100200477,\n",
       "  100100200478,\n",
       "  100100200479,\n",
       "  100100200480,\n",
       "  100100200481,\n",
       "  100100200482,\n",
       "  100100200483,\n",
       "  100100200484,\n",
       "  100100200485,\n",
       "  100100200486,\n",
       "  100100200487,\n",
       "  100100200488,\n",
       "  100100200489,\n",
       "  100100200490,\n",
       "  100100200491,\n",
       "  100100200492,\n",
       "  100100200493,\n",
       "  100100200494,\n",
       "  100100200495,\n",
       "  100100200496,\n",
       "  100100200497,\n",
       "  100100200498,\n",
       "  100100200499,\n",
       "  100100200500,\n",
       "  100100200501,\n",
       "  100100200502,\n",
       "  100100200503,\n",
       "  100100200504,\n",
       "  100100200505,\n",
       "  100100200506,\n",
       "  100100200507,\n",
       "  100100200508,\n",
       "  100100200509,\n",
       "  100100200510,\n",
       "  100100200511,\n",
       "  100100200512,\n",
       "  100100200513,\n",
       "  100100200514,\n",
       "  100100200515,\n",
       "  100100200516,\n",
       "  100100200517,\n",
       "  100100200518,\n",
       "  100100200519,\n",
       "  100100200520,\n",
       "  100100200521,\n",
       "  100100200522,\n",
       "  100100200523,\n",
       "  100100200524,\n",
       "  100100200525,\n",
       "  100100200526,\n",
       "  100100200527,\n",
       "  100100200528,\n",
       "  100100200529,\n",
       "  100100200530,\n",
       "  100100200531,\n",
       "  100100200532,\n",
       "  100100200533,\n",
       "  100100200534,\n",
       "  100100200535,\n",
       "  100100200536,\n",
       "  100100200537,\n",
       "  100100200538,\n",
       "  100100200539,\n",
       "  100100200540,\n",
       "  100100200541,\n",
       "  100100200542,\n",
       "  100100200543,\n",
       "  100100200544,\n",
       "  100100200545,\n",
       "  100100200546,\n",
       "  100100200547,\n",
       "  100100200548,\n",
       "  100100200549,\n",
       "  100100200550,\n",
       "  100100200551,\n",
       "  100100200552,\n",
       "  100100200553,\n",
       "  100100200554,\n",
       "  100100200555,\n",
       "  100100200556,\n",
       "  100100200557,\n",
       "  100100200558,\n",
       "  100100200559,\n",
       "  100100200560,\n",
       "  100100200561,\n",
       "  100100200562,\n",
       "  100100200563,\n",
       "  100100200564,\n",
       "  100100200565,\n",
       "  100100200566,\n",
       "  100100200567,\n",
       "  100100200568,\n",
       "  100100200569,\n",
       "  100100200570,\n",
       "  100100200571,\n",
       "  100100200572,\n",
       "  100100200573,\n",
       "  100100200574,\n",
       "  100100200575,\n",
       "  100100200576,\n",
       "  100100200577,\n",
       "  100100200578,\n",
       "  100100200579,\n",
       "  100100200580,\n",
       "  100100200581,\n",
       "  100100200582,\n",
       "  100100200583,\n",
       "  100100200584,\n",
       "  100100200585,\n",
       "  100100200586,\n",
       "  100100200587,\n",
       "  100100200588,\n",
       "  100100200589,\n",
       "  100100200590,\n",
       "  100100200591,\n",
       "  100100200592,\n",
       "  100100200593,\n",
       "  100100200594,\n",
       "  100100200595,\n",
       "  100100200596,\n",
       "  100100200597,\n",
       "  100100200598,\n",
       "  100100200599,\n",
       "  100100200600,\n",
       "  100100200601,\n",
       "  100100200602,\n",
       "  100100200603,\n",
       "  100100200604,\n",
       "  100100200605,\n",
       "  100100200606,\n",
       "  100100200607,\n",
       "  100100200608,\n",
       "  100100200609,\n",
       "  100100200610,\n",
       "  100100200611,\n",
       "  100100200612,\n",
       "  100100200613,\n",
       "  100100200614,\n",
       "  100100200615,\n",
       "  100100200616,\n",
       "  100100200617,\n",
       "  100100200618,\n",
       "  100100200619,\n",
       "  100100200620,\n",
       "  100100200621,\n",
       "  100100200622,\n",
       "  100100200623,\n",
       "  100100200624,\n",
       "  100100200625,\n",
       "  100100200626,\n",
       "  100100200627,\n",
       "  100100200628,\n",
       "  100100200629,\n",
       "  100100200630,\n",
       "  100100200631,\n",
       "  100100200632,\n",
       "  100100200633,\n",
       "  100100200634,\n",
       "  100100200635,\n",
       "  100100200636,\n",
       "  100100200637,\n",
       "  100100200638,\n",
       "  100100200639,\n",
       "  100100200640,\n",
       "  100100200641,\n",
       "  100100200642,\n",
       "  100100200643,\n",
       "  100100200644,\n",
       "  100100200645,\n",
       "  100100200646,\n",
       "  100100200647,\n",
       "  100100200648,\n",
       "  100100200649,\n",
       "  100100200650,\n",
       "  100100200651,\n",
       "  100100200652,\n",
       "  100100200653,\n",
       "  100100200654,\n",
       "  100100200655,\n",
       "  100100200656,\n",
       "  100100200657,\n",
       "  100100200658,\n",
       "  100100200659,\n",
       "  100100200660,\n",
       "  100100200661,\n",
       "  100100200662,\n",
       "  100100200663,\n",
       "  100100200664,\n",
       "  100100200665,\n",
       "  100100200666,\n",
       "  100100200667,\n",
       "  100100200668,\n",
       "  100100200669,\n",
       "  100100200670,\n",
       "  100100200671,\n",
       "  100100200672,\n",
       "  100100200673,\n",
       "  100100200674,\n",
       "  100100200675,\n",
       "  100100200676,\n",
       "  100100200677,\n",
       "  100100200678,\n",
       "  100100200679,\n",
       "  100100200680,\n",
       "  100100200681,\n",
       "  100100200682,\n",
       "  100100200683,\n",
       "  100100200684,\n",
       "  100100200685,\n",
       "  100100200686,\n",
       "  100100200687,\n",
       "  100100200688,\n",
       "  100100200689,\n",
       "  100100200690,\n",
       "  100100200691,\n",
       "  100100200692,\n",
       "  100100200693,\n",
       "  100100200694,\n",
       "  100100200695,\n",
       "  100100200696,\n",
       "  100100200697,\n",
       "  100100200698,\n",
       "  100100200699,\n",
       "  100100200700,\n",
       "  100100200701,\n",
       "  100100200702,\n",
       "  100100200703,\n",
       "  100100200704,\n",
       "  100100200705,\n",
       "  100100200706,\n",
       "  100100200707,\n",
       "  100100200708,\n",
       "  100100200709,\n",
       "  100100200710,\n",
       "  100100200711,\n",
       "  100100200712,\n",
       "  100100200713,\n",
       "  100100200714,\n",
       "  100100200715,\n",
       "  100100200716,\n",
       "  100100200717,\n",
       "  100100200718,\n",
       "  100100200719,\n",
       "  100100200720,\n",
       "  100100200721,\n",
       "  100100200722,\n",
       "  100100200723,\n",
       "  100100200724,\n",
       "  100100200725,\n",
       "  100100200726,\n",
       "  100100200727,\n",
       "  100100200728,\n",
       "  100100200729,\n",
       "  100100200730,\n",
       "  100100200731,\n",
       "  100100200732,\n",
       "  100100200733,\n",
       "  100100200734,\n",
       "  100100200735,\n",
       "  100100200736,\n",
       "  100100200737,\n",
       "  100100200738,\n",
       "  100100200739,\n",
       "  100100200740,\n",
       "  100100200741,\n",
       "  100100200742,\n",
       "  100100200743,\n",
       "  100100200744,\n",
       "  100100200745,\n",
       "  100100200746,\n",
       "  100100200747,\n",
       "  100100200748,\n",
       "  100100200749,\n",
       "  100100200750,\n",
       "  100100200751,\n",
       "  100100200752,\n",
       "  100100200753,\n",
       "  100100200754,\n",
       "  100100200755,\n",
       "  100100200756,\n",
       "  100100200757,\n",
       "  100100200758,\n",
       "  100100200759,\n",
       "  100100200760,\n",
       "  100100200761,\n",
       "  100100200762,\n",
       "  100100200763,\n",
       "  100100200764,\n",
       "  100100200765,\n",
       "  100100200766,\n",
       "  100100200767,\n",
       "  100100200768,\n",
       "  100100200769,\n",
       "  100100200770,\n",
       "  100100200771,\n",
       "  100100200772,\n",
       "  100100200773,\n",
       "  100100200774,\n",
       "  100100200775,\n",
       "  100100200776,\n",
       "  100100200777,\n",
       "  100100200778,\n",
       "  100100200779,\n",
       "  100100200780,\n",
       "  100100200781,\n",
       "  100100200782,\n",
       "  100100200783,\n",
       "  100100200784,\n",
       "  100100200785,\n",
       "  100100200786,\n",
       "  100100200787,\n",
       "  100100200788,\n",
       "  100100200789,\n",
       "  100100200790,\n",
       "  100100200791,\n",
       "  100100200792,\n",
       "  100100200793,\n",
       "  100100200794,\n",
       "  100100200795,\n",
       "  100100200796,\n",
       "  100100200797,\n",
       "  100100200798,\n",
       "  100100200799,\n",
       "  100100200800,\n",
       "  100100200801,\n",
       "  100100200802,\n",
       "  100100200803,\n",
       "  100100200804,\n",
       "  100100200805,\n",
       "  100100200806,\n",
       "  100100200807,\n",
       "  100100200808,\n",
       "  100100200809,\n",
       "  100100200810,\n",
       "  100100200811,\n",
       "  100100200812,\n",
       "  100100200813,\n",
       "  100100200814,\n",
       "  100100200815,\n",
       "  100100200816,\n",
       "  100100200817,\n",
       "  100100200818,\n",
       "  100100200819,\n",
       "  100100200820,\n",
       "  100100200821,\n",
       "  100100200822,\n",
       "  100100200823,\n",
       "  100100200824,\n",
       "  100100200825,\n",
       "  100100200826,\n",
       "  100100200827,\n",
       "  100100200828,\n",
       "  100100200829,\n",
       "  100100200830,\n",
       "  100100200831,\n",
       "  100100200832,\n",
       "  100100200833,\n",
       "  100100200834,\n",
       "  100100200835,\n",
       "  100100200836,\n",
       "  100100200837,\n",
       "  100100200838,\n",
       "  100100200839,\n",
       "  100100200840,\n",
       "  100100200841,\n",
       "  100100200842,\n",
       "  100100200843,\n",
       "  100100200844,\n",
       "  100100200845,\n",
       "  100100200846,\n",
       "  100100200847,\n",
       "  100100200848,\n",
       "  100100200849,\n",
       "  100100200850,\n",
       "  100100200851,\n",
       "  100100200852,\n",
       "  100100200853,\n",
       "  100100200854,\n",
       "  100100200855,\n",
       "  100100200856,\n",
       "  100100200857,\n",
       "  100100200858,\n",
       "  100100200859,\n",
       "  100100200860,\n",
       "  100100200861,\n",
       "  100100200862,\n",
       "  100100200863,\n",
       "  100100200864,\n",
       "  100100200865,\n",
       "  100100200866,\n",
       "  100100200867,\n",
       "  100100200868,\n",
       "  100100200869,\n",
       "  100100200870,\n",
       "  100100200871,\n",
       "  100100200872,\n",
       "  100100200873,\n",
       "  100100200874,\n",
       "  100100200875,\n",
       "  100100200876,\n",
       "  100100200877,\n",
       "  100100200878,\n",
       "  100100200879,\n",
       "  100100200880,\n",
       "  100100200881,\n",
       "  100100200882,\n",
       "  100100200883,\n",
       "  100100200884,\n",
       "  100100200885,\n",
       "  100100200886,\n",
       "  100100200887,\n",
       "  100100200888,\n",
       "  100100200889,\n",
       "  100100200890,\n",
       "  100100200891,\n",
       "  100100200892,\n",
       "  100100200893,\n",
       "  100100200894,\n",
       "  100100200895,\n",
       "  100100200896,\n",
       "  100100200897,\n",
       "  100100200898,\n",
       "  100100200899,\n",
       "  100100200900,\n",
       "  100100200901,\n",
       "  100100200902,\n",
       "  100100200903,\n",
       "  100100200904,\n",
       "  100100200905,\n",
       "  100100200906,\n",
       "  100100200907,\n",
       "  100100200908,\n",
       "  100100200909,\n",
       "  100100200910,\n",
       "  100100200911,\n",
       "  100100200912,\n",
       "  100100200913,\n",
       "  100100200914,\n",
       "  100100200915,\n",
       "  100100200916,\n",
       "  100100200917,\n",
       "  100100200918,\n",
       "  100100200919,\n",
       "  100100200920,\n",
       "  100100200921,\n",
       "  100100200922,\n",
       "  100100200923,\n",
       "  100100200924,\n",
       "  100100200925,\n",
       "  100100200926,\n",
       "  100100200927,\n",
       "  100100200928,\n",
       "  100100200929,\n",
       "  100100200930,\n",
       "  100100200931,\n",
       "  100100200932,\n",
       "  100100200933,\n",
       "  100100200934,\n",
       "  100100200935,\n",
       "  100100200936,\n",
       "  100100200937,\n",
       "  100100200938,\n",
       "  100100200939,\n",
       "  100100200940,\n",
       "  100100200941,\n",
       "  100100200942,\n",
       "  100100200943,\n",
       "  100100200944,\n",
       "  100100200945,\n",
       "  100100200946,\n",
       "  100100200947,\n",
       "  100100200948,\n",
       "  100100200949,\n",
       "  100100200950,\n",
       "  100100200951,\n",
       "  100100200952,\n",
       "  100100200953,\n",
       "  100100200954,\n",
       "  100100200955,\n",
       "  100100200956,\n",
       "  100100200957,\n",
       "  100100200958,\n",
       "  100100200959,\n",
       "  100100200960,\n",
       "  100100200961,\n",
       "  100100200962,\n",
       "  100100200963,\n",
       "  100100200964,\n",
       "  100100200965,\n",
       "  100100200966,\n",
       "  100100200967,\n",
       "  100100200968,\n",
       "  100100200969,\n",
       "  100100200970,\n",
       "  100100200971,\n",
       "  100100200972,\n",
       "  100100200973,\n",
       "  100100200974,\n",
       "  100100200975,\n",
       "  100100200976,\n",
       "  100100200977,\n",
       "  100100200978,\n",
       "  100100200979,\n",
       "  100100200980,\n",
       "  100100200981,\n",
       "  100100200982,\n",
       "  100100200983,\n",
       "  100100200984,\n",
       "  100100200985,\n",
       "  100100200986,\n",
       "  100100200987,\n",
       "  100100200988,\n",
       "  100100200989,\n",
       "  100100200990,\n",
       "  100100200991,\n",
       "  100100200992,\n",
       "  100100200993,\n",
       "  100100200994,\n",
       "  100100200995,\n",
       "  100100200996,\n",
       "  100100200997,\n",
       "  100100200998,\n",
       "  100100200999,\n",
       "  ...],\n",
       " [100000100000,\n",
       "  100000100001,\n",
       "  100000100002,\n",
       "  100000100003,\n",
       "  100000100004,\n",
       "  100000100005,\n",
       "  100000100006,\n",
       "  100000100007,\n",
       "  100000100008,\n",
       "  100000100009,\n",
       "  100000100010,\n",
       "  100000100011,\n",
       "  100000100012,\n",
       "  100000100013,\n",
       "  100000100014,\n",
       "  100000100015,\n",
       "  100000100016,\n",
       "  100000100017,\n",
       "  100000100018,\n",
       "  100000100019,\n",
       "  100000100020,\n",
       "  100000100021,\n",
       "  100000100022,\n",
       "  100000100023,\n",
       "  100000100024,\n",
       "  100000100025,\n",
       "  100000100026,\n",
       "  100000100027,\n",
       "  100000100028,\n",
       "  100000100029,\n",
       "  100000100030,\n",
       "  100000100031,\n",
       "  100000100032,\n",
       "  100000100033,\n",
       "  100000100034,\n",
       "  100000100035,\n",
       "  100000100036,\n",
       "  100000100037,\n",
       "  100000100038,\n",
       "  100000100039,\n",
       "  100000100040,\n",
       "  100000100041,\n",
       "  100000100042,\n",
       "  100000100043,\n",
       "  100000100044,\n",
       "  100000100045,\n",
       "  100000100046,\n",
       "  100000100047,\n",
       "  100000100048,\n",
       "  100000100049,\n",
       "  100000100050,\n",
       "  100000100051,\n",
       "  100000100052,\n",
       "  100000100053,\n",
       "  100000100054,\n",
       "  100000100055,\n",
       "  100000100056,\n",
       "  100000100057,\n",
       "  100000100058,\n",
       "  100000100059,\n",
       "  100000100060,\n",
       "  100000100061,\n",
       "  100000100062,\n",
       "  100000100063,\n",
       "  100000100064,\n",
       "  100000100065,\n",
       "  100000100066,\n",
       "  100000100067,\n",
       "  100000100068,\n",
       "  100000100069,\n",
       "  100000100070,\n",
       "  100000100071,\n",
       "  100000100072,\n",
       "  100000100073,\n",
       "  100000100074,\n",
       "  100000100075,\n",
       "  100000100076,\n",
       "  100000100077,\n",
       "  100000100078,\n",
       "  100000100079,\n",
       "  100000100080,\n",
       "  100000100081,\n",
       "  100000100082,\n",
       "  100000100083,\n",
       "  100000100084,\n",
       "  100000100085,\n",
       "  100000100086,\n",
       "  100000100087,\n",
       "  100000100088,\n",
       "  100000100089,\n",
       "  100000100090,\n",
       "  100000100091,\n",
       "  100000100092,\n",
       "  100000100093,\n",
       "  100000100094,\n",
       "  100000100095,\n",
       "  100000100096,\n",
       "  100000100097,\n",
       "  100000100098,\n",
       "  100000100099,\n",
       "  100000100100,\n",
       "  100000100101,\n",
       "  100000100102,\n",
       "  100000100103,\n",
       "  100000100104,\n",
       "  100000100105,\n",
       "  100000100106,\n",
       "  100000100107,\n",
       "  100000100108,\n",
       "  100000100109,\n",
       "  100000100110,\n",
       "  100000100111,\n",
       "  100000100112,\n",
       "  100000100113,\n",
       "  100000100114,\n",
       "  100000100115,\n",
       "  100000100116,\n",
       "  100000100117,\n",
       "  100000100118,\n",
       "  100000100119,\n",
       "  100000100120,\n",
       "  100000100121,\n",
       "  100000100122,\n",
       "  100000100123,\n",
       "  100000100124,\n",
       "  100000100125,\n",
       "  100000100126,\n",
       "  100000100127,\n",
       "  100000100128,\n",
       "  100000100129,\n",
       "  100000100130,\n",
       "  100000100131,\n",
       "  100000100132,\n",
       "  100000100133,\n",
       "  100000100134,\n",
       "  100000100135,\n",
       "  100000100136,\n",
       "  100000100137,\n",
       "  100000100138,\n",
       "  100000100139,\n",
       "  100000100140,\n",
       "  100000100141,\n",
       "  100000100142,\n",
       "  100000100143,\n",
       "  100000100144,\n",
       "  100000100145,\n",
       "  100000100146,\n",
       "  100000100147,\n",
       "  100000100148,\n",
       "  100000100149,\n",
       "  100000100150,\n",
       "  100000100151,\n",
       "  100000100152,\n",
       "  100000100153,\n",
       "  100000100154,\n",
       "  100000100155,\n",
       "  100000100156,\n",
       "  100000100157,\n",
       "  100000100158,\n",
       "  100000100159,\n",
       "  100000100160,\n",
       "  100000100161,\n",
       "  100000100162,\n",
       "  100000100163,\n",
       "  100000100164,\n",
       "  100000100165,\n",
       "  100000100166,\n",
       "  100000100167,\n",
       "  100000100168,\n",
       "  100000100169,\n",
       "  100000100170,\n",
       "  100000100171,\n",
       "  100000100172,\n",
       "  100000100173,\n",
       "  100000100174,\n",
       "  100000100175,\n",
       "  100000100176,\n",
       "  100000100177,\n",
       "  100000100178,\n",
       "  100000100179,\n",
       "  100000100180,\n",
       "  100000100181,\n",
       "  100000100182,\n",
       "  100000100183,\n",
       "  100000100184,\n",
       "  100000100185,\n",
       "  100000100186,\n",
       "  100000100187,\n",
       "  100000100188,\n",
       "  100000100189,\n",
       "  100000100190,\n",
       "  100000100191,\n",
       "  100000100192,\n",
       "  100000100193,\n",
       "  100000100194,\n",
       "  100000100195,\n",
       "  100000100196,\n",
       "  100000100197,\n",
       "  100000100198,\n",
       "  100000100199,\n",
       "  100000100200,\n",
       "  100000100201,\n",
       "  100000100202,\n",
       "  100000100203,\n",
       "  100000100204,\n",
       "  100000100205,\n",
       "  100000100206,\n",
       "  100000100207,\n",
       "  100000100208,\n",
       "  100000100209,\n",
       "  100000100210,\n",
       "  100000100211,\n",
       "  100000100212,\n",
       "  100000100213,\n",
       "  100000100214,\n",
       "  100000100215,\n",
       "  100000100216,\n",
       "  100000100217,\n",
       "  100000100218,\n",
       "  100000100219,\n",
       "  100000100220,\n",
       "  100000100221,\n",
       "  100000100222,\n",
       "  100000100223,\n",
       "  100000100224,\n",
       "  100000100225,\n",
       "  100000100226,\n",
       "  100000100227,\n",
       "  100000100228,\n",
       "  100000100229,\n",
       "  100000100230,\n",
       "  100000100231,\n",
       "  100000100232,\n",
       "  100000100233,\n",
       "  100000100234,\n",
       "  100000100235,\n",
       "  100000100236,\n",
       "  100000100237,\n",
       "  100000100238,\n",
       "  100000100239,\n",
       "  100000100240,\n",
       "  100000100241,\n",
       "  100000100242,\n",
       "  100000100243,\n",
       "  100000100244,\n",
       "  100000100245,\n",
       "  100000100246,\n",
       "  100000100247,\n",
       "  100000100248,\n",
       "  100000100249,\n",
       "  100000100250,\n",
       "  100000100251,\n",
       "  100000100252,\n",
       "  100000100253,\n",
       "  100000100254,\n",
       "  100000100255,\n",
       "  100000100256,\n",
       "  100000100257,\n",
       "  100000100258,\n",
       "  100000100259,\n",
       "  100000100260,\n",
       "  100000100261,\n",
       "  100000100262,\n",
       "  100000100263,\n",
       "  100000100264,\n",
       "  100000100265,\n",
       "  100000100266,\n",
       "  100000100267,\n",
       "  100000100268,\n",
       "  100000100269,\n",
       "  100000100270,\n",
       "  100000100271,\n",
       "  100000100272,\n",
       "  100000100273,\n",
       "  100000100274,\n",
       "  100000100275,\n",
       "  100000100276,\n",
       "  100000100277,\n",
       "  100000100278,\n",
       "  100000100279,\n",
       "  100000100280,\n",
       "  100000100281,\n",
       "  100000100282,\n",
       "  100000100283,\n",
       "  100000100284,\n",
       "  100000100285,\n",
       "  100000100286,\n",
       "  100000100287,\n",
       "  100000100288,\n",
       "  100000100289,\n",
       "  100000100290,\n",
       "  100000100291,\n",
       "  100000100292,\n",
       "  100000100293,\n",
       "  100000100294,\n",
       "  100000100295,\n",
       "  100000100296,\n",
       "  100000100297,\n",
       "  100000100298,\n",
       "  100000100299,\n",
       "  100000100300,\n",
       "  100000100301,\n",
       "  100000100302,\n",
       "  100000100303,\n",
       "  100000100304,\n",
       "  100000100305,\n",
       "  100000100306,\n",
       "  100000100307,\n",
       "  100000100308,\n",
       "  100000100309,\n",
       "  100000100310,\n",
       "  100000100311,\n",
       "  100000100312,\n",
       "  100000100313,\n",
       "  100000100314,\n",
       "  100000100315,\n",
       "  100000100316,\n",
       "  100000100317,\n",
       "  100000100318,\n",
       "  100000100319,\n",
       "  100000100320,\n",
       "  100000100321,\n",
       "  100000100322,\n",
       "  100000100323,\n",
       "  100000100324,\n",
       "  100000100325,\n",
       "  100000100326,\n",
       "  100000100327,\n",
       "  100000100328,\n",
       "  100000100329,\n",
       "  100000100330,\n",
       "  100000100331,\n",
       "  100000100332,\n",
       "  100000100333,\n",
       "  100000100334,\n",
       "  100000100335,\n",
       "  100000100336,\n",
       "  100000100337,\n",
       "  100000100338,\n",
       "  100000100339,\n",
       "  100000100340,\n",
       "  100000100341,\n",
       "  100000100342,\n",
       "  100000100343,\n",
       "  100000100344,\n",
       "  100000100345,\n",
       "  100000100346,\n",
       "  100000100347,\n",
       "  100000100348,\n",
       "  100000100349,\n",
       "  100000100350,\n",
       "  100000100351,\n",
       "  100000100352,\n",
       "  100000100353,\n",
       "  100000100354,\n",
       "  100000100355,\n",
       "  100000100356,\n",
       "  100000100357,\n",
       "  100000100358,\n",
       "  100000100359,\n",
       "  100000100360,\n",
       "  100000100361,\n",
       "  100000100362,\n",
       "  100000100363,\n",
       "  100000100364,\n",
       "  100000100365,\n",
       "  100000100366,\n",
       "  100000100367,\n",
       "  100000100368,\n",
       "  100000100369,\n",
       "  100000100370,\n",
       "  100000100371,\n",
       "  100000100372,\n",
       "  100000100373,\n",
       "  100000100374,\n",
       "  100000100375,\n",
       "  100000100376,\n",
       "  100000100377,\n",
       "  100000100378,\n",
       "  100000100379,\n",
       "  100000100380,\n",
       "  100000100381,\n",
       "  100000100382,\n",
       "  100000100383,\n",
       "  100000100384,\n",
       "  100000100385,\n",
       "  100000100386,\n",
       "  100000100387,\n",
       "  100000100388,\n",
       "  100000100389,\n",
       "  100000100390,\n",
       "  100000100391,\n",
       "  100000100392,\n",
       "  100000100393,\n",
       "  100000100394,\n",
       "  100000100395,\n",
       "  100000100396,\n",
       "  100000100397,\n",
       "  100000100398,\n",
       "  100000100399,\n",
       "  100000100400,\n",
       "  100000100401,\n",
       "  100000100402,\n",
       "  100000100403,\n",
       "  100000100404,\n",
       "  100000100405,\n",
       "  100000100406,\n",
       "  100000100407,\n",
       "  100000100408,\n",
       "  100000100409,\n",
       "  100000100410,\n",
       "  100000100411,\n",
       "  100000100412,\n",
       "  100000100413,\n",
       "  100000100414,\n",
       "  100000100415,\n",
       "  100000100416,\n",
       "  100000100417,\n",
       "  100000100418,\n",
       "  100000100419,\n",
       "  100000100420,\n",
       "  100000100421,\n",
       "  100000100422,\n",
       "  100000100423,\n",
       "  100000100424,\n",
       "  100000100425,\n",
       "  100000100426,\n",
       "  100000100427,\n",
       "  100000100428,\n",
       "  100000100429,\n",
       "  100000100430,\n",
       "  100000100431,\n",
       "  100000100432,\n",
       "  100000100433,\n",
       "  100000100434,\n",
       "  100000100435,\n",
       "  100000100436,\n",
       "  100000100437,\n",
       "  100000100438,\n",
       "  100000100439,\n",
       "  100000100440,\n",
       "  100000100441,\n",
       "  100000100442,\n",
       "  100000100443,\n",
       "  100000100444,\n",
       "  100000100445,\n",
       "  100000100446,\n",
       "  100000100447,\n",
       "  100000100448,\n",
       "  100000100449,\n",
       "  100000100450,\n",
       "  100000100451,\n",
       "  100000100452,\n",
       "  100000100453,\n",
       "  100000100454,\n",
       "  100000100455,\n",
       "  100000100456,\n",
       "  100000100457,\n",
       "  100000100458,\n",
       "  100000100459,\n",
       "  100000100460,\n",
       "  100000100461,\n",
       "  100000100462,\n",
       "  100000100463,\n",
       "  100000100464,\n",
       "  100000100465,\n",
       "  100000100466,\n",
       "  100000100467,\n",
       "  100000100468,\n",
       "  100000100469,\n",
       "  100000100470,\n",
       "  100000100471,\n",
       "  100000100472,\n",
       "  100000100473,\n",
       "  100000100474,\n",
       "  100000100475,\n",
       "  100000100476,\n",
       "  100000100477,\n",
       "  100000100478,\n",
       "  100000100479,\n",
       "  100000100480,\n",
       "  100000100481,\n",
       "  100000100482,\n",
       "  100000100483,\n",
       "  100000100484,\n",
       "  100000100485,\n",
       "  100000100486,\n",
       "  100000100487,\n",
       "  100000100488,\n",
       "  100000100489,\n",
       "  100000100490,\n",
       "  100000100491,\n",
       "  100000100492,\n",
       "  100000100493,\n",
       "  100000100494,\n",
       "  100000100495,\n",
       "  100000100496,\n",
       "  100000100497,\n",
       "  100000100498,\n",
       "  100000100499,\n",
       "  100000100500,\n",
       "  100000100501,\n",
       "  100000100502,\n",
       "  100000100503,\n",
       "  100000100504,\n",
       "  100000100505,\n",
       "  100000100506,\n",
       "  100000100507,\n",
       "  100000100508,\n",
       "  100000100509,\n",
       "  100000100510,\n",
       "  100000100511,\n",
       "  100000100512,\n",
       "  100000100513,\n",
       "  100000100514,\n",
       "  100000100515,\n",
       "  100000100516,\n",
       "  100000100517,\n",
       "  100000100518,\n",
       "  100000100519,\n",
       "  100000100520,\n",
       "  100000100521,\n",
       "  100000100522,\n",
       "  100000100523,\n",
       "  100000100524,\n",
       "  100000100525,\n",
       "  100000100526,\n",
       "  100000100527,\n",
       "  100000100528,\n",
       "  100000100529,\n",
       "  100000100530,\n",
       "  100000100531,\n",
       "  100000100532,\n",
       "  100000100533,\n",
       "  100000100534,\n",
       "  100000100535,\n",
       "  100000100536,\n",
       "  100000100537,\n",
       "  100000100538,\n",
       "  100000100539,\n",
       "  100000100540,\n",
       "  100000100541,\n",
       "  100000100542,\n",
       "  100000100543,\n",
       "  100000100544,\n",
       "  100000100545,\n",
       "  100000100546,\n",
       "  100000100547,\n",
       "  100000100548,\n",
       "  100000100549,\n",
       "  100000100550,\n",
       "  100000100551,\n",
       "  100000100552,\n",
       "  100000100553,\n",
       "  100000100554,\n",
       "  100000100555,\n",
       "  100000100556,\n",
       "  100000100557,\n",
       "  100000100558,\n",
       "  100000100559,\n",
       "  100000100560,\n",
       "  100000100561,\n",
       "  100000100562,\n",
       "  100000100563,\n",
       "  100000100564,\n",
       "  100000100565,\n",
       "  100000100566,\n",
       "  100000100567,\n",
       "  100000100568,\n",
       "  100000100569,\n",
       "  100000100570,\n",
       "  100000100571,\n",
       "  100000100572,\n",
       "  100000100573,\n",
       "  100000100574,\n",
       "  100000100575,\n",
       "  100000100576,\n",
       "  100000100577,\n",
       "  100000100578,\n",
       "  100000100579,\n",
       "  100000100580,\n",
       "  100000100581,\n",
       "  100000100582,\n",
       "  100000100583,\n",
       "  100000100584,\n",
       "  100000100585,\n",
       "  100000100586,\n",
       "  100000100587,\n",
       "  100000100588,\n",
       "  100000100589,\n",
       "  100000100590,\n",
       "  100000100591,\n",
       "  100000100592,\n",
       "  100000100593,\n",
       "  100000100594,\n",
       "  100000100595,\n",
       "  100000100596,\n",
       "  100000100597,\n",
       "  100000100598,\n",
       "  100000100599,\n",
       "  100000100600,\n",
       "  100000100601,\n",
       "  100000100602,\n",
       "  100000100603,\n",
       "  100000100604,\n",
       "  100000100605,\n",
       "  100000100606,\n",
       "  100000100607,\n",
       "  100000100608,\n",
       "  100000100609,\n",
       "  100000100610,\n",
       "  100000100611,\n",
       "  100000100612,\n",
       "  100000100613,\n",
       "  100000100614,\n",
       "  100000100615,\n",
       "  100000100616,\n",
       "  100000100617,\n",
       "  100000100618,\n",
       "  100000100619,\n",
       "  100000100620,\n",
       "  100000100621,\n",
       "  100000100622,\n",
       "  100000100623,\n",
       "  100000100624,\n",
       "  100000100625,\n",
       "  100000100626,\n",
       "  100000100627,\n",
       "  100000100628,\n",
       "  100000100629,\n",
       "  100000100630,\n",
       "  100000100631,\n",
       "  100000100632,\n",
       "  100000100633,\n",
       "  100000100634,\n",
       "  100000100635,\n",
       "  100000100636,\n",
       "  100000100637,\n",
       "  100000100638,\n",
       "  100000100639,\n",
       "  100000100640,\n",
       "  100000100641,\n",
       "  100000100642,\n",
       "  100000100643,\n",
       "  100000100644,\n",
       "  100000100645,\n",
       "  100000100646,\n",
       "  100000100647,\n",
       "  100000100648,\n",
       "  100000100649,\n",
       "  100000100650,\n",
       "  100000100651,\n",
       "  100000100652,\n",
       "  100000100653,\n",
       "  100000100654,\n",
       "  100000100655,\n",
       "  100000100656,\n",
       "  100000100657,\n",
       "  100000100658,\n",
       "  100000100659,\n",
       "  100000100660,\n",
       "  100000100661,\n",
       "  100000100662,\n",
       "  100000100663,\n",
       "  100000100664,\n",
       "  100000100665,\n",
       "  100000100666,\n",
       "  100000100667,\n",
       "  100000100668,\n",
       "  100000100669,\n",
       "  100000100670,\n",
       "  100000100671,\n",
       "  100000100672,\n",
       "  100000100673,\n",
       "  100000100674,\n",
       "  100000100675,\n",
       "  100000100676,\n",
       "  100000100677,\n",
       "  100000100678,\n",
       "  100000100679,\n",
       "  100000100680,\n",
       "  100000100681,\n",
       "  100000100682,\n",
       "  100000100683,\n",
       "  100000100684,\n",
       "  100000100685,\n",
       "  100000100686,\n",
       "  100000100687,\n",
       "  100000100688,\n",
       "  100000100689,\n",
       "  100000100690,\n",
       "  100000100691,\n",
       "  100000100692,\n",
       "  100000100693,\n",
       "  100000100694,\n",
       "  100000100695,\n",
       "  100000100696,\n",
       "  100000100697,\n",
       "  100000100698,\n",
       "  100000100699,\n",
       "  100000100700,\n",
       "  100000100701,\n",
       "  100000100702,\n",
       "  100000100703,\n",
       "  100000100704,\n",
       "  100000100705,\n",
       "  100000100706,\n",
       "  100000100707,\n",
       "  100000100708,\n",
       "  100000100709,\n",
       "  100000100710,\n",
       "  100000100711,\n",
       "  100000100712,\n",
       "  100000100713,\n",
       "  100000100714,\n",
       "  100000100715,\n",
       "  100000100716,\n",
       "  100000100717,\n",
       "  100000100718,\n",
       "  100000100719,\n",
       "  100000100720,\n",
       "  100000100721,\n",
       "  100000100722,\n",
       "  100000100723,\n",
       "  100000100724,\n",
       "  100000100725,\n",
       "  100000100726,\n",
       "  100000100727,\n",
       "  100000100728,\n",
       "  100000100729,\n",
       "  100000100730,\n",
       "  100000100731,\n",
       "  100000100732,\n",
       "  100000100733,\n",
       "  100000100734,\n",
       "  100000100735,\n",
       "  100000100736,\n",
       "  100000100737,\n",
       "  100000100738,\n",
       "  100000100739,\n",
       "  100000100740,\n",
       "  100000100741,\n",
       "  100000100742,\n",
       "  100000100743,\n",
       "  100000100744,\n",
       "  100000100745,\n",
       "  100000100746,\n",
       "  100000100747,\n",
       "  100000100748,\n",
       "  100000100749,\n",
       "  100000100750,\n",
       "  100000100751,\n",
       "  100000100752,\n",
       "  100000100753,\n",
       "  100000100754,\n",
       "  100000100755,\n",
       "  100000100756,\n",
       "  100000100757,\n",
       "  100000100758,\n",
       "  100000100759,\n",
       "  100000100760,\n",
       "  100000100761,\n",
       "  100000100762,\n",
       "  100000100763,\n",
       "  100000100764,\n",
       "  100000100765,\n",
       "  100000100766,\n",
       "  100000100767,\n",
       "  100000100768,\n",
       "  100000100769,\n",
       "  100000100770,\n",
       "  100000100771,\n",
       "  100000100772,\n",
       "  100000100773,\n",
       "  100000100774,\n",
       "  100000100775,\n",
       "  100000100776,\n",
       "  100000100777,\n",
       "  100000100778,\n",
       "  100000100779,\n",
       "  100000100780,\n",
       "  100000100781,\n",
       "  100000100782,\n",
       "  100000100783,\n",
       "  100000100784,\n",
       "  100000100785,\n",
       "  100000100786,\n",
       "  100000100787,\n",
       "  100000100788,\n",
       "  100000100789,\n",
       "  100000100790,\n",
       "  100000100791,\n",
       "  100000100792,\n",
       "  100000100793,\n",
       "  100000100794,\n",
       "  100000100795,\n",
       "  100000100796,\n",
       "  100000100797,\n",
       "  100000100798,\n",
       "  100000100799,\n",
       "  100000100800,\n",
       "  100000100801,\n",
       "  100000100802,\n",
       "  100000100803,\n",
       "  100000100804,\n",
       "  100000100805,\n",
       "  100000100806,\n",
       "  100000100807,\n",
       "  100000100808,\n",
       "  100000100809,\n",
       "  100000100810,\n",
       "  100000100811,\n",
       "  100000100812,\n",
       "  100000100813,\n",
       "  100000100814,\n",
       "  100000100815,\n",
       "  100000100816,\n",
       "  100000100817,\n",
       "  100000100818,\n",
       "  100000100819,\n",
       "  100000100820,\n",
       "  100000100821,\n",
       "  100000100822,\n",
       "  100000100823,\n",
       "  100000100824,\n",
       "  100000100825,\n",
       "  100000100826,\n",
       "  100000100827,\n",
       "  100000100828,\n",
       "  100000100829,\n",
       "  100000100830,\n",
       "  100000100831,\n",
       "  100000100832,\n",
       "  100000100833,\n",
       "  100000100834,\n",
       "  100000100835,\n",
       "  100000100836,\n",
       "  100000100837,\n",
       "  100000100838,\n",
       "  100000100839,\n",
       "  100000100840,\n",
       "  100000100841,\n",
       "  100000100842,\n",
       "  100000100843,\n",
       "  100000100844,\n",
       "  100000100845,\n",
       "  100000100846,\n",
       "  100000100847,\n",
       "  100000100848,\n",
       "  100000100849,\n",
       "  100000100850,\n",
       "  100000100851,\n",
       "  100000100852,\n",
       "  100000100853,\n",
       "  100000100854,\n",
       "  100000100855,\n",
       "  100000100856,\n",
       "  100000100857,\n",
       "  100000100858,\n",
       "  100000100859,\n",
       "  100000100860,\n",
       "  100000100861,\n",
       "  100000100862,\n",
       "  100000100863,\n",
       "  100000100864,\n",
       "  100000100865,\n",
       "  100000100866,\n",
       "  100000100867,\n",
       "  100000100868,\n",
       "  100000100869,\n",
       "  100000100870,\n",
       "  100000100871,\n",
       "  100000100872,\n",
       "  100000100873,\n",
       "  100000100874,\n",
       "  100000100875,\n",
       "  100000100876,\n",
       "  100000100877,\n",
       "  100000100878,\n",
       "  100000100879,\n",
       "  100000100880,\n",
       "  100000100881,\n",
       "  100000100882,\n",
       "  100000100883,\n",
       "  100000100884,\n",
       "  100000100885,\n",
       "  100000100886,\n",
       "  100000100887,\n",
       "  100000100888,\n",
       "  100000100889,\n",
       "  100000100890,\n",
       "  100000100891,\n",
       "  100000100892,\n",
       "  100000100893,\n",
       "  100000100894,\n",
       "  100000100895,\n",
       "  100000100896,\n",
       "  100000100897,\n",
       "  100000100898,\n",
       "  100000100899,\n",
       "  100000100900,\n",
       "  100000100901,\n",
       "  100000100902,\n",
       "  100000100903,\n",
       "  100000100904,\n",
       "  100000100905,\n",
       "  100000100906,\n",
       "  100000100907,\n",
       "  100000100908,\n",
       "  100000100909,\n",
       "  100000100910,\n",
       "  100000100911,\n",
       "  100000100912,\n",
       "  100000100913,\n",
       "  100000100914,\n",
       "  100000100915,\n",
       "  100000100916,\n",
       "  100000100917,\n",
       "  100000100918,\n",
       "  100000100919,\n",
       "  100000100920,\n",
       "  100000100921,\n",
       "  100000100922,\n",
       "  100000100923,\n",
       "  100000100924,\n",
       "  100000100925,\n",
       "  100000100926,\n",
       "  100000100927,\n",
       "  100000100928,\n",
       "  100000100929,\n",
       "  100000100930,\n",
       "  100000100931,\n",
       "  100000100932,\n",
       "  100000100933,\n",
       "  100000100934,\n",
       "  100000100935,\n",
       "  100000100936,\n",
       "  100000100937,\n",
       "  100000100938,\n",
       "  100000100939,\n",
       "  100000100940,\n",
       "  100000100941,\n",
       "  100000100942,\n",
       "  100000100943,\n",
       "  100000100944,\n",
       "  100000100945,\n",
       "  100000100946,\n",
       "  100000100947,\n",
       "  100000100948,\n",
       "  100000100949,\n",
       "  100000100950,\n",
       "  100000100951,\n",
       "  100000100952,\n",
       "  100000100953,\n",
       "  100000100954,\n",
       "  100000100955,\n",
       "  100000100956,\n",
       "  100000100957,\n",
       "  100000100958,\n",
       "  100000100959,\n",
       "  100000100960,\n",
       "  100000100961,\n",
       "  100000100962,\n",
       "  100000100963,\n",
       "  100000100964,\n",
       "  100000100965,\n",
       "  100000100966,\n",
       "  100000100967,\n",
       "  100000100968,\n",
       "  100000100969,\n",
       "  100000100970,\n",
       "  100000100971,\n",
       "  100000100972,\n",
       "  100000100973,\n",
       "  100000100974,\n",
       "  100000100975,\n",
       "  100000100976,\n",
       "  100000100977,\n",
       "  100000100978,\n",
       "  100000100979,\n",
       "  100000100980,\n",
       "  100000100981,\n",
       "  100000100982,\n",
       "  100000100983,\n",
       "  100000100984,\n",
       "  100000100985,\n",
       "  100000100986,\n",
       "  100000100987,\n",
       "  100000100988,\n",
       "  100000100989,\n",
       "  100000100990,\n",
       "  100000100991,\n",
       "  100000100992,\n",
       "  100000100993,\n",
       "  100000100994,\n",
       "  100000100995,\n",
       "  100000100996,\n",
       "  100000100997,\n",
       "  100000100998,\n",
       "  100000100999,\n",
       "  ...],\n",
       " [100000100000,\n",
       "  100000100001,\n",
       "  100000100002,\n",
       "  100000100003,\n",
       "  100000100004,\n",
       "  100000100005,\n",
       "  100000100006,\n",
       "  100000100007,\n",
       "  100000100008,\n",
       "  100000100009,\n",
       "  100000100010,\n",
       "  100000100011,\n",
       "  100000100012,\n",
       "  100000100013,\n",
       "  100000100014,\n",
       "  100000100015,\n",
       "  100000100016,\n",
       "  100000100017,\n",
       "  100000100018,\n",
       "  100000100019,\n",
       "  100000100020,\n",
       "  100000100021,\n",
       "  100000100022,\n",
       "  100000100023,\n",
       "  100000100024,\n",
       "  100000100025,\n",
       "  100000100026,\n",
       "  100000100027,\n",
       "  100000100028,\n",
       "  100000100029,\n",
       "  100000100030,\n",
       "  100000100031,\n",
       "  100000100032,\n",
       "  100000100033,\n",
       "  100000100034,\n",
       "  100000100035,\n",
       "  100000100036,\n",
       "  100000100037,\n",
       "  100000100038,\n",
       "  100000100039,\n",
       "  100000100040,\n",
       "  100000100041,\n",
       "  100000100042,\n",
       "  100000100043,\n",
       "  100000100044,\n",
       "  100000100045,\n",
       "  100000100046,\n",
       "  100000100047,\n",
       "  100000100048,\n",
       "  100000100049,\n",
       "  100000100050,\n",
       "  100000100051,\n",
       "  100000100052,\n",
       "  100000100053,\n",
       "  100000100054,\n",
       "  100000100055,\n",
       "  100000100056,\n",
       "  100000100057,\n",
       "  100000100058,\n",
       "  100000100059,\n",
       "  100000100060,\n",
       "  100000100061,\n",
       "  100000100062,\n",
       "  100000100063,\n",
       "  100000100064,\n",
       "  100000100065,\n",
       "  100000100066,\n",
       "  100000100067,\n",
       "  100000100068,\n",
       "  100000100069,\n",
       "  100000100070,\n",
       "  100000100071,\n",
       "  100000100072,\n",
       "  100000100073,\n",
       "  100000100074,\n",
       "  100000100075,\n",
       "  100000100076,\n",
       "  100000100077,\n",
       "  100000100078,\n",
       "  100000100079,\n",
       "  100000100080,\n",
       "  100000100081,\n",
       "  100000100082,\n",
       "  100000100083,\n",
       "  100000100084,\n",
       "  100000100085,\n",
       "  100000100086,\n",
       "  100000100087,\n",
       "  100000100088,\n",
       "  100000100089,\n",
       "  100000100090,\n",
       "  100000100091,\n",
       "  100000100092,\n",
       "  100000100093,\n",
       "  100000100094,\n",
       "  100000100095,\n",
       "  100000100096,\n",
       "  100000100097,\n",
       "  100000100098,\n",
       "  100000100099,\n",
       "  100000100100,\n",
       "  100000100101,\n",
       "  100000100102,\n",
       "  100000100103,\n",
       "  100000100104,\n",
       "  100000100105,\n",
       "  100000100106,\n",
       "  100000100107,\n",
       "  100000100108,\n",
       "  100000100109,\n",
       "  100000100110,\n",
       "  100000100111,\n",
       "  100000100112,\n",
       "  100000100113,\n",
       "  100000100114,\n",
       "  100000100115,\n",
       "  100000100116,\n",
       "  100000100117,\n",
       "  100000100118,\n",
       "  100000100119,\n",
       "  100000100120,\n",
       "  100000100121,\n",
       "  100000100122,\n",
       "  100000100123,\n",
       "  100000100124,\n",
       "  100000100125,\n",
       "  100000100126,\n",
       "  100000100127,\n",
       "  100000100128,\n",
       "  100000100129,\n",
       "  100000100130,\n",
       "  100000100131,\n",
       "  100000100132,\n",
       "  100000100133,\n",
       "  100000100134,\n",
       "  100000100135,\n",
       "  100000100136,\n",
       "  100000100137,\n",
       "  100000100138,\n",
       "  100000100139,\n",
       "  100000100140,\n",
       "  100000100141,\n",
       "  100000100142,\n",
       "  100000100143,\n",
       "  100000100144,\n",
       "  100000100145,\n",
       "  100000100146,\n",
       "  100000100147,\n",
       "  100000100148,\n",
       "  100000100149,\n",
       "  100000100150,\n",
       "  100000100151,\n",
       "  100000100152,\n",
       "  100000100153,\n",
       "  100000100154,\n",
       "  100000100155,\n",
       "  100000100156,\n",
       "  100000100157,\n",
       "  100000100158,\n",
       "  100000100159,\n",
       "  100000100160,\n",
       "  100000100161,\n",
       "  100000100162,\n",
       "  100000100163,\n",
       "  100000100164,\n",
       "  100000100165,\n",
       "  100000100166,\n",
       "  100000100167,\n",
       "  100000100168,\n",
       "  100000100169,\n",
       "  100000100170,\n",
       "  100000100171,\n",
       "  100000100172,\n",
       "  100000100173,\n",
       "  100000100174,\n",
       "  100000100175,\n",
       "  100000100176,\n",
       "  100000100177,\n",
       "  100000100178,\n",
       "  100000100179,\n",
       "  100000100180,\n",
       "  100000100181,\n",
       "  100000100182,\n",
       "  100000100183,\n",
       "  100000100184,\n",
       "  100000100185,\n",
       "  100000100186,\n",
       "  100000100187,\n",
       "  100000100188,\n",
       "  100000100189,\n",
       "  100000100190,\n",
       "  100000100191,\n",
       "  100000100192,\n",
       "  100000100193,\n",
       "  100000100194,\n",
       "  100000100195,\n",
       "  100000100196,\n",
       "  100000100197,\n",
       "  100000100198,\n",
       "  100000100199,\n",
       "  100000100200,\n",
       "  100000100201,\n",
       "  100000100202,\n",
       "  100000100203,\n",
       "  100000100204,\n",
       "  100000100205,\n",
       "  100000100206,\n",
       "  100000100207,\n",
       "  100000100208,\n",
       "  100000100209,\n",
       "  100000100210,\n",
       "  100000100211,\n",
       "  100000100212,\n",
       "  100000100213,\n",
       "  100000100214,\n",
       "  100000100215,\n",
       "  100000100216,\n",
       "  100000100217,\n",
       "  100000100218,\n",
       "  100000100219,\n",
       "  100000100220,\n",
       "  100000100221,\n",
       "  100000100222,\n",
       "  100000100223,\n",
       "  100000100224,\n",
       "  100000100225,\n",
       "  100000100226,\n",
       "  100000100227,\n",
       "  100000100228,\n",
       "  100000100229,\n",
       "  100000100230,\n",
       "  100000100231,\n",
       "  100000100232,\n",
       "  100000100233,\n",
       "  100000100234,\n",
       "  100000100235,\n",
       "  100000100236,\n",
       "  100000100237,\n",
       "  100000100238,\n",
       "  100000100239,\n",
       "  100000100240,\n",
       "  100000100241,\n",
       "  100000100242,\n",
       "  100000100243,\n",
       "  100000100244,\n",
       "  100000100245,\n",
       "  100000100246,\n",
       "  100000100247,\n",
       "  100000100248,\n",
       "  100000100249,\n",
       "  100000100250,\n",
       "  100000100251,\n",
       "  100000100252,\n",
       "  100000100253,\n",
       "  100000100254,\n",
       "  100000100255,\n",
       "  100000100256,\n",
       "  100000100257,\n",
       "  100000100258,\n",
       "  100000100259,\n",
       "  100000100260,\n",
       "  100000100261,\n",
       "  100000100262,\n",
       "  100000100263,\n",
       "  100000100264,\n",
       "  100000100265,\n",
       "  100000100266,\n",
       "  100000100267,\n",
       "  100000100268,\n",
       "  100000100269,\n",
       "  100000100270,\n",
       "  100000100271,\n",
       "  100000100272,\n",
       "  100000100273,\n",
       "  100000100274,\n",
       "  100000100275,\n",
       "  100000100276,\n",
       "  100000100277,\n",
       "  100000100278,\n",
       "  100000100279,\n",
       "  100000100280,\n",
       "  100000100281,\n",
       "  100000100282,\n",
       "  100000100283,\n",
       "  100000100284,\n",
       "  100000100285,\n",
       "  100000100286,\n",
       "  100000100287,\n",
       "  100000100288,\n",
       "  100000100289,\n",
       "  100000100290,\n",
       "  100000100291,\n",
       "  100000100292,\n",
       "  100000100293,\n",
       "  100000100294,\n",
       "  100000100295,\n",
       "  100000100296,\n",
       "  100000100297,\n",
       "  100000100298,\n",
       "  100000100299,\n",
       "  100000100300,\n",
       "  100000100301,\n",
       "  100000100302,\n",
       "  100000100303,\n",
       "  100000100304,\n",
       "  100000100305,\n",
       "  100000100306,\n",
       "  100000100307,\n",
       "  100000100308,\n",
       "  100000100309,\n",
       "  100000100310,\n",
       "  100000100311,\n",
       "  100000100312,\n",
       "  100000100313,\n",
       "  100000100314,\n",
       "  100000100315,\n",
       "  100000100316,\n",
       "  100000100317,\n",
       "  100000100318,\n",
       "  100000100319,\n",
       "  100000100320,\n",
       "  100000100321,\n",
       "  100000100322,\n",
       "  100000100323,\n",
       "  100000100324,\n",
       "  100000100325,\n",
       "  100000100326,\n",
       "  100000100327,\n",
       "  100000100328,\n",
       "  100000100329,\n",
       "  100000100330,\n",
       "  100000100331,\n",
       "  100000100332,\n",
       "  100000100333,\n",
       "  100000100334,\n",
       "  100000100335,\n",
       "  100000100336,\n",
       "  100000100337,\n",
       "  100000100338,\n",
       "  100000100339,\n",
       "  100000100340,\n",
       "  100000100341,\n",
       "  100000100342,\n",
       "  100000100343,\n",
       "  100000100344,\n",
       "  100000100345,\n",
       "  100000100346,\n",
       "  100000100347,\n",
       "  100000100348,\n",
       "  100000100349,\n",
       "  100000100350,\n",
       "  100000100351,\n",
       "  100000100352,\n",
       "  100000100353,\n",
       "  100000100354,\n",
       "  100000100355,\n",
       "  100000100356,\n",
       "  100000100357,\n",
       "  100000100358,\n",
       "  100000100359,\n",
       "  100000100360,\n",
       "  100000100361,\n",
       "  100000100362,\n",
       "  100000100363,\n",
       "  100000100364,\n",
       "  100000100365,\n",
       "  100000100366,\n",
       "  100000100367,\n",
       "  100000100368,\n",
       "  100000100369,\n",
       "  100000100370,\n",
       "  100000100371,\n",
       "  100000100372,\n",
       "  100000100373,\n",
       "  100000100374,\n",
       "  100000100375,\n",
       "  100000100376,\n",
       "  100000100377,\n",
       "  100000100378,\n",
       "  100000100379,\n",
       "  100000100380,\n",
       "  100000100381,\n",
       "  100000100382,\n",
       "  100000100383,\n",
       "  100000100384,\n",
       "  100000100385,\n",
       "  100000100386,\n",
       "  100000100387,\n",
       "  100000100388,\n",
       "  100000100389,\n",
       "  100000100390,\n",
       "  100000100391,\n",
       "  100000100392,\n",
       "  100000100393,\n",
       "  100000100394,\n",
       "  100000100395,\n",
       "  100000100396,\n",
       "  100000100397,\n",
       "  100000100398,\n",
       "  100000100399,\n",
       "  100000100400,\n",
       "  100000100401,\n",
       "  100000100402,\n",
       "  100000100403,\n",
       "  100000100404,\n",
       "  100000100405,\n",
       "  100000100406,\n",
       "  100000100407,\n",
       "  100000100408,\n",
       "  100000100409,\n",
       "  100000100410,\n",
       "  100000100411,\n",
       "  100000100412,\n",
       "  100000100413,\n",
       "  100000100414,\n",
       "  100000100415,\n",
       "  100000100416,\n",
       "  100000100417,\n",
       "  100000100418,\n",
       "  100000100419,\n",
       "  100000100420,\n",
       "  100000100421,\n",
       "  100000100422,\n",
       "  100000100423,\n",
       "  100000100424,\n",
       "  100000100425,\n",
       "  100000100426,\n",
       "  100000100427,\n",
       "  100000100428,\n",
       "  100000100429,\n",
       "  100000100430,\n",
       "  100000100431,\n",
       "  100000100432,\n",
       "  100000100433,\n",
       "  100000100434,\n",
       "  100000100435,\n",
       "  100000100436,\n",
       "  100000100437,\n",
       "  100000100438,\n",
       "  100000100439,\n",
       "  100000100440,\n",
       "  100000100441,\n",
       "  100000100442,\n",
       "  100000100443,\n",
       "  100000100444,\n",
       "  100000100445,\n",
       "  100000100446,\n",
       "  100000100447,\n",
       "  100000100448,\n",
       "  100000100449,\n",
       "  100000100450,\n",
       "  100000100451,\n",
       "  100000100452,\n",
       "  100000100453,\n",
       "  100000100454,\n",
       "  100000100455,\n",
       "  100000100456,\n",
       "  100000100457,\n",
       "  100000100458,\n",
       "  100000100459,\n",
       "  100000100460,\n",
       "  100000100461,\n",
       "  100000100462,\n",
       "  100000100463,\n",
       "  100000100464,\n",
       "  100000100465,\n",
       "  100000100466,\n",
       "  100000100467,\n",
       "  100000100468,\n",
       "  100000100469,\n",
       "  100000100470,\n",
       "  100000100471,\n",
       "  100000100472,\n",
       "  100000100473,\n",
       "  100000100474,\n",
       "  100000100475,\n",
       "  100000100476,\n",
       "  100000100477,\n",
       "  100000100478,\n",
       "  100000100479,\n",
       "  100000100480,\n",
       "  100000100481,\n",
       "  100000100482,\n",
       "  100000100483,\n",
       "  100000100484,\n",
       "  100000100485,\n",
       "  100000100486,\n",
       "  100000100487,\n",
       "  100000100488,\n",
       "  100000100489,\n",
       "  100000100490,\n",
       "  100000100491,\n",
       "  100000100492,\n",
       "  100000100493,\n",
       "  100000100494,\n",
       "  100000100495,\n",
       "  100000100496,\n",
       "  100000100497,\n",
       "  100000100498,\n",
       "  100000100499,\n",
       "  100000100500,\n",
       "  100000100501,\n",
       "  100000100502,\n",
       "  100000100503,\n",
       "  100000100504,\n",
       "  100000100505,\n",
       "  100000100506,\n",
       "  100000100507,\n",
       "  100000100508,\n",
       "  100000100509,\n",
       "  100000100510,\n",
       "  100000100511,\n",
       "  100000100512,\n",
       "  100000100513,\n",
       "  100000100514,\n",
       "  100000100515,\n",
       "  100000100516,\n",
       "  100000100517,\n",
       "  100000100518,\n",
       "  100000100519,\n",
       "  100000100520,\n",
       "  100000100521,\n",
       "  100000100522,\n",
       "  100000100523,\n",
       "  100000100524,\n",
       "  100000100525,\n",
       "  100000100526,\n",
       "  100000100527,\n",
       "  100000100528,\n",
       "  100000100529,\n",
       "  100000100530,\n",
       "  100000100531,\n",
       "  100000100532,\n",
       "  100000100533,\n",
       "  100000100534,\n",
       "  100000100535,\n",
       "  100000100536,\n",
       "  100000100537,\n",
       "  100000100538,\n",
       "  100000100539,\n",
       "  100000100540,\n",
       "  100000100541,\n",
       "  100000100542,\n",
       "  100000100543,\n",
       "  100000100544,\n",
       "  100000100545,\n",
       "  100000100546,\n",
       "  100000100547,\n",
       "  100000100548,\n",
       "  100000100549,\n",
       "  100000100550,\n",
       "  100000100551,\n",
       "  100000100552,\n",
       "  100000100553,\n",
       "  100000100554,\n",
       "  100000100555,\n",
       "  100000100556,\n",
       "  100000100557,\n",
       "  100000100558,\n",
       "  100000100559,\n",
       "  100000100560,\n",
       "  100000100561,\n",
       "  100000100562,\n",
       "  100000100563,\n",
       "  100000100564,\n",
       "  100000100565,\n",
       "  100000100566,\n",
       "  100000100567,\n",
       "  100000100568,\n",
       "  100000100569,\n",
       "  100000100570,\n",
       "  100000100571,\n",
       "  100000100572,\n",
       "  100000100573,\n",
       "  100000100574,\n",
       "  100000100575,\n",
       "  100000100576,\n",
       "  100000100577,\n",
       "  100000100578,\n",
       "  100000100579,\n",
       "  100000100580,\n",
       "  100000100581,\n",
       "  100000100582,\n",
       "  100000100583,\n",
       "  100000100584,\n",
       "  100000100585,\n",
       "  100000100586,\n",
       "  100000100587,\n",
       "  100000100588,\n",
       "  100000100589,\n",
       "  100000100590,\n",
       "  100000100591,\n",
       "  100000100592,\n",
       "  100000100593,\n",
       "  100000100594,\n",
       "  100000100595,\n",
       "  100000100596,\n",
       "  100000100597,\n",
       "  100000100598,\n",
       "  100000100599,\n",
       "  100000100600,\n",
       "  100000100601,\n",
       "  100000100602,\n",
       "  100000100603,\n",
       "  100000100604,\n",
       "  100000100605,\n",
       "  100000100606,\n",
       "  100000100607,\n",
       "  100000100608,\n",
       "  100000100609,\n",
       "  100000100610,\n",
       "  100000100611,\n",
       "  100000100612,\n",
       "  100000100613,\n",
       "  100000100614,\n",
       "  100000100615,\n",
       "  100000100616,\n",
       "  100000100617,\n",
       "  100000100618,\n",
       "  100000100619,\n",
       "  100000100620,\n",
       "  100000100621,\n",
       "  100000100622,\n",
       "  100000100623,\n",
       "  100000100624,\n",
       "  100000100625,\n",
       "  100000100626,\n",
       "  100000100627,\n",
       "  100000100628,\n",
       "  100000100629,\n",
       "  100000100630,\n",
       "  100000100631,\n",
       "  100000100632,\n",
       "  100000100633,\n",
       "  100000100634,\n",
       "  100000100635,\n",
       "  100000100636,\n",
       "  100000100637,\n",
       "  100000100638,\n",
       "  100000100639,\n",
       "  100000100640,\n",
       "  100000100641,\n",
       "  100000100642,\n",
       "  100000100643,\n",
       "  100000100644,\n",
       "  100000100645,\n",
       "  100000100646,\n",
       "  100000100647,\n",
       "  100000100648,\n",
       "  100000100649,\n",
       "  100000100650,\n",
       "  100000100651,\n",
       "  100000100652,\n",
       "  100000100653,\n",
       "  100000100654,\n",
       "  100000100655,\n",
       "  100000100656,\n",
       "  100000100657,\n",
       "  100000100658,\n",
       "  100000100659,\n",
       "  100000100660,\n",
       "  100000100661,\n",
       "  100000100662,\n",
       "  100000100663,\n",
       "  100000100664,\n",
       "  100000100665,\n",
       "  100000100666,\n",
       "  100000100667,\n",
       "  100000100668,\n",
       "  100000100669,\n",
       "  100000100670,\n",
       "  100000100671,\n",
       "  100000100672,\n",
       "  100000100673,\n",
       "  100000100674,\n",
       "  100000100675,\n",
       "  100000100676,\n",
       "  100000100677,\n",
       "  100000100678,\n",
       "  100000100679,\n",
       "  100000100680,\n",
       "  100000100681,\n",
       "  100000100682,\n",
       "  100000100683,\n",
       "  100000100684,\n",
       "  100000100685,\n",
       "  100000100686,\n",
       "  100000100687,\n",
       "  100000100688,\n",
       "  100000100689,\n",
       "  100000100690,\n",
       "  100000100691,\n",
       "  100000100692,\n",
       "  100000100693,\n",
       "  100000100694,\n",
       "  100000100695,\n",
       "  100000100696,\n",
       "  100000100697,\n",
       "  100000100698,\n",
       "  100000100699,\n",
       "  100000100700,\n",
       "  100000100701,\n",
       "  100000100702,\n",
       "  100000100703,\n",
       "  100000100704,\n",
       "  100000100705,\n",
       "  100000100706,\n",
       "  100000100707,\n",
       "  100000100708,\n",
       "  100000100709,\n",
       "  100000100710,\n",
       "  100000100711,\n",
       "  100000100712,\n",
       "  100000100713,\n",
       "  100000100714,\n",
       "  100000100715,\n",
       "  100000100716,\n",
       "  100000100717,\n",
       "  100000100718,\n",
       "  100000100719,\n",
       "  100000100720,\n",
       "  100000100721,\n",
       "  100000100722,\n",
       "  100000100723,\n",
       "  100000100724,\n",
       "  100000100725,\n",
       "  100000100726,\n",
       "  100000100727,\n",
       "  100000100728,\n",
       "  100000100729,\n",
       "  100000100730,\n",
       "  100000100731,\n",
       "  100000100732,\n",
       "  100000100733,\n",
       "  100000100734,\n",
       "  100000100735,\n",
       "  100000100736,\n",
       "  100000100737,\n",
       "  100000100738,\n",
       "  100000100739,\n",
       "  100000100740,\n",
       "  100000100741,\n",
       "  100000100742,\n",
       "  100000100743,\n",
       "  100000100744,\n",
       "  100000100745,\n",
       "  100000100746,\n",
       "  100000100747,\n",
       "  100000100748,\n",
       "  100000100749,\n",
       "  100000100750,\n",
       "  100000100751,\n",
       "  100000100752,\n",
       "  100000100753,\n",
       "  100000100754,\n",
       "  100000100755,\n",
       "  100000100756,\n",
       "  100000100757,\n",
       "  100000100758,\n",
       "  100000100759,\n",
       "  100000100760,\n",
       "  100000100761,\n",
       "  100000100762,\n",
       "  100000100763,\n",
       "  100000100764,\n",
       "  100000100765,\n",
       "  100000100766,\n",
       "  100000100767,\n",
       "  100000100768,\n",
       "  100000100769,\n",
       "  100000100770,\n",
       "  100000100771,\n",
       "  100000100772,\n",
       "  100000100773,\n",
       "  100000100774,\n",
       "  100000100775,\n",
       "  100000100776,\n",
       "  100000100777,\n",
       "  100000100778,\n",
       "  100000100779,\n",
       "  100000100780,\n",
       "  100000100781,\n",
       "  100000100782,\n",
       "  100000100783,\n",
       "  100000100784,\n",
       "  100000100785,\n",
       "  100000100786,\n",
       "  100000100787,\n",
       "  100000100788,\n",
       "  100000100789,\n",
       "  100000100790,\n",
       "  100000100791,\n",
       "  100000100792,\n",
       "  100000100793,\n",
       "  100000100794,\n",
       "  100000100795,\n",
       "  100000100796,\n",
       "  100000100797,\n",
       "  100000100798,\n",
       "  100000100799,\n",
       "  100000100800,\n",
       "  100000100801,\n",
       "  100000100802,\n",
       "  100000100803,\n",
       "  100000100804,\n",
       "  100000100805,\n",
       "  100000100806,\n",
       "  100000100807,\n",
       "  100000100808,\n",
       "  100000100809,\n",
       "  100000100810,\n",
       "  100000100811,\n",
       "  100000100812,\n",
       "  100000100813,\n",
       "  100000100814,\n",
       "  100000100815,\n",
       "  100000100816,\n",
       "  100000100817,\n",
       "  100000100818,\n",
       "  100000100819,\n",
       "  100000100820,\n",
       "  100000100821,\n",
       "  100000100822,\n",
       "  100000100823,\n",
       "  100000100824,\n",
       "  100000100825,\n",
       "  100000100826,\n",
       "  100000100827,\n",
       "  100000100828,\n",
       "  100000100829,\n",
       "  100000100830,\n",
       "  100000100831,\n",
       "  100000100832,\n",
       "  100000100833,\n",
       "  100000100834,\n",
       "  100000100835,\n",
       "  100000100836,\n",
       "  100000100837,\n",
       "  100000100838,\n",
       "  100000100839,\n",
       "  100000100840,\n",
       "  100000100841,\n",
       "  100000100842,\n",
       "  100000100843,\n",
       "  100000100844,\n",
       "  100000100845,\n",
       "  100000100846,\n",
       "  100000100847,\n",
       "  100000100848,\n",
       "  100000100849,\n",
       "  100000100850,\n",
       "  100000100851,\n",
       "  100000100852,\n",
       "  100000100853,\n",
       "  100000100854,\n",
       "  100000100855,\n",
       "  100000100856,\n",
       "  100000100857,\n",
       "  100000100858,\n",
       "  100000100859,\n",
       "  100000100860,\n",
       "  100000100861,\n",
       "  100000100862,\n",
       "  100000100863,\n",
       "  100000100864,\n",
       "  100000100865,\n",
       "  100000100866,\n",
       "  100000100867,\n",
       "  100000100868,\n",
       "  100000100869,\n",
       "  100000100870,\n",
       "  100000100871,\n",
       "  100000100872,\n",
       "  100000100873,\n",
       "  100000100874,\n",
       "  100000100875,\n",
       "  100000100876,\n",
       "  100000100877,\n",
       "  100000100878,\n",
       "  100000100879,\n",
       "  100000100880,\n",
       "  100000100881,\n",
       "  100000100882,\n",
       "  100000100883,\n",
       "  100000100884,\n",
       "  100000100885,\n",
       "  100000100886,\n",
       "  100000100887,\n",
       "  100000100888,\n",
       "  100000100889,\n",
       "  100000100890,\n",
       "  100000100891,\n",
       "  100000100892,\n",
       "  100000100893,\n",
       "  100000100894,\n",
       "  100000100895,\n",
       "  100000100896,\n",
       "  100000100897,\n",
       "  100000100898,\n",
       "  100000100899,\n",
       "  100000100900,\n",
       "  100000100901,\n",
       "  100000100902,\n",
       "  100000100903,\n",
       "  100000100904,\n",
       "  100000100905,\n",
       "  100000100906,\n",
       "  100000100907,\n",
       "  100000100908,\n",
       "  100000100909,\n",
       "  100000100910,\n",
       "  100000100911,\n",
       "  100000100912,\n",
       "  100000100913,\n",
       "  100000100914,\n",
       "  100000100915,\n",
       "  100000100916,\n",
       "  100000100917,\n",
       "  100000100918,\n",
       "  100000100919,\n",
       "  100000100920,\n",
       "  100000100921,\n",
       "  100000100922,\n",
       "  100000100923,\n",
       "  100000100924,\n",
       "  100000100925,\n",
       "  100000100926,\n",
       "  100000100927,\n",
       "  100000100928,\n",
       "  100000100929,\n",
       "  100000100930,\n",
       "  100000100931,\n",
       "  100000100932,\n",
       "  100000100933,\n",
       "  100000100934,\n",
       "  100000100935,\n",
       "  100000100936,\n",
       "  100000100937,\n",
       "  100000100938,\n",
       "  100000100939,\n",
       "  100000100940,\n",
       "  100000100941,\n",
       "  100000100942,\n",
       "  100000100943,\n",
       "  100000100944,\n",
       "  100000100945,\n",
       "  100000100946,\n",
       "  100000100947,\n",
       "  100000100948,\n",
       "  100000100949,\n",
       "  100000100950,\n",
       "  100000100951,\n",
       "  100000100952,\n",
       "  100000100953,\n",
       "  100000100954,\n",
       "  100000100955,\n",
       "  100000100956,\n",
       "  100000100957,\n",
       "  100000100958,\n",
       "  100000100959,\n",
       "  100000100960,\n",
       "  100000100961,\n",
       "  100000100962,\n",
       "  100000100963,\n",
       "  100000100964,\n",
       "  100000100965,\n",
       "  100000100966,\n",
       "  100000100967,\n",
       "  100000100968,\n",
       "  100000100969,\n",
       "  100000100970,\n",
       "  100000100971,\n",
       "  100000100972,\n",
       "  100000100973,\n",
       "  100000100974,\n",
       "  100000100975,\n",
       "  100000100976,\n",
       "  100000100977,\n",
       "  100000100978,\n",
       "  100000100979,\n",
       "  100000100980,\n",
       "  100000100981,\n",
       "  100000100982,\n",
       "  100000100983,\n",
       "  100000100984,\n",
       "  100000100985,\n",
       "  100000100986,\n",
       "  100000100987,\n",
       "  100000100988,\n",
       "  100000100989,\n",
       "  100000100990,\n",
       "  100000100991,\n",
       "  100000100992,\n",
       "  100000100993,\n",
       "  100000100994,\n",
       "  100000100995,\n",
       "  100000100996,\n",
       "  100000100997,\n",
       "  100000100998,\n",
       "  100000100999,\n",
       "  ...],\n",
       " [100000100000,\n",
       "  100000100001,\n",
       "  100000100002,\n",
       "  100000100003,\n",
       "  100000100004,\n",
       "  100000100005,\n",
       "  100000100006,\n",
       "  100000100007,\n",
       "  100000100008,\n",
       "  100000100009,\n",
       "  100000100010,\n",
       "  100000100011,\n",
       "  100000100012,\n",
       "  100000100013,\n",
       "  100000100014,\n",
       "  100000100015,\n",
       "  100000100016,\n",
       "  100000100017,\n",
       "  100000100018,\n",
       "  100000100019,\n",
       "  100000100020,\n",
       "  100000100021,\n",
       "  100000100022,\n",
       "  100000100023,\n",
       "  100000100024,\n",
       "  100000100025,\n",
       "  100000100026,\n",
       "  100000100027,\n",
       "  100000100028,\n",
       "  100000100029,\n",
       "  100000100030,\n",
       "  100000100031,\n",
       "  100000100032,\n",
       "  100000100033,\n",
       "  100000100034,\n",
       "  100000100035,\n",
       "  100000100036,\n",
       "  100000100037,\n",
       "  100000100038,\n",
       "  100000100039,\n",
       "  100000100040,\n",
       "  100000100041,\n",
       "  100000100042,\n",
       "  100000100043,\n",
       "  100000100044,\n",
       "  100000100045,\n",
       "  100000100046,\n",
       "  100000100047,\n",
       "  100000100048,\n",
       "  100000100049,\n",
       "  100000100050,\n",
       "  100000100051,\n",
       "  100000100052,\n",
       "  100000100053,\n",
       "  100000100054,\n",
       "  100000100055,\n",
       "  100000100056,\n",
       "  100000100057,\n",
       "  100000100058,\n",
       "  100000100059,\n",
       "  100000100060,\n",
       "  100000100061,\n",
       "  100000100062,\n",
       "  100000100063,\n",
       "  100000100064,\n",
       "  100000100065,\n",
       "  100000100066,\n",
       "  100000100067,\n",
       "  100000100068,\n",
       "  100000100069,\n",
       "  100000100070,\n",
       "  100000100071,\n",
       "  100000100072,\n",
       "  100000100073,\n",
       "  100000100074,\n",
       "  100000100075,\n",
       "  100000100076,\n",
       "  100000100077,\n",
       "  100000100078,\n",
       "  100000100079,\n",
       "  100000100080,\n",
       "  100000100081,\n",
       "  100000100082,\n",
       "  100000100083,\n",
       "  100000100084,\n",
       "  100000100085,\n",
       "  100000100086,\n",
       "  100000100087,\n",
       "  100000100088,\n",
       "  100000100089,\n",
       "  100000100090,\n",
       "  100000100091,\n",
       "  100000100092,\n",
       "  100000100093,\n",
       "  100000100094,\n",
       "  100000100095,\n",
       "  100000100096,\n",
       "  100000100097,\n",
       "  100000100098,\n",
       "  100000100099,\n",
       "  100000100100,\n",
       "  100000100101,\n",
       "  100000100102,\n",
       "  100000100103,\n",
       "  100000100104,\n",
       "  100000100105,\n",
       "  100000100106,\n",
       "  100000100107,\n",
       "  100000100108,\n",
       "  100000100109,\n",
       "  100000100110,\n",
       "  100000100111,\n",
       "  100000100112,\n",
       "  100000100113,\n",
       "  100000100114,\n",
       "  100000100115,\n",
       "  100000100116,\n",
       "  100000100117,\n",
       "  100000100118,\n",
       "  100000100119,\n",
       "  100000100120,\n",
       "  100000100121,\n",
       "  100000100122,\n",
       "  100000100123,\n",
       "  100000100124,\n",
       "  100000100125,\n",
       "  100000100126,\n",
       "  100000100127,\n",
       "  100000100128,\n",
       "  100000100129,\n",
       "  100000100130,\n",
       "  100000100131,\n",
       "  100000100132,\n",
       "  100000100133,\n",
       "  100000100134,\n",
       "  100000100135,\n",
       "  100000100136,\n",
       "  100000100137,\n",
       "  100000100138,\n",
       "  100000100139,\n",
       "  100000100140,\n",
       "  100000100141,\n",
       "  100000100142,\n",
       "  100000100143,\n",
       "  100000100144,\n",
       "  100000100145,\n",
       "  100000100146,\n",
       "  100000100147,\n",
       "  100000100148,\n",
       "  100000100149,\n",
       "  100000100150,\n",
       "  100000100151,\n",
       "  100000100152,\n",
       "  100000100153,\n",
       "  100000100154,\n",
       "  100000100155,\n",
       "  100000100156,\n",
       "  100000100157,\n",
       "  100000100158,\n",
       "  100000100159,\n",
       "  100000100160,\n",
       "  100000100161,\n",
       "  100000100162,\n",
       "  100000100163,\n",
       "  100000100164,\n",
       "  100000100165,\n",
       "  100000100166,\n",
       "  100000100167,\n",
       "  100000100168,\n",
       "  100000100169,\n",
       "  100000100170,\n",
       "  100000100171,\n",
       "  100000100172,\n",
       "  100000100173,\n",
       "  100000100174,\n",
       "  100000100175,\n",
       "  100000100176,\n",
       "  100000100177,\n",
       "  100000100178,\n",
       "  100000100179,\n",
       "  100000100180,\n",
       "  100000100181,\n",
       "  100000100182,\n",
       "  100000100183,\n",
       "  100000100184,\n",
       "  100000100185,\n",
       "  100000100186,\n",
       "  100000100187,\n",
       "  100000100188,\n",
       "  100000100189,\n",
       "  100000100190,\n",
       "  100000100191,\n",
       "  100000100192,\n",
       "  100000100193,\n",
       "  100000100194,\n",
       "  100000100195,\n",
       "  100000100196,\n",
       "  100000100197,\n",
       "  100000100198,\n",
       "  100000100199,\n",
       "  100000100200,\n",
       "  100000100201,\n",
       "  100000100202,\n",
       "  100000100203,\n",
       "  100000100204,\n",
       "  100000100205,\n",
       "  100000100206,\n",
       "  100000100207,\n",
       "  100000100208,\n",
       "  100000100209,\n",
       "  100000100210,\n",
       "  100000100211,\n",
       "  100000100212,\n",
       "  100000100213,\n",
       "  100000100214,\n",
       "  100000100215,\n",
       "  100000100216,\n",
       "  100000100217,\n",
       "  100000100218,\n",
       "  100000100219,\n",
       "  100000100220,\n",
       "  100000100221,\n",
       "  100000100222,\n",
       "  100000100223,\n",
       "  100000100224,\n",
       "  100000100225,\n",
       "  100000100226,\n",
       "  100000100227,\n",
       "  100000100228,\n",
       "  100000100229,\n",
       "  100000100230,\n",
       "  100000100231,\n",
       "  100000100232,\n",
       "  100000100233,\n",
       "  100000100234,\n",
       "  100000100235,\n",
       "  100000100236,\n",
       "  100000100237,\n",
       "  100000100238,\n",
       "  100000100239,\n",
       "  100000100240,\n",
       "  100000100241,\n",
       "  100000100242,\n",
       "  100000100243,\n",
       "  100000100244,\n",
       "  100000100245,\n",
       "  100000100246,\n",
       "  100000100247,\n",
       "  100000100248,\n",
       "  100000100249,\n",
       "  100000100250,\n",
       "  100000100251,\n",
       "  100000100252,\n",
       "  100000100253,\n",
       "  100000100254,\n",
       "  100000100255,\n",
       "  100000100256,\n",
       "  100000100257,\n",
       "  100000100258,\n",
       "  100000100259,\n",
       "  100000100260,\n",
       "  100000100261,\n",
       "  100000100262,\n",
       "  100000100263,\n",
       "  100000100264,\n",
       "  100000100265,\n",
       "  100000100266,\n",
       "  100000100267,\n",
       "  100000100268,\n",
       "  100000100269,\n",
       "  100000100270,\n",
       "  100000100271,\n",
       "  100000100272,\n",
       "  100000100273,\n",
       "  100000100274,\n",
       "  100000100275,\n",
       "  100000100276,\n",
       "  100000100277,\n",
       "  100000100278,\n",
       "  100000100279,\n",
       "  100000100280,\n",
       "  100000100281,\n",
       "  100000100282,\n",
       "  100000100283,\n",
       "  100000100284,\n",
       "  100000100285,\n",
       "  100000100286,\n",
       "  100000100287,\n",
       "  100000100288,\n",
       "  100000100289,\n",
       "  100000100290,\n",
       "  100000100291,\n",
       "  100000100292,\n",
       "  100000100293,\n",
       "  100000100294,\n",
       "  100000100295,\n",
       "  100000100296,\n",
       "  100000100297,\n",
       "  100000100298,\n",
       "  100000100299,\n",
       "  100000100300,\n",
       "  100000100301,\n",
       "  100000100302,\n",
       "  100000100303,\n",
       "  100000100304,\n",
       "  100000100305,\n",
       "  100000100306,\n",
       "  100000100307,\n",
       "  100000100308,\n",
       "  100000100309,\n",
       "  100000100310,\n",
       "  100000100311,\n",
       "  100000100312,\n",
       "  100000100313,\n",
       "  100000100314,\n",
       "  100000100315,\n",
       "  100000100316,\n",
       "  100000100317,\n",
       "  100000100318,\n",
       "  100000100319,\n",
       "  100000100320,\n",
       "  100000100321,\n",
       "  100000100322,\n",
       "  100000100323,\n",
       "  100000100324,\n",
       "  100000100325,\n",
       "  100000100326,\n",
       "  100000100327,\n",
       "  100000100328,\n",
       "  100000100329,\n",
       "  100000100330,\n",
       "  100000100331,\n",
       "  100000100332,\n",
       "  100000100333,\n",
       "  100000100334,\n",
       "  100000100335,\n",
       "  100000100336,\n",
       "  100000100337,\n",
       "  100000100338,\n",
       "  100000100339,\n",
       "  100000100340,\n",
       "  100000100341,\n",
       "  100000100342,\n",
       "  100000100343,\n",
       "  100000100344,\n",
       "  100000100345,\n",
       "  100000100346,\n",
       "  100000100347,\n",
       "  100000100348,\n",
       "  100000100349,\n",
       "  100000100350,\n",
       "  100000100351,\n",
       "  100000100352,\n",
       "  100000100353,\n",
       "  100000100354,\n",
       "  100000100355,\n",
       "  100000100356,\n",
       "  100000100357,\n",
       "  100000100358,\n",
       "  100000100359,\n",
       "  100000100360,\n",
       "  100000100361,\n",
       "  100000100362,\n",
       "  100000100363,\n",
       "  100000100364,\n",
       "  100000100365,\n",
       "  100000100366,\n",
       "  100000100367,\n",
       "  100000100368,\n",
       "  100000100369,\n",
       "  100000100370,\n",
       "  100000100371,\n",
       "  100000100372,\n",
       "  100000100373,\n",
       "  100000100374,\n",
       "  100000100375,\n",
       "  100000100376,\n",
       "  100000100377,\n",
       "  100000100378,\n",
       "  100000100379,\n",
       "  100000100380,\n",
       "  100000100381,\n",
       "  100000100382,\n",
       "  100000100383,\n",
       "  100000100384,\n",
       "  100000100385,\n",
       "  100000100386,\n",
       "  100000100387,\n",
       "  100000100388,\n",
       "  100000100389,\n",
       "  100000100390,\n",
       "  100000100391,\n",
       "  100000100392,\n",
       "  100000100393,\n",
       "  100000100394,\n",
       "  100000100395,\n",
       "  100000100396,\n",
       "  100000100397,\n",
       "  100000100398,\n",
       "  100000100399,\n",
       "  100000100400,\n",
       "  100000100401,\n",
       "  100000100402,\n",
       "  100000100403,\n",
       "  100000100404,\n",
       "  100000100405,\n",
       "  100000100406,\n",
       "  100000100407,\n",
       "  100000100408,\n",
       "  100000100409,\n",
       "  100000100410,\n",
       "  100000100411,\n",
       "  100000100412,\n",
       "  100000100413,\n",
       "  100000100414,\n",
       "  100000100415,\n",
       "  100000100416,\n",
       "  100000100417,\n",
       "  100000100418,\n",
       "  100000100419,\n",
       "  100000100420,\n",
       "  100000100421,\n",
       "  100000100422,\n",
       "  100000100423,\n",
       "  100000100424,\n",
       "  100000100425,\n",
       "  100000100426,\n",
       "  100000100427,\n",
       "  100000100428,\n",
       "  100000100429,\n",
       "  100000100430,\n",
       "  100000100431,\n",
       "  100000100432,\n",
       "  100000100433,\n",
       "  100000100434,\n",
       "  100000100435,\n",
       "  100000100436,\n",
       "  100000100437,\n",
       "  100000100438,\n",
       "  100000100439,\n",
       "  100000100440,\n",
       "  100000100441,\n",
       "  100000100442,\n",
       "  100000100443,\n",
       "  100000100444,\n",
       "  100000100445,\n",
       "  100000100446,\n",
       "  100000100447,\n",
       "  100000100448,\n",
       "  100000100449,\n",
       "  100000100450,\n",
       "  100000100451,\n",
       "  100000100452,\n",
       "  100000100453,\n",
       "  100000100454,\n",
       "  100000100455,\n",
       "  100000100456,\n",
       "  100000100457,\n",
       "  100000100458,\n",
       "  100000100459,\n",
       "  100000100460,\n",
       "  100000100461,\n",
       "  100000100462,\n",
       "  100000100463,\n",
       "  100000100464,\n",
       "  100000100465,\n",
       "  100000100466,\n",
       "  100000100467,\n",
       "  100000100468,\n",
       "  100000100469,\n",
       "  100000100470,\n",
       "  100000100471,\n",
       "  100000100472,\n",
       "  100000100473,\n",
       "  100000100474,\n",
       "  100000100475,\n",
       "  100000100476,\n",
       "  100000100477,\n",
       "  100000100478,\n",
       "  100000100479,\n",
       "  100000100480,\n",
       "  100000100481,\n",
       "  100000100482,\n",
       "  100000100483,\n",
       "  100000100484,\n",
       "  100000100485,\n",
       "  100000100486,\n",
       "  100000100487,\n",
       "  100000100488,\n",
       "  100000100489,\n",
       "  100000100490,\n",
       "  100000100491,\n",
       "  100000100492,\n",
       "  100000100493,\n",
       "  100000100494,\n",
       "  100000100495,\n",
       "  100000100496,\n",
       "  100000100497,\n",
       "  100000100498,\n",
       "  100000100499,\n",
       "  100000100500,\n",
       "  100000100501,\n",
       "  100000100502,\n",
       "  100000100503,\n",
       "  100000100504,\n",
       "  100000100505,\n",
       "  100000100506,\n",
       "  100000100507,\n",
       "  100000100508,\n",
       "  100000100509,\n",
       "  100000100510,\n",
       "  100000100511,\n",
       "  100000100512,\n",
       "  100000100513,\n",
       "  100000100514,\n",
       "  100000100515,\n",
       "  100000100516,\n",
       "  100000100517,\n",
       "  100000100518,\n",
       "  100000100519,\n",
       "  100000100520,\n",
       "  100000100521,\n",
       "  100000100522,\n",
       "  100000100523,\n",
       "  100000100524,\n",
       "  100000100525,\n",
       "  100000100526,\n",
       "  100000100527,\n",
       "  100000100528,\n",
       "  100000100529,\n",
       "  100000100530,\n",
       "  100000100531,\n",
       "  100000100532,\n",
       "  100000100533,\n",
       "  100000100534,\n",
       "  100000100535,\n",
       "  100000100536,\n",
       "  100000100537,\n",
       "  100000100538,\n",
       "  100000100539,\n",
       "  100000100540,\n",
       "  100000100541,\n",
       "  100000100542,\n",
       "  100000100543,\n",
       "  100000100544,\n",
       "  100000100545,\n",
       "  100000100546,\n",
       "  100000100547,\n",
       "  100000100548,\n",
       "  100000100549,\n",
       "  100000100550,\n",
       "  100000100551,\n",
       "  100000100552,\n",
       "  100000100553,\n",
       "  100000100554,\n",
       "  100000100555,\n",
       "  100000100556,\n",
       "  100000100557,\n",
       "  100000100558,\n",
       "  100000100559,\n",
       "  100000100560,\n",
       "  100000100561,\n",
       "  100000100562,\n",
       "  100000100563,\n",
       "  100000100564,\n",
       "  100000100565,\n",
       "  100000100566,\n",
       "  100000100567,\n",
       "  100000100568,\n",
       "  100000100569,\n",
       "  100000100570,\n",
       "  100000100571,\n",
       "  100000100572,\n",
       "  100000100573,\n",
       "  100000100574,\n",
       "  100000100575,\n",
       "  100000100576,\n",
       "  100000100577,\n",
       "  100000100578,\n",
       "  100000100579,\n",
       "  100000100580,\n",
       "  100000100581,\n",
       "  100000100582,\n",
       "  100000100583,\n",
       "  100000100584,\n",
       "  100000100585,\n",
       "  100000100586,\n",
       "  100000100587,\n",
       "  100000100588,\n",
       "  100000100589,\n",
       "  100000100590,\n",
       "  100000100591,\n",
       "  100000100592,\n",
       "  100000100593,\n",
       "  100000100594,\n",
       "  100000100595,\n",
       "  100000100596,\n",
       "  100000100597,\n",
       "  100000100598,\n",
       "  100000100599,\n",
       "  100000100600,\n",
       "  100000100601,\n",
       "  100000100602,\n",
       "  100000100603,\n",
       "  100000100604,\n",
       "  100000100605,\n",
       "  100000100606,\n",
       "  100000100607,\n",
       "  100000100608,\n",
       "  100000100609,\n",
       "  100000100610,\n",
       "  100000100611,\n",
       "  100000100612,\n",
       "  100000100613,\n",
       "  100000100614,\n",
       "  100000100615,\n",
       "  100000100616,\n",
       "  100000100617,\n",
       "  100000100618,\n",
       "  100000100619,\n",
       "  100000100620,\n",
       "  100000100621,\n",
       "  100000100622,\n",
       "  100000100623,\n",
       "  100000100624,\n",
       "  100000100625,\n",
       "  100000100626,\n",
       "  100000100627,\n",
       "  100000100628,\n",
       "  100000100629,\n",
       "  100000100630,\n",
       "  100000100631,\n",
       "  100000100632,\n",
       "  100000100633,\n",
       "  100000100634,\n",
       "  100000100635,\n",
       "  100000100636,\n",
       "  100000100637,\n",
       "  100000100638,\n",
       "  100000100639,\n",
       "  100000100640,\n",
       "  100000100641,\n",
       "  100000100642,\n",
       "  100000100643,\n",
       "  100000100644,\n",
       "  100000100645,\n",
       "  100000100646,\n",
       "  100000100647,\n",
       "  100000100648,\n",
       "  100000100649,\n",
       "  100000100650,\n",
       "  100000100651,\n",
       "  100000100652,\n",
       "  100000100653,\n",
       "  100000100654,\n",
       "  100000100655,\n",
       "  100000100656,\n",
       "  100000100657,\n",
       "  100000100658,\n",
       "  100000100659,\n",
       "  100000100660,\n",
       "  100000100661,\n",
       "  100000100662,\n",
       "  100000100663,\n",
       "  100000100664,\n",
       "  100000100665,\n",
       "  100000100666,\n",
       "  100000100667,\n",
       "  100000100668,\n",
       "  100000100669,\n",
       "  100000100670,\n",
       "  100000100671,\n",
       "  100000100672,\n",
       "  100000100673,\n",
       "  100000100674,\n",
       "  100000100675,\n",
       "  100000100676,\n",
       "  100000100677,\n",
       "  100000100678,\n",
       "  100000100679,\n",
       "  100000100680,\n",
       "  100000100681,\n",
       "  100000100682,\n",
       "  100000100683,\n",
       "  100000100684,\n",
       "  100000100685,\n",
       "  100000100686,\n",
       "  100000100687,\n",
       "  100000100688,\n",
       "  100000100689,\n",
       "  100000100690,\n",
       "  100000100691,\n",
       "  100000100692,\n",
       "  100000100693,\n",
       "  100000100694,\n",
       "  100000100695,\n",
       "  100000100696,\n",
       "  100000100697,\n",
       "  100000100698,\n",
       "  100000100699,\n",
       "  100000100700,\n",
       "  100000100701,\n",
       "  100000100702,\n",
       "  100000100703,\n",
       "  100000100704,\n",
       "  100000100705,\n",
       "  100000100706,\n",
       "  100000100707,\n",
       "  100000100708,\n",
       "  100000100709,\n",
       "  100000100710,\n",
       "  100000100711,\n",
       "  100000100712,\n",
       "  100000100713,\n",
       "  100000100714,\n",
       "  100000100715,\n",
       "  100000100716,\n",
       "  100000100717,\n",
       "  100000100718,\n",
       "  100000100719,\n",
       "  100000100720,\n",
       "  100000100721,\n",
       "  100000100722,\n",
       "  100000100723,\n",
       "  100000100724,\n",
       "  100000100725,\n",
       "  100000100726,\n",
       "  100000100727,\n",
       "  100000100728,\n",
       "  100000100729,\n",
       "  100000100730,\n",
       "  100000100731,\n",
       "  100000100732,\n",
       "  100000100733,\n",
       "  100000100734,\n",
       "  100000100735,\n",
       "  100000100736,\n",
       "  100000100737,\n",
       "  100000100738,\n",
       "  100000100739,\n",
       "  100000100740,\n",
       "  100000100741,\n",
       "  100000100742,\n",
       "  100000100743,\n",
       "  100000100744,\n",
       "  100000100745,\n",
       "  100000100746,\n",
       "  100000100747,\n",
       "  100000100748,\n",
       "  100000100749,\n",
       "  100000100750,\n",
       "  100000100751,\n",
       "  100000100752,\n",
       "  100000100753,\n",
       "  100000100754,\n",
       "  100000100755,\n",
       "  100000100756,\n",
       "  100000100757,\n",
       "  100000100758,\n",
       "  100000100759,\n",
       "  100000100760,\n",
       "  100000100761,\n",
       "  100000100762,\n",
       "  100000100763,\n",
       "  100000100764,\n",
       "  100000100765,\n",
       "  100000100766,\n",
       "  100000100767,\n",
       "  100000100768,\n",
       "  100000100769,\n",
       "  100000100770,\n",
       "  100000100771,\n",
       "  100000100772,\n",
       "  100000100773,\n",
       "  100000100774,\n",
       "  100000100775,\n",
       "  100000100776,\n",
       "  100000100777,\n",
       "  100000100778,\n",
       "  100000100779,\n",
       "  100000100780,\n",
       "  100000100781,\n",
       "  100000100782,\n",
       "  100000100783,\n",
       "  100000100784,\n",
       "  100000100785,\n",
       "  100000100786,\n",
       "  100000100787,\n",
       "  100000100788,\n",
       "  100000100789,\n",
       "  100000100790,\n",
       "  100000100791,\n",
       "  100000100792,\n",
       "  100000100793,\n",
       "  100000100794,\n",
       "  100000100795,\n",
       "  100000100796,\n",
       "  100000100797,\n",
       "  100000100798,\n",
       "  100000100799,\n",
       "  100000100800,\n",
       "  100000100801,\n",
       "  100000100802,\n",
       "  100000100803,\n",
       "  100000100804,\n",
       "  100000100805,\n",
       "  100000100806,\n",
       "  100000100807,\n",
       "  100000100808,\n",
       "  100000100809,\n",
       "  100000100810,\n",
       "  100000100811,\n",
       "  100000100812,\n",
       "  100000100813,\n",
       "  100000100814,\n",
       "  100000100815,\n",
       "  100000100816,\n",
       "  100000100817,\n",
       "  100000100818,\n",
       "  100000100819,\n",
       "  100000100820,\n",
       "  100000100821,\n",
       "  100000100822,\n",
       "  100000100823,\n",
       "  100000100824,\n",
       "  100000100825,\n",
       "  100000100826,\n",
       "  100000100827,\n",
       "  100000100828,\n",
       "  100000100829,\n",
       "  100000100830,\n",
       "  100000100831,\n",
       "  100000100832,\n",
       "  100000100833,\n",
       "  100000100834,\n",
       "  100000100835,\n",
       "  100000100836,\n",
       "  100000100837,\n",
       "  100000100838,\n",
       "  100000100839,\n",
       "  100000100840,\n",
       "  100000100841,\n",
       "  100000100842,\n",
       "  100000100843,\n",
       "  100000100844,\n",
       "  100000100845,\n",
       "  100000100846,\n",
       "  100000100847,\n",
       "  100000100848,\n",
       "  100000100849,\n",
       "  100000100850,\n",
       "  100000100851,\n",
       "  100000100852,\n",
       "  100000100853,\n",
       "  100000100854,\n",
       "  100000100855,\n",
       "  100000100856,\n",
       "  100000100857,\n",
       "  100000100858,\n",
       "  100000100859,\n",
       "  100000100860,\n",
       "  100000100861,\n",
       "  100000100862,\n",
       "  100000100863,\n",
       "  100000100864,\n",
       "  100000100865,\n",
       "  100000100866,\n",
       "  100000100867,\n",
       "  100000100868,\n",
       "  100000100869,\n",
       "  100000100870,\n",
       "  100000100871,\n",
       "  100000100872,\n",
       "  100000100873,\n",
       "  100000100874,\n",
       "  100000100875,\n",
       "  100000100876,\n",
       "  100000100877,\n",
       "  100000100878,\n",
       "  100000100879,\n",
       "  100000100880,\n",
       "  100000100881,\n",
       "  100000100882,\n",
       "  100000100883,\n",
       "  100000100884,\n",
       "  100000100885,\n",
       "  100000100886,\n",
       "  100000100887,\n",
       "  100000100888,\n",
       "  100000100889,\n",
       "  100000100890,\n",
       "  100000100891,\n",
       "  100000100892,\n",
       "  100000100893,\n",
       "  100000100894,\n",
       "  100000100895,\n",
       "  100000100896,\n",
       "  100000100897,\n",
       "  100000100898,\n",
       "  100000100899,\n",
       "  100000100900,\n",
       "  100000100901,\n",
       "  100000100902,\n",
       "  100000100903,\n",
       "  100000100904,\n",
       "  100000100905,\n",
       "  100000100906,\n",
       "  100000100907,\n",
       "  100000100908,\n",
       "  100000100909,\n",
       "  100000100910,\n",
       "  100000100911,\n",
       "  100000100912,\n",
       "  100000100913,\n",
       "  100000100914,\n",
       "  100000100915,\n",
       "  100000100916,\n",
       "  100000100917,\n",
       "  100000100918,\n",
       "  100000100919,\n",
       "  100000100920,\n",
       "  100000100921,\n",
       "  100000100922,\n",
       "  100000100923,\n",
       "  100000100924,\n",
       "  100000100925,\n",
       "  100000100926,\n",
       "  100000100927,\n",
       "  100000100928,\n",
       "  100000100929,\n",
       "  100000100930,\n",
       "  100000100931,\n",
       "  100000100932,\n",
       "  100000100933,\n",
       "  100000100934,\n",
       "  100000100935,\n",
       "  100000100936,\n",
       "  100000100937,\n",
       "  100000100938,\n",
       "  100000100939,\n",
       "  100000100940,\n",
       "  100000100941,\n",
       "  100000100942,\n",
       "  100000100943,\n",
       "  100000100944,\n",
       "  100000100945,\n",
       "  100000100946,\n",
       "  100000100947,\n",
       "  100000100948,\n",
       "  100000100949,\n",
       "  100000100950,\n",
       "  100000100951,\n",
       "  100000100952,\n",
       "  100000100953,\n",
       "  100000100954,\n",
       "  100000100955,\n",
       "  100000100956,\n",
       "  100000100957,\n",
       "  100000100958,\n",
       "  100000100959,\n",
       "  100000100960,\n",
       "  100000100961,\n",
       "  100000100962,\n",
       "  100000100963,\n",
       "  100000100964,\n",
       "  100000100965,\n",
       "  100000100966,\n",
       "  100000100967,\n",
       "  100000100968,\n",
       "  100000100969,\n",
       "  100000100970,\n",
       "  100000100971,\n",
       "  100000100972,\n",
       "  100000100973,\n",
       "  100000100974,\n",
       "  100000100975,\n",
       "  100000100976,\n",
       "  100000100977,\n",
       "  100000100978,\n",
       "  100000100979,\n",
       "  100000100980,\n",
       "  100000100981,\n",
       "  100000100982,\n",
       "  100000100983,\n",
       "  100000100984,\n",
       "  100000100985,\n",
       "  100000100986,\n",
       "  100000100987,\n",
       "  100000100988,\n",
       "  100000100989,\n",
       "  100000100990,\n",
       "  100000100991,\n",
       "  100000100992,\n",
       "  100000100993,\n",
       "  100000100994,\n",
       "  100000100995,\n",
       "  100000100996,\n",
       "  100000100997,\n",
       "  100000100998,\n",
       "  100000100999,\n",
       "  ...],\n",
       " [100000100000,\n",
       "  100000100001,\n",
       "  100000100002,\n",
       "  100000100003,\n",
       "  100000100004,\n",
       "  100000100005,\n",
       "  100000100006,\n",
       "  100000100007,\n",
       "  100000100008,\n",
       "  100000100009,\n",
       "  100000100010,\n",
       "  100000100011,\n",
       "  100000100012,\n",
       "  100000100013,\n",
       "  100000100014,\n",
       "  100000100015,\n",
       "  100000100016,\n",
       "  100000100017,\n",
       "  100000100018,\n",
       "  100000100019,\n",
       "  100000100020,\n",
       "  100000100021,\n",
       "  100000100022,\n",
       "  100000100023,\n",
       "  100000100024,\n",
       "  100000100025,\n",
       "  100000100026,\n",
       "  100000100027,\n",
       "  100000100028,\n",
       "  100000100029,\n",
       "  100000100030,\n",
       "  100000100031,\n",
       "  100000100032,\n",
       "  100000100033,\n",
       "  100000100034,\n",
       "  100000100035,\n",
       "  100000100036,\n",
       "  100000100037,\n",
       "  100000100038,\n",
       "  100000100039,\n",
       "  100000100040,\n",
       "  100000100041,\n",
       "  100000100042,\n",
       "  100000100043,\n",
       "  100000100044,\n",
       "  100000100045,\n",
       "  100000100046,\n",
       "  100000100047,\n",
       "  100000100048,\n",
       "  100000100049,\n",
       "  100000100050,\n",
       "  100000100051,\n",
       "  100000100052,\n",
       "  100000100053,\n",
       "  100000100054,\n",
       "  100000100055,\n",
       "  100000100056,\n",
       "  100000100057,\n",
       "  100000100058,\n",
       "  100000100059,\n",
       "  100000100060,\n",
       "  100000100061,\n",
       "  100000100062,\n",
       "  100000100063,\n",
       "  100000100064,\n",
       "  100000100065,\n",
       "  100000100066,\n",
       "  100000100067,\n",
       "  100000100068,\n",
       "  100000100069,\n",
       "  100000100070,\n",
       "  100000100071,\n",
       "  100000100072,\n",
       "  100000100073,\n",
       "  100000100074,\n",
       "  100000100075,\n",
       "  100000100076,\n",
       "  100000100077,\n",
       "  100000100078,\n",
       "  100000100079,\n",
       "  100000100080,\n",
       "  100000100081,\n",
       "  100000100082,\n",
       "  100000100083,\n",
       "  100000100084,\n",
       "  100000100085,\n",
       "  100000100086,\n",
       "  100000100087,\n",
       "  100000100088,\n",
       "  100000100089,\n",
       "  100000100090,\n",
       "  100000100091,\n",
       "  100000100092,\n",
       "  100000100093,\n",
       "  100000100094,\n",
       "  100000100095,\n",
       "  100000100096,\n",
       "  100000100097,\n",
       "  100000100098,\n",
       "  100000100099,\n",
       "  100000100100,\n",
       "  100000100101,\n",
       "  100000100102,\n",
       "  100000100103,\n",
       "  100000100104,\n",
       "  100000100105,\n",
       "  100000100106,\n",
       "  100000100107,\n",
       "  100000100108,\n",
       "  100000100109,\n",
       "  100000100110,\n",
       "  100000100111,\n",
       "  100000100112,\n",
       "  100000100113,\n",
       "  100000100114,\n",
       "  100000100115,\n",
       "  100000100116,\n",
       "  100000100117,\n",
       "  100000100118,\n",
       "  100000100119,\n",
       "  100000100120,\n",
       "  100000100121,\n",
       "  100000100122,\n",
       "  100000100123,\n",
       "  100000100124,\n",
       "  100000100125,\n",
       "  100000100126,\n",
       "  100000100127,\n",
       "  100000100128,\n",
       "  100000100129,\n",
       "  100000100130,\n",
       "  100000100131,\n",
       "  100000100132,\n",
       "  100000100133,\n",
       "  100000100134,\n",
       "  100000100135,\n",
       "  100000100136,\n",
       "  100000100137,\n",
       "  100000100138,\n",
       "  100000100139,\n",
       "  100000100140,\n",
       "  100000100141,\n",
       "  100000100142,\n",
       "  100000100143,\n",
       "  100000100144,\n",
       "  100000100145,\n",
       "  100000100146,\n",
       "  100000100147,\n",
       "  100000100148,\n",
       "  100000100149,\n",
       "  100000100150,\n",
       "  100000100151,\n",
       "  100000100152,\n",
       "  100000100153,\n",
       "  100000100154,\n",
       "  100000100155,\n",
       "  100000100156,\n",
       "  100000100157,\n",
       "  100000100158,\n",
       "  100000100159,\n",
       "  100000100160,\n",
       "  100000100161,\n",
       "  100000100162,\n",
       "  100000100163,\n",
       "  100000100164,\n",
       "  100000100165,\n",
       "  100000100166,\n",
       "  100000100167,\n",
       "  100000100168,\n",
       "  100000100169,\n",
       "  100000100170,\n",
       "  100000100171,\n",
       "  100000100172,\n",
       "  100000100173,\n",
       "  100000100174,\n",
       "  100000100175,\n",
       "  100000100176,\n",
       "  100000100177,\n",
       "  100000100178,\n",
       "  100000100179,\n",
       "  100000100180,\n",
       "  100000100181,\n",
       "  100000100182,\n",
       "  100000100183,\n",
       "  100000100184,\n",
       "  100000100185,\n",
       "  100000100186,\n",
       "  100000100187,\n",
       "  100000100188,\n",
       "  100000100189,\n",
       "  100000100190,\n",
       "  100000100191,\n",
       "  100000100192,\n",
       "  100000100193,\n",
       "  100000100194,\n",
       "  100000100195,\n",
       "  100000100196,\n",
       "  100000100197,\n",
       "  100000100198,\n",
       "  100000100199,\n",
       "  100000100200,\n",
       "  100000100201,\n",
       "  100000100202,\n",
       "  100000100203,\n",
       "  100000100204,\n",
       "  100000100205,\n",
       "  100000100206,\n",
       "  100000100207,\n",
       "  100000100208,\n",
       "  100000100209,\n",
       "  100000100210,\n",
       "  100000100211,\n",
       "  100000100212,\n",
       "  100000100213,\n",
       "  100000100214,\n",
       "  100000100215,\n",
       "  100000100216,\n",
       "  100000100217,\n",
       "  100000100218,\n",
       "  100000100219,\n",
       "  100000100220,\n",
       "  100000100221,\n",
       "  100000100222,\n",
       "  100000100223,\n",
       "  100000100224,\n",
       "  100000100225,\n",
       "  100000100226,\n",
       "  100000100227,\n",
       "  100000100228,\n",
       "  100000100229,\n",
       "  100000100230,\n",
       "  100000100231,\n",
       "  100000100232,\n",
       "  100000100233,\n",
       "  100000100234,\n",
       "  100000100235,\n",
       "  100000100236,\n",
       "  100000100237,\n",
       "  100000100238,\n",
       "  100000100239,\n",
       "  100000100240,\n",
       "  100000100241,\n",
       "  100000100242,\n",
       "  100000100243,\n",
       "  100000100244,\n",
       "  100000100245,\n",
       "  100000100246,\n",
       "  100000100247,\n",
       "  100000100248,\n",
       "  100000100249,\n",
       "  100000100250,\n",
       "  100000100251,\n",
       "  100000100252,\n",
       "  100000100253,\n",
       "  100000100254,\n",
       "  100000100255,\n",
       "  100000100256,\n",
       "  100000100257,\n",
       "  100000100258,\n",
       "  100000100259,\n",
       "  100000100260,\n",
       "  100000100261,\n",
       "  100000100262,\n",
       "  100000100263,\n",
       "  100000100264,\n",
       "  100000100265,\n",
       "  100000100266,\n",
       "  100000100267,\n",
       "  100000100268,\n",
       "  100000100269,\n",
       "  100000100270,\n",
       "  100000100271,\n",
       "  100000100272,\n",
       "  100000100273,\n",
       "  100000100274,\n",
       "  100000100275,\n",
       "  100000100276,\n",
       "  100000100277,\n",
       "  100000100278,\n",
       "  100000100279,\n",
       "  100000100280,\n",
       "  100000100281,\n",
       "  100000100282,\n",
       "  100000100283,\n",
       "  100000100284,\n",
       "  100000100285,\n",
       "  100000100286,\n",
       "  100000100287,\n",
       "  100000100288,\n",
       "  100000100289,\n",
       "  100000100290,\n",
       "  100000100291,\n",
       "  100000100292,\n",
       "  100000100293,\n",
       "  100000100294,\n",
       "  100000100295,\n",
       "  100000100296,\n",
       "  100000100297,\n",
       "  100000100298,\n",
       "  100000100299,\n",
       "  100000100300,\n",
       "  100000100301,\n",
       "  100000100302,\n",
       "  100000100303,\n",
       "  100000100304,\n",
       "  100000100305,\n",
       "  100000100306,\n",
       "  100000100307,\n",
       "  100000100308,\n",
       "  100000100309,\n",
       "  100000100310,\n",
       "  100000100311,\n",
       "  100000100312,\n",
       "  100000100313,\n",
       "  100000100314,\n",
       "  100000100315,\n",
       "  100000100316,\n",
       "  100000100317,\n",
       "  100000100318,\n",
       "  100000100319,\n",
       "  100000100320,\n",
       "  100000100321,\n",
       "  100000100322,\n",
       "  100000100323,\n",
       "  100000100324,\n",
       "  100000100325,\n",
       "  100000100326,\n",
       "  100000100327,\n",
       "  100000100328,\n",
       "  100000100329,\n",
       "  100000100330,\n",
       "  100000100331,\n",
       "  100000100332,\n",
       "  100000100333,\n",
       "  100000100334,\n",
       "  100000100335,\n",
       "  100000100336,\n",
       "  100000100337,\n",
       "  100000100338,\n",
       "  100000100339,\n",
       "  100000100340,\n",
       "  100000100341,\n",
       "  100000100342,\n",
       "  100000100343,\n",
       "  100000100344,\n",
       "  100000100345,\n",
       "  100000100346,\n",
       "  100000100347,\n",
       "  100000100348,\n",
       "  100000100349,\n",
       "  100000100350,\n",
       "  100000100351,\n",
       "  100000100352,\n",
       "  100000100353,\n",
       "  100000100354,\n",
       "  100000100355,\n",
       "  100000100356,\n",
       "  100000100357,\n",
       "  100000100358,\n",
       "  100000100359,\n",
       "  100000100360,\n",
       "  100000100361,\n",
       "  100000100362,\n",
       "  100000100363,\n",
       "  100000100364,\n",
       "  100000100365,\n",
       "  100000100366,\n",
       "  100000100367,\n",
       "  100000100368,\n",
       "  100000100369,\n",
       "  100000100370,\n",
       "  100000100371,\n",
       "  100000100372,\n",
       "  100000100373,\n",
       "  100000100374,\n",
       "  100000100375,\n",
       "  100000100376,\n",
       "  100000100377,\n",
       "  100000100378,\n",
       "  100000100379,\n",
       "  100000100380,\n",
       "  100000100381,\n",
       "  100000100382,\n",
       "  100000100383,\n",
       "  100000100384,\n",
       "  100000100385,\n",
       "  100000100386,\n",
       "  100000100387,\n",
       "  100000100388,\n",
       "  100000100389,\n",
       "  100000100390,\n",
       "  100000100391,\n",
       "  100000100392,\n",
       "  100000100393,\n",
       "  100000100394,\n",
       "  100000100395,\n",
       "  100000100396,\n",
       "  100000100397,\n",
       "  100000100398,\n",
       "  100000100399,\n",
       "  100000100400,\n",
       "  100000100401,\n",
       "  100000100402,\n",
       "  100000100403,\n",
       "  100000100404,\n",
       "  100000100405,\n",
       "  100000100406,\n",
       "  100000100407,\n",
       "  100000100408,\n",
       "  100000100409,\n",
       "  100000100410,\n",
       "  100000100411,\n",
       "  100000100412,\n",
       "  100000100413,\n",
       "  100000100414,\n",
       "  100000100415,\n",
       "  100000100416,\n",
       "  100000100417,\n",
       "  100000100418,\n",
       "  100000100419,\n",
       "  100000100420,\n",
       "  100000100421,\n",
       "  100000100422,\n",
       "  100000100423,\n",
       "  100000100424,\n",
       "  100000100425,\n",
       "  100000100426,\n",
       "  100000100427,\n",
       "  100000100428,\n",
       "  100000100429,\n",
       "  100000100430,\n",
       "  100000100431,\n",
       "  100000100432,\n",
       "  100000100433,\n",
       "  100000100434,\n",
       "  100000100435,\n",
       "  100000100436,\n",
       "  100000100437,\n",
       "  100000100438,\n",
       "  100000100439,\n",
       "  100000100440,\n",
       "  100000100441,\n",
       "  100000100442,\n",
       "  100000100443,\n",
       "  100000100444,\n",
       "  100000100445,\n",
       "  100000100446,\n",
       "  100000100447,\n",
       "  100000100448,\n",
       "  100000100449,\n",
       "  100000100450,\n",
       "  100000100451,\n",
       "  100000100452,\n",
       "  100000100453,\n",
       "  100000100454,\n",
       "  100000100455,\n",
       "  100000100456,\n",
       "  100000100457,\n",
       "  100000100458,\n",
       "  100000100459,\n",
       "  100000100460,\n",
       "  100000100461,\n",
       "  100000100462,\n",
       "  100000100463,\n",
       "  100000100464,\n",
       "  100000100465,\n",
       "  100000100466,\n",
       "  100000100467,\n",
       "  100000100468,\n",
       "  100000100469,\n",
       "  100000100470,\n",
       "  100000100471,\n",
       "  100000100472,\n",
       "  100000100473,\n",
       "  100000100474,\n",
       "  100000100475,\n",
       "  100000100476,\n",
       "  100000100477,\n",
       "  100000100478,\n",
       "  100000100479,\n",
       "  100000100480,\n",
       "  100000100481,\n",
       "  100000100482,\n",
       "  100000100483,\n",
       "  100000100484,\n",
       "  100000100485,\n",
       "  100000100486,\n",
       "  100000100487,\n",
       "  100000100488,\n",
       "  100000100489,\n",
       "  100000100490,\n",
       "  100000100491,\n",
       "  100000100492,\n",
       "  100000100493,\n",
       "  100000100494,\n",
       "  100000100495,\n",
       "  100000100496,\n",
       "  100000100497,\n",
       "  100000100498,\n",
       "  100000100499,\n",
       "  100000100500,\n",
       "  100000100501,\n",
       "  100000100502,\n",
       "  100000100503,\n",
       "  100000100504,\n",
       "  100000100505,\n",
       "  100000100506,\n",
       "  100000100507,\n",
       "  100000100508,\n",
       "  100000100509,\n",
       "  100000100510,\n",
       "  100000100511,\n",
       "  100000100512,\n",
       "  100000100513,\n",
       "  100000100514,\n",
       "  100000100515,\n",
       "  100000100516,\n",
       "  100000100517,\n",
       "  100000100518,\n",
       "  100000100519,\n",
       "  100000100520,\n",
       "  100000100521,\n",
       "  100000100522,\n",
       "  100000100523,\n",
       "  100000100524,\n",
       "  100000100525,\n",
       "  100000100526,\n",
       "  100000100527,\n",
       "  100000100528,\n",
       "  100000100529,\n",
       "  100000100530,\n",
       "  100000100531,\n",
       "  100000100532,\n",
       "  100000100533,\n",
       "  100000100534,\n",
       "  100000100535,\n",
       "  100000100536,\n",
       "  100000100537,\n",
       "  100000100538,\n",
       "  100000100539,\n",
       "  100000100540,\n",
       "  100000100541,\n",
       "  100000100542,\n",
       "  100000100543,\n",
       "  100000100544,\n",
       "  100000100545,\n",
       "  100000100546,\n",
       "  100000100547,\n",
       "  100000100548,\n",
       "  100000100549,\n",
       "  100000100550,\n",
       "  100000100551,\n",
       "  100000100552,\n",
       "  100000100553,\n",
       "  100000100554,\n",
       "  100000100555,\n",
       "  100000100556,\n",
       "  100000100557,\n",
       "  100000100558,\n",
       "  100000100559,\n",
       "  100000100560,\n",
       "  100000100561,\n",
       "  100000100562,\n",
       "  100000100563,\n",
       "  100000100564,\n",
       "  100000100565,\n",
       "  100000100566,\n",
       "  100000100567,\n",
       "  100000100568,\n",
       "  100000100569,\n",
       "  100000100570,\n",
       "  100000100571,\n",
       "  100000100572,\n",
       "  100000100573,\n",
       "  100000100574,\n",
       "  100000100575,\n",
       "  100000100576,\n",
       "  100000100577,\n",
       "  100000100578,\n",
       "  100000100579,\n",
       "  100000100580,\n",
       "  100000100581,\n",
       "  100000100582,\n",
       "  100000100583,\n",
       "  100000100584,\n",
       "  100000100585,\n",
       "  100000100586,\n",
       "  100000100587,\n",
       "  100000100588,\n",
       "  100000100589,\n",
       "  100000100590,\n",
       "  100000100591,\n",
       "  100000100592,\n",
       "  100000100593,\n",
       "  100000100594,\n",
       "  100000100595,\n",
       "  100000100596,\n",
       "  100000100597,\n",
       "  100000100598,\n",
       "  100000100599,\n",
       "  100000100600,\n",
       "  100000100601,\n",
       "  100000100602,\n",
       "  100000100603,\n",
       "  100000100604,\n",
       "  100000100605,\n",
       "  100000100606,\n",
       "  100000100607,\n",
       "  100000100608,\n",
       "  100000100609,\n",
       "  100000100610,\n",
       "  100000100611,\n",
       "  100000100612,\n",
       "  100000100613,\n",
       "  100000100614,\n",
       "  100000100615,\n",
       "  100000100616,\n",
       "  100000100617,\n",
       "  100000100618,\n",
       "  100000100619,\n",
       "  100000100620,\n",
       "  100000100621,\n",
       "  100000100622,\n",
       "  100000100623,\n",
       "  100000100624,\n",
       "  100000100625,\n",
       "  100000100626,\n",
       "  100000100627,\n",
       "  100000100628,\n",
       "  100000100629,\n",
       "  100000100630,\n",
       "  100000100631,\n",
       "  100000100632,\n",
       "  100000100633,\n",
       "  100000100634,\n",
       "  100000100635,\n",
       "  100000100636,\n",
       "  100000100637,\n",
       "  100000100638,\n",
       "  100000100639,\n",
       "  100000100640,\n",
       "  100000100641,\n",
       "  100000100642,\n",
       "  100000100643,\n",
       "  100000100644,\n",
       "  100000100645,\n",
       "  100000100646,\n",
       "  100000100647,\n",
       "  100000100648,\n",
       "  100000100649,\n",
       "  100000100650,\n",
       "  100000100651,\n",
       "  100000100652,\n",
       "  100000100653,\n",
       "  100000100654,\n",
       "  100000100655,\n",
       "  100000100656,\n",
       "  100000100657,\n",
       "  100000100658,\n",
       "  100000100659,\n",
       "  100000100660,\n",
       "  100000100661,\n",
       "  100000100662,\n",
       "  100000100663,\n",
       "  100000100664,\n",
       "  100000100665,\n",
       "  100000100666,\n",
       "  100000100667,\n",
       "  100000100668,\n",
       "  100000100669,\n",
       "  100000100670,\n",
       "  100000100671,\n",
       "  100000100672,\n",
       "  100000100673,\n",
       "  100000100674,\n",
       "  100000100675,\n",
       "  100000100676,\n",
       "  100000100677,\n",
       "  100000100678,\n",
       "  100000100679,\n",
       "  100000100680,\n",
       "  100000100681,\n",
       "  100000100682,\n",
       "  100000100683,\n",
       "  100000100684,\n",
       "  100000100685,\n",
       "  100000100686,\n",
       "  100000100687,\n",
       "  100000100688,\n",
       "  100000100689,\n",
       "  100000100690,\n",
       "  100000100691,\n",
       "  100000100692,\n",
       "  100000100693,\n",
       "  100000100694,\n",
       "  100000100695,\n",
       "  100000100696,\n",
       "  100000100697,\n",
       "  100000100698,\n",
       "  100000100699,\n",
       "  100000100700,\n",
       "  100000100701,\n",
       "  100000100702,\n",
       "  100000100703,\n",
       "  100000100704,\n",
       "  100000100705,\n",
       "  100000100706,\n",
       "  100000100707,\n",
       "  100000100708,\n",
       "  100000100709,\n",
       "  100000100710,\n",
       "  100000100711,\n",
       "  100000100712,\n",
       "  100000100713,\n",
       "  100000100714,\n",
       "  100000100715,\n",
       "  100000100716,\n",
       "  100000100717,\n",
       "  100000100718,\n",
       "  100000100719,\n",
       "  100000100720,\n",
       "  100000100721,\n",
       "  100000100722,\n",
       "  100000100723,\n",
       "  100000100724,\n",
       "  100000100725,\n",
       "  100000100726,\n",
       "  100000100727,\n",
       "  100000100728,\n",
       "  100000100729,\n",
       "  100000100730,\n",
       "  100000100731,\n",
       "  100000100732,\n",
       "  100000100733,\n",
       "  100000100734,\n",
       "  100000100735,\n",
       "  100000100736,\n",
       "  100000100737,\n",
       "  100000100738,\n",
       "  100000100739,\n",
       "  100000100740,\n",
       "  100000100741,\n",
       "  100000100742,\n",
       "  100000100743,\n",
       "  100000100744,\n",
       "  100000100745,\n",
       "  100000100746,\n",
       "  100000100747,\n",
       "  100000100748,\n",
       "  100000100749,\n",
       "  100000100750,\n",
       "  100000100751,\n",
       "  100000100752,\n",
       "  100000100753,\n",
       "  100000100754,\n",
       "  100000100755,\n",
       "  100000100756,\n",
       "  100000100757,\n",
       "  100000100758,\n",
       "  100000100759,\n",
       "  100000100760,\n",
       "  100000100761,\n",
       "  100000100762,\n",
       "  100000100763,\n",
       "  100000100764,\n",
       "  100000100765,\n",
       "  100000100766,\n",
       "  100000100767,\n",
       "  100000100768,\n",
       "  100000100769,\n",
       "  100000100770,\n",
       "  100000100771,\n",
       "  100000100772,\n",
       "  100000100773,\n",
       "  100000100774,\n",
       "  100000100775,\n",
       "  100000100776,\n",
       "  100000100777,\n",
       "  100000100778,\n",
       "  100000100779,\n",
       "  100000100780,\n",
       "  100000100781,\n",
       "  100000100782,\n",
       "  100000100783,\n",
       "  100000100784,\n",
       "  100000100785,\n",
       "  100000100786,\n",
       "  100000100787,\n",
       "  100000100788,\n",
       "  100000100789,\n",
       "  100000100790,\n",
       "  100000100791,\n",
       "  100000100792,\n",
       "  100000100793,\n",
       "  100000100794,\n",
       "  100000100795,\n",
       "  100000100796,\n",
       "  100000100797,\n",
       "  100000100798,\n",
       "  100000100799,\n",
       "  100000100800,\n",
       "  100000100801,\n",
       "  100000100802,\n",
       "  100000100803,\n",
       "  100000100804,\n",
       "  100000100805,\n",
       "  100000100806,\n",
       "  100000100807,\n",
       "  100000100808,\n",
       "  100000100809,\n",
       "  100000100810,\n",
       "  100000100811,\n",
       "  100000100812,\n",
       "  100000100813,\n",
       "  100000100814,\n",
       "  100000100815,\n",
       "  100000100816,\n",
       "  100000100817,\n",
       "  100000100818,\n",
       "  100000100819,\n",
       "  100000100820,\n",
       "  100000100821,\n",
       "  100000100822,\n",
       "  100000100823,\n",
       "  100000100824,\n",
       "  100000100825,\n",
       "  100000100826,\n",
       "  100000100827,\n",
       "  100000100828,\n",
       "  100000100829,\n",
       "  100000100830,\n",
       "  100000100831,\n",
       "  100000100832,\n",
       "  100000100833,\n",
       "  100000100834,\n",
       "  100000100835,\n",
       "  100000100836,\n",
       "  100000100837,\n",
       "  100000100838,\n",
       "  100000100839,\n",
       "  100000100840,\n",
       "  100000100841,\n",
       "  100000100842,\n",
       "  100000100843,\n",
       "  100000100844,\n",
       "  100000100845,\n",
       "  100000100846,\n",
       "  100000100847,\n",
       "  100000100848,\n",
       "  100000100849,\n",
       "  100000100850,\n",
       "  100000100851,\n",
       "  100000100852,\n",
       "  100000100853,\n",
       "  100000100854,\n",
       "  100000100855,\n",
       "  100000100856,\n",
       "  100000100857,\n",
       "  100000100858,\n",
       "  100000100859,\n",
       "  100000100860,\n",
       "  100000100861,\n",
       "  100000100862,\n",
       "  100000100863,\n",
       "  100000100864,\n",
       "  100000100865,\n",
       "  100000100866,\n",
       "  100000100867,\n",
       "  100000100868,\n",
       "  100000100869,\n",
       "  100000100870,\n",
       "  100000100871,\n",
       "  100000100872,\n",
       "  100000100873,\n",
       "  100000100874,\n",
       "  100000100875,\n",
       "  100000100876,\n",
       "  100000100877,\n",
       "  100000100878,\n",
       "  100000100879,\n",
       "  100000100880,\n",
       "  100000100881,\n",
       "  100000100882,\n",
       "  100000100883,\n",
       "  100000100884,\n",
       "  100000100885,\n",
       "  100000100886,\n",
       "  100000100887,\n",
       "  100000100888,\n",
       "  100000100889,\n",
       "  100000100890,\n",
       "  100000100891,\n",
       "  100000100892,\n",
       "  100000100893,\n",
       "  100000100894,\n",
       "  100000100895,\n",
       "  100000100896,\n",
       "  100000100897,\n",
       "  100000100898,\n",
       "  100000100899,\n",
       "  100000100900,\n",
       "  100000100901,\n",
       "  100000100902,\n",
       "  100000100903,\n",
       "  100000100904,\n",
       "  100000100905,\n",
       "  100000100906,\n",
       "  100000100907,\n",
       "  100000100908,\n",
       "  100000100909,\n",
       "  100000100910,\n",
       "  100000100911,\n",
       "  100000100912,\n",
       "  100000100913,\n",
       "  100000100914,\n",
       "  100000100915,\n",
       "  100000100916,\n",
       "  100000100917,\n",
       "  100000100918,\n",
       "  100000100919,\n",
       "  100000100920,\n",
       "  100000100921,\n",
       "  100000100922,\n",
       "  100000100923,\n",
       "  100000100924,\n",
       "  100000100925,\n",
       "  100000100926,\n",
       "  100000100927,\n",
       "  100000100928,\n",
       "  100000100929,\n",
       "  100000100930,\n",
       "  100000100931,\n",
       "  100000100932,\n",
       "  100000100933,\n",
       "  100000100934,\n",
       "  100000100935,\n",
       "  100000100936,\n",
       "  100000100937,\n",
       "  100000100938,\n",
       "  100000100939,\n",
       "  100000100940,\n",
       "  100000100941,\n",
       "  100000100942,\n",
       "  100000100943,\n",
       "  100000100944,\n",
       "  100000100945,\n",
       "  100000100946,\n",
       "  100000100947,\n",
       "  100000100948,\n",
       "  100000100949,\n",
       "  100000100950,\n",
       "  100000100951,\n",
       "  100000100952,\n",
       "  100000100953,\n",
       "  100000100954,\n",
       "  100000100955,\n",
       "  100000100956,\n",
       "  100000100957,\n",
       "  100000100958,\n",
       "  100000100959,\n",
       "  100000100960,\n",
       "  100000100961,\n",
       "  100000100962,\n",
       "  100000100963,\n",
       "  100000100964,\n",
       "  100000100965,\n",
       "  100000100966,\n",
       "  100000100967,\n",
       "  100000100968,\n",
       "  100000100969,\n",
       "  100000100970,\n",
       "  100000100971,\n",
       "  100000100972,\n",
       "  100000100973,\n",
       "  100000100974,\n",
       "  100000100975,\n",
       "  100000100976,\n",
       "  100000100977,\n",
       "  100000100978,\n",
       "  100000100979,\n",
       "  100000100980,\n",
       "  100000100981,\n",
       "  100000100982,\n",
       "  100000100983,\n",
       "  100000100984,\n",
       "  100000100985,\n",
       "  100000100986,\n",
       "  100000100987,\n",
       "  100000100988,\n",
       "  100000100989,\n",
       "  100000100990,\n",
       "  100000100991,\n",
       "  100000100992,\n",
       "  100000100993,\n",
       "  100000100994,\n",
       "  100000100995,\n",
       "  100000100996,\n",
       "  100000100997,\n",
       "  100000100998,\n",
       "  100000100999,\n",
       "  ...],\n",
       " [100000100000,\n",
       "  100000100001,\n",
       "  100000100002,\n",
       "  100000100003,\n",
       "  100000100004,\n",
       "  100000100005,\n",
       "  100000100006,\n",
       "  100000100007,\n",
       "  100000100008,\n",
       "  100000100009,\n",
       "  100000100010,\n",
       "  100000100011,\n",
       "  100000100012,\n",
       "  100000100013,\n",
       "  100000100014,\n",
       "  100000100015,\n",
       "  100000100016,\n",
       "  100000100017,\n",
       "  100000100018,\n",
       "  100000100019,\n",
       "  100000100020,\n",
       "  100000100021,\n",
       "  100000100022,\n",
       "  100000100023,\n",
       "  100000100024,\n",
       "  100000100025,\n",
       "  100000100026,\n",
       "  100000100027,\n",
       "  100000100028,\n",
       "  100000100029,\n",
       "  100000100030,\n",
       "  100000100031,\n",
       "  100000100032,\n",
       "  100000100033,\n",
       "  100000100034,\n",
       "  100000100035,\n",
       "  100000100036,\n",
       "  100000100037,\n",
       "  100000100038,\n",
       "  100000100039,\n",
       "  100000100040,\n",
       "  100000100041,\n",
       "  100000100042,\n",
       "  100000100043,\n",
       "  100000100044,\n",
       "  100000100045,\n",
       "  100000100046,\n",
       "  100000100047,\n",
       "  100000100048,\n",
       "  100000100049,\n",
       "  100000100050,\n",
       "  100000100051,\n",
       "  100000100052,\n",
       "  100000100053,\n",
       "  100000100054,\n",
       "  100000100055,\n",
       "  100000100056,\n",
       "  100000100057,\n",
       "  100000100058,\n",
       "  100000100059,\n",
       "  100000100060,\n",
       "  100000100061,\n",
       "  100000100062,\n",
       "  100000100063,\n",
       "  100000100064,\n",
       "  100000100065,\n",
       "  100000100066,\n",
       "  100000100067,\n",
       "  100000100068,\n",
       "  100000100069,\n",
       "  100000100070,\n",
       "  100000100071,\n",
       "  100000100072,\n",
       "  100000100073,\n",
       "  100000100074,\n",
       "  100000100075,\n",
       "  100000100076,\n",
       "  100000100077,\n",
       "  100000100078,\n",
       "  100000100079,\n",
       "  100000100080,\n",
       "  100000100081,\n",
       "  100000100082,\n",
       "  100000100083,\n",
       "  100000100084,\n",
       "  100000100085,\n",
       "  100000100086,\n",
       "  100000100087,\n",
       "  100000100088,\n",
       "  100000100089,\n",
       "  100000100090,\n",
       "  100000100091,\n",
       "  100000100092,\n",
       "  100000100093,\n",
       "  100000100094,\n",
       "  100000100095,\n",
       "  100000100096,\n",
       "  100000100097,\n",
       "  100000100098,\n",
       "  100000100099,\n",
       "  100000100100,\n",
       "  100000100101,\n",
       "  100000100102,\n",
       "  100000100103,\n",
       "  100000100104,\n",
       "  100000100105,\n",
       "  100000100106,\n",
       "  100000100107,\n",
       "  100000100108,\n",
       "  100000100109,\n",
       "  100000100110,\n",
       "  100000100111,\n",
       "  100000100112,\n",
       "  100000100113,\n",
       "  100000100114,\n",
       "  100000100115,\n",
       "  100000100116,\n",
       "  100000100117,\n",
       "  100000100118,\n",
       "  100000100119,\n",
       "  100000100120,\n",
       "  100000100121,\n",
       "  100000100122,\n",
       "  100000100123,\n",
       "  100000100124,\n",
       "  100000100125,\n",
       "  100000100126,\n",
       "  100000100127,\n",
       "  100000100128,\n",
       "  100000100129,\n",
       "  100000100130,\n",
       "  100000100131,\n",
       "  100000100132,\n",
       "  100000100133,\n",
       "  100000100134,\n",
       "  100000100135,\n",
       "  100000100136,\n",
       "  100000100137,\n",
       "  100000100138,\n",
       "  100000100139,\n",
       "  100000100140,\n",
       "  100000100141,\n",
       "  100000100142,\n",
       "  100000100143,\n",
       "  100000100144,\n",
       "  100000100145,\n",
       "  100000100146,\n",
       "  100000100147,\n",
       "  100000100148,\n",
       "  100000100149,\n",
       "  100000100150,\n",
       "  100000100151,\n",
       "  100000100152,\n",
       "  100000100153,\n",
       "  100000100154,\n",
       "  100000100155,\n",
       "  100000100156,\n",
       "  100000100157,\n",
       "  100000100158,\n",
       "  100000100159,\n",
       "  100000100160,\n",
       "  100000100161,\n",
       "  100000100162,\n",
       "  100000100163,\n",
       "  100000100164,\n",
       "  100000100165,\n",
       "  100000100166,\n",
       "  100000100167,\n",
       "  100000100168,\n",
       "  100000100169,\n",
       "  100000100170,\n",
       "  100000100171,\n",
       "  100000100172,\n",
       "  100000100173,\n",
       "  100000100174,\n",
       "  100000100175,\n",
       "  100000100176,\n",
       "  100000100177,\n",
       "  100000100178,\n",
       "  100000100179,\n",
       "  100000100180,\n",
       "  100000100181,\n",
       "  100000100182,\n",
       "  100000100183,\n",
       "  100000100184,\n",
       "  100000100185,\n",
       "  100000100186,\n",
       "  100000100187,\n",
       "  100000100188,\n",
       "  100000100189,\n",
       "  100000100190,\n",
       "  100000100191,\n",
       "  100000100192,\n",
       "  100000100193,\n",
       "  100000100194,\n",
       "  100000100195,\n",
       "  100000100196,\n",
       "  100000100197,\n",
       "  100000100198,\n",
       "  100000100199,\n",
       "  100000100200,\n",
       "  100000100201,\n",
       "  100000100202,\n",
       "  100000100203,\n",
       "  100000100204,\n",
       "  100000100205,\n",
       "  100000100206,\n",
       "  100000100207,\n",
       "  100000100208,\n",
       "  100000100209,\n",
       "  100000100210,\n",
       "  100000100211,\n",
       "  100000100212,\n",
       "  100000100213,\n",
       "  100000100214,\n",
       "  100000100215,\n",
       "  100000100216,\n",
       "  100000100217,\n",
       "  100000100218,\n",
       "  100000100219,\n",
       "  100000100220,\n",
       "  100000100221,\n",
       "  100000100222,\n",
       "  100000100223,\n",
       "  100000100224,\n",
       "  100000100225,\n",
       "  100000100226,\n",
       "  100000100227,\n",
       "  100000100228,\n",
       "  100000100229,\n",
       "  100000100230,\n",
       "  100000100231,\n",
       "  100000100232,\n",
       "  100000100233,\n",
       "  100000100234,\n",
       "  100000100235,\n",
       "  100000100236,\n",
       "  100000100237,\n",
       "  100000100238,\n",
       "  100000100239,\n",
       "  100000100240,\n",
       "  100000100241,\n",
       "  100000100242,\n",
       "  100000100243,\n",
       "  100000100244,\n",
       "  100000100245,\n",
       "  100000100246,\n",
       "  100000100247,\n",
       "  100000100248,\n",
       "  100000100249,\n",
       "  100000100250,\n",
       "  100000100251,\n",
       "  100000100252,\n",
       "  100000100253,\n",
       "  100000100254,\n",
       "  100000100255,\n",
       "  100000100256,\n",
       "  100000100257,\n",
       "  100000100258,\n",
       "  100000100259,\n",
       "  100000100260,\n",
       "  100000100261,\n",
       "  100000100262,\n",
       "  100000100263,\n",
       "  100000100264,\n",
       "  100000100265,\n",
       "  100000100266,\n",
       "  100000100267,\n",
       "  100000100268,\n",
       "  100000100269,\n",
       "  100000100270,\n",
       "  100000100271,\n",
       "  100000100272,\n",
       "  100000100273,\n",
       "  100000100274,\n",
       "  100000100275,\n",
       "  100000100276,\n",
       "  100000100277,\n",
       "  100000100278,\n",
       "  100000100279,\n",
       "  100000100280,\n",
       "  100000100281,\n",
       "  100000100282,\n",
       "  100000100283,\n",
       "  100000100284,\n",
       "  100000100285,\n",
       "  100000100286,\n",
       "  100000100287,\n",
       "  100000100288,\n",
       "  100000100289,\n",
       "  100000100290,\n",
       "  100000100291,\n",
       "  100000100292,\n",
       "  100000100293,\n",
       "  100000100294,\n",
       "  100000100295,\n",
       "  100000100296,\n",
       "  100000100297,\n",
       "  100000100298,\n",
       "  100000100299,\n",
       "  100000100300,\n",
       "  100000100301,\n",
       "  100000100302,\n",
       "  100000100303,\n",
       "  100000100304,\n",
       "  100000100305,\n",
       "  100000100306,\n",
       "  100000100307,\n",
       "  100000100308,\n",
       "  100000100309,\n",
       "  100000100310,\n",
       "  100000100311,\n",
       "  100000100312,\n",
       "  100000100313,\n",
       "  100000100314,\n",
       "  100000100315,\n",
       "  100000100316,\n",
       "  100000100317,\n",
       "  100000100318,\n",
       "  100000100319,\n",
       "  100000100320,\n",
       "  100000100321,\n",
       "  100000100322,\n",
       "  100000100323,\n",
       "  100000100324,\n",
       "  100000100325,\n",
       "  100000100326,\n",
       "  100000100327,\n",
       "  100000100328,\n",
       "  100000100329,\n",
       "  100000100330,\n",
       "  100000100331,\n",
       "  100000100332,\n",
       "  100000100333,\n",
       "  100000100334,\n",
       "  100000100335,\n",
       "  100000100336,\n",
       "  100000100337,\n",
       "  100000100338,\n",
       "  100000100339,\n",
       "  100000100340,\n",
       "  100000100341,\n",
       "  100000100342,\n",
       "  100000100343,\n",
       "  100000100344,\n",
       "  100000100345,\n",
       "  100000100346,\n",
       "  100000100347,\n",
       "  100000100348,\n",
       "  100000100349,\n",
       "  100000100350,\n",
       "  100000100351,\n",
       "  100000100352,\n",
       "  100000100353,\n",
       "  100000100354,\n",
       "  100000100355,\n",
       "  100000100356,\n",
       "  100000100357,\n",
       "  100000100358,\n",
       "  100000100359,\n",
       "  100000100360,\n",
       "  100000100361,\n",
       "  100000100362,\n",
       "  100000100363,\n",
       "  100000100364,\n",
       "  100000100365,\n",
       "  100000100366,\n",
       "  100000100367,\n",
       "  100000100368,\n",
       "  100000100369,\n",
       "  100000100370,\n",
       "  100000100371,\n",
       "  100000100372,\n",
       "  100000100373,\n",
       "  100000100374,\n",
       "  100000100375,\n",
       "  100000100376,\n",
       "  100000100377,\n",
       "  100000100378,\n",
       "  100000100379,\n",
       "  100000100380,\n",
       "  100000100381,\n",
       "  100000100382,\n",
       "  100000100383,\n",
       "  100000100384,\n",
       "  100000100385,\n",
       "  100000100386,\n",
       "  100000100387,\n",
       "  100000100388,\n",
       "  100000100389,\n",
       "  100000100390,\n",
       "  100000100391,\n",
       "  100000100392,\n",
       "  100000100393,\n",
       "  100000100394,\n",
       "  100000100395,\n",
       "  100000100396,\n",
       "  100000100397,\n",
       "  100000100398,\n",
       "  100000100399,\n",
       "  100000100400,\n",
       "  100000100401,\n",
       "  100000100402,\n",
       "  100000100403,\n",
       "  100000100404,\n",
       "  100000100405,\n",
       "  100000100406,\n",
       "  100000100407,\n",
       "  100000100408,\n",
       "  100000100409,\n",
       "  100000100410,\n",
       "  100000100411,\n",
       "  100000100412,\n",
       "  100000100413,\n",
       "  100000100414,\n",
       "  100000100415,\n",
       "  100000100416,\n",
       "  100000100417,\n",
       "  100000100418,\n",
       "  100000100419,\n",
       "  100000100420,\n",
       "  100000100421,\n",
       "  100000100422,\n",
       "  100000100423,\n",
       "  100000100424,\n",
       "  100000100425,\n",
       "  100000100426,\n",
       "  100000100427,\n",
       "  100000100428,\n",
       "  100000100429,\n",
       "  100000100430,\n",
       "  100000100431,\n",
       "  100000100432,\n",
       "  100000100433,\n",
       "  100000100434,\n",
       "  100000100435,\n",
       "  100000100436,\n",
       "  100000100437,\n",
       "  100000100438,\n",
       "  100000100439,\n",
       "  100000100440,\n",
       "  100000100441,\n",
       "  100000100442,\n",
       "  100000100443,\n",
       "  100000100444,\n",
       "  100000100445,\n",
       "  100000100446,\n",
       "  100000100447,\n",
       "  100000100448,\n",
       "  100000100449,\n",
       "  100000100450,\n",
       "  100000100451,\n",
       "  100000100452,\n",
       "  100000100453,\n",
       "  100000100454,\n",
       "  100000100455,\n",
       "  100000100456,\n",
       "  100000100457,\n",
       "  100000100458,\n",
       "  100000100459,\n",
       "  100000100460,\n",
       "  100000100461,\n",
       "  100000100462,\n",
       "  100000100463,\n",
       "  100000100464,\n",
       "  100000100465,\n",
       "  100000100466,\n",
       "  100000100467,\n",
       "  100000100468,\n",
       "  100000100469,\n",
       "  100000100470,\n",
       "  100000100471,\n",
       "  100000100472,\n",
       "  100000100473,\n",
       "  100000100474,\n",
       "  100000100475,\n",
       "  100000100476,\n",
       "  100000100477,\n",
       "  100000100478,\n",
       "  100000100479,\n",
       "  100000100480,\n",
       "  100000100481,\n",
       "  100000100482,\n",
       "  100000100483,\n",
       "  100000100484,\n",
       "  100000100485,\n",
       "  100000100486,\n",
       "  100000100487,\n",
       "  100000100488,\n",
       "  100000100489,\n",
       "  100000100490,\n",
       "  100000100491,\n",
       "  100000100492,\n",
       "  100000100493,\n",
       "  100000100494,\n",
       "  100000100495,\n",
       "  100000100496,\n",
       "  100000100497,\n",
       "  100000100498,\n",
       "  100000100499,\n",
       "  100000100500,\n",
       "  100000100501,\n",
       "  100000100502,\n",
       "  100000100503,\n",
       "  100000100504,\n",
       "  100000100505,\n",
       "  100000100506,\n",
       "  100000100507,\n",
       "  100000100508,\n",
       "  100000100509,\n",
       "  100000100510,\n",
       "  100000100511,\n",
       "  100000100512,\n",
       "  100000100513,\n",
       "  100000100514,\n",
       "  100000100515,\n",
       "  100000100516,\n",
       "  100000100517,\n",
       "  100000100518,\n",
       "  100000100519,\n",
       "  100000100520,\n",
       "  100000100521,\n",
       "  100000100522,\n",
       "  100000100523,\n",
       "  100000100524,\n",
       "  100000100525,\n",
       "  100000100526,\n",
       "  100000100527,\n",
       "  100000100528,\n",
       "  100000100529,\n",
       "  100000100530,\n",
       "  100000100531,\n",
       "  100000100532,\n",
       "  100000100533,\n",
       "  100000100534,\n",
       "  100000100535,\n",
       "  100000100536,\n",
       "  100000100537,\n",
       "  100000100538,\n",
       "  100000100539,\n",
       "  100000100540,\n",
       "  100000100541,\n",
       "  100000100542,\n",
       "  100000100543,\n",
       "  100000100544,\n",
       "  100000100545,\n",
       "  100000100546,\n",
       "  100000100547,\n",
       "  100000100548,\n",
       "  100000100549,\n",
       "  100000100550,\n",
       "  100000100551,\n",
       "  100000100552,\n",
       "  100000100553,\n",
       "  100000100554,\n",
       "  100000100555,\n",
       "  100000100556,\n",
       "  100000100557,\n",
       "  100000100558,\n",
       "  100000100559,\n",
       "  100000100560,\n",
       "  100000100561,\n",
       "  100000100562,\n",
       "  100000100563,\n",
       "  100000100564,\n",
       "  100000100565,\n",
       "  100000100566,\n",
       "  100000100567,\n",
       "  100000100568,\n",
       "  100000100569,\n",
       "  100000100570,\n",
       "  100000100571,\n",
       "  100000100572,\n",
       "  100000100573,\n",
       "  100000100574,\n",
       "  100000100575,\n",
       "  100000100576,\n",
       "  100000100577,\n",
       "  100000100578,\n",
       "  100000100579,\n",
       "  100000100580,\n",
       "  100000100581,\n",
       "  100000100582,\n",
       "  100000100583,\n",
       "  100000100584,\n",
       "  100000100585,\n",
       "  100000100586,\n",
       "  100000100587,\n",
       "  100000100588,\n",
       "  100000100589,\n",
       "  100000100590,\n",
       "  100000100591,\n",
       "  100000100592,\n",
       "  100000100593,\n",
       "  100000100594,\n",
       "  100000100595,\n",
       "  100000100596,\n",
       "  100000100597,\n",
       "  100000100598,\n",
       "  100000100599,\n",
       "  100000100600,\n",
       "  100000100601,\n",
       "  100000100602,\n",
       "  100000100603,\n",
       "  100000100604,\n",
       "  100000100605,\n",
       "  100000100606,\n",
       "  100000100607,\n",
       "  100000100608,\n",
       "  100000100609,\n",
       "  100000100610,\n",
       "  100000100611,\n",
       "  100000100612,\n",
       "  100000100613,\n",
       "  100000100614,\n",
       "  100000100615,\n",
       "  100000100616,\n",
       "  100000100617,\n",
       "  100000100618,\n",
       "  100000100619,\n",
       "  100000100620,\n",
       "  100000100621,\n",
       "  100000100622,\n",
       "  100000100623,\n",
       "  100000100624,\n",
       "  100000100625,\n",
       "  100000100626,\n",
       "  100000100627,\n",
       "  100000100628,\n",
       "  100000100629,\n",
       "  100000100630,\n",
       "  100000100631,\n",
       "  100000100632,\n",
       "  100000100633,\n",
       "  100000100634,\n",
       "  100000100635,\n",
       "  100000100636,\n",
       "  100000100637,\n",
       "  100000100638,\n",
       "  100000100639,\n",
       "  100000100640,\n",
       "  100000100641,\n",
       "  100000100642,\n",
       "  100000100643,\n",
       "  100000100644,\n",
       "  100000100645,\n",
       "  100000100646,\n",
       "  100000100647,\n",
       "  100000100648,\n",
       "  100000100649,\n",
       "  100000100650,\n",
       "  100000100651,\n",
       "  100000100652,\n",
       "  100000100653,\n",
       "  100000100654,\n",
       "  100000100655,\n",
       "  100000100656,\n",
       "  100000100657,\n",
       "  100000100658,\n",
       "  100000100659,\n",
       "  100000100660,\n",
       "  100000100661,\n",
       "  100000100662,\n",
       "  100000100663,\n",
       "  100000100664,\n",
       "  100000100665,\n",
       "  100000100666,\n",
       "  100000100667,\n",
       "  100000100668,\n",
       "  100000100669,\n",
       "  100000100670,\n",
       "  100000100671,\n",
       "  100000100672,\n",
       "  100000100673,\n",
       "  100000100674,\n",
       "  100000100675,\n",
       "  100000100676,\n",
       "  100000100677,\n",
       "  100000100678,\n",
       "  100000100679,\n",
       "  100000100680,\n",
       "  100000100681,\n",
       "  100000100682,\n",
       "  100000100683,\n",
       "  100000100684,\n",
       "  100000100685,\n",
       "  100000100686,\n",
       "  100000100687,\n",
       "  100000100688,\n",
       "  100000100689,\n",
       "  100000100690,\n",
       "  100000100691,\n",
       "  100000100692,\n",
       "  100000100693,\n",
       "  100000100694,\n",
       "  100000100695,\n",
       "  100000100696,\n",
       "  100000100697,\n",
       "  100000100698,\n",
       "  100000100699,\n",
       "  100000100700,\n",
       "  100000100701,\n",
       "  100000100702,\n",
       "  100000100703,\n",
       "  100000100704,\n",
       "  100000100705,\n",
       "  100000100706,\n",
       "  100000100707,\n",
       "  100000100708,\n",
       "  100000100709,\n",
       "  100000100710,\n",
       "  100000100711,\n",
       "  100000100712,\n",
       "  100000100713,\n",
       "  100000100714,\n",
       "  100000100715,\n",
       "  100000100716,\n",
       "  100000100717,\n",
       "  100000100718,\n",
       "  100000100719,\n",
       "  100000100720,\n",
       "  100000100721,\n",
       "  100000100722,\n",
       "  100000100723,\n",
       "  100000100724,\n",
       "  100000100725,\n",
       "  100000100726,\n",
       "  100000100727,\n",
       "  100000100728,\n",
       "  100000100729,\n",
       "  100000100730,\n",
       "  100000100731,\n",
       "  100000100732,\n",
       "  100000100733,\n",
       "  100000100734,\n",
       "  100000100735,\n",
       "  100000100736,\n",
       "  100000100737,\n",
       "  100000100738,\n",
       "  100000100739,\n",
       "  100000100740,\n",
       "  100000100741,\n",
       "  100000100742,\n",
       "  100000100743,\n",
       "  100000100744,\n",
       "  100000100745,\n",
       "  100000100746,\n",
       "  100000100747,\n",
       "  100000100748,\n",
       "  100000100749,\n",
       "  100000100750,\n",
       "  100000100751,\n",
       "  100000100752,\n",
       "  100000100753,\n",
       "  100000100754,\n",
       "  100000100755,\n",
       "  100000100756,\n",
       "  100000100757,\n",
       "  100000100758,\n",
       "  100000100759,\n",
       "  100000100760,\n",
       "  100000100761,\n",
       "  100000100762,\n",
       "  100000100763,\n",
       "  100000100764,\n",
       "  100000100765,\n",
       "  100000100766,\n",
       "  100000100767,\n",
       "  100000100768,\n",
       "  100000100769,\n",
       "  100000100770,\n",
       "  100000100771,\n",
       "  100000100772,\n",
       "  100000100773,\n",
       "  100000100774,\n",
       "  100000100775,\n",
       "  100000100776,\n",
       "  100000100777,\n",
       "  100000100778,\n",
       "  100000100779,\n",
       "  100000100780,\n",
       "  100000100781,\n",
       "  100000100782,\n",
       "  100000100783,\n",
       "  100000100784,\n",
       "  100000100785,\n",
       "  100000100786,\n",
       "  100000100787,\n",
       "  100000100788,\n",
       "  100000100789,\n",
       "  100000100790,\n",
       "  100000100791,\n",
       "  100000100792,\n",
       "  100000100793,\n",
       "  100000100794,\n",
       "  100000100795,\n",
       "  100000100796,\n",
       "  100000100797,\n",
       "  100000100798,\n",
       "  100000100799,\n",
       "  100000100800,\n",
       "  100000100801,\n",
       "  100000100802,\n",
       "  100000100803,\n",
       "  100000100804,\n",
       "  100000100805,\n",
       "  100000100806,\n",
       "  100000100807,\n",
       "  100000100808,\n",
       "  100000100809,\n",
       "  100000100810,\n",
       "  100000100811,\n",
       "  100000100812,\n",
       "  100000100813,\n",
       "  100000100814,\n",
       "  100000100815,\n",
       "  100000100816,\n",
       "  100000100817,\n",
       "  100000100818,\n",
       "  100000100819,\n",
       "  100000100820,\n",
       "  100000100821,\n",
       "  100000100822,\n",
       "  100000100823,\n",
       "  100000100824,\n",
       "  100000100825,\n",
       "  100000100826,\n",
       "  100000100827,\n",
       "  100000100828,\n",
       "  100000100829,\n",
       "  100000100830,\n",
       "  100000100831,\n",
       "  100000100832,\n",
       "  100000100833,\n",
       "  100000100834,\n",
       "  100000100835,\n",
       "  100000100836,\n",
       "  100000100837,\n",
       "  100000100838,\n",
       "  100000100839,\n",
       "  100000100840,\n",
       "  100000100841,\n",
       "  100000100842,\n",
       "  100000100843,\n",
       "  100000100844,\n",
       "  100000100845,\n",
       "  100000100846,\n",
       "  100000100847,\n",
       "  100000100848,\n",
       "  100000100849,\n",
       "  100000100850,\n",
       "  100000100851,\n",
       "  100000100852,\n",
       "  100000100853,\n",
       "  100000100854,\n",
       "  100000100855,\n",
       "  100000100856,\n",
       "  100000100857,\n",
       "  100000100858,\n",
       "  100000100859,\n",
       "  100000100860,\n",
       "  100000100861,\n",
       "  100000100862,\n",
       "  100000100863,\n",
       "  100000100864,\n",
       "  100000100865,\n",
       "  100000100866,\n",
       "  100000100867,\n",
       "  100000100868,\n",
       "  100000100869,\n",
       "  100000100870,\n",
       "  100000100871,\n",
       "  100000100872,\n",
       "  100000100873,\n",
       "  100000100874,\n",
       "  100000100875,\n",
       "  100000100876,\n",
       "  100000100877,\n",
       "  100000100878,\n",
       "  100000100879,\n",
       "  100000100880,\n",
       "  100000100881,\n",
       "  100000100882,\n",
       "  100000100883,\n",
       "  100000100884,\n",
       "  100000100885,\n",
       "  100000100886,\n",
       "  100000100887,\n",
       "  100000100888,\n",
       "  100000100889,\n",
       "  100000100890,\n",
       "  100000100891,\n",
       "  100000100892,\n",
       "  100000100893,\n",
       "  100000100894,\n",
       "  100000100895,\n",
       "  100000100896,\n",
       "  100000100897,\n",
       "  100000100898,\n",
       "  100000100899,\n",
       "  100000100900,\n",
       "  100000100901,\n",
       "  100000100902,\n",
       "  100000100903,\n",
       "  100000100904,\n",
       "  100000100905,\n",
       "  100000100906,\n",
       "  100000100907,\n",
       "  100000100908,\n",
       "  100000100909,\n",
       "  100000100910,\n",
       "  100000100911,\n",
       "  100000100912,\n",
       "  100000100913,\n",
       "  100000100914,\n",
       "  100000100915,\n",
       "  100000100916,\n",
       "  100000100917,\n",
       "  100000100918,\n",
       "  100000100919,\n",
       "  100000100920,\n",
       "  100000100921,\n",
       "  100000100922,\n",
       "  100000100923,\n",
       "  100000100924,\n",
       "  100000100925,\n",
       "  100000100926,\n",
       "  100000100927,\n",
       "  100000100928,\n",
       "  100000100929,\n",
       "  100000100930,\n",
       "  100000100931,\n",
       "  100000100932,\n",
       "  100000100933,\n",
       "  100000100934,\n",
       "  100000100935,\n",
       "  100000100936,\n",
       "  100000100937,\n",
       "  100000100938,\n",
       "  100000100939,\n",
       "  100000100940,\n",
       "  100000100941,\n",
       "  100000100942,\n",
       "  100000100943,\n",
       "  100000100944,\n",
       "  100000100945,\n",
       "  100000100946,\n",
       "  100000100947,\n",
       "  100000100948,\n",
       "  100000100949,\n",
       "  100000100950,\n",
       "  100000100951,\n",
       "  100000100952,\n",
       "  100000100953,\n",
       "  100000100954,\n",
       "  100000100955,\n",
       "  100000100956,\n",
       "  100000100957,\n",
       "  100000100958,\n",
       "  100000100959,\n",
       "  100000100960,\n",
       "  100000100961,\n",
       "  100000100962,\n",
       "  100000100963,\n",
       "  100000100964,\n",
       "  100000100965,\n",
       "  100000100966,\n",
       "  100000100967,\n",
       "  100000100968,\n",
       "  100000100969,\n",
       "  100000100970,\n",
       "  100000100971,\n",
       "  100000100972,\n",
       "  100000100973,\n",
       "  100000100974,\n",
       "  100000100975,\n",
       "  100000100976,\n",
       "  100000100977,\n",
       "  100000100978,\n",
       "  100000100979,\n",
       "  100000100980,\n",
       "  100000100981,\n",
       "  100000100982,\n",
       "  100000100983,\n",
       "  100000100984,\n",
       "  100000100985,\n",
       "  100000100986,\n",
       "  100000100987,\n",
       "  100000100988,\n",
       "  100000100989,\n",
       "  100000100990,\n",
       "  100000100991,\n",
       "  100000100992,\n",
       "  100000100993,\n",
       "  100000100994,\n",
       "  100000100995,\n",
       "  100000100996,\n",
       "  100000100997,\n",
       "  100000100998,\n",
       "  100000100999,\n",
       "  ...],\n",
       " [100000100000,\n",
       "  100000100001,\n",
       "  100000100002,\n",
       "  100000100003,\n",
       "  100000100004,\n",
       "  100000100005,\n",
       "  100000100006,\n",
       "  100000100007,\n",
       "  100000100008,\n",
       "  100000100009,\n",
       "  100000100010,\n",
       "  100000100011,\n",
       "  100000100012,\n",
       "  100000100013,\n",
       "  100000100014,\n",
       "  100000100015,\n",
       "  100000100016,\n",
       "  100000100017,\n",
       "  100000100018,\n",
       "  100000100019,\n",
       "  100000100020,\n",
       "  100000100021,\n",
       "  100000100022,\n",
       "  100000100023,\n",
       "  100000100024,\n",
       "  100000100025,\n",
       "  100000100026,\n",
       "  100000100027,\n",
       "  100000100028,\n",
       "  100000100029,\n",
       "  100000100030,\n",
       "  100000100031,\n",
       "  100000100032,\n",
       "  100000100033,\n",
       "  100000100034,\n",
       "  100000100035,\n",
       "  100000100036,\n",
       "  100000100037,\n",
       "  100000100038,\n",
       "  100000100039,\n",
       "  100000100040,\n",
       "  100000100041,\n",
       "  100000100042,\n",
       "  100000100043,\n",
       "  100000100044,\n",
       "  100000100045,\n",
       "  100000100046,\n",
       "  100000100047,\n",
       "  100000100048,\n",
       "  100000100049,\n",
       "  100000100050,\n",
       "  100000100051,\n",
       "  100000100052,\n",
       "  100000100053,\n",
       "  100000100054,\n",
       "  100000100055,\n",
       "  100000100056,\n",
       "  100000100057,\n",
       "  100000100058,\n",
       "  100000100059,\n",
       "  100000100060,\n",
       "  100000100061,\n",
       "  100000100062,\n",
       "  100000100063,\n",
       "  100000100064,\n",
       "  100000100065,\n",
       "  100000100066,\n",
       "  100000100067,\n",
       "  100000100068,\n",
       "  100000100069,\n",
       "  100000100070,\n",
       "  100000100071,\n",
       "  100000100072,\n",
       "  100000100073,\n",
       "  100000100074,\n",
       "  100000100075,\n",
       "  100000100076,\n",
       "  100000100077,\n",
       "  100000100078,\n",
       "  100000100079,\n",
       "  100000100080,\n",
       "  100000100081,\n",
       "  100000100082,\n",
       "  100000100083,\n",
       "  100000100084,\n",
       "  100000100085,\n",
       "  100000100086,\n",
       "  100000100087,\n",
       "  100000100088,\n",
       "  100000100089,\n",
       "  100000100090,\n",
       "  100000100091,\n",
       "  100000100092,\n",
       "  100000100093,\n",
       "  100000100094,\n",
       "  100000100095,\n",
       "  100000100096,\n",
       "  100000100097,\n",
       "  100000100098,\n",
       "  100000100099,\n",
       "  100000100100,\n",
       "  100000100101,\n",
       "  100000100102,\n",
       "  100000100103,\n",
       "  100000100104,\n",
       "  100000100105,\n",
       "  100000100106,\n",
       "  100000100107,\n",
       "  100000100108,\n",
       "  100000100109,\n",
       "  100000100110,\n",
       "  100000100111,\n",
       "  100000100112,\n",
       "  100000100113,\n",
       "  100000100114,\n",
       "  100000100115,\n",
       "  100000100116,\n",
       "  100000100117,\n",
       "  100000100118,\n",
       "  100000100119,\n",
       "  100000100120,\n",
       "  100000100121,\n",
       "  100000100122,\n",
       "  100000100123,\n",
       "  100000100124,\n",
       "  100000100125,\n",
       "  100000100126,\n",
       "  100000100127,\n",
       "  100000100128,\n",
       "  100000100129,\n",
       "  100000100130,\n",
       "  100000100131,\n",
       "  100000100132,\n",
       "  100000100133,\n",
       "  100000100134,\n",
       "  100000100135,\n",
       "  100000100136,\n",
       "  100000100137,\n",
       "  100000100138,\n",
       "  100000100139,\n",
       "  100000100140,\n",
       "  100000100141,\n",
       "  100000100142,\n",
       "  100000100143,\n",
       "  100000100144,\n",
       "  100000100145,\n",
       "  100000100146,\n",
       "  100000100147,\n",
       "  100000100148,\n",
       "  100000100149,\n",
       "  100000100150,\n",
       "  100000100151,\n",
       "  100000100152,\n",
       "  100000100153,\n",
       "  100000100154,\n",
       "  100000100155,\n",
       "  100000100156,\n",
       "  100000100157,\n",
       "  100000100158,\n",
       "  100000100159,\n",
       "  100000100160,\n",
       "  100000100161,\n",
       "  100000100162,\n",
       "  100000100163,\n",
       "  100000100164,\n",
       "  100000100165,\n",
       "  100000100166,\n",
       "  100000100167,\n",
       "  100000100168,\n",
       "  100000100169,\n",
       "  100000100170,\n",
       "  100000100171,\n",
       "  100000100172,\n",
       "  100000100173,\n",
       "  100000100174,\n",
       "  100000100175,\n",
       "  100000100176,\n",
       "  100000100177,\n",
       "  100000100178,\n",
       "  100000100179,\n",
       "  100000100180,\n",
       "  100000100181,\n",
       "  100000100182,\n",
       "  100000100183,\n",
       "  100000100184,\n",
       "  100000100185,\n",
       "  100000100186,\n",
       "  100000100187,\n",
       "  100000100188,\n",
       "  100000100189,\n",
       "  100000100190,\n",
       "  100000100191,\n",
       "  100000100192,\n",
       "  100000100193,\n",
       "  100000100194,\n",
       "  100000100195,\n",
       "  100000100196,\n",
       "  100000100197,\n",
       "  100000100198,\n",
       "  100000100199,\n",
       "  100000100200,\n",
       "  100000100201,\n",
       "  100000100202,\n",
       "  100000100203,\n",
       "  100000100204,\n",
       "  100000100205,\n",
       "  100000100206,\n",
       "  100000100207,\n",
       "  100000100208,\n",
       "  100000100209,\n",
       "  100000100210,\n",
       "  100000100211,\n",
       "  100000100212,\n",
       "  100000100213,\n",
       "  100000100214,\n",
       "  100000100215,\n",
       "  100000100216,\n",
       "  100000100217,\n",
       "  100000100218,\n",
       "  100000100219,\n",
       "  100000100220,\n",
       "  100000100221,\n",
       "  100000100222,\n",
       "  100000100223,\n",
       "  100000100224,\n",
       "  100000100225,\n",
       "  100000100226,\n",
       "  100000100227,\n",
       "  100000100228,\n",
       "  100000100229,\n",
       "  100000100230,\n",
       "  100000100231,\n",
       "  100000100232,\n",
       "  100000100233,\n",
       "  100000100234,\n",
       "  100000100235,\n",
       "  100000100236,\n",
       "  100000100237,\n",
       "  100000100238,\n",
       "  100000100239,\n",
       "  100000100240,\n",
       "  100000100241,\n",
       "  100000100242,\n",
       "  100000100243,\n",
       "  100000100244,\n",
       "  100000100245,\n",
       "  100000100246,\n",
       "  100000100247,\n",
       "  100000100248,\n",
       "  100000100249,\n",
       "  100000100250,\n",
       "  100000100251,\n",
       "  100000100252,\n",
       "  100000100253,\n",
       "  100000100254,\n",
       "  100000100255,\n",
       "  100000100256,\n",
       "  100000100257,\n",
       "  100000100258,\n",
       "  100000100259,\n",
       "  100000100260,\n",
       "  100000100261,\n",
       "  100000100262,\n",
       "  100000100263,\n",
       "  100000100264,\n",
       "  100000100265,\n",
       "  100000100266,\n",
       "  100000100267,\n",
       "  100000100268,\n",
       "  100000100269,\n",
       "  100000100270,\n",
       "  100000100271,\n",
       "  100000100272,\n",
       "  100000100273,\n",
       "  100000100274,\n",
       "  100000100275,\n",
       "  100000100276,\n",
       "  100000100277,\n",
       "  100000100278,\n",
       "  100000100279,\n",
       "  100000100280,\n",
       "  100000100281,\n",
       "  100000100282,\n",
       "  100000100283,\n",
       "  100000100284,\n",
       "  100000100285,\n",
       "  100000100286,\n",
       "  100000100287,\n",
       "  100000100288,\n",
       "  100000100289,\n",
       "  100000100290,\n",
       "  100000100291,\n",
       "  100000100292,\n",
       "  100000100293,\n",
       "  100000100294,\n",
       "  100000100295,\n",
       "  100000100296,\n",
       "  100000100297,\n",
       "  100000100298,\n",
       "  100000100299,\n",
       "  100000100300,\n",
       "  100000100301,\n",
       "  100000100302,\n",
       "  100000100303,\n",
       "  100000100304,\n",
       "  100000100305,\n",
       "  100000100306,\n",
       "  100000100307,\n",
       "  100000100308,\n",
       "  100000100309,\n",
       "  100000100310,\n",
       "  100000100311,\n",
       "  100000100312,\n",
       "  100000100313,\n",
       "  100000100314,\n",
       "  100000100315,\n",
       "  100000100316,\n",
       "  100000100317,\n",
       "  100000100318,\n",
       "  100000100319,\n",
       "  100000100320,\n",
       "  100000100321,\n",
       "  100000100322,\n",
       "  100000100323,\n",
       "  100000100324,\n",
       "  100000100325,\n",
       "  100000100326,\n",
       "  100000100327,\n",
       "  100000100328,\n",
       "  100000100329,\n",
       "  100000100330,\n",
       "  100000100331,\n",
       "  100000100332,\n",
       "  100000100333,\n",
       "  100000100334,\n",
       "  100000100335,\n",
       "  100000100336,\n",
       "  100000100337,\n",
       "  100000100338,\n",
       "  100000100339,\n",
       "  100000100340,\n",
       "  100000100341,\n",
       "  100000100342,\n",
       "  100000100343,\n",
       "  100000100344,\n",
       "  100000100345,\n",
       "  100000100346,\n",
       "  100000100347,\n",
       "  100000100348,\n",
       "  100000100349,\n",
       "  100000100350,\n",
       "  100000100351,\n",
       "  100000100352,\n",
       "  100000100353,\n",
       "  100000100354,\n",
       "  100000100355,\n",
       "  100000100356,\n",
       "  100000100357,\n",
       "  100000100358,\n",
       "  100000100359,\n",
       "  100000100360,\n",
       "  100000100361,\n",
       "  100000100362,\n",
       "  100000100363,\n",
       "  100000100364,\n",
       "  100000100365,\n",
       "  100000100366,\n",
       "  100000100367,\n",
       "  100000100368,\n",
       "  100000100369,\n",
       "  100000100370,\n",
       "  100000100371,\n",
       "  100000100372,\n",
       "  100000100373,\n",
       "  100000100374,\n",
       "  100000100375,\n",
       "  100000100376,\n",
       "  100000100377,\n",
       "  100000100378,\n",
       "  100000100379,\n",
       "  100000100380,\n",
       "  100000100381,\n",
       "  100000100382,\n",
       "  100000100383,\n",
       "  100000100384,\n",
       "  100000100385,\n",
       "  100000100386,\n",
       "  100000100387,\n",
       "  100000100388,\n",
       "  100000100389,\n",
       "  100000100390,\n",
       "  100000100391,\n",
       "  100000100392,\n",
       "  100000100393,\n",
       "  100000100394,\n",
       "  100000100395,\n",
       "  100000100396,\n",
       "  100000100397,\n",
       "  100000100398,\n",
       "  100000100399,\n",
       "  100000100400,\n",
       "  100000100401,\n",
       "  100000100402,\n",
       "  100000100403,\n",
       "  100000100404,\n",
       "  100000100405,\n",
       "  100000100406,\n",
       "  100000100407,\n",
       "  100000100408,\n",
       "  100000100409,\n",
       "  100000100410,\n",
       "  100000100411,\n",
       "  100000100412,\n",
       "  100000100413,\n",
       "  100000100414,\n",
       "  100000100415,\n",
       "  100000100416,\n",
       "  100000100417,\n",
       "  100000100418,\n",
       "  100000100419,\n",
       "  100000100420,\n",
       "  100000100421,\n",
       "  100000100422,\n",
       "  100000100423,\n",
       "  100000100424,\n",
       "  100000100425,\n",
       "  100000100426,\n",
       "  100000100427,\n",
       "  100000100428,\n",
       "  100000100429,\n",
       "  100000100430,\n",
       "  100000100431,\n",
       "  100000100432,\n",
       "  100000100433,\n",
       "  100000100434,\n",
       "  100000100435,\n",
       "  100000100436,\n",
       "  100000100437,\n",
       "  100000100438,\n",
       "  100000100439,\n",
       "  100000100440,\n",
       "  100000100441,\n",
       "  100000100442,\n",
       "  100000100443,\n",
       "  100000100444,\n",
       "  100000100445,\n",
       "  100000100446,\n",
       "  100000100447,\n",
       "  100000100448,\n",
       "  100000100449,\n",
       "  100000100450,\n",
       "  100000100451,\n",
       "  100000100452,\n",
       "  100000100453,\n",
       "  100000100454,\n",
       "  100000100455,\n",
       "  100000100456,\n",
       "  100000100457,\n",
       "  100000100458,\n",
       "  100000100459,\n",
       "  100000100460,\n",
       "  100000100461,\n",
       "  100000100462,\n",
       "  100000100463,\n",
       "  100000100464,\n",
       "  100000100465,\n",
       "  100000100466,\n",
       "  100000100467,\n",
       "  100000100468,\n",
       "  100000100469,\n",
       "  100000100470,\n",
       "  100000100471,\n",
       "  100000100472,\n",
       "  100000100473,\n",
       "  100000100474,\n",
       "  100000100475,\n",
       "  100000100476,\n",
       "  100000100477,\n",
       "  100000100478,\n",
       "  100000100479,\n",
       "  100000100480,\n",
       "  100000100481,\n",
       "  100000100482,\n",
       "  100000100483,\n",
       "  100000100484,\n",
       "  100000100485,\n",
       "  100000100486,\n",
       "  100000100487,\n",
       "  100000100488,\n",
       "  100000100489,\n",
       "  100000100490,\n",
       "  100000100491,\n",
       "  100000100492,\n",
       "  100000100493,\n",
       "  100000100494,\n",
       "  100000100495,\n",
       "  100000100496,\n",
       "  100000100497,\n",
       "  100000100498,\n",
       "  100000100499,\n",
       "  100000100500,\n",
       "  100000100501,\n",
       "  100000100502,\n",
       "  100000100503,\n",
       "  100000100504,\n",
       "  100000100505,\n",
       "  100000100506,\n",
       "  100000100507,\n",
       "  100000100508,\n",
       "  100000100509,\n",
       "  100000100510,\n",
       "  100000100511,\n",
       "  100000100512,\n",
       "  100000100513,\n",
       "  100000100514,\n",
       "  100000100515,\n",
       "  100000100516,\n",
       "  100000100517,\n",
       "  100000100518,\n",
       "  100000100519,\n",
       "  100000100520,\n",
       "  100000100521,\n",
       "  100000100522,\n",
       "  100000100523,\n",
       "  100000100524,\n",
       "  100000100525,\n",
       "  100000100526,\n",
       "  100000100527,\n",
       "  100000100528,\n",
       "  100000100529,\n",
       "  100000100530,\n",
       "  100000100531,\n",
       "  100000100532,\n",
       "  100000100533,\n",
       "  100000100534,\n",
       "  100000100535,\n",
       "  100000100536,\n",
       "  100000100537,\n",
       "  100000100538,\n",
       "  100000100539,\n",
       "  100000100540,\n",
       "  100000100541,\n",
       "  100000100542,\n",
       "  100000100543,\n",
       "  100000100544,\n",
       "  100000100545,\n",
       "  100000100546,\n",
       "  100000100547,\n",
       "  100000100548,\n",
       "  100000100549,\n",
       "  100000100550,\n",
       "  100000100551,\n",
       "  100000100552,\n",
       "  100000100553,\n",
       "  100000100554,\n",
       "  100000100555,\n",
       "  100000100556,\n",
       "  100000100557,\n",
       "  100000100558,\n",
       "  100000100559,\n",
       "  100000100560,\n",
       "  100000100561,\n",
       "  100000100562,\n",
       "  100000100563,\n",
       "  100000100564,\n",
       "  100000100565,\n",
       "  100000100566,\n",
       "  100000100567,\n",
       "  100000100568,\n",
       "  100000100569,\n",
       "  100000100570,\n",
       "  100000100571,\n",
       "  100000100572,\n",
       "  100000100573,\n",
       "  100000100574,\n",
       "  100000100575,\n",
       "  100000100576,\n",
       "  100000100577,\n",
       "  100000100578,\n",
       "  100000100579,\n",
       "  100000100580,\n",
       "  100000100581,\n",
       "  100000100582,\n",
       "  100000100583,\n",
       "  100000100584,\n",
       "  100000100585,\n",
       "  100000100586,\n",
       "  100000100587,\n",
       "  100000100588,\n",
       "  100000100589,\n",
       "  100000100590,\n",
       "  100000100591,\n",
       "  100000100592,\n",
       "  100000100593,\n",
       "  100000100594,\n",
       "  100000100595,\n",
       "  100000100596,\n",
       "  100000100597,\n",
       "  100000100598,\n",
       "  100000100599,\n",
       "  100000100600,\n",
       "  100000100601,\n",
       "  100000100602,\n",
       "  100000100603,\n",
       "  100000100604,\n",
       "  100000100605,\n",
       "  100000100606,\n",
       "  100000100607,\n",
       "  100000100608,\n",
       "  100000100609,\n",
       "  100000100610,\n",
       "  100000100611,\n",
       "  100000100612,\n",
       "  100000100613,\n",
       "  100000100614,\n",
       "  100000100615,\n",
       "  100000100616,\n",
       "  100000100617,\n",
       "  100000100618,\n",
       "  100000100619,\n",
       "  100000100620,\n",
       "  100000100621,\n",
       "  100000100622,\n",
       "  100000100623,\n",
       "  100000100624,\n",
       "  100000100625,\n",
       "  100000100626,\n",
       "  100000100627,\n",
       "  100000100628,\n",
       "  100000100629,\n",
       "  100000100630,\n",
       "  100000100631,\n",
       "  100000100632,\n",
       "  100000100633,\n",
       "  100000100634,\n",
       "  100000100635,\n",
       "  100000100636,\n",
       "  100000100637,\n",
       "  100000100638,\n",
       "  100000100639,\n",
       "  100000100640,\n",
       "  100000100641,\n",
       "  100000100642,\n",
       "  100000100643,\n",
       "  100000100644,\n",
       "  100000100645,\n",
       "  100000100646,\n",
       "  100000100647,\n",
       "  100000100648,\n",
       "  100000100649,\n",
       "  100000100650,\n",
       "  100000100651,\n",
       "  100000100652,\n",
       "  100000100653,\n",
       "  100000100654,\n",
       "  100000100655,\n",
       "  100000100656,\n",
       "  100000100657,\n",
       "  100000100658,\n",
       "  100000100659,\n",
       "  100000100660,\n",
       "  100000100661,\n",
       "  100000100662,\n",
       "  100000100663,\n",
       "  100000100664,\n",
       "  100000100665,\n",
       "  100000100666,\n",
       "  100000100667,\n",
       "  100000100668,\n",
       "  100000100669,\n",
       "  100000100670,\n",
       "  100000100671,\n",
       "  100000100672,\n",
       "  100000100673,\n",
       "  100000100674,\n",
       "  100000100675,\n",
       "  100000100676,\n",
       "  100000100677,\n",
       "  100000100678,\n",
       "  100000100679,\n",
       "  100000100680,\n",
       "  100000100681,\n",
       "  100000100682,\n",
       "  100000100683,\n",
       "  100000100684,\n",
       "  100000100685,\n",
       "  100000100686,\n",
       "  100000100687,\n",
       "  100000100688,\n",
       "  100000100689,\n",
       "  100000100690,\n",
       "  100000100691,\n",
       "  100000100692,\n",
       "  100000100693,\n",
       "  100000100694,\n",
       "  100000100695,\n",
       "  100000100696,\n",
       "  100000100697,\n",
       "  100000100698,\n",
       "  100000100699,\n",
       "  100000100700,\n",
       "  100000100701,\n",
       "  100000100702,\n",
       "  100000100703,\n",
       "  100000100704,\n",
       "  100000100705,\n",
       "  100000100706,\n",
       "  100000100707,\n",
       "  100000100708,\n",
       "  100000100709,\n",
       "  100000100710,\n",
       "  100000100711,\n",
       "  100000100712,\n",
       "  100000100713,\n",
       "  100000100714,\n",
       "  100000100715,\n",
       "  100000100716,\n",
       "  100000100717,\n",
       "  100000100718,\n",
       "  100000100719,\n",
       "  100000100720,\n",
       "  100000100721,\n",
       "  100000100722,\n",
       "  100000100723,\n",
       "  100000100724,\n",
       "  100000100725,\n",
       "  100000100726,\n",
       "  100000100727,\n",
       "  100000100728,\n",
       "  100000100729,\n",
       "  100000100730,\n",
       "  100000100731,\n",
       "  100000100732,\n",
       "  100000100733,\n",
       "  100000100734,\n",
       "  100000100735,\n",
       "  100000100736,\n",
       "  100000100737,\n",
       "  100000100738,\n",
       "  100000100739,\n",
       "  100000100740,\n",
       "  100000100741,\n",
       "  100000100742,\n",
       "  100000100743,\n",
       "  100000100744,\n",
       "  100000100745,\n",
       "  100000100746,\n",
       "  100000100747,\n",
       "  100000100748,\n",
       "  100000100749,\n",
       "  100000100750,\n",
       "  100000100751,\n",
       "  100000100752,\n",
       "  100000100753,\n",
       "  100000100754,\n",
       "  100000100755,\n",
       "  100000100756,\n",
       "  100000100757,\n",
       "  100000100758,\n",
       "  100000100759,\n",
       "  100000100760,\n",
       "  100000100761,\n",
       "  100000100762,\n",
       "  100000100763,\n",
       "  100000100764,\n",
       "  100000100765,\n",
       "  100000100766,\n",
       "  100000100767,\n",
       "  100000100768,\n",
       "  100000100769,\n",
       "  100000100770,\n",
       "  100000100771,\n",
       "  100000100772,\n",
       "  100000100773,\n",
       "  100000100774,\n",
       "  100000100775,\n",
       "  100000100776,\n",
       "  100000100777,\n",
       "  100000100778,\n",
       "  100000100779,\n",
       "  100000100780,\n",
       "  100000100781,\n",
       "  100000100782,\n",
       "  100000100783,\n",
       "  100000100784,\n",
       "  100000100785,\n",
       "  100000100786,\n",
       "  100000100787,\n",
       "  100000100788,\n",
       "  100000100789,\n",
       "  100000100790,\n",
       "  100000100791,\n",
       "  100000100792,\n",
       "  100000100793,\n",
       "  100000100794,\n",
       "  100000100795,\n",
       "  100000100796,\n",
       "  100000100797,\n",
       "  100000100798,\n",
       "  100000100799,\n",
       "  100000100800,\n",
       "  100000100801,\n",
       "  100000100802,\n",
       "  100000100803,\n",
       "  100000100804,\n",
       "  100000100805,\n",
       "  100000100806,\n",
       "  100000100807,\n",
       "  100000100808,\n",
       "  100000100809,\n",
       "  100000100810,\n",
       "  100000100811,\n",
       "  100000100812,\n",
       "  100000100813,\n",
       "  100000100814,\n",
       "  100000100815,\n",
       "  100000100816,\n",
       "  100000100817,\n",
       "  100000100818,\n",
       "  100000100819,\n",
       "  100000100820,\n",
       "  100000100821,\n",
       "  100000100822,\n",
       "  100000100823,\n",
       "  100000100824,\n",
       "  100000100825,\n",
       "  100000100826,\n",
       "  100000100827,\n",
       "  100000100828,\n",
       "  100000100829,\n",
       "  100000100830,\n",
       "  100000100831,\n",
       "  100000100832,\n",
       "  100000100833,\n",
       "  100000100834,\n",
       "  100000100835,\n",
       "  100000100836,\n",
       "  100000100837,\n",
       "  100000100838,\n",
       "  100000100839,\n",
       "  100000100840,\n",
       "  100000100841,\n",
       "  100000100842,\n",
       "  100000100843,\n",
       "  100000100844,\n",
       "  100000100845,\n",
       "  100000100846,\n",
       "  100000100847,\n",
       "  100000100848,\n",
       "  100000100849,\n",
       "  100000100850,\n",
       "  100000100851,\n",
       "  100000100852,\n",
       "  100000100853,\n",
       "  100000100854,\n",
       "  100000100855,\n",
       "  100000100856,\n",
       "  100000100857,\n",
       "  100000100858,\n",
       "  100000100859,\n",
       "  100000100860,\n",
       "  100000100861,\n",
       "  100000100862,\n",
       "  100000100863,\n",
       "  100000100864,\n",
       "  100000100865,\n",
       "  100000100866,\n",
       "  100000100867,\n",
       "  100000100868,\n",
       "  100000100869,\n",
       "  100000100870,\n",
       "  100000100871,\n",
       "  100000100872,\n",
       "  100000100873,\n",
       "  100000100874,\n",
       "  100000100875,\n",
       "  100000100876,\n",
       "  100000100877,\n",
       "  100000100878,\n",
       "  100000100879,\n",
       "  100000100880,\n",
       "  100000100881,\n",
       "  100000100882,\n",
       "  100000100883,\n",
       "  100000100884,\n",
       "  100000100885,\n",
       "  100000100886,\n",
       "  100000100887,\n",
       "  100000100888,\n",
       "  100000100889,\n",
       "  100000100890,\n",
       "  100000100891,\n",
       "  100000100892,\n",
       "  100000100893,\n",
       "  100000100894,\n",
       "  100000100895,\n",
       "  100000100896,\n",
       "  100000100897,\n",
       "  100000100898,\n",
       "  100000100899,\n",
       "  100000100900,\n",
       "  100000100901,\n",
       "  100000100902,\n",
       "  100000100903,\n",
       "  100000100904,\n",
       "  100000100905,\n",
       "  100000100906,\n",
       "  100000100907,\n",
       "  100000100908,\n",
       "  100000100909,\n",
       "  100000100910,\n",
       "  100000100911,\n",
       "  100000100912,\n",
       "  100000100913,\n",
       "  100000100914,\n",
       "  100000100915,\n",
       "  100000100916,\n",
       "  100000100917,\n",
       "  100000100918,\n",
       "  100000100919,\n",
       "  100000100920,\n",
       "  100000100921,\n",
       "  100000100922,\n",
       "  100000100923,\n",
       "  100000100924,\n",
       "  100000100925,\n",
       "  100000100926,\n",
       "  100000100927,\n",
       "  100000100928,\n",
       "  100000100929,\n",
       "  100000100930,\n",
       "  100000100931,\n",
       "  100000100932,\n",
       "  100000100933,\n",
       "  100000100934,\n",
       "  100000100935,\n",
       "  100000100936,\n",
       "  100000100937,\n",
       "  100000100938,\n",
       "  100000100939,\n",
       "  100000100940,\n",
       "  100000100941,\n",
       "  100000100942,\n",
       "  100000100943,\n",
       "  100000100944,\n",
       "  100000100945,\n",
       "  100000100946,\n",
       "  100000100947,\n",
       "  100000100948,\n",
       "  100000100949,\n",
       "  100000100950,\n",
       "  100000100951,\n",
       "  100000100952,\n",
       "  100000100953,\n",
       "  100000100954,\n",
       "  100000100955,\n",
       "  100000100956,\n",
       "  100000100957,\n",
       "  100000100958,\n",
       "  100000100959,\n",
       "  100000100960,\n",
       "  100000100961,\n",
       "  100000100962,\n",
       "  100000100963,\n",
       "  100000100964,\n",
       "  100000100965,\n",
       "  100000100966,\n",
       "  100000100967,\n",
       "  100000100968,\n",
       "  100000100969,\n",
       "  100000100970,\n",
       "  100000100971,\n",
       "  100000100972,\n",
       "  100000100973,\n",
       "  100000100974,\n",
       "  100000100975,\n",
       "  100000100976,\n",
       "  100000100977,\n",
       "  100000100978,\n",
       "  100000100979,\n",
       "  100000100980,\n",
       "  100000100981,\n",
       "  100000100982,\n",
       "  100000100983,\n",
       "  100000100984,\n",
       "  100000100985,\n",
       "  100000100986,\n",
       "  100000100987,\n",
       "  100000100988,\n",
       "  100000100989,\n",
       "  100000100990,\n",
       "  100000100991,\n",
       "  100000100992,\n",
       "  100000100993,\n",
       "  100000100994,\n",
       "  100000100995,\n",
       "  100000100996,\n",
       "  100000100997,\n",
       "  100000100998,\n",
       "  100000100999,\n",
       "  ...],\n",
       " [100000100000,\n",
       "  100000100001,\n",
       "  100000100002,\n",
       "  100000100003,\n",
       "  100000100004,\n",
       "  100000100005,\n",
       "  100000100006,\n",
       "  100000100007,\n",
       "  100000100008,\n",
       "  100000100009,\n",
       "  100000100010,\n",
       "  100000100011,\n",
       "  100000100012,\n",
       "  100000100013,\n",
       "  100000100014,\n",
       "  100000100015,\n",
       "  100000100016,\n",
       "  100000100017,\n",
       "  100000100018,\n",
       "  100000100019,\n",
       "  100000100020,\n",
       "  100000100021,\n",
       "  100000100022,\n",
       "  100000100023,\n",
       "  100000100024,\n",
       "  100000100025,\n",
       "  100000100026,\n",
       "  100000100027,\n",
       "  100000100028,\n",
       "  100000100029,\n",
       "  100000100030,\n",
       "  100000100031,\n",
       "  100000100032,\n",
       "  100000100033,\n",
       "  100000100034,\n",
       "  100000100035,\n",
       "  100000100036,\n",
       "  100000100037,\n",
       "  100000100038,\n",
       "  100000100039,\n",
       "  100000100040,\n",
       "  100000100041,\n",
       "  100000100042,\n",
       "  100000100043,\n",
       "  100000100044,\n",
       "  100000100045,\n",
       "  100000100046,\n",
       "  100000100047,\n",
       "  100000100048,\n",
       "  100000100049,\n",
       "  100000100050,\n",
       "  100000100051,\n",
       "  100000100052,\n",
       "  100000100053,\n",
       "  100000100054,\n",
       "  100000100055,\n",
       "  100000100056,\n",
       "  100000100057,\n",
       "  100000100058,\n",
       "  100000100059,\n",
       "  100000100060,\n",
       "  100000100061,\n",
       "  100000100062,\n",
       "  100000100063,\n",
       "  100000100064,\n",
       "  100000100065,\n",
       "  100000100066,\n",
       "  100000100067,\n",
       "  100000100068,\n",
       "  100000100069,\n",
       "  100000100070,\n",
       "  100000100071,\n",
       "  100000100072,\n",
       "  100000100073,\n",
       "  100000100074,\n",
       "  100000100075,\n",
       "  100000100076,\n",
       "  100000100077,\n",
       "  100000100078,\n",
       "  100000100079,\n",
       "  100000100080,\n",
       "  100000100081,\n",
       "  100000100082,\n",
       "  100000100083,\n",
       "  100000100084,\n",
       "  100000100085,\n",
       "  100000100086,\n",
       "  100000100087,\n",
       "  100000100088,\n",
       "  100000100089,\n",
       "  100000100090,\n",
       "  100000100091,\n",
       "  100000100092,\n",
       "  100000100093,\n",
       "  100000100094,\n",
       "  100000100095,\n",
       "  100000100096,\n",
       "  100000100097,\n",
       "  100000100098,\n",
       "  100000100099,\n",
       "  100000100100,\n",
       "  100000100101,\n",
       "  100000100102,\n",
       "  100000100103,\n",
       "  100000100104,\n",
       "  100000100105,\n",
       "  100000100106,\n",
       "  100000100107,\n",
       "  100000100108,\n",
       "  100000100109,\n",
       "  100000100110,\n",
       "  100000100111,\n",
       "  100000100112,\n",
       "  100000100113,\n",
       "  100000100114,\n",
       "  100000100115,\n",
       "  100000100116,\n",
       "  100000100117,\n",
       "  100000100118,\n",
       "  100000100119,\n",
       "  100000100120,\n",
       "  100000100121,\n",
       "  100000100122,\n",
       "  100000100123,\n",
       "  100000100124,\n",
       "  100000100125,\n",
       "  100000100126,\n",
       "  100000100127,\n",
       "  100000100128,\n",
       "  100000100129,\n",
       "  100000100130,\n",
       "  100000100131,\n",
       "  100000100132,\n",
       "  100000100133,\n",
       "  100000100134,\n",
       "  100000100135,\n",
       "  100000100136,\n",
       "  100000100137,\n",
       "  100000100138,\n",
       "  100000100139,\n",
       "  100000100140,\n",
       "  100000100141,\n",
       "  100000100142,\n",
       "  100000100143,\n",
       "  100000100144,\n",
       "  100000100145,\n",
       "  100000100146,\n",
       "  100000100147,\n",
       "  100000100148,\n",
       "  100000100149,\n",
       "  100000100150,\n",
       "  100000100151,\n",
       "  100000100152,\n",
       "  100000100153,\n",
       "  100000100154,\n",
       "  100000100155,\n",
       "  100000100156,\n",
       "  100000100157,\n",
       "  100000100158,\n",
       "  100000100159,\n",
       "  100000100160,\n",
       "  100000100161,\n",
       "  100000100162,\n",
       "  100000100163,\n",
       "  100000100164,\n",
       "  100000100165,\n",
       "  100000100166,\n",
       "  100000100167,\n",
       "  100000100168,\n",
       "  100000100169,\n",
       "  100000100170,\n",
       "  100000100171,\n",
       "  100000100172,\n",
       "  100000100173,\n",
       "  100000100174,\n",
       "  100000100175,\n",
       "  100000100176,\n",
       "  100000100177,\n",
       "  100000100178,\n",
       "  100000100179,\n",
       "  100000100180,\n",
       "  100000100181,\n",
       "  100000100182,\n",
       "  100000100183,\n",
       "  100000100184,\n",
       "  100000100185,\n",
       "  100000100186,\n",
       "  100000100187,\n",
       "  100000100188,\n",
       "  100000100189,\n",
       "  100000100190,\n",
       "  100000100191,\n",
       "  100000100192,\n",
       "  100000100193,\n",
       "  100000100194,\n",
       "  100000100195,\n",
       "  100000100196,\n",
       "  100000100197,\n",
       "  100000100198,\n",
       "  100000100199,\n",
       "  100000100200,\n",
       "  100000100201,\n",
       "  100000100202,\n",
       "  100000100203,\n",
       "  100000100204,\n",
       "  100000100205,\n",
       "  100000100206,\n",
       "  100000100207,\n",
       "  100000100208,\n",
       "  100000100209,\n",
       "  100000100210,\n",
       "  100000100211,\n",
       "  100000100212,\n",
       "  100000100213,\n",
       "  100000100214,\n",
       "  100000100215,\n",
       "  100000100216,\n",
       "  100000100217,\n",
       "  100000100218,\n",
       "  100000100219,\n",
       "  100000100220,\n",
       "  100000100221,\n",
       "  100000100222,\n",
       "  100000100223,\n",
       "  100000100224,\n",
       "  100000100225,\n",
       "  100000100226,\n",
       "  100000100227,\n",
       "  100000100228,\n",
       "  100000100229,\n",
       "  100000100230,\n",
       "  100000100231,\n",
       "  100000100232,\n",
       "  100000100233,\n",
       "  100000100234,\n",
       "  100000100235,\n",
       "  100000100236,\n",
       "  100000100237,\n",
       "  100000100238,\n",
       "  100000100239,\n",
       "  100000100240,\n",
       "  100000100241,\n",
       "  100000100242,\n",
       "  100000100243,\n",
       "  100000100244,\n",
       "  100000100245,\n",
       "  100000100246,\n",
       "  100000100247,\n",
       "  100000100248,\n",
       "  100000100249,\n",
       "  100000100250,\n",
       "  100000100251,\n",
       "  100000100252,\n",
       "  100000100253,\n",
       "  100000100254,\n",
       "  100000100255,\n",
       "  100000100256,\n",
       "  100000100257,\n",
       "  100000100258,\n",
       "  100000100259,\n",
       "  100000100260,\n",
       "  100000100261,\n",
       "  100000100262,\n",
       "  100000100263,\n",
       "  100000100264,\n",
       "  100000100265,\n",
       "  100000100266,\n",
       "  100000100267,\n",
       "  100000100268,\n",
       "  100000100269,\n",
       "  100000100270,\n",
       "  100000100271,\n",
       "  100000100272,\n",
       "  100000100273,\n",
       "  100000100274,\n",
       "  100000100275,\n",
       "  100000100276,\n",
       "  100000100277,\n",
       "  100000100278,\n",
       "  100000100279,\n",
       "  100000100280,\n",
       "  100000100281,\n",
       "  100000100282,\n",
       "  100000100283,\n",
       "  100000100284,\n",
       "  100000100285,\n",
       "  100000100286,\n",
       "  100000100287,\n",
       "  100000100288,\n",
       "  100000100289,\n",
       "  100000100290,\n",
       "  100000100291,\n",
       "  100000100292,\n",
       "  100000100293,\n",
       "  100000100294,\n",
       "  100000100295,\n",
       "  100000100296,\n",
       "  100000100297,\n",
       "  100000100298,\n",
       "  100000100299,\n",
       "  100000100300,\n",
       "  100000100301,\n",
       "  100000100302,\n",
       "  100000100303,\n",
       "  100000100304,\n",
       "  100000100305,\n",
       "  100000100306,\n",
       "  100000100307,\n",
       "  100000100308,\n",
       "  100000100309,\n",
       "  100000100310,\n",
       "  100000100311,\n",
       "  100000100312,\n",
       "  100000100313,\n",
       "  100000100314,\n",
       "  100000100315,\n",
       "  100000100316,\n",
       "  100000100317,\n",
       "  100000100318,\n",
       "  100000100319,\n",
       "  100000100320,\n",
       "  100000100321,\n",
       "  100000100322,\n",
       "  100000100323,\n",
       "  100000100324,\n",
       "  100000100325,\n",
       "  100000100326,\n",
       "  100000100327,\n",
       "  100000100328,\n",
       "  100000100329,\n",
       "  100000100330,\n",
       "  100000100331,\n",
       "  100000100332,\n",
       "  100000100333,\n",
       "  100000100334,\n",
       "  100000100335,\n",
       "  100000100336,\n",
       "  100000100337,\n",
       "  100000100338,\n",
       "  100000100339,\n",
       "  100000100340,\n",
       "  100000100341,\n",
       "  100000100342,\n",
       "  100000100343,\n",
       "  100000100344,\n",
       "  100000100345,\n",
       "  100000100346,\n",
       "  100000100347,\n",
       "  100000100348,\n",
       "  100000100349,\n",
       "  100000100350,\n",
       "  100000100351,\n",
       "  100000100352,\n",
       "  100000100353,\n",
       "  100000100354,\n",
       "  100000100355,\n",
       "  100000100356,\n",
       "  100000100357,\n",
       "  100000100358,\n",
       "  100000100359,\n",
       "  100000100360,\n",
       "  100000100361,\n",
       "  100000100362,\n",
       "  100000100363,\n",
       "  100000100364,\n",
       "  100000100365,\n",
       "  100000100366,\n",
       "  100000100367,\n",
       "  100000100368,\n",
       "  100000100369,\n",
       "  100000100370,\n",
       "  100000100371,\n",
       "  100000100372,\n",
       "  100000100373,\n",
       "  100000100374,\n",
       "  100000100375,\n",
       "  100000100376,\n",
       "  100000100377,\n",
       "  100000100378,\n",
       "  100000100379,\n",
       "  100000100380,\n",
       "  100000100381,\n",
       "  100000100382,\n",
       "  100000100383,\n",
       "  100000100384,\n",
       "  100000100385,\n",
       "  100000100386,\n",
       "  100000100387,\n",
       "  100000100388,\n",
       "  100000100389,\n",
       "  100000100390,\n",
       "  100000100391,\n",
       "  100000100392,\n",
       "  100000100393,\n",
       "  100000100394,\n",
       "  100000100395,\n",
       "  100000100396,\n",
       "  100000100397,\n",
       "  100000100398,\n",
       "  100000100399,\n",
       "  100000100400,\n",
       "  100000100401,\n",
       "  100000100402,\n",
       "  100000100403,\n",
       "  100000100404,\n",
       "  100000100405,\n",
       "  100000100406,\n",
       "  100000100407,\n",
       "  100000100408,\n",
       "  100000100409,\n",
       "  100000100410,\n",
       "  100000100411,\n",
       "  100000100412,\n",
       "  100000100413,\n",
       "  100000100414,\n",
       "  100000100415,\n",
       "  100000100416,\n",
       "  100000100417,\n",
       "  100000100418,\n",
       "  100000100419,\n",
       "  100000100420,\n",
       "  100000100421,\n",
       "  100000100422,\n",
       "  100000100423,\n",
       "  100000100424,\n",
       "  100000100425,\n",
       "  100000100426,\n",
       "  100000100427,\n",
       "  100000100428,\n",
       "  100000100429,\n",
       "  100000100430,\n",
       "  100000100431,\n",
       "  100000100432,\n",
       "  100000100433,\n",
       "  100000100434,\n",
       "  100000100435,\n",
       "  100000100436,\n",
       "  100000100437,\n",
       "  100000100438,\n",
       "  100000100439,\n",
       "  100000100440,\n",
       "  100000100441,\n",
       "  100000100442,\n",
       "  100000100443,\n",
       "  100000100444,\n",
       "  100000100445,\n",
       "  100000100446,\n",
       "  100000100447,\n",
       "  100000100448,\n",
       "  100000100449,\n",
       "  100000100450,\n",
       "  100000100451,\n",
       "  100000100452,\n",
       "  100000100453,\n",
       "  100000100454,\n",
       "  100000100455,\n",
       "  100000100456,\n",
       "  100000100457,\n",
       "  100000100458,\n",
       "  100000100459,\n",
       "  100000100460,\n",
       "  100000100461,\n",
       "  100000100462,\n",
       "  100000100463,\n",
       "  100000100464,\n",
       "  100000100465,\n",
       "  100000100466,\n",
       "  100000100467,\n",
       "  100000100468,\n",
       "  100000100469,\n",
       "  100000100470,\n",
       "  100000100471,\n",
       "  100000100472,\n",
       "  100000100473,\n",
       "  100000100474,\n",
       "  100000100475,\n",
       "  100000100476,\n",
       "  100000100477,\n",
       "  100000100478,\n",
       "  100000100479,\n",
       "  100000100480,\n",
       "  100000100481,\n",
       "  100000100482,\n",
       "  100000100483,\n",
       "  100000100484,\n",
       "  100000100485,\n",
       "  100000100486,\n",
       "  100000100487,\n",
       "  100000100488,\n",
       "  100000100489,\n",
       "  100000100490,\n",
       "  100000100491,\n",
       "  100000100492,\n",
       "  100000100493,\n",
       "  100000100494,\n",
       "  100000100495,\n",
       "  100000100496,\n",
       "  100000100497,\n",
       "  100000100498,\n",
       "  100000100499,\n",
       "  100000100500,\n",
       "  100000100501,\n",
       "  100000100502,\n",
       "  100000100503,\n",
       "  100000100504,\n",
       "  100000100505,\n",
       "  100000100506,\n",
       "  100000100507,\n",
       "  100000100508,\n",
       "  100000100509,\n",
       "  100000100510,\n",
       "  100000100511,\n",
       "  100000100512,\n",
       "  100000100513,\n",
       "  100000100514,\n",
       "  100000100515,\n",
       "  100000100516,\n",
       "  100000100517,\n",
       "  100000100518,\n",
       "  100000100519,\n",
       "  100000100520,\n",
       "  100000100521,\n",
       "  100000100522,\n",
       "  100000100523,\n",
       "  100000100524,\n",
       "  100000100525,\n",
       "  100000100526,\n",
       "  100000100527,\n",
       "  100000100528,\n",
       "  100000100529,\n",
       "  100000100530,\n",
       "  100000100531,\n",
       "  100000100532,\n",
       "  100000100533,\n",
       "  100000100534,\n",
       "  100000100535,\n",
       "  100000100536,\n",
       "  100000100537,\n",
       "  100000100538,\n",
       "  100000100539,\n",
       "  100000100540,\n",
       "  100000100541,\n",
       "  100000100542,\n",
       "  100000100543,\n",
       "  100000100544,\n",
       "  100000100545,\n",
       "  100000100546,\n",
       "  100000100547,\n",
       "  100000100548,\n",
       "  100000100549,\n",
       "  100000100550,\n",
       "  100000100551,\n",
       "  100000100552,\n",
       "  100000100553,\n",
       "  100000100554,\n",
       "  100000100555,\n",
       "  100000100556,\n",
       "  100000100557,\n",
       "  100000100558,\n",
       "  100000100559,\n",
       "  100000100560,\n",
       "  100000100561,\n",
       "  100000100562,\n",
       "  100000100563,\n",
       "  100000100564,\n",
       "  100000100565,\n",
       "  100000100566,\n",
       "  100000100567,\n",
       "  100000100568,\n",
       "  100000100569,\n",
       "  100000100570,\n",
       "  100000100571,\n",
       "  100000100572,\n",
       "  100000100573,\n",
       "  100000100574,\n",
       "  100000100575,\n",
       "  100000100576,\n",
       "  100000100577,\n",
       "  100000100578,\n",
       "  100000100579,\n",
       "  100000100580,\n",
       "  100000100581,\n",
       "  100000100582,\n",
       "  100000100583,\n",
       "  100000100584,\n",
       "  100000100585,\n",
       "  100000100586,\n",
       "  100000100587,\n",
       "  100000100588,\n",
       "  100000100589,\n",
       "  100000100590,\n",
       "  100000100591,\n",
       "  100000100592,\n",
       "  100000100593,\n",
       "  100000100594,\n",
       "  100000100595,\n",
       "  100000100596,\n",
       "  100000100597,\n",
       "  100000100598,\n",
       "  100000100599,\n",
       "  100000100600,\n",
       "  100000100601,\n",
       "  100000100602,\n",
       "  100000100603,\n",
       "  100000100604,\n",
       "  100000100605,\n",
       "  100000100606,\n",
       "  100000100607,\n",
       "  100000100608,\n",
       "  100000100609,\n",
       "  100000100610,\n",
       "  100000100611,\n",
       "  100000100612,\n",
       "  100000100613,\n",
       "  100000100614,\n",
       "  100000100615,\n",
       "  100000100616,\n",
       "  100000100617,\n",
       "  100000100618,\n",
       "  100000100619,\n",
       "  100000100620,\n",
       "  100000100621,\n",
       "  100000100622,\n",
       "  100000100623,\n",
       "  100000100624,\n",
       "  100000100625,\n",
       "  100000100626,\n",
       "  100000100627,\n",
       "  100000100628,\n",
       "  100000100629,\n",
       "  100000100630,\n",
       "  100000100631,\n",
       "  100000100632,\n",
       "  100000100633,\n",
       "  100000100634,\n",
       "  100000100635,\n",
       "  100000100636,\n",
       "  100000100637,\n",
       "  100000100638,\n",
       "  100000100639,\n",
       "  100000100640,\n",
       "  100000100641,\n",
       "  100000100642,\n",
       "  100000100643,\n",
       "  100000100644,\n",
       "  100000100645,\n",
       "  100000100646,\n",
       "  100000100647,\n",
       "  100000100648,\n",
       "  100000100649,\n",
       "  100000100650,\n",
       "  100000100651,\n",
       "  100000100652,\n",
       "  100000100653,\n",
       "  100000100654,\n",
       "  100000100655,\n",
       "  100000100656,\n",
       "  100000100657,\n",
       "  100000100658,\n",
       "  100000100659,\n",
       "  100000100660,\n",
       "  100000100661,\n",
       "  100000100662,\n",
       "  100000100663,\n",
       "  100000100664,\n",
       "  100000100665,\n",
       "  100000100666,\n",
       "  100000100667,\n",
       "  100000100668,\n",
       "  100000100669,\n",
       "  100000100670,\n",
       "  100000100671,\n",
       "  100000100672,\n",
       "  100000100673,\n",
       "  100000100674,\n",
       "  100000100675,\n",
       "  100000100676,\n",
       "  100000100677,\n",
       "  100000100678,\n",
       "  100000100679,\n",
       "  100000100680,\n",
       "  100000100681,\n",
       "  100000100682,\n",
       "  100000100683,\n",
       "  100000100684,\n",
       "  100000100685,\n",
       "  100000100686,\n",
       "  100000100687,\n",
       "  100000100688,\n",
       "  100000100689,\n",
       "  100000100690,\n",
       "  100000100691,\n",
       "  100000100692,\n",
       "  100000100693,\n",
       "  100000100694,\n",
       "  100000100695,\n",
       "  100000100696,\n",
       "  100000100697,\n",
       "  100000100698,\n",
       "  100000100699,\n",
       "  100000100700,\n",
       "  100000100701,\n",
       "  100000100702,\n",
       "  100000100703,\n",
       "  100000100704,\n",
       "  100000100705,\n",
       "  100000100706,\n",
       "  100000100707,\n",
       "  100000100708,\n",
       "  100000100709,\n",
       "  100000100710,\n",
       "  100000100711,\n",
       "  100000100712,\n",
       "  100000100713,\n",
       "  100000100714,\n",
       "  100000100715,\n",
       "  100000100716,\n",
       "  100000100717,\n",
       "  100000100718,\n",
       "  100000100719,\n",
       "  100000100720,\n",
       "  100000100721,\n",
       "  100000100722,\n",
       "  100000100723,\n",
       "  100000100724,\n",
       "  100000100725,\n",
       "  100000100726,\n",
       "  100000100727,\n",
       "  100000100728,\n",
       "  100000100729,\n",
       "  100000100730,\n",
       "  100000100731,\n",
       "  100000100732,\n",
       "  100000100733,\n",
       "  100000100734,\n",
       "  100000100735,\n",
       "  100000100736,\n",
       "  100000100737,\n",
       "  100000100738,\n",
       "  100000100739,\n",
       "  100000100740,\n",
       "  100000100741,\n",
       "  100000100742,\n",
       "  100000100743,\n",
       "  100000100744,\n",
       "  100000100745,\n",
       "  100000100746,\n",
       "  100000100747,\n",
       "  100000100748,\n",
       "  100000100749,\n",
       "  100000100750,\n",
       "  100000100751,\n",
       "  100000100752,\n",
       "  100000100753,\n",
       "  100000100754,\n",
       "  100000100755,\n",
       "  100000100756,\n",
       "  100000100757,\n",
       "  100000100758,\n",
       "  100000100759,\n",
       "  100000100760,\n",
       "  100000100761,\n",
       "  100000100762,\n",
       "  100000100763,\n",
       "  100000100764,\n",
       "  100000100765,\n",
       "  100000100766,\n",
       "  100000100767,\n",
       "  100000100768,\n",
       "  100000100769,\n",
       "  100000100770,\n",
       "  100000100771,\n",
       "  100000100772,\n",
       "  100000100773,\n",
       "  100000100774,\n",
       "  100000100775,\n",
       "  100000100776,\n",
       "  100000100777,\n",
       "  100000100778,\n",
       "  100000100779,\n",
       "  100000100780,\n",
       "  100000100781,\n",
       "  100000100782,\n",
       "  100000100783,\n",
       "  100000100784,\n",
       "  100000100785,\n",
       "  100000100786,\n",
       "  100000100787,\n",
       "  100000100788,\n",
       "  100000100789,\n",
       "  100000100790,\n",
       "  100000100791,\n",
       "  100000100792,\n",
       "  100000100793,\n",
       "  100000100794,\n",
       "  100000100795,\n",
       "  100000100796,\n",
       "  100000100797,\n",
       "  100000100798,\n",
       "  100000100799,\n",
       "  100000100800,\n",
       "  100000100801,\n",
       "  100000100802,\n",
       "  100000100803,\n",
       "  100000100804,\n",
       "  100000100805,\n",
       "  100000100806,\n",
       "  100000100807,\n",
       "  100000100808,\n",
       "  100000100809,\n",
       "  100000100810,\n",
       "  100000100811,\n",
       "  100000100812,\n",
       "  100000100813,\n",
       "  100000100814,\n",
       "  100000100815,\n",
       "  100000100816,\n",
       "  100000100817,\n",
       "  100000100818,\n",
       "  100000100819,\n",
       "  100000100820,\n",
       "  100000100821,\n",
       "  100000100822,\n",
       "  100000100823,\n",
       "  100000100824,\n",
       "  100000100825,\n",
       "  100000100826,\n",
       "  100000100827,\n",
       "  100000100828,\n",
       "  100000100829,\n",
       "  100000100830,\n",
       "  100000100831,\n",
       "  100000100832,\n",
       "  100000100833,\n",
       "  100000100834,\n",
       "  100000100835,\n",
       "  100000100836,\n",
       "  100000100837,\n",
       "  100000100838,\n",
       "  100000100839,\n",
       "  100000100840,\n",
       "  100000100841,\n",
       "  100000100842,\n",
       "  100000100843,\n",
       "  100000100844,\n",
       "  100000100845,\n",
       "  100000100846,\n",
       "  100000100847,\n",
       "  100000100848,\n",
       "  100000100849,\n",
       "  100000100850,\n",
       "  100000100851,\n",
       "  100000100852,\n",
       "  100000100853,\n",
       "  100000100854,\n",
       "  100000100855,\n",
       "  100000100856,\n",
       "  100000100857,\n",
       "  100000100858,\n",
       "  100000100859,\n",
       "  100000100860,\n",
       "  100000100861,\n",
       "  100000100862,\n",
       "  100000100863,\n",
       "  100000100864,\n",
       "  100000100865,\n",
       "  100000100866,\n",
       "  100000100867,\n",
       "  100000100868,\n",
       "  100000100869,\n",
       "  100000100870,\n",
       "  100000100871,\n",
       "  100000100872,\n",
       "  100000100873,\n",
       "  100000100874,\n",
       "  100000100875,\n",
       "  100000100876,\n",
       "  100000100877,\n",
       "  100000100878,\n",
       "  100000100879,\n",
       "  100000100880,\n",
       "  100000100881,\n",
       "  100000100882,\n",
       "  100000100883,\n",
       "  100000100884,\n",
       "  100000100885,\n",
       "  100000100886,\n",
       "  100000100887,\n",
       "  100000100888,\n",
       "  100000100889,\n",
       "  100000100890,\n",
       "  100000100891,\n",
       "  100000100892,\n",
       "  100000100893,\n",
       "  100000100894,\n",
       "  100000100895,\n",
       "  100000100896,\n",
       "  100000100897,\n",
       "  100000100898,\n",
       "  100000100899,\n",
       "  100000100900,\n",
       "  100000100901,\n",
       "  100000100902,\n",
       "  100000100903,\n",
       "  100000100904,\n",
       "  100000100905,\n",
       "  100000100906,\n",
       "  100000100907,\n",
       "  100000100908,\n",
       "  100000100909,\n",
       "  100000100910,\n",
       "  100000100911,\n",
       "  100000100912,\n",
       "  100000100913,\n",
       "  100000100914,\n",
       "  100000100915,\n",
       "  100000100916,\n",
       "  100000100917,\n",
       "  100000100918,\n",
       "  100000100919,\n",
       "  100000100920,\n",
       "  100000100921,\n",
       "  100000100922,\n",
       "  100000100923,\n",
       "  100000100924,\n",
       "  100000100925,\n",
       "  100000100926,\n",
       "  100000100927,\n",
       "  100000100928,\n",
       "  100000100929,\n",
       "  100000100930,\n",
       "  100000100931,\n",
       "  100000100932,\n",
       "  100000100933,\n",
       "  100000100934,\n",
       "  100000100935,\n",
       "  100000100936,\n",
       "  100000100937,\n",
       "  100000100938,\n",
       "  100000100939,\n",
       "  100000100940,\n",
       "  100000100941,\n",
       "  100000100942,\n",
       "  100000100943,\n",
       "  100000100944,\n",
       "  100000100945,\n",
       "  100000100946,\n",
       "  100000100947,\n",
       "  100000100948,\n",
       "  100000100949,\n",
       "  100000100950,\n",
       "  100000100951,\n",
       "  100000100952,\n",
       "  100000100953,\n",
       "  100000100954,\n",
       "  100000100955,\n",
       "  100000100956,\n",
       "  100000100957,\n",
       "  100000100958,\n",
       "  100000100959,\n",
       "  100000100960,\n",
       "  100000100961,\n",
       "  100000100962,\n",
       "  100000100963,\n",
       "  100000100964,\n",
       "  100000100965,\n",
       "  100000100966,\n",
       "  100000100967,\n",
       "  100000100968,\n",
       "  100000100969,\n",
       "  100000100970,\n",
       "  100000100971,\n",
       "  100000100972,\n",
       "  100000100973,\n",
       "  100000100974,\n",
       "  100000100975,\n",
       "  100000100976,\n",
       "  100000100977,\n",
       "  100000100978,\n",
       "  100000100979,\n",
       "  100000100980,\n",
       "  100000100981,\n",
       "  100000100982,\n",
       "  100000100983,\n",
       "  100000100984,\n",
       "  100000100985,\n",
       "  100000100986,\n",
       "  100000100987,\n",
       "  100000100988,\n",
       "  100000100989,\n",
       "  100000100990,\n",
       "  100000100991,\n",
       "  100000100992,\n",
       "  100000100993,\n",
       "  100000100994,\n",
       "  100000100995,\n",
       "  100000100996,\n",
       "  100000100997,\n",
       "  100000100998,\n",
       "  100000100999,\n",
       "  ...]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cv_indices, val_cv_indices, test_indices = split_dataset(data_indices=my_data.all_IDs,\n",
    "                                                         validation_method=validation_method,\n",
    "                                                         n_splits=1,\n",
    "                                                         validation_ratio=config['val_ratio'],\n",
    "                                                         test_set_ratio=config['test_ratio'],  # used only if test_indices not explicitly specified\n",
    "                                                         test_indices=test_indices,\n",
    "                                                         random_seed=1337,\n",
    "                                                         labels=labels)\n",
    "train_cv_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26ad86f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_cv_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8a967d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100701001729"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cv_indices[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4003c828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11666"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_cv_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "142da4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:19:54,370 | INFO : 98814 \t samples may be used for training\n",
      "2023-05-24 10:19:54,371 | INFO : 11666 \t samples will be used for validation\n",
      "2023-05-24 10:19:54,371 | INFO : 0 \t samples will be used for testing\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"{} \\t samples may be used for training\".format(len(train_cv_indices[0])))\n",
    "logger.info(\"{} \\t samples will be used for validation\".format(len(val_cv_indices[0])))\n",
    "logger.info(\"{} \\t samples will be used for testing\".format(len(test_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a597c0b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:20:02,139 | INFO : Creating model ...\n",
      "2023-05-24 10:20:02,165 | INFO : Model:\n",
      "TSTransformerEncoderClassiregressor(\n",
      "  (project_inp): Linear(in_features=7, out_features=64, bias=True)\n",
      "  (pos_enc): LearnablePositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=3072, out_features=1, bias=True)\n",
      ")\n",
      "2023-05-24 10:20:02,166 | INFO : Total number of parameters: 156609\n",
      "2023-05-24 10:20:02,167 | INFO : Trainable parameters: 156609\n",
      "2023-05-24 10:20:06,863 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 0   0.0% | batch:         0 of        92\t|\tloss: 1172.57\n",
      "Evaluating Epoch 0   1.1% | batch:         1 of        92\t|\tloss: 749.243\n",
      "Evaluating Epoch 0   2.2% | batch:         2 of        92\t|\tloss: 923.352\n",
      "Evaluating Epoch 0   3.3% | batch:         3 of        92\t|\tloss: 789.848\n",
      "Evaluating Epoch 0   4.3% | batch:         4 of        92\t|\tloss: 223.932\n",
      "Evaluating Epoch 0   5.4% | batch:         5 of        92\t|\tloss: 790.005\n",
      "Evaluating Epoch 0   6.5% | batch:         6 of        92\t|\tloss: 1412.65\n",
      "Evaluating Epoch 0   7.6% | batch:         7 of        92\t|\tloss: 509.785\n",
      "Evaluating Epoch 0   8.7% | batch:         8 of        92\t|\tloss: 870.334\n",
      "Evaluating Epoch 0   9.8% | batch:         9 of        92\t|\tloss: 340.328\n",
      "Evaluating Epoch 0  10.9% | batch:        10 of        92\t|\tloss: 1285.42\n",
      "Evaluating Epoch 0  12.0% | batch:        11 of        92\t|\tloss: 363.03\n",
      "Evaluating Epoch 0  13.0% | batch:        12 of        92\t|\tloss: 640.968\n",
      "Evaluating Epoch 0  14.1% | batch:        13 of        92\t|\tloss: 802.746\n",
      "Evaluating Epoch 0  15.2% | batch:        14 of        92\t|\tloss: 1134.06\n",
      "Evaluating Epoch 0  16.3% | batch:        15 of        92\t|\tloss: 344.029\n",
      "Evaluating Epoch 0  17.4% | batch:        16 of        92\t|\tloss: 334.205\n",
      "Evaluating Epoch 0  18.5% | batch:        17 of        92\t|\tloss: 584.164\n",
      "Evaluating Epoch 0  19.6% | batch:        18 of        92\t|\tloss: 757.255\n",
      "Evaluating Epoch 0  20.7% | batch:        19 of        92\t|\tloss: 377.683\n",
      "Evaluating Epoch 0  21.7% | batch:        20 of        92\t|\tloss: 1410.21\n",
      "Evaluating Epoch 0  22.8% | batch:        21 of        92\t|\tloss: 366.546\n",
      "Evaluating Epoch 0  23.9% | batch:        22 of        92\t|\tloss: 923.558\n",
      "Evaluating Epoch 0  25.0% | batch:        23 of        92\t|\tloss: 840.544\n",
      "Evaluating Epoch 0  26.1% | batch:        24 of        92\t|\tloss: 1124.12\n",
      "Evaluating Epoch 0  27.2% | batch:        25 of        92\t|\tloss: 386.589\n",
      "Evaluating Epoch 0  28.3% | batch:        26 of        92\t|\tloss: 155.736\n",
      "Evaluating Epoch 0  29.3% | batch:        27 of        92\t|\tloss: 319.202\n",
      "Evaluating Epoch 0  30.4% | batch:        28 of        92\t|\tloss: 638.586\n",
      "Evaluating Epoch 0  31.5% | batch:        29 of        92\t|\tloss: 707.815\n",
      "Evaluating Epoch 0  32.6% | batch:        30 of        92\t|\tloss: 374.658\n",
      "Evaluating Epoch 0  33.7% | batch:        31 of        92\t|\tloss: 1386.07\n",
      "Evaluating Epoch 0  34.8% | batch:        32 of        92\t|\tloss: 287.994\n",
      "Evaluating Epoch 0  35.9% | batch:        33 of        92\t|\tloss: 1290.27\n",
      "Evaluating Epoch 0  37.0% | batch:        34 of        92\t|\tloss: 1800.93\n",
      "Evaluating Epoch 0  38.0% | batch:        35 of        92\t|\tloss: 720.793\n",
      "Evaluating Epoch 0  39.1% | batch:        36 of        92\t|\tloss: 508.48\n",
      "Evaluating Epoch 0  40.2% | batch:        37 of        92\t|\tloss: 448.206\n",
      "Evaluating Epoch 0  41.3% | batch:        38 of        92\t|\tloss: 882.89\n",
      "Evaluating Epoch 0  42.4% | batch:        39 of        92\t|\tloss: 306.912\n",
      "Evaluating Epoch 0  43.5% | batch:        40 of        92\t|\tloss: 1335.89\n",
      "Evaluating Epoch 0  44.6% | batch:        41 of        92\t|\tloss: 323.516\n",
      "Evaluating Epoch 0  45.7% | batch:        42 of        92\t|\tloss: 624.634\n",
      "Evaluating Epoch 0  46.7% | batch:        43 of        92\t|\tloss: 456.908\n",
      "Evaluating Epoch 0  47.8% | batch:        44 of        92\t|\tloss: 1482.7\n",
      "Evaluating Epoch 0  48.9% | batch:        45 of        92\t|\tloss: 549.885\n",
      "Evaluating Epoch 0  50.0% | batch:        46 of        92\t|\tloss: 535.602\n",
      "Evaluating Epoch 0  51.1% | batch:        47 of        92\t|\tloss: 881.976\n",
      "Evaluating Epoch 0  52.2% | batch:        48 of        92\t|\tloss: 422.037\n",
      "Evaluating Epoch 0  53.3% | batch:        49 of        92\t|\tloss: 1213.84\n",
      "Evaluating Epoch 0  54.3% | batch:        50 of        92\t|\tloss: 321.644\n",
      "Evaluating Epoch 0  55.4% | batch:        51 of        92\t|\tloss: 616.265\n",
      "Evaluating Epoch 0  56.5% | batch:        52 of        92\t|\tloss: 584.571\n",
      "Evaluating Epoch 0  57.6% | batch:        53 of        92\t|\tloss: 1085.38\n",
      "Evaluating Epoch 0  58.7% | batch:        54 of        92\t|\tloss: 596.185\n",
      "Evaluating Epoch 0  59.8% | batch:        55 of        92\t|\tloss: 707.719\n",
      "Evaluating Epoch 0  60.9% | batch:        56 of        92\t|\tloss: 372.083\n",
      "Evaluating Epoch 0  62.0% | batch:        57 of        92\t|\tloss: 1290.62\n",
      "Evaluating Epoch 0  63.0% | batch:        58 of        92\t|\tloss: 415.605\n",
      "Evaluating Epoch 0  64.1% | batch:        59 of        92\t|\tloss: 780.02\n",
      "Evaluating Epoch 0  65.2% | batch:        60 of        92\t|\tloss: 558.316\n",
      "Evaluating Epoch 0  66.3% | batch:        61 of        92\t|\tloss: 1187.1\n",
      "Evaluating Epoch 0  67.4% | batch:        62 of        92\t|\tloss: 508.629\n",
      "Evaluating Epoch 0  68.5% | batch:        63 of        92\t|\tloss: 173.854\n",
      "Evaluating Epoch 0  69.6% | batch:        64 of        92\t|\tloss: 347.748\n",
      "Evaluating Epoch 0  70.7% | batch:        65 of        92\t|\tloss: 643.727\n",
      "Evaluating Epoch 0  71.7% | batch:        66 of        92\t|\tloss: 777.716\n",
      "Evaluating Epoch 0  72.8% | batch:        67 of        92\t|\tloss: 495.312\n",
      "Evaluating Epoch 0  73.9% | batch:        68 of        92\t|\tloss: 1306.12\n",
      "Evaluating Epoch 0  75.0% | batch:        69 of        92\t|\tloss: 196.163\n",
      "Evaluating Epoch 0  76.1% | batch:        70 of        92\t|\tloss: 1640.94\n",
      "Evaluating Epoch 0  77.2% | batch:        71 of        92\t|\tloss: 1427.33\n",
      "Evaluating Epoch 0  78.3% | batch:        72 of        92\t|\tloss: 835.814\n",
      "Evaluating Epoch 0  79.3% | batch:        73 of        92\t|\tloss: 171.762\n",
      "Evaluating Epoch 0  80.4% | batch:        74 of        92\t|\tloss: 286.599\n",
      "Evaluating Epoch 0  81.5% | batch:        75 of        92\t|\tloss: 581.897\n",
      "Evaluating Epoch 0  82.6% | batch:        76 of        92\t|\tloss: 653.688\n",
      "Evaluating Epoch 0  83.7% | batch:        77 of        92\t|\tloss: 284.182\n",
      "Evaluating Epoch 0  84.8% | batch:        78 of        92\t|\tloss: 1315.41\n",
      "Evaluating Epoch 0  85.9% | batch:        79 of        92\t|\tloss: 244.386\n",
      "Evaluating Epoch 0  87.0% | batch:        80 of        92\t|\tloss: 1111.77\n",
      "Evaluating Epoch 0  88.0% | batch:        81 of        92\t|\tloss: 591.9\n",
      "Evaluating Epoch 0  89.1% | batch:        82 of        92\t|\tloss: 858.09\n",
      "Evaluating Epoch 0  90.2% | batch:        83 of        92\t|\tloss: 665.713\n",
      "Evaluating Epoch 0  91.3% | batch:        84 of        92\t|\tloss: 856.667\n",
      "Evaluating Epoch 0  92.4% | batch:        85 of        92\t|\tloss: 458.442\n",
      "Evaluating Epoch 0  93.5% | batch:        86 of        92\t|\tloss: 1283.71\n",
      "Evaluating Epoch 0  94.6% | batch:        87 of        92\t|\tloss: 310.822\n",
      "Evaluating Epoch 0  95.7% | batch:        88 of        92\t|\tloss: 748.807\n",
      "Evaluating Epoch 0  96.7% | batch:        89 of        92\t|\tloss: 655.274\n",
      "Evaluating Epoch 0  97.8% | batch:        90 of        92\t|\tloss: 1143.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:20:08,809 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.9455015659332275 seconds\n",
      "\n",
      "2023-05-24 10:20:08,809 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.9455015659332275 seconds\n",
      "2023-05-24 10:20:08,809 | INFO : Avg batch val. time: 0.021146756151448124 seconds\n",
      "2023-05-24 10:20:08,810 | INFO : Avg sample val. time: 0.00016676680661179732 seconds\n",
      "2023-05-24 10:20:08,810 | INFO : Epoch 0 Validation Summary: epoch: 0.000000 | loss: 728.205929 | \n",
      "/home/tianyi/anaconda3/envs/transformer/lib/python3.8/site-packages/numpy/lib/npyio.py:713: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val = np.asanyarray(val)\n",
      "2023-05-24 10:20:08,831 | INFO : Starting training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 0  98.9% | batch:        91 of        92\t|\tloss: 1172.99\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce76bb7e73aa405c92b8d37f7956a5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1   0.0% | batch:         0 of       772\t|\tloss: 193.589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianyi/Documents/mvts_transformer/src/optimizers.py:69: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1   0.1% | batch:         1 of       772\t|\tloss: 200.911\n",
      "Training Epoch 1   0.3% | batch:         2 of       772\t|\tloss: 197.089\n",
      "Training Epoch 1   0.4% | batch:         3 of       772\t|\tloss: 227.796\n",
      "Training Epoch 1   0.5% | batch:         4 of       772\t|\tloss: 217.266\n",
      "Training Epoch 1   0.6% | batch:         5 of       772\t|\tloss: 176.828\n",
      "Training Epoch 1   0.8% | batch:         6 of       772\t|\tloss: 204.284\n",
      "Training Epoch 1   0.9% | batch:         7 of       772\t|\tloss: 227.994\n",
      "Training Epoch 1   1.0% | batch:         8 of       772\t|\tloss: 175.258\n",
      "Training Epoch 1   1.2% | batch:         9 of       772\t|\tloss: 178.202\n",
      "Training Epoch 1   1.3% | batch:        10 of       772\t|\tloss: 174.115\n",
      "Training Epoch 1   1.4% | batch:        11 of       772\t|\tloss: 189.76\n",
      "Training Epoch 1   1.6% | batch:        12 of       772\t|\tloss: 230.296\n",
      "Training Epoch 1   1.7% | batch:        13 of       772\t|\tloss: 168.441\n",
      "Training Epoch 1   1.8% | batch:        14 of       772\t|\tloss: 174.99\n",
      "Training Epoch 1   1.9% | batch:        15 of       772\t|\tloss: 159.671\n",
      "Training Epoch 1   2.1% | batch:        16 of       772\t|\tloss: 170.184\n",
      "Training Epoch 1   2.2% | batch:        17 of       772\t|\tloss: 154.56\n",
      "Training Epoch 1   2.3% | batch:        18 of       772\t|\tloss: 142.684\n",
      "Training Epoch 1   2.5% | batch:        19 of       772\t|\tloss: 151.592\n",
      "Training Epoch 1   2.6% | batch:        20 of       772\t|\tloss: 143.411\n",
      "Training Epoch 1   2.7% | batch:        21 of       772\t|\tloss: 145.349\n",
      "Training Epoch 1   2.8% | batch:        22 of       772\t|\tloss: 118.045\n",
      "Training Epoch 1   3.0% | batch:        23 of       772\t|\tloss: 112.705\n",
      "Training Epoch 1   3.1% | batch:        24 of       772\t|\tloss: 131.915\n",
      "Training Epoch 1   3.2% | batch:        25 of       772\t|\tloss: 100.775\n",
      "Training Epoch 1   3.4% | batch:        26 of       772\t|\tloss: 110.632\n",
      "Training Epoch 1   3.5% | batch:        27 of       772\t|\tloss: 100.867\n",
      "Training Epoch 1   3.6% | batch:        28 of       772\t|\tloss: 102.511\n",
      "Training Epoch 1   3.8% | batch:        29 of       772\t|\tloss: 126.152\n",
      "Training Epoch 1   3.9% | batch:        30 of       772\t|\tloss: 97.6073\n",
      "Training Epoch 1   4.0% | batch:        31 of       772\t|\tloss: 100.418\n",
      "Training Epoch 1   4.1% | batch:        32 of       772\t|\tloss: 102.045\n",
      "Training Epoch 1   4.3% | batch:        33 of       772\t|\tloss: 87.5579\n",
      "Training Epoch 1   4.4% | batch:        34 of       772\t|\tloss: 71.5873\n",
      "Training Epoch 1   4.5% | batch:        35 of       772\t|\tloss: 73.6829\n",
      "Training Epoch 1   4.7% | batch:        36 of       772\t|\tloss: 65.274\n",
      "Training Epoch 1   4.8% | batch:        37 of       772\t|\tloss: 67.3783\n",
      "Training Epoch 1   4.9% | batch:        38 of       772\t|\tloss: 82.8349\n",
      "Training Epoch 1   5.1% | batch:        39 of       772\t|\tloss: 58.2997\n",
      "Training Epoch 1   5.2% | batch:        40 of       772\t|\tloss: 64.0388\n",
      "Training Epoch 1   5.3% | batch:        41 of       772\t|\tloss: 65.334\n",
      "Training Epoch 1   5.4% | batch:        42 of       772\t|\tloss: 71.9092\n",
      "Training Epoch 1   5.6% | batch:        43 of       772\t|\tloss: 71.2962\n",
      "Training Epoch 1   5.7% | batch:        44 of       772\t|\tloss: 55.7297\n",
      "Training Epoch 1   5.8% | batch:        45 of       772\t|\tloss: 50.7324\n",
      "Training Epoch 1   6.0% | batch:        46 of       772\t|\tloss: 64.5912\n",
      "Training Epoch 1   6.1% | batch:        47 of       772\t|\tloss: 79.3922\n",
      "Training Epoch 1   6.2% | batch:        48 of       772\t|\tloss: 51.0475\n",
      "Training Epoch 1   6.3% | batch:        49 of       772\t|\tloss: 52.3391\n",
      "Training Epoch 1   6.5% | batch:        50 of       772\t|\tloss: 54.7116\n",
      "Training Epoch 1   6.6% | batch:        51 of       772\t|\tloss: 52.1776\n",
      "Training Epoch 1   6.7% | batch:        52 of       772\t|\tloss: 57.334\n",
      "Training Epoch 1   6.9% | batch:        53 of       772\t|\tloss: 58.8999\n",
      "Training Epoch 1   7.0% | batch:        54 of       772\t|\tloss: 53.4751\n",
      "Training Epoch 1   7.1% | batch:        55 of       772\t|\tloss: 63.9647\n",
      "Training Epoch 1   7.3% | batch:        56 of       772\t|\tloss: 48.2887\n",
      "Training Epoch 1   7.4% | batch:        57 of       772\t|\tloss: 55.8095\n",
      "Training Epoch 1   7.5% | batch:        58 of       772\t|\tloss: 53.5089\n",
      "Training Epoch 1   7.6% | batch:        59 of       772\t|\tloss: 51.3346\n",
      "Training Epoch 1   7.8% | batch:        60 of       772\t|\tloss: 59.3855\n",
      "Training Epoch 1   7.9% | batch:        61 of       772\t|\tloss: 43.4537\n",
      "Training Epoch 1   8.0% | batch:        62 of       772\t|\tloss: 48.9048\n",
      "Training Epoch 1   8.2% | batch:        63 of       772\t|\tloss: 47.8496\n",
      "Training Epoch 1   8.3% | batch:        64 of       772\t|\tloss: 47.5064\n",
      "Training Epoch 1   8.4% | batch:        65 of       772\t|\tloss: 45.9605\n",
      "Training Epoch 1   8.5% | batch:        66 of       772\t|\tloss: 42.6244\n",
      "Training Epoch 1   8.7% | batch:        67 of       772\t|\tloss: 40.6512\n",
      "Training Epoch 1   8.8% | batch:        68 of       772\t|\tloss: 59.8611\n",
      "Training Epoch 1   8.9% | batch:        69 of       772\t|\tloss: 46.1855\n",
      "Training Epoch 1   9.1% | batch:        70 of       772\t|\tloss: 35.4748\n",
      "Training Epoch 1   9.2% | batch:        71 of       772\t|\tloss: 45.9992\n",
      "Training Epoch 1   9.3% | batch:        72 of       772\t|\tloss: 46.3079\n",
      "Training Epoch 1   9.5% | batch:        73 of       772\t|\tloss: 45.4038\n",
      "Training Epoch 1   9.6% | batch:        74 of       772\t|\tloss: 55.3513\n",
      "Training Epoch 1   9.7% | batch:        75 of       772\t|\tloss: 42.1423\n",
      "Training Epoch 1   9.8% | batch:        76 of       772\t|\tloss: 42.8887\n",
      "Training Epoch 1  10.0% | batch:        77 of       772\t|\tloss: 38.5018\n",
      "Training Epoch 1  10.1% | batch:        78 of       772\t|\tloss: 45.6668\n",
      "Training Epoch 1  10.2% | batch:        79 of       772\t|\tloss: 40.9727\n",
      "Training Epoch 1  10.4% | batch:        80 of       772\t|\tloss: 51.3667\n",
      "Training Epoch 1  10.5% | batch:        81 of       772\t|\tloss: 38.8319\n",
      "Training Epoch 1  10.6% | batch:        82 of       772\t|\tloss: 47.4053\n",
      "Training Epoch 1  10.8% | batch:        83 of       772\t|\tloss: 61.0453\n",
      "Training Epoch 1  10.9% | batch:        84 of       772\t|\tloss: 48.7689\n",
      "Training Epoch 1  11.0% | batch:        85 of       772\t|\tloss: 39.3928\n",
      "Training Epoch 1  11.1% | batch:        86 of       772\t|\tloss: 39.0263\n",
      "Training Epoch 1  11.3% | batch:        87 of       772\t|\tloss: 39.5475\n",
      "Training Epoch 1  11.4% | batch:        88 of       772\t|\tloss: 49.1596\n",
      "Training Epoch 1  11.5% | batch:        89 of       772\t|\tloss: 43.9792\n",
      "Training Epoch 1  11.7% | batch:        90 of       772\t|\tloss: 53.21\n",
      "Training Epoch 1  11.8% | batch:        91 of       772\t|\tloss: 45.7156\n",
      "Training Epoch 1  11.9% | batch:        92 of       772\t|\tloss: 48.5268\n",
      "Training Epoch 1  12.0% | batch:        93 of       772\t|\tloss: 39.6715\n",
      "Training Epoch 1  12.2% | batch:        94 of       772\t|\tloss: 57.1532\n",
      "Training Epoch 1  12.3% | batch:        95 of       772\t|\tloss: 53.8302\n",
      "Training Epoch 1  12.4% | batch:        96 of       772\t|\tloss: 50.8533\n",
      "Training Epoch 1  12.6% | batch:        97 of       772\t|\tloss: 42.5193\n",
      "Training Epoch 1  12.7% | batch:        98 of       772\t|\tloss: 44.6762\n",
      "Training Epoch 1  12.8% | batch:        99 of       772\t|\tloss: 48.0965\n",
      "Training Epoch 1  13.0% | batch:       100 of       772\t|\tloss: 38.4837\n",
      "Training Epoch 1  13.1% | batch:       101 of       772\t|\tloss: 39.0095\n",
      "Training Epoch 1  13.2% | batch:       102 of       772\t|\tloss: 46.7426\n",
      "Training Epoch 1  13.3% | batch:       103 of       772\t|\tloss: 43.5427\n",
      "Training Epoch 1  13.5% | batch:       104 of       772\t|\tloss: 63.4437\n",
      "Training Epoch 1  13.6% | batch:       105 of       772\t|\tloss: 38.1279\n",
      "Training Epoch 1  13.7% | batch:       106 of       772\t|\tloss: 50.2542\n",
      "Training Epoch 1  13.9% | batch:       107 of       772\t|\tloss: 40.9459\n",
      "Training Epoch 1  14.0% | batch:       108 of       772\t|\tloss: 54.7235\n",
      "Training Epoch 1  14.1% | batch:       109 of       772\t|\tloss: 50.419\n",
      "Training Epoch 1  14.2% | batch:       110 of       772\t|\tloss: 37.2494\n",
      "Training Epoch 1  14.4% | batch:       111 of       772\t|\tloss: 45.2444\n",
      "Training Epoch 1  14.5% | batch:       112 of       772\t|\tloss: 46.6458\n",
      "Training Epoch 1  14.6% | batch:       113 of       772\t|\tloss: 40.8788\n",
      "Training Epoch 1  14.8% | batch:       114 of       772\t|\tloss: 45.3216\n",
      "Training Epoch 1  14.9% | batch:       115 of       772\t|\tloss: 47.4384\n",
      "Training Epoch 1  15.0% | batch:       116 of       772\t|\tloss: 52.8916\n",
      "Training Epoch 1  15.2% | batch:       117 of       772\t|\tloss: 47.9182\n",
      "Training Epoch 1  15.3% | batch:       118 of       772\t|\tloss: 46.651\n",
      "Training Epoch 1  15.4% | batch:       119 of       772\t|\tloss: 49.3015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  15.5% | batch:       120 of       772\t|\tloss: 42.1447\n",
      "Training Epoch 1  15.7% | batch:       121 of       772\t|\tloss: 49.817\n",
      "Training Epoch 1  15.8% | batch:       122 of       772\t|\tloss: 43.3705\n",
      "Training Epoch 1  15.9% | batch:       123 of       772\t|\tloss: 42.3788\n",
      "Training Epoch 1  16.1% | batch:       124 of       772\t|\tloss: 39.2976\n",
      "Training Epoch 1  16.2% | batch:       125 of       772\t|\tloss: 39.4579\n",
      "Training Epoch 1  16.3% | batch:       126 of       772\t|\tloss: 38.6217\n",
      "Training Epoch 1  16.5% | batch:       127 of       772\t|\tloss: 40.0037\n",
      "Training Epoch 1  16.6% | batch:       128 of       772\t|\tloss: 50.7317\n",
      "Training Epoch 1  16.7% | batch:       129 of       772\t|\tloss: 43.6548\n",
      "Training Epoch 1  16.8% | batch:       130 of       772\t|\tloss: 37.1352\n",
      "Training Epoch 1  17.0% | batch:       131 of       772\t|\tloss: 33.8572\n",
      "Training Epoch 1  17.1% | batch:       132 of       772\t|\tloss: 46.1646\n",
      "Training Epoch 1  17.2% | batch:       133 of       772\t|\tloss: 38.0764\n",
      "Training Epoch 1  17.4% | batch:       134 of       772\t|\tloss: 33.2496\n",
      "Training Epoch 1  17.5% | batch:       135 of       772\t|\tloss: 41.0943\n",
      "Training Epoch 1  17.6% | batch:       136 of       772\t|\tloss: 37.528\n",
      "Training Epoch 1  17.7% | batch:       137 of       772\t|\tloss: 57.5921\n",
      "Training Epoch 1  17.9% | batch:       138 of       772\t|\tloss: 41.764\n",
      "Training Epoch 1  18.0% | batch:       139 of       772\t|\tloss: 51.8838\n",
      "Training Epoch 1  18.1% | batch:       140 of       772\t|\tloss: 37.8885\n",
      "Training Epoch 1  18.3% | batch:       141 of       772\t|\tloss: 42.3058\n",
      "Training Epoch 1  18.4% | batch:       142 of       772\t|\tloss: 41.4571\n",
      "Training Epoch 1  18.5% | batch:       143 of       772\t|\tloss: 50.2063\n",
      "Training Epoch 1  18.7% | batch:       144 of       772\t|\tloss: 37.748\n",
      "Training Epoch 1  18.8% | batch:       145 of       772\t|\tloss: 41.6503\n",
      "Training Epoch 1  18.9% | batch:       146 of       772\t|\tloss: 38.6235\n",
      "Training Epoch 1  19.0% | batch:       147 of       772\t|\tloss: 37.5576\n",
      "Training Epoch 1  19.2% | batch:       148 of       772\t|\tloss: 53.3604\n",
      "Training Epoch 1  19.3% | batch:       149 of       772\t|\tloss: 42.4496\n",
      "Training Epoch 1  19.4% | batch:       150 of       772\t|\tloss: 36.5083\n",
      "Training Epoch 1  19.6% | batch:       151 of       772\t|\tloss: 36.269\n",
      "Training Epoch 1  19.7% | batch:       152 of       772\t|\tloss: 49.3861\n",
      "Training Epoch 1  19.8% | batch:       153 of       772\t|\tloss: 43.986\n",
      "Training Epoch 1  19.9% | batch:       154 of       772\t|\tloss: 44.4303\n",
      "Training Epoch 1  20.1% | batch:       155 of       772\t|\tloss: 37.5491\n",
      "Training Epoch 1  20.2% | batch:       156 of       772\t|\tloss: 42.0779\n",
      "Training Epoch 1  20.3% | batch:       157 of       772\t|\tloss: 40.041\n",
      "Training Epoch 1  20.5% | batch:       158 of       772\t|\tloss: 54.0613\n",
      "Training Epoch 1  20.6% | batch:       159 of       772\t|\tloss: 44.0152\n",
      "Training Epoch 1  20.7% | batch:       160 of       772\t|\tloss: 49.6479\n",
      "Training Epoch 1  20.9% | batch:       161 of       772\t|\tloss: 44.1483\n",
      "Training Epoch 1  21.0% | batch:       162 of       772\t|\tloss: 38.405\n",
      "Training Epoch 1  21.1% | batch:       163 of       772\t|\tloss: 35.8536\n",
      "Training Epoch 1  21.2% | batch:       164 of       772\t|\tloss: 37.3083\n",
      "Training Epoch 1  21.4% | batch:       165 of       772\t|\tloss: 38.6032\n",
      "Training Epoch 1  21.5% | batch:       166 of       772\t|\tloss: 39.9253\n",
      "Training Epoch 1  21.6% | batch:       167 of       772\t|\tloss: 49.7891\n",
      "Training Epoch 1  21.8% | batch:       168 of       772\t|\tloss: 39.6824\n",
      "Training Epoch 1  21.9% | batch:       169 of       772\t|\tloss: 36.3065\n",
      "Training Epoch 1  22.0% | batch:       170 of       772\t|\tloss: 40.5131\n",
      "Training Epoch 1  22.2% | batch:       171 of       772\t|\tloss: 42.3522\n",
      "Training Epoch 1  22.3% | batch:       172 of       772\t|\tloss: 42.9981\n",
      "Training Epoch 1  22.4% | batch:       173 of       772\t|\tloss: 34.026\n",
      "Training Epoch 1  22.5% | batch:       174 of       772\t|\tloss: 44.9281\n",
      "Training Epoch 1  22.7% | batch:       175 of       772\t|\tloss: 40.6008\n",
      "Training Epoch 1  22.8% | batch:       176 of       772\t|\tloss: 43.9676\n",
      "Training Epoch 1  22.9% | batch:       177 of       772\t|\tloss: 38.8254\n",
      "Training Epoch 1  23.1% | batch:       178 of       772\t|\tloss: 45.6734\n",
      "Training Epoch 1  23.2% | batch:       179 of       772\t|\tloss: 32.2254\n",
      "Training Epoch 1  23.3% | batch:       180 of       772\t|\tloss: 44.4144\n",
      "Training Epoch 1  23.4% | batch:       181 of       772\t|\tloss: 38.5617\n",
      "Training Epoch 1  23.6% | batch:       182 of       772\t|\tloss: 37.7641\n",
      "Training Epoch 1  23.7% | batch:       183 of       772\t|\tloss: 35.3253\n",
      "Training Epoch 1  23.8% | batch:       184 of       772\t|\tloss: 32.8845\n",
      "Training Epoch 1  24.0% | batch:       185 of       772\t|\tloss: 39.0913\n",
      "Training Epoch 1  24.1% | batch:       186 of       772\t|\tloss: 39.5364\n",
      "Training Epoch 1  24.2% | batch:       187 of       772\t|\tloss: 39.4194\n",
      "Training Epoch 1  24.4% | batch:       188 of       772\t|\tloss: 33.959\n",
      "Training Epoch 1  24.5% | batch:       189 of       772\t|\tloss: 40.6253\n",
      "Training Epoch 1  24.6% | batch:       190 of       772\t|\tloss: 37.5299\n",
      "Training Epoch 1  24.7% | batch:       191 of       772\t|\tloss: 41.0457\n",
      "Training Epoch 1  24.9% | batch:       192 of       772\t|\tloss: 39.6002\n",
      "Training Epoch 1  25.0% | batch:       193 of       772\t|\tloss: 41.0328\n",
      "Training Epoch 1  25.1% | batch:       194 of       772\t|\tloss: 40.7238\n",
      "Training Epoch 1  25.3% | batch:       195 of       772\t|\tloss: 42.7343\n",
      "Training Epoch 1  25.4% | batch:       196 of       772\t|\tloss: 39.0754\n",
      "Training Epoch 1  25.5% | batch:       197 of       772\t|\tloss: 33.493\n",
      "Training Epoch 1  25.6% | batch:       198 of       772\t|\tloss: 42.7738\n",
      "Training Epoch 1  25.8% | batch:       199 of       772\t|\tloss: 47.3245\n",
      "Training Epoch 1  25.9% | batch:       200 of       772\t|\tloss: 43.7007\n",
      "Training Epoch 1  26.0% | batch:       201 of       772\t|\tloss: 34.0507\n",
      "Training Epoch 1  26.2% | batch:       202 of       772\t|\tloss: 40.7537\n",
      "Training Epoch 1  26.3% | batch:       203 of       772\t|\tloss: 37.6969\n",
      "Training Epoch 1  26.4% | batch:       204 of       772\t|\tloss: 41.134\n",
      "Training Epoch 1  26.6% | batch:       205 of       772\t|\tloss: 39.8215\n",
      "Training Epoch 1  26.7% | batch:       206 of       772\t|\tloss: 43.2428\n",
      "Training Epoch 1  26.8% | batch:       207 of       772\t|\tloss: 33.8962\n",
      "Training Epoch 1  26.9% | batch:       208 of       772\t|\tloss: 42.8035\n",
      "Training Epoch 1  27.1% | batch:       209 of       772\t|\tloss: 44.6962\n",
      "Training Epoch 1  27.2% | batch:       210 of       772\t|\tloss: 35.4416\n",
      "Training Epoch 1  27.3% | batch:       211 of       772\t|\tloss: 37.3263\n",
      "Training Epoch 1  27.5% | batch:       212 of       772\t|\tloss: 39.3329\n",
      "Training Epoch 1  27.6% | batch:       213 of       772\t|\tloss: 39.3534\n",
      "Training Epoch 1  27.7% | batch:       214 of       772\t|\tloss: 36.6006\n",
      "Training Epoch 1  27.8% | batch:       215 of       772\t|\tloss: 41.5682\n",
      "Training Epoch 1  28.0% | batch:       216 of       772\t|\tloss: 41.6955\n",
      "Training Epoch 1  28.1% | batch:       217 of       772\t|\tloss: 36.0991\n",
      "Training Epoch 1  28.2% | batch:       218 of       772\t|\tloss: 39.8427\n",
      "Training Epoch 1  28.4% | batch:       219 of       772\t|\tloss: 35.2873\n",
      "Training Epoch 1  28.5% | batch:       220 of       772\t|\tloss: 40.3784\n",
      "Training Epoch 1  28.6% | batch:       221 of       772\t|\tloss: 41.7067\n",
      "Training Epoch 1  28.8% | batch:       222 of       772\t|\tloss: 41.7405\n",
      "Training Epoch 1  28.9% | batch:       223 of       772\t|\tloss: 38.2368\n",
      "Training Epoch 1  29.0% | batch:       224 of       772\t|\tloss: 35.2739\n",
      "Training Epoch 1  29.1% | batch:       225 of       772\t|\tloss: 37.4014\n",
      "Training Epoch 1  29.3% | batch:       226 of       772\t|\tloss: 38.7903\n",
      "Training Epoch 1  29.4% | batch:       227 of       772\t|\tloss: 33.5121\n",
      "Training Epoch 1  29.5% | batch:       228 of       772\t|\tloss: 36.7704\n",
      "Training Epoch 1  29.7% | batch:       229 of       772\t|\tloss: 47.3306\n",
      "Training Epoch 1  29.8% | batch:       230 of       772\t|\tloss: 33.9605\n",
      "Training Epoch 1  29.9% | batch:       231 of       772\t|\tloss: 37.301\n",
      "Training Epoch 1  30.1% | batch:       232 of       772\t|\tloss: 38.6971\n",
      "Training Epoch 1  30.2% | batch:       233 of       772\t|\tloss: 29.0047\n",
      "Training Epoch 1  30.3% | batch:       234 of       772\t|\tloss: 30.3938\n",
      "Training Epoch 1  30.4% | batch:       235 of       772\t|\tloss: 33.5339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  30.6% | batch:       236 of       772\t|\tloss: 35.6032\n",
      "Training Epoch 1  30.7% | batch:       237 of       772\t|\tloss: 28.982\n",
      "Training Epoch 1  30.8% | batch:       238 of       772\t|\tloss: 37.1709\n",
      "Training Epoch 1  31.0% | batch:       239 of       772\t|\tloss: 36.4875\n",
      "Training Epoch 1  31.1% | batch:       240 of       772\t|\tloss: 31.2111\n",
      "Training Epoch 1  31.2% | batch:       241 of       772\t|\tloss: 36.7247\n",
      "Training Epoch 1  31.3% | batch:       242 of       772\t|\tloss: 43.3048\n",
      "Training Epoch 1  31.5% | batch:       243 of       772\t|\tloss: 38.0945\n",
      "Training Epoch 1  31.6% | batch:       244 of       772\t|\tloss: 39.5878\n",
      "Training Epoch 1  31.7% | batch:       245 of       772\t|\tloss: 42.4417\n",
      "Training Epoch 1  31.9% | batch:       246 of       772\t|\tloss: 41.0269\n",
      "Training Epoch 1  32.0% | batch:       247 of       772\t|\tloss: 32.9448\n",
      "Training Epoch 1  32.1% | batch:       248 of       772\t|\tloss: 33.3454\n",
      "Training Epoch 1  32.3% | batch:       249 of       772\t|\tloss: 31.9398\n",
      "Training Epoch 1  32.4% | batch:       250 of       772\t|\tloss: 34.5088\n",
      "Training Epoch 1  32.5% | batch:       251 of       772\t|\tloss: 31.5311\n",
      "Training Epoch 1  32.6% | batch:       252 of       772\t|\tloss: 24.8263\n",
      "Training Epoch 1  32.8% | batch:       253 of       772\t|\tloss: 38.4429\n",
      "Training Epoch 1  32.9% | batch:       254 of       772\t|\tloss: 35.0371\n",
      "Training Epoch 1  33.0% | batch:       255 of       772\t|\tloss: 37.9744\n",
      "Training Epoch 1  33.2% | batch:       256 of       772\t|\tloss: 36.8713\n",
      "Training Epoch 1  33.3% | batch:       257 of       772\t|\tloss: 34.9613\n",
      "Training Epoch 1  33.4% | batch:       258 of       772\t|\tloss: 37.763\n",
      "Training Epoch 1  33.5% | batch:       259 of       772\t|\tloss: 34.8863\n",
      "Training Epoch 1  33.7% | batch:       260 of       772\t|\tloss: 33.9262\n",
      "Training Epoch 1  33.8% | batch:       261 of       772\t|\tloss: 33.0329\n",
      "Training Epoch 1  33.9% | batch:       262 of       772\t|\tloss: 32.3563\n",
      "Training Epoch 1  34.1% | batch:       263 of       772\t|\tloss: 42.408\n",
      "Training Epoch 1  34.2% | batch:       264 of       772\t|\tloss: 39.7406\n",
      "Training Epoch 1  34.3% | batch:       265 of       772\t|\tloss: 39.2746\n",
      "Training Epoch 1  34.5% | batch:       266 of       772\t|\tloss: 39.0266\n",
      "Training Epoch 1  34.6% | batch:       267 of       772\t|\tloss: 34.7331\n",
      "Training Epoch 1  34.7% | batch:       268 of       772\t|\tloss: 35.2342\n",
      "Training Epoch 1  34.8% | batch:       269 of       772\t|\tloss: 35.4648\n",
      "Training Epoch 1  35.0% | batch:       270 of       772\t|\tloss: 33.6363\n",
      "Training Epoch 1  35.1% | batch:       271 of       772\t|\tloss: 35.5351\n",
      "Training Epoch 1  35.2% | batch:       272 of       772\t|\tloss: 33.8702\n",
      "Training Epoch 1  35.4% | batch:       273 of       772\t|\tloss: 30.2338\n",
      "Training Epoch 1  35.5% | batch:       274 of       772\t|\tloss: 35.0349\n",
      "Training Epoch 1  35.6% | batch:       275 of       772\t|\tloss: 34.1999\n",
      "Training Epoch 1  35.8% | batch:       276 of       772\t|\tloss: 32.1875\n",
      "Training Epoch 1  35.9% | batch:       277 of       772\t|\tloss: 32.886\n",
      "Training Epoch 1  36.0% | batch:       278 of       772\t|\tloss: 30.3924\n",
      "Training Epoch 1  36.1% | batch:       279 of       772\t|\tloss: 33.3941\n",
      "Training Epoch 1  36.3% | batch:       280 of       772\t|\tloss: 28.848\n",
      "Training Epoch 1  36.4% | batch:       281 of       772\t|\tloss: 43.2646\n",
      "Training Epoch 1  36.5% | batch:       282 of       772\t|\tloss: 32.8244\n",
      "Training Epoch 1  36.7% | batch:       283 of       772\t|\tloss: 37.6653\n",
      "Training Epoch 1  36.8% | batch:       284 of       772\t|\tloss: 29.5471\n",
      "Training Epoch 1  36.9% | batch:       285 of       772\t|\tloss: 34.1842\n",
      "Training Epoch 1  37.0% | batch:       286 of       772\t|\tloss: 32.2302\n",
      "Training Epoch 1  37.2% | batch:       287 of       772\t|\tloss: 32.8263\n",
      "Training Epoch 1  37.3% | batch:       288 of       772\t|\tloss: 31.7296\n",
      "Training Epoch 1  37.4% | batch:       289 of       772\t|\tloss: 32.3984\n",
      "Training Epoch 1  37.6% | batch:       290 of       772\t|\tloss: 30.4001\n",
      "Training Epoch 1  37.7% | batch:       291 of       772\t|\tloss: 30.162\n",
      "Training Epoch 1  37.8% | batch:       292 of       772\t|\tloss: 34.0311\n",
      "Training Epoch 1  38.0% | batch:       293 of       772\t|\tloss: 31.454\n",
      "Training Epoch 1  38.1% | batch:       294 of       772\t|\tloss: 31.4791\n",
      "Training Epoch 1  38.2% | batch:       295 of       772\t|\tloss: 33.3361\n",
      "Training Epoch 1  38.3% | batch:       296 of       772\t|\tloss: 32.3423\n",
      "Training Epoch 1  38.5% | batch:       297 of       772\t|\tloss: 30.209\n",
      "Training Epoch 1  38.6% | batch:       298 of       772\t|\tloss: 36.7406\n",
      "Training Epoch 1  38.7% | batch:       299 of       772\t|\tloss: 31.7243\n",
      "Training Epoch 1  38.9% | batch:       300 of       772\t|\tloss: 27.8649\n",
      "Training Epoch 1  39.0% | batch:       301 of       772\t|\tloss: 34.4011\n",
      "Training Epoch 1  39.1% | batch:       302 of       772\t|\tloss: 31.7269\n",
      "Training Epoch 1  39.2% | batch:       303 of       772\t|\tloss: 28.0786\n",
      "Training Epoch 1  39.4% | batch:       304 of       772\t|\tloss: 40.1251\n",
      "Training Epoch 1  39.5% | batch:       305 of       772\t|\tloss: 40.8359\n",
      "Training Epoch 1  39.6% | batch:       306 of       772\t|\tloss: 28.2494\n",
      "Training Epoch 1  39.8% | batch:       307 of       772\t|\tloss: 38.7056\n",
      "Training Epoch 1  39.9% | batch:       308 of       772\t|\tloss: 26.4981\n",
      "Training Epoch 1  40.0% | batch:       309 of       772\t|\tloss: 34.1979\n",
      "Training Epoch 1  40.2% | batch:       310 of       772\t|\tloss: 36.3306\n",
      "Training Epoch 1  40.3% | batch:       311 of       772\t|\tloss: 38.8523\n",
      "Training Epoch 1  40.4% | batch:       312 of       772\t|\tloss: 38.1161\n",
      "Training Epoch 1  40.5% | batch:       313 of       772\t|\tloss: 36.917\n",
      "Training Epoch 1  40.7% | batch:       314 of       772\t|\tloss: 26.8223\n",
      "Training Epoch 1  40.8% | batch:       315 of       772\t|\tloss: 34.9545\n",
      "Training Epoch 1  40.9% | batch:       316 of       772\t|\tloss: 35.2923\n",
      "Training Epoch 1  41.1% | batch:       317 of       772\t|\tloss: 33.2716\n",
      "Training Epoch 1  41.2% | batch:       318 of       772\t|\tloss: 32.7857\n",
      "Training Epoch 1  41.3% | batch:       319 of       772\t|\tloss: 30.5407\n",
      "Training Epoch 1  41.5% | batch:       320 of       772\t|\tloss: 36.5203\n",
      "Training Epoch 1  41.6% | batch:       321 of       772\t|\tloss: 26.9301\n",
      "Training Epoch 1  41.7% | batch:       322 of       772\t|\tloss: 30.9148\n",
      "Training Epoch 1  41.8% | batch:       323 of       772\t|\tloss: 35.5517\n",
      "Training Epoch 1  42.0% | batch:       324 of       772\t|\tloss: 34.7921\n",
      "Training Epoch 1  42.1% | batch:       325 of       772\t|\tloss: 30.2408\n",
      "Training Epoch 1  42.2% | batch:       326 of       772\t|\tloss: 32.0209\n",
      "Training Epoch 1  42.4% | batch:       327 of       772\t|\tloss: 28.1449\n",
      "Training Epoch 1  42.5% | batch:       328 of       772\t|\tloss: 42.5889\n",
      "Training Epoch 1  42.6% | batch:       329 of       772\t|\tloss: 30.4548\n",
      "Training Epoch 1  42.7% | batch:       330 of       772\t|\tloss: 31.4166\n",
      "Training Epoch 1  42.9% | batch:       331 of       772\t|\tloss: 23.6523\n",
      "Training Epoch 1  43.0% | batch:       332 of       772\t|\tloss: 36.3601\n",
      "Training Epoch 1  43.1% | batch:       333 of       772\t|\tloss: 30.7502\n",
      "Training Epoch 1  43.3% | batch:       334 of       772\t|\tloss: 32.626\n",
      "Training Epoch 1  43.4% | batch:       335 of       772\t|\tloss: 27.3227\n",
      "Training Epoch 1  43.5% | batch:       336 of       772\t|\tloss: 28.7403\n",
      "Training Epoch 1  43.7% | batch:       337 of       772\t|\tloss: 31.8193\n",
      "Training Epoch 1  43.8% | batch:       338 of       772\t|\tloss: 26.8669\n",
      "Training Epoch 1  43.9% | batch:       339 of       772\t|\tloss: 29.5812\n",
      "Training Epoch 1  44.0% | batch:       340 of       772\t|\tloss: 38.9632\n",
      "Training Epoch 1  44.2% | batch:       341 of       772\t|\tloss: 37.5728\n",
      "Training Epoch 1  44.3% | batch:       342 of       772\t|\tloss: 25.4034\n",
      "Training Epoch 1  44.4% | batch:       343 of       772\t|\tloss: 31.1125\n",
      "Training Epoch 1  44.6% | batch:       344 of       772\t|\tloss: 31.3631\n",
      "Training Epoch 1  44.7% | batch:       345 of       772\t|\tloss: 30.6395\n",
      "Training Epoch 1  44.8% | batch:       346 of       772\t|\tloss: 34.4133\n",
      "Training Epoch 1  44.9% | batch:       347 of       772\t|\tloss: 28.3748\n",
      "Training Epoch 1  45.1% | batch:       348 of       772\t|\tloss: 24.6289\n",
      "Training Epoch 1  45.2% | batch:       349 of       772\t|\tloss: 31.0665\n",
      "Training Epoch 1  45.3% | batch:       350 of       772\t|\tloss: 31.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  45.5% | batch:       351 of       772\t|\tloss: 39.2796\n",
      "Training Epoch 1  45.6% | batch:       352 of       772\t|\tloss: 34.7876\n",
      "Training Epoch 1  45.7% | batch:       353 of       772\t|\tloss: 38.2066\n",
      "Training Epoch 1  45.9% | batch:       354 of       772\t|\tloss: 31.4785\n",
      "Training Epoch 1  46.0% | batch:       355 of       772\t|\tloss: 36.6194\n",
      "Training Epoch 1  46.1% | batch:       356 of       772\t|\tloss: 32.2018\n",
      "Training Epoch 1  46.2% | batch:       357 of       772\t|\tloss: 28.2346\n",
      "Training Epoch 1  46.4% | batch:       358 of       772\t|\tloss: 34.3515\n",
      "Training Epoch 1  46.5% | batch:       359 of       772\t|\tloss: 24.5266\n",
      "Training Epoch 1  46.6% | batch:       360 of       772\t|\tloss: 28.3624\n",
      "Training Epoch 1  46.8% | batch:       361 of       772\t|\tloss: 30.9239\n",
      "Training Epoch 1  46.9% | batch:       362 of       772\t|\tloss: 33.2356\n",
      "Training Epoch 1  47.0% | batch:       363 of       772\t|\tloss: 25.8145\n",
      "Training Epoch 1  47.2% | batch:       364 of       772\t|\tloss: 29.3374\n",
      "Training Epoch 1  47.3% | batch:       365 of       772\t|\tloss: 34.4536\n",
      "Training Epoch 1  47.4% | batch:       366 of       772\t|\tloss: 24.6192\n",
      "Training Epoch 1  47.5% | batch:       367 of       772\t|\tloss: 26.9082\n",
      "Training Epoch 1  47.7% | batch:       368 of       772\t|\tloss: 31.4294\n",
      "Training Epoch 1  47.8% | batch:       369 of       772\t|\tloss: 36.2995\n",
      "Training Epoch 1  47.9% | batch:       370 of       772\t|\tloss: 27.1785\n",
      "Training Epoch 1  48.1% | batch:       371 of       772\t|\tloss: 30.0858\n",
      "Training Epoch 1  48.2% | batch:       372 of       772\t|\tloss: 27.6891\n",
      "Training Epoch 1  48.3% | batch:       373 of       772\t|\tloss: 38.101\n",
      "Training Epoch 1  48.4% | batch:       374 of       772\t|\tloss: 29.4881\n",
      "Training Epoch 1  48.6% | batch:       375 of       772\t|\tloss: 28.9398\n",
      "Training Epoch 1  48.7% | batch:       376 of       772\t|\tloss: 29.7552\n",
      "Training Epoch 1  48.8% | batch:       377 of       772\t|\tloss: 27.029\n",
      "Training Epoch 1  49.0% | batch:       378 of       772\t|\tloss: 29.5328\n",
      "Training Epoch 1  49.1% | batch:       379 of       772\t|\tloss: 35.348\n",
      "Training Epoch 1  49.2% | batch:       380 of       772\t|\tloss: 27.4677\n",
      "Training Epoch 1  49.4% | batch:       381 of       772\t|\tloss: 28.0219\n",
      "Training Epoch 1  49.5% | batch:       382 of       772\t|\tloss: 28.5298\n",
      "Training Epoch 1  49.6% | batch:       383 of       772\t|\tloss: 29.0115\n",
      "Training Epoch 1  49.7% | batch:       384 of       772\t|\tloss: 29.2295\n",
      "Training Epoch 1  49.9% | batch:       385 of       772\t|\tloss: 32.7323\n",
      "Training Epoch 1  50.0% | batch:       386 of       772\t|\tloss: 24.7106\n",
      "Training Epoch 1  50.1% | batch:       387 of       772\t|\tloss: 32.4143\n",
      "Training Epoch 1  50.3% | batch:       388 of       772\t|\tloss: 29.1078\n",
      "Training Epoch 1  50.4% | batch:       389 of       772\t|\tloss: 25.7864\n",
      "Training Epoch 1  50.5% | batch:       390 of       772\t|\tloss: 29.354\n",
      "Training Epoch 1  50.6% | batch:       391 of       772\t|\tloss: 28.5664\n",
      "Training Epoch 1  50.8% | batch:       392 of       772\t|\tloss: 37.7148\n",
      "Training Epoch 1  50.9% | batch:       393 of       772\t|\tloss: 30.8757\n",
      "Training Epoch 1  51.0% | batch:       394 of       772\t|\tloss: 25.3942\n",
      "Training Epoch 1  51.2% | batch:       395 of       772\t|\tloss: 25.5703\n",
      "Training Epoch 1  51.3% | batch:       396 of       772\t|\tloss: 32.6736\n",
      "Training Epoch 1  51.4% | batch:       397 of       772\t|\tloss: 31.9197\n",
      "Training Epoch 1  51.6% | batch:       398 of       772\t|\tloss: 24.6227\n",
      "Training Epoch 1  51.7% | batch:       399 of       772\t|\tloss: 25.3266\n",
      "Training Epoch 1  51.8% | batch:       400 of       772\t|\tloss: 27.0928\n",
      "Training Epoch 1  51.9% | batch:       401 of       772\t|\tloss: 28.7527\n",
      "Training Epoch 1  52.1% | batch:       402 of       772\t|\tloss: 27.8079\n",
      "Training Epoch 1  52.2% | batch:       403 of       772\t|\tloss: 27.9709\n",
      "Training Epoch 1  52.3% | batch:       404 of       772\t|\tloss: 29.6938\n",
      "Training Epoch 1  52.5% | batch:       405 of       772\t|\tloss: 30.679\n",
      "Training Epoch 1  52.6% | batch:       406 of       772\t|\tloss: 25.9431\n",
      "Training Epoch 1  52.7% | batch:       407 of       772\t|\tloss: 23.746\n",
      "Training Epoch 1  52.8% | batch:       408 of       772\t|\tloss: 26.1881\n",
      "Training Epoch 1  53.0% | batch:       409 of       772\t|\tloss: 29.1902\n",
      "Training Epoch 1  53.1% | batch:       410 of       772\t|\tloss: 29.8818\n",
      "Training Epoch 1  53.2% | batch:       411 of       772\t|\tloss: 25.4236\n",
      "Training Epoch 1  53.4% | batch:       412 of       772\t|\tloss: 24.399\n",
      "Training Epoch 1  53.5% | batch:       413 of       772\t|\tloss: 28.596\n",
      "Training Epoch 1  53.6% | batch:       414 of       772\t|\tloss: 23.6602\n",
      "Training Epoch 1  53.8% | batch:       415 of       772\t|\tloss: 29.0329\n",
      "Training Epoch 1  53.9% | batch:       416 of       772\t|\tloss: 29.0906\n",
      "Training Epoch 1  54.0% | batch:       417 of       772\t|\tloss: 29.5396\n",
      "Training Epoch 1  54.1% | batch:       418 of       772\t|\tloss: 29.6243\n",
      "Training Epoch 1  54.3% | batch:       419 of       772\t|\tloss: 28.9766\n",
      "Training Epoch 1  54.4% | batch:       420 of       772\t|\tloss: 25.7087\n",
      "Training Epoch 1  54.5% | batch:       421 of       772\t|\tloss: 33.5882\n",
      "Training Epoch 1  54.7% | batch:       422 of       772\t|\tloss: 23.7897\n",
      "Training Epoch 1  54.8% | batch:       423 of       772\t|\tloss: 24.4525\n",
      "Training Epoch 1  54.9% | batch:       424 of       772\t|\tloss: 30.6096\n",
      "Training Epoch 1  55.1% | batch:       425 of       772\t|\tloss: 23.1209\n",
      "Training Epoch 1  55.2% | batch:       426 of       772\t|\tloss: 27.3121\n",
      "Training Epoch 1  55.3% | batch:       427 of       772\t|\tloss: 26.2911\n",
      "Training Epoch 1  55.4% | batch:       428 of       772\t|\tloss: 31.791\n",
      "Training Epoch 1  55.6% | batch:       429 of       772\t|\tloss: 25.1246\n",
      "Training Epoch 1  55.7% | batch:       430 of       772\t|\tloss: 32.9576\n",
      "Training Epoch 1  55.8% | batch:       431 of       772\t|\tloss: 35.6362\n",
      "Training Epoch 1  56.0% | batch:       432 of       772\t|\tloss: 24.9204\n",
      "Training Epoch 1  56.1% | batch:       433 of       772\t|\tloss: 24.7364\n",
      "Training Epoch 1  56.2% | batch:       434 of       772\t|\tloss: 27.9608\n",
      "Training Epoch 1  56.3% | batch:       435 of       772\t|\tloss: 25.7506\n",
      "Training Epoch 1  56.5% | batch:       436 of       772\t|\tloss: 30.0115\n",
      "Training Epoch 1  56.6% | batch:       437 of       772\t|\tloss: 29.0163\n",
      "Training Epoch 1  56.7% | batch:       438 of       772\t|\tloss: 32.7176\n",
      "Training Epoch 1  56.9% | batch:       439 of       772\t|\tloss: 25.3631\n",
      "Training Epoch 1  57.0% | batch:       440 of       772\t|\tloss: 22.4594\n",
      "Training Epoch 1  57.1% | batch:       441 of       772\t|\tloss: 33.421\n",
      "Training Epoch 1  57.3% | batch:       442 of       772\t|\tloss: 29.2361\n",
      "Training Epoch 1  57.4% | batch:       443 of       772\t|\tloss: 29.0481\n",
      "Training Epoch 1  57.5% | batch:       444 of       772\t|\tloss: 22.1847\n",
      "Training Epoch 1  57.6% | batch:       445 of       772\t|\tloss: 24.1824\n",
      "Training Epoch 1  57.8% | batch:       446 of       772\t|\tloss: 27.8433\n",
      "Training Epoch 1  57.9% | batch:       447 of       772\t|\tloss: 25.8744\n",
      "Training Epoch 1  58.0% | batch:       448 of       772\t|\tloss: 29.5075\n",
      "Training Epoch 1  58.2% | batch:       449 of       772\t|\tloss: 32.5557\n",
      "Training Epoch 1  58.3% | batch:       450 of       772\t|\tloss: 21.9937\n",
      "Training Epoch 1  58.4% | batch:       451 of       772\t|\tloss: 26.3215\n",
      "Training Epoch 1  58.5% | batch:       452 of       772\t|\tloss: 28.3588\n",
      "Training Epoch 1  58.7% | batch:       453 of       772\t|\tloss: 28.9017\n",
      "Training Epoch 1  58.8% | batch:       454 of       772\t|\tloss: 30.1156\n",
      "Training Epoch 1  58.9% | batch:       455 of       772\t|\tloss: 26.175\n",
      "Training Epoch 1  59.1% | batch:       456 of       772\t|\tloss: 17.1541\n",
      "Training Epoch 1  59.2% | batch:       457 of       772\t|\tloss: 27.6375\n",
      "Training Epoch 1  59.3% | batch:       458 of       772\t|\tloss: 30.8137\n",
      "Training Epoch 1  59.5% | batch:       459 of       772\t|\tloss: 25.8413\n",
      "Training Epoch 1  59.6% | batch:       460 of       772\t|\tloss: 24.0113\n",
      "Training Epoch 1  59.7% | batch:       461 of       772\t|\tloss: 23.5765\n",
      "Training Epoch 1  59.8% | batch:       462 of       772\t|\tloss: 24.6428\n",
      "Training Epoch 1  60.0% | batch:       463 of       772\t|\tloss: 33.4807\n",
      "Training Epoch 1  60.1% | batch:       464 of       772\t|\tloss: 24.1397\n",
      "Training Epoch 1  60.2% | batch:       465 of       772\t|\tloss: 24.6401\n",
      "Training Epoch 1  60.4% | batch:       466 of       772\t|\tloss: 31.3366\n",
      "Training Epoch 1  60.5% | batch:       467 of       772\t|\tloss: 29.2631\n",
      "Training Epoch 1  60.6% | batch:       468 of       772\t|\tloss: 30.0385\n",
      "Training Epoch 1  60.8% | batch:       469 of       772\t|\tloss: 31.2876\n",
      "Training Epoch 1  60.9% | batch:       470 of       772\t|\tloss: 30.5368\n",
      "Training Epoch 1  61.0% | batch:       471 of       772\t|\tloss: 28.7704\n",
      "Training Epoch 1  61.1% | batch:       472 of       772\t|\tloss: 22.0773\n",
      "Training Epoch 1  61.3% | batch:       473 of       772\t|\tloss: 23.8521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  61.4% | batch:       474 of       772\t|\tloss: 28.8582\n",
      "Training Epoch 1  61.5% | batch:       475 of       772\t|\tloss: 22.68\n",
      "Training Epoch 1  61.7% | batch:       476 of       772\t|\tloss: 25.8239\n",
      "Training Epoch 1  61.8% | batch:       477 of       772\t|\tloss: 30.4757\n",
      "Training Epoch 1  61.9% | batch:       478 of       772\t|\tloss: 32.9114\n",
      "Training Epoch 1  62.0% | batch:       479 of       772\t|\tloss: 22.9521\n",
      "Training Epoch 1  62.2% | batch:       480 of       772\t|\tloss: 23.935\n",
      "Training Epoch 1  62.3% | batch:       481 of       772\t|\tloss: 27.0195\n",
      "Training Epoch 1  62.4% | batch:       482 of       772\t|\tloss: 28.0797\n",
      "Training Epoch 1  62.6% | batch:       483 of       772\t|\tloss: 25.0433\n",
      "Training Epoch 1  62.7% | batch:       484 of       772\t|\tloss: 21.6337\n",
      "Training Epoch 1  62.8% | batch:       485 of       772\t|\tloss: 25.9929\n",
      "Training Epoch 1  63.0% | batch:       486 of       772\t|\tloss: 22.1719\n",
      "Training Epoch 1  63.1% | batch:       487 of       772\t|\tloss: 28.3593\n",
      "Training Epoch 1  63.2% | batch:       488 of       772\t|\tloss: 25.3088\n",
      "Training Epoch 1  63.3% | batch:       489 of       772\t|\tloss: 35.7907\n",
      "Training Epoch 1  63.5% | batch:       490 of       772\t|\tloss: 24.7972\n",
      "Training Epoch 1  63.6% | batch:       491 of       772\t|\tloss: 20.6904\n",
      "Training Epoch 1  63.7% | batch:       492 of       772\t|\tloss: 24.4645\n",
      "Training Epoch 1  63.9% | batch:       493 of       772\t|\tloss: 23.411\n",
      "Training Epoch 1  64.0% | batch:       494 of       772\t|\tloss: 29.1194\n",
      "Training Epoch 1  64.1% | batch:       495 of       772\t|\tloss: 21.4397\n",
      "Training Epoch 1  64.2% | batch:       496 of       772\t|\tloss: 29.1372\n",
      "Training Epoch 1  64.4% | batch:       497 of       772\t|\tloss: 23.8435\n",
      "Training Epoch 1  64.5% | batch:       498 of       772\t|\tloss: 25.255\n",
      "Training Epoch 1  64.6% | batch:       499 of       772\t|\tloss: 28.1717\n",
      "Training Epoch 1  64.8% | batch:       500 of       772\t|\tloss: 22.6189\n",
      "Training Epoch 1  64.9% | batch:       501 of       772\t|\tloss: 33.9433\n",
      "Training Epoch 1  65.0% | batch:       502 of       772\t|\tloss: 19.2695\n",
      "Training Epoch 1  65.2% | batch:       503 of       772\t|\tloss: 23.1211\n",
      "Training Epoch 1  65.3% | batch:       504 of       772\t|\tloss: 21.6026\n",
      "Training Epoch 1  65.4% | batch:       505 of       772\t|\tloss: 22.3391\n",
      "Training Epoch 1  65.5% | batch:       506 of       772\t|\tloss: 23.3624\n",
      "Training Epoch 1  65.7% | batch:       507 of       772\t|\tloss: 26.9353\n",
      "Training Epoch 1  65.8% | batch:       508 of       772\t|\tloss: 26.2597\n",
      "Training Epoch 1  65.9% | batch:       509 of       772\t|\tloss: 20.6417\n",
      "Training Epoch 1  66.1% | batch:       510 of       772\t|\tloss: 25.5633\n",
      "Training Epoch 1  66.2% | batch:       511 of       772\t|\tloss: 32.1124\n",
      "Training Epoch 1  66.3% | batch:       512 of       772\t|\tloss: 27.223\n",
      "Training Epoch 1  66.5% | batch:       513 of       772\t|\tloss: 23.3082\n",
      "Training Epoch 1  66.6% | batch:       514 of       772\t|\tloss: 25.3968\n",
      "Training Epoch 1  66.7% | batch:       515 of       772\t|\tloss: 23.5712\n",
      "Training Epoch 1  66.8% | batch:       516 of       772\t|\tloss: 27.9726\n",
      "Training Epoch 1  67.0% | batch:       517 of       772\t|\tloss: 22.9169\n",
      "Training Epoch 1  67.1% | batch:       518 of       772\t|\tloss: 24.4723\n",
      "Training Epoch 1  67.2% | batch:       519 of       772\t|\tloss: 22.8723\n",
      "Training Epoch 1  67.4% | batch:       520 of       772\t|\tloss: 19.8055\n",
      "Training Epoch 1  67.5% | batch:       521 of       772\t|\tloss: 26.6057\n",
      "Training Epoch 1  67.6% | batch:       522 of       772\t|\tloss: 20.9276\n",
      "Training Epoch 1  67.7% | batch:       523 of       772\t|\tloss: 20.3547\n",
      "Training Epoch 1  67.9% | batch:       524 of       772\t|\tloss: 19.9302\n",
      "Training Epoch 1  68.0% | batch:       525 of       772\t|\tloss: 27.1636\n",
      "Training Epoch 1  68.1% | batch:       526 of       772\t|\tloss: 22.1341\n",
      "Training Epoch 1  68.3% | batch:       527 of       772\t|\tloss: 28.4161\n",
      "Training Epoch 1  68.4% | batch:       528 of       772\t|\tloss: 28.2921\n",
      "Training Epoch 1  68.5% | batch:       529 of       772\t|\tloss: 24.3224\n",
      "Training Epoch 1  68.7% | batch:       530 of       772\t|\tloss: 25.7815\n",
      "Training Epoch 1  68.8% | batch:       531 of       772\t|\tloss: 23.3973\n",
      "Training Epoch 1  68.9% | batch:       532 of       772\t|\tloss: 29.7239\n",
      "Training Epoch 1  69.0% | batch:       533 of       772\t|\tloss: 25.7955\n",
      "Training Epoch 1  69.2% | batch:       534 of       772\t|\tloss: 29.9179\n",
      "Training Epoch 1  69.3% | batch:       535 of       772\t|\tloss: 28.6605\n",
      "Training Epoch 1  69.4% | batch:       536 of       772\t|\tloss: 26.1952\n",
      "Training Epoch 1  69.6% | batch:       537 of       772\t|\tloss: 22.1117\n",
      "Training Epoch 1  69.7% | batch:       538 of       772\t|\tloss: 28.0804\n",
      "Training Epoch 1  69.8% | batch:       539 of       772\t|\tloss: 19.5134\n",
      "Training Epoch 1  69.9% | batch:       540 of       772\t|\tloss: 25.0324\n",
      "Training Epoch 1  70.1% | batch:       541 of       772\t|\tloss: 23.4659\n",
      "Training Epoch 1  70.2% | batch:       542 of       772\t|\tloss: 25.4028\n",
      "Training Epoch 1  70.3% | batch:       543 of       772\t|\tloss: 18.0072\n",
      "Training Epoch 1  70.5% | batch:       544 of       772\t|\tloss: 21.1036\n",
      "Training Epoch 1  70.6% | batch:       545 of       772\t|\tloss: 24.8839\n",
      "Training Epoch 1  70.7% | batch:       546 of       772\t|\tloss: 23.3264\n",
      "Training Epoch 1  70.9% | batch:       547 of       772\t|\tloss: 23.2399\n",
      "Training Epoch 1  71.0% | batch:       548 of       772\t|\tloss: 22.6076\n",
      "Training Epoch 1  71.1% | batch:       549 of       772\t|\tloss: 24.0348\n",
      "Training Epoch 1  71.2% | batch:       550 of       772\t|\tloss: 23.1162\n",
      "Training Epoch 1  71.4% | batch:       551 of       772\t|\tloss: 22.0413\n",
      "Training Epoch 1  71.5% | batch:       552 of       772\t|\tloss: 21.0386\n",
      "Training Epoch 1  71.6% | batch:       553 of       772\t|\tloss: 24.5774\n",
      "Training Epoch 1  71.8% | batch:       554 of       772\t|\tloss: 23.6032\n",
      "Training Epoch 1  71.9% | batch:       555 of       772\t|\tloss: 21.3603\n",
      "Training Epoch 1  72.0% | batch:       556 of       772\t|\tloss: 21.2635\n",
      "Training Epoch 1  72.2% | batch:       557 of       772\t|\tloss: 24.0669\n",
      "Training Epoch 1  72.3% | batch:       558 of       772\t|\tloss: 25.4301\n",
      "Training Epoch 1  72.4% | batch:       559 of       772\t|\tloss: 23.6375\n",
      "Training Epoch 1  72.5% | batch:       560 of       772\t|\tloss: 21.9868\n",
      "Training Epoch 1  72.7% | batch:       561 of       772\t|\tloss: 24.9873\n",
      "Training Epoch 1  72.8% | batch:       562 of       772\t|\tloss: 23.522\n",
      "Training Epoch 1  72.9% | batch:       563 of       772\t|\tloss: 22.965\n",
      "Training Epoch 1  73.1% | batch:       564 of       772\t|\tloss: 22.2265\n",
      "Training Epoch 1  73.2% | batch:       565 of       772\t|\tloss: 23.5796\n",
      "Training Epoch 1  73.3% | batch:       566 of       772\t|\tloss: 23.8007\n",
      "Training Epoch 1  73.4% | batch:       567 of       772\t|\tloss: 21.9917\n",
      "Training Epoch 1  73.6% | batch:       568 of       772\t|\tloss: 21.5767\n",
      "Training Epoch 1  73.7% | batch:       569 of       772\t|\tloss: 24.6893\n",
      "Training Epoch 1  73.8% | batch:       570 of       772\t|\tloss: 20.5723\n",
      "Training Epoch 1  74.0% | batch:       571 of       772\t|\tloss: 21.3968\n",
      "Training Epoch 1  74.1% | batch:       572 of       772\t|\tloss: 23.9107\n",
      "Training Epoch 1  74.2% | batch:       573 of       772\t|\tloss: 25.2939\n",
      "Training Epoch 1  74.4% | batch:       574 of       772\t|\tloss: 18.5366\n",
      "Training Epoch 1  74.5% | batch:       575 of       772\t|\tloss: 18.3152\n",
      "Training Epoch 1  74.6% | batch:       576 of       772\t|\tloss: 20.7188\n",
      "Training Epoch 1  74.7% | batch:       577 of       772\t|\tloss: 22.5033\n",
      "Training Epoch 1  74.9% | batch:       578 of       772\t|\tloss: 18.6915\n",
      "Training Epoch 1  75.0% | batch:       579 of       772\t|\tloss: 22.5827\n",
      "Training Epoch 1  75.1% | batch:       580 of       772\t|\tloss: 21.5318\n",
      "Training Epoch 1  75.3% | batch:       581 of       772\t|\tloss: 21.0217\n",
      "Training Epoch 1  75.4% | batch:       582 of       772\t|\tloss: 21.9959\n",
      "Training Epoch 1  75.5% | batch:       583 of       772\t|\tloss: 21.6159\n",
      "Training Epoch 1  75.6% | batch:       584 of       772\t|\tloss: 23.0192\n",
      "Training Epoch 1  75.8% | batch:       585 of       772\t|\tloss: 17.9951\n",
      "Training Epoch 1  75.9% | batch:       586 of       772\t|\tloss: 21.7498\n",
      "Training Epoch 1  76.0% | batch:       587 of       772\t|\tloss: 19.0314\n",
      "Training Epoch 1  76.2% | batch:       588 of       772\t|\tloss: 21.9531\n",
      "Training Epoch 1  76.3% | batch:       589 of       772\t|\tloss: 23.9787\n",
      "Training Epoch 1  76.4% | batch:       590 of       772\t|\tloss: 22.7748\n",
      "Training Epoch 1  76.6% | batch:       591 of       772\t|\tloss: 24.2271\n",
      "Training Epoch 1  76.7% | batch:       592 of       772\t|\tloss: 24.4669\n",
      "Training Epoch 1  76.8% | batch:       593 of       772\t|\tloss: 21.1206\n",
      "Training Epoch 1  76.9% | batch:       594 of       772\t|\tloss: 25.2422\n",
      "Training Epoch 1  77.1% | batch:       595 of       772\t|\tloss: 21.6374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  77.2% | batch:       596 of       772\t|\tloss: 23.3266\n",
      "Training Epoch 1  77.3% | batch:       597 of       772\t|\tloss: 20.4984\n",
      "Training Epoch 1  77.5% | batch:       598 of       772\t|\tloss: 23.0875\n",
      "Training Epoch 1  77.6% | batch:       599 of       772\t|\tloss: 21.9155\n",
      "Training Epoch 1  77.7% | batch:       600 of       772\t|\tloss: 18.1145\n",
      "Training Epoch 1  77.8% | batch:       601 of       772\t|\tloss: 21.809\n",
      "Training Epoch 1  78.0% | batch:       602 of       772\t|\tloss: 21.5847\n",
      "Training Epoch 1  78.1% | batch:       603 of       772\t|\tloss: 23.0853\n",
      "Training Epoch 1  78.2% | batch:       604 of       772\t|\tloss: 23.2111\n",
      "Training Epoch 1  78.4% | batch:       605 of       772\t|\tloss: 18.8276\n",
      "Training Epoch 1  78.5% | batch:       606 of       772\t|\tloss: 22.086\n",
      "Training Epoch 1  78.6% | batch:       607 of       772\t|\tloss: 20.2542\n",
      "Training Epoch 1  78.8% | batch:       608 of       772\t|\tloss: 17.1083\n",
      "Training Epoch 1  78.9% | batch:       609 of       772\t|\tloss: 17.7744\n",
      "Training Epoch 1  79.0% | batch:       610 of       772\t|\tloss: 21.0509\n",
      "Training Epoch 1  79.1% | batch:       611 of       772\t|\tloss: 17.1006\n",
      "Training Epoch 1  79.3% | batch:       612 of       772\t|\tloss: 21.6888\n",
      "Training Epoch 1  79.4% | batch:       613 of       772\t|\tloss: 22.205\n",
      "Training Epoch 1  79.5% | batch:       614 of       772\t|\tloss: 19.2258\n",
      "Training Epoch 1  79.7% | batch:       615 of       772\t|\tloss: 18.1571\n",
      "Training Epoch 1  79.8% | batch:       616 of       772\t|\tloss: 23.5119\n",
      "Training Epoch 1  79.9% | batch:       617 of       772\t|\tloss: 18.2994\n",
      "Training Epoch 1  80.1% | batch:       618 of       772\t|\tloss: 21.0062\n",
      "Training Epoch 1  80.2% | batch:       619 of       772\t|\tloss: 22.7574\n",
      "Training Epoch 1  80.3% | batch:       620 of       772\t|\tloss: 21.0169\n",
      "Training Epoch 1  80.4% | batch:       621 of       772\t|\tloss: 17.1346\n",
      "Training Epoch 1  80.6% | batch:       622 of       772\t|\tloss: 21.4175\n",
      "Training Epoch 1  80.7% | batch:       623 of       772\t|\tloss: 17.0953\n",
      "Training Epoch 1  80.8% | batch:       624 of       772\t|\tloss: 17.5982\n",
      "Training Epoch 1  81.0% | batch:       625 of       772\t|\tloss: 18.487\n",
      "Training Epoch 1  81.1% | batch:       626 of       772\t|\tloss: 20.9023\n",
      "Training Epoch 1  81.2% | batch:       627 of       772\t|\tloss: 18.3893\n",
      "Training Epoch 1  81.3% | batch:       628 of       772\t|\tloss: 21.2065\n",
      "Training Epoch 1  81.5% | batch:       629 of       772\t|\tloss: 18.6319\n",
      "Training Epoch 1  81.6% | batch:       630 of       772\t|\tloss: 22.3342\n",
      "Training Epoch 1  81.7% | batch:       631 of       772\t|\tloss: 21.1954\n",
      "Training Epoch 1  81.9% | batch:       632 of       772\t|\tloss: 16.8784\n",
      "Training Epoch 1  82.0% | batch:       633 of       772\t|\tloss: 24.6481\n",
      "Training Epoch 1  82.1% | batch:       634 of       772\t|\tloss: 17.9114\n",
      "Training Epoch 1  82.3% | batch:       635 of       772\t|\tloss: 18.1828\n",
      "Training Epoch 1  82.4% | batch:       636 of       772\t|\tloss: 19.0005\n",
      "Training Epoch 1  82.5% | batch:       637 of       772\t|\tloss: 17.7278\n",
      "Training Epoch 1  82.6% | batch:       638 of       772\t|\tloss: 24.2834\n",
      "Training Epoch 1  82.8% | batch:       639 of       772\t|\tloss: 18.2158\n",
      "Training Epoch 1  82.9% | batch:       640 of       772\t|\tloss: 21.798\n",
      "Training Epoch 1  83.0% | batch:       641 of       772\t|\tloss: 22.1831\n",
      "Training Epoch 1  83.2% | batch:       642 of       772\t|\tloss: 27.5931\n",
      "Training Epoch 1  83.3% | batch:       643 of       772\t|\tloss: 18.2215\n",
      "Training Epoch 1  83.4% | batch:       644 of       772\t|\tloss: 15.505\n",
      "Training Epoch 1  83.5% | batch:       645 of       772\t|\tloss: 17.3375\n",
      "Training Epoch 1  83.7% | batch:       646 of       772\t|\tloss: 19.4038\n",
      "Training Epoch 1  83.8% | batch:       647 of       772\t|\tloss: 19.4823\n",
      "Training Epoch 1  83.9% | batch:       648 of       772\t|\tloss: 16.1001\n",
      "Training Epoch 1  84.1% | batch:       649 of       772\t|\tloss: 22.8096\n",
      "Training Epoch 1  84.2% | batch:       650 of       772\t|\tloss: 20.1855\n",
      "Training Epoch 1  84.3% | batch:       651 of       772\t|\tloss: 18.2049\n",
      "Training Epoch 1  84.5% | batch:       652 of       772\t|\tloss: 17.0607\n",
      "Training Epoch 1  84.6% | batch:       653 of       772\t|\tloss: 15.2584\n",
      "Training Epoch 1  84.7% | batch:       654 of       772\t|\tloss: 18.0548\n",
      "Training Epoch 1  84.8% | batch:       655 of       772\t|\tloss: 19.0479\n",
      "Training Epoch 1  85.0% | batch:       656 of       772\t|\tloss: 19.3001\n",
      "Training Epoch 1  85.1% | batch:       657 of       772\t|\tloss: 17.2112\n",
      "Training Epoch 1  85.2% | batch:       658 of       772\t|\tloss: 20.3349\n",
      "Training Epoch 1  85.4% | batch:       659 of       772\t|\tloss: 18.193\n",
      "Training Epoch 1  85.5% | batch:       660 of       772\t|\tloss: 23.3682\n",
      "Training Epoch 1  85.6% | batch:       661 of       772\t|\tloss: 19.0633\n",
      "Training Epoch 1  85.8% | batch:       662 of       772\t|\tloss: 18.4823\n",
      "Training Epoch 1  85.9% | batch:       663 of       772\t|\tloss: 23.4324\n",
      "Training Epoch 1  86.0% | batch:       664 of       772\t|\tloss: 21.6748\n",
      "Training Epoch 1  86.1% | batch:       665 of       772\t|\tloss: 17.8102\n",
      "Training Epoch 1  86.3% | batch:       666 of       772\t|\tloss: 19.5098\n",
      "Training Epoch 1  86.4% | batch:       667 of       772\t|\tloss: 19.9483\n",
      "Training Epoch 1  86.5% | batch:       668 of       772\t|\tloss: 17.6396\n",
      "Training Epoch 1  86.7% | batch:       669 of       772\t|\tloss: 23.7112\n",
      "Training Epoch 1  86.8% | batch:       670 of       772\t|\tloss: 16.1657\n",
      "Training Epoch 1  86.9% | batch:       671 of       772\t|\tloss: 18.5105\n",
      "Training Epoch 1  87.0% | batch:       672 of       772\t|\tloss: 16.2448\n",
      "Training Epoch 1  87.2% | batch:       673 of       772\t|\tloss: 18.8636\n",
      "Training Epoch 1  87.3% | batch:       674 of       772\t|\tloss: 14.9581\n",
      "Training Epoch 1  87.4% | batch:       675 of       772\t|\tloss: 17.3513\n",
      "Training Epoch 1  87.6% | batch:       676 of       772\t|\tloss: 19.6523\n",
      "Training Epoch 1  87.7% | batch:       677 of       772\t|\tloss: 17.7913\n",
      "Training Epoch 1  87.8% | batch:       678 of       772\t|\tloss: 17.0339\n",
      "Training Epoch 1  88.0% | batch:       679 of       772\t|\tloss: 18.2795\n",
      "Training Epoch 1  88.1% | batch:       680 of       772\t|\tloss: 16.932\n",
      "Training Epoch 1  88.2% | batch:       681 of       772\t|\tloss: 20.0046\n",
      "Training Epoch 1  88.3% | batch:       682 of       772\t|\tloss: 18.0534\n",
      "Training Epoch 1  88.5% | batch:       683 of       772\t|\tloss: 21.9047\n",
      "Training Epoch 1  88.6% | batch:       684 of       772\t|\tloss: 15.8576\n",
      "Training Epoch 1  88.7% | batch:       685 of       772\t|\tloss: 21.7445\n",
      "Training Epoch 1  88.9% | batch:       686 of       772\t|\tloss: 17.1113\n",
      "Training Epoch 1  89.0% | batch:       687 of       772\t|\tloss: 20.1242\n",
      "Training Epoch 1  89.1% | batch:       688 of       772\t|\tloss: 16.8556\n",
      "Training Epoch 1  89.2% | batch:       689 of       772\t|\tloss: 20.5811\n",
      "Training Epoch 1  89.4% | batch:       690 of       772\t|\tloss: 24.5662\n",
      "Training Epoch 1  89.5% | batch:       691 of       772\t|\tloss: 16.4957\n",
      "Training Epoch 1  89.6% | batch:       692 of       772\t|\tloss: 17.1488\n",
      "Training Epoch 1  89.8% | batch:       693 of       772\t|\tloss: 20.2687\n",
      "Training Epoch 1  89.9% | batch:       694 of       772\t|\tloss: 14.1764\n",
      "Training Epoch 1  90.0% | batch:       695 of       772\t|\tloss: 18.9117\n",
      "Training Epoch 1  90.2% | batch:       696 of       772\t|\tloss: 20.4535\n",
      "Training Epoch 1  90.3% | batch:       697 of       772\t|\tloss: 16.434\n",
      "Training Epoch 1  90.4% | batch:       698 of       772\t|\tloss: 25.2124\n",
      "Training Epoch 1  90.5% | batch:       699 of       772\t|\tloss: 19.4595\n",
      "Training Epoch 1  90.7% | batch:       700 of       772\t|\tloss: 16.8994\n",
      "Training Epoch 1  90.8% | batch:       701 of       772\t|\tloss: 18.5829\n",
      "Training Epoch 1  90.9% | batch:       702 of       772\t|\tloss: 17.5305\n",
      "Training Epoch 1  91.1% | batch:       703 of       772\t|\tloss: 20.4298\n",
      "Training Epoch 1  91.2% | batch:       704 of       772\t|\tloss: 23.3599\n",
      "Training Epoch 1  91.3% | batch:       705 of       772\t|\tloss: 18.0534\n",
      "Training Epoch 1  91.5% | batch:       706 of       772\t|\tloss: 23.3933\n",
      "Training Epoch 1  91.6% | batch:       707 of       772\t|\tloss: 21.4411\n",
      "Training Epoch 1  91.7% | batch:       708 of       772\t|\tloss: 14.5233\n",
      "Training Epoch 1  91.8% | batch:       709 of       772\t|\tloss: 17.478\n",
      "Training Epoch 1  92.0% | batch:       710 of       772\t|\tloss: 16.7354\n",
      "Training Epoch 1  92.1% | batch:       711 of       772\t|\tloss: 15.0081\n",
      "Training Epoch 1  92.2% | batch:       712 of       772\t|\tloss: 21.6861\n",
      "Training Epoch 1  92.4% | batch:       713 of       772\t|\tloss: 17.8974\n",
      "Training Epoch 1  92.5% | batch:       714 of       772\t|\tloss: 13.1491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  92.6% | batch:       715 of       772\t|\tloss: 20.884\n",
      "Training Epoch 1  92.7% | batch:       716 of       772\t|\tloss: 16.0152\n",
      "Training Epoch 1  92.9% | batch:       717 of       772\t|\tloss: 20.7263\n",
      "Training Epoch 1  93.0% | batch:       718 of       772\t|\tloss: 18.5258\n",
      "Training Epoch 1  93.1% | batch:       719 of       772\t|\tloss: 17.8989\n",
      "Training Epoch 1  93.3% | batch:       720 of       772\t|\tloss: 17.7701\n",
      "Training Epoch 1  93.4% | batch:       721 of       772\t|\tloss: 17.6094\n",
      "Training Epoch 1  93.5% | batch:       722 of       772\t|\tloss: 17.3524\n",
      "Training Epoch 1  93.7% | batch:       723 of       772\t|\tloss: 17.9052\n",
      "Training Epoch 1  93.8% | batch:       724 of       772\t|\tloss: 15.2755\n",
      "Training Epoch 1  93.9% | batch:       725 of       772\t|\tloss: 17.075\n",
      "Training Epoch 1  94.0% | batch:       726 of       772\t|\tloss: 14.8\n",
      "Training Epoch 1  94.2% | batch:       727 of       772\t|\tloss: 15.9903\n",
      "Training Epoch 1  94.3% | batch:       728 of       772\t|\tloss: 17.5701\n",
      "Training Epoch 1  94.4% | batch:       729 of       772\t|\tloss: 17.86\n",
      "Training Epoch 1  94.6% | batch:       730 of       772\t|\tloss: 19.1525\n",
      "Training Epoch 1  94.7% | batch:       731 of       772\t|\tloss: 17.4251\n",
      "Training Epoch 1  94.8% | batch:       732 of       772\t|\tloss: 15.917\n",
      "Training Epoch 1  94.9% | batch:       733 of       772\t|\tloss: 17.948\n",
      "Training Epoch 1  95.1% | batch:       734 of       772\t|\tloss: 15.4941\n",
      "Training Epoch 1  95.2% | batch:       735 of       772\t|\tloss: 16.0217\n",
      "Training Epoch 1  95.3% | batch:       736 of       772\t|\tloss: 17.8044\n",
      "Training Epoch 1  95.5% | batch:       737 of       772\t|\tloss: 15.4408\n",
      "Training Epoch 1  95.6% | batch:       738 of       772\t|\tloss: 15.7624\n",
      "Training Epoch 1  95.7% | batch:       739 of       772\t|\tloss: 19.3013\n",
      "Training Epoch 1  95.9% | batch:       740 of       772\t|\tloss: 15.0834\n",
      "Training Epoch 1  96.0% | batch:       741 of       772\t|\tloss: 16.1997\n",
      "Training Epoch 1  96.1% | batch:       742 of       772\t|\tloss: 18.6162\n",
      "Training Epoch 1  96.2% | batch:       743 of       772\t|\tloss: 13.4823\n",
      "Training Epoch 1  96.4% | batch:       744 of       772\t|\tloss: 20.3562\n",
      "Training Epoch 1  96.5% | batch:       745 of       772\t|\tloss: 15.7884\n",
      "Training Epoch 1  96.6% | batch:       746 of       772\t|\tloss: 15.0862\n",
      "Training Epoch 1  96.8% | batch:       747 of       772\t|\tloss: 16.4663\n",
      "Training Epoch 1  96.9% | batch:       748 of       772\t|\tloss: 19.2699\n",
      "Training Epoch 1  97.0% | batch:       749 of       772\t|\tloss: 15.4689\n",
      "Training Epoch 1  97.2% | batch:       750 of       772\t|\tloss: 21.9408\n",
      "Training Epoch 1  97.3% | batch:       751 of       772\t|\tloss: 14.492\n",
      "Training Epoch 1  97.4% | batch:       752 of       772\t|\tloss: 19.5927\n",
      "Training Epoch 1  97.5% | batch:       753 of       772\t|\tloss: 17\n",
      "Training Epoch 1  97.7% | batch:       754 of       772\t|\tloss: 20.5358\n",
      "Training Epoch 1  97.8% | batch:       755 of       772\t|\tloss: 15.7756\n",
      "Training Epoch 1  97.9% | batch:       756 of       772\t|\tloss: 15.1115\n",
      "Training Epoch 1  98.1% | batch:       757 of       772\t|\tloss: 15.5666\n",
      "Training Epoch 1  98.2% | batch:       758 of       772\t|\tloss: 14.8447\n",
      "Training Epoch 1  98.3% | batch:       759 of       772\t|\tloss: 16.6623\n",
      "Training Epoch 1  98.4% | batch:       760 of       772\t|\tloss: 18.3295\n",
      "Training Epoch 1  98.6% | batch:       761 of       772\t|\tloss: 14.3384\n",
      "Training Epoch 1  98.7% | batch:       762 of       772\t|\tloss: 18.0327\n",
      "Training Epoch 1  98.8% | batch:       763 of       772\t|\tloss: 18.1164\n",
      "Training Epoch 1  99.0% | batch:       764 of       772\t|\tloss: 15.6024\n",
      "Training Epoch 1  99.1% | batch:       765 of       772\t|\tloss: 18.4525\n",
      "Training Epoch 1  99.2% | batch:       766 of       772\t|\tloss: 13.5727\n",
      "Training Epoch 1  99.4% | batch:       767 of       772\t|\tloss: 14.8012\n",
      "Training Epoch 1  99.5% | batch:       768 of       772\t|\tloss: 14.3548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:20:26,053 | INFO : Epoch 1 Training Summary: epoch: 1.000000 | loss: 36.199474 | \n",
      "2023-05-24 10:20:26,054 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 17.21369218826294 seconds\n",
      "\n",
      "2023-05-24 10:20:26,054 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 17.21369218826294 seconds\n",
      "2023-05-24 10:20:26,055 | INFO : Avg batch train. time: 0.022297528741273238 seconds\n",
      "2023-05-24 10:20:26,055 | INFO : Avg sample train. time: 0.00017420296909610925 seconds\n",
      "2023-05-24 10:20:26,055 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  99.6% | batch:       769 of       772\t|\tloss: 15.102\n",
      "Training Epoch 1  99.7% | batch:       770 of       772\t|\tloss: 18.94\n",
      "Training Epoch 1  99.9% | batch:       771 of       772\t|\tloss: 18.2211\n",
      "\n",
      "Evaluating Epoch 1   0.0% | batch:         0 of        92\t|\tloss: 1.51483\n",
      "Evaluating Epoch 1   1.1% | batch:         1 of        92\t|\tloss: 6.46383\n",
      "Evaluating Epoch 1   2.2% | batch:         2 of        92\t|\tloss: 2.97379\n",
      "Evaluating Epoch 1   3.3% | batch:         3 of        92\t|\tloss: 4.08192\n",
      "Evaluating Epoch 1   4.3% | batch:         4 of        92\t|\tloss: 2.62692\n",
      "Evaluating Epoch 1   5.4% | batch:         5 of        92\t|\tloss: 9.09552\n",
      "Evaluating Epoch 1   6.5% | batch:         6 of        92\t|\tloss: 3.86846\n",
      "Evaluating Epoch 1   7.6% | batch:         7 of        92\t|\tloss: 1.66573\n",
      "Evaluating Epoch 1   8.7% | batch:         8 of        92\t|\tloss: 4.57701\n",
      "Evaluating Epoch 1   9.8% | batch:         9 of        92\t|\tloss: 4.07568\n",
      "Evaluating Epoch 1  10.9% | batch:        10 of        92\t|\tloss: 3.64368\n",
      "Evaluating Epoch 1  12.0% | batch:        11 of        92\t|\tloss: 2.337\n",
      "Evaluating Epoch 1  13.0% | batch:        12 of        92\t|\tloss: 7.72028\n",
      "Evaluating Epoch 1  14.1% | batch:        13 of        92\t|\tloss: 5.28437\n",
      "Evaluating Epoch 1  15.2% | batch:        14 of        92\t|\tloss: 2.14612\n",
      "Evaluating Epoch 1  16.3% | batch:        15 of        92\t|\tloss: 0.434912\n",
      "Evaluating Epoch 1  17.4% | batch:        16 of        92\t|\tloss: 1.97519\n",
      "Evaluating Epoch 1  18.5% | batch:        17 of        92\t|\tloss: 0.672271\n",
      "Evaluating Epoch 1  19.6% | batch:        18 of        92\t|\tloss: 2.96408\n",
      "Evaluating Epoch 1  20.7% | batch:        19 of        92\t|\tloss: 4.64198\n",
      "Evaluating Epoch 1  21.7% | batch:        20 of        92\t|\tloss: 2.12777\n",
      "Evaluating Epoch 1  22.8% | batch:        21 of        92\t|\tloss: 2.18432\n",
      "Evaluating Epoch 1  23.9% | batch:        22 of        92\t|\tloss: 5.93848\n",
      "Evaluating Epoch 1  25.0% | batch:        23 of        92\t|\tloss: 5.67444\n",
      "Evaluating Epoch 1  26.1% | batch:        24 of        92\t|\tloss: 2.26043\n",
      "Evaluating Epoch 1  27.2% | batch:        25 of        92\t|\tloss: 0.781648\n",
      "Evaluating Epoch 1  28.3% | batch:        26 of        92\t|\tloss: 0.757793\n",
      "Evaluating Epoch 1  29.3% | batch:        27 of        92\t|\tloss: 2.36162\n",
      "Evaluating Epoch 1  30.4% | batch:        28 of        92\t|\tloss: 1.83581\n",
      "Evaluating Epoch 1  31.5% | batch:        29 of        92\t|\tloss: 2.16205\n",
      "Evaluating Epoch 1  32.6% | batch:        30 of        92\t|\tloss: 2.62179\n",
      "Evaluating Epoch 1  33.7% | batch:        31 of        92\t|\tloss: 2.31216\n",
      "Evaluating Epoch 1  34.8% | batch:        32 of        92\t|\tloss: 1.90949\n",
      "Evaluating Epoch 1  35.9% | batch:        33 of        92\t|\tloss: 4.54751\n",
      "Evaluating Epoch 1  37.0% | batch:        34 of        92\t|\tloss: 2.84522\n",
      "Evaluating Epoch 1  38.0% | batch:        35 of        92\t|\tloss: 2.14664\n",
      "Evaluating Epoch 1  39.1% | batch:        36 of        92\t|\tloss: 1.11332\n",
      "Evaluating Epoch 1  40.2% | batch:        37 of        92\t|\tloss: 1.63888\n",
      "Evaluating Epoch 1  41.3% | batch:        38 of        92\t|\tloss: 2.14607\n",
      "Evaluating Epoch 1  42.4% | batch:        39 of        92\t|\tloss: 4.84552\n",
      "Evaluating Epoch 1  43.5% | batch:        40 of        92\t|\tloss: 3.28884\n",
      "Evaluating Epoch 1  44.6% | batch:        41 of        92\t|\tloss: 2.96339\n",
      "Evaluating Epoch 1  45.7% | batch:        42 of        92\t|\tloss: 4.90091\n",
      "Evaluating Epoch 1  46.7% | batch:        43 of        92\t|\tloss: 6.95827\n",
      "Evaluating Epoch 1  47.8% | batch:        44 of        92\t|\tloss: 1.83349\n",
      "Evaluating Epoch 1  48.9% | batch:        45 of        92\t|\tloss: 1.20744\n",
      "Evaluating Epoch 1  50.0% | batch:        46 of        92\t|\tloss: 0.881739\n",
      "Evaluating Epoch 1  51.1% | batch:        47 of        92\t|\tloss: 3.49642\n",
      "Evaluating Epoch 1  52.2% | batch:        48 of        92\t|\tloss: 4.36263\n",
      "Evaluating Epoch 1  53.3% | batch:        49 of        92\t|\tloss: 3.80285\n",
      "Evaluating Epoch 1  54.3% | batch:        50 of        92\t|\tloss: 2.83389\n",
      "Evaluating Epoch 1  55.4% | batch:        51 of        92\t|\tloss: 6.70585\n",
      "Evaluating Epoch 1  56.5% | batch:        52 of        92\t|\tloss: 5.87685\n",
      "Evaluating Epoch 1  57.6% | batch:        53 of        92\t|\tloss: 1.97168\n",
      "Evaluating Epoch 1  58.7% | batch:        54 of        92\t|\tloss: 1.067\n",
      "Evaluating Epoch 1  59.8% | batch:        55 of        92\t|\tloss: 4.0936\n",
      "Evaluating Epoch 1  60.9% | batch:        56 of        92\t|\tloss: 4.87227\n",
      "Evaluating Epoch 1  62.0% | batch:        57 of        92\t|\tloss: 3.5807\n",
      "Evaluating Epoch 1  63.0% | batch:        58 of        92\t|\tloss: 2.80922\n",
      "Evaluating Epoch 1  64.1% | batch:        59 of        92\t|\tloss: 6.71656\n",
      "Evaluating Epoch 1  65.2% | batch:        60 of        92\t|\tloss: 5.8486\n",
      "Evaluating Epoch 1  66.3% | batch:        61 of        92\t|\tloss: 2.1436\n",
      "Evaluating Epoch 1  67.4% | batch:        62 of        92\t|\tloss: 0.435164\n",
      "Evaluating Epoch 1  68.5% | batch:        63 of        92\t|\tloss: 0.641059\n",
      "Evaluating Epoch 1  69.6% | batch:        64 of        92\t|\tloss: 3.06227\n",
      "Evaluating Epoch 1  70.7% | batch:        65 of        92\t|\tloss: 4.96797\n",
      "Evaluating Epoch 1  71.7% | batch:        66 of        92\t|\tloss: 1.68325\n",
      "Evaluating Epoch 1  72.8% | batch:        67 of        92\t|\tloss: 2.57186\n",
      "Evaluating Epoch 1  73.9% | batch:        68 of        92\t|\tloss: 2.87538\n",
      "Evaluating Epoch 1  75.0% | batch:        69 of        92\t|\tloss: 2.16036\n",
      "Evaluating Epoch 1  76.1% | batch:        70 of        92\t|\tloss: 4.88001\n",
      "Evaluating Epoch 1  77.2% | batch:        71 of        92\t|\tloss: 2.53731\n",
      "Evaluating Epoch 1  78.3% | batch:        72 of        92\t|\tloss: 2.0779\n",
      "Evaluating Epoch 1  79.3% | batch:        73 of        92\t|\tloss: 1.22199\n",
      "Evaluating Epoch 1  80.4% | batch:        74 of        92\t|\tloss: 3.04505\n",
      "Evaluating Epoch 1  81.5% | batch:        75 of        92\t|\tloss: 0.617991\n",
      "Evaluating Epoch 1  82.6% | batch:        76 of        92\t|\tloss: 2.62652\n",
      "Evaluating Epoch 1  83.7% | batch:        77 of        92\t|\tloss: 4.72673\n",
      "Evaluating Epoch 1  84.8% | batch:        78 of        92\t|\tloss: 3.32757\n",
      "Evaluating Epoch 1  85.9% | batch:        79 of        92\t|\tloss: 3.42259\n",
      "Evaluating Epoch 1  87.0% | batch:        80 of        92\t|\tloss: 4.09596\n",
      "Evaluating Epoch 1  88.0% | batch:        81 of        92\t|\tloss: 3.61935\n",
      "Evaluating Epoch 1  89.1% | batch:        82 of        92\t|\tloss: 1.98637\n",
      "Evaluating Epoch 1  90.2% | batch:        83 of        92\t|\tloss: 1.00429\n",
      "Evaluating Epoch 1  91.3% | batch:        84 of        92\t|\tloss: 3.02495\n",
      "Evaluating Epoch 1  92.4% | batch:        85 of        92\t|\tloss: 4.26404\n",
      "Evaluating Epoch 1  93.5% | batch:        86 of        92\t|\tloss: 3.58296\n",
      "Evaluating Epoch 1  94.6% | batch:        87 of        92\t|\tloss: 2.52235\n",
      "Evaluating Epoch 1  95.7% | batch:        88 of        92\t|\tloss: 5.96499\n",
      "Evaluating Epoch 1  96.7% | batch:        89 of        92\t|\tloss: 5.51239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:20:27,275 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.2194445133209229 seconds\n",
      "\n",
      "2023-05-24 10:20:27,275 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.5824730396270752 seconds\n",
      "2023-05-24 10:20:27,276 | INFO : Avg batch val. time: 0.017200793908989948 seconds\n",
      "2023-05-24 10:20:27,276 | INFO : Avg sample val. time: 0.00013564829758503988 seconds\n",
      "2023-05-24 10:20:27,277 | INFO : Epoch 1 Validation Summary: epoch: 1.000000 | loss: 3.202857 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 1  97.8% | batch:        90 of        92\t|\tloss: 2.03922\n",
      "Evaluating Epoch 1  98.9% | batch:        91 of        92\t|\tloss: 1.57996\n",
      "\n",
      "Training Epoch 2   0.0% | batch:         0 of       772\t|\tloss: 17.2374\n",
      "Training Epoch 2   0.1% | batch:         1 of       772\t|\tloss: 16.0281\n",
      "Training Epoch 2   0.3% | batch:         2 of       772\t|\tloss: 17.9748\n",
      "Training Epoch 2   0.4% | batch:         3 of       772\t|\tloss: 17.944\n",
      "Training Epoch 2   0.5% | batch:         4 of       772\t|\tloss: 12.9809\n",
      "Training Epoch 2   0.6% | batch:         5 of       772\t|\tloss: 17.9242\n",
      "Training Epoch 2   0.8% | batch:         6 of       772\t|\tloss: 18.845\n",
      "Training Epoch 2   0.9% | batch:         7 of       772\t|\tloss: 16.1601\n",
      "Training Epoch 2   1.0% | batch:         8 of       772\t|\tloss: 13.42\n",
      "Training Epoch 2   1.2% | batch:         9 of       772\t|\tloss: 17.0431\n",
      "Training Epoch 2   1.3% | batch:        10 of       772\t|\tloss: 18.0757\n",
      "Training Epoch 2   1.4% | batch:        11 of       772\t|\tloss: 16.7245\n",
      "Training Epoch 2   1.6% | batch:        12 of       772\t|\tloss: 12.3129\n",
      "Training Epoch 2   1.7% | batch:        13 of       772\t|\tloss: 14.4132\n",
      "Training Epoch 2   1.8% | batch:        14 of       772\t|\tloss: 18.2955\n",
      "Training Epoch 2   1.9% | batch:        15 of       772\t|\tloss: 15.7649\n",
      "Training Epoch 2   2.1% | batch:        16 of       772\t|\tloss: 14.9149\n",
      "Training Epoch 2   2.2% | batch:        17 of       772\t|\tloss: 16.7703\n",
      "Training Epoch 2   2.3% | batch:        18 of       772\t|\tloss: 16.7443\n",
      "Training Epoch 2   2.5% | batch:        19 of       772\t|\tloss: 14.3399\n",
      "Training Epoch 2   2.6% | batch:        20 of       772\t|\tloss: 17.4892\n",
      "Training Epoch 2   2.7% | batch:        21 of       772\t|\tloss: 14.8197\n",
      "Training Epoch 2   2.8% | batch:        22 of       772\t|\tloss: 11.7843\n",
      "Training Epoch 2   3.0% | batch:        23 of       772\t|\tloss: 13.886\n",
      "Training Epoch 2   3.1% | batch:        24 of       772\t|\tloss: 14.1945\n",
      "Training Epoch 2   3.2% | batch:        25 of       772\t|\tloss: 15.7631\n",
      "Training Epoch 2   3.4% | batch:        26 of       772\t|\tloss: 14.3807\n",
      "Training Epoch 2   3.5% | batch:        27 of       772\t|\tloss: 16.8909\n",
      "Training Epoch 2   3.6% | batch:        28 of       772\t|\tloss: 15.3113\n",
      "Training Epoch 2   3.8% | batch:        29 of       772\t|\tloss: 13.0097\n",
      "Training Epoch 2   3.9% | batch:        30 of       772\t|\tloss: 14.5808\n",
      "Training Epoch 2   4.0% | batch:        31 of       772\t|\tloss: 15.574\n",
      "Training Epoch 2   4.1% | batch:        32 of       772\t|\tloss: 12.5533\n",
      "Training Epoch 2   4.3% | batch:        33 of       772\t|\tloss: 18.4238\n",
      "Training Epoch 2   4.4% | batch:        34 of       772\t|\tloss: 14.8147\n",
      "Training Epoch 2   4.5% | batch:        35 of       772\t|\tloss: 20.8032\n",
      "Training Epoch 2   4.7% | batch:        36 of       772\t|\tloss: 15.9956\n",
      "Training Epoch 2   4.8% | batch:        37 of       772\t|\tloss: 12.0798\n",
      "Training Epoch 2   4.9% | batch:        38 of       772\t|\tloss: 14.5455\n",
      "Training Epoch 2   5.1% | batch:        39 of       772\t|\tloss: 12.6228\n",
      "Training Epoch 2   5.2% | batch:        40 of       772\t|\tloss: 15.1813\n",
      "Training Epoch 2   5.3% | batch:        41 of       772\t|\tloss: 15.905\n",
      "Training Epoch 2   5.4% | batch:        42 of       772\t|\tloss: 14.5096\n",
      "Training Epoch 2   5.6% | batch:        43 of       772\t|\tloss: 15.991\n",
      "Training Epoch 2   5.7% | batch:        44 of       772\t|\tloss: 16.3987\n",
      "Training Epoch 2   5.8% | batch:        45 of       772\t|\tloss: 14.6828\n",
      "Training Epoch 2   6.0% | batch:        46 of       772\t|\tloss: 14.7959\n",
      "Training Epoch 2   6.1% | batch:        47 of       772\t|\tloss: 14.1253\n",
      "Training Epoch 2   6.2% | batch:        48 of       772\t|\tloss: 12.5083\n",
      "Training Epoch 2   6.3% | batch:        49 of       772\t|\tloss: 16.1986\n",
      "Training Epoch 2   6.5% | batch:        50 of       772\t|\tloss: 13.6178\n",
      "Training Epoch 2   6.6% | batch:        51 of       772\t|\tloss: 15.3384\n",
      "Training Epoch 2   6.7% | batch:        52 of       772\t|\tloss: 16.9763\n",
      "Training Epoch 2   6.9% | batch:        53 of       772\t|\tloss: 14.7532\n",
      "Training Epoch 2   7.0% | batch:        54 of       772\t|\tloss: 15.3657\n",
      "Training Epoch 2   7.1% | batch:        55 of       772\t|\tloss: 12.5277\n",
      "Training Epoch 2   7.3% | batch:        56 of       772\t|\tloss: 14.4743\n",
      "Training Epoch 2   7.4% | batch:        57 of       772\t|\tloss: 13.5956\n",
      "Training Epoch 2   7.5% | batch:        58 of       772\t|\tloss: 15.0952\n",
      "Training Epoch 2   7.6% | batch:        59 of       772\t|\tloss: 15.8531\n",
      "Training Epoch 2   7.8% | batch:        60 of       772\t|\tloss: 13.7974\n",
      "Training Epoch 2   7.9% | batch:        61 of       772\t|\tloss: 15.4146\n",
      "Training Epoch 2   8.0% | batch:        62 of       772\t|\tloss: 15.5176\n",
      "Training Epoch 2   8.2% | batch:        63 of       772\t|\tloss: 13.3626\n",
      "Training Epoch 2   8.3% | batch:        64 of       772\t|\tloss: 16.0037\n",
      "Training Epoch 2   8.4% | batch:        65 of       772\t|\tloss: 13.1389\n",
      "Training Epoch 2   8.5% | batch:        66 of       772\t|\tloss: 16.345\n",
      "Training Epoch 2   8.7% | batch:        67 of       772\t|\tloss: 18.1617\n",
      "Training Epoch 2   8.8% | batch:        68 of       772\t|\tloss: 15.3593\n",
      "Training Epoch 2   8.9% | batch:        69 of       772\t|\tloss: 17.0069\n",
      "Training Epoch 2   9.1% | batch:        70 of       772\t|\tloss: 13.7899\n",
      "Training Epoch 2   9.2% | batch:        71 of       772\t|\tloss: 14.6573\n",
      "Training Epoch 2   9.3% | batch:        72 of       772\t|\tloss: 18.6091\n",
      "Training Epoch 2   9.5% | batch:        73 of       772\t|\tloss: 17.4914\n",
      "Training Epoch 2   9.6% | batch:        74 of       772\t|\tloss: 15.563\n",
      "Training Epoch 2   9.7% | batch:        75 of       772\t|\tloss: 14.8553\n",
      "Training Epoch 2   9.8% | batch:        76 of       772\t|\tloss: 15.6628\n",
      "Training Epoch 2  10.0% | batch:        77 of       772\t|\tloss: 15.7132\n",
      "Training Epoch 2  10.1% | batch:        78 of       772\t|\tloss: 16.1432\n",
      "Training Epoch 2  10.2% | batch:        79 of       772\t|\tloss: 14.1141\n",
      "Training Epoch 2  10.4% | batch:        80 of       772\t|\tloss: 14.978\n",
      "Training Epoch 2  10.5% | batch:        81 of       772\t|\tloss: 11.4642\n",
      "Training Epoch 2  10.6% | batch:        82 of       772\t|\tloss: 14.1107\n",
      "Training Epoch 2  10.8% | batch:        83 of       772\t|\tloss: 14.8408\n",
      "Training Epoch 2  10.9% | batch:        84 of       772\t|\tloss: 15.3477\n",
      "Training Epoch 2  11.0% | batch:        85 of       772\t|\tloss: 15.202\n",
      "Training Epoch 2  11.1% | batch:        86 of       772\t|\tloss: 14.7843\n",
      "Training Epoch 2  11.3% | batch:        87 of       772\t|\tloss: 13.5044\n",
      "Training Epoch 2  11.4% | batch:        88 of       772\t|\tloss: 13.5346\n",
      "Training Epoch 2  11.5% | batch:        89 of       772\t|\tloss: 9.91726\n",
      "Training Epoch 2  11.7% | batch:        90 of       772\t|\tloss: 15.4677\n",
      "Training Epoch 2  11.8% | batch:        91 of       772\t|\tloss: 13.1052\n",
      "Training Epoch 2  11.9% | batch:        92 of       772\t|\tloss: 13.6803\n",
      "Training Epoch 2  12.0% | batch:        93 of       772\t|\tloss: 12.7027\n",
      "Training Epoch 2  12.2% | batch:        94 of       772\t|\tloss: 12.9552\n",
      "Training Epoch 2  12.3% | batch:        95 of       772\t|\tloss: 14.1329\n",
      "Training Epoch 2  12.4% | batch:        96 of       772\t|\tloss: 13.2959\n",
      "Training Epoch 2  12.6% | batch:        97 of       772\t|\tloss: 14.2837\n",
      "Training Epoch 2  12.7% | batch:        98 of       772\t|\tloss: 14.135\n",
      "Training Epoch 2  12.8% | batch:        99 of       772\t|\tloss: 13.2584\n",
      "Training Epoch 2  13.0% | batch:       100 of       772\t|\tloss: 12.807\n",
      "Training Epoch 2  13.1% | batch:       101 of       772\t|\tloss: 17.0254\n",
      "Training Epoch 2  13.2% | batch:       102 of       772\t|\tloss: 11.7947\n",
      "Training Epoch 2  13.3% | batch:       103 of       772\t|\tloss: 12.1081\n",
      "Training Epoch 2  13.5% | batch:       104 of       772\t|\tloss: 14.0852\n",
      "Training Epoch 2  13.6% | batch:       105 of       772\t|\tloss: 12.1052\n",
      "Training Epoch 2  13.7% | batch:       106 of       772\t|\tloss: 13.4309\n",
      "Training Epoch 2  13.9% | batch:       107 of       772\t|\tloss: 13.5883\n",
      "Training Epoch 2  14.0% | batch:       108 of       772\t|\tloss: 17.7143\n",
      "Training Epoch 2  14.1% | batch:       109 of       772\t|\tloss: 12.1405\n",
      "Training Epoch 2  14.2% | batch:       110 of       772\t|\tloss: 11.2529\n",
      "Training Epoch 2  14.4% | batch:       111 of       772\t|\tloss: 12.9185\n",
      "Training Epoch 2  14.5% | batch:       112 of       772\t|\tloss: 11.2768\n",
      "Training Epoch 2  14.6% | batch:       113 of       772\t|\tloss: 11.1769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  14.8% | batch:       114 of       772\t|\tloss: 15.5796\n",
      "Training Epoch 2  14.9% | batch:       115 of       772\t|\tloss: 13.6552\n",
      "Training Epoch 2  15.0% | batch:       116 of       772\t|\tloss: 13.9154\n",
      "Training Epoch 2  15.2% | batch:       117 of       772\t|\tloss: 12.1288\n",
      "Training Epoch 2  15.3% | batch:       118 of       772\t|\tloss: 13.7819\n",
      "Training Epoch 2  15.4% | batch:       119 of       772\t|\tloss: 13.6137\n",
      "Training Epoch 2  15.5% | batch:       120 of       772\t|\tloss: 13.2956\n",
      "Training Epoch 2  15.7% | batch:       121 of       772\t|\tloss: 11.2847\n",
      "Training Epoch 2  15.8% | batch:       122 of       772\t|\tloss: 12.9198\n",
      "Training Epoch 2  15.9% | batch:       123 of       772\t|\tloss: 17.0734\n",
      "Training Epoch 2  16.1% | batch:       124 of       772\t|\tloss: 13.3194\n",
      "Training Epoch 2  16.2% | batch:       125 of       772\t|\tloss: 11.7943\n",
      "Training Epoch 2  16.3% | batch:       126 of       772\t|\tloss: 13.2816\n",
      "Training Epoch 2  16.5% | batch:       127 of       772\t|\tloss: 13.3196\n",
      "Training Epoch 2  16.6% | batch:       128 of       772\t|\tloss: 12.0599\n",
      "Training Epoch 2  16.7% | batch:       129 of       772\t|\tloss: 11.0307\n",
      "Training Epoch 2  16.8% | batch:       130 of       772\t|\tloss: 14.1881\n",
      "Training Epoch 2  17.0% | batch:       131 of       772\t|\tloss: 12.026\n",
      "Training Epoch 2  17.1% | batch:       132 of       772\t|\tloss: 14.1103\n",
      "Training Epoch 2  17.2% | batch:       133 of       772\t|\tloss: 14.5514\n",
      "Training Epoch 2  17.4% | batch:       134 of       772\t|\tloss: 13.29\n",
      "Training Epoch 2  17.5% | batch:       135 of       772\t|\tloss: 14.6347\n",
      "Training Epoch 2  17.6% | batch:       136 of       772\t|\tloss: 12.8244\n",
      "Training Epoch 2  17.7% | batch:       137 of       772\t|\tloss: 13.5804\n",
      "Training Epoch 2  17.9% | batch:       138 of       772\t|\tloss: 13.6113\n",
      "Training Epoch 2  18.0% | batch:       139 of       772\t|\tloss: 15.0947\n",
      "Training Epoch 2  18.1% | batch:       140 of       772\t|\tloss: 12.0384\n",
      "Training Epoch 2  18.3% | batch:       141 of       772\t|\tloss: 19.549\n",
      "Training Epoch 2  18.4% | batch:       142 of       772\t|\tloss: 13.4715\n",
      "Training Epoch 2  18.5% | batch:       143 of       772\t|\tloss: 11.6091\n",
      "Training Epoch 2  18.7% | batch:       144 of       772\t|\tloss: 14.8214\n",
      "Training Epoch 2  18.8% | batch:       145 of       772\t|\tloss: 13.3161\n",
      "Training Epoch 2  18.9% | batch:       146 of       772\t|\tloss: 14.2529\n",
      "Training Epoch 2  19.0% | batch:       147 of       772\t|\tloss: 13.0671\n",
      "Training Epoch 2  19.2% | batch:       148 of       772\t|\tloss: 10.5021\n",
      "Training Epoch 2  19.3% | batch:       149 of       772\t|\tloss: 13.0306\n",
      "Training Epoch 2  19.4% | batch:       150 of       772\t|\tloss: 11.2242\n",
      "Training Epoch 2  19.6% | batch:       151 of       772\t|\tloss: 15.1784\n",
      "Training Epoch 2  19.7% | batch:       152 of       772\t|\tloss: 16.2255\n",
      "Training Epoch 2  19.8% | batch:       153 of       772\t|\tloss: 14.8571\n",
      "Training Epoch 2  19.9% | batch:       154 of       772\t|\tloss: 10.8759\n",
      "Training Epoch 2  20.1% | batch:       155 of       772\t|\tloss: 14.396\n",
      "Training Epoch 2  20.2% | batch:       156 of       772\t|\tloss: 14.1759\n",
      "Training Epoch 2  20.3% | batch:       157 of       772\t|\tloss: 11.0331\n",
      "Training Epoch 2  20.5% | batch:       158 of       772\t|\tloss: 11.2305\n",
      "Training Epoch 2  20.6% | batch:       159 of       772\t|\tloss: 12.0366\n",
      "Training Epoch 2  20.7% | batch:       160 of       772\t|\tloss: 12.7202\n",
      "Training Epoch 2  20.9% | batch:       161 of       772\t|\tloss: 11.8797\n",
      "Training Epoch 2  21.0% | batch:       162 of       772\t|\tloss: 13.1543\n",
      "Training Epoch 2  21.1% | batch:       163 of       772\t|\tloss: 11.273\n",
      "Training Epoch 2  21.2% | batch:       164 of       772\t|\tloss: 10.98\n",
      "Training Epoch 2  21.4% | batch:       165 of       772\t|\tloss: 12.5708\n",
      "Training Epoch 2  21.5% | batch:       166 of       772\t|\tloss: 8.9306\n",
      "Training Epoch 2  21.6% | batch:       167 of       772\t|\tloss: 11.3548\n",
      "Training Epoch 2  21.8% | batch:       168 of       772\t|\tloss: 14.4145\n",
      "Training Epoch 2  21.9% | batch:       169 of       772\t|\tloss: 11.7636\n",
      "Training Epoch 2  22.0% | batch:       170 of       772\t|\tloss: 11.7837\n",
      "Training Epoch 2  22.2% | batch:       171 of       772\t|\tloss: 12.6121\n",
      "Training Epoch 2  22.3% | batch:       172 of       772\t|\tloss: 13.8225\n",
      "Training Epoch 2  22.4% | batch:       173 of       772\t|\tloss: 10.7309\n",
      "Training Epoch 2  22.5% | batch:       174 of       772\t|\tloss: 13.9869\n",
      "Training Epoch 2  22.7% | batch:       175 of       772\t|\tloss: 11.5847\n",
      "Training Epoch 2  22.8% | batch:       176 of       772\t|\tloss: 9.38361\n",
      "Training Epoch 2  22.9% | batch:       177 of       772\t|\tloss: 12.7373\n",
      "Training Epoch 2  23.1% | batch:       178 of       772\t|\tloss: 15.1728\n",
      "Training Epoch 2  23.2% | batch:       179 of       772\t|\tloss: 14.5264\n",
      "Training Epoch 2  23.3% | batch:       180 of       772\t|\tloss: 13.9338\n",
      "Training Epoch 2  23.4% | batch:       181 of       772\t|\tloss: 11.8319\n",
      "Training Epoch 2  23.6% | batch:       182 of       772\t|\tloss: 11.7245\n",
      "Training Epoch 2  23.7% | batch:       183 of       772\t|\tloss: 11.8309\n",
      "Training Epoch 2  23.8% | batch:       184 of       772\t|\tloss: 12.7975\n",
      "Training Epoch 2  24.0% | batch:       185 of       772\t|\tloss: 11.1181\n",
      "Training Epoch 2  24.1% | batch:       186 of       772\t|\tloss: 11.5962\n",
      "Training Epoch 2  24.2% | batch:       187 of       772\t|\tloss: 13.0325\n",
      "Training Epoch 2  24.4% | batch:       188 of       772\t|\tloss: 12.8692\n",
      "Training Epoch 2  24.5% | batch:       189 of       772\t|\tloss: 11.5656\n",
      "Training Epoch 2  24.6% | batch:       190 of       772\t|\tloss: 9.87257\n",
      "Training Epoch 2  24.7% | batch:       191 of       772\t|\tloss: 11.4128\n",
      "Training Epoch 2  24.9% | batch:       192 of       772\t|\tloss: 11.1813\n",
      "Training Epoch 2  25.0% | batch:       193 of       772\t|\tloss: 11.6242\n",
      "Training Epoch 2  25.1% | batch:       194 of       772\t|\tloss: 11.9288\n",
      "Training Epoch 2  25.3% | batch:       195 of       772\t|\tloss: 11.4163\n",
      "Training Epoch 2  25.4% | batch:       196 of       772\t|\tloss: 10.9428\n",
      "Training Epoch 2  25.5% | batch:       197 of       772\t|\tloss: 11.7622\n",
      "Training Epoch 2  25.6% | batch:       198 of       772\t|\tloss: 9.4273\n",
      "Training Epoch 2  25.8% | batch:       199 of       772\t|\tloss: 11.9272\n",
      "Training Epoch 2  25.9% | batch:       200 of       772\t|\tloss: 11.6723\n",
      "Training Epoch 2  26.0% | batch:       201 of       772\t|\tloss: 12.2211\n",
      "Training Epoch 2  26.2% | batch:       202 of       772\t|\tloss: 11.2485\n",
      "Training Epoch 2  26.3% | batch:       203 of       772\t|\tloss: 11.8917\n",
      "Training Epoch 2  26.4% | batch:       204 of       772\t|\tloss: 11.8272\n",
      "Training Epoch 2  26.6% | batch:       205 of       772\t|\tloss: 11.9351\n",
      "Training Epoch 2  26.7% | batch:       206 of       772\t|\tloss: 12.8383\n",
      "Training Epoch 2  26.8% | batch:       207 of       772\t|\tloss: 9.1232\n",
      "Training Epoch 2  26.9% | batch:       208 of       772\t|\tloss: 10.7507\n",
      "Training Epoch 2  27.1% | batch:       209 of       772\t|\tloss: 12.9168\n",
      "Training Epoch 2  27.2% | batch:       210 of       772\t|\tloss: 12.1849\n",
      "Training Epoch 2  27.3% | batch:       211 of       772\t|\tloss: 12.0197\n",
      "Training Epoch 2  27.5% | batch:       212 of       772\t|\tloss: 10.736\n",
      "Training Epoch 2  27.6% | batch:       213 of       772\t|\tloss: 11.4391\n",
      "Training Epoch 2  27.7% | batch:       214 of       772\t|\tloss: 10.5522\n",
      "Training Epoch 2  27.8% | batch:       215 of       772\t|\tloss: 13.4593\n",
      "Training Epoch 2  28.0% | batch:       216 of       772\t|\tloss: 10.877\n",
      "Training Epoch 2  28.1% | batch:       217 of       772\t|\tloss: 14.5908\n",
      "Training Epoch 2  28.2% | batch:       218 of       772\t|\tloss: 11.6551\n",
      "Training Epoch 2  28.4% | batch:       219 of       772\t|\tloss: 13.7812\n",
      "Training Epoch 2  28.5% | batch:       220 of       772\t|\tloss: 12.1445\n",
      "Training Epoch 2  28.6% | batch:       221 of       772\t|\tloss: 11.497\n",
      "Training Epoch 2  28.8% | batch:       222 of       772\t|\tloss: 11.5956\n",
      "Training Epoch 2  28.9% | batch:       223 of       772\t|\tloss: 10.2332\n",
      "Training Epoch 2  29.0% | batch:       224 of       772\t|\tloss: 11.1483\n",
      "Training Epoch 2  29.1% | batch:       225 of       772\t|\tloss: 11.2215\n",
      "Training Epoch 2  29.3% | batch:       226 of       772\t|\tloss: 14.129\n",
      "Training Epoch 2  29.4% | batch:       227 of       772\t|\tloss: 11.478\n",
      "Training Epoch 2  29.5% | batch:       228 of       772\t|\tloss: 9.27291\n",
      "Training Epoch 2  29.7% | batch:       229 of       772\t|\tloss: 12.7837\n",
      "Training Epoch 2  29.8% | batch:       230 of       772\t|\tloss: 10.1022\n",
      "Training Epoch 2  29.9% | batch:       231 of       772\t|\tloss: 10.7484\n",
      "Training Epoch 2  30.1% | batch:       232 of       772\t|\tloss: 10.9255\n",
      "Training Epoch 2  30.2% | batch:       233 of       772\t|\tloss: 10.4524\n",
      "Training Epoch 2  30.3% | batch:       234 of       772\t|\tloss: 13.041\n",
      "Training Epoch 2  30.4% | batch:       235 of       772\t|\tloss: 11.7784\n",
      "Training Epoch 2  30.6% | batch:       236 of       772\t|\tloss: 8.32843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  30.7% | batch:       237 of       772\t|\tloss: 12.9493\n",
      "Training Epoch 2  30.8% | batch:       238 of       772\t|\tloss: 12.9872\n",
      "Training Epoch 2  31.0% | batch:       239 of       772\t|\tloss: 10.0264\n",
      "Training Epoch 2  31.1% | batch:       240 of       772\t|\tloss: 12.1576\n",
      "Training Epoch 2  31.2% | batch:       241 of       772\t|\tloss: 14.2464\n",
      "Training Epoch 2  31.3% | batch:       242 of       772\t|\tloss: 13.0326\n",
      "Training Epoch 2  31.5% | batch:       243 of       772\t|\tloss: 11.2201\n",
      "Training Epoch 2  31.6% | batch:       244 of       772\t|\tloss: 12.7347\n",
      "Training Epoch 2  31.7% | batch:       245 of       772\t|\tloss: 10.2125\n",
      "Training Epoch 2  31.9% | batch:       246 of       772\t|\tloss: 11.3248\n",
      "Training Epoch 2  32.0% | batch:       247 of       772\t|\tloss: 11.4629\n",
      "Training Epoch 2  32.1% | batch:       248 of       772\t|\tloss: 9.65562\n",
      "Training Epoch 2  32.3% | batch:       249 of       772\t|\tloss: 8.93683\n",
      "Training Epoch 2  32.4% | batch:       250 of       772\t|\tloss: 11.0205\n",
      "Training Epoch 2  32.5% | batch:       251 of       772\t|\tloss: 8.7695\n",
      "Training Epoch 2  32.6% | batch:       252 of       772\t|\tloss: 9.19673\n",
      "Training Epoch 2  32.8% | batch:       253 of       772\t|\tloss: 9.88553\n",
      "Training Epoch 2  32.9% | batch:       254 of       772\t|\tloss: 11.8432\n",
      "Training Epoch 2  33.0% | batch:       255 of       772\t|\tloss: 9.87871\n",
      "Training Epoch 2  33.2% | batch:       256 of       772\t|\tloss: 11.0869\n",
      "Training Epoch 2  33.3% | batch:       257 of       772\t|\tloss: 8.45793\n",
      "Training Epoch 2  33.4% | batch:       258 of       772\t|\tloss: 9.31389\n",
      "Training Epoch 2  33.5% | batch:       259 of       772\t|\tloss: 11.6142\n",
      "Training Epoch 2  33.7% | batch:       260 of       772\t|\tloss: 11.4075\n",
      "Training Epoch 2  33.8% | batch:       261 of       772\t|\tloss: 10.0155\n",
      "Training Epoch 2  33.9% | batch:       262 of       772\t|\tloss: 10.2112\n",
      "Training Epoch 2  34.1% | batch:       263 of       772\t|\tloss: 10.5847\n",
      "Training Epoch 2  34.2% | batch:       264 of       772\t|\tloss: 11.7125\n",
      "Training Epoch 2  34.3% | batch:       265 of       772\t|\tloss: 12.0846\n",
      "Training Epoch 2  34.5% | batch:       266 of       772\t|\tloss: 11.1761\n",
      "Training Epoch 2  34.6% | batch:       267 of       772\t|\tloss: 11.0065\n",
      "Training Epoch 2  34.7% | batch:       268 of       772\t|\tloss: 10.9495\n",
      "Training Epoch 2  34.8% | batch:       269 of       772\t|\tloss: 9.71075\n",
      "Training Epoch 2  35.0% | batch:       270 of       772\t|\tloss: 12.3722\n",
      "Training Epoch 2  35.1% | batch:       271 of       772\t|\tloss: 9.26724\n",
      "Training Epoch 2  35.2% | batch:       272 of       772\t|\tloss: 10.9966\n",
      "Training Epoch 2  35.4% | batch:       273 of       772\t|\tloss: 10.6717\n",
      "Training Epoch 2  35.5% | batch:       274 of       772\t|\tloss: 12.0331\n",
      "Training Epoch 2  35.6% | batch:       275 of       772\t|\tloss: 10.0581\n",
      "Training Epoch 2  35.8% | batch:       276 of       772\t|\tloss: 8.75835\n",
      "Training Epoch 2  35.9% | batch:       277 of       772\t|\tloss: 8.86905\n",
      "Training Epoch 2  36.0% | batch:       278 of       772\t|\tloss: 12.7388\n",
      "Training Epoch 2  36.1% | batch:       279 of       772\t|\tloss: 7.99131\n",
      "Training Epoch 2  36.3% | batch:       280 of       772\t|\tloss: 11.8011\n",
      "Training Epoch 2  36.4% | batch:       281 of       772\t|\tloss: 11.7592\n",
      "Training Epoch 2  36.5% | batch:       282 of       772\t|\tloss: 10.1701\n",
      "Training Epoch 2  36.7% | batch:       283 of       772\t|\tloss: 8.98807\n",
      "Training Epoch 2  36.8% | batch:       284 of       772\t|\tloss: 10.3998\n",
      "Training Epoch 2  36.9% | batch:       285 of       772\t|\tloss: 10.5928\n",
      "Training Epoch 2  37.0% | batch:       286 of       772\t|\tloss: 10.8108\n",
      "Training Epoch 2  37.2% | batch:       287 of       772\t|\tloss: 9.83132\n",
      "Training Epoch 2  37.3% | batch:       288 of       772\t|\tloss: 11.8084\n",
      "Training Epoch 2  37.4% | batch:       289 of       772\t|\tloss: 12.5325\n",
      "Training Epoch 2  37.6% | batch:       290 of       772\t|\tloss: 10.2291\n",
      "Training Epoch 2  37.7% | batch:       291 of       772\t|\tloss: 8.62482\n",
      "Training Epoch 2  37.8% | batch:       292 of       772\t|\tloss: 9.79817\n",
      "Training Epoch 2  38.0% | batch:       293 of       772\t|\tloss: 8.93897\n",
      "Training Epoch 2  38.1% | batch:       294 of       772\t|\tloss: 12.0569\n",
      "Training Epoch 2  38.2% | batch:       295 of       772\t|\tloss: 10.3539\n",
      "Training Epoch 2  38.3% | batch:       296 of       772\t|\tloss: 11.8713\n",
      "Training Epoch 2  38.5% | batch:       297 of       772\t|\tloss: 10.6855\n",
      "Training Epoch 2  38.6% | batch:       298 of       772\t|\tloss: 9.50585\n",
      "Training Epoch 2  38.7% | batch:       299 of       772\t|\tloss: 9.50645\n",
      "Training Epoch 2  38.9% | batch:       300 of       772\t|\tloss: 10.1053\n",
      "Training Epoch 2  39.0% | batch:       301 of       772\t|\tloss: 10.5408\n",
      "Training Epoch 2  39.1% | batch:       302 of       772\t|\tloss: 8.94525\n",
      "Training Epoch 2  39.2% | batch:       303 of       772\t|\tloss: 11.0442\n",
      "Training Epoch 2  39.4% | batch:       304 of       772\t|\tloss: 9.1962\n",
      "Training Epoch 2  39.5% | batch:       305 of       772\t|\tloss: 9.77642\n",
      "Training Epoch 2  39.6% | batch:       306 of       772\t|\tloss: 11.449\n",
      "Training Epoch 2  39.8% | batch:       307 of       772\t|\tloss: 10.4656\n",
      "Training Epoch 2  39.9% | batch:       308 of       772\t|\tloss: 8.31521\n",
      "Training Epoch 2  40.0% | batch:       309 of       772\t|\tloss: 8.94455\n",
      "Training Epoch 2  40.2% | batch:       310 of       772\t|\tloss: 10.127\n",
      "Training Epoch 2  40.3% | batch:       311 of       772\t|\tloss: 7.96929\n",
      "Training Epoch 2  40.4% | batch:       312 of       772\t|\tloss: 10.3687\n",
      "Training Epoch 2  40.5% | batch:       313 of       772\t|\tloss: 10.8549\n",
      "Training Epoch 2  40.7% | batch:       314 of       772\t|\tloss: 9.93957\n",
      "Training Epoch 2  40.8% | batch:       315 of       772\t|\tloss: 10.5313\n",
      "Training Epoch 2  40.9% | batch:       316 of       772\t|\tloss: 11.4617\n",
      "Training Epoch 2  41.1% | batch:       317 of       772\t|\tloss: 10.9306\n",
      "Training Epoch 2  41.2% | batch:       318 of       772\t|\tloss: 10.2218\n",
      "Training Epoch 2  41.3% | batch:       319 of       772\t|\tloss: 7.50756\n",
      "Training Epoch 2  41.5% | batch:       320 of       772\t|\tloss: 11.2361\n",
      "Training Epoch 2  41.6% | batch:       321 of       772\t|\tloss: 9.2224\n",
      "Training Epoch 2  41.7% | batch:       322 of       772\t|\tloss: 10.1409\n",
      "Training Epoch 2  41.8% | batch:       323 of       772\t|\tloss: 11.3623\n",
      "Training Epoch 2  42.0% | batch:       324 of       772\t|\tloss: 8.27987\n",
      "Training Epoch 2  42.1% | batch:       325 of       772\t|\tloss: 10.3379\n",
      "Training Epoch 2  42.2% | batch:       326 of       772\t|\tloss: 10.6279\n",
      "Training Epoch 2  42.4% | batch:       327 of       772\t|\tloss: 10.1096\n",
      "Training Epoch 2  42.5% | batch:       328 of       772\t|\tloss: 7.72031\n",
      "Training Epoch 2  42.6% | batch:       329 of       772\t|\tloss: 9.99615\n",
      "Training Epoch 2  42.7% | batch:       330 of       772\t|\tloss: 9.19072\n",
      "Training Epoch 2  42.9% | batch:       331 of       772\t|\tloss: 9.69798\n",
      "Training Epoch 2  43.0% | batch:       332 of       772\t|\tloss: 7.89894\n",
      "Training Epoch 2  43.1% | batch:       333 of       772\t|\tloss: 11.8427\n",
      "Training Epoch 2  43.3% | batch:       334 of       772\t|\tloss: 11.1052\n",
      "Training Epoch 2  43.4% | batch:       335 of       772\t|\tloss: 9.4457\n",
      "Training Epoch 2  43.5% | batch:       336 of       772\t|\tloss: 8.09131\n",
      "Training Epoch 2  43.7% | batch:       337 of       772\t|\tloss: 8.6242\n",
      "Training Epoch 2  43.8% | batch:       338 of       772\t|\tloss: 7.3789\n",
      "Training Epoch 2  43.9% | batch:       339 of       772\t|\tloss: 8.39682\n",
      "Training Epoch 2  44.0% | batch:       340 of       772\t|\tloss: 9.98352\n",
      "Training Epoch 2  44.2% | batch:       341 of       772\t|\tloss: 8.77842\n",
      "Training Epoch 2  44.3% | batch:       342 of       772\t|\tloss: 9.6904\n",
      "Training Epoch 2  44.4% | batch:       343 of       772\t|\tloss: 9.74416\n",
      "Training Epoch 2  44.6% | batch:       344 of       772\t|\tloss: 8.68956\n",
      "Training Epoch 2  44.7% | batch:       345 of       772\t|\tloss: 10.683\n",
      "Training Epoch 2  44.8% | batch:       346 of       772\t|\tloss: 8.65169\n",
      "Training Epoch 2  44.9% | batch:       347 of       772\t|\tloss: 10.8448\n",
      "Training Epoch 2  45.1% | batch:       348 of       772\t|\tloss: 8.22559\n",
      "Training Epoch 2  45.2% | batch:       349 of       772\t|\tloss: 9.23428\n",
      "Training Epoch 2  45.3% | batch:       350 of       772\t|\tloss: 10.4323\n",
      "Training Epoch 2  45.5% | batch:       351 of       772\t|\tloss: 11.437\n",
      "Training Epoch 2  45.6% | batch:       352 of       772\t|\tloss: 8.83914\n",
      "Training Epoch 2  45.7% | batch:       353 of       772\t|\tloss: 9.43359\n",
      "Training Epoch 2  45.9% | batch:       354 of       772\t|\tloss: 9.0799\n",
      "Training Epoch 2  46.0% | batch:       355 of       772\t|\tloss: 11.7737\n",
      "Training Epoch 2  46.1% | batch:       356 of       772\t|\tloss: 11.8599\n",
      "Training Epoch 2  46.2% | batch:       357 of       772\t|\tloss: 9.78988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  46.4% | batch:       358 of       772\t|\tloss: 9.36272\n",
      "Training Epoch 2  46.5% | batch:       359 of       772\t|\tloss: 11.6294\n",
      "Training Epoch 2  46.6% | batch:       360 of       772\t|\tloss: 9.87205\n",
      "Training Epoch 2  46.8% | batch:       361 of       772\t|\tloss: 9.78161\n",
      "Training Epoch 2  46.9% | batch:       362 of       772\t|\tloss: 8.86777\n",
      "Training Epoch 2  47.0% | batch:       363 of       772\t|\tloss: 10.1303\n",
      "Training Epoch 2  47.2% | batch:       364 of       772\t|\tloss: 9.34896\n",
      "Training Epoch 2  47.3% | batch:       365 of       772\t|\tloss: 10.5228\n",
      "Training Epoch 2  47.4% | batch:       366 of       772\t|\tloss: 8.19429\n",
      "Training Epoch 2  47.5% | batch:       367 of       772\t|\tloss: 8.173\n",
      "Training Epoch 2  47.7% | batch:       368 of       772\t|\tloss: 9.95934\n",
      "Training Epoch 2  47.8% | batch:       369 of       772\t|\tloss: 7.93192\n",
      "Training Epoch 2  47.9% | batch:       370 of       772\t|\tloss: 9.50557\n",
      "Training Epoch 2  48.1% | batch:       371 of       772\t|\tloss: 8.8249\n",
      "Training Epoch 2  48.2% | batch:       372 of       772\t|\tloss: 8.52954\n",
      "Training Epoch 2  48.3% | batch:       373 of       772\t|\tloss: 7.60457\n",
      "Training Epoch 2  48.4% | batch:       374 of       772\t|\tloss: 7.92372\n",
      "Training Epoch 2  48.6% | batch:       375 of       772\t|\tloss: 9.70249\n",
      "Training Epoch 2  48.7% | batch:       376 of       772\t|\tloss: 9.42617\n",
      "Training Epoch 2  48.8% | batch:       377 of       772\t|\tloss: 9.53706\n",
      "Training Epoch 2  49.0% | batch:       378 of       772\t|\tloss: 7.75163\n",
      "Training Epoch 2  49.1% | batch:       379 of       772\t|\tloss: 10.925\n",
      "Training Epoch 2  49.2% | batch:       380 of       772\t|\tloss: 10.4901\n",
      "Training Epoch 2  49.4% | batch:       381 of       772\t|\tloss: 10.2605\n",
      "Training Epoch 2  49.5% | batch:       382 of       772\t|\tloss: 9.43172\n",
      "Training Epoch 2  49.6% | batch:       383 of       772\t|\tloss: 8.66645\n",
      "Training Epoch 2  49.7% | batch:       384 of       772\t|\tloss: 9.71505\n",
      "Training Epoch 2  49.9% | batch:       385 of       772\t|\tloss: 6.5418\n",
      "Training Epoch 2  50.0% | batch:       386 of       772\t|\tloss: 9.93843\n",
      "Training Epoch 2  50.1% | batch:       387 of       772\t|\tloss: 9.42102\n",
      "Training Epoch 2  50.3% | batch:       388 of       772\t|\tloss: 7.63053\n",
      "Training Epoch 2  50.4% | batch:       389 of       772\t|\tloss: 8.23347\n",
      "Training Epoch 2  50.5% | batch:       390 of       772\t|\tloss: 8.73726\n",
      "Training Epoch 2  50.6% | batch:       391 of       772\t|\tloss: 7.67319\n",
      "Training Epoch 2  50.8% | batch:       392 of       772\t|\tloss: 8.90017\n",
      "Training Epoch 2  50.9% | batch:       393 of       772\t|\tloss: 7.61807\n",
      "Training Epoch 2  51.0% | batch:       394 of       772\t|\tloss: 9.33529\n",
      "Training Epoch 2  51.2% | batch:       395 of       772\t|\tloss: 8.75269\n",
      "Training Epoch 2  51.3% | batch:       396 of       772\t|\tloss: 12.2868\n",
      "Training Epoch 2  51.4% | batch:       397 of       772\t|\tloss: 8.38741\n",
      "Training Epoch 2  51.6% | batch:       398 of       772\t|\tloss: 8.73413\n",
      "Training Epoch 2  51.7% | batch:       399 of       772\t|\tloss: 9.14005\n",
      "Training Epoch 2  51.8% | batch:       400 of       772\t|\tloss: 8.31861\n",
      "Training Epoch 2  51.9% | batch:       401 of       772\t|\tloss: 7.0957\n",
      "Training Epoch 2  52.1% | batch:       402 of       772\t|\tloss: 8.24085\n",
      "Training Epoch 2  52.2% | batch:       403 of       772\t|\tloss: 9.13351\n",
      "Training Epoch 2  52.3% | batch:       404 of       772\t|\tloss: 7.39817\n",
      "Training Epoch 2  52.5% | batch:       405 of       772\t|\tloss: 9.37856\n",
      "Training Epoch 2  52.6% | batch:       406 of       772\t|\tloss: 7.9185\n",
      "Training Epoch 2  52.7% | batch:       407 of       772\t|\tloss: 8.00309\n",
      "Training Epoch 2  52.8% | batch:       408 of       772\t|\tloss: 7.27864\n",
      "Training Epoch 2  53.0% | batch:       409 of       772\t|\tloss: 8.21354\n",
      "Training Epoch 2  53.1% | batch:       410 of       772\t|\tloss: 7.11499\n",
      "Training Epoch 2  53.2% | batch:       411 of       772\t|\tloss: 8.40913\n",
      "Training Epoch 2  53.4% | batch:       412 of       772\t|\tloss: 9.21132\n",
      "Training Epoch 2  53.5% | batch:       413 of       772\t|\tloss: 7.70803\n",
      "Training Epoch 2  53.6% | batch:       414 of       772\t|\tloss: 9.69907\n",
      "Training Epoch 2  53.8% | batch:       415 of       772\t|\tloss: 7.20924\n",
      "Training Epoch 2  53.9% | batch:       416 of       772\t|\tloss: 7.60919\n",
      "Training Epoch 2  54.0% | batch:       417 of       772\t|\tloss: 8.03198\n",
      "Training Epoch 2  54.1% | batch:       418 of       772\t|\tloss: 8.14749\n",
      "Training Epoch 2  54.3% | batch:       419 of       772\t|\tloss: 11.8784\n",
      "Training Epoch 2  54.4% | batch:       420 of       772\t|\tloss: 8.13729\n",
      "Training Epoch 2  54.5% | batch:       421 of       772\t|\tloss: 6.94023\n",
      "Training Epoch 2  54.7% | batch:       422 of       772\t|\tloss: 7.74276\n",
      "Training Epoch 2  54.8% | batch:       423 of       772\t|\tloss: 8.44676\n",
      "Training Epoch 2  54.9% | batch:       424 of       772\t|\tloss: 7.10954\n",
      "Training Epoch 2  55.1% | batch:       425 of       772\t|\tloss: 8.32809\n",
      "Training Epoch 2  55.2% | batch:       426 of       772\t|\tloss: 7.56197\n",
      "Training Epoch 2  55.3% | batch:       427 of       772\t|\tloss: 8.60365\n",
      "Training Epoch 2  55.4% | batch:       428 of       772\t|\tloss: 7.90991\n",
      "Training Epoch 2  55.6% | batch:       429 of       772\t|\tloss: 7.72558\n",
      "Training Epoch 2  55.7% | batch:       430 of       772\t|\tloss: 9.55753\n",
      "Training Epoch 2  55.8% | batch:       431 of       772\t|\tloss: 6.94184\n",
      "Training Epoch 2  56.0% | batch:       432 of       772\t|\tloss: 7.69404\n",
      "Training Epoch 2  56.1% | batch:       433 of       772\t|\tloss: 7.37927\n",
      "Training Epoch 2  56.2% | batch:       434 of       772\t|\tloss: 9.84336\n",
      "Training Epoch 2  56.3% | batch:       435 of       772\t|\tloss: 8.04558\n",
      "Training Epoch 2  56.5% | batch:       436 of       772\t|\tloss: 8.41269\n",
      "Training Epoch 2  56.6% | batch:       437 of       772\t|\tloss: 8.47052\n",
      "Training Epoch 2  56.7% | batch:       438 of       772\t|\tloss: 7.93149\n",
      "Training Epoch 2  56.9% | batch:       439 of       772\t|\tloss: 9.70356\n",
      "Training Epoch 2  57.0% | batch:       440 of       772\t|\tloss: 7.97499\n",
      "Training Epoch 2  57.1% | batch:       441 of       772\t|\tloss: 8.12654\n",
      "Training Epoch 2  57.3% | batch:       442 of       772\t|\tloss: 7.67645\n",
      "Training Epoch 2  57.4% | batch:       443 of       772\t|\tloss: 8.99962\n",
      "Training Epoch 2  57.5% | batch:       444 of       772\t|\tloss: 8.72692\n",
      "Training Epoch 2  57.6% | batch:       445 of       772\t|\tloss: 9.04154\n",
      "Training Epoch 2  57.8% | batch:       446 of       772\t|\tloss: 7.36817\n",
      "Training Epoch 2  57.9% | batch:       447 of       772\t|\tloss: 8.28135\n",
      "Training Epoch 2  58.0% | batch:       448 of       772\t|\tloss: 10.2085\n",
      "Training Epoch 2  58.2% | batch:       449 of       772\t|\tloss: 8.30777\n",
      "Training Epoch 2  58.3% | batch:       450 of       772\t|\tloss: 7.69774\n",
      "Training Epoch 2  58.4% | batch:       451 of       772\t|\tloss: 8.66481\n",
      "Training Epoch 2  58.5% | batch:       452 of       772\t|\tloss: 7.71295\n",
      "Training Epoch 2  58.7% | batch:       453 of       772\t|\tloss: 8.17265\n",
      "Training Epoch 2  58.8% | batch:       454 of       772\t|\tloss: 7.36665\n",
      "Training Epoch 2  58.9% | batch:       455 of       772\t|\tloss: 6.87647\n",
      "Training Epoch 2  59.1% | batch:       456 of       772\t|\tloss: 8.09447\n",
      "Training Epoch 2  59.2% | batch:       457 of       772\t|\tloss: 7.59652\n",
      "Training Epoch 2  59.3% | batch:       458 of       772\t|\tloss: 8.45164\n",
      "Training Epoch 2  59.5% | batch:       459 of       772\t|\tloss: 7.94056\n",
      "Training Epoch 2  59.6% | batch:       460 of       772\t|\tloss: 7.35337\n",
      "Training Epoch 2  59.7% | batch:       461 of       772\t|\tloss: 7.44338\n",
      "Training Epoch 2  59.8% | batch:       462 of       772\t|\tloss: 8.05012\n",
      "Training Epoch 2  60.0% | batch:       463 of       772\t|\tloss: 9.4091\n",
      "Training Epoch 2  60.1% | batch:       464 of       772\t|\tloss: 7.14239\n",
      "Training Epoch 2  60.2% | batch:       465 of       772\t|\tloss: 8.21746\n",
      "Training Epoch 2  60.4% | batch:       466 of       772\t|\tloss: 8.1547\n",
      "Training Epoch 2  60.5% | batch:       467 of       772\t|\tloss: 6.60704\n",
      "Training Epoch 2  60.6% | batch:       468 of       772\t|\tloss: 7.99165\n",
      "Training Epoch 2  60.8% | batch:       469 of       772\t|\tloss: 8.09735\n",
      "Training Epoch 2  60.9% | batch:       470 of       772\t|\tloss: 7.73849\n",
      "Training Epoch 2  61.0% | batch:       471 of       772\t|\tloss: 6.47423\n",
      "Training Epoch 2  61.1% | batch:       472 of       772\t|\tloss: 8.30516\n",
      "Training Epoch 2  61.3% | batch:       473 of       772\t|\tloss: 8.37311\n",
      "Training Epoch 2  61.4% | batch:       474 of       772\t|\tloss: 6.42295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  61.5% | batch:       475 of       772\t|\tloss: 8.42585\n",
      "Training Epoch 2  61.7% | batch:       476 of       772\t|\tloss: 8.79335\n",
      "Training Epoch 2  61.8% | batch:       477 of       772\t|\tloss: 9.03627\n",
      "Training Epoch 2  61.9% | batch:       478 of       772\t|\tloss: 7.22338\n",
      "Training Epoch 2  62.0% | batch:       479 of       772\t|\tloss: 7.20196\n",
      "Training Epoch 2  62.2% | batch:       480 of       772\t|\tloss: 7.45363\n",
      "Training Epoch 2  62.3% | batch:       481 of       772\t|\tloss: 7.17289\n",
      "Training Epoch 2  62.4% | batch:       482 of       772\t|\tloss: 8.13718\n",
      "Training Epoch 2  62.6% | batch:       483 of       772\t|\tloss: 6.09377\n",
      "Training Epoch 2  62.7% | batch:       484 of       772\t|\tloss: 8.27754\n",
      "Training Epoch 2  62.8% | batch:       485 of       772\t|\tloss: 7.39453\n",
      "Training Epoch 2  63.0% | batch:       486 of       772\t|\tloss: 7.43017\n",
      "Training Epoch 2  63.1% | batch:       487 of       772\t|\tloss: 7.25669\n",
      "Training Epoch 2  63.2% | batch:       488 of       772\t|\tloss: 6.8046\n",
      "Training Epoch 2  63.3% | batch:       489 of       772\t|\tloss: 7.4063\n",
      "Training Epoch 2  63.5% | batch:       490 of       772\t|\tloss: 9.34819\n",
      "Training Epoch 2  63.6% | batch:       491 of       772\t|\tloss: 7.30218\n",
      "Training Epoch 2  63.7% | batch:       492 of       772\t|\tloss: 6.45904\n",
      "Training Epoch 2  63.9% | batch:       493 of       772\t|\tloss: 8.83627\n",
      "Training Epoch 2  64.0% | batch:       494 of       772\t|\tloss: 7.05434\n",
      "Training Epoch 2  64.1% | batch:       495 of       772\t|\tloss: 6.87008\n",
      "Training Epoch 2  64.2% | batch:       496 of       772\t|\tloss: 7.00613\n",
      "Training Epoch 2  64.4% | batch:       497 of       772\t|\tloss: 6.69521\n",
      "Training Epoch 2  64.5% | batch:       498 of       772\t|\tloss: 7.32406\n",
      "Training Epoch 2  64.6% | batch:       499 of       772\t|\tloss: 6.46379\n",
      "Training Epoch 2  64.8% | batch:       500 of       772\t|\tloss: 6.47053\n",
      "Training Epoch 2  64.9% | batch:       501 of       772\t|\tloss: 7.09465\n",
      "Training Epoch 2  65.0% | batch:       502 of       772\t|\tloss: 8.22582\n",
      "Training Epoch 2  65.2% | batch:       503 of       772\t|\tloss: 7.02303\n",
      "Training Epoch 2  65.3% | batch:       504 of       772\t|\tloss: 6.32658\n",
      "Training Epoch 2  65.4% | batch:       505 of       772\t|\tloss: 7.18918\n",
      "Training Epoch 2  65.5% | batch:       506 of       772\t|\tloss: 8.3974\n",
      "Training Epoch 2  65.7% | batch:       507 of       772\t|\tloss: 7.22108\n",
      "Training Epoch 2  65.8% | batch:       508 of       772\t|\tloss: 6.88459\n",
      "Training Epoch 2  65.9% | batch:       509 of       772\t|\tloss: 6.60781\n",
      "Training Epoch 2  66.1% | batch:       510 of       772\t|\tloss: 5.69598\n",
      "Training Epoch 2  66.2% | batch:       511 of       772\t|\tloss: 6.64684\n",
      "Training Epoch 2  66.3% | batch:       512 of       772\t|\tloss: 7.7468\n",
      "Training Epoch 2  66.5% | batch:       513 of       772\t|\tloss: 8.17113\n",
      "Training Epoch 2  66.6% | batch:       514 of       772\t|\tloss: 7.61604\n",
      "Training Epoch 2  66.7% | batch:       515 of       772\t|\tloss: 6.99837\n",
      "Training Epoch 2  66.8% | batch:       516 of       772\t|\tloss: 6.71566\n",
      "Training Epoch 2  67.0% | batch:       517 of       772\t|\tloss: 6.96373\n",
      "Training Epoch 2  67.1% | batch:       518 of       772\t|\tloss: 6.10583\n",
      "Training Epoch 2  67.2% | batch:       519 of       772\t|\tloss: 6.95837\n",
      "Training Epoch 2  67.4% | batch:       520 of       772\t|\tloss: 6.35369\n",
      "Training Epoch 2  67.5% | batch:       521 of       772\t|\tloss: 6.1068\n",
      "Training Epoch 2  67.6% | batch:       522 of       772\t|\tloss: 7.8597\n",
      "Training Epoch 2  67.7% | batch:       523 of       772\t|\tloss: 6.50795\n",
      "Training Epoch 2  67.9% | batch:       524 of       772\t|\tloss: 7.90263\n",
      "Training Epoch 2  68.0% | batch:       525 of       772\t|\tloss: 6.82038\n",
      "Training Epoch 2  68.1% | batch:       526 of       772\t|\tloss: 7.89816\n",
      "Training Epoch 2  68.3% | batch:       527 of       772\t|\tloss: 8.17844\n",
      "Training Epoch 2  68.4% | batch:       528 of       772\t|\tloss: 6.64972\n",
      "Training Epoch 2  68.5% | batch:       529 of       772\t|\tloss: 7.38159\n",
      "Training Epoch 2  68.7% | batch:       530 of       772\t|\tloss: 8.563\n",
      "Training Epoch 2  68.8% | batch:       531 of       772\t|\tloss: 8.25951\n",
      "Training Epoch 2  68.9% | batch:       532 of       772\t|\tloss: 7.77981\n",
      "Training Epoch 2  69.0% | batch:       533 of       772\t|\tloss: 6.69405\n",
      "Training Epoch 2  69.2% | batch:       534 of       772\t|\tloss: 6.89451\n",
      "Training Epoch 2  69.3% | batch:       535 of       772\t|\tloss: 8.00248\n",
      "Training Epoch 2  69.4% | batch:       536 of       772\t|\tloss: 6.13808\n",
      "Training Epoch 2  69.6% | batch:       537 of       772\t|\tloss: 7.19031\n",
      "Training Epoch 2  69.7% | batch:       538 of       772\t|\tloss: 6.1321\n",
      "Training Epoch 2  69.8% | batch:       539 of       772\t|\tloss: 6.59925\n",
      "Training Epoch 2  69.9% | batch:       540 of       772\t|\tloss: 6.12176\n",
      "Training Epoch 2  70.1% | batch:       541 of       772\t|\tloss: 7.98155\n",
      "Training Epoch 2  70.2% | batch:       542 of       772\t|\tloss: 6.85572\n",
      "Training Epoch 2  70.3% | batch:       543 of       772\t|\tloss: 5.77605\n",
      "Training Epoch 2  70.5% | batch:       544 of       772\t|\tloss: 6.05937\n",
      "Training Epoch 2  70.6% | batch:       545 of       772\t|\tloss: 6.19176\n",
      "Training Epoch 2  70.7% | batch:       546 of       772\t|\tloss: 6.22783\n",
      "Training Epoch 2  70.9% | batch:       547 of       772\t|\tloss: 7.19242\n",
      "Training Epoch 2  71.0% | batch:       548 of       772\t|\tloss: 6.32028\n",
      "Training Epoch 2  71.1% | batch:       549 of       772\t|\tloss: 7.30325\n",
      "Training Epoch 2  71.2% | batch:       550 of       772\t|\tloss: 6.56183\n",
      "Training Epoch 2  71.4% | batch:       551 of       772\t|\tloss: 6.65218\n",
      "Training Epoch 2  71.5% | batch:       552 of       772\t|\tloss: 6.73385\n",
      "Training Epoch 2  71.6% | batch:       553 of       772\t|\tloss: 7.38578\n",
      "Training Epoch 2  71.8% | batch:       554 of       772\t|\tloss: 6.98144\n",
      "Training Epoch 2  71.9% | batch:       555 of       772\t|\tloss: 7.07808\n",
      "Training Epoch 2  72.0% | batch:       556 of       772\t|\tloss: 6.13438\n",
      "Training Epoch 2  72.2% | batch:       557 of       772\t|\tloss: 6.45907\n",
      "Training Epoch 2  72.3% | batch:       558 of       772\t|\tloss: 7.91934\n",
      "Training Epoch 2  72.4% | batch:       559 of       772\t|\tloss: 5.84188\n",
      "Training Epoch 2  72.5% | batch:       560 of       772\t|\tloss: 6.24007\n",
      "Training Epoch 2  72.7% | batch:       561 of       772\t|\tloss: 5.78936\n",
      "Training Epoch 2  72.8% | batch:       562 of       772\t|\tloss: 7.21211\n",
      "Training Epoch 2  72.9% | batch:       563 of       772\t|\tloss: 6.85188\n",
      "Training Epoch 2  73.1% | batch:       564 of       772\t|\tloss: 7.43345\n",
      "Training Epoch 2  73.2% | batch:       565 of       772\t|\tloss: 7.51016\n",
      "Training Epoch 2  73.3% | batch:       566 of       772\t|\tloss: 6.04107\n",
      "Training Epoch 2  73.4% | batch:       567 of       772\t|\tloss: 6.14575\n",
      "Training Epoch 2  73.6% | batch:       568 of       772\t|\tloss: 5.03018\n",
      "Training Epoch 2  73.7% | batch:       569 of       772\t|\tloss: 7.88167\n",
      "Training Epoch 2  73.8% | batch:       570 of       772\t|\tloss: 6.37343\n",
      "Training Epoch 2  74.0% | batch:       571 of       772\t|\tloss: 5.48025\n",
      "Training Epoch 2  74.1% | batch:       572 of       772\t|\tloss: 6.89458\n",
      "Training Epoch 2  74.2% | batch:       573 of       772\t|\tloss: 5.56364\n",
      "Training Epoch 2  74.4% | batch:       574 of       772\t|\tloss: 5.42144\n",
      "Training Epoch 2  74.5% | batch:       575 of       772\t|\tloss: 7.18307\n",
      "Training Epoch 2  74.6% | batch:       576 of       772\t|\tloss: 7.02919\n",
      "Training Epoch 2  74.7% | batch:       577 of       772\t|\tloss: 7.47103\n",
      "Training Epoch 2  74.9% | batch:       578 of       772\t|\tloss: 7.77427\n",
      "Training Epoch 2  75.0% | batch:       579 of       772\t|\tloss: 5.23332\n",
      "Training Epoch 2  75.1% | batch:       580 of       772\t|\tloss: 7.22836\n",
      "Training Epoch 2  75.3% | batch:       581 of       772\t|\tloss: 7.46986\n",
      "Training Epoch 2  75.4% | batch:       582 of       772\t|\tloss: 5.94921\n",
      "Training Epoch 2  75.5% | batch:       583 of       772\t|\tloss: 6.53326\n",
      "Training Epoch 2  75.6% | batch:       584 of       772\t|\tloss: 5.32801\n",
      "Training Epoch 2  75.8% | batch:       585 of       772\t|\tloss: 6.35512\n",
      "Training Epoch 2  75.9% | batch:       586 of       772\t|\tloss: 6.20613\n",
      "Training Epoch 2  76.0% | batch:       587 of       772\t|\tloss: 8.47832\n",
      "Training Epoch 2  76.2% | batch:       588 of       772\t|\tloss: 6.56275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  76.3% | batch:       589 of       772\t|\tloss: 6.08265\n",
      "Training Epoch 2  76.4% | batch:       590 of       772\t|\tloss: 6.16075\n",
      "Training Epoch 2  76.6% | batch:       591 of       772\t|\tloss: 7.2799\n",
      "Training Epoch 2  76.7% | batch:       592 of       772\t|\tloss: 7.72806\n",
      "Training Epoch 2  76.8% | batch:       593 of       772\t|\tloss: 8.20073\n",
      "Training Epoch 2  76.9% | batch:       594 of       772\t|\tloss: 6.80787\n",
      "Training Epoch 2  77.1% | batch:       595 of       772\t|\tloss: 6.03352\n",
      "Training Epoch 2  77.2% | batch:       596 of       772\t|\tloss: 7.07918\n",
      "Training Epoch 2  77.3% | batch:       597 of       772\t|\tloss: 6.76379\n",
      "Training Epoch 2  77.5% | batch:       598 of       772\t|\tloss: 6.94357\n",
      "Training Epoch 2  77.6% | batch:       599 of       772\t|\tloss: 5.59885\n",
      "Training Epoch 2  77.7% | batch:       600 of       772\t|\tloss: 6.92746\n",
      "Training Epoch 2  77.8% | batch:       601 of       772\t|\tloss: 5.04667\n",
      "Training Epoch 2  78.0% | batch:       602 of       772\t|\tloss: 6.78874\n",
      "Training Epoch 2  78.1% | batch:       603 of       772\t|\tloss: 6.48456\n",
      "Training Epoch 2  78.2% | batch:       604 of       772\t|\tloss: 5.80961\n",
      "Training Epoch 2  78.4% | batch:       605 of       772\t|\tloss: 5.62093\n",
      "Training Epoch 2  78.5% | batch:       606 of       772\t|\tloss: 6.28473\n",
      "Training Epoch 2  78.6% | batch:       607 of       772\t|\tloss: 7.30257\n",
      "Training Epoch 2  78.8% | batch:       608 of       772\t|\tloss: 5.66827\n",
      "Training Epoch 2  78.9% | batch:       609 of       772\t|\tloss: 6.5455\n",
      "Training Epoch 2  79.0% | batch:       610 of       772\t|\tloss: 6.91236\n",
      "Training Epoch 2  79.1% | batch:       611 of       772\t|\tloss: 6.36226\n",
      "Training Epoch 2  79.3% | batch:       612 of       772\t|\tloss: 6.20747\n",
      "Training Epoch 2  79.4% | batch:       613 of       772\t|\tloss: 7.9932\n",
      "Training Epoch 2  79.5% | batch:       614 of       772\t|\tloss: 5.84317\n",
      "Training Epoch 2  79.7% | batch:       615 of       772\t|\tloss: 6.18813\n",
      "Training Epoch 2  79.8% | batch:       616 of       772\t|\tloss: 6.75889\n",
      "Training Epoch 2  79.9% | batch:       617 of       772\t|\tloss: 6.60386\n",
      "Training Epoch 2  80.1% | batch:       618 of       772\t|\tloss: 6.342\n",
      "Training Epoch 2  80.2% | batch:       619 of       772\t|\tloss: 6.03239\n",
      "Training Epoch 2  80.3% | batch:       620 of       772\t|\tloss: 5.40239\n",
      "Training Epoch 2  80.4% | batch:       621 of       772\t|\tloss: 5.16562\n",
      "Training Epoch 2  80.6% | batch:       622 of       772\t|\tloss: 6.27215\n",
      "Training Epoch 2  80.7% | batch:       623 of       772\t|\tloss: 5.93449\n",
      "Training Epoch 2  80.8% | batch:       624 of       772\t|\tloss: 6.64932\n",
      "Training Epoch 2  81.0% | batch:       625 of       772\t|\tloss: 6.40307\n",
      "Training Epoch 2  81.1% | batch:       626 of       772\t|\tloss: 6.06884\n",
      "Training Epoch 2  81.2% | batch:       627 of       772\t|\tloss: 6.90868\n",
      "Training Epoch 2  81.3% | batch:       628 of       772\t|\tloss: 6.14\n",
      "Training Epoch 2  81.5% | batch:       629 of       772\t|\tloss: 5.37834\n",
      "Training Epoch 2  81.6% | batch:       630 of       772\t|\tloss: 6.0404\n",
      "Training Epoch 2  81.7% | batch:       631 of       772\t|\tloss: 4.95467\n",
      "Training Epoch 2  81.9% | batch:       632 of       772\t|\tloss: 6.4233\n",
      "Training Epoch 2  82.0% | batch:       633 of       772\t|\tloss: 7.06936\n",
      "Training Epoch 2  82.1% | batch:       634 of       772\t|\tloss: 5.68141\n",
      "Training Epoch 2  82.3% | batch:       635 of       772\t|\tloss: 6.03681\n",
      "Training Epoch 2  82.4% | batch:       636 of       772\t|\tloss: 6.18532\n",
      "Training Epoch 2  82.5% | batch:       637 of       772\t|\tloss: 5.79289\n",
      "Training Epoch 2  82.6% | batch:       638 of       772\t|\tloss: 6.34595\n",
      "Training Epoch 2  82.8% | batch:       639 of       772\t|\tloss: 4.88504\n",
      "Training Epoch 2  82.9% | batch:       640 of       772\t|\tloss: 6.20043\n",
      "Training Epoch 2  83.0% | batch:       641 of       772\t|\tloss: 6.43113\n",
      "Training Epoch 2  83.2% | batch:       642 of       772\t|\tloss: 5.60984\n",
      "Training Epoch 2  83.3% | batch:       643 of       772\t|\tloss: 5.42156\n",
      "Training Epoch 2  83.4% | batch:       644 of       772\t|\tloss: 5.32626\n",
      "Training Epoch 2  83.5% | batch:       645 of       772\t|\tloss: 6.32395\n",
      "Training Epoch 2  83.7% | batch:       646 of       772\t|\tloss: 6.03249\n",
      "Training Epoch 2  83.8% | batch:       647 of       772\t|\tloss: 6.5764\n",
      "Training Epoch 2  83.9% | batch:       648 of       772\t|\tloss: 6.17046\n",
      "Training Epoch 2  84.1% | batch:       649 of       772\t|\tloss: 5.38454\n",
      "Training Epoch 2  84.2% | batch:       650 of       772\t|\tloss: 5.19729\n",
      "Training Epoch 2  84.3% | batch:       651 of       772\t|\tloss: 5.84193\n",
      "Training Epoch 2  84.5% | batch:       652 of       772\t|\tloss: 6.74473\n",
      "Training Epoch 2  84.6% | batch:       653 of       772\t|\tloss: 5.18921\n",
      "Training Epoch 2  84.7% | batch:       654 of       772\t|\tloss: 7.51167\n",
      "Training Epoch 2  84.8% | batch:       655 of       772\t|\tloss: 5.02765\n",
      "Training Epoch 2  85.0% | batch:       656 of       772\t|\tloss: 6.5074\n",
      "Training Epoch 2  85.1% | batch:       657 of       772\t|\tloss: 4.71974\n",
      "Training Epoch 2  85.2% | batch:       658 of       772\t|\tloss: 5.5476\n",
      "Training Epoch 2  85.4% | batch:       659 of       772\t|\tloss: 5.19547\n",
      "Training Epoch 2  85.5% | batch:       660 of       772\t|\tloss: 5.61158\n",
      "Training Epoch 2  85.6% | batch:       661 of       772\t|\tloss: 4.79249\n",
      "Training Epoch 2  85.8% | batch:       662 of       772\t|\tloss: 4.44804\n",
      "Training Epoch 2  85.9% | batch:       663 of       772\t|\tloss: 5.95221\n",
      "Training Epoch 2  86.0% | batch:       664 of       772\t|\tloss: 6.50115\n",
      "Training Epoch 2  86.1% | batch:       665 of       772\t|\tloss: 4.83089\n",
      "Training Epoch 2  86.3% | batch:       666 of       772\t|\tloss: 5.80082\n",
      "Training Epoch 2  86.4% | batch:       667 of       772\t|\tloss: 4.36395\n",
      "Training Epoch 2  86.5% | batch:       668 of       772\t|\tloss: 5.11355\n",
      "Training Epoch 2  86.7% | batch:       669 of       772\t|\tloss: 5.82047\n",
      "Training Epoch 2  86.8% | batch:       670 of       772\t|\tloss: 6.2521\n",
      "Training Epoch 2  86.9% | batch:       671 of       772\t|\tloss: 4.84996\n",
      "Training Epoch 2  87.0% | batch:       672 of       772\t|\tloss: 5.4357\n",
      "Training Epoch 2  87.2% | batch:       673 of       772\t|\tloss: 6.02724\n",
      "Training Epoch 2  87.3% | batch:       674 of       772\t|\tloss: 6.15726\n",
      "Training Epoch 2  87.4% | batch:       675 of       772\t|\tloss: 5.44041\n",
      "Training Epoch 2  87.6% | batch:       676 of       772\t|\tloss: 6.90468\n",
      "Training Epoch 2  87.7% | batch:       677 of       772\t|\tloss: 5.67537\n",
      "Training Epoch 2  87.8% | batch:       678 of       772\t|\tloss: 5.65909\n",
      "Training Epoch 2  88.0% | batch:       679 of       772\t|\tloss: 5.97727\n",
      "Training Epoch 2  88.1% | batch:       680 of       772\t|\tloss: 5.58093\n",
      "Training Epoch 2  88.2% | batch:       681 of       772\t|\tloss: 5.94212\n",
      "Training Epoch 2  88.3% | batch:       682 of       772\t|\tloss: 7.51784\n",
      "Training Epoch 2  88.5% | batch:       683 of       772\t|\tloss: 7.02914\n",
      "Training Epoch 2  88.6% | batch:       684 of       772\t|\tloss: 6.98191\n",
      "Training Epoch 2  88.7% | batch:       685 of       772\t|\tloss: 6.43943\n",
      "Training Epoch 2  88.9% | batch:       686 of       772\t|\tloss: 6.04059\n",
      "Training Epoch 2  89.0% | batch:       687 of       772\t|\tloss: 5.42885\n",
      "Training Epoch 2  89.1% | batch:       688 of       772\t|\tloss: 5.46287\n",
      "Training Epoch 2  89.2% | batch:       689 of       772\t|\tloss: 4.93216\n",
      "Training Epoch 2  89.4% | batch:       690 of       772\t|\tloss: 5.40303\n",
      "Training Epoch 2  89.5% | batch:       691 of       772\t|\tloss: 5.81979\n",
      "Training Epoch 2  89.6% | batch:       692 of       772\t|\tloss: 4.04156\n",
      "Training Epoch 2  89.8% | batch:       693 of       772\t|\tloss: 5.45639\n",
      "Training Epoch 2  89.9% | batch:       694 of       772\t|\tloss: 4.81185\n",
      "Training Epoch 2  90.0% | batch:       695 of       772\t|\tloss: 6.38163\n",
      "Training Epoch 2  90.2% | batch:       696 of       772\t|\tloss: 5.72636\n",
      "Training Epoch 2  90.3% | batch:       697 of       772\t|\tloss: 6.08536\n",
      "Training Epoch 2  90.4% | batch:       698 of       772\t|\tloss: 5.52134\n",
      "Training Epoch 2  90.5% | batch:       699 of       772\t|\tloss: 5.9824\n",
      "Training Epoch 2  90.7% | batch:       700 of       772\t|\tloss: 6.57968\n",
      "Training Epoch 2  90.8% | batch:       701 of       772\t|\tloss: 5.22597\n",
      "Training Epoch 2  90.9% | batch:       702 of       772\t|\tloss: 5.0633\n",
      "Training Epoch 2  91.1% | batch:       703 of       772\t|\tloss: 5.23813\n",
      "Training Epoch 2  91.2% | batch:       704 of       772\t|\tloss: 4.47527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  91.3% | batch:       705 of       772\t|\tloss: 6.67863\n",
      "Training Epoch 2  91.5% | batch:       706 of       772\t|\tloss: 5.4698\n",
      "Training Epoch 2  91.6% | batch:       707 of       772\t|\tloss: 5.23781\n",
      "Training Epoch 2  91.7% | batch:       708 of       772\t|\tloss: 5.17688\n",
      "Training Epoch 2  91.8% | batch:       709 of       772\t|\tloss: 5.9516\n",
      "Training Epoch 2  92.0% | batch:       710 of       772\t|\tloss: 5.75266\n",
      "Training Epoch 2  92.1% | batch:       711 of       772\t|\tloss: 4.80153\n",
      "Training Epoch 2  92.2% | batch:       712 of       772\t|\tloss: 5.69847\n",
      "Training Epoch 2  92.4% | batch:       713 of       772\t|\tloss: 5.55257\n",
      "Training Epoch 2  92.5% | batch:       714 of       772\t|\tloss: 4.55424\n",
      "Training Epoch 2  92.6% | batch:       715 of       772\t|\tloss: 4.68561\n",
      "Training Epoch 2  92.7% | batch:       716 of       772\t|\tloss: 4.99061\n",
      "Training Epoch 2  92.9% | batch:       717 of       772\t|\tloss: 7.06981\n",
      "Training Epoch 2  93.0% | batch:       718 of       772\t|\tloss: 5.12672\n",
      "Training Epoch 2  93.1% | batch:       719 of       772\t|\tloss: 5.5525\n",
      "Training Epoch 2  93.3% | batch:       720 of       772\t|\tloss: 5.04929\n",
      "Training Epoch 2  93.4% | batch:       721 of       772\t|\tloss: 5.57905\n",
      "Training Epoch 2  93.5% | batch:       722 of       772\t|\tloss: 5.88621\n",
      "Training Epoch 2  93.7% | batch:       723 of       772\t|\tloss: 5.93754\n",
      "Training Epoch 2  93.8% | batch:       724 of       772\t|\tloss: 4.0767\n",
      "Training Epoch 2  93.9% | batch:       725 of       772\t|\tloss: 4.58613\n",
      "Training Epoch 2  94.0% | batch:       726 of       772\t|\tloss: 4.40957\n",
      "Training Epoch 2  94.2% | batch:       727 of       772\t|\tloss: 5.85439\n",
      "Training Epoch 2  94.3% | batch:       728 of       772\t|\tloss: 5.36943\n",
      "Training Epoch 2  94.4% | batch:       729 of       772\t|\tloss: 5.65155\n",
      "Training Epoch 2  94.6% | batch:       730 of       772\t|\tloss: 4.8974\n",
      "Training Epoch 2  94.7% | batch:       731 of       772\t|\tloss: 6.65367\n",
      "Training Epoch 2  94.8% | batch:       732 of       772\t|\tloss: 5.1415\n",
      "Training Epoch 2  94.9% | batch:       733 of       772\t|\tloss: 5.29317\n",
      "Training Epoch 2  95.1% | batch:       734 of       772\t|\tloss: 4.34421\n",
      "Training Epoch 2  95.2% | batch:       735 of       772\t|\tloss: 6.51961\n",
      "Training Epoch 2  95.3% | batch:       736 of       772\t|\tloss: 5.43233\n",
      "Training Epoch 2  95.5% | batch:       737 of       772\t|\tloss: 5.86453\n",
      "Training Epoch 2  95.6% | batch:       738 of       772\t|\tloss: 5.56315\n",
      "Training Epoch 2  95.7% | batch:       739 of       772\t|\tloss: 4.71877\n",
      "Training Epoch 2  95.9% | batch:       740 of       772\t|\tloss: 5.42151\n",
      "Training Epoch 2  96.0% | batch:       741 of       772\t|\tloss: 4.93768\n",
      "Training Epoch 2  96.1% | batch:       742 of       772\t|\tloss: 4.66944\n",
      "Training Epoch 2  96.2% | batch:       743 of       772\t|\tloss: 6.31417\n",
      "Training Epoch 2  96.4% | batch:       744 of       772\t|\tloss: 5.20129\n",
      "Training Epoch 2  96.5% | batch:       745 of       772\t|\tloss: 4.86723\n",
      "Training Epoch 2  96.6% | batch:       746 of       772\t|\tloss: 4.77297\n",
      "Training Epoch 2  96.8% | batch:       747 of       772\t|\tloss: 4.38565\n",
      "Training Epoch 2  96.9% | batch:       748 of       772\t|\tloss: 5.68198\n",
      "Training Epoch 2  97.0% | batch:       749 of       772\t|\tloss: 5.86928\n",
      "Training Epoch 2  97.2% | batch:       750 of       772\t|\tloss: 5.83755\n",
      "Training Epoch 2  97.3% | batch:       751 of       772\t|\tloss: 5.33177\n",
      "Training Epoch 2  97.4% | batch:       752 of       772\t|\tloss: 5.10567\n",
      "Training Epoch 2  97.5% | batch:       753 of       772\t|\tloss: 5.017\n",
      "Training Epoch 2  97.7% | batch:       754 of       772\t|\tloss: 4.09744\n",
      "Training Epoch 2  97.8% | batch:       755 of       772\t|\tloss: 5.40685\n",
      "Training Epoch 2  97.9% | batch:       756 of       772\t|\tloss: 5.82327\n",
      "Training Epoch 2  98.1% | batch:       757 of       772\t|\tloss: 4.10294\n",
      "Training Epoch 2  98.2% | batch:       758 of       772\t|\tloss: 5.25502\n",
      "Training Epoch 2  98.3% | batch:       759 of       772\t|\tloss: 5.42964\n",
      "Training Epoch 2  98.4% | batch:       760 of       772\t|\tloss: 3.7308\n",
      "Training Epoch 2  98.6% | batch:       761 of       772\t|\tloss: 4.34057\n",
      "Training Epoch 2  98.7% | batch:       762 of       772\t|\tloss: 4.83279\n",
      "Training Epoch 2  98.8% | batch:       763 of       772\t|\tloss: 4.50988\n",
      "Training Epoch 2  99.0% | batch:       764 of       772\t|\tloss: 4.35677\n",
      "Training Epoch 2  99.1% | batch:       765 of       772\t|\tloss: 3.89995\n",
      "Training Epoch 2  99.2% | batch:       766 of       772\t|\tloss: 4.80099\n",
      "Training Epoch 2  99.4% | batch:       767 of       772\t|\tloss: 4.31388\n",
      "Training Epoch 2  99.5% | batch:       768 of       772\t|\tloss: 4.75954\n",
      "Training Epoch 2  99.6% | batch:       769 of       772\t|\tloss: 5.11247\n",
      "Training Epoch 2  99.7% | batch:       770 of       772\t|\tloss: 4.60239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:20:43,840 | INFO : Epoch 2 Training Summary: epoch: 2.000000 | loss: 9.436957 | \n",
      "2023-05-24 10:20:43,841 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 16.534754753112793 seconds\n",
      "\n",
      "2023-05-24 10:20:43,841 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 16.874223470687866 seconds\n",
      "2023-05-24 10:20:43,842 | INFO : Avg batch train. time: 0.02185780242317081 seconds\n",
      "2023-05-24 10:20:43,842 | INFO : Avg sample train. time: 0.00017076753770404869 seconds\n",
      "2023-05-24 10:20:43,843 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  99.9% | batch:       771 of       772\t|\tloss: 5.37018\n",
      "\n",
      "Evaluating Epoch 2   0.0% | batch:         0 of        92\t|\tloss: 1.09174\n",
      "Evaluating Epoch 2   1.1% | batch:         1 of        92\t|\tloss: 11.0311\n",
      "Evaluating Epoch 2   2.2% | batch:         2 of        92\t|\tloss: 3.30502\n",
      "Evaluating Epoch 2   3.3% | batch:         3 of        92\t|\tloss: 7.74247\n",
      "Evaluating Epoch 2   4.3% | batch:         4 of        92\t|\tloss: 4.2232\n",
      "Evaluating Epoch 2   5.4% | batch:         5 of        92\t|\tloss: 9.77361\n",
      "Evaluating Epoch 2   6.5% | batch:         6 of        92\t|\tloss: 4.66967\n",
      "Evaluating Epoch 2   7.6% | batch:         7 of        92\t|\tloss: 3.01685\n",
      "Evaluating Epoch 2   8.7% | batch:         8 of        92\t|\tloss: 8.84558\n",
      "Evaluating Epoch 2   9.8% | batch:         9 of        92\t|\tloss: 4.99276\n",
      "Evaluating Epoch 2  10.9% | batch:        10 of        92\t|\tloss: 7.3583\n",
      "Evaluating Epoch 2  12.0% | batch:        11 of        92\t|\tloss: 4.41407\n",
      "Evaluating Epoch 2  13.0% | batch:        12 of        92\t|\tloss: 9.1638\n",
      "Evaluating Epoch 2  14.1% | batch:        13 of        92\t|\tloss: 6.09971\n",
      "Evaluating Epoch 2  15.2% | batch:        14 of        92\t|\tloss: 2.54404\n",
      "Evaluating Epoch 2  16.3% | batch:        15 of        92\t|\tloss: 0.792782\n",
      "Evaluating Epoch 2  17.4% | batch:        16 of        92\t|\tloss: 2.911\n",
      "Evaluating Epoch 2  18.5% | batch:        17 of        92\t|\tloss: 1.66986\n",
      "Evaluating Epoch 2  19.6% | batch:        18 of        92\t|\tloss: 5.83594\n",
      "Evaluating Epoch 2  20.7% | batch:        19 of        92\t|\tloss: 6.06722\n",
      "Evaluating Epoch 2  21.7% | batch:        20 of        92\t|\tloss: 4.88468\n",
      "Evaluating Epoch 2  22.8% | batch:        21 of        92\t|\tloss: 4.28685\n",
      "Evaluating Epoch 2  23.9% | batch:        22 of        92\t|\tloss: 7.0156\n",
      "Evaluating Epoch 2  25.0% | batch:        23 of        92\t|\tloss: 6.99442\n",
      "Evaluating Epoch 2  26.1% | batch:        24 of        92\t|\tloss: 2.83096\n",
      "Evaluating Epoch 2  27.2% | batch:        25 of        92\t|\tloss: 0.316866\n",
      "Evaluating Epoch 2  28.3% | batch:        26 of        92\t|\tloss: 1.43258\n",
      "Evaluating Epoch 2  29.3% | batch:        27 of        92\t|\tloss: 3.21643\n",
      "Evaluating Epoch 2  30.4% | batch:        28 of        92\t|\tloss: 3.2799\n",
      "Evaluating Epoch 2  31.5% | batch:        29 of        92\t|\tloss: 5.04398\n",
      "Evaluating Epoch 2  32.6% | batch:        30 of        92\t|\tloss: 3.119\n",
      "Evaluating Epoch 2  33.7% | batch:        31 of        92\t|\tloss: 5.83327\n",
      "Evaluating Epoch 2  34.8% | batch:        32 of        92\t|\tloss: 3.45374\n",
      "Evaluating Epoch 2  35.9% | batch:        33 of        92\t|\tloss: 7.21069\n",
      "Evaluating Epoch 2  37.0% | batch:        34 of        92\t|\tloss: 4.13434\n",
      "Evaluating Epoch 2  38.0% | batch:        35 of        92\t|\tloss: 3.46197\n",
      "Evaluating Epoch 2  39.1% | batch:        36 of        92\t|\tloss: 1.59466\n",
      "Evaluating Epoch 2  40.2% | batch:        37 of        92\t|\tloss: 2.72472\n",
      "Evaluating Epoch 2  41.3% | batch:        38 of        92\t|\tloss: 3.64547\n",
      "Evaluating Epoch 2  42.4% | batch:        39 of        92\t|\tloss: 8.90789\n",
      "Evaluating Epoch 2  43.5% | batch:        40 of        92\t|\tloss: 4.20247\n",
      "Evaluating Epoch 2  44.6% | batch:        41 of        92\t|\tloss: 5.99032\n",
      "Evaluating Epoch 2  45.7% | batch:        42 of        92\t|\tloss: 5.66052\n",
      "Evaluating Epoch 2  46.7% | batch:        43 of        92\t|\tloss: 8.27792\n",
      "Evaluating Epoch 2  47.8% | batch:        44 of        92\t|\tloss: 2.39732\n",
      "Evaluating Epoch 2  48.9% | batch:        45 of        92\t|\tloss: 2.17195\n",
      "Evaluating Epoch 2  50.0% | batch:        46 of        92\t|\tloss: 1.94401\n",
      "Evaluating Epoch 2  51.1% | batch:        47 of        92\t|\tloss: 6.28054\n",
      "Evaluating Epoch 2  52.2% | batch:        48 of        92\t|\tloss: 6.91947\n",
      "Evaluating Epoch 2  53.3% | batch:        49 of        92\t|\tloss: 5.86101\n",
      "Evaluating Epoch 2  54.3% | batch:        50 of        92\t|\tloss: 5.44779\n",
      "Evaluating Epoch 2  55.4% | batch:        51 of        92\t|\tloss: 8.04323\n",
      "Evaluating Epoch 2  56.5% | batch:        52 of        92\t|\tloss: 6.96801\n",
      "Evaluating Epoch 2  57.6% | batch:        53 of        92\t|\tloss: 2.33632\n",
      "Evaluating Epoch 2  58.7% | batch:        54 of        92\t|\tloss: 1.99114\n",
      "Evaluating Epoch 2  59.8% | batch:        55 of        92\t|\tloss: 7.31918\n",
      "Evaluating Epoch 2  60.9% | batch:        56 of        92\t|\tloss: 7.41838\n",
      "Evaluating Epoch 2  62.0% | batch:        57 of        92\t|\tloss: 6.17328\n",
      "Evaluating Epoch 2  63.0% | batch:        58 of        92\t|\tloss: 4.97973\n",
      "Evaluating Epoch 2  64.1% | batch:        59 of        92\t|\tloss: 8.13682\n",
      "Evaluating Epoch 2  65.2% | batch:        60 of        92\t|\tloss: 7.00002\n",
      "Evaluating Epoch 2  66.3% | batch:        61 of        92\t|\tloss: 2.60861\n",
      "Evaluating Epoch 2  67.4% | batch:        62 of        92\t|\tloss: 0.560486\n",
      "Evaluating Epoch 2  68.5% | batch:        63 of        92\t|\tloss: 1.51442\n",
      "Evaluating Epoch 2  69.6% | batch:        64 of        92\t|\tloss: 3.97479\n",
      "Evaluating Epoch 2  70.7% | batch:        65 of        92\t|\tloss: 7.2753\n",
      "Evaluating Epoch 2  71.7% | batch:        66 of        92\t|\tloss: 4.75234\n",
      "Evaluating Epoch 2  72.8% | batch:        67 of        92\t|\tloss: 4.43662\n",
      "Evaluating Epoch 2  73.9% | batch:        68 of        92\t|\tloss: 6.47988\n",
      "Evaluating Epoch 2  75.0% | batch:        69 of        92\t|\tloss: 4.05789\n",
      "Evaluating Epoch 2  76.1% | batch:        70 of        92\t|\tloss: 7.96856\n",
      "Evaluating Epoch 2  77.2% | batch:        71 of        92\t|\tloss: 3.85736\n",
      "Evaluating Epoch 2  78.3% | batch:        72 of        92\t|\tloss: 4.37665\n",
      "Evaluating Epoch 2  79.3% | batch:        73 of        92\t|\tloss: 2.14661\n",
      "Evaluating Epoch 2  80.4% | batch:        74 of        92\t|\tloss: 4.47169\n",
      "Evaluating Epoch 2  81.5% | batch:        75 of        92\t|\tloss: 1.58341\n",
      "Evaluating Epoch 2  82.6% | batch:        76 of        92\t|\tloss: 5.75641\n",
      "Evaluating Epoch 2  83.7% | batch:        77 of        92\t|\tloss: 5.79418\n",
      "Evaluating Epoch 2  84.8% | batch:        78 of        92\t|\tloss: 6.18526\n",
      "Evaluating Epoch 2  85.9% | batch:        79 of        92\t|\tloss: 5.38292\n",
      "Evaluating Epoch 2  87.0% | batch:        80 of        92\t|\tloss: 6.69056\n",
      "Evaluating Epoch 2  88.0% | batch:        81 of        92\t|\tloss: 6.01045\n",
      "Evaluating Epoch 2  89.1% | batch:        82 of        92\t|\tloss: 2.25146\n",
      "Evaluating Epoch 2  90.2% | batch:        83 of        92\t|\tloss: 1.74703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:20:45,177 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.3336706161499023 seconds\n",
      "\n",
      "2023-05-24 10:20:45,177 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.4995388984680176 seconds\n",
      "2023-05-24 10:20:45,178 | INFO : Avg batch val. time: 0.016299335852913235 seconds\n",
      "2023-05-24 10:20:45,178 | INFO : Avg sample val. time: 0.00012853925068301197 seconds\n",
      "2023-05-24 10:20:45,179 | INFO : Epoch 2 Validation Summary: epoch: 2.000000 | loss: 4.880551 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 2  91.3% | batch:        84 of        92\t|\tloss: 6.09652\n",
      "Evaluating Epoch 2  92.4% | batch:        85 of        92\t|\tloss: 6.7915\n",
      "Evaluating Epoch 2  93.5% | batch:        86 of        92\t|\tloss: 5.81603\n",
      "Evaluating Epoch 2  94.6% | batch:        87 of        92\t|\tloss: 4.73181\n",
      "Evaluating Epoch 2  95.7% | batch:        88 of        92\t|\tloss: 7.30495\n",
      "Evaluating Epoch 2  96.7% | batch:        89 of        92\t|\tloss: 6.97017\n",
      "Evaluating Epoch 2  97.8% | batch:        90 of        92\t|\tloss: 2.61045\n",
      "Evaluating Epoch 2  98.9% | batch:        91 of        92\t|\tloss: 1.1097\n",
      "\n",
      "Training Epoch 3   0.0% | batch:         0 of       772\t|\tloss: 4.04506\n",
      "Training Epoch 3   0.1% | batch:         1 of       772\t|\tloss: 4.97256\n",
      "Training Epoch 3   0.3% | batch:         2 of       772\t|\tloss: 4.49818\n",
      "Training Epoch 3   0.4% | batch:         3 of       772\t|\tloss: 5.30513\n",
      "Training Epoch 3   0.5% | batch:         4 of       772\t|\tloss: 4.63539\n",
      "Training Epoch 3   0.6% | batch:         5 of       772\t|\tloss: 4.02099\n",
      "Training Epoch 3   0.8% | batch:         6 of       772\t|\tloss: 5.69378\n",
      "Training Epoch 3   0.9% | batch:         7 of       772\t|\tloss: 5.02407\n",
      "Training Epoch 3   1.0% | batch:         8 of       772\t|\tloss: 5.20483\n",
      "Training Epoch 3   1.2% | batch:         9 of       772\t|\tloss: 4.57601\n",
      "Training Epoch 3   1.3% | batch:        10 of       772\t|\tloss: 4.53169\n",
      "Training Epoch 3   1.4% | batch:        11 of       772\t|\tloss: 5.11409\n",
      "Training Epoch 3   1.6% | batch:        12 of       772\t|\tloss: 4.27687\n",
      "Training Epoch 3   1.7% | batch:        13 of       772\t|\tloss: 4.53407\n",
      "Training Epoch 3   1.8% | batch:        14 of       772\t|\tloss: 4.42872\n",
      "Training Epoch 3   1.9% | batch:        15 of       772\t|\tloss: 3.62771\n",
      "Training Epoch 3   2.1% | batch:        16 of       772\t|\tloss: 4.22463\n",
      "Training Epoch 3   2.2% | batch:        17 of       772\t|\tloss: 4.86652\n",
      "Training Epoch 3   2.3% | batch:        18 of       772\t|\tloss: 4.77997\n",
      "Training Epoch 3   2.5% | batch:        19 of       772\t|\tloss: 5.15415\n",
      "Training Epoch 3   2.6% | batch:        20 of       772\t|\tloss: 5.82064\n",
      "Training Epoch 3   2.7% | batch:        21 of       772\t|\tloss: 5.76763\n",
      "Training Epoch 3   2.8% | batch:        22 of       772\t|\tloss: 4.73483\n",
      "Training Epoch 3   3.0% | batch:        23 of       772\t|\tloss: 5.31911\n",
      "Training Epoch 3   3.1% | batch:        24 of       772\t|\tloss: 5.04157\n",
      "Training Epoch 3   3.2% | batch:        25 of       772\t|\tloss: 5.07884\n",
      "Training Epoch 3   3.4% | batch:        26 of       772\t|\tloss: 4.25232\n",
      "Training Epoch 3   3.5% | batch:        27 of       772\t|\tloss: 4.20104\n",
      "Training Epoch 3   3.6% | batch:        28 of       772\t|\tloss: 3.84028\n",
      "Training Epoch 3   3.8% | batch:        29 of       772\t|\tloss: 5.00641\n",
      "Training Epoch 3   3.9% | batch:        30 of       772\t|\tloss: 4.87043\n",
      "Training Epoch 3   4.0% | batch:        31 of       772\t|\tloss: 5.53837\n",
      "Training Epoch 3   4.1% | batch:        32 of       772\t|\tloss: 3.99992\n",
      "Training Epoch 3   4.3% | batch:        33 of       772\t|\tloss: 4.76411\n",
      "Training Epoch 3   4.4% | batch:        34 of       772\t|\tloss: 4.95419\n",
      "Training Epoch 3   4.5% | batch:        35 of       772\t|\tloss: 5.34252\n",
      "Training Epoch 3   4.7% | batch:        36 of       772\t|\tloss: 3.74292\n",
      "Training Epoch 3   4.8% | batch:        37 of       772\t|\tloss: 4.41415\n",
      "Training Epoch 3   4.9% | batch:        38 of       772\t|\tloss: 4.90425\n",
      "Training Epoch 3   5.1% | batch:        39 of       772\t|\tloss: 3.94893\n",
      "Training Epoch 3   5.2% | batch:        40 of       772\t|\tloss: 4.76736\n",
      "Training Epoch 3   5.3% | batch:        41 of       772\t|\tloss: 4.64283\n",
      "Training Epoch 3   5.4% | batch:        42 of       772\t|\tloss: 3.6767\n",
      "Training Epoch 3   5.6% | batch:        43 of       772\t|\tloss: 5.4469\n",
      "Training Epoch 3   5.7% | batch:        44 of       772\t|\tloss: 4.85139\n",
      "Training Epoch 3   5.8% | batch:        45 of       772\t|\tloss: 4.21695\n",
      "Training Epoch 3   6.0% | batch:        46 of       772\t|\tloss: 4.30566\n",
      "Training Epoch 3   6.1% | batch:        47 of       772\t|\tloss: 3.47818\n",
      "Training Epoch 3   6.2% | batch:        48 of       772\t|\tloss: 5.21464\n",
      "Training Epoch 3   6.3% | batch:        49 of       772\t|\tloss: 4.75625\n",
      "Training Epoch 3   6.5% | batch:        50 of       772\t|\tloss: 3.87911\n",
      "Training Epoch 3   6.6% | batch:        51 of       772\t|\tloss: 4.52439\n",
      "Training Epoch 3   6.7% | batch:        52 of       772\t|\tloss: 4.3354\n",
      "Training Epoch 3   6.9% | batch:        53 of       772\t|\tloss: 4.109\n",
      "Training Epoch 3   7.0% | batch:        54 of       772\t|\tloss: 4.29341\n",
      "Training Epoch 3   7.1% | batch:        55 of       772\t|\tloss: 5.35057\n",
      "Training Epoch 3   7.3% | batch:        56 of       772\t|\tloss: 4.53109\n",
      "Training Epoch 3   7.4% | batch:        57 of       772\t|\tloss: 3.55958\n",
      "Training Epoch 3   7.5% | batch:        58 of       772\t|\tloss: 4.29546\n",
      "Training Epoch 3   7.6% | batch:        59 of       772\t|\tloss: 4.2736\n",
      "Training Epoch 3   7.8% | batch:        60 of       772\t|\tloss: 4.45453\n",
      "Training Epoch 3   7.9% | batch:        61 of       772\t|\tloss: 3.39674\n",
      "Training Epoch 3   8.0% | batch:        62 of       772\t|\tloss: 3.67738\n",
      "Training Epoch 3   8.2% | batch:        63 of       772\t|\tloss: 4.85916\n",
      "Training Epoch 3   8.3% | batch:        64 of       772\t|\tloss: 5.41384\n",
      "Training Epoch 3   8.4% | batch:        65 of       772\t|\tloss: 3.85184\n",
      "Training Epoch 3   8.5% | batch:        66 of       772\t|\tloss: 5.27215\n",
      "Training Epoch 3   8.7% | batch:        67 of       772\t|\tloss: 3.01578\n",
      "Training Epoch 3   8.8% | batch:        68 of       772\t|\tloss: 4.10376\n",
      "Training Epoch 3   8.9% | batch:        69 of       772\t|\tloss: 4.17522\n",
      "Training Epoch 3   9.1% | batch:        70 of       772\t|\tloss: 4.17554\n",
      "Training Epoch 3   9.2% | batch:        71 of       772\t|\tloss: 4.41491\n",
      "Training Epoch 3   9.3% | batch:        72 of       772\t|\tloss: 5.10902\n",
      "Training Epoch 3   9.5% | batch:        73 of       772\t|\tloss: 4.90504\n",
      "Training Epoch 3   9.6% | batch:        74 of       772\t|\tloss: 4.29482\n",
      "Training Epoch 3   9.7% | batch:        75 of       772\t|\tloss: 3.53556\n",
      "Training Epoch 3   9.8% | batch:        76 of       772\t|\tloss: 4.60073\n",
      "Training Epoch 3  10.0% | batch:        77 of       772\t|\tloss: 3.89194\n",
      "Training Epoch 3  10.1% | batch:        78 of       772\t|\tloss: 3.78914\n",
      "Training Epoch 3  10.2% | batch:        79 of       772\t|\tloss: 3.37111\n",
      "Training Epoch 3  10.4% | batch:        80 of       772\t|\tloss: 4.408\n",
      "Training Epoch 3  10.5% | batch:        81 of       772\t|\tloss: 4.29049\n",
      "Training Epoch 3  10.6% | batch:        82 of       772\t|\tloss: 4.75783\n",
      "Training Epoch 3  10.8% | batch:        83 of       772\t|\tloss: 4.76851\n",
      "Training Epoch 3  10.9% | batch:        84 of       772\t|\tloss: 3.52234\n",
      "Training Epoch 3  11.0% | batch:        85 of       772\t|\tloss: 4.06127\n",
      "Training Epoch 3  11.1% | batch:        86 of       772\t|\tloss: 4.59776\n",
      "Training Epoch 3  11.3% | batch:        87 of       772\t|\tloss: 4.66564\n",
      "Training Epoch 3  11.4% | batch:        88 of       772\t|\tloss: 3.39599\n",
      "Training Epoch 3  11.5% | batch:        89 of       772\t|\tloss: 4.79421\n",
      "Training Epoch 3  11.7% | batch:        90 of       772\t|\tloss: 4.43269\n",
      "Training Epoch 3  11.8% | batch:        91 of       772\t|\tloss: 5.00005\n",
      "Training Epoch 3  11.9% | batch:        92 of       772\t|\tloss: 4.14897\n",
      "Training Epoch 3  12.0% | batch:        93 of       772\t|\tloss: 4.25409\n",
      "Training Epoch 3  12.2% | batch:        94 of       772\t|\tloss: 3.66402\n",
      "Training Epoch 3  12.3% | batch:        95 of       772\t|\tloss: 5.04116\n",
      "Training Epoch 3  12.4% | batch:        96 of       772\t|\tloss: 4.48583\n",
      "Training Epoch 3  12.6% | batch:        97 of       772\t|\tloss: 4.21524\n",
      "Training Epoch 3  12.7% | batch:        98 of       772\t|\tloss: 4.72839\n",
      "Training Epoch 3  12.8% | batch:        99 of       772\t|\tloss: 3.89028\n",
      "Training Epoch 3  13.0% | batch:       100 of       772\t|\tloss: 3.74603\n",
      "Training Epoch 3  13.1% | batch:       101 of       772\t|\tloss: 4.19966\n",
      "Training Epoch 3  13.2% | batch:       102 of       772\t|\tloss: 4.19872\n",
      "Training Epoch 3  13.3% | batch:       103 of       772\t|\tloss: 3.43549\n",
      "Training Epoch 3  13.5% | batch:       104 of       772\t|\tloss: 4.09954\n",
      "Training Epoch 3  13.6% | batch:       105 of       772\t|\tloss: 4.02037\n",
      "Training Epoch 3  13.7% | batch:       106 of       772\t|\tloss: 3.73868\n",
      "Training Epoch 3  13.9% | batch:       107 of       772\t|\tloss: 3.46732\n",
      "Training Epoch 3  14.0% | batch:       108 of       772\t|\tloss: 4.04342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  14.1% | batch:       109 of       772\t|\tloss: 4.50286\n",
      "Training Epoch 3  14.2% | batch:       110 of       772\t|\tloss: 4.25922\n",
      "Training Epoch 3  14.4% | batch:       111 of       772\t|\tloss: 3.99366\n",
      "Training Epoch 3  14.5% | batch:       112 of       772\t|\tloss: 5.18846\n",
      "Training Epoch 3  14.6% | batch:       113 of       772\t|\tloss: 3.52126\n",
      "Training Epoch 3  14.8% | batch:       114 of       772\t|\tloss: 3.70686\n",
      "Training Epoch 3  14.9% | batch:       115 of       772\t|\tloss: 3.35951\n",
      "Training Epoch 3  15.0% | batch:       116 of       772\t|\tloss: 3.8046\n",
      "Training Epoch 3  15.2% | batch:       117 of       772\t|\tloss: 3.69761\n",
      "Training Epoch 3  15.3% | batch:       118 of       772\t|\tloss: 3.72032\n",
      "Training Epoch 3  15.4% | batch:       119 of       772\t|\tloss: 4.13123\n",
      "Training Epoch 3  15.5% | batch:       120 of       772\t|\tloss: 3.5803\n",
      "Training Epoch 3  15.7% | batch:       121 of       772\t|\tloss: 4.10852\n",
      "Training Epoch 3  15.8% | batch:       122 of       772\t|\tloss: 3.60651\n",
      "Training Epoch 3  15.9% | batch:       123 of       772\t|\tloss: 4.53436\n",
      "Training Epoch 3  16.1% | batch:       124 of       772\t|\tloss: 4.59263\n",
      "Training Epoch 3  16.2% | batch:       125 of       772\t|\tloss: 4.32378\n",
      "Training Epoch 3  16.3% | batch:       126 of       772\t|\tloss: 3.48503\n",
      "Training Epoch 3  16.5% | batch:       127 of       772\t|\tloss: 4.97372\n",
      "Training Epoch 3  16.6% | batch:       128 of       772\t|\tloss: 3.8823\n",
      "Training Epoch 3  16.7% | batch:       129 of       772\t|\tloss: 3.49756\n",
      "Training Epoch 3  16.8% | batch:       130 of       772\t|\tloss: 3.20133\n",
      "Training Epoch 3  17.0% | batch:       131 of       772\t|\tloss: 4.13739\n",
      "Training Epoch 3  17.1% | batch:       132 of       772\t|\tloss: 3.70931\n",
      "Training Epoch 3  17.2% | batch:       133 of       772\t|\tloss: 3.52894\n",
      "Training Epoch 3  17.4% | batch:       134 of       772\t|\tloss: 3.6879\n",
      "Training Epoch 3  17.5% | batch:       135 of       772\t|\tloss: 3.15384\n",
      "Training Epoch 3  17.6% | batch:       136 of       772\t|\tloss: 4.09853\n",
      "Training Epoch 3  17.7% | batch:       137 of       772\t|\tloss: 4.0839\n",
      "Training Epoch 3  17.9% | batch:       138 of       772\t|\tloss: 3.19861\n",
      "Training Epoch 3  18.0% | batch:       139 of       772\t|\tloss: 3.56716\n",
      "Training Epoch 3  18.1% | batch:       140 of       772\t|\tloss: 4.06439\n",
      "Training Epoch 3  18.3% | batch:       141 of       772\t|\tloss: 3.72176\n",
      "Training Epoch 3  18.4% | batch:       142 of       772\t|\tloss: 3.19086\n",
      "Training Epoch 3  18.5% | batch:       143 of       772\t|\tloss: 4.1285\n",
      "Training Epoch 3  18.7% | batch:       144 of       772\t|\tloss: 3.53235\n",
      "Training Epoch 3  18.8% | batch:       145 of       772\t|\tloss: 3.50201\n",
      "Training Epoch 3  18.9% | batch:       146 of       772\t|\tloss: 4.1668\n",
      "Training Epoch 3  19.0% | batch:       147 of       772\t|\tloss: 4.30469\n",
      "Training Epoch 3  19.2% | batch:       148 of       772\t|\tloss: 4.26204\n",
      "Training Epoch 3  19.3% | batch:       149 of       772\t|\tloss: 3.28275\n",
      "Training Epoch 3  19.4% | batch:       150 of       772\t|\tloss: 4.25796\n",
      "Training Epoch 3  19.6% | batch:       151 of       772\t|\tloss: 4.03677\n",
      "Training Epoch 3  19.7% | batch:       152 of       772\t|\tloss: 4.02135\n",
      "Training Epoch 3  19.8% | batch:       153 of       772\t|\tloss: 3.95855\n",
      "Training Epoch 3  19.9% | batch:       154 of       772\t|\tloss: 3.07973\n",
      "Training Epoch 3  20.1% | batch:       155 of       772\t|\tloss: 3.55798\n",
      "Training Epoch 3  20.2% | batch:       156 of       772\t|\tloss: 4.00665\n",
      "Training Epoch 3  20.3% | batch:       157 of       772\t|\tloss: 4.0121\n",
      "Training Epoch 3  20.5% | batch:       158 of       772\t|\tloss: 3.33957\n",
      "Training Epoch 3  20.6% | batch:       159 of       772\t|\tloss: 4.54778\n",
      "Training Epoch 3  20.7% | batch:       160 of       772\t|\tloss: 3.13906\n",
      "Training Epoch 3  20.9% | batch:       161 of       772\t|\tloss: 3.74301\n",
      "Training Epoch 3  21.0% | batch:       162 of       772\t|\tloss: 3.88676\n",
      "Training Epoch 3  21.1% | batch:       163 of       772\t|\tloss: 4.571\n",
      "Training Epoch 3  21.2% | batch:       164 of       772\t|\tloss: 4.31666\n",
      "Training Epoch 3  21.4% | batch:       165 of       772\t|\tloss: 4.54201\n",
      "Training Epoch 3  21.5% | batch:       166 of       772\t|\tloss: 3.27618\n",
      "Training Epoch 3  21.6% | batch:       167 of       772\t|\tloss: 2.83901\n",
      "Training Epoch 3  21.8% | batch:       168 of       772\t|\tloss: 3.88517\n",
      "Training Epoch 3  21.9% | batch:       169 of       772\t|\tloss: 3.51696\n",
      "Training Epoch 3  22.0% | batch:       170 of       772\t|\tloss: 3.43183\n",
      "Training Epoch 3  22.2% | batch:       171 of       772\t|\tloss: 3.10463\n",
      "Training Epoch 3  22.3% | batch:       172 of       772\t|\tloss: 3.30226\n",
      "Training Epoch 3  22.4% | batch:       173 of       772\t|\tloss: 4.52102\n",
      "Training Epoch 3  22.5% | batch:       174 of       772\t|\tloss: 3.33164\n",
      "Training Epoch 3  22.7% | batch:       175 of       772\t|\tloss: 3.91715\n",
      "Training Epoch 3  22.8% | batch:       176 of       772\t|\tloss: 3.46197\n",
      "Training Epoch 3  22.9% | batch:       177 of       772\t|\tloss: 4.02075\n",
      "Training Epoch 3  23.1% | batch:       178 of       772\t|\tloss: 3.78382\n",
      "Training Epoch 3  23.2% | batch:       179 of       772\t|\tloss: 4.68032\n",
      "Training Epoch 3  23.3% | batch:       180 of       772\t|\tloss: 4.09743\n",
      "Training Epoch 3  23.4% | batch:       181 of       772\t|\tloss: 4.81728\n",
      "Training Epoch 3  23.6% | batch:       182 of       772\t|\tloss: 3.2569\n",
      "Training Epoch 3  23.7% | batch:       183 of       772\t|\tloss: 3.3702\n",
      "Training Epoch 3  23.8% | batch:       184 of       772\t|\tloss: 4.04119\n",
      "Training Epoch 3  24.0% | batch:       185 of       772\t|\tloss: 3.81386\n",
      "Training Epoch 3  24.1% | batch:       186 of       772\t|\tloss: 4.87166\n",
      "Training Epoch 3  24.2% | batch:       187 of       772\t|\tloss: 2.74422\n",
      "Training Epoch 3  24.4% | batch:       188 of       772\t|\tloss: 3.71549\n",
      "Training Epoch 3  24.5% | batch:       189 of       772\t|\tloss: 3.68891\n",
      "Training Epoch 3  24.6% | batch:       190 of       772\t|\tloss: 3.565\n",
      "Training Epoch 3  24.7% | batch:       191 of       772\t|\tloss: 3.83929\n",
      "Training Epoch 3  24.9% | batch:       192 of       772\t|\tloss: 3.13567\n",
      "Training Epoch 3  25.0% | batch:       193 of       772\t|\tloss: 3.38845\n",
      "Training Epoch 3  25.1% | batch:       194 of       772\t|\tloss: 3.93724\n",
      "Training Epoch 3  25.3% | batch:       195 of       772\t|\tloss: 3.70152\n",
      "Training Epoch 3  25.4% | batch:       196 of       772\t|\tloss: 3.10771\n",
      "Training Epoch 3  25.5% | batch:       197 of       772\t|\tloss: 3.53927\n",
      "Training Epoch 3  25.6% | batch:       198 of       772\t|\tloss: 2.51045\n",
      "Training Epoch 3  25.8% | batch:       199 of       772\t|\tloss: 3.38583\n",
      "Training Epoch 3  25.9% | batch:       200 of       772\t|\tloss: 2.95654\n",
      "Training Epoch 3  26.0% | batch:       201 of       772\t|\tloss: 4.38965\n",
      "Training Epoch 3  26.2% | batch:       202 of       772\t|\tloss: 4.36235\n",
      "Training Epoch 3  26.3% | batch:       203 of       772\t|\tloss: 3.76215\n",
      "Training Epoch 3  26.4% | batch:       204 of       772\t|\tloss: 3.88722\n",
      "Training Epoch 3  26.6% | batch:       205 of       772\t|\tloss: 2.69079\n",
      "Training Epoch 3  26.7% | batch:       206 of       772\t|\tloss: 2.83062\n",
      "Training Epoch 3  26.8% | batch:       207 of       772\t|\tloss: 2.49256\n",
      "Training Epoch 3  26.9% | batch:       208 of       772\t|\tloss: 3.79252\n",
      "Training Epoch 3  27.1% | batch:       209 of       772\t|\tloss: 3.10377\n",
      "Training Epoch 3  27.2% | batch:       210 of       772\t|\tloss: 3.27909\n",
      "Training Epoch 3  27.3% | batch:       211 of       772\t|\tloss: 3.79358\n",
      "Training Epoch 3  27.5% | batch:       212 of       772\t|\tloss: 3.46118\n",
      "Training Epoch 3  27.6% | batch:       213 of       772\t|\tloss: 3.94513\n",
      "Training Epoch 3  27.7% | batch:       214 of       772\t|\tloss: 3.60272\n",
      "Training Epoch 3  27.8% | batch:       215 of       772\t|\tloss: 3.30412\n",
      "Training Epoch 3  28.0% | batch:       216 of       772\t|\tloss: 3.08785\n",
      "Training Epoch 3  28.1% | batch:       217 of       772\t|\tloss: 2.74641\n",
      "Training Epoch 3  28.2% | batch:       218 of       772\t|\tloss: 3.64305\n",
      "Training Epoch 3  28.4% | batch:       219 of       772\t|\tloss: 4.12294\n",
      "Training Epoch 3  28.5% | batch:       220 of       772\t|\tloss: 3.24276\n",
      "Training Epoch 3  28.6% | batch:       221 of       772\t|\tloss: 3.3159\n",
      "Training Epoch 3  28.8% | batch:       222 of       772\t|\tloss: 3.27238\n",
      "Training Epoch 3  28.9% | batch:       223 of       772\t|\tloss: 3.65485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  29.0% | batch:       224 of       772\t|\tloss: 3.34015\n",
      "Training Epoch 3  29.1% | batch:       225 of       772\t|\tloss: 2.99027\n",
      "Training Epoch 3  29.3% | batch:       226 of       772\t|\tloss: 3.13019\n",
      "Training Epoch 3  29.4% | batch:       227 of       772\t|\tloss: 4.0293\n",
      "Training Epoch 3  29.5% | batch:       228 of       772\t|\tloss: 3.43943\n",
      "Training Epoch 3  29.7% | batch:       229 of       772\t|\tloss: 3.62667\n",
      "Training Epoch 3  29.8% | batch:       230 of       772\t|\tloss: 3.37905\n",
      "Training Epoch 3  29.9% | batch:       231 of       772\t|\tloss: 2.84783\n",
      "Training Epoch 3  30.1% | batch:       232 of       772\t|\tloss: 3.48363\n",
      "Training Epoch 3  30.2% | batch:       233 of       772\t|\tloss: 2.5491\n",
      "Training Epoch 3  30.3% | batch:       234 of       772\t|\tloss: 3.06268\n",
      "Training Epoch 3  30.4% | batch:       235 of       772\t|\tloss: 4.34667\n",
      "Training Epoch 3  30.6% | batch:       236 of       772\t|\tloss: 4.2067\n",
      "Training Epoch 3  30.7% | batch:       237 of       772\t|\tloss: 3.38575\n",
      "Training Epoch 3  30.8% | batch:       238 of       772\t|\tloss: 3.58152\n",
      "Training Epoch 3  31.0% | batch:       239 of       772\t|\tloss: 3.08927\n",
      "Training Epoch 3  31.1% | batch:       240 of       772\t|\tloss: 3.63055\n",
      "Training Epoch 3  31.2% | batch:       241 of       772\t|\tloss: 3.45993\n",
      "Training Epoch 3  31.3% | batch:       242 of       772\t|\tloss: 2.82266\n",
      "Training Epoch 3  31.5% | batch:       243 of       772\t|\tloss: 3.85132\n",
      "Training Epoch 3  31.6% | batch:       244 of       772\t|\tloss: 2.93858\n",
      "Training Epoch 3  31.7% | batch:       245 of       772\t|\tloss: 2.39684\n",
      "Training Epoch 3  31.9% | batch:       246 of       772\t|\tloss: 2.91569\n",
      "Training Epoch 3  32.0% | batch:       247 of       772\t|\tloss: 3.15318\n",
      "Training Epoch 3  32.1% | batch:       248 of       772\t|\tloss: 2.89608\n",
      "Training Epoch 3  32.3% | batch:       249 of       772\t|\tloss: 3.47808\n",
      "Training Epoch 3  32.4% | batch:       250 of       772\t|\tloss: 2.37417\n",
      "Training Epoch 3  32.5% | batch:       251 of       772\t|\tloss: 2.76072\n",
      "Training Epoch 3  32.6% | batch:       252 of       772\t|\tloss: 2.72502\n",
      "Training Epoch 3  32.8% | batch:       253 of       772\t|\tloss: 3.31734\n",
      "Training Epoch 3  32.9% | batch:       254 of       772\t|\tloss: 3.0249\n",
      "Training Epoch 3  33.0% | batch:       255 of       772\t|\tloss: 2.73409\n",
      "Training Epoch 3  33.2% | batch:       256 of       772\t|\tloss: 3.0421\n",
      "Training Epoch 3  33.3% | batch:       257 of       772\t|\tloss: 3.09018\n",
      "Training Epoch 3  33.4% | batch:       258 of       772\t|\tloss: 3.55777\n",
      "Training Epoch 3  33.5% | batch:       259 of       772\t|\tloss: 3.04426\n",
      "Training Epoch 3  33.7% | batch:       260 of       772\t|\tloss: 3.3806\n",
      "Training Epoch 3  33.8% | batch:       261 of       772\t|\tloss: 3.38774\n",
      "Training Epoch 3  33.9% | batch:       262 of       772\t|\tloss: 2.46762\n",
      "Training Epoch 3  34.1% | batch:       263 of       772\t|\tloss: 2.96135\n",
      "Training Epoch 3  34.2% | batch:       264 of       772\t|\tloss: 2.80814\n",
      "Training Epoch 3  34.3% | batch:       265 of       772\t|\tloss: 2.96149\n",
      "Training Epoch 3  34.5% | batch:       266 of       772\t|\tloss: 3.10099\n",
      "Training Epoch 3  34.6% | batch:       267 of       772\t|\tloss: 2.90129\n",
      "Training Epoch 3  34.7% | batch:       268 of       772\t|\tloss: 3.52126\n",
      "Training Epoch 3  34.8% | batch:       269 of       772\t|\tloss: 3.1996\n",
      "Training Epoch 3  35.0% | batch:       270 of       772\t|\tloss: 3.24668\n",
      "Training Epoch 3  35.1% | batch:       271 of       772\t|\tloss: 2.72623\n",
      "Training Epoch 3  35.2% | batch:       272 of       772\t|\tloss: 3.10214\n",
      "Training Epoch 3  35.4% | batch:       273 of       772\t|\tloss: 3.06581\n",
      "Training Epoch 3  35.5% | batch:       274 of       772\t|\tloss: 4.04727\n",
      "Training Epoch 3  35.6% | batch:       275 of       772\t|\tloss: 3.3043\n",
      "Training Epoch 3  35.8% | batch:       276 of       772\t|\tloss: 2.95532\n",
      "Training Epoch 3  35.9% | batch:       277 of       772\t|\tloss: 3.4266\n",
      "Training Epoch 3  36.0% | batch:       278 of       772\t|\tloss: 3.20112\n",
      "Training Epoch 3  36.1% | batch:       279 of       772\t|\tloss: 3.41756\n",
      "Training Epoch 3  36.3% | batch:       280 of       772\t|\tloss: 2.5089\n",
      "Training Epoch 3  36.4% | batch:       281 of       772\t|\tloss: 3.19687\n",
      "Training Epoch 3  36.5% | batch:       282 of       772\t|\tloss: 2.62746\n",
      "Training Epoch 3  36.7% | batch:       283 of       772\t|\tloss: 3.00809\n",
      "Training Epoch 3  36.8% | batch:       284 of       772\t|\tloss: 3.17635\n",
      "Training Epoch 3  36.9% | batch:       285 of       772\t|\tloss: 3.25663\n",
      "Training Epoch 3  37.0% | batch:       286 of       772\t|\tloss: 2.76499\n",
      "Training Epoch 3  37.2% | batch:       287 of       772\t|\tloss: 2.99719\n",
      "Training Epoch 3  37.3% | batch:       288 of       772\t|\tloss: 3.43964\n",
      "Training Epoch 3  37.4% | batch:       289 of       772\t|\tloss: 3.01521\n",
      "Training Epoch 3  37.6% | batch:       290 of       772\t|\tloss: 2.40944\n",
      "Training Epoch 3  37.7% | batch:       291 of       772\t|\tloss: 3.21536\n",
      "Training Epoch 3  37.8% | batch:       292 of       772\t|\tloss: 2.9307\n",
      "Training Epoch 3  38.0% | batch:       293 of       772\t|\tloss: 3.81954\n",
      "Training Epoch 3  38.1% | batch:       294 of       772\t|\tloss: 2.71461\n",
      "Training Epoch 3  38.2% | batch:       295 of       772\t|\tloss: 3.72458\n",
      "Training Epoch 3  38.3% | batch:       296 of       772\t|\tloss: 3.99671\n",
      "Training Epoch 3  38.5% | batch:       297 of       772\t|\tloss: 3.08636\n",
      "Training Epoch 3  38.6% | batch:       298 of       772\t|\tloss: 2.66086\n",
      "Training Epoch 3  38.7% | batch:       299 of       772\t|\tloss: 2.85934\n",
      "Training Epoch 3  38.9% | batch:       300 of       772\t|\tloss: 3.19298\n",
      "Training Epoch 3  39.0% | batch:       301 of       772\t|\tloss: 3.1795\n",
      "Training Epoch 3  39.1% | batch:       302 of       772\t|\tloss: 2.53671\n",
      "Training Epoch 3  39.2% | batch:       303 of       772\t|\tloss: 2.99212\n",
      "Training Epoch 3  39.4% | batch:       304 of       772\t|\tloss: 2.54253\n",
      "Training Epoch 3  39.5% | batch:       305 of       772\t|\tloss: 2.67372\n",
      "Training Epoch 3  39.6% | batch:       306 of       772\t|\tloss: 3.3255\n",
      "Training Epoch 3  39.8% | batch:       307 of       772\t|\tloss: 3.30618\n",
      "Training Epoch 3  39.9% | batch:       308 of       772\t|\tloss: 2.63773\n",
      "Training Epoch 3  40.0% | batch:       309 of       772\t|\tloss: 2.24901\n",
      "Training Epoch 3  40.2% | batch:       310 of       772\t|\tloss: 2.98169\n",
      "Training Epoch 3  40.3% | batch:       311 of       772\t|\tloss: 2.81347\n",
      "Training Epoch 3  40.4% | batch:       312 of       772\t|\tloss: 3.34154\n",
      "Training Epoch 3  40.5% | batch:       313 of       772\t|\tloss: 3.27081\n",
      "Training Epoch 3  40.7% | batch:       314 of       772\t|\tloss: 2.99782\n",
      "Training Epoch 3  40.8% | batch:       315 of       772\t|\tloss: 3.30259\n",
      "Training Epoch 3  40.9% | batch:       316 of       772\t|\tloss: 2.85676\n",
      "Training Epoch 3  41.1% | batch:       317 of       772\t|\tloss: 2.8084\n",
      "Training Epoch 3  41.2% | batch:       318 of       772\t|\tloss: 2.96588\n",
      "Training Epoch 3  41.3% | batch:       319 of       772\t|\tloss: 3.25999\n",
      "Training Epoch 3  41.5% | batch:       320 of       772\t|\tloss: 2.81547\n",
      "Training Epoch 3  41.6% | batch:       321 of       772\t|\tloss: 3.15444\n",
      "Training Epoch 3  41.7% | batch:       322 of       772\t|\tloss: 2.78237\n",
      "Training Epoch 3  41.8% | batch:       323 of       772\t|\tloss: 2.50209\n",
      "Training Epoch 3  42.0% | batch:       324 of       772\t|\tloss: 3.10961\n",
      "Training Epoch 3  42.1% | batch:       325 of       772\t|\tloss: 2.99627\n",
      "Training Epoch 3  42.2% | batch:       326 of       772\t|\tloss: 3.37218\n",
      "Training Epoch 3  42.4% | batch:       327 of       772\t|\tloss: 2.56421\n",
      "Training Epoch 3  42.5% | batch:       328 of       772\t|\tloss: 3.00368\n",
      "Training Epoch 3  42.6% | batch:       329 of       772\t|\tloss: 3.06853\n",
      "Training Epoch 3  42.7% | batch:       330 of       772\t|\tloss: 2.86348\n",
      "Training Epoch 3  42.9% | batch:       331 of       772\t|\tloss: 3.11412\n",
      "Training Epoch 3  43.0% | batch:       332 of       772\t|\tloss: 2.54376\n",
      "Training Epoch 3  43.1% | batch:       333 of       772\t|\tloss: 3.21853\n",
      "Training Epoch 3  43.3% | batch:       334 of       772\t|\tloss: 3.72571\n",
      "Training Epoch 3  43.4% | batch:       335 of       772\t|\tloss: 3.3859\n",
      "Training Epoch 3  43.5% | batch:       336 of       772\t|\tloss: 3.85124\n",
      "Training Epoch 3  43.7% | batch:       337 of       772\t|\tloss: 2.75104\n",
      "Training Epoch 3  43.8% | batch:       338 of       772\t|\tloss: 3.15522\n",
      "Training Epoch 3  43.9% | batch:       339 of       772\t|\tloss: 2.24061\n",
      "Training Epoch 3  44.0% | batch:       340 of       772\t|\tloss: 3.44485\n",
      "Training Epoch 3  44.2% | batch:       341 of       772\t|\tloss: 3.20977\n",
      "Training Epoch 3  44.3% | batch:       342 of       772\t|\tloss: 2.65726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  44.4% | batch:       343 of       772\t|\tloss: 3.21354\n",
      "Training Epoch 3  44.6% | batch:       344 of       772\t|\tloss: 2.71025\n",
      "Training Epoch 3  44.7% | batch:       345 of       772\t|\tloss: 3.14506\n",
      "Training Epoch 3  44.8% | batch:       346 of       772\t|\tloss: 2.87775\n",
      "Training Epoch 3  44.9% | batch:       347 of       772\t|\tloss: 2.21012\n",
      "Training Epoch 3  45.1% | batch:       348 of       772\t|\tloss: 2.55456\n",
      "Training Epoch 3  45.2% | batch:       349 of       772\t|\tloss: 3.12496\n",
      "Training Epoch 3  45.3% | batch:       350 of       772\t|\tloss: 2.7916\n",
      "Training Epoch 3  45.5% | batch:       351 of       772\t|\tloss: 2.71899\n",
      "Training Epoch 3  45.6% | batch:       352 of       772\t|\tloss: 2.65931\n",
      "Training Epoch 3  45.7% | batch:       353 of       772\t|\tloss: 2.99058\n",
      "Training Epoch 3  45.9% | batch:       354 of       772\t|\tloss: 3.10518\n",
      "Training Epoch 3  46.0% | batch:       355 of       772\t|\tloss: 2.54902\n",
      "Training Epoch 3  46.1% | batch:       356 of       772\t|\tloss: 2.66962\n",
      "Training Epoch 3  46.2% | batch:       357 of       772\t|\tloss: 3.40051\n",
      "Training Epoch 3  46.4% | batch:       358 of       772\t|\tloss: 2.6228\n",
      "Training Epoch 3  46.5% | batch:       359 of       772\t|\tloss: 2.57983\n",
      "Training Epoch 3  46.6% | batch:       360 of       772\t|\tloss: 2.37481\n",
      "Training Epoch 3  46.8% | batch:       361 of       772\t|\tloss: 2.91269\n",
      "Training Epoch 3  46.9% | batch:       362 of       772\t|\tloss: 3.07853\n",
      "Training Epoch 3  47.0% | batch:       363 of       772\t|\tloss: 3.09002\n",
      "Training Epoch 3  47.2% | batch:       364 of       772\t|\tloss: 2.69396\n",
      "Training Epoch 3  47.3% | batch:       365 of       772\t|\tloss: 2.68758\n",
      "Training Epoch 3  47.4% | batch:       366 of       772\t|\tloss: 2.71454\n",
      "Training Epoch 3  47.5% | batch:       367 of       772\t|\tloss: 3.23719\n",
      "Training Epoch 3  47.7% | batch:       368 of       772\t|\tloss: 3.12379\n",
      "Training Epoch 3  47.8% | batch:       369 of       772\t|\tloss: 2.41742\n",
      "Training Epoch 3  47.9% | batch:       370 of       772\t|\tloss: 2.4411\n",
      "Training Epoch 3  48.1% | batch:       371 of       772\t|\tloss: 2.90067\n",
      "Training Epoch 3  48.2% | batch:       372 of       772\t|\tloss: 3.51907\n",
      "Training Epoch 3  48.3% | batch:       373 of       772\t|\tloss: 3.83237\n",
      "Training Epoch 3  48.4% | batch:       374 of       772\t|\tloss: 2.87759\n",
      "Training Epoch 3  48.6% | batch:       375 of       772\t|\tloss: 2.4748\n",
      "Training Epoch 3  48.7% | batch:       376 of       772\t|\tloss: 2.66638\n",
      "Training Epoch 3  48.8% | batch:       377 of       772\t|\tloss: 2.79444\n",
      "Training Epoch 3  49.0% | batch:       378 of       772\t|\tloss: 2.70432\n",
      "Training Epoch 3  49.1% | batch:       379 of       772\t|\tloss: 3.04662\n",
      "Training Epoch 3  49.2% | batch:       380 of       772\t|\tloss: 3.22979\n",
      "Training Epoch 3  49.4% | batch:       381 of       772\t|\tloss: 2.34742\n",
      "Training Epoch 3  49.5% | batch:       382 of       772\t|\tloss: 2.51251\n",
      "Training Epoch 3  49.6% | batch:       383 of       772\t|\tloss: 2.68282\n",
      "Training Epoch 3  49.7% | batch:       384 of       772\t|\tloss: 2.9364\n",
      "Training Epoch 3  49.9% | batch:       385 of       772\t|\tloss: 3.15106\n",
      "Training Epoch 3  50.0% | batch:       386 of       772\t|\tloss: 2.82297\n",
      "Training Epoch 3  50.1% | batch:       387 of       772\t|\tloss: 2.4879\n",
      "Training Epoch 3  50.3% | batch:       388 of       772\t|\tloss: 2.83701\n",
      "Training Epoch 3  50.4% | batch:       389 of       772\t|\tloss: 3.47285\n",
      "Training Epoch 3  50.5% | batch:       390 of       772\t|\tloss: 2.16691\n",
      "Training Epoch 3  50.6% | batch:       391 of       772\t|\tloss: 2.44335\n",
      "Training Epoch 3  50.8% | batch:       392 of       772\t|\tloss: 2.59102\n",
      "Training Epoch 3  50.9% | batch:       393 of       772\t|\tloss: 2.30748\n",
      "Training Epoch 3  51.0% | batch:       394 of       772\t|\tloss: 3.07799\n",
      "Training Epoch 3  51.2% | batch:       395 of       772\t|\tloss: 2.65844\n",
      "Training Epoch 3  51.3% | batch:       396 of       772\t|\tloss: 2.84881\n",
      "Training Epoch 3  51.4% | batch:       397 of       772\t|\tloss: 2.73445\n",
      "Training Epoch 3  51.6% | batch:       398 of       772\t|\tloss: 3.0951\n",
      "Training Epoch 3  51.7% | batch:       399 of       772\t|\tloss: 2.81948\n",
      "Training Epoch 3  51.8% | batch:       400 of       772\t|\tloss: 2.58673\n",
      "Training Epoch 3  51.9% | batch:       401 of       772\t|\tloss: 2.46204\n",
      "Training Epoch 3  52.1% | batch:       402 of       772\t|\tloss: 2.82263\n",
      "Training Epoch 3  52.2% | batch:       403 of       772\t|\tloss: 3.3611\n",
      "Training Epoch 3  52.3% | batch:       404 of       772\t|\tloss: 2.73243\n",
      "Training Epoch 3  52.5% | batch:       405 of       772\t|\tloss: 2.93112\n",
      "Training Epoch 3  52.6% | batch:       406 of       772\t|\tloss: 2.61358\n",
      "Training Epoch 3  52.7% | batch:       407 of       772\t|\tloss: 2.2061\n",
      "Training Epoch 3  52.8% | batch:       408 of       772\t|\tloss: 2.51109\n",
      "Training Epoch 3  53.0% | batch:       409 of       772\t|\tloss: 2.41861\n",
      "Training Epoch 3  53.1% | batch:       410 of       772\t|\tloss: 2.2538\n",
      "Training Epoch 3  53.2% | batch:       411 of       772\t|\tloss: 2.31868\n",
      "Training Epoch 3  53.4% | batch:       412 of       772\t|\tloss: 3.14662\n",
      "Training Epoch 3  53.5% | batch:       413 of       772\t|\tloss: 2.64876\n",
      "Training Epoch 3  53.6% | batch:       414 of       772\t|\tloss: 2.62219\n",
      "Training Epoch 3  53.8% | batch:       415 of       772\t|\tloss: 2.19287\n",
      "Training Epoch 3  53.9% | batch:       416 of       772\t|\tloss: 3.26062\n",
      "Training Epoch 3  54.0% | batch:       417 of       772\t|\tloss: 2.38965\n",
      "Training Epoch 3  54.1% | batch:       418 of       772\t|\tloss: 2.59711\n",
      "Training Epoch 3  54.3% | batch:       419 of       772\t|\tloss: 2.37816\n",
      "Training Epoch 3  54.4% | batch:       420 of       772\t|\tloss: 2.66819\n",
      "Training Epoch 3  54.5% | batch:       421 of       772\t|\tloss: 3.54582\n",
      "Training Epoch 3  54.7% | batch:       422 of       772\t|\tloss: 3.40702\n",
      "Training Epoch 3  54.8% | batch:       423 of       772\t|\tloss: 2.47655\n",
      "Training Epoch 3  54.9% | batch:       424 of       772\t|\tloss: 2.61411\n",
      "Training Epoch 3  55.1% | batch:       425 of       772\t|\tloss: 2.32313\n",
      "Training Epoch 3  55.2% | batch:       426 of       772\t|\tloss: 3.19488\n",
      "Training Epoch 3  55.3% | batch:       427 of       772\t|\tloss: 2.83943\n",
      "Training Epoch 3  55.4% | batch:       428 of       772\t|\tloss: 3.08887\n",
      "Training Epoch 3  55.6% | batch:       429 of       772\t|\tloss: 2.32563\n",
      "Training Epoch 3  55.7% | batch:       430 of       772\t|\tloss: 2.46115\n",
      "Training Epoch 3  55.8% | batch:       431 of       772\t|\tloss: 2.4189\n",
      "Training Epoch 3  56.0% | batch:       432 of       772\t|\tloss: 2.60443\n",
      "Training Epoch 3  56.1% | batch:       433 of       772\t|\tloss: 2.41217\n",
      "Training Epoch 3  56.2% | batch:       434 of       772\t|\tloss: 2.8664\n",
      "Training Epoch 3  56.3% | batch:       435 of       772\t|\tloss: 3.12127\n",
      "Training Epoch 3  56.5% | batch:       436 of       772\t|\tloss: 3.41154\n",
      "Training Epoch 3  56.6% | batch:       437 of       772\t|\tloss: 2.52385\n",
      "Training Epoch 3  56.7% | batch:       438 of       772\t|\tloss: 2.85891\n",
      "Training Epoch 3  56.9% | batch:       439 of       772\t|\tloss: 2.51177\n",
      "Training Epoch 3  57.0% | batch:       440 of       772\t|\tloss: 2.51513\n",
      "Training Epoch 3  57.1% | batch:       441 of       772\t|\tloss: 2.52371\n",
      "Training Epoch 3  57.3% | batch:       442 of       772\t|\tloss: 2.78057\n",
      "Training Epoch 3  57.4% | batch:       443 of       772\t|\tloss: 2.93525\n",
      "Training Epoch 3  57.5% | batch:       444 of       772\t|\tloss: 3.11891\n",
      "Training Epoch 3  57.6% | batch:       445 of       772\t|\tloss: 2.35143\n",
      "Training Epoch 3  57.8% | batch:       446 of       772\t|\tloss: 2.74845\n",
      "Training Epoch 3  57.9% | batch:       447 of       772\t|\tloss: 2.73891\n",
      "Training Epoch 3  58.0% | batch:       448 of       772\t|\tloss: 3.04165\n",
      "Training Epoch 3  58.2% | batch:       449 of       772\t|\tloss: 1.99615\n",
      "Training Epoch 3  58.3% | batch:       450 of       772\t|\tloss: 2.7256\n",
      "Training Epoch 3  58.4% | batch:       451 of       772\t|\tloss: 2.29623\n",
      "Training Epoch 3  58.5% | batch:       452 of       772\t|\tloss: 2.55165\n",
      "Training Epoch 3  58.7% | batch:       453 of       772\t|\tloss: 2.33028\n",
      "Training Epoch 3  58.8% | batch:       454 of       772\t|\tloss: 2.29481\n",
      "Training Epoch 3  58.9% | batch:       455 of       772\t|\tloss: 2.81948\n",
      "Training Epoch 3  59.1% | batch:       456 of       772\t|\tloss: 2.44972\n",
      "Training Epoch 3  59.2% | batch:       457 of       772\t|\tloss: 2.62641\n",
      "Training Epoch 3  59.3% | batch:       458 of       772\t|\tloss: 2.47145\n",
      "Training Epoch 3  59.5% | batch:       459 of       772\t|\tloss: 2.32776\n",
      "Training Epoch 3  59.6% | batch:       460 of       772\t|\tloss: 2.63347\n",
      "Training Epoch 3  59.7% | batch:       461 of       772\t|\tloss: 2.90247\n",
      "Training Epoch 3  59.8% | batch:       462 of       772\t|\tloss: 2.16342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  60.0% | batch:       463 of       772\t|\tloss: 2.52963\n",
      "Training Epoch 3  60.1% | batch:       464 of       772\t|\tloss: 2.36862\n",
      "Training Epoch 3  60.2% | batch:       465 of       772\t|\tloss: 2.36443\n",
      "Training Epoch 3  60.4% | batch:       466 of       772\t|\tloss: 2.01448\n",
      "Training Epoch 3  60.5% | batch:       467 of       772\t|\tloss: 3.21581\n",
      "Training Epoch 3  60.6% | batch:       468 of       772\t|\tloss: 2.49354\n",
      "Training Epoch 3  60.8% | batch:       469 of       772\t|\tloss: 2.20987\n",
      "Training Epoch 3  60.9% | batch:       470 of       772\t|\tloss: 2.50197\n",
      "Training Epoch 3  61.0% | batch:       471 of       772\t|\tloss: 2.81634\n",
      "Training Epoch 3  61.1% | batch:       472 of       772\t|\tloss: 2.13622\n",
      "Training Epoch 3  61.3% | batch:       473 of       772\t|\tloss: 2.14448\n",
      "Training Epoch 3  61.4% | batch:       474 of       772\t|\tloss: 2.34153\n",
      "Training Epoch 3  61.5% | batch:       475 of       772\t|\tloss: 2.79291\n",
      "Training Epoch 3  61.7% | batch:       476 of       772\t|\tloss: 2.03833\n",
      "Training Epoch 3  61.8% | batch:       477 of       772\t|\tloss: 2.02244\n",
      "Training Epoch 3  61.9% | batch:       478 of       772\t|\tloss: 1.99419\n",
      "Training Epoch 3  62.0% | batch:       479 of       772\t|\tloss: 2.28033\n",
      "Training Epoch 3  62.2% | batch:       480 of       772\t|\tloss: 2.38113\n",
      "Training Epoch 3  62.3% | batch:       481 of       772\t|\tloss: 2.21641\n",
      "Training Epoch 3  62.4% | batch:       482 of       772\t|\tloss: 2.55741\n",
      "Training Epoch 3  62.6% | batch:       483 of       772\t|\tloss: 1.7201\n",
      "Training Epoch 3  62.7% | batch:       484 of       772\t|\tloss: 2.2204\n",
      "Training Epoch 3  62.8% | batch:       485 of       772\t|\tloss: 2.89561\n",
      "Training Epoch 3  63.0% | batch:       486 of       772\t|\tloss: 2.93483\n",
      "Training Epoch 3  63.1% | batch:       487 of       772\t|\tloss: 2.5787\n",
      "Training Epoch 3  63.2% | batch:       488 of       772\t|\tloss: 2.1118\n",
      "Training Epoch 3  63.3% | batch:       489 of       772\t|\tloss: 2.39469\n",
      "Training Epoch 3  63.5% | batch:       490 of       772\t|\tloss: 2.63584\n",
      "Training Epoch 3  63.6% | batch:       491 of       772\t|\tloss: 3.00483\n",
      "Training Epoch 3  63.7% | batch:       492 of       772\t|\tloss: 2.53796\n",
      "Training Epoch 3  63.9% | batch:       493 of       772\t|\tloss: 2.39549\n",
      "Training Epoch 3  64.0% | batch:       494 of       772\t|\tloss: 2.41042\n",
      "Training Epoch 3  64.1% | batch:       495 of       772\t|\tloss: 2.35113\n",
      "Training Epoch 3  64.2% | batch:       496 of       772\t|\tloss: 2.5253\n",
      "Training Epoch 3  64.4% | batch:       497 of       772\t|\tloss: 1.98835\n",
      "Training Epoch 3  64.5% | batch:       498 of       772\t|\tloss: 2.09817\n",
      "Training Epoch 3  64.6% | batch:       499 of       772\t|\tloss: 3.05103\n",
      "Training Epoch 3  64.8% | batch:       500 of       772\t|\tloss: 2.06304\n",
      "Training Epoch 3  64.9% | batch:       501 of       772\t|\tloss: 2.73724\n",
      "Training Epoch 3  65.0% | batch:       502 of       772\t|\tloss: 3.0307\n",
      "Training Epoch 3  65.2% | batch:       503 of       772\t|\tloss: 3.08916\n",
      "Training Epoch 3  65.3% | batch:       504 of       772\t|\tloss: 2.09249\n",
      "Training Epoch 3  65.4% | batch:       505 of       772\t|\tloss: 2.22144\n",
      "Training Epoch 3  65.5% | batch:       506 of       772\t|\tloss: 2.63131\n",
      "Training Epoch 3  65.7% | batch:       507 of       772\t|\tloss: 2.33654\n",
      "Training Epoch 3  65.8% | batch:       508 of       772\t|\tloss: 2.024\n",
      "Training Epoch 3  65.9% | batch:       509 of       772\t|\tloss: 2.07191\n",
      "Training Epoch 3  66.1% | batch:       510 of       772\t|\tloss: 2.60303\n",
      "Training Epoch 3  66.2% | batch:       511 of       772\t|\tloss: 1.85734\n",
      "Training Epoch 3  66.3% | batch:       512 of       772\t|\tloss: 2.99708\n",
      "Training Epoch 3  66.5% | batch:       513 of       772\t|\tloss: 2.79772\n",
      "Training Epoch 3  66.6% | batch:       514 of       772\t|\tloss: 2.42671\n",
      "Training Epoch 3  66.7% | batch:       515 of       772\t|\tloss: 2.22666\n",
      "Training Epoch 3  66.8% | batch:       516 of       772\t|\tloss: 1.96165\n",
      "Training Epoch 3  67.0% | batch:       517 of       772\t|\tloss: 3.85007\n",
      "Training Epoch 3  67.1% | batch:       518 of       772\t|\tloss: 2.07309\n",
      "Training Epoch 3  67.2% | batch:       519 of       772\t|\tloss: 2.51961\n",
      "Training Epoch 3  67.4% | batch:       520 of       772\t|\tloss: 2.2464\n",
      "Training Epoch 3  67.5% | batch:       521 of       772\t|\tloss: 1.99666\n",
      "Training Epoch 3  67.6% | batch:       522 of       772\t|\tloss: 2.14226\n",
      "Training Epoch 3  67.7% | batch:       523 of       772\t|\tloss: 2.50755\n",
      "Training Epoch 3  67.9% | batch:       524 of       772\t|\tloss: 2.16622\n",
      "Training Epoch 3  68.0% | batch:       525 of       772\t|\tloss: 2.42803\n",
      "Training Epoch 3  68.1% | batch:       526 of       772\t|\tloss: 2.31543\n",
      "Training Epoch 3  68.3% | batch:       527 of       772\t|\tloss: 2.54835\n",
      "Training Epoch 3  68.4% | batch:       528 of       772\t|\tloss: 2.45876\n",
      "Training Epoch 3  68.5% | batch:       529 of       772\t|\tloss: 2.58776\n",
      "Training Epoch 3  68.7% | batch:       530 of       772\t|\tloss: 1.83872\n",
      "Training Epoch 3  68.8% | batch:       531 of       772\t|\tloss: 2.42724\n",
      "Training Epoch 3  68.9% | batch:       532 of       772\t|\tloss: 2.24579\n",
      "Training Epoch 3  69.0% | batch:       533 of       772\t|\tloss: 2.36804\n",
      "Training Epoch 3  69.2% | batch:       534 of       772\t|\tloss: 2.44476\n",
      "Training Epoch 3  69.3% | batch:       535 of       772\t|\tloss: 2.18382\n",
      "Training Epoch 3  69.4% | batch:       536 of       772\t|\tloss: 2.79065\n",
      "Training Epoch 3  69.6% | batch:       537 of       772\t|\tloss: 2.19086\n",
      "Training Epoch 3  69.7% | batch:       538 of       772\t|\tloss: 2.61599\n",
      "Training Epoch 3  69.8% | batch:       539 of       772\t|\tloss: 1.85149\n",
      "Training Epoch 3  69.9% | batch:       540 of       772\t|\tloss: 2.3906\n",
      "Training Epoch 3  70.1% | batch:       541 of       772\t|\tloss: 2.1761\n",
      "Training Epoch 3  70.2% | batch:       542 of       772\t|\tloss: 2.64378\n",
      "Training Epoch 3  70.3% | batch:       543 of       772\t|\tloss: 2.42888\n",
      "Training Epoch 3  70.5% | batch:       544 of       772\t|\tloss: 1.81271\n",
      "Training Epoch 3  70.6% | batch:       545 of       772\t|\tloss: 2.65244\n",
      "Training Epoch 3  70.7% | batch:       546 of       772\t|\tloss: 2.2167\n",
      "Training Epoch 3  70.9% | batch:       547 of       772\t|\tloss: 2.30855\n",
      "Training Epoch 3  71.0% | batch:       548 of       772\t|\tloss: 2.40893\n",
      "Training Epoch 3  71.1% | batch:       549 of       772\t|\tloss: 2.10248\n",
      "Training Epoch 3  71.2% | batch:       550 of       772\t|\tloss: 2.42288\n",
      "Training Epoch 3  71.4% | batch:       551 of       772\t|\tloss: 1.93194\n",
      "Training Epoch 3  71.5% | batch:       552 of       772\t|\tloss: 2.51542\n",
      "Training Epoch 3  71.6% | batch:       553 of       772\t|\tloss: 2.46921\n",
      "Training Epoch 3  71.8% | batch:       554 of       772\t|\tloss: 2.50502\n",
      "Training Epoch 3  71.9% | batch:       555 of       772\t|\tloss: 1.9634\n",
      "Training Epoch 3  72.0% | batch:       556 of       772\t|\tloss: 2.23646\n",
      "Training Epoch 3  72.2% | batch:       557 of       772\t|\tloss: 2.16981\n",
      "Training Epoch 3  72.3% | batch:       558 of       772\t|\tloss: 1.90218\n",
      "Training Epoch 3  72.4% | batch:       559 of       772\t|\tloss: 2.11668\n",
      "Training Epoch 3  72.5% | batch:       560 of       772\t|\tloss: 1.93689\n",
      "Training Epoch 3  72.7% | batch:       561 of       772\t|\tloss: 1.99333\n",
      "Training Epoch 3  72.8% | batch:       562 of       772\t|\tloss: 2.08963\n",
      "Training Epoch 3  72.9% | batch:       563 of       772\t|\tloss: 2.19306\n",
      "Training Epoch 3  73.1% | batch:       564 of       772\t|\tloss: 2.08161\n",
      "Training Epoch 3  73.2% | batch:       565 of       772\t|\tloss: 1.87913\n",
      "Training Epoch 3  73.3% | batch:       566 of       772\t|\tloss: 2.53301\n",
      "Training Epoch 3  73.4% | batch:       567 of       772\t|\tloss: 2.17764\n",
      "Training Epoch 3  73.6% | batch:       568 of       772\t|\tloss: 2.08371\n",
      "Training Epoch 3  73.7% | batch:       569 of       772\t|\tloss: 2.05057\n",
      "Training Epoch 3  73.8% | batch:       570 of       772\t|\tloss: 1.86948\n",
      "Training Epoch 3  74.0% | batch:       571 of       772\t|\tloss: 1.76473\n",
      "Training Epoch 3  74.1% | batch:       572 of       772\t|\tloss: 2.29005\n",
      "Training Epoch 3  74.2% | batch:       573 of       772\t|\tloss: 1.81033\n",
      "Training Epoch 3  74.4% | batch:       574 of       772\t|\tloss: 1.84667\n",
      "Training Epoch 3  74.5% | batch:       575 of       772\t|\tloss: 1.96357\n",
      "Training Epoch 3  74.6% | batch:       576 of       772\t|\tloss: 2.22009\n",
      "Training Epoch 3  74.7% | batch:       577 of       772\t|\tloss: 2.09999\n",
      "Training Epoch 3  74.9% | batch:       578 of       772\t|\tloss: 1.94453\n",
      "Training Epoch 3  75.0% | batch:       579 of       772\t|\tloss: 2.09547\n",
      "Training Epoch 3  75.1% | batch:       580 of       772\t|\tloss: 2.5103\n",
      "Training Epoch 3  75.3% | batch:       581 of       772\t|\tloss: 2.03779\n",
      "Training Epoch 3  75.4% | batch:       582 of       772\t|\tloss: 2.16686\n",
      "Training Epoch 3  75.5% | batch:       583 of       772\t|\tloss: 2.91022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  75.6% | batch:       584 of       772\t|\tloss: 2.0732\n",
      "Training Epoch 3  75.8% | batch:       585 of       772\t|\tloss: 1.74547\n",
      "Training Epoch 3  75.9% | batch:       586 of       772\t|\tloss: 2.09325\n",
      "Training Epoch 3  76.0% | batch:       587 of       772\t|\tloss: 2.5565\n",
      "Training Epoch 3  76.2% | batch:       588 of       772\t|\tloss: 2.02393\n",
      "Training Epoch 3  76.3% | batch:       589 of       772\t|\tloss: 2.00828\n",
      "Training Epoch 3  76.4% | batch:       590 of       772\t|\tloss: 2.81819\n",
      "Training Epoch 3  76.6% | batch:       591 of       772\t|\tloss: 2.42291\n",
      "Training Epoch 3  76.7% | batch:       592 of       772\t|\tloss: 2.03882\n",
      "Training Epoch 3  76.8% | batch:       593 of       772\t|\tloss: 2.09414\n",
      "Training Epoch 3  76.9% | batch:       594 of       772\t|\tloss: 1.95734\n",
      "Training Epoch 3  77.1% | batch:       595 of       772\t|\tloss: 2.79147\n",
      "Training Epoch 3  77.2% | batch:       596 of       772\t|\tloss: 2.02134\n",
      "Training Epoch 3  77.3% | batch:       597 of       772\t|\tloss: 2.29195\n",
      "Training Epoch 3  77.5% | batch:       598 of       772\t|\tloss: 2.35845\n",
      "Training Epoch 3  77.6% | batch:       599 of       772\t|\tloss: 1.93462\n",
      "Training Epoch 3  77.7% | batch:       600 of       772\t|\tloss: 2.09796\n",
      "Training Epoch 3  77.8% | batch:       601 of       772\t|\tloss: 2.50896\n",
      "Training Epoch 3  78.0% | batch:       602 of       772\t|\tloss: 1.84063\n",
      "Training Epoch 3  78.1% | batch:       603 of       772\t|\tloss: 1.87631\n",
      "Training Epoch 3  78.2% | batch:       604 of       772\t|\tloss: 2.50761\n",
      "Training Epoch 3  78.4% | batch:       605 of       772\t|\tloss: 1.87267\n",
      "Training Epoch 3  78.5% | batch:       606 of       772\t|\tloss: 2.19788\n",
      "Training Epoch 3  78.6% | batch:       607 of       772\t|\tloss: 2.44488\n",
      "Training Epoch 3  78.8% | batch:       608 of       772\t|\tloss: 1.91456\n",
      "Training Epoch 3  78.9% | batch:       609 of       772\t|\tloss: 2.15508\n",
      "Training Epoch 3  79.0% | batch:       610 of       772\t|\tloss: 2.16155\n",
      "Training Epoch 3  79.1% | batch:       611 of       772\t|\tloss: 1.59574\n",
      "Training Epoch 3  79.3% | batch:       612 of       772\t|\tloss: 1.69782\n",
      "Training Epoch 3  79.4% | batch:       613 of       772\t|\tloss: 1.85393\n",
      "Training Epoch 3  79.5% | batch:       614 of       772\t|\tloss: 1.92044\n",
      "Training Epoch 3  79.7% | batch:       615 of       772\t|\tloss: 1.62299\n",
      "Training Epoch 3  79.8% | batch:       616 of       772\t|\tloss: 2.39166\n",
      "Training Epoch 3  79.9% | batch:       617 of       772\t|\tloss: 1.92794\n",
      "Training Epoch 3  80.1% | batch:       618 of       772\t|\tloss: 3.04998\n",
      "Training Epoch 3  80.2% | batch:       619 of       772\t|\tloss: 2.10019\n",
      "Training Epoch 3  80.3% | batch:       620 of       772\t|\tloss: 2.34374\n",
      "Training Epoch 3  80.4% | batch:       621 of       772\t|\tloss: 2.57295\n",
      "Training Epoch 3  80.6% | batch:       622 of       772\t|\tloss: 1.76825\n",
      "Training Epoch 3  80.7% | batch:       623 of       772\t|\tloss: 2.07225\n",
      "Training Epoch 3  80.8% | batch:       624 of       772\t|\tloss: 1.81261\n",
      "Training Epoch 3  81.0% | batch:       625 of       772\t|\tloss: 2.33205\n",
      "Training Epoch 3  81.1% | batch:       626 of       772\t|\tloss: 1.48361\n",
      "Training Epoch 3  81.2% | batch:       627 of       772\t|\tloss: 1.96587\n",
      "Training Epoch 3  81.3% | batch:       628 of       772\t|\tloss: 2.14556\n",
      "Training Epoch 3  81.5% | batch:       629 of       772\t|\tloss: 1.75504\n",
      "Training Epoch 3  81.6% | batch:       630 of       772\t|\tloss: 2.07802\n",
      "Training Epoch 3  81.7% | batch:       631 of       772\t|\tloss: 1.82382\n",
      "Training Epoch 3  81.9% | batch:       632 of       772\t|\tloss: 2.03352\n",
      "Training Epoch 3  82.0% | batch:       633 of       772\t|\tloss: 2.33889\n",
      "Training Epoch 3  82.1% | batch:       634 of       772\t|\tloss: 1.88448\n",
      "Training Epoch 3  82.3% | batch:       635 of       772\t|\tloss: 2.00194\n",
      "Training Epoch 3  82.4% | batch:       636 of       772\t|\tloss: 2.07018\n",
      "Training Epoch 3  82.5% | batch:       637 of       772\t|\tloss: 2.22365\n",
      "Training Epoch 3  82.6% | batch:       638 of       772\t|\tloss: 2.17563\n",
      "Training Epoch 3  82.8% | batch:       639 of       772\t|\tloss: 1.86646\n",
      "Training Epoch 3  82.9% | batch:       640 of       772\t|\tloss: 1.88886\n",
      "Training Epoch 3  83.0% | batch:       641 of       772\t|\tloss: 2.46755\n",
      "Training Epoch 3  83.2% | batch:       642 of       772\t|\tloss: 1.58248\n",
      "Training Epoch 3  83.3% | batch:       643 of       772\t|\tloss: 1.77989\n",
      "Training Epoch 3  83.4% | batch:       644 of       772\t|\tloss: 1.66159\n",
      "Training Epoch 3  83.5% | batch:       645 of       772\t|\tloss: 1.73272\n",
      "Training Epoch 3  83.7% | batch:       646 of       772\t|\tloss: 1.99762\n",
      "Training Epoch 3  83.8% | batch:       647 of       772\t|\tloss: 2.27584\n",
      "Training Epoch 3  83.9% | batch:       648 of       772\t|\tloss: 1.71003\n",
      "Training Epoch 3  84.1% | batch:       649 of       772\t|\tloss: 2.37022\n",
      "Training Epoch 3  84.2% | batch:       650 of       772\t|\tloss: 1.95865\n",
      "Training Epoch 3  84.3% | batch:       651 of       772\t|\tloss: 2.40315\n",
      "Training Epoch 3  84.5% | batch:       652 of       772\t|\tloss: 1.9898\n",
      "Training Epoch 3  84.6% | batch:       653 of       772\t|\tloss: 2.10909\n",
      "Training Epoch 3  84.7% | batch:       654 of       772\t|\tloss: 1.80766\n",
      "Training Epoch 3  84.8% | batch:       655 of       772\t|\tloss: 2.05484\n",
      "Training Epoch 3  85.0% | batch:       656 of       772\t|\tloss: 1.70386\n",
      "Training Epoch 3  85.1% | batch:       657 of       772\t|\tloss: 1.7008\n",
      "Training Epoch 3  85.2% | batch:       658 of       772\t|\tloss: 1.82483\n",
      "Training Epoch 3  85.4% | batch:       659 of       772\t|\tloss: 1.98\n",
      "Training Epoch 3  85.5% | batch:       660 of       772\t|\tloss: 2.44154\n",
      "Training Epoch 3  85.6% | batch:       661 of       772\t|\tloss: 1.93845\n",
      "Training Epoch 3  85.8% | batch:       662 of       772\t|\tloss: 1.93497\n",
      "Training Epoch 3  85.9% | batch:       663 of       772\t|\tloss: 2.13242\n",
      "Training Epoch 3  86.0% | batch:       664 of       772\t|\tloss: 1.93554\n",
      "Training Epoch 3  86.1% | batch:       665 of       772\t|\tloss: 2.29758\n",
      "Training Epoch 3  86.3% | batch:       666 of       772\t|\tloss: 2.18189\n",
      "Training Epoch 3  86.4% | batch:       667 of       772\t|\tloss: 2.13128\n",
      "Training Epoch 3  86.5% | batch:       668 of       772\t|\tloss: 2.57031\n",
      "Training Epoch 3  86.7% | batch:       669 of       772\t|\tloss: 2.27213\n",
      "Training Epoch 3  86.8% | batch:       670 of       772\t|\tloss: 1.87797\n",
      "Training Epoch 3  86.9% | batch:       671 of       772\t|\tloss: 2.22736\n",
      "Training Epoch 3  87.0% | batch:       672 of       772\t|\tloss: 1.87552\n",
      "Training Epoch 3  87.2% | batch:       673 of       772\t|\tloss: 1.9758\n",
      "Training Epoch 3  87.3% | batch:       674 of       772\t|\tloss: 2.19932\n",
      "Training Epoch 3  87.4% | batch:       675 of       772\t|\tloss: 2.00867\n",
      "Training Epoch 3  87.6% | batch:       676 of       772\t|\tloss: 1.93156\n",
      "Training Epoch 3  87.7% | batch:       677 of       772\t|\tloss: 1.98689\n",
      "Training Epoch 3  87.8% | batch:       678 of       772\t|\tloss: 2.02342\n",
      "Training Epoch 3  88.0% | batch:       679 of       772\t|\tloss: 2.44347\n",
      "Training Epoch 3  88.1% | batch:       680 of       772\t|\tloss: 1.87483\n",
      "Training Epoch 3  88.2% | batch:       681 of       772\t|\tloss: 1.42631\n",
      "Training Epoch 3  88.3% | batch:       682 of       772\t|\tloss: 2.27222\n",
      "Training Epoch 3  88.5% | batch:       683 of       772\t|\tloss: 1.85369\n",
      "Training Epoch 3  88.6% | batch:       684 of       772\t|\tloss: 1.62723\n",
      "Training Epoch 3  88.7% | batch:       685 of       772\t|\tloss: 2.00253\n",
      "Training Epoch 3  88.9% | batch:       686 of       772\t|\tloss: 1.56376\n",
      "Training Epoch 3  89.0% | batch:       687 of       772\t|\tloss: 2.12703\n",
      "Training Epoch 3  89.1% | batch:       688 of       772\t|\tloss: 2.09275\n",
      "Training Epoch 3  89.2% | batch:       689 of       772\t|\tloss: 1.89189\n",
      "Training Epoch 3  89.4% | batch:       690 of       772\t|\tloss: 2.26936\n",
      "Training Epoch 3  89.5% | batch:       691 of       772\t|\tloss: 1.76571\n",
      "Training Epoch 3  89.6% | batch:       692 of       772\t|\tloss: 2.1131\n",
      "Training Epoch 3  89.8% | batch:       693 of       772\t|\tloss: 1.70858\n",
      "Training Epoch 3  89.9% | batch:       694 of       772\t|\tloss: 1.94944\n",
      "Training Epoch 3  90.0% | batch:       695 of       772\t|\tloss: 1.88523\n",
      "Training Epoch 3  90.2% | batch:       696 of       772\t|\tloss: 2.10853\n",
      "Training Epoch 3  90.3% | batch:       697 of       772\t|\tloss: 1.96375\n",
      "Training Epoch 3  90.4% | batch:       698 of       772\t|\tloss: 1.41136\n",
      "Training Epoch 3  90.5% | batch:       699 of       772\t|\tloss: 1.98894\n",
      "Training Epoch 3  90.7% | batch:       700 of       772\t|\tloss: 1.75551\n",
      "Training Epoch 3  90.8% | batch:       701 of       772\t|\tloss: 2.38767\n",
      "Training Epoch 3  90.9% | batch:       702 of       772\t|\tloss: 1.82026\n",
      "Training Epoch 3  91.1% | batch:       703 of       772\t|\tloss: 1.7647\n",
      "Training Epoch 3  91.2% | batch:       704 of       772\t|\tloss: 1.75752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  91.3% | batch:       705 of       772\t|\tloss: 2.13247\n",
      "Training Epoch 3  91.5% | batch:       706 of       772\t|\tloss: 1.77249\n",
      "Training Epoch 3  91.6% | batch:       707 of       772\t|\tloss: 1.76385\n",
      "Training Epoch 3  91.7% | batch:       708 of       772\t|\tloss: 1.98664\n",
      "Training Epoch 3  91.8% | batch:       709 of       772\t|\tloss: 1.99876\n",
      "Training Epoch 3  92.0% | batch:       710 of       772\t|\tloss: 1.99008\n",
      "Training Epoch 3  92.1% | batch:       711 of       772\t|\tloss: 2.33305\n",
      "Training Epoch 3  92.2% | batch:       712 of       772\t|\tloss: 1.98682\n",
      "Training Epoch 3  92.4% | batch:       713 of       772\t|\tloss: 1.80493\n",
      "Training Epoch 3  92.5% | batch:       714 of       772\t|\tloss: 1.88699\n",
      "Training Epoch 3  92.6% | batch:       715 of       772\t|\tloss: 1.65218\n",
      "Training Epoch 3  92.7% | batch:       716 of       772\t|\tloss: 1.77123\n",
      "Training Epoch 3  92.9% | batch:       717 of       772\t|\tloss: 1.8194\n",
      "Training Epoch 3  93.0% | batch:       718 of       772\t|\tloss: 1.84309\n",
      "Training Epoch 3  93.1% | batch:       719 of       772\t|\tloss: 2.1012\n",
      "Training Epoch 3  93.3% | batch:       720 of       772\t|\tloss: 1.89038\n",
      "Training Epoch 3  93.4% | batch:       721 of       772\t|\tloss: 1.3256\n",
      "Training Epoch 3  93.5% | batch:       722 of       772\t|\tloss: 1.91011\n",
      "Training Epoch 3  93.7% | batch:       723 of       772\t|\tloss: 1.71728\n",
      "Training Epoch 3  93.8% | batch:       724 of       772\t|\tloss: 2.07904\n",
      "Training Epoch 3  93.9% | batch:       725 of       772\t|\tloss: 1.68289\n",
      "Training Epoch 3  94.0% | batch:       726 of       772\t|\tloss: 1.8308\n",
      "Training Epoch 3  94.2% | batch:       727 of       772\t|\tloss: 2.02852\n",
      "Training Epoch 3  94.3% | batch:       728 of       772\t|\tloss: 2.46018\n",
      "Training Epoch 3  94.4% | batch:       729 of       772\t|\tloss: 2.1429\n",
      "Training Epoch 3  94.6% | batch:       730 of       772\t|\tloss: 2.39263\n",
      "Training Epoch 3  94.7% | batch:       731 of       772\t|\tloss: 1.91811\n",
      "Training Epoch 3  94.8% | batch:       732 of       772\t|\tloss: 1.84854\n",
      "Training Epoch 3  94.9% | batch:       733 of       772\t|\tloss: 2.00402\n",
      "Training Epoch 3  95.1% | batch:       734 of       772\t|\tloss: 1.92712\n",
      "Training Epoch 3  95.2% | batch:       735 of       772\t|\tloss: 1.77729\n",
      "Training Epoch 3  95.3% | batch:       736 of       772\t|\tloss: 1.82248\n",
      "Training Epoch 3  95.5% | batch:       737 of       772\t|\tloss: 1.95731\n",
      "Training Epoch 3  95.6% | batch:       738 of       772\t|\tloss: 1.87126\n",
      "Training Epoch 3  95.7% | batch:       739 of       772\t|\tloss: 1.85181\n",
      "Training Epoch 3  95.9% | batch:       740 of       772\t|\tloss: 1.80428\n",
      "Training Epoch 3  96.0% | batch:       741 of       772\t|\tloss: 1.46795\n",
      "Training Epoch 3  96.1% | batch:       742 of       772\t|\tloss: 2.03139\n",
      "Training Epoch 3  96.2% | batch:       743 of       772\t|\tloss: 1.42921\n",
      "Training Epoch 3  96.4% | batch:       744 of       772\t|\tloss: 1.4775\n",
      "Training Epoch 3  96.5% | batch:       745 of       772\t|\tloss: 1.83868\n",
      "Training Epoch 3  96.6% | batch:       746 of       772\t|\tloss: 1.61431\n",
      "Training Epoch 3  96.8% | batch:       747 of       772\t|\tloss: 1.62923\n",
      "Training Epoch 3  96.9% | batch:       748 of       772\t|\tloss: 1.81488\n",
      "Training Epoch 3  97.0% | batch:       749 of       772\t|\tloss: 2.01402\n",
      "Training Epoch 3  97.2% | batch:       750 of       772\t|\tloss: 1.71992\n",
      "Training Epoch 3  97.3% | batch:       751 of       772\t|\tloss: 1.81554\n",
      "Training Epoch 3  97.4% | batch:       752 of       772\t|\tloss: 2.12718\n",
      "Training Epoch 3  97.5% | batch:       753 of       772\t|\tloss: 1.65982\n",
      "Training Epoch 3  97.7% | batch:       754 of       772\t|\tloss: 1.59082\n",
      "Training Epoch 3  97.8% | batch:       755 of       772\t|\tloss: 1.74952\n",
      "Training Epoch 3  97.9% | batch:       756 of       772\t|\tloss: 1.88161\n",
      "Training Epoch 3  98.1% | batch:       757 of       772\t|\tloss: 2.29652\n",
      "Training Epoch 3  98.2% | batch:       758 of       772\t|\tloss: 1.82255\n",
      "Training Epoch 3  98.3% | batch:       759 of       772\t|\tloss: 1.74526\n",
      "Training Epoch 3  98.4% | batch:       760 of       772\t|\tloss: 1.91575\n",
      "Training Epoch 3  98.6% | batch:       761 of       772\t|\tloss: 1.62451\n",
      "Training Epoch 3  98.7% | batch:       762 of       772\t|\tloss: 1.66303\n",
      "Training Epoch 3  98.8% | batch:       763 of       772\t|\tloss: 1.50707\n",
      "Training Epoch 3  99.0% | batch:       764 of       772\t|\tloss: 1.59727\n",
      "Training Epoch 3  99.1% | batch:       765 of       772\t|\tloss: 1.83283\n",
      "Training Epoch 3  99.2% | batch:       766 of       772\t|\tloss: 1.54585\n",
      "Training Epoch 3  99.4% | batch:       767 of       772\t|\tloss: 1.76585\n",
      "Training Epoch 3  99.5% | batch:       768 of       772\t|\tloss: 1.52566\n",
      "Training Epoch 3  99.6% | batch:       769 of       772\t|\tloss: 1.66066\n",
      "Training Epoch 3  99.7% | batch:       770 of       772\t|\tloss: 1.53739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:21:02,065 | INFO : Epoch 3 Training Summary: epoch: 3.000000 | loss: 2.927122 | \n",
      "2023-05-24 10:21:02,065 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 16.87193512916565 seconds\n",
      "\n",
      "2023-05-24 10:21:02,066 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 16.873460690180462 seconds\n",
      "2023-05-24 10:21:02,066 | INFO : Avg batch train. time: 0.021856814365518732 seconds\n",
      "2023-05-24 10:21:02,067 | INFO : Avg sample train. time: 0.00017075981834740485 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  99.9% | batch:       771 of       772\t|\tloss: 1.51946\n",
      "\n",
      "Training Epoch 4   0.0% | batch:         0 of       772\t|\tloss: 1.86014\n",
      "Training Epoch 4   0.1% | batch:         1 of       772\t|\tloss: 1.73151\n",
      "Training Epoch 4   0.3% | batch:         2 of       772\t|\tloss: 1.73257\n",
      "Training Epoch 4   0.4% | batch:         3 of       772\t|\tloss: 1.77724\n",
      "Training Epoch 4   0.5% | batch:         4 of       772\t|\tloss: 1.65965\n",
      "Training Epoch 4   0.6% | batch:         5 of       772\t|\tloss: 1.93299\n",
      "Training Epoch 4   0.8% | batch:         6 of       772\t|\tloss: 1.7416\n",
      "Training Epoch 4   0.9% | batch:         7 of       772\t|\tloss: 1.48054\n",
      "Training Epoch 4   1.0% | batch:         8 of       772\t|\tloss: 1.58852\n",
      "Training Epoch 4   1.2% | batch:         9 of       772\t|\tloss: 1.58914\n",
      "Training Epoch 4   1.3% | batch:        10 of       772\t|\tloss: 1.27573\n",
      "Training Epoch 4   1.4% | batch:        11 of       772\t|\tloss: 2.30744\n",
      "Training Epoch 4   1.6% | batch:        12 of       772\t|\tloss: 1.65015\n",
      "Training Epoch 4   1.7% | batch:        13 of       772\t|\tloss: 1.44942\n",
      "Training Epoch 4   1.8% | batch:        14 of       772\t|\tloss: 1.7846\n",
      "Training Epoch 4   1.9% | batch:        15 of       772\t|\tloss: 1.69961\n",
      "Training Epoch 4   2.1% | batch:        16 of       772\t|\tloss: 1.29529\n",
      "Training Epoch 4   2.2% | batch:        17 of       772\t|\tloss: 1.38663\n",
      "Training Epoch 4   2.3% | batch:        18 of       772\t|\tloss: 1.68342\n",
      "Training Epoch 4   2.5% | batch:        19 of       772\t|\tloss: 1.37217\n",
      "Training Epoch 4   2.6% | batch:        20 of       772\t|\tloss: 1.54577\n",
      "Training Epoch 4   2.7% | batch:        21 of       772\t|\tloss: 1.70419\n",
      "Training Epoch 4   2.8% | batch:        22 of       772\t|\tloss: 1.3078\n",
      "Training Epoch 4   3.0% | batch:        23 of       772\t|\tloss: 1.61679\n",
      "Training Epoch 4   3.1% | batch:        24 of       772\t|\tloss: 1.86005\n",
      "Training Epoch 4   3.2% | batch:        25 of       772\t|\tloss: 1.3352\n",
      "Training Epoch 4   3.4% | batch:        26 of       772\t|\tloss: 1.32646\n",
      "Training Epoch 4   3.5% | batch:        27 of       772\t|\tloss: 1.37146\n",
      "Training Epoch 4   3.6% | batch:        28 of       772\t|\tloss: 1.39149\n",
      "Training Epoch 4   3.8% | batch:        29 of       772\t|\tloss: 2.15215\n",
      "Training Epoch 4   3.9% | batch:        30 of       772\t|\tloss: 1.7297\n",
      "Training Epoch 4   4.0% | batch:        31 of       772\t|\tloss: 2.45217\n",
      "Training Epoch 4   4.1% | batch:        32 of       772\t|\tloss: 1.37263\n",
      "Training Epoch 4   4.3% | batch:        33 of       772\t|\tloss: 1.59612\n",
      "Training Epoch 4   4.4% | batch:        34 of       772\t|\tloss: 1.56257\n",
      "Training Epoch 4   4.5% | batch:        35 of       772\t|\tloss: 1.88344\n",
      "Training Epoch 4   4.7% | batch:        36 of       772\t|\tloss: 1.17312\n",
      "Training Epoch 4   4.8% | batch:        37 of       772\t|\tloss: 1.48014\n",
      "Training Epoch 4   4.9% | batch:        38 of       772\t|\tloss: 1.74256\n",
      "Training Epoch 4   5.1% | batch:        39 of       772\t|\tloss: 1.50649\n",
      "Training Epoch 4   5.2% | batch:        40 of       772\t|\tloss: 1.87724\n",
      "Training Epoch 4   5.3% | batch:        41 of       772\t|\tloss: 1.80821\n",
      "Training Epoch 4   5.4% | batch:        42 of       772\t|\tloss: 1.46378\n",
      "Training Epoch 4   5.6% | batch:        43 of       772\t|\tloss: 1.5822\n",
      "Training Epoch 4   5.7% | batch:        44 of       772\t|\tloss: 1.4843\n",
      "Training Epoch 4   5.8% | batch:        45 of       772\t|\tloss: 1.86114\n",
      "Training Epoch 4   6.0% | batch:        46 of       772\t|\tloss: 1.39301\n",
      "Training Epoch 4   6.1% | batch:        47 of       772\t|\tloss: 1.5354\n",
      "Training Epoch 4   6.2% | batch:        48 of       772\t|\tloss: 1.77841\n",
      "Training Epoch 4   6.3% | batch:        49 of       772\t|\tloss: 2.16667\n",
      "Training Epoch 4   6.5% | batch:        50 of       772\t|\tloss: 2.57416\n",
      "Training Epoch 4   6.6% | batch:        51 of       772\t|\tloss: 1.42308\n",
      "Training Epoch 4   6.7% | batch:        52 of       772\t|\tloss: 1.49008\n",
      "Training Epoch 4   6.9% | batch:        53 of       772\t|\tloss: 1.84203\n",
      "Training Epoch 4   7.0% | batch:        54 of       772\t|\tloss: 1.98676\n",
      "Training Epoch 4   7.1% | batch:        55 of       772\t|\tloss: 1.35744\n",
      "Training Epoch 4   7.3% | batch:        56 of       772\t|\tloss: 1.70806\n",
      "Training Epoch 4   7.4% | batch:        57 of       772\t|\tloss: 1.61678\n",
      "Training Epoch 4   7.5% | batch:        58 of       772\t|\tloss: 1.67577\n",
      "Training Epoch 4   7.6% | batch:        59 of       772\t|\tloss: 1.55261\n",
      "Training Epoch 4   7.8% | batch:        60 of       772\t|\tloss: 1.96402\n",
      "Training Epoch 4   7.9% | batch:        61 of       772\t|\tloss: 1.38755\n",
      "Training Epoch 4   8.0% | batch:        62 of       772\t|\tloss: 1.40181\n",
      "Training Epoch 4   8.2% | batch:        63 of       772\t|\tloss: 1.69659\n",
      "Training Epoch 4   8.3% | batch:        64 of       772\t|\tloss: 1.44372\n",
      "Training Epoch 4   8.4% | batch:        65 of       772\t|\tloss: 1.5004\n",
      "Training Epoch 4   8.5% | batch:        66 of       772\t|\tloss: 1.99849\n",
      "Training Epoch 4   8.7% | batch:        67 of       772\t|\tloss: 1.42789\n",
      "Training Epoch 4   8.8% | batch:        68 of       772\t|\tloss: 2.03585\n",
      "Training Epoch 4   8.9% | batch:        69 of       772\t|\tloss: 1.51171\n",
      "Training Epoch 4   9.1% | batch:        70 of       772\t|\tloss: 1.48735\n",
      "Training Epoch 4   9.2% | batch:        71 of       772\t|\tloss: 1.55981\n",
      "Training Epoch 4   9.3% | batch:        72 of       772\t|\tloss: 1.5481\n",
      "Training Epoch 4   9.5% | batch:        73 of       772\t|\tloss: 1.50898\n",
      "Training Epoch 4   9.6% | batch:        74 of       772\t|\tloss: 1.59426\n",
      "Training Epoch 4   9.7% | batch:        75 of       772\t|\tloss: 1.58743\n",
      "Training Epoch 4   9.8% | batch:        76 of       772\t|\tloss: 1.78427\n",
      "Training Epoch 4  10.0% | batch:        77 of       772\t|\tloss: 1.3108\n",
      "Training Epoch 4  10.1% | batch:        78 of       772\t|\tloss: 1.45499\n",
      "Training Epoch 4  10.2% | batch:        79 of       772\t|\tloss: 1.47819\n",
      "Training Epoch 4  10.4% | batch:        80 of       772\t|\tloss: 1.79487\n",
      "Training Epoch 4  10.5% | batch:        81 of       772\t|\tloss: 1.25742\n",
      "Training Epoch 4  10.6% | batch:        82 of       772\t|\tloss: 1.93133\n",
      "Training Epoch 4  10.8% | batch:        83 of       772\t|\tloss: 1.49068\n",
      "Training Epoch 4  10.9% | batch:        84 of       772\t|\tloss: 1.90724\n",
      "Training Epoch 4  11.0% | batch:        85 of       772\t|\tloss: 1.80503\n",
      "Training Epoch 4  11.1% | batch:        86 of       772\t|\tloss: 1.49589\n",
      "Training Epoch 4  11.3% | batch:        87 of       772\t|\tloss: 1.50406\n",
      "Training Epoch 4  11.4% | batch:        88 of       772\t|\tloss: 1.56297\n",
      "Training Epoch 4  11.5% | batch:        89 of       772\t|\tloss: 1.7696\n",
      "Training Epoch 4  11.7% | batch:        90 of       772\t|\tloss: 1.33372\n",
      "Training Epoch 4  11.8% | batch:        91 of       772\t|\tloss: 1.7612\n",
      "Training Epoch 4  11.9% | batch:        92 of       772\t|\tloss: 1.91931\n",
      "Training Epoch 4  12.0% | batch:        93 of       772\t|\tloss: 1.40265\n",
      "Training Epoch 4  12.2% | batch:        94 of       772\t|\tloss: 1.47966\n",
      "Training Epoch 4  12.3% | batch:        95 of       772\t|\tloss: 1.70086\n",
      "Training Epoch 4  12.4% | batch:        96 of       772\t|\tloss: 1.70169\n",
      "Training Epoch 4  12.6% | batch:        97 of       772\t|\tloss: 1.60051\n",
      "Training Epoch 4  12.7% | batch:        98 of       772\t|\tloss: 1.63828\n",
      "Training Epoch 4  12.8% | batch:        99 of       772\t|\tloss: 1.63333\n",
      "Training Epoch 4  13.0% | batch:       100 of       772\t|\tloss: 1.49402\n",
      "Training Epoch 4  13.1% | batch:       101 of       772\t|\tloss: 1.54103\n",
      "Training Epoch 4  13.2% | batch:       102 of       772\t|\tloss: 1.72312\n",
      "Training Epoch 4  13.3% | batch:       103 of       772\t|\tloss: 1.59187\n",
      "Training Epoch 4  13.5% | batch:       104 of       772\t|\tloss: 2.08062\n",
      "Training Epoch 4  13.6% | batch:       105 of       772\t|\tloss: 1.54376\n",
      "Training Epoch 4  13.7% | batch:       106 of       772\t|\tloss: 1.41957\n",
      "Training Epoch 4  13.9% | batch:       107 of       772\t|\tloss: 1.83676\n",
      "Training Epoch 4  14.0% | batch:       108 of       772\t|\tloss: 2.24276\n",
      "Training Epoch 4  14.1% | batch:       109 of       772\t|\tloss: 1.25132\n",
      "Training Epoch 4  14.2% | batch:       110 of       772\t|\tloss: 1.18902\n",
      "Training Epoch 4  14.4% | batch:       111 of       772\t|\tloss: 1.78856\n",
      "Training Epoch 4  14.5% | batch:       112 of       772\t|\tloss: 1.90535\n",
      "Training Epoch 4  14.6% | batch:       113 of       772\t|\tloss: 1.39315\n",
      "Training Epoch 4  14.8% | batch:       114 of       772\t|\tloss: 1.83598\n",
      "Training Epoch 4  14.9% | batch:       115 of       772\t|\tloss: 1.30058\n",
      "Training Epoch 4  15.0% | batch:       116 of       772\t|\tloss: 1.26793\n",
      "Training Epoch 4  15.2% | batch:       117 of       772\t|\tloss: 1.76489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  15.3% | batch:       118 of       772\t|\tloss: 1.81305\n",
      "Training Epoch 4  15.4% | batch:       119 of       772\t|\tloss: 1.75259\n",
      "Training Epoch 4  15.5% | batch:       120 of       772\t|\tloss: 1.40158\n",
      "Training Epoch 4  15.7% | batch:       121 of       772\t|\tloss: 1.47819\n",
      "Training Epoch 4  15.8% | batch:       122 of       772\t|\tloss: 1.77132\n",
      "Training Epoch 4  15.9% | batch:       123 of       772\t|\tloss: 1.59964\n",
      "Training Epoch 4  16.1% | batch:       124 of       772\t|\tloss: 1.41992\n",
      "Training Epoch 4  16.2% | batch:       125 of       772\t|\tloss: 1.93245\n",
      "Training Epoch 4  16.3% | batch:       126 of       772\t|\tloss: 1.31803\n",
      "Training Epoch 4  16.5% | batch:       127 of       772\t|\tloss: 1.40682\n",
      "Training Epoch 4  16.6% | batch:       128 of       772\t|\tloss: 1.77993\n",
      "Training Epoch 4  16.7% | batch:       129 of       772\t|\tloss: 1.51123\n",
      "Training Epoch 4  16.8% | batch:       130 of       772\t|\tloss: 1.62442\n",
      "Training Epoch 4  17.0% | batch:       131 of       772\t|\tloss: 1.47578\n",
      "Training Epoch 4  17.1% | batch:       132 of       772\t|\tloss: 1.84604\n",
      "Training Epoch 4  17.2% | batch:       133 of       772\t|\tloss: 1.46357\n",
      "Training Epoch 4  17.4% | batch:       134 of       772\t|\tloss: 1.56239\n",
      "Training Epoch 4  17.5% | batch:       135 of       772\t|\tloss: 1.29301\n",
      "Training Epoch 4  17.6% | batch:       136 of       772\t|\tloss: 1.35245\n",
      "Training Epoch 4  17.7% | batch:       137 of       772\t|\tloss: 1.91101\n",
      "Training Epoch 4  17.9% | batch:       138 of       772\t|\tloss: 1.32575\n",
      "Training Epoch 4  18.0% | batch:       139 of       772\t|\tloss: 1.60823\n",
      "Training Epoch 4  18.1% | batch:       140 of       772\t|\tloss: 1.49262\n",
      "Training Epoch 4  18.3% | batch:       141 of       772\t|\tloss: 1.38675\n",
      "Training Epoch 4  18.4% | batch:       142 of       772\t|\tloss: 1.1089\n",
      "Training Epoch 4  18.5% | batch:       143 of       772\t|\tloss: 1.49359\n",
      "Training Epoch 4  18.7% | batch:       144 of       772\t|\tloss: 1.53993\n",
      "Training Epoch 4  18.8% | batch:       145 of       772\t|\tloss: 1.14418\n",
      "Training Epoch 4  18.9% | batch:       146 of       772\t|\tloss: 1.53732\n",
      "Training Epoch 4  19.0% | batch:       147 of       772\t|\tloss: 1.49029\n",
      "Training Epoch 4  19.2% | batch:       148 of       772\t|\tloss: 1.34809\n",
      "Training Epoch 4  19.3% | batch:       149 of       772\t|\tloss: 1.55041\n",
      "Training Epoch 4  19.4% | batch:       150 of       772\t|\tloss: 1.23055\n",
      "Training Epoch 4  19.6% | batch:       151 of       772\t|\tloss: 1.07306\n",
      "Training Epoch 4  19.7% | batch:       152 of       772\t|\tloss: 1.98926\n",
      "Training Epoch 4  19.8% | batch:       153 of       772\t|\tloss: 1.53658\n",
      "Training Epoch 4  19.9% | batch:       154 of       772\t|\tloss: 1.31402\n",
      "Training Epoch 4  20.1% | batch:       155 of       772\t|\tloss: 1.34488\n",
      "Training Epoch 4  20.2% | batch:       156 of       772\t|\tloss: 1.28246\n",
      "Training Epoch 4  20.3% | batch:       157 of       772\t|\tloss: 1.73286\n",
      "Training Epoch 4  20.5% | batch:       158 of       772\t|\tloss: 1.68463\n",
      "Training Epoch 4  20.6% | batch:       159 of       772\t|\tloss: 1.43032\n",
      "Training Epoch 4  20.7% | batch:       160 of       772\t|\tloss: 1.39528\n",
      "Training Epoch 4  20.9% | batch:       161 of       772\t|\tloss: 1.29206\n",
      "Training Epoch 4  21.0% | batch:       162 of       772\t|\tloss: 1.49349\n",
      "Training Epoch 4  21.1% | batch:       163 of       772\t|\tloss: 1.72974\n",
      "Training Epoch 4  21.2% | batch:       164 of       772\t|\tloss: 1.85468\n",
      "Training Epoch 4  21.4% | batch:       165 of       772\t|\tloss: 1.42542\n",
      "Training Epoch 4  21.5% | batch:       166 of       772\t|\tloss: 1.50618\n",
      "Training Epoch 4  21.6% | batch:       167 of       772\t|\tloss: 1.63876\n",
      "Training Epoch 4  21.8% | batch:       168 of       772\t|\tloss: 1.91919\n",
      "Training Epoch 4  21.9% | batch:       169 of       772\t|\tloss: 1.51382\n",
      "Training Epoch 4  22.0% | batch:       170 of       772\t|\tloss: 1.60005\n",
      "Training Epoch 4  22.2% | batch:       171 of       772\t|\tloss: 1.67187\n",
      "Training Epoch 4  22.3% | batch:       172 of       772\t|\tloss: 1.15799\n",
      "Training Epoch 4  22.4% | batch:       173 of       772\t|\tloss: 1.19488\n",
      "Training Epoch 4  22.5% | batch:       174 of       772\t|\tloss: 1.16872\n",
      "Training Epoch 4  22.7% | batch:       175 of       772\t|\tloss: 1.09127\n",
      "Training Epoch 4  22.8% | batch:       176 of       772\t|\tloss: 1.87948\n",
      "Training Epoch 4  22.9% | batch:       177 of       772\t|\tloss: 1.9339\n",
      "Training Epoch 4  23.1% | batch:       178 of       772\t|\tloss: 1.55445\n",
      "Training Epoch 4  23.2% | batch:       179 of       772\t|\tloss: 1.42669\n",
      "Training Epoch 4  23.3% | batch:       180 of       772\t|\tloss: 1.59726\n",
      "Training Epoch 4  23.4% | batch:       181 of       772\t|\tloss: 1.10583\n",
      "Training Epoch 4  23.6% | batch:       182 of       772\t|\tloss: 1.2193\n",
      "Training Epoch 4  23.7% | batch:       183 of       772\t|\tloss: 1.27411\n",
      "Training Epoch 4  23.8% | batch:       184 of       772\t|\tloss: 1.04187\n",
      "Training Epoch 4  24.0% | batch:       185 of       772\t|\tloss: 1.81515\n",
      "Training Epoch 4  24.1% | batch:       186 of       772\t|\tloss: 1.46078\n",
      "Training Epoch 4  24.2% | batch:       187 of       772\t|\tloss: 1.67599\n",
      "Training Epoch 4  24.4% | batch:       188 of       772\t|\tloss: 1.24997\n",
      "Training Epoch 4  24.5% | batch:       189 of       772\t|\tloss: 1.31628\n",
      "Training Epoch 4  24.6% | batch:       190 of       772\t|\tloss: 1.10853\n",
      "Training Epoch 4  24.7% | batch:       191 of       772\t|\tloss: 1.57616\n",
      "Training Epoch 4  24.9% | batch:       192 of       772\t|\tloss: 1.5377\n",
      "Training Epoch 4  25.0% | batch:       193 of       772\t|\tloss: 1.24139\n",
      "Training Epoch 4  25.1% | batch:       194 of       772\t|\tloss: 1.58959\n",
      "Training Epoch 4  25.3% | batch:       195 of       772\t|\tloss: 1.42108\n",
      "Training Epoch 4  25.4% | batch:       196 of       772\t|\tloss: 1.70783\n",
      "Training Epoch 4  25.5% | batch:       197 of       772\t|\tloss: 1.34067\n",
      "Training Epoch 4  25.6% | batch:       198 of       772\t|\tloss: 1.37913\n",
      "Training Epoch 4  25.8% | batch:       199 of       772\t|\tloss: 1.351\n",
      "Training Epoch 4  25.9% | batch:       200 of       772\t|\tloss: 1.56129\n",
      "Training Epoch 4  26.0% | batch:       201 of       772\t|\tloss: 1.33457\n",
      "Training Epoch 4  26.2% | batch:       202 of       772\t|\tloss: 1.64195\n",
      "Training Epoch 4  26.3% | batch:       203 of       772\t|\tloss: 1.26184\n",
      "Training Epoch 4  26.4% | batch:       204 of       772\t|\tloss: 1.99211\n",
      "Training Epoch 4  26.6% | batch:       205 of       772\t|\tloss: 1.55191\n",
      "Training Epoch 4  26.7% | batch:       206 of       772\t|\tloss: 1.84541\n",
      "Training Epoch 4  26.8% | batch:       207 of       772\t|\tloss: 1.51499\n",
      "Training Epoch 4  26.9% | batch:       208 of       772\t|\tloss: 1.30074\n",
      "Training Epoch 4  27.1% | batch:       209 of       772\t|\tloss: 1.25566\n",
      "Training Epoch 4  27.2% | batch:       210 of       772\t|\tloss: 1.66479\n",
      "Training Epoch 4  27.3% | batch:       211 of       772\t|\tloss: 1.46604\n",
      "Training Epoch 4  27.5% | batch:       212 of       772\t|\tloss: 1.54358\n",
      "Training Epoch 4  27.6% | batch:       213 of       772\t|\tloss: 1.19623\n",
      "Training Epoch 4  27.7% | batch:       214 of       772\t|\tloss: 1.73393\n",
      "Training Epoch 4  27.8% | batch:       215 of       772\t|\tloss: 1.52754\n",
      "Training Epoch 4  28.0% | batch:       216 of       772\t|\tloss: 1.64833\n",
      "Training Epoch 4  28.1% | batch:       217 of       772\t|\tloss: 1.49003\n",
      "Training Epoch 4  28.2% | batch:       218 of       772\t|\tloss: 1.55175\n",
      "Training Epoch 4  28.4% | batch:       219 of       772\t|\tloss: 1.43759\n",
      "Training Epoch 4  28.5% | batch:       220 of       772\t|\tloss: 1.49863\n",
      "Training Epoch 4  28.6% | batch:       221 of       772\t|\tloss: 1.55951\n",
      "Training Epoch 4  28.8% | batch:       222 of       772\t|\tloss: 1.39716\n",
      "Training Epoch 4  28.9% | batch:       223 of       772\t|\tloss: 1.20825\n",
      "Training Epoch 4  29.0% | batch:       224 of       772\t|\tloss: 1.67574\n",
      "Training Epoch 4  29.1% | batch:       225 of       772\t|\tloss: 1.27891\n",
      "Training Epoch 4  29.3% | batch:       226 of       772\t|\tloss: 1.27077\n",
      "Training Epoch 4  29.4% | batch:       227 of       772\t|\tloss: 1.19555\n",
      "Training Epoch 4  29.5% | batch:       228 of       772\t|\tloss: 1.26481\n",
      "Training Epoch 4  29.7% | batch:       229 of       772\t|\tloss: 1.51361\n",
      "Training Epoch 4  29.8% | batch:       230 of       772\t|\tloss: 1.26821\n",
      "Training Epoch 4  29.9% | batch:       231 of       772\t|\tloss: 1.16249\n",
      "Training Epoch 4  30.1% | batch:       232 of       772\t|\tloss: 1.67755\n",
      "Training Epoch 4  30.2% | batch:       233 of       772\t|\tloss: 1.0593\n",
      "Training Epoch 4  30.3% | batch:       234 of       772\t|\tloss: 1.26661\n",
      "Training Epoch 4  30.4% | batch:       235 of       772\t|\tloss: 1.52632\n",
      "Training Epoch 4  30.6% | batch:       236 of       772\t|\tloss: 1.54435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  30.7% | batch:       237 of       772\t|\tloss: 1.24206\n",
      "Training Epoch 4  30.8% | batch:       238 of       772\t|\tloss: 1.73804\n",
      "Training Epoch 4  31.0% | batch:       239 of       772\t|\tloss: 1.30977\n",
      "Training Epoch 4  31.1% | batch:       240 of       772\t|\tloss: 1.45726\n",
      "Training Epoch 4  31.2% | batch:       241 of       772\t|\tloss: 1.53005\n",
      "Training Epoch 4  31.3% | batch:       242 of       772\t|\tloss: 1.50227\n",
      "Training Epoch 4  31.5% | batch:       243 of       772\t|\tloss: 1.0857\n",
      "Training Epoch 4  31.6% | batch:       244 of       772\t|\tloss: 1.13471\n",
      "Training Epoch 4  31.7% | batch:       245 of       772\t|\tloss: 1.17842\n",
      "Training Epoch 4  31.9% | batch:       246 of       772\t|\tloss: 2.02\n",
      "Training Epoch 4  32.0% | batch:       247 of       772\t|\tloss: 1.36292\n",
      "Training Epoch 4  32.1% | batch:       248 of       772\t|\tloss: 1.79601\n",
      "Training Epoch 4  32.3% | batch:       249 of       772\t|\tloss: 1.29126\n",
      "Training Epoch 4  32.4% | batch:       250 of       772\t|\tloss: 1.29545\n",
      "Training Epoch 4  32.5% | batch:       251 of       772\t|\tloss: 1.2955\n",
      "Training Epoch 4  32.6% | batch:       252 of       772\t|\tloss: 1.2935\n",
      "Training Epoch 4  32.8% | batch:       253 of       772\t|\tloss: 1.34378\n",
      "Training Epoch 4  32.9% | batch:       254 of       772\t|\tloss: 1.41073\n",
      "Training Epoch 4  33.0% | batch:       255 of       772\t|\tloss: 1.37948\n",
      "Training Epoch 4  33.2% | batch:       256 of       772\t|\tloss: 1.14675\n",
      "Training Epoch 4  33.3% | batch:       257 of       772\t|\tloss: 0.811401\n",
      "Training Epoch 4  33.4% | batch:       258 of       772\t|\tloss: 1.02057\n",
      "Training Epoch 4  33.5% | batch:       259 of       772\t|\tloss: 1.42396\n",
      "Training Epoch 4  33.7% | batch:       260 of       772\t|\tloss: 1.26556\n",
      "Training Epoch 4  33.8% | batch:       261 of       772\t|\tloss: 1.52226\n",
      "Training Epoch 4  33.9% | batch:       262 of       772\t|\tloss: 1.24158\n",
      "Training Epoch 4  34.1% | batch:       263 of       772\t|\tloss: 1.50088\n",
      "Training Epoch 4  34.2% | batch:       264 of       772\t|\tloss: 1.18812\n",
      "Training Epoch 4  34.3% | batch:       265 of       772\t|\tloss: 1.84244\n",
      "Training Epoch 4  34.5% | batch:       266 of       772\t|\tloss: 1.2494\n",
      "Training Epoch 4  34.6% | batch:       267 of       772\t|\tloss: 1.39332\n",
      "Training Epoch 4  34.7% | batch:       268 of       772\t|\tloss: 1.36818\n",
      "Training Epoch 4  34.8% | batch:       269 of       772\t|\tloss: 1.24005\n",
      "Training Epoch 4  35.0% | batch:       270 of       772\t|\tloss: 1.18553\n",
      "Training Epoch 4  35.1% | batch:       271 of       772\t|\tloss: 1.86163\n",
      "Training Epoch 4  35.2% | batch:       272 of       772\t|\tloss: 1.4911\n",
      "Training Epoch 4  35.4% | batch:       273 of       772\t|\tloss: 1.56741\n",
      "Training Epoch 4  35.5% | batch:       274 of       772\t|\tloss: 1.56236\n",
      "Training Epoch 4  35.6% | batch:       275 of       772\t|\tloss: 1.72361\n",
      "Training Epoch 4  35.8% | batch:       276 of       772\t|\tloss: 1.25072\n",
      "Training Epoch 4  35.9% | batch:       277 of       772\t|\tloss: 1.04771\n",
      "Training Epoch 4  36.0% | batch:       278 of       772\t|\tloss: 1.16254\n",
      "Training Epoch 4  36.1% | batch:       279 of       772\t|\tloss: 1.45338\n",
      "Training Epoch 4  36.3% | batch:       280 of       772\t|\tloss: 1.20031\n",
      "Training Epoch 4  36.4% | batch:       281 of       772\t|\tloss: 1.33897\n",
      "Training Epoch 4  36.5% | batch:       282 of       772\t|\tloss: 1.08306\n",
      "Training Epoch 4  36.7% | batch:       283 of       772\t|\tloss: 1.74786\n",
      "Training Epoch 4  36.8% | batch:       284 of       772\t|\tloss: 1.5623\n",
      "Training Epoch 4  36.9% | batch:       285 of       772\t|\tloss: 2.37283\n",
      "Training Epoch 4  37.0% | batch:       286 of       772\t|\tloss: 1.32739\n",
      "Training Epoch 4  37.2% | batch:       287 of       772\t|\tloss: 1.26799\n",
      "Training Epoch 4  37.3% | batch:       288 of       772\t|\tloss: 1.68509\n",
      "Training Epoch 4  37.4% | batch:       289 of       772\t|\tloss: 1.28607\n",
      "Training Epoch 4  37.6% | batch:       290 of       772\t|\tloss: 1.4406\n",
      "Training Epoch 4  37.7% | batch:       291 of       772\t|\tloss: 1.03778\n",
      "Training Epoch 4  37.8% | batch:       292 of       772\t|\tloss: 1.0975\n",
      "Training Epoch 4  38.0% | batch:       293 of       772\t|\tloss: 1.37794\n",
      "Training Epoch 4  38.1% | batch:       294 of       772\t|\tloss: 1.3645\n",
      "Training Epoch 4  38.2% | batch:       295 of       772\t|\tloss: 1.71072\n",
      "Training Epoch 4  38.3% | batch:       296 of       772\t|\tloss: 1.19617\n",
      "Training Epoch 4  38.5% | batch:       297 of       772\t|\tloss: 1.47586\n",
      "Training Epoch 4  38.6% | batch:       298 of       772\t|\tloss: 1.02472\n",
      "Training Epoch 4  38.7% | batch:       299 of       772\t|\tloss: 1.50581\n",
      "Training Epoch 4  38.9% | batch:       300 of       772\t|\tloss: 2.064\n",
      "Training Epoch 4  39.0% | batch:       301 of       772\t|\tloss: 1.5782\n",
      "Training Epoch 4  39.1% | batch:       302 of       772\t|\tloss: 1.39202\n",
      "Training Epoch 4  39.2% | batch:       303 of       772\t|\tloss: 0.993816\n",
      "Training Epoch 4  39.4% | batch:       304 of       772\t|\tloss: 1.14672\n",
      "Training Epoch 4  39.5% | batch:       305 of       772\t|\tloss: 1.22255\n",
      "Training Epoch 4  39.6% | batch:       306 of       772\t|\tloss: 1.32278\n",
      "Training Epoch 4  39.8% | batch:       307 of       772\t|\tloss: 1.62389\n",
      "Training Epoch 4  39.9% | batch:       308 of       772\t|\tloss: 1.33638\n",
      "Training Epoch 4  40.0% | batch:       309 of       772\t|\tloss: 1.36472\n",
      "Training Epoch 4  40.2% | batch:       310 of       772\t|\tloss: 1.2879\n",
      "Training Epoch 4  40.3% | batch:       311 of       772\t|\tloss: 1.34049\n",
      "Training Epoch 4  40.4% | batch:       312 of       772\t|\tloss: 1.26578\n",
      "Training Epoch 4  40.5% | batch:       313 of       772\t|\tloss: 1.01014\n",
      "Training Epoch 4  40.7% | batch:       314 of       772\t|\tloss: 1.16509\n",
      "Training Epoch 4  40.8% | batch:       315 of       772\t|\tloss: 1.1663\n",
      "Training Epoch 4  40.9% | batch:       316 of       772\t|\tloss: 1.33877\n",
      "Training Epoch 4  41.1% | batch:       317 of       772\t|\tloss: 0.869463\n",
      "Training Epoch 4  41.2% | batch:       318 of       772\t|\tloss: 1.14747\n",
      "Training Epoch 4  41.3% | batch:       319 of       772\t|\tloss: 1.341\n",
      "Training Epoch 4  41.5% | batch:       320 of       772\t|\tloss: 1.081\n",
      "Training Epoch 4  41.6% | batch:       321 of       772\t|\tloss: 1.52841\n",
      "Training Epoch 4  41.7% | batch:       322 of       772\t|\tloss: 1.173\n",
      "Training Epoch 4  41.8% | batch:       323 of       772\t|\tloss: 1.85899\n",
      "Training Epoch 4  42.0% | batch:       324 of       772\t|\tloss: 1.51305\n",
      "Training Epoch 4  42.1% | batch:       325 of       772\t|\tloss: 1.22751\n",
      "Training Epoch 4  42.2% | batch:       326 of       772\t|\tloss: 1.25385\n",
      "Training Epoch 4  42.4% | batch:       327 of       772\t|\tloss: 1.58827\n",
      "Training Epoch 4  42.5% | batch:       328 of       772\t|\tloss: 1.76815\n",
      "Training Epoch 4  42.6% | batch:       329 of       772\t|\tloss: 1.35722\n",
      "Training Epoch 4  42.7% | batch:       330 of       772\t|\tloss: 1.59981\n",
      "Training Epoch 4  42.9% | batch:       331 of       772\t|\tloss: 0.912715\n",
      "Training Epoch 4  43.0% | batch:       332 of       772\t|\tloss: 1.64839\n",
      "Training Epoch 4  43.1% | batch:       333 of       772\t|\tloss: 1.66404\n",
      "Training Epoch 4  43.3% | batch:       334 of       772\t|\tloss: 1.14941\n",
      "Training Epoch 4  43.4% | batch:       335 of       772\t|\tloss: 1.16762\n",
      "Training Epoch 4  43.5% | batch:       336 of       772\t|\tloss: 1.08142\n",
      "Training Epoch 4  43.7% | batch:       337 of       772\t|\tloss: 1.42539\n",
      "Training Epoch 4  43.8% | batch:       338 of       772\t|\tloss: 1.80378\n",
      "Training Epoch 4  43.9% | batch:       339 of       772\t|\tloss: 1.08093\n",
      "Training Epoch 4  44.0% | batch:       340 of       772\t|\tloss: 1.20274\n",
      "Training Epoch 4  44.2% | batch:       341 of       772\t|\tloss: 1.61977\n",
      "Training Epoch 4  44.3% | batch:       342 of       772\t|\tloss: 1.90328\n",
      "Training Epoch 4  44.4% | batch:       343 of       772\t|\tloss: 1.15504\n",
      "Training Epoch 4  44.6% | batch:       344 of       772\t|\tloss: 1.17587\n",
      "Training Epoch 4  44.7% | batch:       345 of       772\t|\tloss: 1.35596\n",
      "Training Epoch 4  44.8% | batch:       346 of       772\t|\tloss: 1.82815\n",
      "Training Epoch 4  44.9% | batch:       347 of       772\t|\tloss: 1.10645\n",
      "Training Epoch 4  45.1% | batch:       348 of       772\t|\tloss: 1.57772\n",
      "Training Epoch 4  45.2% | batch:       349 of       772\t|\tloss: 1.55682\n",
      "Training Epoch 4  45.3% | batch:       350 of       772\t|\tloss: 1.32299\n",
      "Training Epoch 4  45.5% | batch:       351 of       772\t|\tloss: 1.1569\n",
      "Training Epoch 4  45.6% | batch:       352 of       772\t|\tloss: 1.20197\n",
      "Training Epoch 4  45.7% | batch:       353 of       772\t|\tloss: 1.08396\n",
      "Training Epoch 4  45.9% | batch:       354 of       772\t|\tloss: 1.2945\n",
      "Training Epoch 4  46.0% | batch:       355 of       772\t|\tloss: 1.47941\n",
      "Training Epoch 4  46.1% | batch:       356 of       772\t|\tloss: 1.51866\n",
      "Training Epoch 4  46.2% | batch:       357 of       772\t|\tloss: 1.16392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  46.4% | batch:       358 of       772\t|\tloss: 1.03375\n",
      "Training Epoch 4  46.5% | batch:       359 of       772\t|\tloss: 1.42345\n",
      "Training Epoch 4  46.6% | batch:       360 of       772\t|\tloss: 1.51875\n",
      "Training Epoch 4  46.8% | batch:       361 of       772\t|\tloss: 1.80842\n",
      "Training Epoch 4  46.9% | batch:       362 of       772\t|\tloss: 1.32484\n",
      "Training Epoch 4  47.0% | batch:       363 of       772\t|\tloss: 1.04368\n",
      "Training Epoch 4  47.2% | batch:       364 of       772\t|\tloss: 1.31394\n",
      "Training Epoch 4  47.3% | batch:       365 of       772\t|\tloss: 1.97626\n",
      "Training Epoch 4  47.4% | batch:       366 of       772\t|\tloss: 1.67763\n",
      "Training Epoch 4  47.5% | batch:       367 of       772\t|\tloss: 1.35636\n",
      "Training Epoch 4  47.7% | batch:       368 of       772\t|\tloss: 1.34402\n",
      "Training Epoch 4  47.8% | batch:       369 of       772\t|\tloss: 1.3894\n",
      "Training Epoch 4  47.9% | batch:       370 of       772\t|\tloss: 1.19261\n",
      "Training Epoch 4  48.1% | batch:       371 of       772\t|\tloss: 1.17421\n",
      "Training Epoch 4  48.2% | batch:       372 of       772\t|\tloss: 1.18684\n",
      "Training Epoch 4  48.3% | batch:       373 of       772\t|\tloss: 1.53408\n",
      "Training Epoch 4  48.4% | batch:       374 of       772\t|\tloss: 1.23998\n",
      "Training Epoch 4  48.6% | batch:       375 of       772\t|\tloss: 1.15496\n",
      "Training Epoch 4  48.7% | batch:       376 of       772\t|\tloss: 0.866391\n",
      "Training Epoch 4  48.8% | batch:       377 of       772\t|\tloss: 1.1915\n",
      "Training Epoch 4  49.0% | batch:       378 of       772\t|\tloss: 1.18508\n",
      "Training Epoch 4  49.1% | batch:       379 of       772\t|\tloss: 1.01323\n",
      "Training Epoch 4  49.2% | batch:       380 of       772\t|\tloss: 1.54068\n",
      "Training Epoch 4  49.4% | batch:       381 of       772\t|\tloss: 1.32592\n",
      "Training Epoch 4  49.5% | batch:       382 of       772\t|\tloss: 1.53444\n",
      "Training Epoch 4  49.6% | batch:       383 of       772\t|\tloss: 1.32906\n",
      "Training Epoch 4  49.7% | batch:       384 of       772\t|\tloss: 1.25233\n",
      "Training Epoch 4  49.9% | batch:       385 of       772\t|\tloss: 1.17229\n",
      "Training Epoch 4  50.0% | batch:       386 of       772\t|\tloss: 0.885311\n",
      "Training Epoch 4  50.1% | batch:       387 of       772\t|\tloss: 1.22937\n",
      "Training Epoch 4  50.3% | batch:       388 of       772\t|\tloss: 1.61952\n",
      "Training Epoch 4  50.4% | batch:       389 of       772\t|\tloss: 1.43505\n",
      "Training Epoch 4  50.5% | batch:       390 of       772\t|\tloss: 1.24359\n",
      "Training Epoch 4  50.6% | batch:       391 of       772\t|\tloss: 1.43495\n",
      "Training Epoch 4  50.8% | batch:       392 of       772\t|\tloss: 1.39367\n",
      "Training Epoch 4  50.9% | batch:       393 of       772\t|\tloss: 1.2554\n",
      "Training Epoch 4  51.0% | batch:       394 of       772\t|\tloss: 1.25\n",
      "Training Epoch 4  51.2% | batch:       395 of       772\t|\tloss: 1.37572\n",
      "Training Epoch 4  51.3% | batch:       396 of       772\t|\tloss: 1.00526\n",
      "Training Epoch 4  51.4% | batch:       397 of       772\t|\tloss: 1.3241\n",
      "Training Epoch 4  51.6% | batch:       398 of       772\t|\tloss: 1.05427\n",
      "Training Epoch 4  51.7% | batch:       399 of       772\t|\tloss: 1.15105\n",
      "Training Epoch 4  51.8% | batch:       400 of       772\t|\tloss: 1.03228\n",
      "Training Epoch 4  51.9% | batch:       401 of       772\t|\tloss: 1.07071\n",
      "Training Epoch 4  52.1% | batch:       402 of       772\t|\tloss: 1.55591\n",
      "Training Epoch 4  52.2% | batch:       403 of       772\t|\tloss: 1.28426\n",
      "Training Epoch 4  52.3% | batch:       404 of       772\t|\tloss: 1.19541\n",
      "Training Epoch 4  52.5% | batch:       405 of       772\t|\tloss: 1.28237\n",
      "Training Epoch 4  52.6% | batch:       406 of       772\t|\tloss: 0.984468\n",
      "Training Epoch 4  52.7% | batch:       407 of       772\t|\tloss: 1.47045\n",
      "Training Epoch 4  52.8% | batch:       408 of       772\t|\tloss: 1.03026\n",
      "Training Epoch 4  53.0% | batch:       409 of       772\t|\tloss: 1.29657\n",
      "Training Epoch 4  53.1% | batch:       410 of       772\t|\tloss: 1.14306\n",
      "Training Epoch 4  53.2% | batch:       411 of       772\t|\tloss: 1.1627\n",
      "Training Epoch 4  53.4% | batch:       412 of       772\t|\tloss: 1.34896\n",
      "Training Epoch 4  53.5% | batch:       413 of       772\t|\tloss: 1.28222\n",
      "Training Epoch 4  53.6% | batch:       414 of       772\t|\tloss: 0.858686\n",
      "Training Epoch 4  53.8% | batch:       415 of       772\t|\tloss: 1.17064\n",
      "Training Epoch 4  53.9% | batch:       416 of       772\t|\tloss: 1.30389\n",
      "Training Epoch 4  54.0% | batch:       417 of       772\t|\tloss: 1.39362\n",
      "Training Epoch 4  54.1% | batch:       418 of       772\t|\tloss: 1.80314\n",
      "Training Epoch 4  54.3% | batch:       419 of       772\t|\tloss: 0.933925\n",
      "Training Epoch 4  54.4% | batch:       420 of       772\t|\tloss: 1.18747\n",
      "Training Epoch 4  54.5% | batch:       421 of       772\t|\tloss: 1.05941\n",
      "Training Epoch 4  54.7% | batch:       422 of       772\t|\tloss: 1.59021\n",
      "Training Epoch 4  54.8% | batch:       423 of       772\t|\tloss: 1.17795\n",
      "Training Epoch 4  54.9% | batch:       424 of       772\t|\tloss: 1.08427\n",
      "Training Epoch 4  55.1% | batch:       425 of       772\t|\tloss: 1.09529\n",
      "Training Epoch 4  55.2% | batch:       426 of       772\t|\tloss: 1.50497\n",
      "Training Epoch 4  55.3% | batch:       427 of       772\t|\tloss: 1.31584\n",
      "Training Epoch 4  55.4% | batch:       428 of       772\t|\tloss: 1.26452\n",
      "Training Epoch 4  55.6% | batch:       429 of       772\t|\tloss: 1.38006\n",
      "Training Epoch 4  55.7% | batch:       430 of       772\t|\tloss: 1.25253\n",
      "Training Epoch 4  55.8% | batch:       431 of       772\t|\tloss: 1.2369\n",
      "Training Epoch 4  56.0% | batch:       432 of       772\t|\tloss: 1.14448\n",
      "Training Epoch 4  56.1% | batch:       433 of       772\t|\tloss: 1.13168\n",
      "Training Epoch 4  56.2% | batch:       434 of       772\t|\tloss: 1.54019\n",
      "Training Epoch 4  56.3% | batch:       435 of       772\t|\tloss: 1.70238\n",
      "Training Epoch 4  56.5% | batch:       436 of       772\t|\tloss: 1.35439\n",
      "Training Epoch 4  56.6% | batch:       437 of       772\t|\tloss: 1.12574\n",
      "Training Epoch 4  56.7% | batch:       438 of       772\t|\tloss: 1.59321\n",
      "Training Epoch 4  56.9% | batch:       439 of       772\t|\tloss: 1.194\n",
      "Training Epoch 4  57.0% | batch:       440 of       772\t|\tloss: 1.24915\n",
      "Training Epoch 4  57.1% | batch:       441 of       772\t|\tloss: 1.08597\n",
      "Training Epoch 4  57.3% | batch:       442 of       772\t|\tloss: 1.14668\n",
      "Training Epoch 4  57.4% | batch:       443 of       772\t|\tloss: 1.31099\n",
      "Training Epoch 4  57.5% | batch:       444 of       772\t|\tloss: 1.1911\n",
      "Training Epoch 4  57.6% | batch:       445 of       772\t|\tloss: 1.47073\n",
      "Training Epoch 4  57.8% | batch:       446 of       772\t|\tloss: 0.865529\n",
      "Training Epoch 4  57.9% | batch:       447 of       772\t|\tloss: 1.09598\n",
      "Training Epoch 4  58.0% | batch:       448 of       772\t|\tloss: 1.20697\n",
      "Training Epoch 4  58.2% | batch:       449 of       772\t|\tloss: 1.30921\n",
      "Training Epoch 4  58.3% | batch:       450 of       772\t|\tloss: 1.40476\n",
      "Training Epoch 4  58.4% | batch:       451 of       772\t|\tloss: 1.17209\n",
      "Training Epoch 4  58.5% | batch:       452 of       772\t|\tloss: 1.07555\n",
      "Training Epoch 4  58.7% | batch:       453 of       772\t|\tloss: 1.33907\n",
      "Training Epoch 4  58.8% | batch:       454 of       772\t|\tloss: 1.37048\n",
      "Training Epoch 4  58.9% | batch:       455 of       772\t|\tloss: 1.01537\n",
      "Training Epoch 4  59.1% | batch:       456 of       772\t|\tloss: 0.930369\n",
      "Training Epoch 4  59.2% | batch:       457 of       772\t|\tloss: 1.36351\n",
      "Training Epoch 4  59.3% | batch:       458 of       772\t|\tloss: 1.59331\n",
      "Training Epoch 4  59.5% | batch:       459 of       772\t|\tloss: 1.21525\n",
      "Training Epoch 4  59.6% | batch:       460 of       772\t|\tloss: 1.11935\n",
      "Training Epoch 4  59.7% | batch:       461 of       772\t|\tloss: 1.33067\n",
      "Training Epoch 4  59.8% | batch:       462 of       772\t|\tloss: 1.62004\n",
      "Training Epoch 4  60.0% | batch:       463 of       772\t|\tloss: 1.40667\n",
      "Training Epoch 4  60.1% | batch:       464 of       772\t|\tloss: 1.21365\n",
      "Training Epoch 4  60.2% | batch:       465 of       772\t|\tloss: 1.16771\n",
      "Training Epoch 4  60.4% | batch:       466 of       772\t|\tloss: 1.14203\n",
      "Training Epoch 4  60.5% | batch:       467 of       772\t|\tloss: 1.41954\n",
      "Training Epoch 4  60.6% | batch:       468 of       772\t|\tloss: 1.22383\n",
      "Training Epoch 4  60.8% | batch:       469 of       772\t|\tloss: 1.49677\n",
      "Training Epoch 4  60.9% | batch:       470 of       772\t|\tloss: 1.62173\n",
      "Training Epoch 4  61.0% | batch:       471 of       772\t|\tloss: 1.33978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  61.1% | batch:       472 of       772\t|\tloss: 1.38198\n",
      "Training Epoch 4  61.3% | batch:       473 of       772\t|\tloss: 0.936565\n",
      "Training Epoch 4  61.4% | batch:       474 of       772\t|\tloss: 1.5038\n",
      "Training Epoch 4  61.5% | batch:       475 of       772\t|\tloss: 1.46253\n",
      "Training Epoch 4  61.7% | batch:       476 of       772\t|\tloss: 1.04833\n",
      "Training Epoch 4  61.8% | batch:       477 of       772\t|\tloss: 1.07208\n",
      "Training Epoch 4  61.9% | batch:       478 of       772\t|\tloss: 1.56224\n",
      "Training Epoch 4  62.0% | batch:       479 of       772\t|\tloss: 1.35278\n",
      "Training Epoch 4  62.2% | batch:       480 of       772\t|\tloss: 1.47612\n",
      "Training Epoch 4  62.3% | batch:       481 of       772\t|\tloss: 1.02561\n",
      "Training Epoch 4  62.4% | batch:       482 of       772\t|\tloss: 0.960183\n",
      "Training Epoch 4  62.6% | batch:       483 of       772\t|\tloss: 0.975334\n",
      "Training Epoch 4  62.7% | batch:       484 of       772\t|\tloss: 1.06684\n",
      "Training Epoch 4  62.8% | batch:       485 of       772\t|\tloss: 1.24772\n",
      "Training Epoch 4  63.0% | batch:       486 of       772\t|\tloss: 1.09198\n",
      "Training Epoch 4  63.1% | batch:       487 of       772\t|\tloss: 1.16954\n",
      "Training Epoch 4  63.2% | batch:       488 of       772\t|\tloss: 1.19605\n",
      "Training Epoch 4  63.3% | batch:       489 of       772\t|\tloss: 1.17308\n",
      "Training Epoch 4  63.5% | batch:       490 of       772\t|\tloss: 1.09528\n",
      "Training Epoch 4  63.6% | batch:       491 of       772\t|\tloss: 0.932094\n",
      "Training Epoch 4  63.7% | batch:       492 of       772\t|\tloss: 1.31546\n",
      "Training Epoch 4  63.9% | batch:       493 of       772\t|\tloss: 1.58519\n",
      "Training Epoch 4  64.0% | batch:       494 of       772\t|\tloss: 1.16949\n",
      "Training Epoch 4  64.1% | batch:       495 of       772\t|\tloss: 1.09797\n",
      "Training Epoch 4  64.2% | batch:       496 of       772\t|\tloss: 0.989593\n",
      "Training Epoch 4  64.4% | batch:       497 of       772\t|\tloss: 0.993908\n",
      "Training Epoch 4  64.5% | batch:       498 of       772\t|\tloss: 1.32286\n",
      "Training Epoch 4  64.6% | batch:       499 of       772\t|\tloss: 1.18086\n",
      "Training Epoch 4  64.8% | batch:       500 of       772\t|\tloss: 1.15343\n",
      "Training Epoch 4  64.9% | batch:       501 of       772\t|\tloss: 1.19973\n",
      "Training Epoch 4  65.0% | batch:       502 of       772\t|\tloss: 0.965384\n",
      "Training Epoch 4  65.2% | batch:       503 of       772\t|\tloss: 0.902488\n",
      "Training Epoch 4  65.3% | batch:       504 of       772\t|\tloss: 0.894057\n",
      "Training Epoch 4  65.4% | batch:       505 of       772\t|\tloss: 1.27315\n",
      "Training Epoch 4  65.5% | batch:       506 of       772\t|\tloss: 1.28461\n",
      "Training Epoch 4  65.7% | batch:       507 of       772\t|\tloss: 1.037\n",
      "Training Epoch 4  65.8% | batch:       508 of       772\t|\tloss: 1.08601\n",
      "Training Epoch 4  65.9% | batch:       509 of       772\t|\tloss: 1.24488\n",
      "Training Epoch 4  66.1% | batch:       510 of       772\t|\tloss: 1.06104\n",
      "Training Epoch 4  66.2% | batch:       511 of       772\t|\tloss: 0.992847\n",
      "Training Epoch 4  66.3% | batch:       512 of       772\t|\tloss: 1.02291\n",
      "Training Epoch 4  66.5% | batch:       513 of       772\t|\tloss: 1.14543\n",
      "Training Epoch 4  66.6% | batch:       514 of       772\t|\tloss: 1.32091\n",
      "Training Epoch 4  66.7% | batch:       515 of       772\t|\tloss: 1.6747\n",
      "Training Epoch 4  66.8% | batch:       516 of       772\t|\tloss: 1.4366\n",
      "Training Epoch 4  67.0% | batch:       517 of       772\t|\tloss: 1.21818\n",
      "Training Epoch 4  67.1% | batch:       518 of       772\t|\tloss: 1.40931\n",
      "Training Epoch 4  67.2% | batch:       519 of       772\t|\tloss: 1.2339\n",
      "Training Epoch 4  67.4% | batch:       520 of       772\t|\tloss: 1.44216\n",
      "Training Epoch 4  67.5% | batch:       521 of       772\t|\tloss: 1.48103\n",
      "Training Epoch 4  67.6% | batch:       522 of       772\t|\tloss: 1.07139\n",
      "Training Epoch 4  67.7% | batch:       523 of       772\t|\tloss: 1.23127\n",
      "Training Epoch 4  67.9% | batch:       524 of       772\t|\tloss: 1.47973\n",
      "Training Epoch 4  68.0% | batch:       525 of       772\t|\tloss: 1.27448\n",
      "Training Epoch 4  68.1% | batch:       526 of       772\t|\tloss: 1.15272\n",
      "Training Epoch 4  68.3% | batch:       527 of       772\t|\tloss: 0.946689\n",
      "Training Epoch 4  68.4% | batch:       528 of       772\t|\tloss: 1.82867\n",
      "Training Epoch 4  68.5% | batch:       529 of       772\t|\tloss: 1.67219\n",
      "Training Epoch 4  68.7% | batch:       530 of       772\t|\tloss: 1.0931\n",
      "Training Epoch 4  68.8% | batch:       531 of       772\t|\tloss: 1.10775\n",
      "Training Epoch 4  68.9% | batch:       532 of       772\t|\tloss: 1.0784\n",
      "Training Epoch 4  69.0% | batch:       533 of       772\t|\tloss: 1.07205\n",
      "Training Epoch 4  69.2% | batch:       534 of       772\t|\tloss: 1.10458\n",
      "Training Epoch 4  69.3% | batch:       535 of       772\t|\tloss: 1.08581\n",
      "Training Epoch 4  69.4% | batch:       536 of       772\t|\tloss: 1.43838\n",
      "Training Epoch 4  69.6% | batch:       537 of       772\t|\tloss: 1.06538\n",
      "Training Epoch 4  69.7% | batch:       538 of       772\t|\tloss: 0.99206\n",
      "Training Epoch 4  69.8% | batch:       539 of       772\t|\tloss: 1.3557\n",
      "Training Epoch 4  69.9% | batch:       540 of       772\t|\tloss: 1.86547\n",
      "Training Epoch 4  70.1% | batch:       541 of       772\t|\tloss: 1.11705\n",
      "Training Epoch 4  70.2% | batch:       542 of       772\t|\tloss: 0.95242\n",
      "Training Epoch 4  70.3% | batch:       543 of       772\t|\tloss: 1.38232\n",
      "Training Epoch 4  70.5% | batch:       544 of       772\t|\tloss: 1.47506\n",
      "Training Epoch 4  70.6% | batch:       545 of       772\t|\tloss: 1.17335\n",
      "Training Epoch 4  70.7% | batch:       546 of       772\t|\tloss: 1.10567\n",
      "Training Epoch 4  70.9% | batch:       547 of       772\t|\tloss: 1.28631\n",
      "Training Epoch 4  71.0% | batch:       548 of       772\t|\tloss: 1.09208\n",
      "Training Epoch 4  71.1% | batch:       549 of       772\t|\tloss: 1.20885\n",
      "Training Epoch 4  71.2% | batch:       550 of       772\t|\tloss: 1.11211\n",
      "Training Epoch 4  71.4% | batch:       551 of       772\t|\tloss: 1.50608\n",
      "Training Epoch 4  71.5% | batch:       552 of       772\t|\tloss: 1.41417\n",
      "Training Epoch 4  71.6% | batch:       553 of       772\t|\tloss: 1.28051\n",
      "Training Epoch 4  71.8% | batch:       554 of       772\t|\tloss: 1.18652\n",
      "Training Epoch 4  71.9% | batch:       555 of       772\t|\tloss: 1.14574\n",
      "Training Epoch 4  72.0% | batch:       556 of       772\t|\tloss: 1.08896\n",
      "Training Epoch 4  72.2% | batch:       557 of       772\t|\tloss: 1.41816\n",
      "Training Epoch 4  72.3% | batch:       558 of       772\t|\tloss: 1.20374\n",
      "Training Epoch 4  72.4% | batch:       559 of       772\t|\tloss: 1.49609\n",
      "Training Epoch 4  72.5% | batch:       560 of       772\t|\tloss: 0.993957\n",
      "Training Epoch 4  72.7% | batch:       561 of       772\t|\tloss: 1.43312\n",
      "Training Epoch 4  72.8% | batch:       562 of       772\t|\tloss: 1.08172\n",
      "Training Epoch 4  72.9% | batch:       563 of       772\t|\tloss: 1.25105\n",
      "Training Epoch 4  73.1% | batch:       564 of       772\t|\tloss: 1.37529\n",
      "Training Epoch 4  73.2% | batch:       565 of       772\t|\tloss: 0.93281\n",
      "Training Epoch 4  73.3% | batch:       566 of       772\t|\tloss: 1.21942\n",
      "Training Epoch 4  73.4% | batch:       567 of       772\t|\tloss: 1.27658\n",
      "Training Epoch 4  73.6% | batch:       568 of       772\t|\tloss: 1.09918\n",
      "Training Epoch 4  73.7% | batch:       569 of       772\t|\tloss: 1.36523\n",
      "Training Epoch 4  73.8% | batch:       570 of       772\t|\tloss: 1.33677\n",
      "Training Epoch 4  74.0% | batch:       571 of       772\t|\tloss: 1.41619\n",
      "Training Epoch 4  74.1% | batch:       572 of       772\t|\tloss: 0.999369\n",
      "Training Epoch 4  74.2% | batch:       573 of       772\t|\tloss: 0.97289\n",
      "Training Epoch 4  74.4% | batch:       574 of       772\t|\tloss: 0.891578\n",
      "Training Epoch 4  74.5% | batch:       575 of       772\t|\tloss: 1.24011\n",
      "Training Epoch 4  74.6% | batch:       576 of       772\t|\tloss: 1.16341\n",
      "Training Epoch 4  74.7% | batch:       577 of       772\t|\tloss: 1.67669\n",
      "Training Epoch 4  74.9% | batch:       578 of       772\t|\tloss: 1.07893\n",
      "Training Epoch 4  75.0% | batch:       579 of       772\t|\tloss: 1.16592\n",
      "Training Epoch 4  75.1% | batch:       580 of       772\t|\tloss: 0.818503\n",
      "Training Epoch 4  75.3% | batch:       581 of       772\t|\tloss: 0.972685\n",
      "Training Epoch 4  75.4% | batch:       582 of       772\t|\tloss: 1.00355\n",
      "Training Epoch 4  75.5% | batch:       583 of       772\t|\tloss: 1.27481\n",
      "Training Epoch 4  75.6% | batch:       584 of       772\t|\tloss: 1.0446\n",
      "Training Epoch 4  75.8% | batch:       585 of       772\t|\tloss: 1.23894\n",
      "Training Epoch 4  75.9% | batch:       586 of       772\t|\tloss: 1.26196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  76.0% | batch:       587 of       772\t|\tloss: 1.20541\n",
      "Training Epoch 4  76.2% | batch:       588 of       772\t|\tloss: 1.19126\n",
      "Training Epoch 4  76.3% | batch:       589 of       772\t|\tloss: 1.47553\n",
      "Training Epoch 4  76.4% | batch:       590 of       772\t|\tloss: 1.22985\n",
      "Training Epoch 4  76.6% | batch:       591 of       772\t|\tloss: 1.20733\n",
      "Training Epoch 4  76.7% | batch:       592 of       772\t|\tloss: 1.18478\n",
      "Training Epoch 4  76.8% | batch:       593 of       772\t|\tloss: 1.5442\n",
      "Training Epoch 4  76.9% | batch:       594 of       772\t|\tloss: 1.46295\n",
      "Training Epoch 4  77.1% | batch:       595 of       772\t|\tloss: 1.09919\n",
      "Training Epoch 4  77.2% | batch:       596 of       772\t|\tloss: 1.36642\n",
      "Training Epoch 4  77.3% | batch:       597 of       772\t|\tloss: 1.74626\n",
      "Training Epoch 4  77.5% | batch:       598 of       772\t|\tloss: 1.11775\n",
      "Training Epoch 4  77.6% | batch:       599 of       772\t|\tloss: 1.12021\n",
      "Training Epoch 4  77.7% | batch:       600 of       772\t|\tloss: 1.19088\n",
      "Training Epoch 4  77.8% | batch:       601 of       772\t|\tloss: 1.16681\n",
      "Training Epoch 4  78.0% | batch:       602 of       772\t|\tloss: 1.0907\n",
      "Training Epoch 4  78.1% | batch:       603 of       772\t|\tloss: 1.32108\n",
      "Training Epoch 4  78.2% | batch:       604 of       772\t|\tloss: 1.07586\n",
      "Training Epoch 4  78.4% | batch:       605 of       772\t|\tloss: 1.08175\n",
      "Training Epoch 4  78.5% | batch:       606 of       772\t|\tloss: 1.15638\n",
      "Training Epoch 4  78.6% | batch:       607 of       772\t|\tloss: 0.862923\n",
      "Training Epoch 4  78.8% | batch:       608 of       772\t|\tloss: 0.887421\n",
      "Training Epoch 4  78.9% | batch:       609 of       772\t|\tloss: 1.06802\n",
      "Training Epoch 4  79.0% | batch:       610 of       772\t|\tloss: 1.24383\n",
      "Training Epoch 4  79.1% | batch:       611 of       772\t|\tloss: 1.29126\n",
      "Training Epoch 4  79.3% | batch:       612 of       772\t|\tloss: 1.19444\n",
      "Training Epoch 4  79.4% | batch:       613 of       772\t|\tloss: 1.15645\n",
      "Training Epoch 4  79.5% | batch:       614 of       772\t|\tloss: 1.12829\n",
      "Training Epoch 4  79.7% | batch:       615 of       772\t|\tloss: 0.89695\n",
      "Training Epoch 4  79.8% | batch:       616 of       772\t|\tloss: 1.27377\n",
      "Training Epoch 4  79.9% | batch:       617 of       772\t|\tloss: 1.28232\n",
      "Training Epoch 4  80.1% | batch:       618 of       772\t|\tloss: 1.26464\n",
      "Training Epoch 4  80.2% | batch:       619 of       772\t|\tloss: 1.02048\n",
      "Training Epoch 4  80.3% | batch:       620 of       772\t|\tloss: 0.982929\n",
      "Training Epoch 4  80.4% | batch:       621 of       772\t|\tloss: 1.8456\n",
      "Training Epoch 4  80.6% | batch:       622 of       772\t|\tloss: 1.73797\n",
      "Training Epoch 4  80.7% | batch:       623 of       772\t|\tloss: 1.11517\n",
      "Training Epoch 4  80.8% | batch:       624 of       772\t|\tloss: 0.929225\n",
      "Training Epoch 4  81.0% | batch:       625 of       772\t|\tloss: 0.931632\n",
      "Training Epoch 4  81.1% | batch:       626 of       772\t|\tloss: 1.09307\n",
      "Training Epoch 4  81.2% | batch:       627 of       772\t|\tloss: 0.958645\n",
      "Training Epoch 4  81.3% | batch:       628 of       772\t|\tloss: 1.12893\n",
      "Training Epoch 4  81.5% | batch:       629 of       772\t|\tloss: 1.06792\n",
      "Training Epoch 4  81.6% | batch:       630 of       772\t|\tloss: 0.736528\n",
      "Training Epoch 4  81.7% | batch:       631 of       772\t|\tloss: 1.84839\n",
      "Training Epoch 4  81.9% | batch:       632 of       772\t|\tloss: 1.20603\n",
      "Training Epoch 4  82.0% | batch:       633 of       772\t|\tloss: 1.13052\n",
      "Training Epoch 4  82.1% | batch:       634 of       772\t|\tloss: 0.921815\n",
      "Training Epoch 4  82.3% | batch:       635 of       772\t|\tloss: 1.03533\n",
      "Training Epoch 4  82.4% | batch:       636 of       772\t|\tloss: 1.19919\n",
      "Training Epoch 4  82.5% | batch:       637 of       772\t|\tloss: 1.22644\n",
      "Training Epoch 4  82.6% | batch:       638 of       772\t|\tloss: 1.0243\n",
      "Training Epoch 4  82.8% | batch:       639 of       772\t|\tloss: 0.925929\n",
      "Training Epoch 4  82.9% | batch:       640 of       772\t|\tloss: 0.996491\n",
      "Training Epoch 4  83.0% | batch:       641 of       772\t|\tloss: 1.45659\n",
      "Training Epoch 4  83.2% | batch:       642 of       772\t|\tloss: 0.983524\n",
      "Training Epoch 4  83.3% | batch:       643 of       772\t|\tloss: 1.05051\n",
      "Training Epoch 4  83.4% | batch:       644 of       772\t|\tloss: 1.07883\n",
      "Training Epoch 4  83.5% | batch:       645 of       772\t|\tloss: 0.654684\n",
      "Training Epoch 4  83.7% | batch:       646 of       772\t|\tloss: 1.10177\n",
      "Training Epoch 4  83.8% | batch:       647 of       772\t|\tloss: 1.00021\n",
      "Training Epoch 4  83.9% | batch:       648 of       772\t|\tloss: 0.977749\n",
      "Training Epoch 4  84.1% | batch:       649 of       772\t|\tloss: 1.05709\n",
      "Training Epoch 4  84.2% | batch:       650 of       772\t|\tloss: 1.02451\n",
      "Training Epoch 4  84.3% | batch:       651 of       772\t|\tloss: 0.999277\n",
      "Training Epoch 4  84.5% | batch:       652 of       772\t|\tloss: 1.51803\n",
      "Training Epoch 4  84.6% | batch:       653 of       772\t|\tloss: 0.946536\n",
      "Training Epoch 4  84.7% | batch:       654 of       772\t|\tloss: 1.2365\n",
      "Training Epoch 4  84.8% | batch:       655 of       772\t|\tloss: 1.24114\n",
      "Training Epoch 4  85.0% | batch:       656 of       772\t|\tloss: 1.15748\n",
      "Training Epoch 4  85.1% | batch:       657 of       772\t|\tloss: 1.31997\n",
      "Training Epoch 4  85.2% | batch:       658 of       772\t|\tloss: 0.929468\n",
      "Training Epoch 4  85.4% | batch:       659 of       772\t|\tloss: 0.843944\n",
      "Training Epoch 4  85.5% | batch:       660 of       772\t|\tloss: 1.61376\n",
      "Training Epoch 4  85.6% | batch:       661 of       772\t|\tloss: 1.16556\n",
      "Training Epoch 4  85.8% | batch:       662 of       772\t|\tloss: 0.926053\n",
      "Training Epoch 4  85.9% | batch:       663 of       772\t|\tloss: 1.43545\n",
      "Training Epoch 4  86.0% | batch:       664 of       772\t|\tloss: 1.55149\n",
      "Training Epoch 4  86.1% | batch:       665 of       772\t|\tloss: 1.16699\n",
      "Training Epoch 4  86.3% | batch:       666 of       772\t|\tloss: 1.56184\n",
      "Training Epoch 4  86.4% | batch:       667 of       772\t|\tloss: 1.0252\n",
      "Training Epoch 4  86.5% | batch:       668 of       772\t|\tloss: 1.40727\n",
      "Training Epoch 4  86.7% | batch:       669 of       772\t|\tloss: 1.05361\n",
      "Training Epoch 4  86.8% | batch:       670 of       772\t|\tloss: 1.20567\n",
      "Training Epoch 4  86.9% | batch:       671 of       772\t|\tloss: 0.922553\n",
      "Training Epoch 4  87.0% | batch:       672 of       772\t|\tloss: 1.38404\n",
      "Training Epoch 4  87.2% | batch:       673 of       772\t|\tloss: 0.824192\n",
      "Training Epoch 4  87.3% | batch:       674 of       772\t|\tloss: 1.44738\n",
      "Training Epoch 4  87.4% | batch:       675 of       772\t|\tloss: 0.791349\n",
      "Training Epoch 4  87.6% | batch:       676 of       772\t|\tloss: 1.17551\n",
      "Training Epoch 4  87.7% | batch:       677 of       772\t|\tloss: 1.36802\n",
      "Training Epoch 4  87.8% | batch:       678 of       772\t|\tloss: 0.899902\n",
      "Training Epoch 4  88.0% | batch:       679 of       772\t|\tloss: 1.13649\n",
      "Training Epoch 4  88.1% | batch:       680 of       772\t|\tloss: 0.871813\n",
      "Training Epoch 4  88.2% | batch:       681 of       772\t|\tloss: 0.893937\n",
      "Training Epoch 4  88.3% | batch:       682 of       772\t|\tloss: 1.32805\n",
      "Training Epoch 4  88.5% | batch:       683 of       772\t|\tloss: 0.905125\n",
      "Training Epoch 4  88.6% | batch:       684 of       772\t|\tloss: 1.47641\n",
      "Training Epoch 4  88.7% | batch:       685 of       772\t|\tloss: 1.07015\n",
      "Training Epoch 4  88.9% | batch:       686 of       772\t|\tloss: 1.30689\n",
      "Training Epoch 4  89.0% | batch:       687 of       772\t|\tloss: 1.53804\n",
      "Training Epoch 4  89.1% | batch:       688 of       772\t|\tloss: 0.98116\n",
      "Training Epoch 4  89.2% | batch:       689 of       772\t|\tloss: 1.07642\n",
      "Training Epoch 4  89.4% | batch:       690 of       772\t|\tloss: 1.05069\n",
      "Training Epoch 4  89.5% | batch:       691 of       772\t|\tloss: 1.26081\n",
      "Training Epoch 4  89.6% | batch:       692 of       772\t|\tloss: 0.952304\n",
      "Training Epoch 4  89.8% | batch:       693 of       772\t|\tloss: 0.93698\n",
      "Training Epoch 4  89.9% | batch:       694 of       772\t|\tloss: 0.950433\n",
      "Training Epoch 4  90.0% | batch:       695 of       772\t|\tloss: 0.846202\n",
      "Training Epoch 4  90.2% | batch:       696 of       772\t|\tloss: 1.18212\n",
      "Training Epoch 4  90.3% | batch:       697 of       772\t|\tloss: 0.877876\n",
      "Training Epoch 4  90.4% | batch:       698 of       772\t|\tloss: 0.932692\n",
      "Training Epoch 4  90.5% | batch:       699 of       772\t|\tloss: 1.00875\n",
      "Training Epoch 4  90.7% | batch:       700 of       772\t|\tloss: 1.04052\n",
      "Training Epoch 4  90.8% | batch:       701 of       772\t|\tloss: 1.2816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  90.9% | batch:       702 of       772\t|\tloss: 1.10715\n",
      "Training Epoch 4  91.1% | batch:       703 of       772\t|\tloss: 0.777073\n",
      "Training Epoch 4  91.2% | batch:       704 of       772\t|\tloss: 0.932163\n",
      "Training Epoch 4  91.3% | batch:       705 of       772\t|\tloss: 1.21955\n",
      "Training Epoch 4  91.5% | batch:       706 of       772\t|\tloss: 1.11233\n",
      "Training Epoch 4  91.6% | batch:       707 of       772\t|\tloss: 0.876062\n",
      "Training Epoch 4  91.7% | batch:       708 of       772\t|\tloss: 1.47228\n",
      "Training Epoch 4  91.8% | batch:       709 of       772\t|\tloss: 1.13913\n",
      "Training Epoch 4  92.0% | batch:       710 of       772\t|\tloss: 1.02748\n",
      "Training Epoch 4  92.1% | batch:       711 of       772\t|\tloss: 1.18326\n",
      "Training Epoch 4  92.2% | batch:       712 of       772\t|\tloss: 1.06811\n",
      "Training Epoch 4  92.4% | batch:       713 of       772\t|\tloss: 1.26663\n",
      "Training Epoch 4  92.5% | batch:       714 of       772\t|\tloss: 1.71032\n",
      "Training Epoch 4  92.6% | batch:       715 of       772\t|\tloss: 1.32389\n",
      "Training Epoch 4  92.7% | batch:       716 of       772\t|\tloss: 0.972888\n",
      "Training Epoch 4  92.9% | batch:       717 of       772\t|\tloss: 0.948052\n",
      "Training Epoch 4  93.0% | batch:       718 of       772\t|\tloss: 1.72207\n",
      "Training Epoch 4  93.1% | batch:       719 of       772\t|\tloss: 1.43081\n",
      "Training Epoch 4  93.3% | batch:       720 of       772\t|\tloss: 1.26381\n",
      "Training Epoch 4  93.4% | batch:       721 of       772\t|\tloss: 1.06557\n",
      "Training Epoch 4  93.5% | batch:       722 of       772\t|\tloss: 0.748202\n",
      "Training Epoch 4  93.7% | batch:       723 of       772\t|\tloss: 1.48852\n",
      "Training Epoch 4  93.8% | batch:       724 of       772\t|\tloss: 0.872354\n",
      "Training Epoch 4  93.9% | batch:       725 of       772\t|\tloss: 1.15922\n",
      "Training Epoch 4  94.0% | batch:       726 of       772\t|\tloss: 1.13498\n",
      "Training Epoch 4  94.2% | batch:       727 of       772\t|\tloss: 1.44279\n",
      "Training Epoch 4  94.3% | batch:       728 of       772\t|\tloss: 0.967464\n",
      "Training Epoch 4  94.4% | batch:       729 of       772\t|\tloss: 1.03336\n",
      "Training Epoch 4  94.6% | batch:       730 of       772\t|\tloss: 1.36049\n",
      "Training Epoch 4  94.7% | batch:       731 of       772\t|\tloss: 1.14091\n",
      "Training Epoch 4  94.8% | batch:       732 of       772\t|\tloss: 0.785658\n",
      "Training Epoch 4  94.9% | batch:       733 of       772\t|\tloss: 1.30941\n",
      "Training Epoch 4  95.1% | batch:       734 of       772\t|\tloss: 1.02947\n",
      "Training Epoch 4  95.2% | batch:       735 of       772\t|\tloss: 0.965768\n",
      "Training Epoch 4  95.3% | batch:       736 of       772\t|\tloss: 0.890646\n",
      "Training Epoch 4  95.5% | batch:       737 of       772\t|\tloss: 1.35615\n",
      "Training Epoch 4  95.6% | batch:       738 of       772\t|\tloss: 1.35515\n",
      "Training Epoch 4  95.7% | batch:       739 of       772\t|\tloss: 1.00747\n",
      "Training Epoch 4  95.9% | batch:       740 of       772\t|\tloss: 1.16825\n",
      "Training Epoch 4  96.0% | batch:       741 of       772\t|\tloss: 0.882896\n",
      "Training Epoch 4  96.1% | batch:       742 of       772\t|\tloss: 0.799065\n",
      "Training Epoch 4  96.2% | batch:       743 of       772\t|\tloss: 0.841845\n",
      "Training Epoch 4  96.4% | batch:       744 of       772\t|\tloss: 0.771543\n",
      "Training Epoch 4  96.5% | batch:       745 of       772\t|\tloss: 0.860404\n",
      "Training Epoch 4  96.6% | batch:       746 of       772\t|\tloss: 0.866713\n",
      "Training Epoch 4  96.8% | batch:       747 of       772\t|\tloss: 0.809798\n",
      "Training Epoch 4  96.9% | batch:       748 of       772\t|\tloss: 1.0885\n",
      "Training Epoch 4  97.0% | batch:       749 of       772\t|\tloss: 1.16256\n",
      "Training Epoch 4  97.2% | batch:       750 of       772\t|\tloss: 1.65977\n",
      "Training Epoch 4  97.3% | batch:       751 of       772\t|\tloss: 1.36224\n",
      "Training Epoch 4  97.4% | batch:       752 of       772\t|\tloss: 1.11099\n",
      "Training Epoch 4  97.5% | batch:       753 of       772\t|\tloss: 1.15575\n",
      "Training Epoch 4  97.7% | batch:       754 of       772\t|\tloss: 0.914876\n",
      "Training Epoch 4  97.8% | batch:       755 of       772\t|\tloss: 0.916615\n",
      "Training Epoch 4  97.9% | batch:       756 of       772\t|\tloss: 0.886345\n",
      "Training Epoch 4  98.1% | batch:       757 of       772\t|\tloss: 1.73703\n",
      "Training Epoch 4  98.2% | batch:       758 of       772\t|\tloss: 1.12447\n",
      "Training Epoch 4  98.3% | batch:       759 of       772\t|\tloss: 0.91896\n",
      "Training Epoch 4  98.4% | batch:       760 of       772\t|\tloss: 0.952514\n",
      "Training Epoch 4  98.6% | batch:       761 of       772\t|\tloss: 0.899288\n",
      "Training Epoch 4  98.7% | batch:       762 of       772\t|\tloss: 0.879259\n",
      "Training Epoch 4  98.8% | batch:       763 of       772\t|\tloss: 1.05272\n",
      "Training Epoch 4  99.0% | batch:       764 of       772\t|\tloss: 0.945271\n",
      "Training Epoch 4  99.1% | batch:       765 of       772\t|\tloss: 1.23249\n",
      "Training Epoch 4  99.2% | batch:       766 of       772\t|\tloss: 0.912309\n",
      "Training Epoch 4  99.4% | batch:       767 of       772\t|\tloss: 1.14262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:21:19,130 | INFO : Epoch 4 Training Summary: epoch: 4.000000 | loss: 1.331893 | \n",
      "2023-05-24 10:21:19,131 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 17.052053213119507 seconds\n",
      "\n",
      "2023-05-24 10:21:19,132 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 16.918108820915222 seconds\n",
      "2023-05-24 10:21:19,132 | INFO : Avg batch train. time: 0.02191464873175547 seconds\n",
      "2023-05-24 10:21:19,132 | INFO : Avg sample train. time: 0.0001712116584787097 seconds\n",
      "2023-05-24 10:21:19,133 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  99.5% | batch:       768 of       772\t|\tloss: 1.37292\n",
      "Training Epoch 4  99.6% | batch:       769 of       772\t|\tloss: 0.9667\n",
      "Training Epoch 4  99.7% | batch:       770 of       772\t|\tloss: 0.969494\n",
      "Training Epoch 4  99.9% | batch:       771 of       772\t|\tloss: 0.91428\n",
      "\n",
      "Evaluating Epoch 4   0.0% | batch:         0 of        92\t|\tloss: 0.962254\n",
      "Evaluating Epoch 4   1.1% | batch:         1 of        92\t|\tloss: 9.13325\n",
      "Evaluating Epoch 4   2.2% | batch:         2 of        92\t|\tloss: 2.50006\n",
      "Evaluating Epoch 4   3.3% | batch:         3 of        92\t|\tloss: 6.95888\n",
      "Evaluating Epoch 4   4.3% | batch:         4 of        92\t|\tloss: 3.96696\n",
      "Evaluating Epoch 4   5.4% | batch:         5 of        92\t|\tloss: 9.02901\n",
      "Evaluating Epoch 4   6.5% | batch:         6 of        92\t|\tloss: 3.78167\n",
      "Evaluating Epoch 4   7.6% | batch:         7 of        92\t|\tloss: 2.16999\n",
      "Evaluating Epoch 4   8.7% | batch:         8 of        92\t|\tloss: 6.91529\n",
      "Evaluating Epoch 4   9.8% | batch:         9 of        92\t|\tloss: 4.18795\n",
      "Evaluating Epoch 4  10.9% | batch:        10 of        92\t|\tloss: 6.3002\n",
      "Evaluating Epoch 4  12.0% | batch:        11 of        92\t|\tloss: 4.54043\n",
      "Evaluating Epoch 4  13.0% | batch:        12 of        92\t|\tloss: 8.33673\n",
      "Evaluating Epoch 4  14.1% | batch:        13 of        92\t|\tloss: 5.35411\n",
      "Evaluating Epoch 4  15.2% | batch:        14 of        92\t|\tloss: 1.69781\n",
      "Evaluating Epoch 4  16.3% | batch:        15 of        92\t|\tloss: 0.703002\n",
      "Evaluating Epoch 4  17.4% | batch:        16 of        92\t|\tloss: 2.79282\n",
      "Evaluating Epoch 4  18.5% | batch:        17 of        92\t|\tloss: 1.62944\n",
      "Evaluating Epoch 4  19.6% | batch:        18 of        92\t|\tloss: 4.32469\n",
      "Evaluating Epoch 4  20.7% | batch:        19 of        92\t|\tloss: 5.10852\n",
      "Evaluating Epoch 4  21.7% | batch:        20 of        92\t|\tloss: 3.92331\n",
      "Evaluating Epoch 4  22.8% | batch:        21 of        92\t|\tloss: 4.65248\n",
      "Evaluating Epoch 4  23.9% | batch:        22 of        92\t|\tloss: 6.24826\n",
      "Evaluating Epoch 4  25.0% | batch:        23 of        92\t|\tloss: 6.14044\n",
      "Evaluating Epoch 4  26.1% | batch:        24 of        92\t|\tloss: 1.87195\n",
      "Evaluating Epoch 4  27.2% | batch:        25 of        92\t|\tloss: 0.376992\n",
      "Evaluating Epoch 4  28.3% | batch:        26 of        92\t|\tloss: 1.26818\n",
      "Evaluating Epoch 4  29.3% | batch:        27 of        92\t|\tloss: 3.04724\n",
      "Evaluating Epoch 4  30.4% | batch:        28 of        92\t|\tloss: 3.17656\n",
      "Evaluating Epoch 4  31.5% | batch:        29 of        92\t|\tloss: 3.73779\n",
      "Evaluating Epoch 4  32.6% | batch:        30 of        92\t|\tloss: 2.41071\n",
      "Evaluating Epoch 4  33.7% | batch:        31 of        92\t|\tloss: 5.06472\n",
      "Evaluating Epoch 4  34.8% | batch:        32 of        92\t|\tloss: 3.55715\n",
      "Evaluating Epoch 4  35.9% | batch:        33 of        92\t|\tloss: 6.34237\n",
      "Evaluating Epoch 4  37.0% | batch:        34 of        92\t|\tloss: 3.40035\n",
      "Evaluating Epoch 4  38.0% | batch:        35 of        92\t|\tloss: 2.50459\n",
      "Evaluating Epoch 4  39.1% | batch:        36 of        92\t|\tloss: 1.61635\n",
      "Evaluating Epoch 4  40.2% | batch:        37 of        92\t|\tloss: 2.62497\n",
      "Evaluating Epoch 4  41.3% | batch:        38 of        92\t|\tloss: 2.8226\n",
      "Evaluating Epoch 4  42.4% | batch:        39 of        92\t|\tloss: 7.22355\n",
      "Evaluating Epoch 4  43.5% | batch:        40 of        92\t|\tloss: 3.34902\n",
      "Evaluating Epoch 4  44.6% | batch:        41 of        92\t|\tloss: 6.24285\n",
      "Evaluating Epoch 4  45.7% | batch:        42 of        92\t|\tloss: 5.05431\n",
      "Evaluating Epoch 4  46.7% | batch:        43 of        92\t|\tloss: 7.46129\n",
      "Evaluating Epoch 4  47.8% | batch:        44 of        92\t|\tloss: 1.54518\n",
      "Evaluating Epoch 4  48.9% | batch:        45 of        92\t|\tloss: 1.85421\n",
      "Evaluating Epoch 4  50.0% | batch:        46 of        92\t|\tloss: 1.92776\n",
      "Evaluating Epoch 4  51.1% | batch:        47 of        92\t|\tloss: 4.72539\n",
      "Evaluating Epoch 4  52.2% | batch:        48 of        92\t|\tloss: 5.79959\n",
      "Evaluating Epoch 4  53.3% | batch:        49 of        92\t|\tloss: 4.78739\n",
      "Evaluating Epoch 4  54.3% | batch:        50 of        92\t|\tloss: 5.87693\n",
      "Evaluating Epoch 4  55.4% | batch:        51 of        92\t|\tloss: 7.23809\n",
      "Evaluating Epoch 4  56.5% | batch:        52 of        92\t|\tloss: 6.18134\n",
      "Evaluating Epoch 4  57.6% | batch:        53 of        92\t|\tloss: 1.47348\n",
      "Evaluating Epoch 4  58.7% | batch:        54 of        92\t|\tloss: 1.99258\n",
      "Evaluating Epoch 4  59.8% | batch:        55 of        92\t|\tloss: 5.70238\n",
      "Evaluating Epoch 4  60.9% | batch:        56 of        92\t|\tloss: 6.27762\n",
      "Evaluating Epoch 4  62.0% | batch:        57 of        92\t|\tloss: 5.04478\n",
      "Evaluating Epoch 4  63.0% | batch:        58 of        92\t|\tloss: 5.20962\n",
      "Evaluating Epoch 4  64.1% | batch:        59 of        92\t|\tloss: 7.30643\n",
      "Evaluating Epoch 4  65.2% | batch:        60 of        92\t|\tloss: 6.22047\n",
      "Evaluating Epoch 4  66.3% | batch:        61 of        92\t|\tloss: 1.6323\n",
      "Evaluating Epoch 4  67.4% | batch:        62 of        92\t|\tloss: 0.655978\n",
      "Evaluating Epoch 4  68.5% | batch:        63 of        92\t|\tloss: 1.34637\n",
      "Evaluating Epoch 4  69.6% | batch:        64 of        92\t|\tloss: 3.81344\n",
      "Evaluating Epoch 4  70.7% | batch:        65 of        92\t|\tloss: 7.23133\n",
      "Evaluating Epoch 4  71.7% | batch:        66 of        92\t|\tloss: 3.53049\n",
      "Evaluating Epoch 4  72.8% | batch:        67 of        92\t|\tloss: 3.54731\n",
      "Evaluating Epoch 4  73.9% | batch:        68 of        92\t|\tloss: 5.69211\n",
      "Evaluating Epoch 4  75.0% | batch:        69 of        92\t|\tloss: 4.31853\n",
      "Evaluating Epoch 4  76.1% | batch:        70 of        92\t|\tloss: 7.11116\n",
      "Evaluating Epoch 4  77.2% | batch:        71 of        92\t|\tloss: 3.37395\n",
      "Evaluating Epoch 4  78.3% | batch:        72 of        92\t|\tloss: 3.31749\n",
      "Evaluating Epoch 4  79.3% | batch:        73 of        92\t|\tloss: 2.04468\n",
      "Evaluating Epoch 4  80.4% | batch:        74 of        92\t|\tloss: 4.30304\n",
      "Evaluating Epoch 4  81.5% | batch:        75 of        92\t|\tloss: 1.57167\n",
      "Evaluating Epoch 4  82.6% | batch:        76 of        92\t|\tloss: 4.29393\n",
      "Evaluating Epoch 4  83.7% | batch:        77 of        92\t|\tloss: 4.91001\n",
      "Evaluating Epoch 4  84.8% | batch:        78 of        92\t|\tloss: 5.19965\n",
      "Evaluating Epoch 4  85.9% | batch:        79 of        92\t|\tloss: 5.67918\n",
      "Evaluating Epoch 4  87.0% | batch:        80 of        92\t|\tloss: 5.8733\n",
      "Evaluating Epoch 4  88.0% | batch:        81 of        92\t|\tloss: 5.51626\n",
      "Evaluating Epoch 4  89.1% | batch:        82 of        92\t|\tloss: 1.42939\n",
      "Evaluating Epoch 4  90.2% | batch:        83 of        92\t|\tloss: 1.71238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:21:20,417 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.2838537693023682 seconds\n",
      "\n",
      "2023-05-24 10:21:20,418 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.4456176161766052 seconds\n",
      "2023-05-24 10:21:20,418 | INFO : Avg batch val. time: 0.01571323495844136 seconds\n",
      "2023-05-24 10:21:20,418 | INFO : Avg sample val. time: 0.00012391716236727287 seconds\n",
      "2023-05-24 10:21:20,419 | INFO : Epoch 4 Validation Summary: epoch: 4.000000 | loss: 4.237788 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 4  91.3% | batch:        84 of        92\t|\tloss: 4.59591\n",
      "Evaluating Epoch 4  92.4% | batch:        85 of        92\t|\tloss: 5.72802\n",
      "Evaluating Epoch 4  93.5% | batch:        86 of        92\t|\tloss: 4.69459\n",
      "Evaluating Epoch 4  94.6% | batch:        87 of        92\t|\tloss: 4.96717\n",
      "Evaluating Epoch 4  95.7% | batch:        88 of        92\t|\tloss: 6.49801\n",
      "Evaluating Epoch 4  96.7% | batch:        89 of        92\t|\tloss: 6.13783\n",
      "Evaluating Epoch 4  97.8% | batch:        90 of        92\t|\tloss: 1.65303\n",
      "Evaluating Epoch 4  98.9% | batch:        91 of        92\t|\tloss: 1.29283\n",
      "\n",
      "Training Epoch 5   0.0% | batch:         0 of       772\t|\tloss: 0.765556\n",
      "Training Epoch 5   0.1% | batch:         1 of       772\t|\tloss: 1.02101\n",
      "Training Epoch 5   0.3% | batch:         2 of       772\t|\tloss: 1.19844\n",
      "Training Epoch 5   0.4% | batch:         3 of       772\t|\tloss: 0.991212\n",
      "Training Epoch 5   0.5% | batch:         4 of       772\t|\tloss: 0.764428\n",
      "Training Epoch 5   0.6% | batch:         5 of       772\t|\tloss: 0.877441\n",
      "Training Epoch 5   0.8% | batch:         6 of       772\t|\tloss: 0.838269\n",
      "Training Epoch 5   0.9% | batch:         7 of       772\t|\tloss: 0.962489\n",
      "Training Epoch 5   1.0% | batch:         8 of       772\t|\tloss: 0.897957\n",
      "Training Epoch 5   1.2% | batch:         9 of       772\t|\tloss: 1.01065\n",
      "Training Epoch 5   1.3% | batch:        10 of       772\t|\tloss: 0.713312\n",
      "Training Epoch 5   1.4% | batch:        11 of       772\t|\tloss: 0.944152\n",
      "Training Epoch 5   1.6% | batch:        12 of       772\t|\tloss: 1.11537\n",
      "Training Epoch 5   1.7% | batch:        13 of       772\t|\tloss: 0.921703\n",
      "Training Epoch 5   1.8% | batch:        14 of       772\t|\tloss: 1.08154\n",
      "Training Epoch 5   1.9% | batch:        15 of       772\t|\tloss: 1.09775\n",
      "Training Epoch 5   2.1% | batch:        16 of       772\t|\tloss: 0.999917\n",
      "Training Epoch 5   2.2% | batch:        17 of       772\t|\tloss: 1.499\n",
      "Training Epoch 5   2.3% | batch:        18 of       772\t|\tloss: 1.0659\n",
      "Training Epoch 5   2.5% | batch:        19 of       772\t|\tloss: 0.778404\n",
      "Training Epoch 5   2.6% | batch:        20 of       772\t|\tloss: 1.04322\n",
      "Training Epoch 5   2.7% | batch:        21 of       772\t|\tloss: 0.887363\n",
      "Training Epoch 5   2.8% | batch:        22 of       772\t|\tloss: 1.05983\n",
      "Training Epoch 5   3.0% | batch:        23 of       772\t|\tloss: 0.946795\n",
      "Training Epoch 5   3.1% | batch:        24 of       772\t|\tloss: 1.01096\n",
      "Training Epoch 5   3.2% | batch:        25 of       772\t|\tloss: 0.923064\n",
      "Training Epoch 5   3.4% | batch:        26 of       772\t|\tloss: 0.823022\n",
      "Training Epoch 5   3.5% | batch:        27 of       772\t|\tloss: 0.856273\n",
      "Training Epoch 5   3.6% | batch:        28 of       772\t|\tloss: 0.818654\n",
      "Training Epoch 5   3.8% | batch:        29 of       772\t|\tloss: 1.22381\n",
      "Training Epoch 5   3.9% | batch:        30 of       772\t|\tloss: 1.59814\n",
      "Training Epoch 5   4.0% | batch:        31 of       772\t|\tloss: 0.98482\n",
      "Training Epoch 5   4.1% | batch:        32 of       772\t|\tloss: 1.13357\n",
      "Training Epoch 5   4.3% | batch:        33 of       772\t|\tloss: 0.903844\n",
      "Training Epoch 5   4.4% | batch:        34 of       772\t|\tloss: 1.05364\n",
      "Training Epoch 5   4.5% | batch:        35 of       772\t|\tloss: 0.944866\n",
      "Training Epoch 5   4.7% | batch:        36 of       772\t|\tloss: 0.963415\n",
      "Training Epoch 5   4.8% | batch:        37 of       772\t|\tloss: 0.992846\n",
      "Training Epoch 5   4.9% | batch:        38 of       772\t|\tloss: 1.08657\n",
      "Training Epoch 5   5.1% | batch:        39 of       772\t|\tloss: 0.869341\n",
      "Training Epoch 5   5.2% | batch:        40 of       772\t|\tloss: 1.11669\n",
      "Training Epoch 5   5.3% | batch:        41 of       772\t|\tloss: 1.12084\n",
      "Training Epoch 5   5.4% | batch:        42 of       772\t|\tloss: 0.727215\n",
      "Training Epoch 5   5.6% | batch:        43 of       772\t|\tloss: 0.91004\n",
      "Training Epoch 5   5.7% | batch:        44 of       772\t|\tloss: 1.15023\n",
      "Training Epoch 5   5.8% | batch:        45 of       772\t|\tloss: 0.934563\n",
      "Training Epoch 5   6.0% | batch:        46 of       772\t|\tloss: 0.602848\n",
      "Training Epoch 5   6.1% | batch:        47 of       772\t|\tloss: 1.10939\n",
      "Training Epoch 5   6.2% | batch:        48 of       772\t|\tloss: 1.00782\n",
      "Training Epoch 5   6.3% | batch:        49 of       772\t|\tloss: 0.887518\n",
      "Training Epoch 5   6.5% | batch:        50 of       772\t|\tloss: 0.745502\n",
      "Training Epoch 5   6.6% | batch:        51 of       772\t|\tloss: 0.7166\n",
      "Training Epoch 5   6.7% | batch:        52 of       772\t|\tloss: 0.983712\n",
      "Training Epoch 5   6.9% | batch:        53 of       772\t|\tloss: 1.07802\n",
      "Training Epoch 5   7.0% | batch:        54 of       772\t|\tloss: 0.949047\n",
      "Training Epoch 5   7.1% | batch:        55 of       772\t|\tloss: 0.952577\n",
      "Training Epoch 5   7.3% | batch:        56 of       772\t|\tloss: 1.11981\n",
      "Training Epoch 5   7.4% | batch:        57 of       772\t|\tloss: 1.12308\n",
      "Training Epoch 5   7.5% | batch:        58 of       772\t|\tloss: 1.1666\n",
      "Training Epoch 5   7.6% | batch:        59 of       772\t|\tloss: 0.800507\n",
      "Training Epoch 5   7.8% | batch:        60 of       772\t|\tloss: 1.0734\n",
      "Training Epoch 5   7.9% | batch:        61 of       772\t|\tloss: 0.585897\n",
      "Training Epoch 5   8.0% | batch:        62 of       772\t|\tloss: 1.13816\n",
      "Training Epoch 5   8.2% | batch:        63 of       772\t|\tloss: 0.89112\n",
      "Training Epoch 5   8.3% | batch:        64 of       772\t|\tloss: 0.919275\n",
      "Training Epoch 5   8.4% | batch:        65 of       772\t|\tloss: 0.860258\n",
      "Training Epoch 5   8.5% | batch:        66 of       772\t|\tloss: 1.21282\n",
      "Training Epoch 5   8.7% | batch:        67 of       772\t|\tloss: 0.987082\n",
      "Training Epoch 5   8.8% | batch:        68 of       772\t|\tloss: 1.07899\n",
      "Training Epoch 5   8.9% | batch:        69 of       772\t|\tloss: 0.994589\n",
      "Training Epoch 5   9.1% | batch:        70 of       772\t|\tloss: 1.10455\n",
      "Training Epoch 5   9.2% | batch:        71 of       772\t|\tloss: 0.651652\n",
      "Training Epoch 5   9.3% | batch:        72 of       772\t|\tloss: 0.727569\n",
      "Training Epoch 5   9.5% | batch:        73 of       772\t|\tloss: 0.934464\n",
      "Training Epoch 5   9.6% | batch:        74 of       772\t|\tloss: 0.903461\n",
      "Training Epoch 5   9.7% | batch:        75 of       772\t|\tloss: 0.904918\n",
      "Training Epoch 5   9.8% | batch:        76 of       772\t|\tloss: 1.26828\n",
      "Training Epoch 5  10.0% | batch:        77 of       772\t|\tloss: 0.868713\n",
      "Training Epoch 5  10.1% | batch:        78 of       772\t|\tloss: 0.964551\n",
      "Training Epoch 5  10.2% | batch:        79 of       772\t|\tloss: 1.24001\n",
      "Training Epoch 5  10.4% | batch:        80 of       772\t|\tloss: 1.01374\n",
      "Training Epoch 5  10.5% | batch:        81 of       772\t|\tloss: 0.808866\n",
      "Training Epoch 5  10.6% | batch:        82 of       772\t|\tloss: 1.21977\n",
      "Training Epoch 5  10.8% | batch:        83 of       772\t|\tloss: 0.952884\n",
      "Training Epoch 5  10.9% | batch:        84 of       772\t|\tloss: 1.24726\n",
      "Training Epoch 5  11.0% | batch:        85 of       772\t|\tloss: 0.757972\n",
      "Training Epoch 5  11.1% | batch:        86 of       772\t|\tloss: 0.786015\n",
      "Training Epoch 5  11.3% | batch:        87 of       772\t|\tloss: 0.916804\n",
      "Training Epoch 5  11.4% | batch:        88 of       772\t|\tloss: 0.690512\n",
      "Training Epoch 5  11.5% | batch:        89 of       772\t|\tloss: 0.968536\n",
      "Training Epoch 5  11.7% | batch:        90 of       772\t|\tloss: 1.01096\n",
      "Training Epoch 5  11.8% | batch:        91 of       772\t|\tloss: 0.859529\n",
      "Training Epoch 5  11.9% | batch:        92 of       772\t|\tloss: 0.845064\n",
      "Training Epoch 5  12.0% | batch:        93 of       772\t|\tloss: 0.973924\n",
      "Training Epoch 5  12.2% | batch:        94 of       772\t|\tloss: 1.22863\n",
      "Training Epoch 5  12.3% | batch:        95 of       772\t|\tloss: 0.637269\n",
      "Training Epoch 5  12.4% | batch:        96 of       772\t|\tloss: 1.36472\n",
      "Training Epoch 5  12.6% | batch:        97 of       772\t|\tloss: 0.802374\n",
      "Training Epoch 5  12.7% | batch:        98 of       772\t|\tloss: 0.801206\n",
      "Training Epoch 5  12.8% | batch:        99 of       772\t|\tloss: 0.843505\n",
      "Training Epoch 5  13.0% | batch:       100 of       772\t|\tloss: 1.42004\n",
      "Training Epoch 5  13.1% | batch:       101 of       772\t|\tloss: 1.06979\n",
      "Training Epoch 5  13.2% | batch:       102 of       772\t|\tloss: 0.820374\n",
      "Training Epoch 5  13.3% | batch:       103 of       772\t|\tloss: 0.836424\n",
      "Training Epoch 5  13.5% | batch:       104 of       772\t|\tloss: 0.858641\n",
      "Training Epoch 5  13.6% | batch:       105 of       772\t|\tloss: 0.655801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  13.7% | batch:       106 of       772\t|\tloss: 0.971208\n",
      "Training Epoch 5  13.9% | batch:       107 of       772\t|\tloss: 0.948765\n",
      "Training Epoch 5  14.0% | batch:       108 of       772\t|\tloss: 0.83316\n",
      "Training Epoch 5  14.1% | batch:       109 of       772\t|\tloss: 1.48662\n",
      "Training Epoch 5  14.2% | batch:       110 of       772\t|\tloss: 1.08222\n",
      "Training Epoch 5  14.4% | batch:       111 of       772\t|\tloss: 1.46887\n",
      "Training Epoch 5  14.5% | batch:       112 of       772\t|\tloss: 0.850926\n",
      "Training Epoch 5  14.6% | batch:       113 of       772\t|\tloss: 0.87296\n",
      "Training Epoch 5  14.8% | batch:       114 of       772\t|\tloss: 1.07764\n",
      "Training Epoch 5  14.9% | batch:       115 of       772\t|\tloss: 0.952646\n",
      "Training Epoch 5  15.0% | batch:       116 of       772\t|\tloss: 1.03604\n",
      "Training Epoch 5  15.2% | batch:       117 of       772\t|\tloss: 0.79223\n",
      "Training Epoch 5  15.3% | batch:       118 of       772\t|\tloss: 0.98222\n",
      "Training Epoch 5  15.4% | batch:       119 of       772\t|\tloss: 0.882257\n",
      "Training Epoch 5  15.5% | batch:       120 of       772\t|\tloss: 1.0359\n",
      "Training Epoch 5  15.7% | batch:       121 of       772\t|\tloss: 0.791311\n",
      "Training Epoch 5  15.8% | batch:       122 of       772\t|\tloss: 0.970937\n",
      "Training Epoch 5  15.9% | batch:       123 of       772\t|\tloss: 0.736403\n",
      "Training Epoch 5  16.1% | batch:       124 of       772\t|\tloss: 0.718515\n",
      "Training Epoch 5  16.2% | batch:       125 of       772\t|\tloss: 0.923157\n",
      "Training Epoch 5  16.3% | batch:       126 of       772\t|\tloss: 0.659379\n",
      "Training Epoch 5  16.5% | batch:       127 of       772\t|\tloss: 0.744851\n",
      "Training Epoch 5  16.6% | batch:       128 of       772\t|\tloss: 0.739716\n",
      "Training Epoch 5  16.7% | batch:       129 of       772\t|\tloss: 0.714841\n",
      "Training Epoch 5  16.8% | batch:       130 of       772\t|\tloss: 0.85043\n",
      "Training Epoch 5  17.0% | batch:       131 of       772\t|\tloss: 1.14703\n",
      "Training Epoch 5  17.1% | batch:       132 of       772\t|\tloss: 1.05701\n",
      "Training Epoch 5  17.2% | batch:       133 of       772\t|\tloss: 1.12076\n",
      "Training Epoch 5  17.4% | batch:       134 of       772\t|\tloss: 1.08727\n",
      "Training Epoch 5  17.5% | batch:       135 of       772\t|\tloss: 0.811368\n",
      "Training Epoch 5  17.6% | batch:       136 of       772\t|\tloss: 1.44053\n",
      "Training Epoch 5  17.7% | batch:       137 of       772\t|\tloss: 0.779645\n",
      "Training Epoch 5  17.9% | batch:       138 of       772\t|\tloss: 1.09992\n",
      "Training Epoch 5  18.0% | batch:       139 of       772\t|\tloss: 0.714387\n",
      "Training Epoch 5  18.1% | batch:       140 of       772\t|\tloss: 0.76089\n",
      "Training Epoch 5  18.3% | batch:       141 of       772\t|\tloss: 1.1375\n",
      "Training Epoch 5  18.4% | batch:       142 of       772\t|\tloss: 1.04374\n",
      "Training Epoch 5  18.5% | batch:       143 of       772\t|\tloss: 1.30279\n",
      "Training Epoch 5  18.7% | batch:       144 of       772\t|\tloss: 0.894698\n",
      "Training Epoch 5  18.8% | batch:       145 of       772\t|\tloss: 0.863838\n",
      "Training Epoch 5  18.9% | batch:       146 of       772\t|\tloss: 1.01765\n",
      "Training Epoch 5  19.0% | batch:       147 of       772\t|\tloss: 1.04761\n",
      "Training Epoch 5  19.2% | batch:       148 of       772\t|\tloss: 0.889835\n",
      "Training Epoch 5  19.3% | batch:       149 of       772\t|\tloss: 1.35258\n",
      "Training Epoch 5  19.4% | batch:       150 of       772\t|\tloss: 1.04097\n",
      "Training Epoch 5  19.6% | batch:       151 of       772\t|\tloss: 0.727596\n",
      "Training Epoch 5  19.7% | batch:       152 of       772\t|\tloss: 1.20188\n",
      "Training Epoch 5  19.8% | batch:       153 of       772\t|\tloss: 0.799888\n",
      "Training Epoch 5  19.9% | batch:       154 of       772\t|\tloss: 0.934932\n",
      "Training Epoch 5  20.1% | batch:       155 of       772\t|\tloss: 0.936653\n",
      "Training Epoch 5  20.2% | batch:       156 of       772\t|\tloss: 0.93045\n",
      "Training Epoch 5  20.3% | batch:       157 of       772\t|\tloss: 1.08991\n",
      "Training Epoch 5  20.5% | batch:       158 of       772\t|\tloss: 0.81575\n",
      "Training Epoch 5  20.6% | batch:       159 of       772\t|\tloss: 0.8235\n",
      "Training Epoch 5  20.7% | batch:       160 of       772\t|\tloss: 0.926038\n",
      "Training Epoch 5  20.9% | batch:       161 of       772\t|\tloss: 0.686702\n",
      "Training Epoch 5  21.0% | batch:       162 of       772\t|\tloss: 0.6395\n",
      "Training Epoch 5  21.1% | batch:       163 of       772\t|\tloss: 1.04249\n",
      "Training Epoch 5  21.2% | batch:       164 of       772\t|\tloss: 0.845647\n",
      "Training Epoch 5  21.4% | batch:       165 of       772\t|\tloss: 0.89146\n",
      "Training Epoch 5  21.5% | batch:       166 of       772\t|\tloss: 0.937618\n",
      "Training Epoch 5  21.6% | batch:       167 of       772\t|\tloss: 0.873283\n",
      "Training Epoch 5  21.8% | batch:       168 of       772\t|\tloss: 0.573631\n",
      "Training Epoch 5  21.9% | batch:       169 of       772\t|\tloss: 0.903568\n",
      "Training Epoch 5  22.0% | batch:       170 of       772\t|\tloss: 0.734284\n",
      "Training Epoch 5  22.2% | batch:       171 of       772\t|\tloss: 0.870516\n",
      "Training Epoch 5  22.3% | batch:       172 of       772\t|\tloss: 0.755147\n",
      "Training Epoch 5  22.4% | batch:       173 of       772\t|\tloss: 1.21911\n",
      "Training Epoch 5  22.5% | batch:       174 of       772\t|\tloss: 1.3189\n",
      "Training Epoch 5  22.7% | batch:       175 of       772\t|\tloss: 1.46606\n",
      "Training Epoch 5  22.8% | batch:       176 of       772\t|\tloss: 1.33732\n",
      "Training Epoch 5  22.9% | batch:       177 of       772\t|\tloss: 1.12975\n",
      "Training Epoch 5  23.1% | batch:       178 of       772\t|\tloss: 1.1644\n",
      "Training Epoch 5  23.2% | batch:       179 of       772\t|\tloss: 1.00724\n",
      "Training Epoch 5  23.3% | batch:       180 of       772\t|\tloss: 0.937235\n",
      "Training Epoch 5  23.4% | batch:       181 of       772\t|\tloss: 1.47738\n",
      "Training Epoch 5  23.6% | batch:       182 of       772\t|\tloss: 1.05147\n",
      "Training Epoch 5  23.7% | batch:       183 of       772\t|\tloss: 0.700742\n",
      "Training Epoch 5  23.8% | batch:       184 of       772\t|\tloss: 0.822402\n",
      "Training Epoch 5  24.0% | batch:       185 of       772\t|\tloss: 0.905836\n",
      "Training Epoch 5  24.1% | batch:       186 of       772\t|\tloss: 1.16386\n",
      "Training Epoch 5  24.2% | batch:       187 of       772\t|\tloss: 0.585632\n",
      "Training Epoch 5  24.4% | batch:       188 of       772\t|\tloss: 0.627781\n",
      "Training Epoch 5  24.5% | batch:       189 of       772\t|\tloss: 0.870162\n",
      "Training Epoch 5  24.6% | batch:       190 of       772\t|\tloss: 0.963348\n",
      "Training Epoch 5  24.7% | batch:       191 of       772\t|\tloss: 1.23705\n",
      "Training Epoch 5  24.9% | batch:       192 of       772\t|\tloss: 0.752067\n",
      "Training Epoch 5  25.0% | batch:       193 of       772\t|\tloss: 1.22292\n",
      "Training Epoch 5  25.1% | batch:       194 of       772\t|\tloss: 1.11881\n",
      "Training Epoch 5  25.3% | batch:       195 of       772\t|\tloss: 1.06607\n",
      "Training Epoch 5  25.4% | batch:       196 of       772\t|\tloss: 1.00428\n",
      "Training Epoch 5  25.5% | batch:       197 of       772\t|\tloss: 1.11226\n",
      "Training Epoch 5  25.6% | batch:       198 of       772\t|\tloss: 1.27963\n",
      "Training Epoch 5  25.8% | batch:       199 of       772\t|\tloss: 0.872818\n",
      "Training Epoch 5  25.9% | batch:       200 of       772\t|\tloss: 1.21184\n",
      "Training Epoch 5  26.0% | batch:       201 of       772\t|\tloss: 0.928136\n",
      "Training Epoch 5  26.2% | batch:       202 of       772\t|\tloss: 1.01969\n",
      "Training Epoch 5  26.3% | batch:       203 of       772\t|\tloss: 0.99136\n",
      "Training Epoch 5  26.4% | batch:       204 of       772\t|\tloss: 0.82925\n",
      "Training Epoch 5  26.6% | batch:       205 of       772\t|\tloss: 0.796139\n",
      "Training Epoch 5  26.7% | batch:       206 of       772\t|\tloss: 0.893854\n",
      "Training Epoch 5  26.8% | batch:       207 of       772\t|\tloss: 1.07213\n",
      "Training Epoch 5  26.9% | batch:       208 of       772\t|\tloss: 0.873468\n",
      "Training Epoch 5  27.1% | batch:       209 of       772\t|\tloss: 0.596306\n",
      "Training Epoch 5  27.2% | batch:       210 of       772\t|\tloss: 1.19848\n",
      "Training Epoch 5  27.3% | batch:       211 of       772\t|\tloss: 1.00699\n",
      "Training Epoch 5  27.5% | batch:       212 of       772\t|\tloss: 0.758197\n",
      "Training Epoch 5  27.6% | batch:       213 of       772\t|\tloss: 1.01334\n",
      "Training Epoch 5  27.7% | batch:       214 of       772\t|\tloss: 0.739244\n",
      "Training Epoch 5  27.8% | batch:       215 of       772\t|\tloss: 1.12245\n",
      "Training Epoch 5  28.0% | batch:       216 of       772\t|\tloss: 0.850404\n",
      "Training Epoch 5  28.1% | batch:       217 of       772\t|\tloss: 1.22817\n",
      "Training Epoch 5  28.2% | batch:       218 of       772\t|\tloss: 0.873347\n",
      "Training Epoch 5  28.4% | batch:       219 of       772\t|\tloss: 0.837475\n",
      "Training Epoch 5  28.5% | batch:       220 of       772\t|\tloss: 0.68243\n",
      "Training Epoch 5  28.6% | batch:       221 of       772\t|\tloss: 1.01688\n",
      "Training Epoch 5  28.8% | batch:       222 of       772\t|\tloss: 0.849853\n",
      "Training Epoch 5  28.9% | batch:       223 of       772\t|\tloss: 1.29168\n",
      "Training Epoch 5  29.0% | batch:       224 of       772\t|\tloss: 0.722127\n",
      "Training Epoch 5  29.1% | batch:       225 of       772\t|\tloss: 0.889142\n",
      "Training Epoch 5  29.3% | batch:       226 of       772\t|\tloss: 0.787326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  29.4% | batch:       227 of       772\t|\tloss: 0.787488\n",
      "Training Epoch 5  29.5% | batch:       228 of       772\t|\tloss: 0.68016\n",
      "Training Epoch 5  29.7% | batch:       229 of       772\t|\tloss: 0.717088\n",
      "Training Epoch 5  29.8% | batch:       230 of       772\t|\tloss: 0.610234\n",
      "Training Epoch 5  29.9% | batch:       231 of       772\t|\tloss: 1.02126\n",
      "Training Epoch 5  30.1% | batch:       232 of       772\t|\tloss: 0.897371\n",
      "Training Epoch 5  30.2% | batch:       233 of       772\t|\tloss: 1.15196\n",
      "Training Epoch 5  30.3% | batch:       234 of       772\t|\tloss: 0.961218\n",
      "Training Epoch 5  30.4% | batch:       235 of       772\t|\tloss: 0.960344\n",
      "Training Epoch 5  30.6% | batch:       236 of       772\t|\tloss: 1.37434\n",
      "Training Epoch 5  30.7% | batch:       237 of       772\t|\tloss: 0.856879\n",
      "Training Epoch 5  30.8% | batch:       238 of       772\t|\tloss: 0.966103\n",
      "Training Epoch 5  31.0% | batch:       239 of       772\t|\tloss: 1.06013\n",
      "Training Epoch 5  31.1% | batch:       240 of       772\t|\tloss: 1.04323\n",
      "Training Epoch 5  31.2% | batch:       241 of       772\t|\tloss: 1.01284\n",
      "Training Epoch 5  31.3% | batch:       242 of       772\t|\tloss: 0.811421\n",
      "Training Epoch 5  31.5% | batch:       243 of       772\t|\tloss: 0.929293\n",
      "Training Epoch 5  31.6% | batch:       244 of       772\t|\tloss: 1.00428\n",
      "Training Epoch 5  31.7% | batch:       245 of       772\t|\tloss: 0.985681\n",
      "Training Epoch 5  31.9% | batch:       246 of       772\t|\tloss: 0.97727\n",
      "Training Epoch 5  32.0% | batch:       247 of       772\t|\tloss: 0.707364\n",
      "Training Epoch 5  32.1% | batch:       248 of       772\t|\tloss: 1.42166\n",
      "Training Epoch 5  32.3% | batch:       249 of       772\t|\tloss: 1.1084\n",
      "Training Epoch 5  32.4% | batch:       250 of       772\t|\tloss: 0.784957\n",
      "Training Epoch 5  32.5% | batch:       251 of       772\t|\tloss: 1.17512\n",
      "Training Epoch 5  32.6% | batch:       252 of       772\t|\tloss: 0.685186\n",
      "Training Epoch 5  32.8% | batch:       253 of       772\t|\tloss: 1.16949\n",
      "Training Epoch 5  32.9% | batch:       254 of       772\t|\tloss: 1.13274\n",
      "Training Epoch 5  33.0% | batch:       255 of       772\t|\tloss: 0.955172\n",
      "Training Epoch 5  33.2% | batch:       256 of       772\t|\tloss: 1.0444\n",
      "Training Epoch 5  33.3% | batch:       257 of       772\t|\tloss: 0.831001\n",
      "Training Epoch 5  33.4% | batch:       258 of       772\t|\tloss: 1.34476\n",
      "Training Epoch 5  33.5% | batch:       259 of       772\t|\tloss: 1.05207\n",
      "Training Epoch 5  33.7% | batch:       260 of       772\t|\tloss: 0.833513\n",
      "Training Epoch 5  33.8% | batch:       261 of       772\t|\tloss: 0.931241\n",
      "Training Epoch 5  33.9% | batch:       262 of       772\t|\tloss: 1.16605\n",
      "Training Epoch 5  34.1% | batch:       263 of       772\t|\tloss: 0.981348\n",
      "Training Epoch 5  34.2% | batch:       264 of       772\t|\tloss: 1.13774\n",
      "Training Epoch 5  34.3% | batch:       265 of       772\t|\tloss: 0.588481\n",
      "Training Epoch 5  34.5% | batch:       266 of       772\t|\tloss: 0.949834\n",
      "Training Epoch 5  34.6% | batch:       267 of       772\t|\tloss: 1.3872\n",
      "Training Epoch 5  34.7% | batch:       268 of       772\t|\tloss: 1.34262\n",
      "Training Epoch 5  34.8% | batch:       269 of       772\t|\tloss: 1.15866\n",
      "Training Epoch 5  35.0% | batch:       270 of       772\t|\tloss: 0.807838\n",
      "Training Epoch 5  35.1% | batch:       271 of       772\t|\tloss: 0.985966\n",
      "Training Epoch 5  35.2% | batch:       272 of       772\t|\tloss: 1.37301\n",
      "Training Epoch 5  35.4% | batch:       273 of       772\t|\tloss: 1.42663\n",
      "Training Epoch 5  35.5% | batch:       274 of       772\t|\tloss: 1.02334\n",
      "Training Epoch 5  35.6% | batch:       275 of       772\t|\tloss: 1.13148\n",
      "Training Epoch 5  35.8% | batch:       276 of       772\t|\tloss: 0.853812\n",
      "Training Epoch 5  35.9% | batch:       277 of       772\t|\tloss: 1.10605\n",
      "Training Epoch 5  36.0% | batch:       278 of       772\t|\tloss: 0.89419\n",
      "Training Epoch 5  36.1% | batch:       279 of       772\t|\tloss: 0.646798\n",
      "Training Epoch 5  36.3% | batch:       280 of       772\t|\tloss: 1.20903\n",
      "Training Epoch 5  36.4% | batch:       281 of       772\t|\tloss: 1.1109\n",
      "Training Epoch 5  36.5% | batch:       282 of       772\t|\tloss: 0.890399\n",
      "Training Epoch 5  36.7% | batch:       283 of       772\t|\tloss: 0.913671\n",
      "Training Epoch 5  36.8% | batch:       284 of       772\t|\tloss: 1.20004\n",
      "Training Epoch 5  36.9% | batch:       285 of       772\t|\tloss: 0.83974\n",
      "Training Epoch 5  37.0% | batch:       286 of       772\t|\tloss: 0.933842\n",
      "Training Epoch 5  37.2% | batch:       287 of       772\t|\tloss: 0.881638\n",
      "Training Epoch 5  37.3% | batch:       288 of       772\t|\tloss: 0.58914\n",
      "Training Epoch 5  37.4% | batch:       289 of       772\t|\tloss: 1.08779\n",
      "Training Epoch 5  37.6% | batch:       290 of       772\t|\tloss: 1.5156\n",
      "Training Epoch 5  37.7% | batch:       291 of       772\t|\tloss: 1.03242\n",
      "Training Epoch 5  37.8% | batch:       292 of       772\t|\tloss: 0.742442\n",
      "Training Epoch 5  38.0% | batch:       293 of       772\t|\tloss: 0.964703\n",
      "Training Epoch 5  38.1% | batch:       294 of       772\t|\tloss: 0.909479\n",
      "Training Epoch 5  38.2% | batch:       295 of       772\t|\tloss: 0.781568\n",
      "Training Epoch 5  38.3% | batch:       296 of       772\t|\tloss: 1.22657\n",
      "Training Epoch 5  38.5% | batch:       297 of       772\t|\tloss: 1.32579\n",
      "Training Epoch 5  38.6% | batch:       298 of       772\t|\tloss: 0.575734\n",
      "Training Epoch 5  38.7% | batch:       299 of       772\t|\tloss: 0.862952\n",
      "Training Epoch 5  38.9% | batch:       300 of       772\t|\tloss: 0.873591\n",
      "Training Epoch 5  39.0% | batch:       301 of       772\t|\tloss: 0.910079\n",
      "Training Epoch 5  39.1% | batch:       302 of       772\t|\tloss: 1.05723\n",
      "Training Epoch 5  39.2% | batch:       303 of       772\t|\tloss: 0.946427\n",
      "Training Epoch 5  39.4% | batch:       304 of       772\t|\tloss: 1.13676\n",
      "Training Epoch 5  39.5% | batch:       305 of       772\t|\tloss: 0.886386\n",
      "Training Epoch 5  39.6% | batch:       306 of       772\t|\tloss: 1.04615\n",
      "Training Epoch 5  39.8% | batch:       307 of       772\t|\tloss: 1.16831\n",
      "Training Epoch 5  39.9% | batch:       308 of       772\t|\tloss: 0.594356\n",
      "Training Epoch 5  40.0% | batch:       309 of       772\t|\tloss: 0.830648\n",
      "Training Epoch 5  40.2% | batch:       310 of       772\t|\tloss: 0.803958\n",
      "Training Epoch 5  40.3% | batch:       311 of       772\t|\tloss: 1.1749\n",
      "Training Epoch 5  40.4% | batch:       312 of       772\t|\tloss: 0.89478\n",
      "Training Epoch 5  40.5% | batch:       313 of       772\t|\tloss: 0.694907\n",
      "Training Epoch 5  40.7% | batch:       314 of       772\t|\tloss: 0.943102\n",
      "Training Epoch 5  40.8% | batch:       315 of       772\t|\tloss: 0.94848\n",
      "Training Epoch 5  40.9% | batch:       316 of       772\t|\tloss: 1.19211\n",
      "Training Epoch 5  41.1% | batch:       317 of       772\t|\tloss: 0.96856\n",
      "Training Epoch 5  41.2% | batch:       318 of       772\t|\tloss: 0.732787\n",
      "Training Epoch 5  41.3% | batch:       319 of       772\t|\tloss: 0.676946\n",
      "Training Epoch 5  41.5% | batch:       320 of       772\t|\tloss: 0.718264\n",
      "Training Epoch 5  41.6% | batch:       321 of       772\t|\tloss: 0.68081\n",
      "Training Epoch 5  41.7% | batch:       322 of       772\t|\tloss: 0.800731\n",
      "Training Epoch 5  41.8% | batch:       323 of       772\t|\tloss: 0.690189\n",
      "Training Epoch 5  42.0% | batch:       324 of       772\t|\tloss: 1.08641\n",
      "Training Epoch 5  42.1% | batch:       325 of       772\t|\tloss: 0.797074\n",
      "Training Epoch 5  42.2% | batch:       326 of       772\t|\tloss: 1.11755\n",
      "Training Epoch 5  42.4% | batch:       327 of       772\t|\tloss: 1.19012\n",
      "Training Epoch 5  42.5% | batch:       328 of       772\t|\tloss: 1.15512\n",
      "Training Epoch 5  42.6% | batch:       329 of       772\t|\tloss: 0.821067\n",
      "Training Epoch 5  42.7% | batch:       330 of       772\t|\tloss: 0.756562\n",
      "Training Epoch 5  42.9% | batch:       331 of       772\t|\tloss: 1.02346\n",
      "Training Epoch 5  43.0% | batch:       332 of       772\t|\tloss: 1.07619\n",
      "Training Epoch 5  43.1% | batch:       333 of       772\t|\tloss: 0.773105\n",
      "Training Epoch 5  43.3% | batch:       334 of       772\t|\tloss: 0.743762\n",
      "Training Epoch 5  43.4% | batch:       335 of       772\t|\tloss: 0.874605\n",
      "Training Epoch 5  43.5% | batch:       336 of       772\t|\tloss: 0.74358\n",
      "Training Epoch 5  43.7% | batch:       337 of       772\t|\tloss: 0.619548\n",
      "Training Epoch 5  43.8% | batch:       338 of       772\t|\tloss: 0.890684\n",
      "Training Epoch 5  43.9% | batch:       339 of       772\t|\tloss: 0.657347\n",
      "Training Epoch 5  44.0% | batch:       340 of       772\t|\tloss: 0.605698\n",
      "Training Epoch 5  44.2% | batch:       341 of       772\t|\tloss: 0.986398\n",
      "Training Epoch 5  44.3% | batch:       342 of       772\t|\tloss: 0.845969\n",
      "Training Epoch 5  44.4% | batch:       343 of       772\t|\tloss: 0.88748\n",
      "Training Epoch 5  44.6% | batch:       344 of       772\t|\tloss: 0.809919\n",
      "Training Epoch 5  44.7% | batch:       345 of       772\t|\tloss: 0.806658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  44.8% | batch:       346 of       772\t|\tloss: 0.769897\n",
      "Training Epoch 5  44.9% | batch:       347 of       772\t|\tloss: 1.00067\n",
      "Training Epoch 5  45.1% | batch:       348 of       772\t|\tloss: 1.31719\n",
      "Training Epoch 5  45.2% | batch:       349 of       772\t|\tloss: 0.951016\n",
      "Training Epoch 5  45.3% | batch:       350 of       772\t|\tloss: 0.704403\n",
      "Training Epoch 5  45.5% | batch:       351 of       772\t|\tloss: 1.06134\n",
      "Training Epoch 5  45.6% | batch:       352 of       772\t|\tloss: 0.987878\n",
      "Training Epoch 5  45.7% | batch:       353 of       772\t|\tloss: 0.980429\n",
      "Training Epoch 5  45.9% | batch:       354 of       772\t|\tloss: 0.90485\n",
      "Training Epoch 5  46.0% | batch:       355 of       772\t|\tloss: 1.14424\n",
      "Training Epoch 5  46.1% | batch:       356 of       772\t|\tloss: 1.28178\n",
      "Training Epoch 5  46.2% | batch:       357 of       772\t|\tloss: 0.828607\n",
      "Training Epoch 5  46.4% | batch:       358 of       772\t|\tloss: 0.828111\n",
      "Training Epoch 5  46.5% | batch:       359 of       772\t|\tloss: 0.849438\n",
      "Training Epoch 5  46.6% | batch:       360 of       772\t|\tloss: 0.958383\n",
      "Training Epoch 5  46.8% | batch:       361 of       772\t|\tloss: 0.788526\n",
      "Training Epoch 5  46.9% | batch:       362 of       772\t|\tloss: 0.762201\n",
      "Training Epoch 5  47.0% | batch:       363 of       772\t|\tloss: 0.974846\n",
      "Training Epoch 5  47.2% | batch:       364 of       772\t|\tloss: 0.665161\n",
      "Training Epoch 5  47.3% | batch:       365 of       772\t|\tloss: 0.816755\n",
      "Training Epoch 5  47.4% | batch:       366 of       772\t|\tloss: 0.920627\n",
      "Training Epoch 5  47.5% | batch:       367 of       772\t|\tloss: 0.702976\n",
      "Training Epoch 5  47.7% | batch:       368 of       772\t|\tloss: 0.696685\n",
      "Training Epoch 5  47.8% | batch:       369 of       772\t|\tloss: 0.649951\n",
      "Training Epoch 5  47.9% | batch:       370 of       772\t|\tloss: 0.946995\n",
      "Training Epoch 5  48.1% | batch:       371 of       772\t|\tloss: 0.673162\n",
      "Training Epoch 5  48.2% | batch:       372 of       772\t|\tloss: 0.760148\n",
      "Training Epoch 5  48.3% | batch:       373 of       772\t|\tloss: 0.928063\n",
      "Training Epoch 5  48.4% | batch:       374 of       772\t|\tloss: 0.588663\n",
      "Training Epoch 5  48.6% | batch:       375 of       772\t|\tloss: 0.923254\n",
      "Training Epoch 5  48.7% | batch:       376 of       772\t|\tloss: 0.877619\n",
      "Training Epoch 5  48.8% | batch:       377 of       772\t|\tloss: 0.988714\n",
      "Training Epoch 5  49.0% | batch:       378 of       772\t|\tloss: 1.23387\n",
      "Training Epoch 5  49.1% | batch:       379 of       772\t|\tloss: 0.953887\n",
      "Training Epoch 5  49.2% | batch:       380 of       772\t|\tloss: 0.656995\n",
      "Training Epoch 5  49.4% | batch:       381 of       772\t|\tloss: 1.14948\n",
      "Training Epoch 5  49.5% | batch:       382 of       772\t|\tloss: 1.76473\n",
      "Training Epoch 5  49.6% | batch:       383 of       772\t|\tloss: 1.48714\n",
      "Training Epoch 5  49.7% | batch:       384 of       772\t|\tloss: 0.750686\n",
      "Training Epoch 5  49.9% | batch:       385 of       772\t|\tloss: 0.855608\n",
      "Training Epoch 5  50.0% | batch:       386 of       772\t|\tloss: 1.01317\n",
      "Training Epoch 5  50.1% | batch:       387 of       772\t|\tloss: 1.46441\n",
      "Training Epoch 5  50.3% | batch:       388 of       772\t|\tloss: 1.33155\n",
      "Training Epoch 5  50.4% | batch:       389 of       772\t|\tloss: 1.21431\n",
      "Training Epoch 5  50.5% | batch:       390 of       772\t|\tloss: 0.836099\n",
      "Training Epoch 5  50.6% | batch:       391 of       772\t|\tloss: 0.868764\n",
      "Training Epoch 5  50.8% | batch:       392 of       772\t|\tloss: 0.789662\n",
      "Training Epoch 5  50.9% | batch:       393 of       772\t|\tloss: 1.11324\n",
      "Training Epoch 5  51.0% | batch:       394 of       772\t|\tloss: 0.952705\n",
      "Training Epoch 5  51.2% | batch:       395 of       772\t|\tloss: 0.867658\n",
      "Training Epoch 5  51.3% | batch:       396 of       772\t|\tloss: 0.685108\n",
      "Training Epoch 5  51.4% | batch:       397 of       772\t|\tloss: 0.830553\n",
      "Training Epoch 5  51.6% | batch:       398 of       772\t|\tloss: 1.08412\n",
      "Training Epoch 5  51.7% | batch:       399 of       772\t|\tloss: 0.604898\n",
      "Training Epoch 5  51.8% | batch:       400 of       772\t|\tloss: 0.823368\n",
      "Training Epoch 5  51.9% | batch:       401 of       772\t|\tloss: 1.04119\n",
      "Training Epoch 5  52.1% | batch:       402 of       772\t|\tloss: 1.00044\n",
      "Training Epoch 5  52.2% | batch:       403 of       772\t|\tloss: 1.04643\n",
      "Training Epoch 5  52.3% | batch:       404 of       772\t|\tloss: 0.926685\n",
      "Training Epoch 5  52.5% | batch:       405 of       772\t|\tloss: 1.35346\n",
      "Training Epoch 5  52.6% | batch:       406 of       772\t|\tloss: 1.06266\n",
      "Training Epoch 5  52.7% | batch:       407 of       772\t|\tloss: 0.78007\n",
      "Training Epoch 5  52.8% | batch:       408 of       772\t|\tloss: 0.694328\n",
      "Training Epoch 5  53.0% | batch:       409 of       772\t|\tloss: 0.803882\n",
      "Training Epoch 5  53.1% | batch:       410 of       772\t|\tloss: 0.830002\n",
      "Training Epoch 5  53.2% | batch:       411 of       772\t|\tloss: 0.740717\n",
      "Training Epoch 5  53.4% | batch:       412 of       772\t|\tloss: 0.74598\n",
      "Training Epoch 5  53.5% | batch:       413 of       772\t|\tloss: 1.04983\n",
      "Training Epoch 5  53.6% | batch:       414 of       772\t|\tloss: 0.802404\n",
      "Training Epoch 5  53.8% | batch:       415 of       772\t|\tloss: 0.902987\n",
      "Training Epoch 5  53.9% | batch:       416 of       772\t|\tloss: 0.693352\n",
      "Training Epoch 5  54.0% | batch:       417 of       772\t|\tloss: 0.688842\n",
      "Training Epoch 5  54.1% | batch:       418 of       772\t|\tloss: 0.906217\n",
      "Training Epoch 5  54.3% | batch:       419 of       772\t|\tloss: 1.11077\n",
      "Training Epoch 5  54.4% | batch:       420 of       772\t|\tloss: 0.587518\n",
      "Training Epoch 5  54.5% | batch:       421 of       772\t|\tloss: 0.789275\n",
      "Training Epoch 5  54.7% | batch:       422 of       772\t|\tloss: 0.824405\n",
      "Training Epoch 5  54.8% | batch:       423 of       772\t|\tloss: 0.83501\n",
      "Training Epoch 5  54.9% | batch:       424 of       772\t|\tloss: 0.79187\n",
      "Training Epoch 5  55.1% | batch:       425 of       772\t|\tloss: 0.801606\n",
      "Training Epoch 5  55.2% | batch:       426 of       772\t|\tloss: 0.547033\n",
      "Training Epoch 5  55.3% | batch:       427 of       772\t|\tloss: 0.642252\n",
      "Training Epoch 5  55.4% | batch:       428 of       772\t|\tloss: 1.34199\n",
      "Training Epoch 5  55.6% | batch:       429 of       772\t|\tloss: 1.29691\n",
      "Training Epoch 5  55.7% | batch:       430 of       772\t|\tloss: 0.828647\n",
      "Training Epoch 5  55.8% | batch:       431 of       772\t|\tloss: 1.14274\n",
      "Training Epoch 5  56.0% | batch:       432 of       772\t|\tloss: 0.812783\n",
      "Training Epoch 5  56.1% | batch:       433 of       772\t|\tloss: 0.876445\n",
      "Training Epoch 5  56.2% | batch:       434 of       772\t|\tloss: 1.08424\n",
      "Training Epoch 5  56.3% | batch:       435 of       772\t|\tloss: 0.769151\n",
      "Training Epoch 5  56.5% | batch:       436 of       772\t|\tloss: 0.814473\n",
      "Training Epoch 5  56.6% | batch:       437 of       772\t|\tloss: 0.771512\n",
      "Training Epoch 5  56.7% | batch:       438 of       772\t|\tloss: 1.35194\n",
      "Training Epoch 5  56.9% | batch:       439 of       772\t|\tloss: 1.27148\n",
      "Training Epoch 5  57.0% | batch:       440 of       772\t|\tloss: 1.18772\n",
      "Training Epoch 5  57.1% | batch:       441 of       772\t|\tloss: 0.85748\n",
      "Training Epoch 5  57.3% | batch:       442 of       772\t|\tloss: 0.713204\n",
      "Training Epoch 5  57.4% | batch:       443 of       772\t|\tloss: 0.882353\n",
      "Training Epoch 5  57.5% | batch:       444 of       772\t|\tloss: 0.950711\n",
      "Training Epoch 5  57.6% | batch:       445 of       772\t|\tloss: 0.559144\n",
      "Training Epoch 5  57.8% | batch:       446 of       772\t|\tloss: 0.909768\n",
      "Training Epoch 5  57.9% | batch:       447 of       772\t|\tloss: 1.36192\n",
      "Training Epoch 5  58.0% | batch:       448 of       772\t|\tloss: 0.928147\n",
      "Training Epoch 5  58.2% | batch:       449 of       772\t|\tloss: 0.591146\n",
      "Training Epoch 5  58.3% | batch:       450 of       772\t|\tloss: 0.941023\n",
      "Training Epoch 5  58.4% | batch:       451 of       772\t|\tloss: 0.677781\n",
      "Training Epoch 5  58.5% | batch:       452 of       772\t|\tloss: 0.909408\n",
      "Training Epoch 5  58.7% | batch:       453 of       772\t|\tloss: 1.19016\n",
      "Training Epoch 5  58.8% | batch:       454 of       772\t|\tloss: 0.858689\n",
      "Training Epoch 5  58.9% | batch:       455 of       772\t|\tloss: 0.718625\n",
      "Training Epoch 5  59.1% | batch:       456 of       772\t|\tloss: 1.08562\n",
      "Training Epoch 5  59.2% | batch:       457 of       772\t|\tloss: 1.21238\n",
      "Training Epoch 5  59.3% | batch:       458 of       772\t|\tloss: 1.46789\n",
      "Training Epoch 5  59.5% | batch:       459 of       772\t|\tloss: 0.689709\n",
      "Training Epoch 5  59.6% | batch:       460 of       772\t|\tloss: 1.12787\n",
      "Training Epoch 5  59.7% | batch:       461 of       772\t|\tloss: 1.40913\n",
      "Training Epoch 5  59.8% | batch:       462 of       772\t|\tloss: 1.14314\n",
      "Training Epoch 5  60.0% | batch:       463 of       772\t|\tloss: 0.835868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  60.1% | batch:       464 of       772\t|\tloss: 0.796087\n",
      "Training Epoch 5  60.2% | batch:       465 of       772\t|\tloss: 1.1095\n",
      "Training Epoch 5  60.4% | batch:       466 of       772\t|\tloss: 1.01912\n",
      "Training Epoch 5  60.5% | batch:       467 of       772\t|\tloss: 0.776936\n",
      "Training Epoch 5  60.6% | batch:       468 of       772\t|\tloss: 0.990764\n",
      "Training Epoch 5  60.8% | batch:       469 of       772\t|\tloss: 1.0098\n",
      "Training Epoch 5  60.9% | batch:       470 of       772\t|\tloss: 1.14091\n",
      "Training Epoch 5  61.0% | batch:       471 of       772\t|\tloss: 1.1299\n",
      "Training Epoch 5  61.1% | batch:       472 of       772\t|\tloss: 0.859806\n",
      "Training Epoch 5  61.3% | batch:       473 of       772\t|\tloss: 1.03558\n",
      "Training Epoch 5  61.4% | batch:       474 of       772\t|\tloss: 1.26345\n",
      "Training Epoch 5  61.5% | batch:       475 of       772\t|\tloss: 0.771403\n",
      "Training Epoch 5  61.7% | batch:       476 of       772\t|\tloss: 0.890095\n",
      "Training Epoch 5  61.8% | batch:       477 of       772\t|\tloss: 0.875561\n",
      "Training Epoch 5  61.9% | batch:       478 of       772\t|\tloss: 1.14517\n",
      "Training Epoch 5  62.0% | batch:       479 of       772\t|\tloss: 0.566199\n",
      "Training Epoch 5  62.2% | batch:       480 of       772\t|\tloss: 0.870892\n",
      "Training Epoch 5  62.3% | batch:       481 of       772\t|\tloss: 0.773397\n",
      "Training Epoch 5  62.4% | batch:       482 of       772\t|\tloss: 0.95699\n",
      "Training Epoch 5  62.6% | batch:       483 of       772\t|\tloss: 1.41915\n",
      "Training Epoch 5  62.7% | batch:       484 of       772\t|\tloss: 0.815612\n",
      "Training Epoch 5  62.8% | batch:       485 of       772\t|\tloss: 0.761804\n",
      "Training Epoch 5  63.0% | batch:       486 of       772\t|\tloss: 0.925521\n",
      "Training Epoch 5  63.1% | batch:       487 of       772\t|\tloss: 0.758093\n",
      "Training Epoch 5  63.2% | batch:       488 of       772\t|\tloss: 0.910583\n",
      "Training Epoch 5  63.3% | batch:       489 of       772\t|\tloss: 0.919556\n",
      "Training Epoch 5  63.5% | batch:       490 of       772\t|\tloss: 0.681629\n",
      "Training Epoch 5  63.6% | batch:       491 of       772\t|\tloss: 0.95192\n",
      "Training Epoch 5  63.7% | batch:       492 of       772\t|\tloss: 0.865077\n",
      "Training Epoch 5  63.9% | batch:       493 of       772\t|\tloss: 0.615944\n",
      "Training Epoch 5  64.0% | batch:       494 of       772\t|\tloss: 0.768939\n",
      "Training Epoch 5  64.1% | batch:       495 of       772\t|\tloss: 0.652349\n",
      "Training Epoch 5  64.2% | batch:       496 of       772\t|\tloss: 0.587691\n",
      "Training Epoch 5  64.4% | batch:       497 of       772\t|\tloss: 0.573358\n",
      "Training Epoch 5  64.5% | batch:       498 of       772\t|\tloss: 0.951328\n",
      "Training Epoch 5  64.6% | batch:       499 of       772\t|\tloss: 0.812268\n",
      "Training Epoch 5  64.8% | batch:       500 of       772\t|\tloss: 0.768765\n",
      "Training Epoch 5  64.9% | batch:       501 of       772\t|\tloss: 1.02696\n",
      "Training Epoch 5  65.0% | batch:       502 of       772\t|\tloss: 0.793267\n",
      "Training Epoch 5  65.2% | batch:       503 of       772\t|\tloss: 1.22969\n",
      "Training Epoch 5  65.3% | batch:       504 of       772\t|\tloss: 0.869155\n",
      "Training Epoch 5  65.4% | batch:       505 of       772\t|\tloss: 0.860254\n",
      "Training Epoch 5  65.5% | batch:       506 of       772\t|\tloss: 0.715779\n",
      "Training Epoch 5  65.7% | batch:       507 of       772\t|\tloss: 0.81315\n",
      "Training Epoch 5  65.8% | batch:       508 of       772\t|\tloss: 0.63993\n",
      "Training Epoch 5  65.9% | batch:       509 of       772\t|\tloss: 0.994632\n",
      "Training Epoch 5  66.1% | batch:       510 of       772\t|\tloss: 1.00854\n",
      "Training Epoch 5  66.2% | batch:       511 of       772\t|\tloss: 0.74902\n",
      "Training Epoch 5  66.3% | batch:       512 of       772\t|\tloss: 1.00831\n",
      "Training Epoch 5  66.5% | batch:       513 of       772\t|\tloss: 1.00471\n",
      "Training Epoch 5  66.6% | batch:       514 of       772\t|\tloss: 1.08194\n",
      "Training Epoch 5  66.7% | batch:       515 of       772\t|\tloss: 0.88715\n",
      "Training Epoch 5  66.8% | batch:       516 of       772\t|\tloss: 1.16849\n",
      "Training Epoch 5  67.0% | batch:       517 of       772\t|\tloss: 0.895147\n",
      "Training Epoch 5  67.1% | batch:       518 of       772\t|\tloss: 1.00437\n",
      "Training Epoch 5  67.2% | batch:       519 of       772\t|\tloss: 0.718474\n",
      "Training Epoch 5  67.4% | batch:       520 of       772\t|\tloss: 0.747049\n",
      "Training Epoch 5  67.5% | batch:       521 of       772\t|\tloss: 0.98744\n",
      "Training Epoch 5  67.6% | batch:       522 of       772\t|\tloss: 1.17298\n",
      "Training Epoch 5  67.7% | batch:       523 of       772\t|\tloss: 1.41297\n",
      "Training Epoch 5  67.9% | batch:       524 of       772\t|\tloss: 0.907409\n",
      "Training Epoch 5  68.0% | batch:       525 of       772\t|\tloss: 1.08857\n",
      "Training Epoch 5  68.1% | batch:       526 of       772\t|\tloss: 0.851755\n",
      "Training Epoch 5  68.3% | batch:       527 of       772\t|\tloss: 0.909893\n",
      "Training Epoch 5  68.4% | batch:       528 of       772\t|\tloss: 0.960373\n",
      "Training Epoch 5  68.5% | batch:       529 of       772\t|\tloss: 1.18598\n",
      "Training Epoch 5  68.7% | batch:       530 of       772\t|\tloss: 1.1879\n",
      "Training Epoch 5  68.8% | batch:       531 of       772\t|\tloss: 1.22018\n",
      "Training Epoch 5  68.9% | batch:       532 of       772\t|\tloss: 0.700813\n",
      "Training Epoch 5  69.0% | batch:       533 of       772\t|\tloss: 1.11826\n",
      "Training Epoch 5  69.2% | batch:       534 of       772\t|\tloss: 1.10832\n",
      "Training Epoch 5  69.3% | batch:       535 of       772\t|\tloss: 1.5436\n",
      "Training Epoch 5  69.4% | batch:       536 of       772\t|\tloss: 1.54108\n",
      "Training Epoch 5  69.6% | batch:       537 of       772\t|\tloss: 0.933325\n",
      "Training Epoch 5  69.7% | batch:       538 of       772\t|\tloss: 0.796046\n",
      "Training Epoch 5  69.8% | batch:       539 of       772\t|\tloss: 1.02797\n",
      "Training Epoch 5  69.9% | batch:       540 of       772\t|\tloss: 1.10487\n",
      "Training Epoch 5  70.1% | batch:       541 of       772\t|\tloss: 1.22667\n",
      "Training Epoch 5  70.2% | batch:       542 of       772\t|\tloss: 1.48065\n",
      "Training Epoch 5  70.3% | batch:       543 of       772\t|\tloss: 0.91789\n",
      "Training Epoch 5  70.5% | batch:       544 of       772\t|\tloss: 0.986592\n",
      "Training Epoch 5  70.6% | batch:       545 of       772\t|\tloss: 0.754014\n",
      "Training Epoch 5  70.7% | batch:       546 of       772\t|\tloss: 0.723747\n",
      "Training Epoch 5  70.9% | batch:       547 of       772\t|\tloss: 0.919431\n",
      "Training Epoch 5  71.0% | batch:       548 of       772\t|\tloss: 1.08915\n",
      "Training Epoch 5  71.1% | batch:       549 of       772\t|\tloss: 1.23369\n",
      "Training Epoch 5  71.2% | batch:       550 of       772\t|\tloss: 1.35415\n",
      "Training Epoch 5  71.4% | batch:       551 of       772\t|\tloss: 1.06221\n",
      "Training Epoch 5  71.5% | batch:       552 of       772\t|\tloss: 0.717853\n",
      "Training Epoch 5  71.6% | batch:       553 of       772\t|\tloss: 0.840749\n",
      "Training Epoch 5  71.8% | batch:       554 of       772\t|\tloss: 1.07789\n",
      "Training Epoch 5  71.9% | batch:       555 of       772\t|\tloss: 1.31865\n",
      "Training Epoch 5  72.0% | batch:       556 of       772\t|\tloss: 1.07069\n",
      "Training Epoch 5  72.2% | batch:       557 of       772\t|\tloss: 0.889912\n",
      "Training Epoch 5  72.3% | batch:       558 of       772\t|\tloss: 1.13929\n",
      "Training Epoch 5  72.4% | batch:       559 of       772\t|\tloss: 1.06585\n",
      "Training Epoch 5  72.5% | batch:       560 of       772\t|\tloss: 1.17652\n",
      "Training Epoch 5  72.7% | batch:       561 of       772\t|\tloss: 0.932738\n",
      "Training Epoch 5  72.8% | batch:       562 of       772\t|\tloss: 0.994391\n",
      "Training Epoch 5  72.9% | batch:       563 of       772\t|\tloss: 0.932406\n",
      "Training Epoch 5  73.1% | batch:       564 of       772\t|\tloss: 1.20569\n",
      "Training Epoch 5  73.2% | batch:       565 of       772\t|\tloss: 0.683881\n",
      "Training Epoch 5  73.3% | batch:       566 of       772\t|\tloss: 1.12661\n",
      "Training Epoch 5  73.4% | batch:       567 of       772\t|\tloss: 1.09367\n",
      "Training Epoch 5  73.6% | batch:       568 of       772\t|\tloss: 0.973876\n",
      "Training Epoch 5  73.7% | batch:       569 of       772\t|\tloss: 0.898987\n",
      "Training Epoch 5  73.8% | batch:       570 of       772\t|\tloss: 0.695389\n",
      "Training Epoch 5  74.0% | batch:       571 of       772\t|\tloss: 0.891991\n",
      "Training Epoch 5  74.1% | batch:       572 of       772\t|\tloss: 0.63949\n",
      "Training Epoch 5  74.2% | batch:       573 of       772\t|\tloss: 0.81451\n",
      "Training Epoch 5  74.4% | batch:       574 of       772\t|\tloss: 0.915703\n",
      "Training Epoch 5  74.5% | batch:       575 of       772\t|\tloss: 0.928256\n",
      "Training Epoch 5  74.6% | batch:       576 of       772\t|\tloss: 1.00886\n",
      "Training Epoch 5  74.7% | batch:       577 of       772\t|\tloss: 1.0654\n",
      "Training Epoch 5  74.9% | batch:       578 of       772\t|\tloss: 0.969794\n",
      "Training Epoch 5  75.0% | batch:       579 of       772\t|\tloss: 0.633291\n",
      "Training Epoch 5  75.1% | batch:       580 of       772\t|\tloss: 0.969838\n",
      "Training Epoch 5  75.3% | batch:       581 of       772\t|\tloss: 0.836797\n",
      "Training Epoch 5  75.4% | batch:       582 of       772\t|\tloss: 0.594726\n",
      "Training Epoch 5  75.5% | batch:       583 of       772\t|\tloss: 0.821219\n",
      "Training Epoch 5  75.6% | batch:       584 of       772\t|\tloss: 0.809229\n",
      "Training Epoch 5  75.8% | batch:       585 of       772\t|\tloss: 0.962671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  75.9% | batch:       586 of       772\t|\tloss: 1.19038\n",
      "Training Epoch 5  76.0% | batch:       587 of       772\t|\tloss: 1.22044\n",
      "Training Epoch 5  76.2% | batch:       588 of       772\t|\tloss: 0.78418\n",
      "Training Epoch 5  76.3% | batch:       589 of       772\t|\tloss: 0.688822\n",
      "Training Epoch 5  76.4% | batch:       590 of       772\t|\tloss: 1.31081\n",
      "Training Epoch 5  76.6% | batch:       591 of       772\t|\tloss: 1.23798\n",
      "Training Epoch 5  76.7% | batch:       592 of       772\t|\tloss: 0.765146\n",
      "Training Epoch 5  76.8% | batch:       593 of       772\t|\tloss: 0.826476\n",
      "Training Epoch 5  76.9% | batch:       594 of       772\t|\tloss: 0.975284\n",
      "Training Epoch 5  77.1% | batch:       595 of       772\t|\tloss: 0.970779\n",
      "Training Epoch 5  77.2% | batch:       596 of       772\t|\tloss: 0.897905\n",
      "Training Epoch 5  77.3% | batch:       597 of       772\t|\tloss: 0.867572\n",
      "Training Epoch 5  77.5% | batch:       598 of       772\t|\tloss: 0.666951\n",
      "Training Epoch 5  77.6% | batch:       599 of       772\t|\tloss: 0.865247\n",
      "Training Epoch 5  77.7% | batch:       600 of       772\t|\tloss: 0.746178\n",
      "Training Epoch 5  77.8% | batch:       601 of       772\t|\tloss: 0.746562\n",
      "Training Epoch 5  78.0% | batch:       602 of       772\t|\tloss: 0.719975\n",
      "Training Epoch 5  78.1% | batch:       603 of       772\t|\tloss: 0.813492\n",
      "Training Epoch 5  78.2% | batch:       604 of       772\t|\tloss: 0.707415\n",
      "Training Epoch 5  78.4% | batch:       605 of       772\t|\tloss: 0.731209\n",
      "Training Epoch 5  78.5% | batch:       606 of       772\t|\tloss: 0.78533\n",
      "Training Epoch 5  78.6% | batch:       607 of       772\t|\tloss: 0.699332\n",
      "Training Epoch 5  78.8% | batch:       608 of       772\t|\tloss: 0.620328\n",
      "Training Epoch 5  78.9% | batch:       609 of       772\t|\tloss: 0.839714\n",
      "Training Epoch 5  79.0% | batch:       610 of       772\t|\tloss: 0.668592\n",
      "Training Epoch 5  79.1% | batch:       611 of       772\t|\tloss: 0.851284\n",
      "Training Epoch 5  79.3% | batch:       612 of       772\t|\tloss: 0.682502\n",
      "Training Epoch 5  79.4% | batch:       613 of       772\t|\tloss: 0.902592\n",
      "Training Epoch 5  79.5% | batch:       614 of       772\t|\tloss: 0.589626\n",
      "Training Epoch 5  79.7% | batch:       615 of       772\t|\tloss: 0.679753\n",
      "Training Epoch 5  79.8% | batch:       616 of       772\t|\tloss: 0.579914\n",
      "Training Epoch 5  79.9% | batch:       617 of       772\t|\tloss: 0.644318\n",
      "Training Epoch 5  80.1% | batch:       618 of       772\t|\tloss: 0.863263\n",
      "Training Epoch 5  80.2% | batch:       619 of       772\t|\tloss: 0.721488\n",
      "Training Epoch 5  80.3% | batch:       620 of       772\t|\tloss: 0.580154\n",
      "Training Epoch 5  80.4% | batch:       621 of       772\t|\tloss: 0.828041\n",
      "Training Epoch 5  80.6% | batch:       622 of       772\t|\tloss: 0.905677\n",
      "Training Epoch 5  80.7% | batch:       623 of       772\t|\tloss: 0.774808\n",
      "Training Epoch 5  80.8% | batch:       624 of       772\t|\tloss: 0.78774\n",
      "Training Epoch 5  81.0% | batch:       625 of       772\t|\tloss: 0.621656\n",
      "Training Epoch 5  81.1% | batch:       626 of       772\t|\tloss: 0.981554\n",
      "Training Epoch 5  81.2% | batch:       627 of       772\t|\tloss: 0.751521\n",
      "Training Epoch 5  81.3% | batch:       628 of       772\t|\tloss: 0.849021\n",
      "Training Epoch 5  81.5% | batch:       629 of       772\t|\tloss: 0.911762\n",
      "Training Epoch 5  81.6% | batch:       630 of       772\t|\tloss: 0.591851\n",
      "Training Epoch 5  81.7% | batch:       631 of       772\t|\tloss: 0.674366\n",
      "Training Epoch 5  81.9% | batch:       632 of       772\t|\tloss: 1.28642\n",
      "Training Epoch 5  82.0% | batch:       633 of       772\t|\tloss: 1.09196\n",
      "Training Epoch 5  82.1% | batch:       634 of       772\t|\tloss: 0.673199\n",
      "Training Epoch 5  82.3% | batch:       635 of       772\t|\tloss: 1.25218\n",
      "Training Epoch 5  82.4% | batch:       636 of       772\t|\tloss: 0.668834\n",
      "Training Epoch 5  82.5% | batch:       637 of       772\t|\tloss: 0.809922\n",
      "Training Epoch 5  82.6% | batch:       638 of       772\t|\tloss: 0.768747\n",
      "Training Epoch 5  82.8% | batch:       639 of       772\t|\tloss: 0.981687\n",
      "Training Epoch 5  82.9% | batch:       640 of       772\t|\tloss: 0.807793\n",
      "Training Epoch 5  83.0% | batch:       641 of       772\t|\tloss: 0.78126\n",
      "Training Epoch 5  83.2% | batch:       642 of       772\t|\tloss: 0.573224\n",
      "Training Epoch 5  83.3% | batch:       643 of       772\t|\tloss: 0.716187\n",
      "Training Epoch 5  83.4% | batch:       644 of       772\t|\tloss: 0.850309\n",
      "Training Epoch 5  83.5% | batch:       645 of       772\t|\tloss: 0.925027\n",
      "Training Epoch 5  83.7% | batch:       646 of       772\t|\tloss: 0.95459\n",
      "Training Epoch 5  83.8% | batch:       647 of       772\t|\tloss: 0.902558\n",
      "Training Epoch 5  83.9% | batch:       648 of       772\t|\tloss: 0.953697\n",
      "Training Epoch 5  84.1% | batch:       649 of       772\t|\tloss: 0.743936\n",
      "Training Epoch 5  84.2% | batch:       650 of       772\t|\tloss: 0.931553\n",
      "Training Epoch 5  84.3% | batch:       651 of       772\t|\tloss: 1.08261\n",
      "Training Epoch 5  84.5% | batch:       652 of       772\t|\tloss: 0.822973\n",
      "Training Epoch 5  84.6% | batch:       653 of       772\t|\tloss: 0.791216\n",
      "Training Epoch 5  84.7% | batch:       654 of       772\t|\tloss: 0.770358\n",
      "Training Epoch 5  84.8% | batch:       655 of       772\t|\tloss: 0.864894\n",
      "Training Epoch 5  85.0% | batch:       656 of       772\t|\tloss: 0.6018\n",
      "Training Epoch 5  85.1% | batch:       657 of       772\t|\tloss: 1.12871\n",
      "Training Epoch 5  85.2% | batch:       658 of       772\t|\tloss: 0.850633\n",
      "Training Epoch 5  85.4% | batch:       659 of       772\t|\tloss: 0.66099\n",
      "Training Epoch 5  85.5% | batch:       660 of       772\t|\tloss: 0.934223\n",
      "Training Epoch 5  85.6% | batch:       661 of       772\t|\tloss: 1.17903\n",
      "Training Epoch 5  85.8% | batch:       662 of       772\t|\tloss: 1.03838\n",
      "Training Epoch 5  85.9% | batch:       663 of       772\t|\tloss: 0.683396\n",
      "Training Epoch 5  86.0% | batch:       664 of       772\t|\tloss: 0.822608\n",
      "Training Epoch 5  86.1% | batch:       665 of       772\t|\tloss: 0.91044\n",
      "Training Epoch 5  86.3% | batch:       666 of       772\t|\tloss: 0.850412\n",
      "Training Epoch 5  86.4% | batch:       667 of       772\t|\tloss: 1.04432\n",
      "Training Epoch 5  86.5% | batch:       668 of       772\t|\tloss: 1.13352\n",
      "Training Epoch 5  86.7% | batch:       669 of       772\t|\tloss: 0.858229\n",
      "Training Epoch 5  86.8% | batch:       670 of       772\t|\tloss: 0.846814\n",
      "Training Epoch 5  86.9% | batch:       671 of       772\t|\tloss: 0.811111\n",
      "Training Epoch 5  87.0% | batch:       672 of       772\t|\tloss: 0.803255\n",
      "Training Epoch 5  87.2% | batch:       673 of       772\t|\tloss: 0.723606\n",
      "Training Epoch 5  87.3% | batch:       674 of       772\t|\tloss: 0.848279\n",
      "Training Epoch 5  87.4% | batch:       675 of       772\t|\tloss: 0.876431\n",
      "Training Epoch 5  87.6% | batch:       676 of       772\t|\tloss: 0.472345\n",
      "Training Epoch 5  87.7% | batch:       677 of       772\t|\tloss: 0.579404\n",
      "Training Epoch 5  87.8% | batch:       678 of       772\t|\tloss: 0.959724\n",
      "Training Epoch 5  88.0% | batch:       679 of       772\t|\tloss: 1.03461\n",
      "Training Epoch 5  88.1% | batch:       680 of       772\t|\tloss: 0.753847\n",
      "Training Epoch 5  88.2% | batch:       681 of       772\t|\tloss: 0.635973\n",
      "Training Epoch 5  88.3% | batch:       682 of       772\t|\tloss: 1.26892\n",
      "Training Epoch 5  88.5% | batch:       683 of       772\t|\tloss: 0.975793\n",
      "Training Epoch 5  88.6% | batch:       684 of       772\t|\tloss: 0.640077\n",
      "Training Epoch 5  88.7% | batch:       685 of       772\t|\tloss: 1.20005\n",
      "Training Epoch 5  88.9% | batch:       686 of       772\t|\tloss: 1.14718\n",
      "Training Epoch 5  89.0% | batch:       687 of       772\t|\tloss: 0.566434\n",
      "Training Epoch 5  89.1% | batch:       688 of       772\t|\tloss: 0.658524\n",
      "Training Epoch 5  89.2% | batch:       689 of       772\t|\tloss: 1.05366\n",
      "Training Epoch 5  89.4% | batch:       690 of       772\t|\tloss: 0.915337\n",
      "Training Epoch 5  89.5% | batch:       691 of       772\t|\tloss: 0.86745\n",
      "Training Epoch 5  89.6% | batch:       692 of       772\t|\tloss: 0.720259\n",
      "Training Epoch 5  89.8% | batch:       693 of       772\t|\tloss: 0.87467\n",
      "Training Epoch 5  89.9% | batch:       694 of       772\t|\tloss: 0.748271\n",
      "Training Epoch 5  90.0% | batch:       695 of       772\t|\tloss: 1.12593\n",
      "Training Epoch 5  90.2% | batch:       696 of       772\t|\tloss: 0.930598\n",
      "Training Epoch 5  90.3% | batch:       697 of       772\t|\tloss: 0.807998\n",
      "Training Epoch 5  90.4% | batch:       698 of       772\t|\tloss: 0.959129\n",
      "Training Epoch 5  90.5% | batch:       699 of       772\t|\tloss: 0.821059\n",
      "Training Epoch 5  90.7% | batch:       700 of       772\t|\tloss: 0.64884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  90.8% | batch:       701 of       772\t|\tloss: 0.578992\n",
      "Training Epoch 5  90.9% | batch:       702 of       772\t|\tloss: 0.718801\n",
      "Training Epoch 5  91.1% | batch:       703 of       772\t|\tloss: 0.57888\n",
      "Training Epoch 5  91.2% | batch:       704 of       772\t|\tloss: 0.873718\n",
      "Training Epoch 5  91.3% | batch:       705 of       772\t|\tloss: 0.841595\n",
      "Training Epoch 5  91.5% | batch:       706 of       772\t|\tloss: 0.733923\n",
      "Training Epoch 5  91.6% | batch:       707 of       772\t|\tloss: 0.830453\n",
      "Training Epoch 5  91.7% | batch:       708 of       772\t|\tloss: 0.803687\n",
      "Training Epoch 5  91.8% | batch:       709 of       772\t|\tloss: 1.24507\n",
      "Training Epoch 5  92.0% | batch:       710 of       772\t|\tloss: 0.864974\n",
      "Training Epoch 5  92.1% | batch:       711 of       772\t|\tloss: 0.95383\n",
      "Training Epoch 5  92.2% | batch:       712 of       772\t|\tloss: 0.736252\n",
      "Training Epoch 5  92.4% | batch:       713 of       772\t|\tloss: 0.934716\n",
      "Training Epoch 5  92.5% | batch:       714 of       772\t|\tloss: 0.979896\n",
      "Training Epoch 5  92.6% | batch:       715 of       772\t|\tloss: 0.8438\n",
      "Training Epoch 5  92.7% | batch:       716 of       772\t|\tloss: 0.728238\n",
      "Training Epoch 5  92.9% | batch:       717 of       772\t|\tloss: 1.05947\n",
      "Training Epoch 5  93.0% | batch:       718 of       772\t|\tloss: 0.989971\n",
      "Training Epoch 5  93.1% | batch:       719 of       772\t|\tloss: 0.819967\n",
      "Training Epoch 5  93.3% | batch:       720 of       772\t|\tloss: 0.795944\n",
      "Training Epoch 5  93.4% | batch:       721 of       772\t|\tloss: 0.908186\n",
      "Training Epoch 5  93.5% | batch:       722 of       772\t|\tloss: 1.87331\n",
      "Training Epoch 5  93.7% | batch:       723 of       772\t|\tloss: 0.892409\n",
      "Training Epoch 5  93.8% | batch:       724 of       772\t|\tloss: 0.708111\n",
      "Training Epoch 5  93.9% | batch:       725 of       772\t|\tloss: 0.642464\n",
      "Training Epoch 5  94.0% | batch:       726 of       772\t|\tloss: 0.777549\n",
      "Training Epoch 5  94.2% | batch:       727 of       772\t|\tloss: 1.20831\n",
      "Training Epoch 5  94.3% | batch:       728 of       772\t|\tloss: 0.855973\n",
      "Training Epoch 5  94.4% | batch:       729 of       772\t|\tloss: 1.06935\n",
      "Training Epoch 5  94.6% | batch:       730 of       772\t|\tloss: 1.09024\n",
      "Training Epoch 5  94.7% | batch:       731 of       772\t|\tloss: 1.08709\n",
      "Training Epoch 5  94.8% | batch:       732 of       772\t|\tloss: 0.871814\n",
      "Training Epoch 5  94.9% | batch:       733 of       772\t|\tloss: 0.929625\n",
      "Training Epoch 5  95.1% | batch:       734 of       772\t|\tloss: 1.20133\n",
      "Training Epoch 5  95.2% | batch:       735 of       772\t|\tloss: 1.1114\n",
      "Training Epoch 5  95.3% | batch:       736 of       772\t|\tloss: 0.953391\n",
      "Training Epoch 5  95.5% | batch:       737 of       772\t|\tloss: 0.831262\n",
      "Training Epoch 5  95.6% | batch:       738 of       772\t|\tloss: 1.32299\n",
      "Training Epoch 5  95.7% | batch:       739 of       772\t|\tloss: 1.04507\n",
      "Training Epoch 5  95.9% | batch:       740 of       772\t|\tloss: 0.842368\n",
      "Training Epoch 5  96.0% | batch:       741 of       772\t|\tloss: 1.14423\n",
      "Training Epoch 5  96.1% | batch:       742 of       772\t|\tloss: 1.34587\n",
      "Training Epoch 5  96.2% | batch:       743 of       772\t|\tloss: 1.06922\n",
      "Training Epoch 5  96.4% | batch:       744 of       772\t|\tloss: 0.833034\n",
      "Training Epoch 5  96.5% | batch:       745 of       772\t|\tloss: 0.892896\n",
      "Training Epoch 5  96.6% | batch:       746 of       772\t|\tloss: 1.10951\n",
      "Training Epoch 5  96.8% | batch:       747 of       772\t|\tloss: 0.852175\n",
      "Training Epoch 5  96.9% | batch:       748 of       772\t|\tloss: 0.890443\n",
      "Training Epoch 5  97.0% | batch:       749 of       772\t|\tloss: 0.958715\n",
      "Training Epoch 5  97.2% | batch:       750 of       772\t|\tloss: 1.15473\n",
      "Training Epoch 5  97.3% | batch:       751 of       772\t|\tloss: 0.84766\n",
      "Training Epoch 5  97.4% | batch:       752 of       772\t|\tloss: 0.693421\n",
      "Training Epoch 5  97.5% | batch:       753 of       772\t|\tloss: 0.906233\n",
      "Training Epoch 5  97.7% | batch:       754 of       772\t|\tloss: 0.616094\n",
      "Training Epoch 5  97.8% | batch:       755 of       772\t|\tloss: 0.637374\n",
      "Training Epoch 5  97.9% | batch:       756 of       772\t|\tloss: 0.730311\n",
      "Training Epoch 5  98.1% | batch:       757 of       772\t|\tloss: 0.881986\n",
      "Training Epoch 5  98.2% | batch:       758 of       772\t|\tloss: 1.22095\n",
      "Training Epoch 5  98.3% | batch:       759 of       772\t|\tloss: 0.587684\n",
      "Training Epoch 5  98.4% | batch:       760 of       772\t|\tloss: 0.728228\n",
      "Training Epoch 5  98.6% | batch:       761 of       772\t|\tloss: 0.780138\n",
      "Training Epoch 5  98.7% | batch:       762 of       772\t|\tloss: 1.06824\n",
      "Training Epoch 5  98.8% | batch:       763 of       772\t|\tloss: 0.988949\n",
      "Training Epoch 5  99.0% | batch:       764 of       772\t|\tloss: 0.811003\n",
      "Training Epoch 5  99.1% | batch:       765 of       772\t|\tloss: 0.927005\n",
      "Training Epoch 5  99.2% | batch:       766 of       772\t|\tloss: 1.06856\n",
      "Training Epoch 5  99.4% | batch:       767 of       772\t|\tloss: 0.888083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:21:37,767 | INFO : Epoch 5 Training Summary: epoch: 5.000000 | loss: 0.933537 | \n",
      "2023-05-24 10:21:37,767 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 17.33598232269287 seconds\n",
      "\n",
      "2023-05-24 10:21:37,769 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 17.001683521270753 seconds\n",
      "2023-05-24 10:21:37,770 | INFO : Avg batch train. time: 0.02202290611563569 seconds\n",
      "2023-05-24 10:21:37,770 | INFO : Avg sample train. time: 0.00017205743640851247 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  99.5% | batch:       768 of       772\t|\tloss: 1.05472\n",
      "Training Epoch 5  99.6% | batch:       769 of       772\t|\tloss: 0.841129\n",
      "Training Epoch 5  99.7% | batch:       770 of       772\t|\tloss: 0.722769\n",
      "Training Epoch 5  99.9% | batch:       771 of       772\t|\tloss: 1.01456\n",
      "\n",
      "Training Epoch 6   0.0% | batch:         0 of       772\t|\tloss: 0.842387\n",
      "Training Epoch 6   0.1% | batch:         1 of       772\t|\tloss: 1.15893\n",
      "Training Epoch 6   0.3% | batch:         2 of       772\t|\tloss: 0.766416\n",
      "Training Epoch 6   0.4% | batch:         3 of       772\t|\tloss: 1.27773\n",
      "Training Epoch 6   0.5% | batch:         4 of       772\t|\tloss: 1.20262\n",
      "Training Epoch 6   0.6% | batch:         5 of       772\t|\tloss: 1.13986\n",
      "Training Epoch 6   0.8% | batch:         6 of       772\t|\tloss: 0.671818\n",
      "Training Epoch 6   0.9% | batch:         7 of       772\t|\tloss: 0.714985\n",
      "Training Epoch 6   1.0% | batch:         8 of       772\t|\tloss: 0.904207\n",
      "Training Epoch 6   1.2% | batch:         9 of       772\t|\tloss: 0.864134\n",
      "Training Epoch 6   1.3% | batch:        10 of       772\t|\tloss: 0.758187\n",
      "Training Epoch 6   1.4% | batch:        11 of       772\t|\tloss: 1.35502\n",
      "Training Epoch 6   1.6% | batch:        12 of       772\t|\tloss: 1.40924\n",
      "Training Epoch 6   1.7% | batch:        13 of       772\t|\tloss: 0.984543\n",
      "Training Epoch 6   1.8% | batch:        14 of       772\t|\tloss: 0.775479\n",
      "Training Epoch 6   1.9% | batch:        15 of       772\t|\tloss: 0.679877\n",
      "Training Epoch 6   2.1% | batch:        16 of       772\t|\tloss: 0.708511\n",
      "Training Epoch 6   2.2% | batch:        17 of       772\t|\tloss: 0.723681\n",
      "Training Epoch 6   2.3% | batch:        18 of       772\t|\tloss: 0.649375\n",
      "Training Epoch 6   2.5% | batch:        19 of       772\t|\tloss: 0.969935\n",
      "Training Epoch 6   2.6% | batch:        20 of       772\t|\tloss: 0.725281\n",
      "Training Epoch 6   2.7% | batch:        21 of       772\t|\tloss: 0.589496\n",
      "Training Epoch 6   2.8% | batch:        22 of       772\t|\tloss: 0.647344\n",
      "Training Epoch 6   3.0% | batch:        23 of       772\t|\tloss: 0.704804\n",
      "Training Epoch 6   3.1% | batch:        24 of       772\t|\tloss: 0.850107\n",
      "Training Epoch 6   3.2% | batch:        25 of       772\t|\tloss: 0.924268\n",
      "Training Epoch 6   3.4% | batch:        26 of       772\t|\tloss: 0.627463\n",
      "Training Epoch 6   3.5% | batch:        27 of       772\t|\tloss: 0.553494\n",
      "Training Epoch 6   3.6% | batch:        28 of       772\t|\tloss: 1.18789\n",
      "Training Epoch 6   3.8% | batch:        29 of       772\t|\tloss: 1.47071\n",
      "Training Epoch 6   3.9% | batch:        30 of       772\t|\tloss: 1.08615\n",
      "Training Epoch 6   4.0% | batch:        31 of       772\t|\tloss: 1.01189\n",
      "Training Epoch 6   4.1% | batch:        32 of       772\t|\tloss: 0.871916\n",
      "Training Epoch 6   4.3% | batch:        33 of       772\t|\tloss: 0.686616\n",
      "Training Epoch 6   4.4% | batch:        34 of       772\t|\tloss: 0.855574\n",
      "Training Epoch 6   4.5% | batch:        35 of       772\t|\tloss: 0.589316\n",
      "Training Epoch 6   4.7% | batch:        36 of       772\t|\tloss: 0.933667\n",
      "Training Epoch 6   4.8% | batch:        37 of       772\t|\tloss: 0.586354\n",
      "Training Epoch 6   4.9% | batch:        38 of       772\t|\tloss: 0.535991\n",
      "Training Epoch 6   5.1% | batch:        39 of       772\t|\tloss: 0.780998\n",
      "Training Epoch 6   5.2% | batch:        40 of       772\t|\tloss: 0.939482\n",
      "Training Epoch 6   5.3% | batch:        41 of       772\t|\tloss: 0.767476\n",
      "Training Epoch 6   5.4% | batch:        42 of       772\t|\tloss: 0.788287\n",
      "Training Epoch 6   5.6% | batch:        43 of       772\t|\tloss: 0.759701\n",
      "Training Epoch 6   5.7% | batch:        44 of       772\t|\tloss: 0.78787\n",
      "Training Epoch 6   5.8% | batch:        45 of       772\t|\tloss: 0.560221\n",
      "Training Epoch 6   6.0% | batch:        46 of       772\t|\tloss: 0.789733\n",
      "Training Epoch 6   6.1% | batch:        47 of       772\t|\tloss: 0.79067\n",
      "Training Epoch 6   6.2% | batch:        48 of       772\t|\tloss: 0.767779\n",
      "Training Epoch 6   6.3% | batch:        49 of       772\t|\tloss: 0.819585\n",
      "Training Epoch 6   6.5% | batch:        50 of       772\t|\tloss: 0.961516\n",
      "Training Epoch 6   6.6% | batch:        51 of       772\t|\tloss: 0.569107\n",
      "Training Epoch 6   6.7% | batch:        52 of       772\t|\tloss: 1.07615\n",
      "Training Epoch 6   6.9% | batch:        53 of       772\t|\tloss: 1.06528\n",
      "Training Epoch 6   7.0% | batch:        54 of       772\t|\tloss: 0.842038\n",
      "Training Epoch 6   7.1% | batch:        55 of       772\t|\tloss: 0.835718\n",
      "Training Epoch 6   7.3% | batch:        56 of       772\t|\tloss: 0.722099\n",
      "Training Epoch 6   7.4% | batch:        57 of       772\t|\tloss: 0.799985\n",
      "Training Epoch 6   7.5% | batch:        58 of       772\t|\tloss: 0.726183\n",
      "Training Epoch 6   7.6% | batch:        59 of       772\t|\tloss: 0.657078\n",
      "Training Epoch 6   7.8% | batch:        60 of       772\t|\tloss: 0.710481\n",
      "Training Epoch 6   7.9% | batch:        61 of       772\t|\tloss: 1.27576\n",
      "Training Epoch 6   8.0% | batch:        62 of       772\t|\tloss: 0.607705\n",
      "Training Epoch 6   8.2% | batch:        63 of       772\t|\tloss: 0.805745\n",
      "Training Epoch 6   8.3% | batch:        64 of       772\t|\tloss: 0.864711\n",
      "Training Epoch 6   8.4% | batch:        65 of       772\t|\tloss: 0.725555\n",
      "Training Epoch 6   8.5% | batch:        66 of       772\t|\tloss: 0.627231\n",
      "Training Epoch 6   8.7% | batch:        67 of       772\t|\tloss: 0.800103\n",
      "Training Epoch 6   8.8% | batch:        68 of       772\t|\tloss: 0.940716\n",
      "Training Epoch 6   8.9% | batch:        69 of       772\t|\tloss: 0.87178\n",
      "Training Epoch 6   9.1% | batch:        70 of       772\t|\tloss: 0.828297\n",
      "Training Epoch 6   9.2% | batch:        71 of       772\t|\tloss: 0.923745\n",
      "Training Epoch 6   9.3% | batch:        72 of       772\t|\tloss: 0.934877\n",
      "Training Epoch 6   9.5% | batch:        73 of       772\t|\tloss: 1.08321\n",
      "Training Epoch 6   9.6% | batch:        74 of       772\t|\tloss: 0.753749\n",
      "Training Epoch 6   9.7% | batch:        75 of       772\t|\tloss: 1.1908\n",
      "Training Epoch 6   9.8% | batch:        76 of       772\t|\tloss: 1.25224\n",
      "Training Epoch 6  10.0% | batch:        77 of       772\t|\tloss: 0.847126\n",
      "Training Epoch 6  10.1% | batch:        78 of       772\t|\tloss: 0.603209\n",
      "Training Epoch 6  10.2% | batch:        79 of       772\t|\tloss: 0.798613\n",
      "Training Epoch 6  10.4% | batch:        80 of       772\t|\tloss: 0.562528\n",
      "Training Epoch 6  10.5% | batch:        81 of       772\t|\tloss: 0.737993\n",
      "Training Epoch 6  10.6% | batch:        82 of       772\t|\tloss: 1.18866\n",
      "Training Epoch 6  10.8% | batch:        83 of       772\t|\tloss: 0.627963\n",
      "Training Epoch 6  10.9% | batch:        84 of       772\t|\tloss: 0.733212\n",
      "Training Epoch 6  11.0% | batch:        85 of       772\t|\tloss: 0.986723\n",
      "Training Epoch 6  11.1% | batch:        86 of       772\t|\tloss: 0.799773\n",
      "Training Epoch 6  11.3% | batch:        87 of       772\t|\tloss: 0.713927\n",
      "Training Epoch 6  11.4% | batch:        88 of       772\t|\tloss: 0.729069\n",
      "Training Epoch 6  11.5% | batch:        89 of       772\t|\tloss: 1.16435\n",
      "Training Epoch 6  11.7% | batch:        90 of       772\t|\tloss: 0.71326\n",
      "Training Epoch 6  11.8% | batch:        91 of       772\t|\tloss: 0.87546\n",
      "Training Epoch 6  11.9% | batch:        92 of       772\t|\tloss: 0.652619\n",
      "Training Epoch 6  12.0% | batch:        93 of       772\t|\tloss: 0.888108\n",
      "Training Epoch 6  12.2% | batch:        94 of       772\t|\tloss: 0.710361\n",
      "Training Epoch 6  12.3% | batch:        95 of       772\t|\tloss: 0.816511\n",
      "Training Epoch 6  12.4% | batch:        96 of       772\t|\tloss: 0.659258\n",
      "Training Epoch 6  12.6% | batch:        97 of       772\t|\tloss: 0.854266\n",
      "Training Epoch 6  12.7% | batch:        98 of       772\t|\tloss: 0.671097\n",
      "Training Epoch 6  12.8% | batch:        99 of       772\t|\tloss: 0.805231\n",
      "Training Epoch 6  13.0% | batch:       100 of       772\t|\tloss: 0.899662\n",
      "Training Epoch 6  13.1% | batch:       101 of       772\t|\tloss: 0.604919\n",
      "Training Epoch 6  13.2% | batch:       102 of       772\t|\tloss: 0.626503\n",
      "Training Epoch 6  13.3% | batch:       103 of       772\t|\tloss: 1.04673\n",
      "Training Epoch 6  13.5% | batch:       104 of       772\t|\tloss: 1.05166\n",
      "Training Epoch 6  13.6% | batch:       105 of       772\t|\tloss: 0.636185\n",
      "Training Epoch 6  13.7% | batch:       106 of       772\t|\tloss: 0.757425\n",
      "Training Epoch 6  13.9% | batch:       107 of       772\t|\tloss: 0.735974\n",
      "Training Epoch 6  14.0% | batch:       108 of       772\t|\tloss: 0.664367\n",
      "Training Epoch 6  14.1% | batch:       109 of       772\t|\tloss: 0.715703\n",
      "Training Epoch 6  14.2% | batch:       110 of       772\t|\tloss: 0.591218\n",
      "Training Epoch 6  14.4% | batch:       111 of       772\t|\tloss: 0.715289\n",
      "Training Epoch 6  14.5% | batch:       112 of       772\t|\tloss: 0.908916\n",
      "Training Epoch 6  14.6% | batch:       113 of       772\t|\tloss: 0.895563\n",
      "Training Epoch 6  14.8% | batch:       114 of       772\t|\tloss: 0.962761\n",
      "Training Epoch 6  14.9% | batch:       115 of       772\t|\tloss: 1.4619\n",
      "Training Epoch 6  15.0% | batch:       116 of       772\t|\tloss: 0.861659\n",
      "Training Epoch 6  15.2% | batch:       117 of       772\t|\tloss: 0.616838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  15.3% | batch:       118 of       772\t|\tloss: 0.65326\n",
      "Training Epoch 6  15.4% | batch:       119 of       772\t|\tloss: 0.679307\n",
      "Training Epoch 6  15.5% | batch:       120 of       772\t|\tloss: 0.725436\n",
      "Training Epoch 6  15.7% | batch:       121 of       772\t|\tloss: 0.848112\n",
      "Training Epoch 6  15.8% | batch:       122 of       772\t|\tloss: 1.04519\n",
      "Training Epoch 6  15.9% | batch:       123 of       772\t|\tloss: 0.63984\n",
      "Training Epoch 6  16.1% | batch:       124 of       772\t|\tloss: 0.773808\n",
      "Training Epoch 6  16.2% | batch:       125 of       772\t|\tloss: 0.66049\n",
      "Training Epoch 6  16.3% | batch:       126 of       772\t|\tloss: 0.643979\n",
      "Training Epoch 6  16.5% | batch:       127 of       772\t|\tloss: 0.759584\n",
      "Training Epoch 6  16.6% | batch:       128 of       772\t|\tloss: 0.678616\n",
      "Training Epoch 6  16.7% | batch:       129 of       772\t|\tloss: 0.868953\n",
      "Training Epoch 6  16.8% | batch:       130 of       772\t|\tloss: 1.05285\n",
      "Training Epoch 6  17.0% | batch:       131 of       772\t|\tloss: 0.762748\n",
      "Training Epoch 6  17.1% | batch:       132 of       772\t|\tloss: 0.460307\n",
      "Training Epoch 6  17.2% | batch:       133 of       772\t|\tloss: 0.771146\n",
      "Training Epoch 6  17.4% | batch:       134 of       772\t|\tloss: 0.613959\n",
      "Training Epoch 6  17.5% | batch:       135 of       772\t|\tloss: 0.716914\n",
      "Training Epoch 6  17.6% | batch:       136 of       772\t|\tloss: 0.814128\n",
      "Training Epoch 6  17.7% | batch:       137 of       772\t|\tloss: 1.17198\n",
      "Training Epoch 6  17.9% | batch:       138 of       772\t|\tloss: 0.990207\n",
      "Training Epoch 6  18.0% | batch:       139 of       772\t|\tloss: 0.643127\n",
      "Training Epoch 6  18.1% | batch:       140 of       772\t|\tloss: 1.00722\n",
      "Training Epoch 6  18.3% | batch:       141 of       772\t|\tloss: 0.841429\n",
      "Training Epoch 6  18.4% | batch:       142 of       772\t|\tloss: 0.630052\n",
      "Training Epoch 6  18.5% | batch:       143 of       772\t|\tloss: 0.631955\n",
      "Training Epoch 6  18.7% | batch:       144 of       772\t|\tloss: 0.939493\n",
      "Training Epoch 6  18.8% | batch:       145 of       772\t|\tloss: 0.714608\n",
      "Training Epoch 6  18.9% | batch:       146 of       772\t|\tloss: 0.611534\n",
      "Training Epoch 6  19.0% | batch:       147 of       772\t|\tloss: 0.798271\n",
      "Training Epoch 6  19.2% | batch:       148 of       772\t|\tloss: 0.955705\n",
      "Training Epoch 6  19.3% | batch:       149 of       772\t|\tloss: 1.0854\n",
      "Training Epoch 6  19.4% | batch:       150 of       772\t|\tloss: 1.0596\n",
      "Training Epoch 6  19.6% | batch:       151 of       772\t|\tloss: 1.30824\n",
      "Training Epoch 6  19.7% | batch:       152 of       772\t|\tloss: 1.18106\n",
      "Training Epoch 6  19.8% | batch:       153 of       772\t|\tloss: 1.02645\n",
      "Training Epoch 6  19.9% | batch:       154 of       772\t|\tloss: 0.8753\n",
      "Training Epoch 6  20.1% | batch:       155 of       772\t|\tloss: 2.06792\n",
      "Training Epoch 6  20.2% | batch:       156 of       772\t|\tloss: 1.1263\n",
      "Training Epoch 6  20.3% | batch:       157 of       772\t|\tloss: 1.12002\n",
      "Training Epoch 6  20.5% | batch:       158 of       772\t|\tloss: 0.688593\n",
      "Training Epoch 6  20.6% | batch:       159 of       772\t|\tloss: 0.755699\n",
      "Training Epoch 6  20.7% | batch:       160 of       772\t|\tloss: 0.69644\n",
      "Training Epoch 6  20.9% | batch:       161 of       772\t|\tloss: 0.962576\n",
      "Training Epoch 6  21.0% | batch:       162 of       772\t|\tloss: 0.517011\n",
      "Training Epoch 6  21.1% | batch:       163 of       772\t|\tloss: 0.589982\n",
      "Training Epoch 6  21.2% | batch:       164 of       772\t|\tloss: 0.623466\n",
      "Training Epoch 6  21.4% | batch:       165 of       772\t|\tloss: 0.710468\n",
      "Training Epoch 6  21.5% | batch:       166 of       772\t|\tloss: 0.793957\n",
      "Training Epoch 6  21.6% | batch:       167 of       772\t|\tloss: 0.724181\n",
      "Training Epoch 6  21.8% | batch:       168 of       772\t|\tloss: 0.659665\n",
      "Training Epoch 6  21.9% | batch:       169 of       772\t|\tloss: 0.612044\n",
      "Training Epoch 6  22.0% | batch:       170 of       772\t|\tloss: 0.613253\n",
      "Training Epoch 6  22.2% | batch:       171 of       772\t|\tloss: 0.775983\n",
      "Training Epoch 6  22.3% | batch:       172 of       772\t|\tloss: 0.734864\n",
      "Training Epoch 6  22.4% | batch:       173 of       772\t|\tloss: 0.634796\n",
      "Training Epoch 6  22.5% | batch:       174 of       772\t|\tloss: 0.486605\n",
      "Training Epoch 6  22.7% | batch:       175 of       772\t|\tloss: 0.85354\n",
      "Training Epoch 6  22.8% | batch:       176 of       772\t|\tloss: 0.495164\n",
      "Training Epoch 6  22.9% | batch:       177 of       772\t|\tloss: 0.807196\n",
      "Training Epoch 6  23.1% | batch:       178 of       772\t|\tloss: 1.3232\n",
      "Training Epoch 6  23.2% | batch:       179 of       772\t|\tloss: 1.37763\n",
      "Training Epoch 6  23.3% | batch:       180 of       772\t|\tloss: 1.00256\n",
      "Training Epoch 6  23.4% | batch:       181 of       772\t|\tloss: 0.831246\n",
      "Training Epoch 6  23.6% | batch:       182 of       772\t|\tloss: 1.41522\n",
      "Training Epoch 6  23.7% | batch:       183 of       772\t|\tloss: 1.08187\n",
      "Training Epoch 6  23.8% | batch:       184 of       772\t|\tloss: 0.982296\n",
      "Training Epoch 6  24.0% | batch:       185 of       772\t|\tloss: 0.666267\n",
      "Training Epoch 6  24.1% | batch:       186 of       772\t|\tloss: 1.20154\n",
      "Training Epoch 6  24.2% | batch:       187 of       772\t|\tloss: 1.26197\n",
      "Training Epoch 6  24.4% | batch:       188 of       772\t|\tloss: 0.763137\n",
      "Training Epoch 6  24.5% | batch:       189 of       772\t|\tloss: 0.717243\n",
      "Training Epoch 6  24.6% | batch:       190 of       772\t|\tloss: 1.00549\n",
      "Training Epoch 6  24.7% | batch:       191 of       772\t|\tloss: 0.691169\n",
      "Training Epoch 6  24.9% | batch:       192 of       772\t|\tloss: 0.862022\n",
      "Training Epoch 6  25.0% | batch:       193 of       772\t|\tloss: 0.705085\n",
      "Training Epoch 6  25.1% | batch:       194 of       772\t|\tloss: 0.959599\n",
      "Training Epoch 6  25.3% | batch:       195 of       772\t|\tloss: 0.76514\n",
      "Training Epoch 6  25.4% | batch:       196 of       772\t|\tloss: 1.04712\n",
      "Training Epoch 6  25.5% | batch:       197 of       772\t|\tloss: 1.33317\n",
      "Training Epoch 6  25.6% | batch:       198 of       772\t|\tloss: 0.813648\n",
      "Training Epoch 6  25.8% | batch:       199 of       772\t|\tloss: 1.19194\n",
      "Training Epoch 6  25.9% | batch:       200 of       772\t|\tloss: 0.734626\n",
      "Training Epoch 6  26.0% | batch:       201 of       772\t|\tloss: 0.817372\n",
      "Training Epoch 6  26.2% | batch:       202 of       772\t|\tloss: 0.689651\n",
      "Training Epoch 6  26.3% | batch:       203 of       772\t|\tloss: 0.706776\n",
      "Training Epoch 6  26.4% | batch:       204 of       772\t|\tloss: 0.763444\n",
      "Training Epoch 6  26.6% | batch:       205 of       772\t|\tloss: 0.949055\n",
      "Training Epoch 6  26.7% | batch:       206 of       772\t|\tloss: 1.08343\n",
      "Training Epoch 6  26.8% | batch:       207 of       772\t|\tloss: 0.808837\n",
      "Training Epoch 6  26.9% | batch:       208 of       772\t|\tloss: 0.831908\n",
      "Training Epoch 6  27.1% | batch:       209 of       772\t|\tloss: 0.866135\n",
      "Training Epoch 6  27.2% | batch:       210 of       772\t|\tloss: 0.569558\n",
      "Training Epoch 6  27.3% | batch:       211 of       772\t|\tloss: 0.84468\n",
      "Training Epoch 6  27.5% | batch:       212 of       772\t|\tloss: 1.55624\n",
      "Training Epoch 6  27.6% | batch:       213 of       772\t|\tloss: 1.05553\n",
      "Training Epoch 6  27.7% | batch:       214 of       772\t|\tloss: 1.04115\n",
      "Training Epoch 6  27.8% | batch:       215 of       772\t|\tloss: 0.751313\n",
      "Training Epoch 6  28.0% | batch:       216 of       772\t|\tloss: 0.731665\n",
      "Training Epoch 6  28.1% | batch:       217 of       772\t|\tloss: 0.846024\n",
      "Training Epoch 6  28.2% | batch:       218 of       772\t|\tloss: 0.852767\n",
      "Training Epoch 6  28.4% | batch:       219 of       772\t|\tloss: 1.22414\n",
      "Training Epoch 6  28.5% | batch:       220 of       772\t|\tloss: 0.943994\n",
      "Training Epoch 6  28.6% | batch:       221 of       772\t|\tloss: 0.802135\n",
      "Training Epoch 6  28.8% | batch:       222 of       772\t|\tloss: 0.839039\n",
      "Training Epoch 6  28.9% | batch:       223 of       772\t|\tloss: 0.636799\n",
      "Training Epoch 6  29.0% | batch:       224 of       772\t|\tloss: 0.64968\n",
      "Training Epoch 6  29.1% | batch:       225 of       772\t|\tloss: 0.74338\n",
      "Training Epoch 6  29.3% | batch:       226 of       772\t|\tloss: 1.38482\n",
      "Training Epoch 6  29.4% | batch:       227 of       772\t|\tloss: 0.764942\n",
      "Training Epoch 6  29.5% | batch:       228 of       772\t|\tloss: 0.84702\n",
      "Training Epoch 6  29.7% | batch:       229 of       772\t|\tloss: 0.837883\n",
      "Training Epoch 6  29.8% | batch:       230 of       772\t|\tloss: 1.1775\n",
      "Training Epoch 6  29.9% | batch:       231 of       772\t|\tloss: 0.609386\n",
      "Training Epoch 6  30.1% | batch:       232 of       772\t|\tloss: 0.715563\n",
      "Training Epoch 6  30.2% | batch:       233 of       772\t|\tloss: 0.990989\n",
      "Training Epoch 6  30.3% | batch:       234 of       772\t|\tloss: 0.995033\n",
      "Training Epoch 6  30.4% | batch:       235 of       772\t|\tloss: 0.801506\n",
      "Training Epoch 6  30.6% | batch:       236 of       772\t|\tloss: 0.952292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  30.7% | batch:       237 of       772\t|\tloss: 0.995007\n",
      "Training Epoch 6  30.8% | batch:       238 of       772\t|\tloss: 1.1931\n",
      "Training Epoch 6  31.0% | batch:       239 of       772\t|\tloss: 1.03116\n",
      "Training Epoch 6  31.1% | batch:       240 of       772\t|\tloss: 0.985652\n",
      "Training Epoch 6  31.2% | batch:       241 of       772\t|\tloss: 1.18777\n",
      "Training Epoch 6  31.3% | batch:       242 of       772\t|\tloss: 1.27186\n",
      "Training Epoch 6  31.5% | batch:       243 of       772\t|\tloss: 0.678713\n",
      "Training Epoch 6  31.6% | batch:       244 of       772\t|\tloss: 0.779344\n",
      "Training Epoch 6  31.7% | batch:       245 of       772\t|\tloss: 0.83069\n",
      "Training Epoch 6  31.9% | batch:       246 of       772\t|\tloss: 0.926451\n",
      "Training Epoch 6  32.0% | batch:       247 of       772\t|\tloss: 1.05286\n",
      "Training Epoch 6  32.1% | batch:       248 of       772\t|\tloss: 0.880777\n",
      "Training Epoch 6  32.3% | batch:       249 of       772\t|\tloss: 0.669432\n",
      "Training Epoch 6  32.4% | batch:       250 of       772\t|\tloss: 1.28015\n",
      "Training Epoch 6  32.5% | batch:       251 of       772\t|\tloss: 0.844539\n",
      "Training Epoch 6  32.6% | batch:       252 of       772\t|\tloss: 0.848364\n",
      "Training Epoch 6  32.8% | batch:       253 of       772\t|\tloss: 0.769466\n",
      "Training Epoch 6  32.9% | batch:       254 of       772\t|\tloss: 0.500297\n",
      "Training Epoch 6  33.0% | batch:       255 of       772\t|\tloss: 0.697158\n",
      "Training Epoch 6  33.2% | batch:       256 of       772\t|\tloss: 0.726373\n",
      "Training Epoch 6  33.3% | batch:       257 of       772\t|\tloss: 1.0942\n",
      "Training Epoch 6  33.4% | batch:       258 of       772\t|\tloss: 0.658599\n",
      "Training Epoch 6  33.5% | batch:       259 of       772\t|\tloss: 0.782777\n",
      "Training Epoch 6  33.7% | batch:       260 of       772\t|\tloss: 1.01625\n",
      "Training Epoch 6  33.8% | batch:       261 of       772\t|\tloss: 0.808089\n",
      "Training Epoch 6  33.9% | batch:       262 of       772\t|\tloss: 0.935034\n",
      "Training Epoch 6  34.1% | batch:       263 of       772\t|\tloss: 0.885694\n",
      "Training Epoch 6  34.2% | batch:       264 of       772\t|\tloss: 0.624204\n",
      "Training Epoch 6  34.3% | batch:       265 of       772\t|\tloss: 0.8413\n",
      "Training Epoch 6  34.5% | batch:       266 of       772\t|\tloss: 0.728456\n",
      "Training Epoch 6  34.6% | batch:       267 of       772\t|\tloss: 0.718286\n",
      "Training Epoch 6  34.7% | batch:       268 of       772\t|\tloss: 0.533767\n",
      "Training Epoch 6  34.8% | batch:       269 of       772\t|\tloss: 0.755158\n",
      "Training Epoch 6  35.0% | batch:       270 of       772\t|\tloss: 0.998376\n",
      "Training Epoch 6  35.1% | batch:       271 of       772\t|\tloss: 1.33812\n",
      "Training Epoch 6  35.2% | batch:       272 of       772\t|\tloss: 1.15659\n",
      "Training Epoch 6  35.4% | batch:       273 of       772\t|\tloss: 0.939343\n",
      "Training Epoch 6  35.5% | batch:       274 of       772\t|\tloss: 1.18153\n",
      "Training Epoch 6  35.6% | batch:       275 of       772\t|\tloss: 0.857577\n",
      "Training Epoch 6  35.8% | batch:       276 of       772\t|\tloss: 0.933464\n",
      "Training Epoch 6  35.9% | batch:       277 of       772\t|\tloss: 0.637725\n",
      "Training Epoch 6  36.0% | batch:       278 of       772\t|\tloss: 0.558362\n",
      "Training Epoch 6  36.1% | batch:       279 of       772\t|\tloss: 0.764105\n",
      "Training Epoch 6  36.3% | batch:       280 of       772\t|\tloss: 0.878876\n",
      "Training Epoch 6  36.4% | batch:       281 of       772\t|\tloss: 1.03742\n",
      "Training Epoch 6  36.5% | batch:       282 of       772\t|\tloss: 0.778307\n",
      "Training Epoch 6  36.7% | batch:       283 of       772\t|\tloss: 0.794655\n",
      "Training Epoch 6  36.8% | batch:       284 of       772\t|\tloss: 0.997616\n",
      "Training Epoch 6  36.9% | batch:       285 of       772\t|\tloss: 1.04373\n",
      "Training Epoch 6  37.0% | batch:       286 of       772\t|\tloss: 0.721986\n",
      "Training Epoch 6  37.2% | batch:       287 of       772\t|\tloss: 1.03073\n",
      "Training Epoch 6  37.3% | batch:       288 of       772\t|\tloss: 1.09447\n",
      "Training Epoch 6  37.4% | batch:       289 of       772\t|\tloss: 0.682879\n",
      "Training Epoch 6  37.6% | batch:       290 of       772\t|\tloss: 0.672178\n",
      "Training Epoch 6  37.7% | batch:       291 of       772\t|\tloss: 0.970177\n",
      "Training Epoch 6  37.8% | batch:       292 of       772\t|\tloss: 0.641955\n",
      "Training Epoch 6  38.0% | batch:       293 of       772\t|\tloss: 0.738936\n",
      "Training Epoch 6  38.1% | batch:       294 of       772\t|\tloss: 0.777381\n",
      "Training Epoch 6  38.2% | batch:       295 of       772\t|\tloss: 0.660831\n",
      "Training Epoch 6  38.3% | batch:       296 of       772\t|\tloss: 0.773492\n",
      "Training Epoch 6  38.5% | batch:       297 of       772\t|\tloss: 0.734094\n",
      "Training Epoch 6  38.6% | batch:       298 of       772\t|\tloss: 0.810823\n",
      "Training Epoch 6  38.7% | batch:       299 of       772\t|\tloss: 0.582349\n",
      "Training Epoch 6  38.9% | batch:       300 of       772\t|\tloss: 0.873519\n",
      "Training Epoch 6  39.0% | batch:       301 of       772\t|\tloss: 0.594765\n",
      "Training Epoch 6  39.1% | batch:       302 of       772\t|\tloss: 0.744341\n",
      "Training Epoch 6  39.2% | batch:       303 of       772\t|\tloss: 0.903148\n",
      "Training Epoch 6  39.4% | batch:       304 of       772\t|\tloss: 0.68723\n",
      "Training Epoch 6  39.5% | batch:       305 of       772\t|\tloss: 0.683544\n",
      "Training Epoch 6  39.6% | batch:       306 of       772\t|\tloss: 0.918813\n",
      "Training Epoch 6  39.8% | batch:       307 of       772\t|\tloss: 1.06548\n",
      "Training Epoch 6  39.9% | batch:       308 of       772\t|\tloss: 0.850601\n",
      "Training Epoch 6  40.0% | batch:       309 of       772\t|\tloss: 0.817841\n",
      "Training Epoch 6  40.2% | batch:       310 of       772\t|\tloss: 0.679723\n",
      "Training Epoch 6  40.3% | batch:       311 of       772\t|\tloss: 0.558664\n",
      "Training Epoch 6  40.4% | batch:       312 of       772\t|\tloss: 0.843843\n",
      "Training Epoch 6  40.5% | batch:       313 of       772\t|\tloss: 0.889649\n",
      "Training Epoch 6  40.7% | batch:       314 of       772\t|\tloss: 1.17765\n",
      "Training Epoch 6  40.8% | batch:       315 of       772\t|\tloss: 0.767561\n",
      "Training Epoch 6  40.9% | batch:       316 of       772\t|\tloss: 0.708829\n",
      "Training Epoch 6  41.1% | batch:       317 of       772\t|\tloss: 0.498582\n",
      "Training Epoch 6  41.2% | batch:       318 of       772\t|\tloss: 0.748465\n",
      "Training Epoch 6  41.3% | batch:       319 of       772\t|\tloss: 0.978072\n",
      "Training Epoch 6  41.5% | batch:       320 of       772\t|\tloss: 1.12865\n",
      "Training Epoch 6  41.6% | batch:       321 of       772\t|\tloss: 0.957882\n",
      "Training Epoch 6  41.7% | batch:       322 of       772\t|\tloss: 0.988508\n",
      "Training Epoch 6  41.8% | batch:       323 of       772\t|\tloss: 0.767354\n",
      "Training Epoch 6  42.0% | batch:       324 of       772\t|\tloss: 0.802389\n",
      "Training Epoch 6  42.1% | batch:       325 of       772\t|\tloss: 0.894632\n",
      "Training Epoch 6  42.2% | batch:       326 of       772\t|\tloss: 0.997078\n",
      "Training Epoch 6  42.4% | batch:       327 of       772\t|\tloss: 1.16728\n",
      "Training Epoch 6  42.5% | batch:       328 of       772\t|\tloss: 0.87053\n",
      "Training Epoch 6  42.6% | batch:       329 of       772\t|\tloss: 0.568187\n",
      "Training Epoch 6  42.7% | batch:       330 of       772\t|\tloss: 0.733011\n",
      "Training Epoch 6  42.9% | batch:       331 of       772\t|\tloss: 0.848948\n",
      "Training Epoch 6  43.0% | batch:       332 of       772\t|\tloss: 0.694465\n",
      "Training Epoch 6  43.1% | batch:       333 of       772\t|\tloss: 1.09095\n",
      "Training Epoch 6  43.3% | batch:       334 of       772\t|\tloss: 0.940968\n",
      "Training Epoch 6  43.4% | batch:       335 of       772\t|\tloss: 0.845579\n",
      "Training Epoch 6  43.5% | batch:       336 of       772\t|\tloss: 0.545321\n",
      "Training Epoch 6  43.7% | batch:       337 of       772\t|\tloss: 1.01235\n",
      "Training Epoch 6  43.8% | batch:       338 of       772\t|\tloss: 0.786352\n",
      "Training Epoch 6  43.9% | batch:       339 of       772\t|\tloss: 0.711783\n",
      "Training Epoch 6  44.0% | batch:       340 of       772\t|\tloss: 0.81194\n",
      "Training Epoch 6  44.2% | batch:       341 of       772\t|\tloss: 1.03205\n",
      "Training Epoch 6  44.3% | batch:       342 of       772\t|\tloss: 0.719448\n",
      "Training Epoch 6  44.4% | batch:       343 of       772\t|\tloss: 0.833548\n",
      "Training Epoch 6  44.6% | batch:       344 of       772\t|\tloss: 0.755461\n",
      "Training Epoch 6  44.7% | batch:       345 of       772\t|\tloss: 0.811998\n",
      "Training Epoch 6  44.8% | batch:       346 of       772\t|\tloss: 0.734302\n",
      "Training Epoch 6  44.9% | batch:       347 of       772\t|\tloss: 0.693973\n",
      "Training Epoch 6  45.1% | batch:       348 of       772\t|\tloss: 0.639614\n",
      "Training Epoch 6  45.2% | batch:       349 of       772\t|\tloss: 1.16834\n",
      "Training Epoch 6  45.3% | batch:       350 of       772\t|\tloss: 1.05462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  45.5% | batch:       351 of       772\t|\tloss: 1.05859\n",
      "Training Epoch 6  45.6% | batch:       352 of       772\t|\tloss: 0.591114\n",
      "Training Epoch 6  45.7% | batch:       353 of       772\t|\tloss: 1.24197\n",
      "Training Epoch 6  45.9% | batch:       354 of       772\t|\tloss: 0.662793\n",
      "Training Epoch 6  46.0% | batch:       355 of       772\t|\tloss: 0.641657\n",
      "Training Epoch 6  46.1% | batch:       356 of       772\t|\tloss: 0.943433\n",
      "Training Epoch 6  46.2% | batch:       357 of       772\t|\tloss: 1.09221\n",
      "Training Epoch 6  46.4% | batch:       358 of       772\t|\tloss: 0.785873\n",
      "Training Epoch 6  46.5% | batch:       359 of       772\t|\tloss: 0.924407\n",
      "Training Epoch 6  46.6% | batch:       360 of       772\t|\tloss: 1.01315\n",
      "Training Epoch 6  46.8% | batch:       361 of       772\t|\tloss: 1.22714\n",
      "Training Epoch 6  46.9% | batch:       362 of       772\t|\tloss: 0.841825\n",
      "Training Epoch 6  47.0% | batch:       363 of       772\t|\tloss: 0.716592\n",
      "Training Epoch 6  47.2% | batch:       364 of       772\t|\tloss: 0.824875\n",
      "Training Epoch 6  47.3% | batch:       365 of       772\t|\tloss: 1.02182\n",
      "Training Epoch 6  47.4% | batch:       366 of       772\t|\tloss: 0.565682\n",
      "Training Epoch 6  47.5% | batch:       367 of       772\t|\tloss: 0.811957\n",
      "Training Epoch 6  47.7% | batch:       368 of       772\t|\tloss: 0.673316\n",
      "Training Epoch 6  47.8% | batch:       369 of       772\t|\tloss: 0.694979\n",
      "Training Epoch 6  47.9% | batch:       370 of       772\t|\tloss: 1.24928\n",
      "Training Epoch 6  48.1% | batch:       371 of       772\t|\tloss: 1.14928\n",
      "Training Epoch 6  48.2% | batch:       372 of       772\t|\tloss: 0.652172\n",
      "Training Epoch 6  48.3% | batch:       373 of       772\t|\tloss: 0.724235\n",
      "Training Epoch 6  48.4% | batch:       374 of       772\t|\tloss: 0.907086\n",
      "Training Epoch 6  48.6% | batch:       375 of       772\t|\tloss: 0.57977\n",
      "Training Epoch 6  48.7% | batch:       376 of       772\t|\tloss: 0.546961\n",
      "Training Epoch 6  48.8% | batch:       377 of       772\t|\tloss: 0.819647\n",
      "Training Epoch 6  49.0% | batch:       378 of       772\t|\tloss: 0.772175\n",
      "Training Epoch 6  49.1% | batch:       379 of       772\t|\tloss: 0.773472\n",
      "Training Epoch 6  49.2% | batch:       380 of       772\t|\tloss: 0.720543\n",
      "Training Epoch 6  49.4% | batch:       381 of       772\t|\tloss: 0.642952\n",
      "Training Epoch 6  49.5% | batch:       382 of       772\t|\tloss: 0.77081\n",
      "Training Epoch 6  49.6% | batch:       383 of       772\t|\tloss: 0.596583\n",
      "Training Epoch 6  49.7% | batch:       384 of       772\t|\tloss: 0.795445\n",
      "Training Epoch 6  49.9% | batch:       385 of       772\t|\tloss: 0.813535\n",
      "Training Epoch 6  50.0% | batch:       386 of       772\t|\tloss: 0.559372\n",
      "Training Epoch 6  50.1% | batch:       387 of       772\t|\tloss: 0.489629\n",
      "Training Epoch 6  50.3% | batch:       388 of       772\t|\tloss: 0.726234\n",
      "Training Epoch 6  50.4% | batch:       389 of       772\t|\tloss: 0.692581\n",
      "Training Epoch 6  50.5% | batch:       390 of       772\t|\tloss: 0.510409\n",
      "Training Epoch 6  50.6% | batch:       391 of       772\t|\tloss: 1.16986\n",
      "Training Epoch 6  50.8% | batch:       392 of       772\t|\tloss: 0.880453\n",
      "Training Epoch 6  50.9% | batch:       393 of       772\t|\tloss: 0.676944\n",
      "Training Epoch 6  51.0% | batch:       394 of       772\t|\tloss: 0.701932\n",
      "Training Epoch 6  51.2% | batch:       395 of       772\t|\tloss: 0.586438\n",
      "Training Epoch 6  51.3% | batch:       396 of       772\t|\tloss: 0.987196\n",
      "Training Epoch 6  51.4% | batch:       397 of       772\t|\tloss: 0.67455\n",
      "Training Epoch 6  51.6% | batch:       398 of       772\t|\tloss: 0.511428\n",
      "Training Epoch 6  51.7% | batch:       399 of       772\t|\tloss: 0.694749\n",
      "Training Epoch 6  51.8% | batch:       400 of       772\t|\tloss: 0.638811\n",
      "Training Epoch 6  51.9% | batch:       401 of       772\t|\tloss: 0.763807\n",
      "Training Epoch 6  52.1% | batch:       402 of       772\t|\tloss: 1.07629\n",
      "Training Epoch 6  52.2% | batch:       403 of       772\t|\tloss: 0.715783\n",
      "Training Epoch 6  52.3% | batch:       404 of       772\t|\tloss: 0.883075\n",
      "Training Epoch 6  52.5% | batch:       405 of       772\t|\tloss: 0.957805\n",
      "Training Epoch 6  52.6% | batch:       406 of       772\t|\tloss: 0.936655\n",
      "Training Epoch 6  52.7% | batch:       407 of       772\t|\tloss: 0.754968\n",
      "Training Epoch 6  52.8% | batch:       408 of       772\t|\tloss: 0.994857\n",
      "Training Epoch 6  53.0% | batch:       409 of       772\t|\tloss: 0.708621\n",
      "Training Epoch 6  53.1% | batch:       410 of       772\t|\tloss: 0.464623\n",
      "Training Epoch 6  53.2% | batch:       411 of       772\t|\tloss: 0.515151\n",
      "Training Epoch 6  53.4% | batch:       412 of       772\t|\tloss: 0.53295\n",
      "Training Epoch 6  53.5% | batch:       413 of       772\t|\tloss: 0.69099\n",
      "Training Epoch 6  53.6% | batch:       414 of       772\t|\tloss: 0.677348\n",
      "Training Epoch 6  53.8% | batch:       415 of       772\t|\tloss: 0.718579\n",
      "Training Epoch 6  53.9% | batch:       416 of       772\t|\tloss: 0.737307\n",
      "Training Epoch 6  54.0% | batch:       417 of       772\t|\tloss: 0.639263\n",
      "Training Epoch 6  54.1% | batch:       418 of       772\t|\tloss: 1.19369\n",
      "Training Epoch 6  54.3% | batch:       419 of       772\t|\tloss: 0.884466\n",
      "Training Epoch 6  54.4% | batch:       420 of       772\t|\tloss: 0.644559\n",
      "Training Epoch 6  54.5% | batch:       421 of       772\t|\tloss: 0.549343\n",
      "Training Epoch 6  54.7% | batch:       422 of       772\t|\tloss: 0.944406\n",
      "Training Epoch 6  54.8% | batch:       423 of       772\t|\tloss: 0.784611\n",
      "Training Epoch 6  54.9% | batch:       424 of       772\t|\tloss: 0.79664\n",
      "Training Epoch 6  55.1% | batch:       425 of       772\t|\tloss: 0.831492\n",
      "Training Epoch 6  55.2% | batch:       426 of       772\t|\tloss: 0.704667\n",
      "Training Epoch 6  55.3% | batch:       427 of       772\t|\tloss: 0.658029\n",
      "Training Epoch 6  55.4% | batch:       428 of       772\t|\tloss: 0.992818\n",
      "Training Epoch 6  55.6% | batch:       429 of       772\t|\tloss: 0.515093\n",
      "Training Epoch 6  55.7% | batch:       430 of       772\t|\tloss: 0.738026\n",
      "Training Epoch 6  55.8% | batch:       431 of       772\t|\tloss: 0.758458\n",
      "Training Epoch 6  56.0% | batch:       432 of       772\t|\tloss: 0.493025\n",
      "Training Epoch 6  56.1% | batch:       433 of       772\t|\tloss: 0.525242\n",
      "Training Epoch 6  56.2% | batch:       434 of       772\t|\tloss: 0.760665\n",
      "Training Epoch 6  56.3% | batch:       435 of       772\t|\tloss: 0.918503\n",
      "Training Epoch 6  56.5% | batch:       436 of       772\t|\tloss: 0.597528\n",
      "Training Epoch 6  56.6% | batch:       437 of       772\t|\tloss: 0.546112\n",
      "Training Epoch 6  56.7% | batch:       438 of       772\t|\tloss: 0.845805\n",
      "Training Epoch 6  56.9% | batch:       439 of       772\t|\tloss: 0.560305\n",
      "Training Epoch 6  57.0% | batch:       440 of       772\t|\tloss: 0.648527\n",
      "Training Epoch 6  57.1% | batch:       441 of       772\t|\tloss: 0.72345\n",
      "Training Epoch 6  57.3% | batch:       442 of       772\t|\tloss: 1.08208\n",
      "Training Epoch 6  57.4% | batch:       443 of       772\t|\tloss: 0.728296\n",
      "Training Epoch 6  57.5% | batch:       444 of       772\t|\tloss: 0.788541\n",
      "Training Epoch 6  57.6% | batch:       445 of       772\t|\tloss: 0.529393\n",
      "Training Epoch 6  57.8% | batch:       446 of       772\t|\tloss: 0.598829\n",
      "Training Epoch 6  57.9% | batch:       447 of       772\t|\tloss: 0.726516\n",
      "Training Epoch 6  58.0% | batch:       448 of       772\t|\tloss: 0.596963\n",
      "Training Epoch 6  58.2% | batch:       449 of       772\t|\tloss: 0.824151\n",
      "Training Epoch 6  58.3% | batch:       450 of       772\t|\tloss: 0.562954\n",
      "Training Epoch 6  58.4% | batch:       451 of       772\t|\tloss: 0.761365\n",
      "Training Epoch 6  58.5% | batch:       452 of       772\t|\tloss: 0.714796\n",
      "Training Epoch 6  58.7% | batch:       453 of       772\t|\tloss: 0.821886\n",
      "Training Epoch 6  58.8% | batch:       454 of       772\t|\tloss: 0.629906\n",
      "Training Epoch 6  58.9% | batch:       455 of       772\t|\tloss: 0.676504\n",
      "Training Epoch 6  59.1% | batch:       456 of       772\t|\tloss: 0.825395\n",
      "Training Epoch 6  59.2% | batch:       457 of       772\t|\tloss: 0.687894\n",
      "Training Epoch 6  59.3% | batch:       458 of       772\t|\tloss: 0.662136\n",
      "Training Epoch 6  59.5% | batch:       459 of       772\t|\tloss: 0.519001\n",
      "Training Epoch 6  59.6% | batch:       460 of       772\t|\tloss: 0.750936\n",
      "Training Epoch 6  59.7% | batch:       461 of       772\t|\tloss: 0.875563\n",
      "Training Epoch 6  59.8% | batch:       462 of       772\t|\tloss: 1.21257\n",
      "Training Epoch 6  60.0% | batch:       463 of       772\t|\tloss: 0.652769\n",
      "Training Epoch 6  60.1% | batch:       464 of       772\t|\tloss: 0.751546\n",
      "Training Epoch 6  60.2% | batch:       465 of       772\t|\tloss: 0.674335\n",
      "Training Epoch 6  60.4% | batch:       466 of       772\t|\tloss: 0.811463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  60.5% | batch:       467 of       772\t|\tloss: 1.279\n",
      "Training Epoch 6  60.6% | batch:       468 of       772\t|\tloss: 0.863338\n",
      "Training Epoch 6  60.8% | batch:       469 of       772\t|\tloss: 0.544877\n",
      "Training Epoch 6  60.9% | batch:       470 of       772\t|\tloss: 0.576302\n",
      "Training Epoch 6  61.0% | batch:       471 of       772\t|\tloss: 0.762446\n",
      "Training Epoch 6  61.1% | batch:       472 of       772\t|\tloss: 0.727555\n",
      "Training Epoch 6  61.3% | batch:       473 of       772\t|\tloss: 0.699706\n",
      "Training Epoch 6  61.4% | batch:       474 of       772\t|\tloss: 0.643268\n",
      "Training Epoch 6  61.5% | batch:       475 of       772\t|\tloss: 0.610886\n",
      "Training Epoch 6  61.7% | batch:       476 of       772\t|\tloss: 0.598895\n",
      "Training Epoch 6  61.8% | batch:       477 of       772\t|\tloss: 0.751476\n",
      "Training Epoch 6  61.9% | batch:       478 of       772\t|\tloss: 0.710716\n",
      "Training Epoch 6  62.0% | batch:       479 of       772\t|\tloss: 0.743329\n",
      "Training Epoch 6  62.2% | batch:       480 of       772\t|\tloss: 0.8303\n",
      "Training Epoch 6  62.3% | batch:       481 of       772\t|\tloss: 1.19319\n",
      "Training Epoch 6  62.4% | batch:       482 of       772\t|\tloss: 1.20064\n",
      "Training Epoch 6  62.6% | batch:       483 of       772\t|\tloss: 0.966464\n",
      "Training Epoch 6  62.7% | batch:       484 of       772\t|\tloss: 0.710878\n",
      "Training Epoch 6  62.8% | batch:       485 of       772\t|\tloss: 1.14488\n",
      "Training Epoch 6  63.0% | batch:       486 of       772\t|\tloss: 0.712769\n",
      "Training Epoch 6  63.1% | batch:       487 of       772\t|\tloss: 0.657831\n",
      "Training Epoch 6  63.2% | batch:       488 of       772\t|\tloss: 0.944841\n",
      "Training Epoch 6  63.3% | batch:       489 of       772\t|\tloss: 0.779464\n",
      "Training Epoch 6  63.5% | batch:       490 of       772\t|\tloss: 0.691446\n",
      "Training Epoch 6  63.6% | batch:       491 of       772\t|\tloss: 0.664288\n",
      "Training Epoch 6  63.7% | batch:       492 of       772\t|\tloss: 0.478158\n",
      "Training Epoch 6  63.9% | batch:       493 of       772\t|\tloss: 0.951061\n",
      "Training Epoch 6  64.0% | batch:       494 of       772\t|\tloss: 0.815549\n",
      "Training Epoch 6  64.1% | batch:       495 of       772\t|\tloss: 0.588898\n",
      "Training Epoch 6  64.2% | batch:       496 of       772\t|\tloss: 1.12607\n",
      "Training Epoch 6  64.4% | batch:       497 of       772\t|\tloss: 1.23477\n",
      "Training Epoch 6  64.5% | batch:       498 of       772\t|\tloss: 0.840532\n",
      "Training Epoch 6  64.6% | batch:       499 of       772\t|\tloss: 1.07056\n",
      "Training Epoch 6  64.8% | batch:       500 of       772\t|\tloss: 0.734242\n",
      "Training Epoch 6  64.9% | batch:       501 of       772\t|\tloss: 0.772419\n",
      "Training Epoch 6  65.0% | batch:       502 of       772\t|\tloss: 0.507084\n",
      "Training Epoch 6  65.2% | batch:       503 of       772\t|\tloss: 0.974585\n",
      "Training Epoch 6  65.3% | batch:       504 of       772\t|\tloss: 0.981761\n",
      "Training Epoch 6  65.4% | batch:       505 of       772\t|\tloss: 0.76711\n",
      "Training Epoch 6  65.5% | batch:       506 of       772\t|\tloss: 0.705252\n",
      "Training Epoch 6  65.7% | batch:       507 of       772\t|\tloss: 0.764186\n",
      "Training Epoch 6  65.8% | batch:       508 of       772\t|\tloss: 0.852893\n",
      "Training Epoch 6  65.9% | batch:       509 of       772\t|\tloss: 0.555755\n",
      "Training Epoch 6  66.1% | batch:       510 of       772\t|\tloss: 0.892576\n",
      "Training Epoch 6  66.2% | batch:       511 of       772\t|\tloss: 0.632676\n",
      "Training Epoch 6  66.3% | batch:       512 of       772\t|\tloss: 0.629187\n",
      "Training Epoch 6  66.5% | batch:       513 of       772\t|\tloss: 0.569011\n",
      "Training Epoch 6  66.6% | batch:       514 of       772\t|\tloss: 0.63049\n",
      "Training Epoch 6  66.7% | batch:       515 of       772\t|\tloss: 0.636355\n",
      "Training Epoch 6  66.8% | batch:       516 of       772\t|\tloss: 1.07265\n",
      "Training Epoch 6  67.0% | batch:       517 of       772\t|\tloss: 0.799545\n",
      "Training Epoch 6  67.1% | batch:       518 of       772\t|\tloss: 0.883173\n",
      "Training Epoch 6  67.2% | batch:       519 of       772\t|\tloss: 0.83426\n",
      "Training Epoch 6  67.4% | batch:       520 of       772\t|\tloss: 0.641861\n",
      "Training Epoch 6  67.5% | batch:       521 of       772\t|\tloss: 0.637749\n",
      "Training Epoch 6  67.6% | batch:       522 of       772\t|\tloss: 0.760617\n",
      "Training Epoch 6  67.7% | batch:       523 of       772\t|\tloss: 1.00948\n",
      "Training Epoch 6  67.9% | batch:       524 of       772\t|\tloss: 0.709995\n",
      "Training Epoch 6  68.0% | batch:       525 of       772\t|\tloss: 0.75103\n",
      "Training Epoch 6  68.1% | batch:       526 of       772\t|\tloss: 0.706753\n",
      "Training Epoch 6  68.3% | batch:       527 of       772\t|\tloss: 0.660349\n",
      "Training Epoch 6  68.4% | batch:       528 of       772\t|\tloss: 0.780129\n",
      "Training Epoch 6  68.5% | batch:       529 of       772\t|\tloss: 0.663421\n",
      "Training Epoch 6  68.7% | batch:       530 of       772\t|\tloss: 0.709915\n",
      "Training Epoch 6  68.8% | batch:       531 of       772\t|\tloss: 0.607657\n",
      "Training Epoch 6  68.9% | batch:       532 of       772\t|\tloss: 0.669304\n",
      "Training Epoch 6  69.0% | batch:       533 of       772\t|\tloss: 0.700465\n",
      "Training Epoch 6  69.2% | batch:       534 of       772\t|\tloss: 1.00928\n",
      "Training Epoch 6  69.3% | batch:       535 of       772\t|\tloss: 0.565602\n",
      "Training Epoch 6  69.4% | batch:       536 of       772\t|\tloss: 0.703105\n",
      "Training Epoch 6  69.6% | batch:       537 of       772\t|\tloss: 0.648538\n",
      "Training Epoch 6  69.7% | batch:       538 of       772\t|\tloss: 0.723769\n",
      "Training Epoch 6  69.8% | batch:       539 of       772\t|\tloss: 1.00526\n",
      "Training Epoch 6  69.9% | batch:       540 of       772\t|\tloss: 0.735973\n",
      "Training Epoch 6  70.1% | batch:       541 of       772\t|\tloss: 0.563164\n",
      "Training Epoch 6  70.2% | batch:       542 of       772\t|\tloss: 0.983087\n",
      "Training Epoch 6  70.3% | batch:       543 of       772\t|\tloss: 0.917181\n",
      "Training Epoch 6  70.5% | batch:       544 of       772\t|\tloss: 0.898385\n",
      "Training Epoch 6  70.6% | batch:       545 of       772\t|\tloss: 0.6728\n",
      "Training Epoch 6  70.7% | batch:       546 of       772\t|\tloss: 0.631204\n",
      "Training Epoch 6  70.9% | batch:       547 of       772\t|\tloss: 0.555994\n",
      "Training Epoch 6  71.0% | batch:       548 of       772\t|\tloss: 0.444578\n",
      "Training Epoch 6  71.1% | batch:       549 of       772\t|\tloss: 0.815951\n",
      "Training Epoch 6  71.2% | batch:       550 of       772\t|\tloss: 0.500859\n",
      "Training Epoch 6  71.4% | batch:       551 of       772\t|\tloss: 0.900775\n",
      "Training Epoch 6  71.5% | batch:       552 of       772\t|\tloss: 1.07782\n",
      "Training Epoch 6  71.6% | batch:       553 of       772\t|\tloss: 0.827056\n",
      "Training Epoch 6  71.8% | batch:       554 of       772\t|\tloss: 0.581425\n",
      "Training Epoch 6  71.9% | batch:       555 of       772\t|\tloss: 0.642308\n",
      "Training Epoch 6  72.0% | batch:       556 of       772\t|\tloss: 0.657773\n",
      "Training Epoch 6  72.2% | batch:       557 of       772\t|\tloss: 0.770795\n",
      "Training Epoch 6  72.3% | batch:       558 of       772\t|\tloss: 0.704063\n",
      "Training Epoch 6  72.4% | batch:       559 of       772\t|\tloss: 0.625196\n",
      "Training Epoch 6  72.5% | batch:       560 of       772\t|\tloss: 0.576245\n",
      "Training Epoch 6  72.7% | batch:       561 of       772\t|\tloss: 0.796459\n",
      "Training Epoch 6  72.8% | batch:       562 of       772\t|\tloss: 0.699914\n",
      "Training Epoch 6  72.9% | batch:       563 of       772\t|\tloss: 0.492844\n",
      "Training Epoch 6  73.1% | batch:       564 of       772\t|\tloss: 0.738913\n",
      "Training Epoch 6  73.2% | batch:       565 of       772\t|\tloss: 0.767392\n",
      "Training Epoch 6  73.3% | batch:       566 of       772\t|\tloss: 0.54561\n",
      "Training Epoch 6  73.4% | batch:       567 of       772\t|\tloss: 0.874242\n",
      "Training Epoch 6  73.6% | batch:       568 of       772\t|\tloss: 0.629023\n",
      "Training Epoch 6  73.7% | batch:       569 of       772\t|\tloss: 0.963566\n",
      "Training Epoch 6  73.8% | batch:       570 of       772\t|\tloss: 0.913287\n",
      "Training Epoch 6  74.0% | batch:       571 of       772\t|\tloss: 0.859227\n",
      "Training Epoch 6  74.1% | batch:       572 of       772\t|\tloss: 1.08865\n",
      "Training Epoch 6  74.2% | batch:       573 of       772\t|\tloss: 0.754168\n",
      "Training Epoch 6  74.4% | batch:       574 of       772\t|\tloss: 0.964563\n",
      "Training Epoch 6  74.5% | batch:       575 of       772\t|\tloss: 0.664406\n",
      "Training Epoch 6  74.6% | batch:       576 of       772\t|\tloss: 0.740191\n",
      "Training Epoch 6  74.7% | batch:       577 of       772\t|\tloss: 0.563096\n",
      "Training Epoch 6  74.9% | batch:       578 of       772\t|\tloss: 0.629762\n",
      "Training Epoch 6  75.0% | batch:       579 of       772\t|\tloss: 0.681422\n",
      "Training Epoch 6  75.1% | batch:       580 of       772\t|\tloss: 0.684031\n",
      "Training Epoch 6  75.3% | batch:       581 of       772\t|\tloss: 0.910649\n",
      "Training Epoch 6  75.4% | batch:       582 of       772\t|\tloss: 0.48793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  75.5% | batch:       583 of       772\t|\tloss: 1.0472\n",
      "Training Epoch 6  75.6% | batch:       584 of       772\t|\tloss: 0.760306\n",
      "Training Epoch 6  75.8% | batch:       585 of       772\t|\tloss: 0.563557\n",
      "Training Epoch 6  75.9% | batch:       586 of       772\t|\tloss: 0.553798\n",
      "Training Epoch 6  76.0% | batch:       587 of       772\t|\tloss: 0.911999\n",
      "Training Epoch 6  76.2% | batch:       588 of       772\t|\tloss: 0.900137\n",
      "Training Epoch 6  76.3% | batch:       589 of       772\t|\tloss: 0.543443\n",
      "Training Epoch 6  76.4% | batch:       590 of       772\t|\tloss: 0.67461\n",
      "Training Epoch 6  76.6% | batch:       591 of       772\t|\tloss: 0.680869\n",
      "Training Epoch 6  76.7% | batch:       592 of       772\t|\tloss: 0.538593\n",
      "Training Epoch 6  76.8% | batch:       593 of       772\t|\tloss: 0.974708\n",
      "Training Epoch 6  76.9% | batch:       594 of       772\t|\tloss: 0.725134\n",
      "Training Epoch 6  77.1% | batch:       595 of       772\t|\tloss: 0.528093\n",
      "Training Epoch 6  77.2% | batch:       596 of       772\t|\tloss: 1.09741\n",
      "Training Epoch 6  77.3% | batch:       597 of       772\t|\tloss: 0.875306\n",
      "Training Epoch 6  77.5% | batch:       598 of       772\t|\tloss: 0.572539\n",
      "Training Epoch 6  77.6% | batch:       599 of       772\t|\tloss: 0.609476\n",
      "Training Epoch 6  77.7% | batch:       600 of       772\t|\tloss: 1.03758\n",
      "Training Epoch 6  77.8% | batch:       601 of       772\t|\tloss: 0.763759\n",
      "Training Epoch 6  78.0% | batch:       602 of       772\t|\tloss: 0.75965\n",
      "Training Epoch 6  78.1% | batch:       603 of       772\t|\tloss: 0.919393\n",
      "Training Epoch 6  78.2% | batch:       604 of       772\t|\tloss: 0.703309\n",
      "Training Epoch 6  78.4% | batch:       605 of       772\t|\tloss: 0.634046\n",
      "Training Epoch 6  78.5% | batch:       606 of       772\t|\tloss: 0.797229\n",
      "Training Epoch 6  78.6% | batch:       607 of       772\t|\tloss: 1.05658\n",
      "Training Epoch 6  78.8% | batch:       608 of       772\t|\tloss: 0.567454\n",
      "Training Epoch 6  78.9% | batch:       609 of       772\t|\tloss: 0.737588\n",
      "Training Epoch 6  79.0% | batch:       610 of       772\t|\tloss: 1.00726\n",
      "Training Epoch 6  79.1% | batch:       611 of       772\t|\tloss: 0.566578\n",
      "Training Epoch 6  79.3% | batch:       612 of       772\t|\tloss: 0.729707\n",
      "Training Epoch 6  79.4% | batch:       613 of       772\t|\tloss: 0.637787\n",
      "Training Epoch 6  79.5% | batch:       614 of       772\t|\tloss: 0.957529\n",
      "Training Epoch 6  79.7% | batch:       615 of       772\t|\tloss: 0.84091\n",
      "Training Epoch 6  79.8% | batch:       616 of       772\t|\tloss: 0.90868\n",
      "Training Epoch 6  79.9% | batch:       617 of       772\t|\tloss: 0.744117\n",
      "Training Epoch 6  80.1% | batch:       618 of       772\t|\tloss: 0.773245\n",
      "Training Epoch 6  80.2% | batch:       619 of       772\t|\tloss: 0.607816\n",
      "Training Epoch 6  80.3% | batch:       620 of       772\t|\tloss: 0.797017\n",
      "Training Epoch 6  80.4% | batch:       621 of       772\t|\tloss: 0.582326\n",
      "Training Epoch 6  80.6% | batch:       622 of       772\t|\tloss: 1.19714\n",
      "Training Epoch 6  80.7% | batch:       623 of       772\t|\tloss: 0.730879\n",
      "Training Epoch 6  80.8% | batch:       624 of       772\t|\tloss: 0.981494\n",
      "Training Epoch 6  81.0% | batch:       625 of       772\t|\tloss: 0.715409\n",
      "Training Epoch 6  81.1% | batch:       626 of       772\t|\tloss: 0.789969\n",
      "Training Epoch 6  81.2% | batch:       627 of       772\t|\tloss: 1.16779\n",
      "Training Epoch 6  81.3% | batch:       628 of       772\t|\tloss: 0.828211\n",
      "Training Epoch 6  81.5% | batch:       629 of       772\t|\tloss: 0.578084\n",
      "Training Epoch 6  81.6% | batch:       630 of       772\t|\tloss: 0.800831\n",
      "Training Epoch 6  81.7% | batch:       631 of       772\t|\tloss: 0.493274\n",
      "Training Epoch 6  81.9% | batch:       632 of       772\t|\tloss: 0.996954\n",
      "Training Epoch 6  82.0% | batch:       633 of       772\t|\tloss: 0.702601\n",
      "Training Epoch 6  82.1% | batch:       634 of       772\t|\tloss: 0.836826\n",
      "Training Epoch 6  82.3% | batch:       635 of       772\t|\tloss: 1.05661\n",
      "Training Epoch 6  82.4% | batch:       636 of       772\t|\tloss: 0.910024\n",
      "Training Epoch 6  82.5% | batch:       637 of       772\t|\tloss: 0.98858\n",
      "Training Epoch 6  82.6% | batch:       638 of       772\t|\tloss: 0.72114\n",
      "Training Epoch 6  82.8% | batch:       639 of       772\t|\tloss: 0.954264\n",
      "Training Epoch 6  82.9% | batch:       640 of       772\t|\tloss: 0.859735\n",
      "Training Epoch 6  83.0% | batch:       641 of       772\t|\tloss: 0.920712\n",
      "Training Epoch 6  83.2% | batch:       642 of       772\t|\tloss: 0.715514\n",
      "Training Epoch 6  83.3% | batch:       643 of       772\t|\tloss: 0.712714\n",
      "Training Epoch 6  83.4% | batch:       644 of       772\t|\tloss: 0.729892\n",
      "Training Epoch 6  83.5% | batch:       645 of       772\t|\tloss: 0.665347\n",
      "Training Epoch 6  83.7% | batch:       646 of       772\t|\tloss: 0.820564\n",
      "Training Epoch 6  83.8% | batch:       647 of       772\t|\tloss: 0.812167\n",
      "Training Epoch 6  83.9% | batch:       648 of       772\t|\tloss: 0.794981\n",
      "Training Epoch 6  84.1% | batch:       649 of       772\t|\tloss: 0.642117\n",
      "Training Epoch 6  84.2% | batch:       650 of       772\t|\tloss: 0.766694\n",
      "Training Epoch 6  84.3% | batch:       651 of       772\t|\tloss: 0.951688\n",
      "Training Epoch 6  84.5% | batch:       652 of       772\t|\tloss: 0.791247\n",
      "Training Epoch 6  84.6% | batch:       653 of       772\t|\tloss: 0.763369\n",
      "Training Epoch 6  84.7% | batch:       654 of       772\t|\tloss: 0.739673\n",
      "Training Epoch 6  84.8% | batch:       655 of       772\t|\tloss: 0.892825\n",
      "Training Epoch 6  85.0% | batch:       656 of       772\t|\tloss: 0.79975\n",
      "Training Epoch 6  85.1% | batch:       657 of       772\t|\tloss: 0.605812\n",
      "Training Epoch 6  85.2% | batch:       658 of       772\t|\tloss: 0.613367\n",
      "Training Epoch 6  85.4% | batch:       659 of       772\t|\tloss: 0.69979\n",
      "Training Epoch 6  85.5% | batch:       660 of       772\t|\tloss: 0.884185\n",
      "Training Epoch 6  85.6% | batch:       661 of       772\t|\tloss: 0.757829\n",
      "Training Epoch 6  85.8% | batch:       662 of       772\t|\tloss: 0.437099\n",
      "Training Epoch 6  85.9% | batch:       663 of       772\t|\tloss: 0.680012\n",
      "Training Epoch 6  86.0% | batch:       664 of       772\t|\tloss: 0.459248\n",
      "Training Epoch 6  86.1% | batch:       665 of       772\t|\tloss: 0.74587\n",
      "Training Epoch 6  86.3% | batch:       666 of       772\t|\tloss: 0.551771\n",
      "Training Epoch 6  86.4% | batch:       667 of       772\t|\tloss: 0.627555\n",
      "Training Epoch 6  86.5% | batch:       668 of       772\t|\tloss: 0.639112\n",
      "Training Epoch 6  86.7% | batch:       669 of       772\t|\tloss: 0.466744\n",
      "Training Epoch 6  86.8% | batch:       670 of       772\t|\tloss: 0.928818\n",
      "Training Epoch 6  86.9% | batch:       671 of       772\t|\tloss: 0.704244\n",
      "Training Epoch 6  87.0% | batch:       672 of       772\t|\tloss: 0.676021\n",
      "Training Epoch 6  87.2% | batch:       673 of       772\t|\tloss: 0.582138\n",
      "Training Epoch 6  87.3% | batch:       674 of       772\t|\tloss: 0.845134\n",
      "Training Epoch 6  87.4% | batch:       675 of       772\t|\tloss: 0.612036\n",
      "Training Epoch 6  87.6% | batch:       676 of       772\t|\tloss: 0.568163\n",
      "Training Epoch 6  87.7% | batch:       677 of       772\t|\tloss: 0.810861\n",
      "Training Epoch 6  87.8% | batch:       678 of       772\t|\tloss: 0.733801\n",
      "Training Epoch 6  88.0% | batch:       679 of       772\t|\tloss: 0.808881\n",
      "Training Epoch 6  88.1% | batch:       680 of       772\t|\tloss: 0.760266\n",
      "Training Epoch 6  88.2% | batch:       681 of       772\t|\tloss: 0.660601\n",
      "Training Epoch 6  88.3% | batch:       682 of       772\t|\tloss: 0.819543\n",
      "Training Epoch 6  88.5% | batch:       683 of       772\t|\tloss: 0.612121\n",
      "Training Epoch 6  88.6% | batch:       684 of       772\t|\tloss: 0.5485\n",
      "Training Epoch 6  88.7% | batch:       685 of       772\t|\tloss: 0.825418\n",
      "Training Epoch 6  88.9% | batch:       686 of       772\t|\tloss: 0.688058\n",
      "Training Epoch 6  89.0% | batch:       687 of       772\t|\tloss: 0.620916\n",
      "Training Epoch 6  89.1% | batch:       688 of       772\t|\tloss: 0.878994\n",
      "Training Epoch 6  89.2% | batch:       689 of       772\t|\tloss: 0.782245\n",
      "Training Epoch 6  89.4% | batch:       690 of       772\t|\tloss: 0.580773\n",
      "Training Epoch 6  89.5% | batch:       691 of       772\t|\tloss: 0.621088\n",
      "Training Epoch 6  89.6% | batch:       692 of       772\t|\tloss: 0.610584\n",
      "Training Epoch 6  89.8% | batch:       693 of       772\t|\tloss: 0.716428\n",
      "Training Epoch 6  89.9% | batch:       694 of       772\t|\tloss: 0.681255\n",
      "Training Epoch 6  90.0% | batch:       695 of       772\t|\tloss: 0.8197\n",
      "Training Epoch 6  90.2% | batch:       696 of       772\t|\tloss: 0.580278\n",
      "Training Epoch 6  90.3% | batch:       697 of       772\t|\tloss: 0.55688\n",
      "Training Epoch 6  90.4% | batch:       698 of       772\t|\tloss: 0.764602\n",
      "Training Epoch 6  90.5% | batch:       699 of       772\t|\tloss: 0.69231\n",
      "Training Epoch 6  90.7% | batch:       700 of       772\t|\tloss: 0.783011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  90.8% | batch:       701 of       772\t|\tloss: 0.861613\n",
      "Training Epoch 6  90.9% | batch:       702 of       772\t|\tloss: 0.570742\n",
      "Training Epoch 6  91.1% | batch:       703 of       772\t|\tloss: 0.796816\n",
      "Training Epoch 6  91.2% | batch:       704 of       772\t|\tloss: 0.533103\n",
      "Training Epoch 6  91.3% | batch:       705 of       772\t|\tloss: 0.484439\n",
      "Training Epoch 6  91.5% | batch:       706 of       772\t|\tloss: 1.09772\n",
      "Training Epoch 6  91.6% | batch:       707 of       772\t|\tloss: 0.794353\n",
      "Training Epoch 6  91.7% | batch:       708 of       772\t|\tloss: 0.619256\n",
      "Training Epoch 6  91.8% | batch:       709 of       772\t|\tloss: 0.664652\n",
      "Training Epoch 6  92.0% | batch:       710 of       772\t|\tloss: 0.557559\n",
      "Training Epoch 6  92.1% | batch:       711 of       772\t|\tloss: 0.889231\n",
      "Training Epoch 6  92.2% | batch:       712 of       772\t|\tloss: 0.820602\n",
      "Training Epoch 6  92.4% | batch:       713 of       772\t|\tloss: 0.52711\n",
      "Training Epoch 6  92.5% | batch:       714 of       772\t|\tloss: 0.555404\n",
      "Training Epoch 6  92.6% | batch:       715 of       772\t|\tloss: 0.596634\n",
      "Training Epoch 6  92.7% | batch:       716 of       772\t|\tloss: 0.619507\n",
      "Training Epoch 6  92.9% | batch:       717 of       772\t|\tloss: 1.12278\n",
      "Training Epoch 6  93.0% | batch:       718 of       772\t|\tloss: 0.881936\n",
      "Training Epoch 6  93.1% | batch:       719 of       772\t|\tloss: 0.62853\n",
      "Training Epoch 6  93.3% | batch:       720 of       772\t|\tloss: 0.561667\n",
      "Training Epoch 6  93.4% | batch:       721 of       772\t|\tloss: 0.915633\n",
      "Training Epoch 6  93.5% | batch:       722 of       772\t|\tloss: 0.866582\n",
      "Training Epoch 6  93.7% | batch:       723 of       772\t|\tloss: 1.52473\n",
      "Training Epoch 6  93.8% | batch:       724 of       772\t|\tloss: 0.92842\n",
      "Training Epoch 6  93.9% | batch:       725 of       772\t|\tloss: 0.947962\n",
      "Training Epoch 6  94.0% | batch:       726 of       772\t|\tloss: 1.09504\n",
      "Training Epoch 6  94.2% | batch:       727 of       772\t|\tloss: 0.621196\n",
      "Training Epoch 6  94.3% | batch:       728 of       772\t|\tloss: 1.08563\n",
      "Training Epoch 6  94.4% | batch:       729 of       772\t|\tloss: 0.681688\n",
      "Training Epoch 6  94.6% | batch:       730 of       772\t|\tloss: 0.816251\n",
      "Training Epoch 6  94.7% | batch:       731 of       772\t|\tloss: 0.662384\n",
      "Training Epoch 6  94.8% | batch:       732 of       772\t|\tloss: 0.82591\n",
      "Training Epoch 6  94.9% | batch:       733 of       772\t|\tloss: 0.868987\n",
      "Training Epoch 6  95.1% | batch:       734 of       772\t|\tloss: 0.862304\n",
      "Training Epoch 6  95.2% | batch:       735 of       772\t|\tloss: 0.815548\n",
      "Training Epoch 6  95.3% | batch:       736 of       772\t|\tloss: 0.946765\n",
      "Training Epoch 6  95.5% | batch:       737 of       772\t|\tloss: 1.10847\n",
      "Training Epoch 6  95.6% | batch:       738 of       772\t|\tloss: 0.899618\n",
      "Training Epoch 6  95.7% | batch:       739 of       772\t|\tloss: 0.78761\n",
      "Training Epoch 6  95.9% | batch:       740 of       772\t|\tloss: 0.644938\n",
      "Training Epoch 6  96.0% | batch:       741 of       772\t|\tloss: 0.60759\n",
      "Training Epoch 6  96.1% | batch:       742 of       772\t|\tloss: 0.940564\n",
      "Training Epoch 6  96.2% | batch:       743 of       772\t|\tloss: 0.696397\n",
      "Training Epoch 6  96.4% | batch:       744 of       772\t|\tloss: 0.672663\n",
      "Training Epoch 6  96.5% | batch:       745 of       772\t|\tloss: 1.0555\n",
      "Training Epoch 6  96.6% | batch:       746 of       772\t|\tloss: 0.582854\n",
      "Training Epoch 6  96.8% | batch:       747 of       772\t|\tloss: 0.537187\n",
      "Training Epoch 6  96.9% | batch:       748 of       772\t|\tloss: 0.677032\n",
      "Training Epoch 6  97.0% | batch:       749 of       772\t|\tloss: 0.675762\n",
      "Training Epoch 6  97.2% | batch:       750 of       772\t|\tloss: 0.810314\n",
      "Training Epoch 6  97.3% | batch:       751 of       772\t|\tloss: 0.666704\n",
      "Training Epoch 6  97.4% | batch:       752 of       772\t|\tloss: 0.69843\n",
      "Training Epoch 6  97.5% | batch:       753 of       772\t|\tloss: 0.602288\n",
      "Training Epoch 6  97.7% | batch:       754 of       772\t|\tloss: 1.06628\n",
      "Training Epoch 6  97.8% | batch:       755 of       772\t|\tloss: 0.73925\n",
      "Training Epoch 6  97.9% | batch:       756 of       772\t|\tloss: 0.56412\n",
      "Training Epoch 6  98.1% | batch:       757 of       772\t|\tloss: 0.504777\n",
      "Training Epoch 6  98.2% | batch:       758 of       772\t|\tloss: 0.721547\n",
      "Training Epoch 6  98.3% | batch:       759 of       772\t|\tloss: 0.718387\n",
      "Training Epoch 6  98.4% | batch:       760 of       772\t|\tloss: 0.88709\n",
      "Training Epoch 6  98.6% | batch:       761 of       772\t|\tloss: 0.605373\n",
      "Training Epoch 6  98.7% | batch:       762 of       772\t|\tloss: 0.589966\n",
      "Training Epoch 6  98.8% | batch:       763 of       772\t|\tloss: 0.888593\n",
      "Training Epoch 6  99.0% | batch:       764 of       772\t|\tloss: 0.517322\n",
      "Training Epoch 6  99.1% | batch:       765 of       772\t|\tloss: 0.855505\n",
      "Training Epoch 6  99.2% | batch:       766 of       772\t|\tloss: 0.813933\n",
      "Training Epoch 6  99.4% | batch:       767 of       772\t|\tloss: 0.946149\n",
      "Training Epoch 6  99.5% | batch:       768 of       772\t|\tloss: 1.00839\n",
      "Training Epoch 6  99.6% | batch:       769 of       772\t|\tloss: 0.86\n",
      "Training Epoch 6  99.7% | batch:       770 of       772\t|\tloss: 0.590452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:21:54,583 | INFO : Epoch 6 Training Summary: epoch: 6.000000 | loss: 0.801854 | \n",
      "2023-05-24 10:21:54,584 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 16.801796913146973 seconds\n",
      "\n",
      "2023-05-24 10:21:54,584 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 16.968369086583454 seconds\n",
      "2023-05-24 10:21:54,585 | INFO : Avg batch train. time: 0.021979752702828308 seconds\n",
      "2023-05-24 10:21:54,585 | INFO : Avg sample train. time: 0.0001717202935473056 seconds\n",
      "2023-05-24 10:21:54,585 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  99.9% | batch:       771 of       772\t|\tloss: 0.665573\n",
      "\n",
      "Evaluating Epoch 6   0.0% | batch:         0 of        92\t|\tloss: 3.41841\n",
      "Evaluating Epoch 6   1.1% | batch:         1 of        92\t|\tloss: 16.0478\n",
      "Evaluating Epoch 6   2.2% | batch:         2 of        92\t|\tloss: 5.99959\n",
      "Evaluating Epoch 6   3.3% | batch:         3 of        92\t|\tloss: 12.6225\n",
      "Evaluating Epoch 6   4.3% | batch:         4 of        92\t|\tloss: 6.8595\n",
      "Evaluating Epoch 6   5.4% | batch:         5 of        92\t|\tloss: 12.3107\n",
      "Evaluating Epoch 6   6.5% | batch:         6 of        92\t|\tloss: 7.02908\n",
      "Evaluating Epoch 6   7.6% | batch:         7 of        92\t|\tloss: 5.39679\n",
      "Evaluating Epoch 6   8.7% | batch:         8 of        92\t|\tloss: 12.5238\n",
      "Evaluating Epoch 6   9.8% | batch:         9 of        92\t|\tloss: 8.87254\n",
      "Evaluating Epoch 6  10.9% | batch:        10 of        92\t|\tloss: 11.5008\n",
      "Evaluating Epoch 6  12.0% | batch:        11 of        92\t|\tloss: 8.2897\n",
      "Evaluating Epoch 6  13.0% | batch:        12 of        92\t|\tloss: 10.9121\n",
      "Evaluating Epoch 6  14.1% | batch:        13 of        92\t|\tloss: 9.00115\n",
      "Evaluating Epoch 6  15.2% | batch:        14 of        92\t|\tloss: 5.06809\n",
      "Evaluating Epoch 6  16.3% | batch:        15 of        92\t|\tloss: 3.01261\n",
      "Evaluating Epoch 6  17.4% | batch:        16 of        92\t|\tloss: 6.6409\n",
      "Evaluating Epoch 6  18.5% | batch:        17 of        92\t|\tloss: 4.55141\n",
      "Evaluating Epoch 6  19.6% | batch:        18 of        92\t|\tloss: 8.64251\n",
      "Evaluating Epoch 6  20.7% | batch:        19 of        92\t|\tloss: 10.3245\n",
      "Evaluating Epoch 6  21.7% | batch:        20 of        92\t|\tloss: 8.41579\n",
      "Evaluating Epoch 6  22.8% | batch:        21 of        92\t|\tloss: 8.71478\n",
      "Evaluating Epoch 6  23.9% | batch:        22 of        92\t|\tloss: 8.34273\n",
      "Evaluating Epoch 6  25.0% | batch:        23 of        92\t|\tloss: 10.1524\n",
      "Evaluating Epoch 6  26.1% | batch:        24 of        92\t|\tloss: 5.37156\n",
      "Evaluating Epoch 6  27.2% | batch:        25 of        92\t|\tloss: 1.78707\n",
      "Evaluating Epoch 6  28.3% | batch:        26 of        92\t|\tloss: 4.01185\n",
      "Evaluating Epoch 6  29.3% | batch:        27 of        92\t|\tloss: 6.76408\n",
      "Evaluating Epoch 6  30.4% | batch:        28 of        92\t|\tloss: 6.76623\n",
      "Evaluating Epoch 6  31.5% | batch:        29 of        92\t|\tloss: 8.28633\n",
      "Evaluating Epoch 6  32.6% | batch:        30 of        92\t|\tloss: 6.23034\n",
      "Evaluating Epoch 6  33.7% | batch:        31 of        92\t|\tloss: 10.1403\n",
      "Evaluating Epoch 6  34.8% | batch:        32 of        92\t|\tloss: 6.57839\n",
      "Evaluating Epoch 6  35.9% | batch:        33 of        92\t|\tloss: 10.2573\n",
      "Evaluating Epoch 6  37.0% | batch:        34 of        92\t|\tloss: 7.06966\n",
      "Evaluating Epoch 6  38.0% | batch:        35 of        92\t|\tloss: 6.39735\n",
      "Evaluating Epoch 6  39.1% | batch:        36 of        92\t|\tloss: 5.08552\n",
      "Evaluating Epoch 6  40.2% | batch:        37 of        92\t|\tloss: 6.23328\n",
      "Evaluating Epoch 6  41.3% | batch:        38 of        92\t|\tloss: 5.80979\n",
      "Evaluating Epoch 6  42.4% | batch:        39 of        92\t|\tloss: 14.1363\n",
      "Evaluating Epoch 6  43.5% | batch:        40 of        92\t|\tloss: 7.00492\n",
      "Evaluating Epoch 6  44.6% | batch:        41 of        92\t|\tloss: 11.5773\n",
      "Evaluating Epoch 6  45.7% | batch:        42 of        92\t|\tloss: 6.69497\n",
      "Evaluating Epoch 6  46.7% | batch:        43 of        92\t|\tloss: 11.5333\n",
      "Evaluating Epoch 6  47.8% | batch:        44 of        92\t|\tloss: 5.08363\n",
      "Evaluating Epoch 6  48.9% | batch:        45 of        92\t|\tloss: 4.87877\n",
      "Evaluating Epoch 6  50.0% | batch:        46 of        92\t|\tloss: 5.05009\n",
      "Evaluating Epoch 6  51.1% | batch:        47 of        92\t|\tloss: 8.81048\n",
      "Evaluating Epoch 6  52.2% | batch:        48 of        92\t|\tloss: 11.7145\n",
      "Evaluating Epoch 6  53.3% | batch:        49 of        92\t|\tloss: 9.15068\n",
      "Evaluating Epoch 6  54.3% | batch:        50 of        92\t|\tloss: 10.485\n",
      "Evaluating Epoch 6  55.4% | batch:        51 of        92\t|\tloss: 9.55776\n",
      "Evaluating Epoch 6  56.5% | batch:        52 of        92\t|\tloss: 10.0846\n",
      "Evaluating Epoch 6  57.6% | batch:        53 of        92\t|\tloss: 4.84575\n",
      "Evaluating Epoch 6  58.7% | batch:        54 of        92\t|\tloss: 4.96338\n",
      "Evaluating Epoch 6  59.8% | batch:        55 of        92\t|\tloss: 10.2021\n",
      "Evaluating Epoch 6  60.9% | batch:        56 of        92\t|\tloss: 12.2712\n",
      "Evaluating Epoch 6  62.0% | batch:        57 of        92\t|\tloss: 9.61039\n",
      "Evaluating Epoch 6  63.0% | batch:        58 of        92\t|\tloss: 9.51512\n",
      "Evaluating Epoch 6  64.1% | batch:        59 of        92\t|\tloss: 9.77436\n",
      "Evaluating Epoch 6  65.2% | batch:        60 of        92\t|\tloss: 10.178\n",
      "Evaluating Epoch 6  66.3% | batch:        61 of        92\t|\tloss: 5.13433\n",
      "Evaluating Epoch 6  67.4% | batch:        62 of        92\t|\tloss: 2.60681\n",
      "Evaluating Epoch 6  68.5% | batch:        63 of        92\t|\tloss: 4.25864\n",
      "Evaluating Epoch 6  69.6% | batch:        64 of        92\t|\tloss: 7.77506\n",
      "Evaluating Epoch 6  70.7% | batch:        65 of        92\t|\tloss: 12.261\n",
      "Evaluating Epoch 6  71.7% | batch:        66 of        92\t|\tloss: 8.05513\n",
      "Evaluating Epoch 6  72.8% | batch:        67 of        92\t|\tloss: 8.29432\n",
      "Evaluating Epoch 6  73.9% | batch:        68 of        92\t|\tloss: 10.6746\n",
      "Evaluating Epoch 6  75.0% | batch:        69 of        92\t|\tloss: 7.68491\n",
      "Evaluating Epoch 6  76.1% | batch:        70 of        92\t|\tloss: 10.9173\n",
      "Evaluating Epoch 6  77.2% | batch:        71 of        92\t|\tloss: 6.99172\n",
      "Evaluating Epoch 6  78.3% | batch:        72 of        92\t|\tloss: 7.62804\n",
      "Evaluating Epoch 6  79.3% | batch:        73 of        92\t|\tloss: 5.40987\n",
      "Evaluating Epoch 6  80.4% | batch:        74 of        92\t|\tloss: 8.88211\n",
      "Evaluating Epoch 6  81.5% | batch:        75 of        92\t|\tloss: 4.43018\n",
      "Evaluating Epoch 6  82.6% | batch:        76 of        92\t|\tloss: 8.85728\n",
      "Evaluating Epoch 6  83.7% | batch:        77 of        92\t|\tloss: 9.80141\n",
      "Evaluating Epoch 6  84.8% | batch:        78 of        92\t|\tloss: 9.77469\n",
      "Evaluating Epoch 6  85.9% | batch:        79 of        92\t|\tloss: 9.76295\n",
      "Evaluating Epoch 6  87.0% | batch:        80 of        92\t|\tloss: 9.29138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:21:55,770 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.1844167709350586 seconds\n",
      "\n",
      "2023-05-24 10:21:55,771 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.3933774471282958 seconds\n",
      "2023-05-24 10:21:55,771 | INFO : Avg batch val. time: 0.015145407034003215 seconds\n",
      "2023-05-24 10:21:55,771 | INFO : Avg sample val. time: 0.00011943917770686575 seconds\n",
      "2023-05-24 10:21:55,772 | INFO : Epoch 6 Validation Summary: epoch: 6.000000 | loss: 8.115708 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 6  88.0% | batch:        81 of        92\t|\tloss: 9.63113\n",
      "Evaluating Epoch 6  89.1% | batch:        82 of        92\t|\tloss: 4.63767\n",
      "Evaluating Epoch 6  90.2% | batch:        83 of        92\t|\tloss: 4.69339\n",
      "Evaluating Epoch 6  91.3% | batch:        84 of        92\t|\tloss: 8.91542\n",
      "Evaluating Epoch 6  92.4% | batch:        85 of        92\t|\tloss: 11.6182\n",
      "Evaluating Epoch 6  93.5% | batch:        86 of        92\t|\tloss: 9.17628\n",
      "Evaluating Epoch 6  94.6% | batch:        87 of        92\t|\tloss: 9.2765\n",
      "Evaluating Epoch 6  95.7% | batch:        88 of        92\t|\tloss: 8.83667\n",
      "Evaluating Epoch 6  96.7% | batch:        89 of        92\t|\tloss: 10.1841\n",
      "Evaluating Epoch 6  97.8% | batch:        90 of        92\t|\tloss: 5.23423\n",
      "Evaluating Epoch 6  98.9% | batch:        91 of        92\t|\tloss: 2.94125\n",
      "\n",
      "Training Epoch 7   0.0% | batch:         0 of       772\t|\tloss: 1.15139\n",
      "Training Epoch 7   0.1% | batch:         1 of       772\t|\tloss: 1.29112\n",
      "Training Epoch 7   0.3% | batch:         2 of       772\t|\tloss: 0.755035\n",
      "Training Epoch 7   0.4% | batch:         3 of       772\t|\tloss: 0.80771\n",
      "Training Epoch 7   0.5% | batch:         4 of       772\t|\tloss: 1.10901\n",
      "Training Epoch 7   0.6% | batch:         5 of       772\t|\tloss: 1.03393\n",
      "Training Epoch 7   0.8% | batch:         6 of       772\t|\tloss: 0.946205\n",
      "Training Epoch 7   0.9% | batch:         7 of       772\t|\tloss: 0.7339\n",
      "Training Epoch 7   1.0% | batch:         8 of       772\t|\tloss: 0.722607\n",
      "Training Epoch 7   1.2% | batch:         9 of       772\t|\tloss: 0.926975\n",
      "Training Epoch 7   1.3% | batch:        10 of       772\t|\tloss: 0.55751\n",
      "Training Epoch 7   1.4% | batch:        11 of       772\t|\tloss: 0.663175\n",
      "Training Epoch 7   1.6% | batch:        12 of       772\t|\tloss: 0.800627\n",
      "Training Epoch 7   1.7% | batch:        13 of       772\t|\tloss: 0.792033\n",
      "Training Epoch 7   1.8% | batch:        14 of       772\t|\tloss: 0.648974\n",
      "Training Epoch 7   1.9% | batch:        15 of       772\t|\tloss: 0.749477\n",
      "Training Epoch 7   2.1% | batch:        16 of       772\t|\tloss: 0.613039\n",
      "Training Epoch 7   2.2% | batch:        17 of       772\t|\tloss: 0.523768\n",
      "Training Epoch 7   2.3% | batch:        18 of       772\t|\tloss: 0.723838\n",
      "Training Epoch 7   2.5% | batch:        19 of       772\t|\tloss: 0.681617\n",
      "Training Epoch 7   2.6% | batch:        20 of       772\t|\tloss: 0.668803\n",
      "Training Epoch 7   2.7% | batch:        21 of       772\t|\tloss: 0.879961\n",
      "Training Epoch 7   2.8% | batch:        22 of       772\t|\tloss: 0.948991\n",
      "Training Epoch 7   3.0% | batch:        23 of       772\t|\tloss: 0.576336\n",
      "Training Epoch 7   3.1% | batch:        24 of       772\t|\tloss: 0.649332\n",
      "Training Epoch 7   3.2% | batch:        25 of       772\t|\tloss: 0.710083\n",
      "Training Epoch 7   3.4% | batch:        26 of       772\t|\tloss: 0.564019\n",
      "Training Epoch 7   3.5% | batch:        27 of       772\t|\tloss: 0.8381\n",
      "Training Epoch 7   3.6% | batch:        28 of       772\t|\tloss: 0.956484\n",
      "Training Epoch 7   3.8% | batch:        29 of       772\t|\tloss: 0.665666\n",
      "Training Epoch 7   3.9% | batch:        30 of       772\t|\tloss: 0.519006\n",
      "Training Epoch 7   4.0% | batch:        31 of       772\t|\tloss: 0.575121\n",
      "Training Epoch 7   4.1% | batch:        32 of       772\t|\tloss: 0.764827\n",
      "Training Epoch 7   4.3% | batch:        33 of       772\t|\tloss: 0.637574\n",
      "Training Epoch 7   4.4% | batch:        34 of       772\t|\tloss: 0.885949\n",
      "Training Epoch 7   4.5% | batch:        35 of       772\t|\tloss: 0.77003\n",
      "Training Epoch 7   4.7% | batch:        36 of       772\t|\tloss: 0.989392\n",
      "Training Epoch 7   4.8% | batch:        37 of       772\t|\tloss: 1.11924\n",
      "Training Epoch 7   4.9% | batch:        38 of       772\t|\tloss: 0.593242\n",
      "Training Epoch 7   5.1% | batch:        39 of       772\t|\tloss: 0.712468\n",
      "Training Epoch 7   5.2% | batch:        40 of       772\t|\tloss: 0.711301\n",
      "Training Epoch 7   5.3% | batch:        41 of       772\t|\tloss: 0.746873\n",
      "Training Epoch 7   5.4% | batch:        42 of       772\t|\tloss: 0.665931\n",
      "Training Epoch 7   5.6% | batch:        43 of       772\t|\tloss: 0.815322\n",
      "Training Epoch 7   5.7% | batch:        44 of       772\t|\tloss: 0.666011\n",
      "Training Epoch 7   5.8% | batch:        45 of       772\t|\tloss: 0.846826\n",
      "Training Epoch 7   6.0% | batch:        46 of       772\t|\tloss: 0.676097\n",
      "Training Epoch 7   6.1% | batch:        47 of       772\t|\tloss: 0.885784\n",
      "Training Epoch 7   6.2% | batch:        48 of       772\t|\tloss: 0.88169\n",
      "Training Epoch 7   6.3% | batch:        49 of       772\t|\tloss: 0.903456\n",
      "Training Epoch 7   6.5% | batch:        50 of       772\t|\tloss: 1.13971\n",
      "Training Epoch 7   6.6% | batch:        51 of       772\t|\tloss: 0.927856\n",
      "Training Epoch 7   6.7% | batch:        52 of       772\t|\tloss: 0.597079\n",
      "Training Epoch 7   6.9% | batch:        53 of       772\t|\tloss: 0.788474\n",
      "Training Epoch 7   7.0% | batch:        54 of       772\t|\tloss: 0.805525\n",
      "Training Epoch 7   7.1% | batch:        55 of       772\t|\tloss: 0.554205\n",
      "Training Epoch 7   7.3% | batch:        56 of       772\t|\tloss: 0.782948\n",
      "Training Epoch 7   7.4% | batch:        57 of       772\t|\tloss: 0.869639\n",
      "Training Epoch 7   7.5% | batch:        58 of       772\t|\tloss: 0.499353\n",
      "Training Epoch 7   7.6% | batch:        59 of       772\t|\tloss: 0.48738\n",
      "Training Epoch 7   7.8% | batch:        60 of       772\t|\tloss: 0.659973\n",
      "Training Epoch 7   7.9% | batch:        61 of       772\t|\tloss: 0.88588\n",
      "Training Epoch 7   8.0% | batch:        62 of       772\t|\tloss: 0.654085\n",
      "Training Epoch 7   8.2% | batch:        63 of       772\t|\tloss: 0.991772\n",
      "Training Epoch 7   8.3% | batch:        64 of       772\t|\tloss: 1.44615\n",
      "Training Epoch 7   8.4% | batch:        65 of       772\t|\tloss: 1.0691\n",
      "Training Epoch 7   8.5% | batch:        66 of       772\t|\tloss: 0.642664\n",
      "Training Epoch 7   8.7% | batch:        67 of       772\t|\tloss: 0.675165\n",
      "Training Epoch 7   8.8% | batch:        68 of       772\t|\tloss: 1.11871\n",
      "Training Epoch 7   8.9% | batch:        69 of       772\t|\tloss: 1.4557\n",
      "Training Epoch 7   9.1% | batch:        70 of       772\t|\tloss: 0.683352\n",
      "Training Epoch 7   9.2% | batch:        71 of       772\t|\tloss: 1.04284\n",
      "Training Epoch 7   9.3% | batch:        72 of       772\t|\tloss: 0.78858\n",
      "Training Epoch 7   9.5% | batch:        73 of       772\t|\tloss: 0.745154\n",
      "Training Epoch 7   9.6% | batch:        74 of       772\t|\tloss: 0.890124\n",
      "Training Epoch 7   9.7% | batch:        75 of       772\t|\tloss: 0.760067\n",
      "Training Epoch 7   9.8% | batch:        76 of       772\t|\tloss: 0.635488\n",
      "Training Epoch 7  10.0% | batch:        77 of       772\t|\tloss: 0.976737\n",
      "Training Epoch 7  10.1% | batch:        78 of       772\t|\tloss: 0.903647\n",
      "Training Epoch 7  10.2% | batch:        79 of       772\t|\tloss: 0.792773\n",
      "Training Epoch 7  10.4% | batch:        80 of       772\t|\tloss: 0.931728\n",
      "Training Epoch 7  10.5% | batch:        81 of       772\t|\tloss: 0.714644\n",
      "Training Epoch 7  10.6% | batch:        82 of       772\t|\tloss: 0.638256\n",
      "Training Epoch 7  10.8% | batch:        83 of       772\t|\tloss: 0.459288\n",
      "Training Epoch 7  10.9% | batch:        84 of       772\t|\tloss: 0.893881\n",
      "Training Epoch 7  11.0% | batch:        85 of       772\t|\tloss: 0.881816\n",
      "Training Epoch 7  11.1% | batch:        86 of       772\t|\tloss: 0.655651\n",
      "Training Epoch 7  11.3% | batch:        87 of       772\t|\tloss: 0.743698\n",
      "Training Epoch 7  11.4% | batch:        88 of       772\t|\tloss: 0.654092\n",
      "Training Epoch 7  11.5% | batch:        89 of       772\t|\tloss: 0.554056\n",
      "Training Epoch 7  11.7% | batch:        90 of       772\t|\tloss: 0.524837\n",
      "Training Epoch 7  11.8% | batch:        91 of       772\t|\tloss: 0.7188\n",
      "Training Epoch 7  11.9% | batch:        92 of       772\t|\tloss: 1.04715\n",
      "Training Epoch 7  12.0% | batch:        93 of       772\t|\tloss: 0.870942\n",
      "Training Epoch 7  12.2% | batch:        94 of       772\t|\tloss: 0.745159\n",
      "Training Epoch 7  12.3% | batch:        95 of       772\t|\tloss: 0.531456\n",
      "Training Epoch 7  12.4% | batch:        96 of       772\t|\tloss: 0.657101\n",
      "Training Epoch 7  12.6% | batch:        97 of       772\t|\tloss: 0.66998\n",
      "Training Epoch 7  12.7% | batch:        98 of       772\t|\tloss: 0.708221\n",
      "Training Epoch 7  12.8% | batch:        99 of       772\t|\tloss: 0.631289\n",
      "Training Epoch 7  13.0% | batch:       100 of       772\t|\tloss: 0.593542\n",
      "Training Epoch 7  13.1% | batch:       101 of       772\t|\tloss: 1.11185\n",
      "Training Epoch 7  13.2% | batch:       102 of       772\t|\tloss: 0.741098\n",
      "Training Epoch 7  13.3% | batch:       103 of       772\t|\tloss: 0.859309\n",
      "Training Epoch 7  13.5% | batch:       104 of       772\t|\tloss: 0.791667\n",
      "Training Epoch 7  13.6% | batch:       105 of       772\t|\tloss: 0.822716\n",
      "Training Epoch 7  13.7% | batch:       106 of       772\t|\tloss: 0.819424\n",
      "Training Epoch 7  13.9% | batch:       107 of       772\t|\tloss: 0.689795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  14.0% | batch:       108 of       772\t|\tloss: 0.655503\n",
      "Training Epoch 7  14.1% | batch:       109 of       772\t|\tloss: 0.95432\n",
      "Training Epoch 7  14.2% | batch:       110 of       772\t|\tloss: 0.966292\n",
      "Training Epoch 7  14.4% | batch:       111 of       772\t|\tloss: 0.783386\n",
      "Training Epoch 7  14.5% | batch:       112 of       772\t|\tloss: 1.15505\n",
      "Training Epoch 7  14.6% | batch:       113 of       772\t|\tloss: 0.868421\n",
      "Training Epoch 7  14.8% | batch:       114 of       772\t|\tloss: 1.11942\n",
      "Training Epoch 7  14.9% | batch:       115 of       772\t|\tloss: 0.811814\n",
      "Training Epoch 7  15.0% | batch:       116 of       772\t|\tloss: 0.578733\n",
      "Training Epoch 7  15.2% | batch:       117 of       772\t|\tloss: 0.989617\n",
      "Training Epoch 7  15.3% | batch:       118 of       772\t|\tloss: 0.591498\n",
      "Training Epoch 7  15.4% | batch:       119 of       772\t|\tloss: 0.928943\n",
      "Training Epoch 7  15.5% | batch:       120 of       772\t|\tloss: 0.754507\n",
      "Training Epoch 7  15.7% | batch:       121 of       772\t|\tloss: 1.11608\n",
      "Training Epoch 7  15.8% | batch:       122 of       772\t|\tloss: 0.704958\n",
      "Training Epoch 7  15.9% | batch:       123 of       772\t|\tloss: 0.781218\n",
      "Training Epoch 7  16.1% | batch:       124 of       772\t|\tloss: 0.801139\n",
      "Training Epoch 7  16.2% | batch:       125 of       772\t|\tloss: 0.769018\n",
      "Training Epoch 7  16.3% | batch:       126 of       772\t|\tloss: 0.574012\n",
      "Training Epoch 7  16.5% | batch:       127 of       772\t|\tloss: 0.731623\n",
      "Training Epoch 7  16.6% | batch:       128 of       772\t|\tloss: 1.02654\n",
      "Training Epoch 7  16.7% | batch:       129 of       772\t|\tloss: 0.934457\n",
      "Training Epoch 7  16.8% | batch:       130 of       772\t|\tloss: 0.794093\n",
      "Training Epoch 7  17.0% | batch:       131 of       772\t|\tloss: 0.469928\n",
      "Training Epoch 7  17.1% | batch:       132 of       772\t|\tloss: 0.761066\n",
      "Training Epoch 7  17.2% | batch:       133 of       772\t|\tloss: 0.62488\n",
      "Training Epoch 7  17.4% | batch:       134 of       772\t|\tloss: 0.828234\n",
      "Training Epoch 7  17.5% | batch:       135 of       772\t|\tloss: 0.71862\n",
      "Training Epoch 7  17.6% | batch:       136 of       772\t|\tloss: 0.897874\n",
      "Training Epoch 7  17.7% | batch:       137 of       772\t|\tloss: 0.955325\n",
      "Training Epoch 7  17.9% | batch:       138 of       772\t|\tloss: 0.908281\n",
      "Training Epoch 7  18.0% | batch:       139 of       772\t|\tloss: 0.771717\n",
      "Training Epoch 7  18.1% | batch:       140 of       772\t|\tloss: 0.864165\n",
      "Training Epoch 7  18.3% | batch:       141 of       772\t|\tloss: 0.858881\n",
      "Training Epoch 7  18.4% | batch:       142 of       772\t|\tloss: 0.776773\n",
      "Training Epoch 7  18.5% | batch:       143 of       772\t|\tloss: 0.636305\n",
      "Training Epoch 7  18.7% | batch:       144 of       772\t|\tloss: 0.512931\n",
      "Training Epoch 7  18.8% | batch:       145 of       772\t|\tloss: 0.783577\n",
      "Training Epoch 7  18.9% | batch:       146 of       772\t|\tloss: 0.591108\n",
      "Training Epoch 7  19.0% | batch:       147 of       772\t|\tloss: 0.841076\n",
      "Training Epoch 7  19.2% | batch:       148 of       772\t|\tloss: 0.927643\n",
      "Training Epoch 7  19.3% | batch:       149 of       772\t|\tloss: 0.68454\n",
      "Training Epoch 7  19.4% | batch:       150 of       772\t|\tloss: 0.863711\n",
      "Training Epoch 7  19.6% | batch:       151 of       772\t|\tloss: 0.642806\n",
      "Training Epoch 7  19.7% | batch:       152 of       772\t|\tloss: 0.628681\n",
      "Training Epoch 7  19.8% | batch:       153 of       772\t|\tloss: 0.724678\n",
      "Training Epoch 7  19.9% | batch:       154 of       772\t|\tloss: 0.686682\n",
      "Training Epoch 7  20.1% | batch:       155 of       772\t|\tloss: 0.70661\n",
      "Training Epoch 7  20.2% | batch:       156 of       772\t|\tloss: 0.438118\n",
      "Training Epoch 7  20.3% | batch:       157 of       772\t|\tloss: 0.546471\n",
      "Training Epoch 7  20.5% | batch:       158 of       772\t|\tloss: 0.471517\n",
      "Training Epoch 7  20.6% | batch:       159 of       772\t|\tloss: 0.884045\n",
      "Training Epoch 7  20.7% | batch:       160 of       772\t|\tloss: 0.793375\n",
      "Training Epoch 7  20.9% | batch:       161 of       772\t|\tloss: 0.67329\n",
      "Training Epoch 7  21.0% | batch:       162 of       772\t|\tloss: 0.681198\n",
      "Training Epoch 7  21.1% | batch:       163 of       772\t|\tloss: 0.59783\n",
      "Training Epoch 7  21.2% | batch:       164 of       772\t|\tloss: 0.456393\n",
      "Training Epoch 7  21.4% | batch:       165 of       772\t|\tloss: 0.563262\n",
      "Training Epoch 7  21.5% | batch:       166 of       772\t|\tloss: 0.54107\n",
      "Training Epoch 7  21.6% | batch:       167 of       772\t|\tloss: 0.72298\n",
      "Training Epoch 7  21.8% | batch:       168 of       772\t|\tloss: 0.777525\n",
      "Training Epoch 7  21.9% | batch:       169 of       772\t|\tloss: 0.738499\n",
      "Training Epoch 7  22.0% | batch:       170 of       772\t|\tloss: 0.794237\n",
      "Training Epoch 7  22.2% | batch:       171 of       772\t|\tloss: 0.708291\n",
      "Training Epoch 7  22.3% | batch:       172 of       772\t|\tloss: 0.765725\n",
      "Training Epoch 7  22.4% | batch:       173 of       772\t|\tloss: 0.750496\n",
      "Training Epoch 7  22.5% | batch:       174 of       772\t|\tloss: 0.821586\n",
      "Training Epoch 7  22.7% | batch:       175 of       772\t|\tloss: 0.63953\n",
      "Training Epoch 7  22.8% | batch:       176 of       772\t|\tloss: 0.941495\n",
      "Training Epoch 7  22.9% | batch:       177 of       772\t|\tloss: 0.982067\n",
      "Training Epoch 7  23.1% | batch:       178 of       772\t|\tloss: 0.858278\n",
      "Training Epoch 7  23.2% | batch:       179 of       772\t|\tloss: 0.776899\n",
      "Training Epoch 7  23.3% | batch:       180 of       772\t|\tloss: 0.919524\n",
      "Training Epoch 7  23.4% | batch:       181 of       772\t|\tloss: 0.663965\n",
      "Training Epoch 7  23.6% | batch:       182 of       772\t|\tloss: 0.83244\n",
      "Training Epoch 7  23.7% | batch:       183 of       772\t|\tloss: 0.634613\n",
      "Training Epoch 7  23.8% | batch:       184 of       772\t|\tloss: 0.937628\n",
      "Training Epoch 7  24.0% | batch:       185 of       772\t|\tloss: 0.703232\n",
      "Training Epoch 7  24.1% | batch:       186 of       772\t|\tloss: 0.637713\n",
      "Training Epoch 7  24.2% | batch:       187 of       772\t|\tloss: 0.682524\n",
      "Training Epoch 7  24.4% | batch:       188 of       772\t|\tloss: 0.672598\n",
      "Training Epoch 7  24.5% | batch:       189 of       772\t|\tloss: 0.636687\n",
      "Training Epoch 7  24.6% | batch:       190 of       772\t|\tloss: 0.667341\n",
      "Training Epoch 7  24.7% | batch:       191 of       772\t|\tloss: 0.561103\n",
      "Training Epoch 7  24.9% | batch:       192 of       772\t|\tloss: 0.883599\n",
      "Training Epoch 7  25.0% | batch:       193 of       772\t|\tloss: 0.900586\n",
      "Training Epoch 7  25.1% | batch:       194 of       772\t|\tloss: 0.72386\n",
      "Training Epoch 7  25.3% | batch:       195 of       772\t|\tloss: 0.753656\n",
      "Training Epoch 7  25.4% | batch:       196 of       772\t|\tloss: 0.743084\n",
      "Training Epoch 7  25.5% | batch:       197 of       772\t|\tloss: 0.550977\n",
      "Training Epoch 7  25.6% | batch:       198 of       772\t|\tloss: 0.677499\n",
      "Training Epoch 7  25.8% | batch:       199 of       772\t|\tloss: 0.758052\n",
      "Training Epoch 7  25.9% | batch:       200 of       772\t|\tloss: 1.11671\n",
      "Training Epoch 7  26.0% | batch:       201 of       772\t|\tloss: 0.989643\n",
      "Training Epoch 7  26.2% | batch:       202 of       772\t|\tloss: 0.875015\n",
      "Training Epoch 7  26.3% | batch:       203 of       772\t|\tloss: 0.779697\n",
      "Training Epoch 7  26.4% | batch:       204 of       772\t|\tloss: 0.510199\n",
      "Training Epoch 7  26.6% | batch:       205 of       772\t|\tloss: 0.543324\n",
      "Training Epoch 7  26.7% | batch:       206 of       772\t|\tloss: 0.797368\n",
      "Training Epoch 7  26.8% | batch:       207 of       772\t|\tloss: 0.789513\n",
      "Training Epoch 7  26.9% | batch:       208 of       772\t|\tloss: 0.545393\n",
      "Training Epoch 7  27.1% | batch:       209 of       772\t|\tloss: 0.622609\n",
      "Training Epoch 7  27.2% | batch:       210 of       772\t|\tloss: 0.773722\n",
      "Training Epoch 7  27.3% | batch:       211 of       772\t|\tloss: 0.8997\n",
      "Training Epoch 7  27.5% | batch:       212 of       772\t|\tloss: 0.654646\n",
      "Training Epoch 7  27.6% | batch:       213 of       772\t|\tloss: 0.876201\n",
      "Training Epoch 7  27.7% | batch:       214 of       772\t|\tloss: 0.530363\n",
      "Training Epoch 7  27.8% | batch:       215 of       772\t|\tloss: 0.728061\n",
      "Training Epoch 7  28.0% | batch:       216 of       772\t|\tloss: 1.6997\n",
      "Training Epoch 7  28.1% | batch:       217 of       772\t|\tloss: 0.743652\n",
      "Training Epoch 7  28.2% | batch:       218 of       772\t|\tloss: 0.713524\n",
      "Training Epoch 7  28.4% | batch:       219 of       772\t|\tloss: 0.686492\n",
      "Training Epoch 7  28.5% | batch:       220 of       772\t|\tloss: 0.666399\n",
      "Training Epoch 7  28.6% | batch:       221 of       772\t|\tloss: 0.752184\n",
      "Training Epoch 7  28.8% | batch:       222 of       772\t|\tloss: 0.578937\n",
      "Training Epoch 7  28.9% | batch:       223 of       772\t|\tloss: 0.47415\n",
      "Training Epoch 7  29.0% | batch:       224 of       772\t|\tloss: 0.460053\n",
      "Training Epoch 7  29.1% | batch:       225 of       772\t|\tloss: 0.614846\n",
      "Training Epoch 7  29.3% | batch:       226 of       772\t|\tloss: 0.692778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  29.4% | batch:       227 of       772\t|\tloss: 0.55981\n",
      "Training Epoch 7  29.5% | batch:       228 of       772\t|\tloss: 0.792565\n",
      "Training Epoch 7  29.7% | batch:       229 of       772\t|\tloss: 0.933625\n",
      "Training Epoch 7  29.8% | batch:       230 of       772\t|\tloss: 0.707902\n",
      "Training Epoch 7  29.9% | batch:       231 of       772\t|\tloss: 0.644838\n",
      "Training Epoch 7  30.1% | batch:       232 of       772\t|\tloss: 0.783511\n",
      "Training Epoch 7  30.2% | batch:       233 of       772\t|\tloss: 0.685121\n",
      "Training Epoch 7  30.3% | batch:       234 of       772\t|\tloss: 0.709148\n",
      "Training Epoch 7  30.4% | batch:       235 of       772\t|\tloss: 0.662909\n",
      "Training Epoch 7  30.6% | batch:       236 of       772\t|\tloss: 0.878504\n",
      "Training Epoch 7  30.7% | batch:       237 of       772\t|\tloss: 0.755662\n",
      "Training Epoch 7  30.8% | batch:       238 of       772\t|\tloss: 1.09279\n",
      "Training Epoch 7  31.0% | batch:       239 of       772\t|\tloss: 0.84295\n",
      "Training Epoch 7  31.1% | batch:       240 of       772\t|\tloss: 0.773338\n",
      "Training Epoch 7  31.2% | batch:       241 of       772\t|\tloss: 1.03302\n",
      "Training Epoch 7  31.3% | batch:       242 of       772\t|\tloss: 1.37587\n",
      "Training Epoch 7  31.5% | batch:       243 of       772\t|\tloss: 0.973361\n",
      "Training Epoch 7  31.6% | batch:       244 of       772\t|\tloss: 0.738628\n",
      "Training Epoch 7  31.7% | batch:       245 of       772\t|\tloss: 1.02393\n",
      "Training Epoch 7  31.9% | batch:       246 of       772\t|\tloss: 0.874885\n",
      "Training Epoch 7  32.0% | batch:       247 of       772\t|\tloss: 0.76451\n",
      "Training Epoch 7  32.1% | batch:       248 of       772\t|\tloss: 0.693399\n",
      "Training Epoch 7  32.3% | batch:       249 of       772\t|\tloss: 0.979845\n",
      "Training Epoch 7  32.4% | batch:       250 of       772\t|\tloss: 0.940565\n",
      "Training Epoch 7  32.5% | batch:       251 of       772\t|\tloss: 0.554921\n",
      "Training Epoch 7  32.6% | batch:       252 of       772\t|\tloss: 0.722914\n",
      "Training Epoch 7  32.8% | batch:       253 of       772\t|\tloss: 0.658659\n",
      "Training Epoch 7  32.9% | batch:       254 of       772\t|\tloss: 0.74238\n",
      "Training Epoch 7  33.0% | batch:       255 of       772\t|\tloss: 0.779876\n",
      "Training Epoch 7  33.2% | batch:       256 of       772\t|\tloss: 0.753345\n",
      "Training Epoch 7  33.3% | batch:       257 of       772\t|\tloss: 0.702738\n",
      "Training Epoch 7  33.4% | batch:       258 of       772\t|\tloss: 0.747509\n",
      "Training Epoch 7  33.5% | batch:       259 of       772\t|\tloss: 0.74938\n",
      "Training Epoch 7  33.7% | batch:       260 of       772\t|\tloss: 0.651345\n",
      "Training Epoch 7  33.8% | batch:       261 of       772\t|\tloss: 0.609427\n",
      "Training Epoch 7  33.9% | batch:       262 of       772\t|\tloss: 0.620179\n",
      "Training Epoch 7  34.1% | batch:       263 of       772\t|\tloss: 0.680889\n",
      "Training Epoch 7  34.2% | batch:       264 of       772\t|\tloss: 0.558915\n",
      "Training Epoch 7  34.3% | batch:       265 of       772\t|\tloss: 0.538979\n",
      "Training Epoch 7  34.5% | batch:       266 of       772\t|\tloss: 0.857106\n",
      "Training Epoch 7  34.6% | batch:       267 of       772\t|\tloss: 0.805191\n",
      "Training Epoch 7  34.7% | batch:       268 of       772\t|\tloss: 0.841818\n",
      "Training Epoch 7  34.8% | batch:       269 of       772\t|\tloss: 0.645488\n",
      "Training Epoch 7  35.0% | batch:       270 of       772\t|\tloss: 0.859436\n",
      "Training Epoch 7  35.1% | batch:       271 of       772\t|\tloss: 0.746912\n",
      "Training Epoch 7  35.2% | batch:       272 of       772\t|\tloss: 0.926364\n",
      "Training Epoch 7  35.4% | batch:       273 of       772\t|\tloss: 0.858207\n",
      "Training Epoch 7  35.5% | batch:       274 of       772\t|\tloss: 0.841191\n",
      "Training Epoch 7  35.6% | batch:       275 of       772\t|\tloss: 0.692038\n",
      "Training Epoch 7  35.8% | batch:       276 of       772\t|\tloss: 0.510135\n",
      "Training Epoch 7  35.9% | batch:       277 of       772\t|\tloss: 1.71716\n",
      "Training Epoch 7  36.0% | batch:       278 of       772\t|\tloss: 0.596414\n",
      "Training Epoch 7  36.1% | batch:       279 of       772\t|\tloss: 0.575676\n",
      "Training Epoch 7  36.3% | batch:       280 of       772\t|\tloss: 0.808331\n",
      "Training Epoch 7  36.4% | batch:       281 of       772\t|\tloss: 0.650879\n",
      "Training Epoch 7  36.5% | batch:       282 of       772\t|\tloss: 0.519673\n",
      "Training Epoch 7  36.7% | batch:       283 of       772\t|\tloss: 0.54243\n",
      "Training Epoch 7  36.8% | batch:       284 of       772\t|\tloss: 0.480821\n",
      "Training Epoch 7  36.9% | batch:       285 of       772\t|\tloss: 0.994802\n",
      "Training Epoch 7  37.0% | batch:       286 of       772\t|\tloss: 0.482014\n",
      "Training Epoch 7  37.2% | batch:       287 of       772\t|\tloss: 0.606458\n",
      "Training Epoch 7  37.3% | batch:       288 of       772\t|\tloss: 0.752583\n",
      "Training Epoch 7  37.4% | batch:       289 of       772\t|\tloss: 0.674619\n",
      "Training Epoch 7  37.6% | batch:       290 of       772\t|\tloss: 0.635008\n",
      "Training Epoch 7  37.7% | batch:       291 of       772\t|\tloss: 0.470097\n",
      "Training Epoch 7  37.8% | batch:       292 of       772\t|\tloss: 1.0241\n",
      "Training Epoch 7  38.0% | batch:       293 of       772\t|\tloss: 1.1409\n",
      "Training Epoch 7  38.1% | batch:       294 of       772\t|\tloss: 0.836254\n",
      "Training Epoch 7  38.2% | batch:       295 of       772\t|\tloss: 0.795379\n",
      "Training Epoch 7  38.3% | batch:       296 of       772\t|\tloss: 1.03608\n",
      "Training Epoch 7  38.5% | batch:       297 of       772\t|\tloss: 0.977092\n",
      "Training Epoch 7  38.6% | batch:       298 of       772\t|\tloss: 1.13561\n",
      "Training Epoch 7  38.7% | batch:       299 of       772\t|\tloss: 0.978001\n",
      "Training Epoch 7  38.9% | batch:       300 of       772\t|\tloss: 0.475408\n",
      "Training Epoch 7  39.0% | batch:       301 of       772\t|\tloss: 0.769317\n",
      "Training Epoch 7  39.1% | batch:       302 of       772\t|\tloss: 0.666991\n",
      "Training Epoch 7  39.2% | batch:       303 of       772\t|\tloss: 0.745013\n",
      "Training Epoch 7  39.4% | batch:       304 of       772\t|\tloss: 0.850122\n",
      "Training Epoch 7  39.5% | batch:       305 of       772\t|\tloss: 0.706491\n",
      "Training Epoch 7  39.6% | batch:       306 of       772\t|\tloss: 0.668844\n",
      "Training Epoch 7  39.8% | batch:       307 of       772\t|\tloss: 0.640688\n",
      "Training Epoch 7  39.9% | batch:       308 of       772\t|\tloss: 0.942533\n",
      "Training Epoch 7  40.0% | batch:       309 of       772\t|\tloss: 0.842894\n",
      "Training Epoch 7  40.2% | batch:       310 of       772\t|\tloss: 0.615719\n",
      "Training Epoch 7  40.3% | batch:       311 of       772\t|\tloss: 0.680921\n",
      "Training Epoch 7  40.4% | batch:       312 of       772\t|\tloss: 0.617967\n",
      "Training Epoch 7  40.5% | batch:       313 of       772\t|\tloss: 0.631146\n",
      "Training Epoch 7  40.7% | batch:       314 of       772\t|\tloss: 0.466265\n",
      "Training Epoch 7  40.8% | batch:       315 of       772\t|\tloss: 0.471165\n",
      "Training Epoch 7  40.9% | batch:       316 of       772\t|\tloss: 0.619642\n",
      "Training Epoch 7  41.1% | batch:       317 of       772\t|\tloss: 0.72025\n",
      "Training Epoch 7  41.2% | batch:       318 of       772\t|\tloss: 0.523626\n",
      "Training Epoch 7  41.3% | batch:       319 of       772\t|\tloss: 0.55237\n",
      "Training Epoch 7  41.5% | batch:       320 of       772\t|\tloss: 0.797978\n",
      "Training Epoch 7  41.6% | batch:       321 of       772\t|\tloss: 1.04508\n",
      "Training Epoch 7  41.7% | batch:       322 of       772\t|\tloss: 0.708269\n",
      "Training Epoch 7  41.8% | batch:       323 of       772\t|\tloss: 0.684932\n",
      "Training Epoch 7  42.0% | batch:       324 of       772\t|\tloss: 0.548413\n",
      "Training Epoch 7  42.1% | batch:       325 of       772\t|\tloss: 0.765698\n",
      "Training Epoch 7  42.2% | batch:       326 of       772\t|\tloss: 0.9068\n",
      "Training Epoch 7  42.4% | batch:       327 of       772\t|\tloss: 0.636358\n",
      "Training Epoch 7  42.5% | batch:       328 of       772\t|\tloss: 0.482891\n",
      "Training Epoch 7  42.6% | batch:       329 of       772\t|\tloss: 0.848921\n",
      "Training Epoch 7  42.7% | batch:       330 of       772\t|\tloss: 0.618451\n",
      "Training Epoch 7  42.9% | batch:       331 of       772\t|\tloss: 0.751433\n",
      "Training Epoch 7  43.0% | batch:       332 of       772\t|\tloss: 0.729514\n",
      "Training Epoch 7  43.1% | batch:       333 of       772\t|\tloss: 0.851716\n",
      "Training Epoch 7  43.3% | batch:       334 of       772\t|\tloss: 0.639983\n",
      "Training Epoch 7  43.4% | batch:       335 of       772\t|\tloss: 0.64133\n",
      "Training Epoch 7  43.5% | batch:       336 of       772\t|\tloss: 0.719698\n",
      "Training Epoch 7  43.7% | batch:       337 of       772\t|\tloss: 0.473249\n",
      "Training Epoch 7  43.8% | batch:       338 of       772\t|\tloss: 1.01739\n",
      "Training Epoch 7  43.9% | batch:       339 of       772\t|\tloss: 0.586246\n",
      "Training Epoch 7  44.0% | batch:       340 of       772\t|\tloss: 0.661797\n",
      "Training Epoch 7  44.2% | batch:       341 of       772\t|\tloss: 0.628992\n",
      "Training Epoch 7  44.3% | batch:       342 of       772\t|\tloss: 0.599882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  44.4% | batch:       343 of       772\t|\tloss: 0.774729\n",
      "Training Epoch 7  44.6% | batch:       344 of       772\t|\tloss: 0.654008\n",
      "Training Epoch 7  44.7% | batch:       345 of       772\t|\tloss: 0.787694\n",
      "Training Epoch 7  44.8% | batch:       346 of       772\t|\tloss: 0.703017\n",
      "Training Epoch 7  44.9% | batch:       347 of       772\t|\tloss: 1.04235\n",
      "Training Epoch 7  45.1% | batch:       348 of       772\t|\tloss: 0.700053\n",
      "Training Epoch 7  45.2% | batch:       349 of       772\t|\tloss: 1.03035\n",
      "Training Epoch 7  45.3% | batch:       350 of       772\t|\tloss: 0.672303\n",
      "Training Epoch 7  45.5% | batch:       351 of       772\t|\tloss: 0.692681\n",
      "Training Epoch 7  45.6% | batch:       352 of       772\t|\tloss: 0.460674\n",
      "Training Epoch 7  45.7% | batch:       353 of       772\t|\tloss: 0.663009\n",
      "Training Epoch 7  45.9% | batch:       354 of       772\t|\tloss: 0.701225\n",
      "Training Epoch 7  46.0% | batch:       355 of       772\t|\tloss: 0.662233\n",
      "Training Epoch 7  46.1% | batch:       356 of       772\t|\tloss: 0.58756\n",
      "Training Epoch 7  46.2% | batch:       357 of       772\t|\tloss: 0.615605\n",
      "Training Epoch 7  46.4% | batch:       358 of       772\t|\tloss: 0.484289\n",
      "Training Epoch 7  46.5% | batch:       359 of       772\t|\tloss: 0.744138\n",
      "Training Epoch 7  46.6% | batch:       360 of       772\t|\tloss: 0.538745\n",
      "Training Epoch 7  46.8% | batch:       361 of       772\t|\tloss: 0.530861\n",
      "Training Epoch 7  46.9% | batch:       362 of       772\t|\tloss: 0.908077\n",
      "Training Epoch 7  47.0% | batch:       363 of       772\t|\tloss: 0.656053\n",
      "Training Epoch 7  47.2% | batch:       364 of       772\t|\tloss: 0.797044\n",
      "Training Epoch 7  47.3% | batch:       365 of       772\t|\tloss: 0.724861\n",
      "Training Epoch 7  47.4% | batch:       366 of       772\t|\tloss: 0.707185\n",
      "Training Epoch 7  47.5% | batch:       367 of       772\t|\tloss: 0.850123\n",
      "Training Epoch 7  47.7% | batch:       368 of       772\t|\tloss: 0.583898\n",
      "Training Epoch 7  47.8% | batch:       369 of       772\t|\tloss: 0.699125\n",
      "Training Epoch 7  47.9% | batch:       370 of       772\t|\tloss: 0.604158\n",
      "Training Epoch 7  48.1% | batch:       371 of       772\t|\tloss: 0.694315\n",
      "Training Epoch 7  48.2% | batch:       372 of       772\t|\tloss: 0.790874\n",
      "Training Epoch 7  48.3% | batch:       373 of       772\t|\tloss: 0.809216\n",
      "Training Epoch 7  48.4% | batch:       374 of       772\t|\tloss: 0.676555\n",
      "Training Epoch 7  48.6% | batch:       375 of       772\t|\tloss: 0.770292\n",
      "Training Epoch 7  48.7% | batch:       376 of       772\t|\tloss: 0.583729\n",
      "Training Epoch 7  48.8% | batch:       377 of       772\t|\tloss: 1.01009\n",
      "Training Epoch 7  49.0% | batch:       378 of       772\t|\tloss: 1.01859\n",
      "Training Epoch 7  49.1% | batch:       379 of       772\t|\tloss: 0.712597\n",
      "Training Epoch 7  49.2% | batch:       380 of       772\t|\tloss: 0.731421\n",
      "Training Epoch 7  49.4% | batch:       381 of       772\t|\tloss: 0.566985\n",
      "Training Epoch 7  49.5% | batch:       382 of       772\t|\tloss: 0.892158\n",
      "Training Epoch 7  49.6% | batch:       383 of       772\t|\tloss: 0.689321\n",
      "Training Epoch 7  49.7% | batch:       384 of       772\t|\tloss: 0.808945\n",
      "Training Epoch 7  49.9% | batch:       385 of       772\t|\tloss: 1.12674\n",
      "Training Epoch 7  50.0% | batch:       386 of       772\t|\tloss: 0.844063\n",
      "Training Epoch 7  50.1% | batch:       387 of       772\t|\tloss: 0.902121\n",
      "Training Epoch 7  50.3% | batch:       388 of       772\t|\tloss: 1.01167\n",
      "Training Epoch 7  50.4% | batch:       389 of       772\t|\tloss: 0.550463\n",
      "Training Epoch 7  50.5% | batch:       390 of       772\t|\tloss: 0.725142\n",
      "Training Epoch 7  50.6% | batch:       391 of       772\t|\tloss: 1.12859\n",
      "Training Epoch 7  50.8% | batch:       392 of       772\t|\tloss: 1.01589\n",
      "Training Epoch 7  50.9% | batch:       393 of       772\t|\tloss: 1.11858\n",
      "Training Epoch 7  51.0% | batch:       394 of       772\t|\tloss: 0.606562\n",
      "Training Epoch 7  51.2% | batch:       395 of       772\t|\tloss: 0.80325\n",
      "Training Epoch 7  51.3% | batch:       396 of       772\t|\tloss: 0.743663\n",
      "Training Epoch 7  51.4% | batch:       397 of       772\t|\tloss: 0.395406\n",
      "Training Epoch 7  51.6% | batch:       398 of       772\t|\tloss: 0.984746\n",
      "Training Epoch 7  51.7% | batch:       399 of       772\t|\tloss: 1.33095\n",
      "Training Epoch 7  51.8% | batch:       400 of       772\t|\tloss: 1.1982\n",
      "Training Epoch 7  51.9% | batch:       401 of       772\t|\tloss: 0.540092\n",
      "Training Epoch 7  52.1% | batch:       402 of       772\t|\tloss: 0.672452\n",
      "Training Epoch 7  52.2% | batch:       403 of       772\t|\tloss: 0.903433\n",
      "Training Epoch 7  52.3% | batch:       404 of       772\t|\tloss: 0.821532\n",
      "Training Epoch 7  52.5% | batch:       405 of       772\t|\tloss: 0.652401\n",
      "Training Epoch 7  52.6% | batch:       406 of       772\t|\tloss: 0.830222\n",
      "Training Epoch 7  52.7% | batch:       407 of       772\t|\tloss: 1.07534\n",
      "Training Epoch 7  52.8% | batch:       408 of       772\t|\tloss: 0.704483\n",
      "Training Epoch 7  53.0% | batch:       409 of       772\t|\tloss: 0.871928\n",
      "Training Epoch 7  53.1% | batch:       410 of       772\t|\tloss: 0.940388\n",
      "Training Epoch 7  53.2% | batch:       411 of       772\t|\tloss: 1.02829\n",
      "Training Epoch 7  53.4% | batch:       412 of       772\t|\tloss: 0.741226\n",
      "Training Epoch 7  53.5% | batch:       413 of       772\t|\tloss: 0.599456\n",
      "Training Epoch 7  53.6% | batch:       414 of       772\t|\tloss: 0.957984\n",
      "Training Epoch 7  53.8% | batch:       415 of       772\t|\tloss: 0.932456\n",
      "Training Epoch 7  53.9% | batch:       416 of       772\t|\tloss: 0.582803\n",
      "Training Epoch 7  54.0% | batch:       417 of       772\t|\tloss: 0.5166\n",
      "Training Epoch 7  54.1% | batch:       418 of       772\t|\tloss: 0.867899\n",
      "Training Epoch 7  54.3% | batch:       419 of       772\t|\tloss: 0.972241\n",
      "Training Epoch 7  54.4% | batch:       420 of       772\t|\tloss: 1.14651\n",
      "Training Epoch 7  54.5% | batch:       421 of       772\t|\tloss: 0.565513\n",
      "Training Epoch 7  54.7% | batch:       422 of       772\t|\tloss: 0.853716\n",
      "Training Epoch 7  54.8% | batch:       423 of       772\t|\tloss: 1.08574\n",
      "Training Epoch 7  54.9% | batch:       424 of       772\t|\tloss: 0.861037\n",
      "Training Epoch 7  55.1% | batch:       425 of       772\t|\tloss: 0.549511\n",
      "Training Epoch 7  55.2% | batch:       426 of       772\t|\tloss: 0.689962\n",
      "Training Epoch 7  55.3% | batch:       427 of       772\t|\tloss: 0.833943\n",
      "Training Epoch 7  55.4% | batch:       428 of       772\t|\tloss: 0.853676\n",
      "Training Epoch 7  55.6% | batch:       429 of       772\t|\tloss: 0.674412\n",
      "Training Epoch 7  55.7% | batch:       430 of       772\t|\tloss: 0.480208\n",
      "Training Epoch 7  55.8% | batch:       431 of       772\t|\tloss: 1.14416\n",
      "Training Epoch 7  56.0% | batch:       432 of       772\t|\tloss: 0.697444\n",
      "Training Epoch 7  56.1% | batch:       433 of       772\t|\tloss: 0.558943\n",
      "Training Epoch 7  56.2% | batch:       434 of       772\t|\tloss: 0.620811\n",
      "Training Epoch 7  56.3% | batch:       435 of       772\t|\tloss: 0.769293\n",
      "Training Epoch 7  56.5% | batch:       436 of       772\t|\tloss: 0.938518\n",
      "Training Epoch 7  56.6% | batch:       437 of       772\t|\tloss: 0.688159\n",
      "Training Epoch 7  56.7% | batch:       438 of       772\t|\tloss: 0.858677\n",
      "Training Epoch 7  56.9% | batch:       439 of       772\t|\tloss: 1.17753\n",
      "Training Epoch 7  57.0% | batch:       440 of       772\t|\tloss: 0.645265\n",
      "Training Epoch 7  57.1% | batch:       441 of       772\t|\tloss: 0.731243\n",
      "Training Epoch 7  57.3% | batch:       442 of       772\t|\tloss: 1.00572\n",
      "Training Epoch 7  57.4% | batch:       443 of       772\t|\tloss: 0.621212\n",
      "Training Epoch 7  57.5% | batch:       444 of       772\t|\tloss: 0.579477\n",
      "Training Epoch 7  57.6% | batch:       445 of       772\t|\tloss: 0.677568\n",
      "Training Epoch 7  57.8% | batch:       446 of       772\t|\tloss: 0.635757\n",
      "Training Epoch 7  57.9% | batch:       447 of       772\t|\tloss: 0.617018\n",
      "Training Epoch 7  58.0% | batch:       448 of       772\t|\tloss: 0.751068\n",
      "Training Epoch 7  58.2% | batch:       449 of       772\t|\tloss: 0.796859\n",
      "Training Epoch 7  58.3% | batch:       450 of       772\t|\tloss: 0.557965\n",
      "Training Epoch 7  58.4% | batch:       451 of       772\t|\tloss: 0.487643\n",
      "Training Epoch 7  58.5% | batch:       452 of       772\t|\tloss: 0.66108\n",
      "Training Epoch 7  58.7% | batch:       453 of       772\t|\tloss: 1.34575\n",
      "Training Epoch 7  58.8% | batch:       454 of       772\t|\tloss: 1.17961\n",
      "Training Epoch 7  58.9% | batch:       455 of       772\t|\tloss: 0.562395\n",
      "Training Epoch 7  59.1% | batch:       456 of       772\t|\tloss: 0.663912\n",
      "Training Epoch 7  59.2% | batch:       457 of       772\t|\tloss: 0.800319\n",
      "Training Epoch 7  59.3% | batch:       458 of       772\t|\tloss: 0.688251\n",
      "Training Epoch 7  59.5% | batch:       459 of       772\t|\tloss: 0.65674\n",
      "Training Epoch 7  59.6% | batch:       460 of       772\t|\tloss: 0.641165\n",
      "Training Epoch 7  59.7% | batch:       461 of       772\t|\tloss: 0.699749\n",
      "Training Epoch 7  59.8% | batch:       462 of       772\t|\tloss: 0.678885\n",
      "Training Epoch 7  60.0% | batch:       463 of       772\t|\tloss: 0.639137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  60.1% | batch:       464 of       772\t|\tloss: 0.742831\n",
      "Training Epoch 7  60.2% | batch:       465 of       772\t|\tloss: 0.538885\n",
      "Training Epoch 7  60.4% | batch:       466 of       772\t|\tloss: 0.570243\n",
      "Training Epoch 7  60.5% | batch:       467 of       772\t|\tloss: 0.515905\n",
      "Training Epoch 7  60.6% | batch:       468 of       772\t|\tloss: 0.616864\n",
      "Training Epoch 7  60.8% | batch:       469 of       772\t|\tloss: 0.585534\n",
      "Training Epoch 7  60.9% | batch:       470 of       772\t|\tloss: 0.956105\n",
      "Training Epoch 7  61.0% | batch:       471 of       772\t|\tloss: 0.611572\n",
      "Training Epoch 7  61.1% | batch:       472 of       772\t|\tloss: 0.715786\n",
      "Training Epoch 7  61.3% | batch:       473 of       772\t|\tloss: 0.931854\n",
      "Training Epoch 7  61.4% | batch:       474 of       772\t|\tloss: 0.690506\n",
      "Training Epoch 7  61.5% | batch:       475 of       772\t|\tloss: 0.730924\n",
      "Training Epoch 7  61.7% | batch:       476 of       772\t|\tloss: 0.705302\n",
      "Training Epoch 7  61.8% | batch:       477 of       772\t|\tloss: 0.714302\n",
      "Training Epoch 7  61.9% | batch:       478 of       772\t|\tloss: 0.599331\n",
      "Training Epoch 7  62.0% | batch:       479 of       772\t|\tloss: 0.523545\n",
      "Training Epoch 7  62.2% | batch:       480 of       772\t|\tloss: 0.692913\n",
      "Training Epoch 7  62.3% | batch:       481 of       772\t|\tloss: 0.446177\n",
      "Training Epoch 7  62.4% | batch:       482 of       772\t|\tloss: 0.541337\n",
      "Training Epoch 7  62.6% | batch:       483 of       772\t|\tloss: 0.535282\n",
      "Training Epoch 7  62.7% | batch:       484 of       772\t|\tloss: 0.624925\n",
      "Training Epoch 7  62.8% | batch:       485 of       772\t|\tloss: 0.632975\n",
      "Training Epoch 7  63.0% | batch:       486 of       772\t|\tloss: 0.792117\n",
      "Training Epoch 7  63.1% | batch:       487 of       772\t|\tloss: 0.530055\n",
      "Training Epoch 7  63.2% | batch:       488 of       772\t|\tloss: 0.547122\n",
      "Training Epoch 7  63.3% | batch:       489 of       772\t|\tloss: 0.592706\n",
      "Training Epoch 7  63.5% | batch:       490 of       772\t|\tloss: 0.705885\n",
      "Training Epoch 7  63.6% | batch:       491 of       772\t|\tloss: 0.724321\n",
      "Training Epoch 7  63.7% | batch:       492 of       772\t|\tloss: 0.529552\n",
      "Training Epoch 7  63.9% | batch:       493 of       772\t|\tloss: 0.403953\n",
      "Training Epoch 7  64.0% | batch:       494 of       772\t|\tloss: 0.732176\n",
      "Training Epoch 7  64.1% | batch:       495 of       772\t|\tloss: 0.723236\n",
      "Training Epoch 7  64.2% | batch:       496 of       772\t|\tloss: 0.630691\n",
      "Training Epoch 7  64.4% | batch:       497 of       772\t|\tloss: 0.728531\n",
      "Training Epoch 7  64.5% | batch:       498 of       772\t|\tloss: 0.679587\n",
      "Training Epoch 7  64.6% | batch:       499 of       772\t|\tloss: 0.793027\n",
      "Training Epoch 7  64.8% | batch:       500 of       772\t|\tloss: 0.702066\n",
      "Training Epoch 7  64.9% | batch:       501 of       772\t|\tloss: 0.72754\n",
      "Training Epoch 7  65.0% | batch:       502 of       772\t|\tloss: 0.733095\n",
      "Training Epoch 7  65.2% | batch:       503 of       772\t|\tloss: 0.756426\n",
      "Training Epoch 7  65.3% | batch:       504 of       772\t|\tloss: 0.586363\n",
      "Training Epoch 7  65.4% | batch:       505 of       772\t|\tloss: 0.698985\n",
      "Training Epoch 7  65.5% | batch:       506 of       772\t|\tloss: 0.758239\n",
      "Training Epoch 7  65.7% | batch:       507 of       772\t|\tloss: 0.69741\n",
      "Training Epoch 7  65.8% | batch:       508 of       772\t|\tloss: 0.651907\n",
      "Training Epoch 7  65.9% | batch:       509 of       772\t|\tloss: 0.749449\n",
      "Training Epoch 7  66.1% | batch:       510 of       772\t|\tloss: 0.60965\n",
      "Training Epoch 7  66.2% | batch:       511 of       772\t|\tloss: 0.713398\n",
      "Training Epoch 7  66.3% | batch:       512 of       772\t|\tloss: 0.632664\n",
      "Training Epoch 7  66.5% | batch:       513 of       772\t|\tloss: 0.570885\n",
      "Training Epoch 7  66.6% | batch:       514 of       772\t|\tloss: 0.814997\n",
      "Training Epoch 7  66.7% | batch:       515 of       772\t|\tloss: 0.781078\n",
      "Training Epoch 7  66.8% | batch:       516 of       772\t|\tloss: 0.963878\n",
      "Training Epoch 7  67.0% | batch:       517 of       772\t|\tloss: 0.829836\n",
      "Training Epoch 7  67.1% | batch:       518 of       772\t|\tloss: 0.701715\n",
      "Training Epoch 7  67.2% | batch:       519 of       772\t|\tloss: 0.988754\n",
      "Training Epoch 7  67.4% | batch:       520 of       772\t|\tloss: 0.741124\n",
      "Training Epoch 7  67.5% | batch:       521 of       772\t|\tloss: 0.692434\n",
      "Training Epoch 7  67.6% | batch:       522 of       772\t|\tloss: 0.848078\n",
      "Training Epoch 7  67.7% | batch:       523 of       772\t|\tloss: 0.880197\n",
      "Training Epoch 7  67.9% | batch:       524 of       772\t|\tloss: 0.987124\n",
      "Training Epoch 7  68.0% | batch:       525 of       772\t|\tloss: 0.637928\n",
      "Training Epoch 7  68.1% | batch:       526 of       772\t|\tloss: 0.588156\n",
      "Training Epoch 7  68.3% | batch:       527 of       772\t|\tloss: 1.08208\n",
      "Training Epoch 7  68.4% | batch:       528 of       772\t|\tloss: 0.56512\n",
      "Training Epoch 7  68.5% | batch:       529 of       772\t|\tloss: 0.608428\n",
      "Training Epoch 7  68.7% | batch:       530 of       772\t|\tloss: 0.55792\n",
      "Training Epoch 7  68.8% | batch:       531 of       772\t|\tloss: 0.439336\n",
      "Training Epoch 7  68.9% | batch:       532 of       772\t|\tloss: 0.891326\n",
      "Training Epoch 7  69.0% | batch:       533 of       772\t|\tloss: 0.740395\n",
      "Training Epoch 7  69.2% | batch:       534 of       772\t|\tloss: 0.826582\n",
      "Training Epoch 7  69.3% | batch:       535 of       772\t|\tloss: 0.843075\n",
      "Training Epoch 7  69.4% | batch:       536 of       772\t|\tloss: 0.745428\n",
      "Training Epoch 7  69.6% | batch:       537 of       772\t|\tloss: 0.723708\n",
      "Training Epoch 7  69.7% | batch:       538 of       772\t|\tloss: 0.658818\n",
      "Training Epoch 7  69.8% | batch:       539 of       772\t|\tloss: 0.719507\n",
      "Training Epoch 7  69.9% | batch:       540 of       772\t|\tloss: 1.03832\n",
      "Training Epoch 7  70.1% | batch:       541 of       772\t|\tloss: 0.872476\n",
      "Training Epoch 7  70.2% | batch:       542 of       772\t|\tloss: 0.754333\n",
      "Training Epoch 7  70.3% | batch:       543 of       772\t|\tloss: 0.850939\n",
      "Training Epoch 7  70.5% | batch:       544 of       772\t|\tloss: 0.564417\n",
      "Training Epoch 7  70.6% | batch:       545 of       772\t|\tloss: 0.836794\n",
      "Training Epoch 7  70.7% | batch:       546 of       772\t|\tloss: 0.833773\n",
      "Training Epoch 7  70.9% | batch:       547 of       772\t|\tloss: 0.773085\n",
      "Training Epoch 7  71.0% | batch:       548 of       772\t|\tloss: 0.841689\n",
      "Training Epoch 7  71.1% | batch:       549 of       772\t|\tloss: 0.533941\n",
      "Training Epoch 7  71.2% | batch:       550 of       772\t|\tloss: 0.674643\n",
      "Training Epoch 7  71.4% | batch:       551 of       772\t|\tloss: 0.736474\n",
      "Training Epoch 7  71.5% | batch:       552 of       772\t|\tloss: 0.789371\n",
      "Training Epoch 7  71.6% | batch:       553 of       772\t|\tloss: 0.750629\n",
      "Training Epoch 7  71.8% | batch:       554 of       772\t|\tloss: 0.727911\n",
      "Training Epoch 7  71.9% | batch:       555 of       772\t|\tloss: 0.780548\n",
      "Training Epoch 7  72.0% | batch:       556 of       772\t|\tloss: 0.923589\n",
      "Training Epoch 7  72.2% | batch:       557 of       772\t|\tloss: 0.560955\n",
      "Training Epoch 7  72.3% | batch:       558 of       772\t|\tloss: 0.767123\n",
      "Training Epoch 7  72.4% | batch:       559 of       772\t|\tloss: 0.602597\n",
      "Training Epoch 7  72.5% | batch:       560 of       772\t|\tloss: 0.765072\n",
      "Training Epoch 7  72.7% | batch:       561 of       772\t|\tloss: 0.625122\n",
      "Training Epoch 7  72.8% | batch:       562 of       772\t|\tloss: 0.601986\n",
      "Training Epoch 7  72.9% | batch:       563 of       772\t|\tloss: 0.831454\n",
      "Training Epoch 7  73.1% | batch:       564 of       772\t|\tloss: 0.647616\n",
      "Training Epoch 7  73.2% | batch:       565 of       772\t|\tloss: 0.579679\n",
      "Training Epoch 7  73.3% | batch:       566 of       772\t|\tloss: 1.02491\n",
      "Training Epoch 7  73.4% | batch:       567 of       772\t|\tloss: 0.71558\n",
      "Training Epoch 7  73.6% | batch:       568 of       772\t|\tloss: 0.537394\n",
      "Training Epoch 7  73.7% | batch:       569 of       772\t|\tloss: 0.544143\n",
      "Training Epoch 7  73.8% | batch:       570 of       772\t|\tloss: 1.03706\n",
      "Training Epoch 7  74.0% | batch:       571 of       772\t|\tloss: 0.828657\n",
      "Training Epoch 7  74.1% | batch:       572 of       772\t|\tloss: 0.635485\n",
      "Training Epoch 7  74.2% | batch:       573 of       772\t|\tloss: 0.751144\n",
      "Training Epoch 7  74.4% | batch:       574 of       772\t|\tloss: 0.759547\n",
      "Training Epoch 7  74.5% | batch:       575 of       772\t|\tloss: 0.675241\n",
      "Training Epoch 7  74.6% | batch:       576 of       772\t|\tloss: 0.649785\n",
      "Training Epoch 7  74.7% | batch:       577 of       772\t|\tloss: 0.714207\n",
      "Training Epoch 7  74.9% | batch:       578 of       772\t|\tloss: 0.538162\n",
      "Training Epoch 7  75.0% | batch:       579 of       772\t|\tloss: 0.790349\n",
      "Training Epoch 7  75.1% | batch:       580 of       772\t|\tloss: 0.77448\n",
      "Training Epoch 7  75.3% | batch:       581 of       772\t|\tloss: 0.867597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  75.4% | batch:       582 of       772\t|\tloss: 0.520484\n",
      "Training Epoch 7  75.5% | batch:       583 of       772\t|\tloss: 0.89644\n",
      "Training Epoch 7  75.6% | batch:       584 of       772\t|\tloss: 0.59647\n",
      "Training Epoch 7  75.8% | batch:       585 of       772\t|\tloss: 0.603333\n",
      "Training Epoch 7  75.9% | batch:       586 of       772\t|\tloss: 0.774105\n",
      "Training Epoch 7  76.0% | batch:       587 of       772\t|\tloss: 0.777272\n",
      "Training Epoch 7  76.2% | batch:       588 of       772\t|\tloss: 0.648172\n",
      "Training Epoch 7  76.3% | batch:       589 of       772\t|\tloss: 0.519202\n",
      "Training Epoch 7  76.4% | batch:       590 of       772\t|\tloss: 0.720483\n",
      "Training Epoch 7  76.6% | batch:       591 of       772\t|\tloss: 0.605053\n",
      "Training Epoch 7  76.7% | batch:       592 of       772\t|\tloss: 0.575363\n",
      "Training Epoch 7  76.8% | batch:       593 of       772\t|\tloss: 0.743418\n",
      "Training Epoch 7  76.9% | batch:       594 of       772\t|\tloss: 0.494817\n",
      "Training Epoch 7  77.1% | batch:       595 of       772\t|\tloss: 1.06038\n",
      "Training Epoch 7  77.2% | batch:       596 of       772\t|\tloss: 0.996456\n",
      "Training Epoch 7  77.3% | batch:       597 of       772\t|\tloss: 0.919743\n",
      "Training Epoch 7  77.5% | batch:       598 of       772\t|\tloss: 0.566405\n",
      "Training Epoch 7  77.6% | batch:       599 of       772\t|\tloss: 0.930401\n",
      "Training Epoch 7  77.7% | batch:       600 of       772\t|\tloss: 1.2378\n",
      "Training Epoch 7  77.8% | batch:       601 of       772\t|\tloss: 0.709323\n",
      "Training Epoch 7  78.0% | batch:       602 of       772\t|\tloss: 0.839752\n",
      "Training Epoch 7  78.1% | batch:       603 of       772\t|\tloss: 0.512609\n",
      "Training Epoch 7  78.2% | batch:       604 of       772\t|\tloss: 0.833945\n",
      "Training Epoch 7  78.4% | batch:       605 of       772\t|\tloss: 0.487079\n",
      "Training Epoch 7  78.5% | batch:       606 of       772\t|\tloss: 0.665426\n",
      "Training Epoch 7  78.6% | batch:       607 of       772\t|\tloss: 0.596456\n",
      "Training Epoch 7  78.8% | batch:       608 of       772\t|\tloss: 0.697103\n",
      "Training Epoch 7  78.9% | batch:       609 of       772\t|\tloss: 0.741207\n",
      "Training Epoch 7  79.0% | batch:       610 of       772\t|\tloss: 0.834466\n",
      "Training Epoch 7  79.1% | batch:       611 of       772\t|\tloss: 0.669748\n",
      "Training Epoch 7  79.3% | batch:       612 of       772\t|\tloss: 0.751225\n",
      "Training Epoch 7  79.4% | batch:       613 of       772\t|\tloss: 0.638582\n",
      "Training Epoch 7  79.5% | batch:       614 of       772\t|\tloss: 0.694019\n",
      "Training Epoch 7  79.7% | batch:       615 of       772\t|\tloss: 0.56613\n",
      "Training Epoch 7  79.8% | batch:       616 of       772\t|\tloss: 0.768883\n",
      "Training Epoch 7  79.9% | batch:       617 of       772\t|\tloss: 0.815487\n",
      "Training Epoch 7  80.1% | batch:       618 of       772\t|\tloss: 0.812983\n",
      "Training Epoch 7  80.2% | batch:       619 of       772\t|\tloss: 0.599675\n",
      "Training Epoch 7  80.3% | batch:       620 of       772\t|\tloss: 0.733588\n",
      "Training Epoch 7  80.4% | batch:       621 of       772\t|\tloss: 0.619598\n",
      "Training Epoch 7  80.6% | batch:       622 of       772\t|\tloss: 0.589859\n",
      "Training Epoch 7  80.7% | batch:       623 of       772\t|\tloss: 0.651215\n",
      "Training Epoch 7  80.8% | batch:       624 of       772\t|\tloss: 0.912416\n",
      "Training Epoch 7  81.0% | batch:       625 of       772\t|\tloss: 0.432091\n",
      "Training Epoch 7  81.1% | batch:       626 of       772\t|\tloss: 0.774213\n",
      "Training Epoch 7  81.2% | batch:       627 of       772\t|\tloss: 0.647978\n",
      "Training Epoch 7  81.3% | batch:       628 of       772\t|\tloss: 0.554077\n",
      "Training Epoch 7  81.5% | batch:       629 of       772\t|\tloss: 0.541591\n",
      "Training Epoch 7  81.6% | batch:       630 of       772\t|\tloss: 0.635433\n",
      "Training Epoch 7  81.7% | batch:       631 of       772\t|\tloss: 0.397686\n",
      "Training Epoch 7  81.9% | batch:       632 of       772\t|\tloss: 0.588612\n",
      "Training Epoch 7  82.0% | batch:       633 of       772\t|\tloss: 0.567945\n",
      "Training Epoch 7  82.1% | batch:       634 of       772\t|\tloss: 0.54926\n",
      "Training Epoch 7  82.3% | batch:       635 of       772\t|\tloss: 0.724001\n",
      "Training Epoch 7  82.4% | batch:       636 of       772\t|\tloss: 0.882639\n",
      "Training Epoch 7  82.5% | batch:       637 of       772\t|\tloss: 0.601385\n",
      "Training Epoch 7  82.6% | batch:       638 of       772\t|\tloss: 0.498561\n",
      "Training Epoch 7  82.8% | batch:       639 of       772\t|\tloss: 0.778707\n",
      "Training Epoch 7  82.9% | batch:       640 of       772\t|\tloss: 0.810256\n",
      "Training Epoch 7  83.0% | batch:       641 of       772\t|\tloss: 0.643381\n",
      "Training Epoch 7  83.2% | batch:       642 of       772\t|\tloss: 0.953895\n",
      "Training Epoch 7  83.3% | batch:       643 of       772\t|\tloss: 0.778447\n",
      "Training Epoch 7  83.4% | batch:       644 of       772\t|\tloss: 0.636837\n",
      "Training Epoch 7  83.5% | batch:       645 of       772\t|\tloss: 0.490145\n",
      "Training Epoch 7  83.7% | batch:       646 of       772\t|\tloss: 1.24768\n",
      "Training Epoch 7  83.8% | batch:       647 of       772\t|\tloss: 1.39802\n",
      "Training Epoch 7  83.9% | batch:       648 of       772\t|\tloss: 1.04061\n",
      "Training Epoch 7  84.1% | batch:       649 of       772\t|\tloss: 1.10655\n",
      "Training Epoch 7  84.2% | batch:       650 of       772\t|\tloss: 0.722722\n",
      "Training Epoch 7  84.3% | batch:       651 of       772\t|\tloss: 0.980057\n",
      "Training Epoch 7  84.5% | batch:       652 of       772\t|\tloss: 1.11833\n",
      "Training Epoch 7  84.6% | batch:       653 of       772\t|\tloss: 0.765102\n",
      "Training Epoch 7  84.7% | batch:       654 of       772\t|\tloss: 0.666822\n",
      "Training Epoch 7  84.8% | batch:       655 of       772\t|\tloss: 1.04136\n",
      "Training Epoch 7  85.0% | batch:       656 of       772\t|\tloss: 1.23934\n",
      "Training Epoch 7  85.1% | batch:       657 of       772\t|\tloss: 1.0804\n",
      "Training Epoch 7  85.2% | batch:       658 of       772\t|\tloss: 1.06618\n",
      "Training Epoch 7  85.4% | batch:       659 of       772\t|\tloss: 0.736772\n",
      "Training Epoch 7  85.5% | batch:       660 of       772\t|\tloss: 0.834558\n",
      "Training Epoch 7  85.6% | batch:       661 of       772\t|\tloss: 0.934751\n",
      "Training Epoch 7  85.8% | batch:       662 of       772\t|\tloss: 0.56565\n",
      "Training Epoch 7  85.9% | batch:       663 of       772\t|\tloss: 0.549152\n",
      "Training Epoch 7  86.0% | batch:       664 of       772\t|\tloss: 0.778303\n",
      "Training Epoch 7  86.1% | batch:       665 of       772\t|\tloss: 0.594983\n",
      "Training Epoch 7  86.3% | batch:       666 of       772\t|\tloss: 0.621553\n",
      "Training Epoch 7  86.4% | batch:       667 of       772\t|\tloss: 0.520423\n",
      "Training Epoch 7  86.5% | batch:       668 of       772\t|\tloss: 0.662169\n",
      "Training Epoch 7  86.7% | batch:       669 of       772\t|\tloss: 0.739654\n",
      "Training Epoch 7  86.8% | batch:       670 of       772\t|\tloss: 0.628306\n",
      "Training Epoch 7  86.9% | batch:       671 of       772\t|\tloss: 0.78298\n",
      "Training Epoch 7  87.0% | batch:       672 of       772\t|\tloss: 0.677573\n",
      "Training Epoch 7  87.2% | batch:       673 of       772\t|\tloss: 0.986242\n",
      "Training Epoch 7  87.3% | batch:       674 of       772\t|\tloss: 0.906914\n",
      "Training Epoch 7  87.4% | batch:       675 of       772\t|\tloss: 0.696482\n",
      "Training Epoch 7  87.6% | batch:       676 of       772\t|\tloss: 0.642035\n",
      "Training Epoch 7  87.7% | batch:       677 of       772\t|\tloss: 0.595431\n",
      "Training Epoch 7  87.8% | batch:       678 of       772\t|\tloss: 0.676286\n",
      "Training Epoch 7  88.0% | batch:       679 of       772\t|\tloss: 0.867802\n",
      "Training Epoch 7  88.1% | batch:       680 of       772\t|\tloss: 0.655529\n",
      "Training Epoch 7  88.2% | batch:       681 of       772\t|\tloss: 0.717372\n",
      "Training Epoch 7  88.3% | batch:       682 of       772\t|\tloss: 0.857047\n",
      "Training Epoch 7  88.5% | batch:       683 of       772\t|\tloss: 0.68585\n",
      "Training Epoch 7  88.6% | batch:       684 of       772\t|\tloss: 0.646388\n",
      "Training Epoch 7  88.7% | batch:       685 of       772\t|\tloss: 0.754318\n",
      "Training Epoch 7  88.9% | batch:       686 of       772\t|\tloss: 0.731218\n",
      "Training Epoch 7  89.0% | batch:       687 of       772\t|\tloss: 0.736873\n",
      "Training Epoch 7  89.1% | batch:       688 of       772\t|\tloss: 0.721757\n",
      "Training Epoch 7  89.2% | batch:       689 of       772\t|\tloss: 0.693922\n",
      "Training Epoch 7  89.4% | batch:       690 of       772\t|\tloss: 0.514521\n",
      "Training Epoch 7  89.5% | batch:       691 of       772\t|\tloss: 0.846424\n",
      "Training Epoch 7  89.6% | batch:       692 of       772\t|\tloss: 0.563414\n",
      "Training Epoch 7  89.8% | batch:       693 of       772\t|\tloss: 0.647293\n",
      "Training Epoch 7  89.9% | batch:       694 of       772\t|\tloss: 0.807582\n",
      "Training Epoch 7  90.0% | batch:       695 of       772\t|\tloss: 0.486636\n",
      "Training Epoch 7  90.2% | batch:       696 of       772\t|\tloss: 0.568223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  90.3% | batch:       697 of       772\t|\tloss: 0.938016\n",
      "Training Epoch 7  90.4% | batch:       698 of       772\t|\tloss: 0.502756\n",
      "Training Epoch 7  90.5% | batch:       699 of       772\t|\tloss: 0.523809\n",
      "Training Epoch 7  90.7% | batch:       700 of       772\t|\tloss: 0.755651\n",
      "Training Epoch 7  90.8% | batch:       701 of       772\t|\tloss: 0.781479\n",
      "Training Epoch 7  90.9% | batch:       702 of       772\t|\tloss: 0.74794\n",
      "Training Epoch 7  91.1% | batch:       703 of       772\t|\tloss: 0.441926\n",
      "Training Epoch 7  91.2% | batch:       704 of       772\t|\tloss: 0.711629\n",
      "Training Epoch 7  91.3% | batch:       705 of       772\t|\tloss: 0.705764\n",
      "Training Epoch 7  91.5% | batch:       706 of       772\t|\tloss: 0.603786\n",
      "Training Epoch 7  91.6% | batch:       707 of       772\t|\tloss: 0.862182\n",
      "Training Epoch 7  91.7% | batch:       708 of       772\t|\tloss: 0.716776\n",
      "Training Epoch 7  91.8% | batch:       709 of       772\t|\tloss: 0.527859\n",
      "Training Epoch 7  92.0% | batch:       710 of       772\t|\tloss: 0.631989\n",
      "Training Epoch 7  92.1% | batch:       711 of       772\t|\tloss: 0.655686\n",
      "Training Epoch 7  92.2% | batch:       712 of       772\t|\tloss: 0.681365\n",
      "Training Epoch 7  92.4% | batch:       713 of       772\t|\tloss: 0.725557\n",
      "Training Epoch 7  92.5% | batch:       714 of       772\t|\tloss: 0.807058\n",
      "Training Epoch 7  92.6% | batch:       715 of       772\t|\tloss: 0.678136\n",
      "Training Epoch 7  92.7% | batch:       716 of       772\t|\tloss: 0.967196\n",
      "Training Epoch 7  92.9% | batch:       717 of       772\t|\tloss: 0.814687\n",
      "Training Epoch 7  93.0% | batch:       718 of       772\t|\tloss: 0.738987\n",
      "Training Epoch 7  93.1% | batch:       719 of       772\t|\tloss: 0.554774\n",
      "Training Epoch 7  93.3% | batch:       720 of       772\t|\tloss: 0.545354\n",
      "Training Epoch 7  93.4% | batch:       721 of       772\t|\tloss: 0.648355\n",
      "Training Epoch 7  93.5% | batch:       722 of       772\t|\tloss: 0.629377\n",
      "Training Epoch 7  93.7% | batch:       723 of       772\t|\tloss: 0.861637\n",
      "Training Epoch 7  93.8% | batch:       724 of       772\t|\tloss: 0.867244\n",
      "Training Epoch 7  93.9% | batch:       725 of       772\t|\tloss: 0.953739\n",
      "Training Epoch 7  94.0% | batch:       726 of       772\t|\tloss: 0.692868\n",
      "Training Epoch 7  94.2% | batch:       727 of       772\t|\tloss: 0.793239\n",
      "Training Epoch 7  94.3% | batch:       728 of       772\t|\tloss: 0.795136\n",
      "Training Epoch 7  94.4% | batch:       729 of       772\t|\tloss: 0.685473\n",
      "Training Epoch 7  94.6% | batch:       730 of       772\t|\tloss: 0.657605\n",
      "Training Epoch 7  94.7% | batch:       731 of       772\t|\tloss: 0.459517\n",
      "Training Epoch 7  94.8% | batch:       732 of       772\t|\tloss: 0.497245\n",
      "Training Epoch 7  94.9% | batch:       733 of       772\t|\tloss: 0.704766\n",
      "Training Epoch 7  95.1% | batch:       734 of       772\t|\tloss: 1.07076\n",
      "Training Epoch 7  95.2% | batch:       735 of       772\t|\tloss: 0.758502\n",
      "Training Epoch 7  95.3% | batch:       736 of       772\t|\tloss: 0.555007\n",
      "Training Epoch 7  95.5% | batch:       737 of       772\t|\tloss: 0.424873\n",
      "Training Epoch 7  95.6% | batch:       738 of       772\t|\tloss: 0.967661\n",
      "Training Epoch 7  95.7% | batch:       739 of       772\t|\tloss: 0.97138\n",
      "Training Epoch 7  95.9% | batch:       740 of       772\t|\tloss: 0.783322\n",
      "Training Epoch 7  96.0% | batch:       741 of       772\t|\tloss: 0.690394\n",
      "Training Epoch 7  96.1% | batch:       742 of       772\t|\tloss: 0.801716\n",
      "Training Epoch 7  96.2% | batch:       743 of       772\t|\tloss: 0.717372\n",
      "Training Epoch 7  96.4% | batch:       744 of       772\t|\tloss: 0.687117\n",
      "Training Epoch 7  96.5% | batch:       745 of       772\t|\tloss: 0.79833\n",
      "Training Epoch 7  96.6% | batch:       746 of       772\t|\tloss: 0.756876\n",
      "Training Epoch 7  96.8% | batch:       747 of       772\t|\tloss: 0.615408\n",
      "Training Epoch 7  96.9% | batch:       748 of       772\t|\tloss: 0.698871\n",
      "Training Epoch 7  97.0% | batch:       749 of       772\t|\tloss: 0.576145\n",
      "Training Epoch 7  97.2% | batch:       750 of       772\t|\tloss: 1.0451\n",
      "Training Epoch 7  97.3% | batch:       751 of       772\t|\tloss: 0.85617\n",
      "Training Epoch 7  97.4% | batch:       752 of       772\t|\tloss: 0.625871\n",
      "Training Epoch 7  97.5% | batch:       753 of       772\t|\tloss: 0.630903\n",
      "Training Epoch 7  97.7% | batch:       754 of       772\t|\tloss: 0.606265\n",
      "Training Epoch 7  97.8% | batch:       755 of       772\t|\tloss: 0.657558\n",
      "Training Epoch 7  97.9% | batch:       756 of       772\t|\tloss: 0.525963\n",
      "Training Epoch 7  98.1% | batch:       757 of       772\t|\tloss: 0.732928\n",
      "Training Epoch 7  98.2% | batch:       758 of       772\t|\tloss: 0.940181\n",
      "Training Epoch 7  98.3% | batch:       759 of       772\t|\tloss: 0.53503\n",
      "Training Epoch 7  98.4% | batch:       760 of       772\t|\tloss: 0.637136\n",
      "Training Epoch 7  98.6% | batch:       761 of       772\t|\tloss: 0.534474\n",
      "Training Epoch 7  98.7% | batch:       762 of       772\t|\tloss: 0.706319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:22:10,910 | INFO : Epoch 7 Training Summary: epoch: 7.000000 | loss: 0.746445 | \n",
      "2023-05-24 10:22:10,911 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 15.128685235977173 seconds\n",
      "\n",
      "2023-05-24 10:22:10,911 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 16.705557107925415 seconds\n",
      "2023-05-24 10:22:10,911 | INFO : Avg batch train. time: 0.021639322678659863 seconds\n",
      "2023-05-24 10:22:10,912 | INFO : Avg sample train. time: 0.00016906063015286715 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  98.8% | batch:       763 of       772\t|\tloss: 0.578474\n",
      "Training Epoch 7  99.0% | batch:       764 of       772\t|\tloss: 0.6832\n",
      "Training Epoch 7  99.1% | batch:       765 of       772\t|\tloss: 0.694861\n",
      "Training Epoch 7  99.2% | batch:       766 of       772\t|\tloss: 0.800303\n",
      "Training Epoch 7  99.4% | batch:       767 of       772\t|\tloss: 0.799866\n",
      "Training Epoch 7  99.5% | batch:       768 of       772\t|\tloss: 0.782388\n",
      "Training Epoch 7  99.6% | batch:       769 of       772\t|\tloss: 0.58793\n",
      "Training Epoch 7  99.7% | batch:       770 of       772\t|\tloss: 0.70435\n",
      "Training Epoch 7  99.9% | batch:       771 of       772\t|\tloss: 0.555886\n",
      "\n",
      "Training Epoch 8   0.0% | batch:         0 of       772\t|\tloss: 0.491657\n",
      "Training Epoch 8   0.1% | batch:         1 of       772\t|\tloss: 0.683595\n",
      "Training Epoch 8   0.3% | batch:         2 of       772\t|\tloss: 0.804279\n",
      "Training Epoch 8   0.4% | batch:         3 of       772\t|\tloss: 0.675293\n",
      "Training Epoch 8   0.5% | batch:         4 of       772\t|\tloss: 0.641218\n",
      "Training Epoch 8   0.6% | batch:         5 of       772\t|\tloss: 0.596063\n",
      "Training Epoch 8   0.8% | batch:         6 of       772\t|\tloss: 0.626045\n",
      "Training Epoch 8   0.9% | batch:         7 of       772\t|\tloss: 0.707194\n",
      "Training Epoch 8   1.0% | batch:         8 of       772\t|\tloss: 0.512858\n",
      "Training Epoch 8   1.2% | batch:         9 of       772\t|\tloss: 0.451819\n",
      "Training Epoch 8   1.3% | batch:        10 of       772\t|\tloss: 0.955662\n",
      "Training Epoch 8   1.4% | batch:        11 of       772\t|\tloss: 0.70738\n",
      "Training Epoch 8   1.6% | batch:        12 of       772\t|\tloss: 0.454948\n",
      "Training Epoch 8   1.7% | batch:        13 of       772\t|\tloss: 0.800498\n",
      "Training Epoch 8   1.8% | batch:        14 of       772\t|\tloss: 0.545807\n",
      "Training Epoch 8   1.9% | batch:        15 of       772\t|\tloss: 0.513943\n",
      "Training Epoch 8   2.1% | batch:        16 of       772\t|\tloss: 0.570355\n",
      "Training Epoch 8   2.2% | batch:        17 of       772\t|\tloss: 0.504764\n",
      "Training Epoch 8   2.3% | batch:        18 of       772\t|\tloss: 0.715943\n",
      "Training Epoch 8   2.5% | batch:        19 of       772\t|\tloss: 0.443069\n",
      "Training Epoch 8   2.6% | batch:        20 of       772\t|\tloss: 0.426972\n",
      "Training Epoch 8   2.7% | batch:        21 of       772\t|\tloss: 0.500106\n",
      "Training Epoch 8   2.8% | batch:        22 of       772\t|\tloss: 0.634303\n",
      "Training Epoch 8   3.0% | batch:        23 of       772\t|\tloss: 0.622548\n",
      "Training Epoch 8   3.1% | batch:        24 of       772\t|\tloss: 0.56196\n",
      "Training Epoch 8   3.2% | batch:        25 of       772\t|\tloss: 0.655117\n",
      "Training Epoch 8   3.4% | batch:        26 of       772\t|\tloss: 0.760257\n",
      "Training Epoch 8   3.5% | batch:        27 of       772\t|\tloss: 0.506584\n",
      "Training Epoch 8   3.6% | batch:        28 of       772\t|\tloss: 0.787076\n",
      "Training Epoch 8   3.8% | batch:        29 of       772\t|\tloss: 0.677357\n",
      "Training Epoch 8   3.9% | batch:        30 of       772\t|\tloss: 0.582601\n",
      "Training Epoch 8   4.0% | batch:        31 of       772\t|\tloss: 0.948655\n",
      "Training Epoch 8   4.1% | batch:        32 of       772\t|\tloss: 0.572131\n",
      "Training Epoch 8   4.3% | batch:        33 of       772\t|\tloss: 0.540723\n",
      "Training Epoch 8   4.4% | batch:        34 of       772\t|\tloss: 0.432173\n",
      "Training Epoch 8   4.5% | batch:        35 of       772\t|\tloss: 0.720248\n",
      "Training Epoch 8   4.7% | batch:        36 of       772\t|\tloss: 0.493216\n",
      "Training Epoch 8   4.8% | batch:        37 of       772\t|\tloss: 0.633259\n",
      "Training Epoch 8   4.9% | batch:        38 of       772\t|\tloss: 0.786474\n",
      "Training Epoch 8   5.1% | batch:        39 of       772\t|\tloss: 0.772705\n",
      "Training Epoch 8   5.2% | batch:        40 of       772\t|\tloss: 0.769565\n",
      "Training Epoch 8   5.3% | batch:        41 of       772\t|\tloss: 0.426414\n",
      "Training Epoch 8   5.4% | batch:        42 of       772\t|\tloss: 0.704817\n",
      "Training Epoch 8   5.6% | batch:        43 of       772\t|\tloss: 0.584443\n",
      "Training Epoch 8   5.7% | batch:        44 of       772\t|\tloss: 0.544419\n",
      "Training Epoch 8   5.8% | batch:        45 of       772\t|\tloss: 0.648257\n",
      "Training Epoch 8   6.0% | batch:        46 of       772\t|\tloss: 0.616717\n",
      "Training Epoch 8   6.1% | batch:        47 of       772\t|\tloss: 0.641079\n",
      "Training Epoch 8   6.2% | batch:        48 of       772\t|\tloss: 0.677945\n",
      "Training Epoch 8   6.3% | batch:        49 of       772\t|\tloss: 0.715162\n",
      "Training Epoch 8   6.5% | batch:        50 of       772\t|\tloss: 0.598598\n",
      "Training Epoch 8   6.6% | batch:        51 of       772\t|\tloss: 0.670619\n",
      "Training Epoch 8   6.7% | batch:        52 of       772\t|\tloss: 0.677997\n",
      "Training Epoch 8   6.9% | batch:        53 of       772\t|\tloss: 0.650909\n",
      "Training Epoch 8   7.0% | batch:        54 of       772\t|\tloss: 0.690179\n",
      "Training Epoch 8   7.1% | batch:        55 of       772\t|\tloss: 0.793789\n",
      "Training Epoch 8   7.3% | batch:        56 of       772\t|\tloss: 1.14812\n",
      "Training Epoch 8   7.4% | batch:        57 of       772\t|\tloss: 0.677328\n",
      "Training Epoch 8   7.5% | batch:        58 of       772\t|\tloss: 0.527224\n",
      "Training Epoch 8   7.6% | batch:        59 of       772\t|\tloss: 0.575438\n",
      "Training Epoch 8   7.8% | batch:        60 of       772\t|\tloss: 0.726211\n",
      "Training Epoch 8   7.9% | batch:        61 of       772\t|\tloss: 0.444696\n",
      "Training Epoch 8   8.0% | batch:        62 of       772\t|\tloss: 0.673995\n",
      "Training Epoch 8   8.2% | batch:        63 of       772\t|\tloss: 0.68436\n",
      "Training Epoch 8   8.3% | batch:        64 of       772\t|\tloss: 0.655334\n",
      "Training Epoch 8   8.4% | batch:        65 of       772\t|\tloss: 0.617387\n",
      "Training Epoch 8   8.5% | batch:        66 of       772\t|\tloss: 0.681443\n",
      "Training Epoch 8   8.7% | batch:        67 of       772\t|\tloss: 0.533729\n",
      "Training Epoch 8   8.8% | batch:        68 of       772\t|\tloss: 1.14416\n",
      "Training Epoch 8   8.9% | batch:        69 of       772\t|\tloss: 1.06108\n",
      "Training Epoch 8   9.1% | batch:        70 of       772\t|\tloss: 0.681743\n",
      "Training Epoch 8   9.2% | batch:        71 of       772\t|\tloss: 0.546789\n",
      "Training Epoch 8   9.3% | batch:        72 of       772\t|\tloss: 0.652119\n",
      "Training Epoch 8   9.5% | batch:        73 of       772\t|\tloss: 0.619022\n",
      "Training Epoch 8   9.6% | batch:        74 of       772\t|\tloss: 0.590421\n",
      "Training Epoch 8   9.7% | batch:        75 of       772\t|\tloss: 0.590398\n",
      "Training Epoch 8   9.8% | batch:        76 of       772\t|\tloss: 0.841682\n",
      "Training Epoch 8  10.0% | batch:        77 of       772\t|\tloss: 1.02879\n",
      "Training Epoch 8  10.1% | batch:        78 of       772\t|\tloss: 0.786726\n",
      "Training Epoch 8  10.2% | batch:        79 of       772\t|\tloss: 0.589788\n",
      "Training Epoch 8  10.4% | batch:        80 of       772\t|\tloss: 0.532309\n",
      "Training Epoch 8  10.5% | batch:        81 of       772\t|\tloss: 0.70487\n",
      "Training Epoch 8  10.6% | batch:        82 of       772\t|\tloss: 0.763641\n",
      "Training Epoch 8  10.8% | batch:        83 of       772\t|\tloss: 0.560313\n",
      "Training Epoch 8  10.9% | batch:        84 of       772\t|\tloss: 0.57128\n",
      "Training Epoch 8  11.0% | batch:        85 of       772\t|\tloss: 0.639167\n",
      "Training Epoch 8  11.1% | batch:        86 of       772\t|\tloss: 0.569504\n",
      "Training Epoch 8  11.3% | batch:        87 of       772\t|\tloss: 0.789533\n",
      "Training Epoch 8  11.4% | batch:        88 of       772\t|\tloss: 0.525326\n",
      "Training Epoch 8  11.5% | batch:        89 of       772\t|\tloss: 0.657421\n",
      "Training Epoch 8  11.7% | batch:        90 of       772\t|\tloss: 0.627341\n",
      "Training Epoch 8  11.8% | batch:        91 of       772\t|\tloss: 1.35502\n",
      "Training Epoch 8  11.9% | batch:        92 of       772\t|\tloss: 0.749644\n",
      "Training Epoch 8  12.0% | batch:        93 of       772\t|\tloss: 0.879933\n",
      "Training Epoch 8  12.2% | batch:        94 of       772\t|\tloss: 1.03061\n",
      "Training Epoch 8  12.3% | batch:        95 of       772\t|\tloss: 0.928667\n",
      "Training Epoch 8  12.4% | batch:        96 of       772\t|\tloss: 0.832764\n",
      "Training Epoch 8  12.6% | batch:        97 of       772\t|\tloss: 0.56455\n",
      "Training Epoch 8  12.7% | batch:        98 of       772\t|\tloss: 0.933539\n",
      "Training Epoch 8  12.8% | batch:        99 of       772\t|\tloss: 0.62834\n",
      "Training Epoch 8  13.0% | batch:       100 of       772\t|\tloss: 0.800215\n",
      "Training Epoch 8  13.1% | batch:       101 of       772\t|\tloss: 0.657236\n",
      "Training Epoch 8  13.2% | batch:       102 of       772\t|\tloss: 0.8789\n",
      "Training Epoch 8  13.3% | batch:       103 of       772\t|\tloss: 0.754046\n",
      "Training Epoch 8  13.5% | batch:       104 of       772\t|\tloss: 0.502713\n",
      "Training Epoch 8  13.6% | batch:       105 of       772\t|\tloss: 0.426579\n",
      "Training Epoch 8  13.7% | batch:       106 of       772\t|\tloss: 0.503137\n",
      "Training Epoch 8  13.9% | batch:       107 of       772\t|\tloss: 0.858983\n",
      "Training Epoch 8  14.0% | batch:       108 of       772\t|\tloss: 0.671136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  14.1% | batch:       109 of       772\t|\tloss: 0.658292\n",
      "Training Epoch 8  14.2% | batch:       110 of       772\t|\tloss: 0.612473\n",
      "Training Epoch 8  14.4% | batch:       111 of       772\t|\tloss: 0.810611\n",
      "Training Epoch 8  14.5% | batch:       112 of       772\t|\tloss: 0.503888\n",
      "Training Epoch 8  14.6% | batch:       113 of       772\t|\tloss: 1.06519\n",
      "Training Epoch 8  14.8% | batch:       114 of       772\t|\tloss: 0.795031\n",
      "Training Epoch 8  14.9% | batch:       115 of       772\t|\tloss: 0.796166\n",
      "Training Epoch 8  15.0% | batch:       116 of       772\t|\tloss: 0.771663\n",
      "Training Epoch 8  15.2% | batch:       117 of       772\t|\tloss: 0.568204\n",
      "Training Epoch 8  15.3% | batch:       118 of       772\t|\tloss: 0.402399\n",
      "Training Epoch 8  15.4% | batch:       119 of       772\t|\tloss: 0.845549\n",
      "Training Epoch 8  15.5% | batch:       120 of       772\t|\tloss: 0.541073\n",
      "Training Epoch 8  15.7% | batch:       121 of       772\t|\tloss: 0.551234\n",
      "Training Epoch 8  15.8% | batch:       122 of       772\t|\tloss: 0.852758\n",
      "Training Epoch 8  15.9% | batch:       123 of       772\t|\tloss: 0.687668\n",
      "Training Epoch 8  16.1% | batch:       124 of       772\t|\tloss: 0.660525\n",
      "Training Epoch 8  16.2% | batch:       125 of       772\t|\tloss: 0.712846\n",
      "Training Epoch 8  16.3% | batch:       126 of       772\t|\tloss: 0.638581\n",
      "Training Epoch 8  16.5% | batch:       127 of       772\t|\tloss: 0.583366\n",
      "Training Epoch 8  16.6% | batch:       128 of       772\t|\tloss: 0.554538\n",
      "Training Epoch 8  16.7% | batch:       129 of       772\t|\tloss: 0.900996\n",
      "Training Epoch 8  16.8% | batch:       130 of       772\t|\tloss: 0.661371\n",
      "Training Epoch 8  17.0% | batch:       131 of       772\t|\tloss: 0.46212\n",
      "Training Epoch 8  17.1% | batch:       132 of       772\t|\tloss: 0.799758\n",
      "Training Epoch 8  17.2% | batch:       133 of       772\t|\tloss: 0.544452\n",
      "Training Epoch 8  17.4% | batch:       134 of       772\t|\tloss: 0.7349\n",
      "Training Epoch 8  17.5% | batch:       135 of       772\t|\tloss: 0.695243\n",
      "Training Epoch 8  17.6% | batch:       136 of       772\t|\tloss: 0.730636\n",
      "Training Epoch 8  17.7% | batch:       137 of       772\t|\tloss: 0.577113\n",
      "Training Epoch 8  17.9% | batch:       138 of       772\t|\tloss: 0.521574\n",
      "Training Epoch 8  18.0% | batch:       139 of       772\t|\tloss: 0.780339\n",
      "Training Epoch 8  18.1% | batch:       140 of       772\t|\tloss: 0.498085\n",
      "Training Epoch 8  18.3% | batch:       141 of       772\t|\tloss: 0.468956\n",
      "Training Epoch 8  18.4% | batch:       142 of       772\t|\tloss: 0.674573\n",
      "Training Epoch 8  18.5% | batch:       143 of       772\t|\tloss: 0.583327\n",
      "Training Epoch 8  18.7% | batch:       144 of       772\t|\tloss: 0.845893\n",
      "Training Epoch 8  18.8% | batch:       145 of       772\t|\tloss: 0.691646\n",
      "Training Epoch 8  18.9% | batch:       146 of       772\t|\tloss: 0.634519\n",
      "Training Epoch 8  19.0% | batch:       147 of       772\t|\tloss: 0.464855\n",
      "Training Epoch 8  19.2% | batch:       148 of       772\t|\tloss: 0.579252\n",
      "Training Epoch 8  19.3% | batch:       149 of       772\t|\tloss: 0.89066\n",
      "Training Epoch 8  19.4% | batch:       150 of       772\t|\tloss: 0.737785\n",
      "Training Epoch 8  19.6% | batch:       151 of       772\t|\tloss: 0.797143\n",
      "Training Epoch 8  19.7% | batch:       152 of       772\t|\tloss: 0.95785\n",
      "Training Epoch 8  19.8% | batch:       153 of       772\t|\tloss: 0.692782\n",
      "Training Epoch 8  19.9% | batch:       154 of       772\t|\tloss: 0.792659\n",
      "Training Epoch 8  20.1% | batch:       155 of       772\t|\tloss: 0.953039\n",
      "Training Epoch 8  20.2% | batch:       156 of       772\t|\tloss: 0.967921\n",
      "Training Epoch 8  20.3% | batch:       157 of       772\t|\tloss: 0.824669\n",
      "Training Epoch 8  20.5% | batch:       158 of       772\t|\tloss: 0.609866\n",
      "Training Epoch 8  20.6% | batch:       159 of       772\t|\tloss: 1.0619\n",
      "Training Epoch 8  20.7% | batch:       160 of       772\t|\tloss: 0.92796\n",
      "Training Epoch 8  20.9% | batch:       161 of       772\t|\tloss: 0.918341\n",
      "Training Epoch 8  21.0% | batch:       162 of       772\t|\tloss: 0.848527\n",
      "Training Epoch 8  21.1% | batch:       163 of       772\t|\tloss: 0.702162\n",
      "Training Epoch 8  21.2% | batch:       164 of       772\t|\tloss: 1.05121\n",
      "Training Epoch 8  21.4% | batch:       165 of       772\t|\tloss: 0.794521\n",
      "Training Epoch 8  21.5% | batch:       166 of       772\t|\tloss: 0.533798\n",
      "Training Epoch 8  21.6% | batch:       167 of       772\t|\tloss: 0.768861\n",
      "Training Epoch 8  21.8% | batch:       168 of       772\t|\tloss: 0.64317\n",
      "Training Epoch 8  21.9% | batch:       169 of       772\t|\tloss: 0.661128\n",
      "Training Epoch 8  22.0% | batch:       170 of       772\t|\tloss: 0.717207\n",
      "Training Epoch 8  22.2% | batch:       171 of       772\t|\tloss: 0.655897\n",
      "Training Epoch 8  22.3% | batch:       172 of       772\t|\tloss: 0.565344\n",
      "Training Epoch 8  22.4% | batch:       173 of       772\t|\tloss: 0.686747\n",
      "Training Epoch 8  22.5% | batch:       174 of       772\t|\tloss: 0.82141\n",
      "Training Epoch 8  22.7% | batch:       175 of       772\t|\tloss: 0.907727\n",
      "Training Epoch 8  22.8% | batch:       176 of       772\t|\tloss: 0.85035\n",
      "Training Epoch 8  22.9% | batch:       177 of       772\t|\tloss: 0.630674\n",
      "Training Epoch 8  23.1% | batch:       178 of       772\t|\tloss: 0.661131\n",
      "Training Epoch 8  23.2% | batch:       179 of       772\t|\tloss: 0.979638\n",
      "Training Epoch 8  23.3% | batch:       180 of       772\t|\tloss: 1.37422\n",
      "Training Epoch 8  23.4% | batch:       181 of       772\t|\tloss: 0.601341\n",
      "Training Epoch 8  23.6% | batch:       182 of       772\t|\tloss: 0.480478\n",
      "Training Epoch 8  23.7% | batch:       183 of       772\t|\tloss: 0.640232\n",
      "Training Epoch 8  23.8% | batch:       184 of       772\t|\tloss: 0.581505\n",
      "Training Epoch 8  24.0% | batch:       185 of       772\t|\tloss: 0.469326\n",
      "Training Epoch 8  24.1% | batch:       186 of       772\t|\tloss: 0.487564\n",
      "Training Epoch 8  24.2% | batch:       187 of       772\t|\tloss: 0.888735\n",
      "Training Epoch 8  24.4% | batch:       188 of       772\t|\tloss: 0.737836\n",
      "Training Epoch 8  24.5% | batch:       189 of       772\t|\tloss: 0.772797\n",
      "Training Epoch 8  24.6% | batch:       190 of       772\t|\tloss: 0.811322\n",
      "Training Epoch 8  24.7% | batch:       191 of       772\t|\tloss: 0.515647\n",
      "Training Epoch 8  24.9% | batch:       192 of       772\t|\tloss: 0.727298\n",
      "Training Epoch 8  25.0% | batch:       193 of       772\t|\tloss: 0.919828\n",
      "Training Epoch 8  25.1% | batch:       194 of       772\t|\tloss: 0.516315\n",
      "Training Epoch 8  25.3% | batch:       195 of       772\t|\tloss: 0.683061\n",
      "Training Epoch 8  25.4% | batch:       196 of       772\t|\tloss: 0.653098\n",
      "Training Epoch 8  25.5% | batch:       197 of       772\t|\tloss: 0.610433\n",
      "Training Epoch 8  25.6% | batch:       198 of       772\t|\tloss: 0.456838\n",
      "Training Epoch 8  25.8% | batch:       199 of       772\t|\tloss: 0.62457\n",
      "Training Epoch 8  25.9% | batch:       200 of       772\t|\tloss: 0.6173\n",
      "Training Epoch 8  26.0% | batch:       201 of       772\t|\tloss: 0.828001\n",
      "Training Epoch 8  26.2% | batch:       202 of       772\t|\tloss: 0.836359\n",
      "Training Epoch 8  26.3% | batch:       203 of       772\t|\tloss: 0.791849\n",
      "Training Epoch 8  26.4% | batch:       204 of       772\t|\tloss: 0.803637\n",
      "Training Epoch 8  26.6% | batch:       205 of       772\t|\tloss: 0.843137\n",
      "Training Epoch 8  26.7% | batch:       206 of       772\t|\tloss: 0.414787\n",
      "Training Epoch 8  26.8% | batch:       207 of       772\t|\tloss: 0.986536\n",
      "Training Epoch 8  26.9% | batch:       208 of       772\t|\tloss: 0.686037\n",
      "Training Epoch 8  27.1% | batch:       209 of       772\t|\tloss: 0.485688\n",
      "Training Epoch 8  27.2% | batch:       210 of       772\t|\tloss: 0.587636\n",
      "Training Epoch 8  27.3% | batch:       211 of       772\t|\tloss: 0.464981\n",
      "Training Epoch 8  27.5% | batch:       212 of       772\t|\tloss: 0.957902\n",
      "Training Epoch 8  27.6% | batch:       213 of       772\t|\tloss: 0.564907\n",
      "Training Epoch 8  27.7% | batch:       214 of       772\t|\tloss: 0.592007\n",
      "Training Epoch 8  27.8% | batch:       215 of       772\t|\tloss: 0.649802\n",
      "Training Epoch 8  28.0% | batch:       216 of       772\t|\tloss: 0.861515\n",
      "Training Epoch 8  28.1% | batch:       217 of       772\t|\tloss: 0.709753\n",
      "Training Epoch 8  28.2% | batch:       218 of       772\t|\tloss: 0.772522\n",
      "Training Epoch 8  28.4% | batch:       219 of       772\t|\tloss: 0.645969\n",
      "Training Epoch 8  28.5% | batch:       220 of       772\t|\tloss: 0.662281\n",
      "Training Epoch 8  28.6% | batch:       221 of       772\t|\tloss: 0.730448\n",
      "Training Epoch 8  28.8% | batch:       222 of       772\t|\tloss: 0.797413\n",
      "Training Epoch 8  28.9% | batch:       223 of       772\t|\tloss: 0.69745\n",
      "Training Epoch 8  29.0% | batch:       224 of       772\t|\tloss: 0.47721\n",
      "Training Epoch 8  29.1% | batch:       225 of       772\t|\tloss: 0.772668\n",
      "Training Epoch 8  29.3% | batch:       226 of       772\t|\tloss: 0.979265\n",
      "Training Epoch 8  29.4% | batch:       227 of       772\t|\tloss: 0.978832\n",
      "Training Epoch 8  29.5% | batch:       228 of       772\t|\tloss: 0.773098\n",
      "Training Epoch 8  29.7% | batch:       229 of       772\t|\tloss: 0.827044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  29.8% | batch:       230 of       772\t|\tloss: 0.698399\n",
      "Training Epoch 8  29.9% | batch:       231 of       772\t|\tloss: 0.56389\n",
      "Training Epoch 8  30.1% | batch:       232 of       772\t|\tloss: 0.476034\n",
      "Training Epoch 8  30.2% | batch:       233 of       772\t|\tloss: 0.715587\n",
      "Training Epoch 8  30.3% | batch:       234 of       772\t|\tloss: 0.693002\n",
      "Training Epoch 8  30.4% | batch:       235 of       772\t|\tloss: 0.776294\n",
      "Training Epoch 8  30.6% | batch:       236 of       772\t|\tloss: 0.399852\n",
      "Training Epoch 8  30.7% | batch:       237 of       772\t|\tloss: 0.741913\n",
      "Training Epoch 8  30.8% | batch:       238 of       772\t|\tloss: 0.649127\n",
      "Training Epoch 8  31.0% | batch:       239 of       772\t|\tloss: 0.734283\n",
      "Training Epoch 8  31.1% | batch:       240 of       772\t|\tloss: 0.664779\n",
      "Training Epoch 8  31.2% | batch:       241 of       772\t|\tloss: 0.521059\n",
      "Training Epoch 8  31.3% | batch:       242 of       772\t|\tloss: 0.893768\n",
      "Training Epoch 8  31.5% | batch:       243 of       772\t|\tloss: 0.817153\n",
      "Training Epoch 8  31.6% | batch:       244 of       772\t|\tloss: 0.646156\n",
      "Training Epoch 8  31.7% | batch:       245 of       772\t|\tloss: 0.738825\n",
      "Training Epoch 8  31.9% | batch:       246 of       772\t|\tloss: 0.549654\n",
      "Training Epoch 8  32.0% | batch:       247 of       772\t|\tloss: 0.514287\n",
      "Training Epoch 8  32.1% | batch:       248 of       772\t|\tloss: 1.0676\n",
      "Training Epoch 8  32.3% | batch:       249 of       772\t|\tloss: 1.14242\n",
      "Training Epoch 8  32.4% | batch:       250 of       772\t|\tloss: 0.812716\n",
      "Training Epoch 8  32.5% | batch:       251 of       772\t|\tloss: 0.876587\n",
      "Training Epoch 8  32.6% | batch:       252 of       772\t|\tloss: 0.771154\n",
      "Training Epoch 8  32.8% | batch:       253 of       772\t|\tloss: 0.451532\n",
      "Training Epoch 8  32.9% | batch:       254 of       772\t|\tloss: 0.885375\n",
      "Training Epoch 8  33.0% | batch:       255 of       772\t|\tloss: 0.675766\n",
      "Training Epoch 8  33.2% | batch:       256 of       772\t|\tloss: 0.674418\n",
      "Training Epoch 8  33.3% | batch:       257 of       772\t|\tloss: 0.64981\n",
      "Training Epoch 8  33.4% | batch:       258 of       772\t|\tloss: 1.0743\n",
      "Training Epoch 8  33.5% | batch:       259 of       772\t|\tloss: 1.39015\n",
      "Training Epoch 8  33.7% | batch:       260 of       772\t|\tloss: 0.886602\n",
      "Training Epoch 8  33.8% | batch:       261 of       772\t|\tloss: 0.597684\n",
      "Training Epoch 8  33.9% | batch:       262 of       772\t|\tloss: 0.897477\n",
      "Training Epoch 8  34.1% | batch:       263 of       772\t|\tloss: 0.945091\n",
      "Training Epoch 8  34.2% | batch:       264 of       772\t|\tloss: 1.03794\n",
      "Training Epoch 8  34.3% | batch:       265 of       772\t|\tloss: 0.578524\n",
      "Training Epoch 8  34.5% | batch:       266 of       772\t|\tloss: 0.766081\n",
      "Training Epoch 8  34.6% | batch:       267 of       772\t|\tloss: 0.598391\n",
      "Training Epoch 8  34.7% | batch:       268 of       772\t|\tloss: 0.713156\n",
      "Training Epoch 8  34.8% | batch:       269 of       772\t|\tloss: 0.57309\n",
      "Training Epoch 8  35.0% | batch:       270 of       772\t|\tloss: 1.00014\n",
      "Training Epoch 8  35.1% | batch:       271 of       772\t|\tloss: 0.897234\n",
      "Training Epoch 8  35.2% | batch:       272 of       772\t|\tloss: 1.24476\n",
      "Training Epoch 8  35.4% | batch:       273 of       772\t|\tloss: 1.02999\n",
      "Training Epoch 8  35.5% | batch:       274 of       772\t|\tloss: 0.673626\n",
      "Training Epoch 8  35.6% | batch:       275 of       772\t|\tloss: 0.871827\n",
      "Training Epoch 8  35.8% | batch:       276 of       772\t|\tloss: 0.636703\n",
      "Training Epoch 8  35.9% | batch:       277 of       772\t|\tloss: 0.686104\n",
      "Training Epoch 8  36.0% | batch:       278 of       772\t|\tloss: 0.846741\n",
      "Training Epoch 8  36.1% | batch:       279 of       772\t|\tloss: 0.946824\n",
      "Training Epoch 8  36.3% | batch:       280 of       772\t|\tloss: 0.600146\n",
      "Training Epoch 8  36.4% | batch:       281 of       772\t|\tloss: 0.65378\n",
      "Training Epoch 8  36.5% | batch:       282 of       772\t|\tloss: 0.63898\n",
      "Training Epoch 8  36.7% | batch:       283 of       772\t|\tloss: 0.619826\n",
      "Training Epoch 8  36.8% | batch:       284 of       772\t|\tloss: 0.42015\n",
      "Training Epoch 8  36.9% | batch:       285 of       772\t|\tloss: 0.895743\n",
      "Training Epoch 8  37.0% | batch:       286 of       772\t|\tloss: 0.52375\n",
      "Training Epoch 8  37.2% | batch:       287 of       772\t|\tloss: 0.496734\n",
      "Training Epoch 8  37.3% | batch:       288 of       772\t|\tloss: 0.478617\n",
      "Training Epoch 8  37.4% | batch:       289 of       772\t|\tloss: 0.584484\n",
      "Training Epoch 8  37.6% | batch:       290 of       772\t|\tloss: 0.704858\n",
      "Training Epoch 8  37.7% | batch:       291 of       772\t|\tloss: 0.590793\n",
      "Training Epoch 8  37.8% | batch:       292 of       772\t|\tloss: 0.694044\n",
      "Training Epoch 8  38.0% | batch:       293 of       772\t|\tloss: 0.596818\n",
      "Training Epoch 8  38.1% | batch:       294 of       772\t|\tloss: 0.725429\n",
      "Training Epoch 8  38.2% | batch:       295 of       772\t|\tloss: 0.635179\n",
      "Training Epoch 8  38.3% | batch:       296 of       772\t|\tloss: 0.739463\n",
      "Training Epoch 8  38.5% | batch:       297 of       772\t|\tloss: 0.72476\n",
      "Training Epoch 8  38.6% | batch:       298 of       772\t|\tloss: 0.462567\n",
      "Training Epoch 8  38.7% | batch:       299 of       772\t|\tloss: 0.676794\n",
      "Training Epoch 8  38.9% | batch:       300 of       772\t|\tloss: 0.627069\n",
      "Training Epoch 8  39.0% | batch:       301 of       772\t|\tloss: 0.736008\n",
      "Training Epoch 8  39.1% | batch:       302 of       772\t|\tloss: 0.641183\n",
      "Training Epoch 8  39.2% | batch:       303 of       772\t|\tloss: 0.820574\n",
      "Training Epoch 8  39.4% | batch:       304 of       772\t|\tloss: 0.633141\n",
      "Training Epoch 8  39.5% | batch:       305 of       772\t|\tloss: 0.683938\n",
      "Training Epoch 8  39.6% | batch:       306 of       772\t|\tloss: 0.741873\n",
      "Training Epoch 8  39.8% | batch:       307 of       772\t|\tloss: 0.496115\n",
      "Training Epoch 8  39.9% | batch:       308 of       772\t|\tloss: 1.01897\n",
      "Training Epoch 8  40.0% | batch:       309 of       772\t|\tloss: 0.811457\n",
      "Training Epoch 8  40.2% | batch:       310 of       772\t|\tloss: 1.51433\n",
      "Training Epoch 8  40.3% | batch:       311 of       772\t|\tloss: 0.592745\n",
      "Training Epoch 8  40.4% | batch:       312 of       772\t|\tloss: 0.896773\n",
      "Training Epoch 8  40.5% | batch:       313 of       772\t|\tloss: 1.0145\n",
      "Training Epoch 8  40.7% | batch:       314 of       772\t|\tloss: 0.97824\n",
      "Training Epoch 8  40.8% | batch:       315 of       772\t|\tloss: 0.818998\n",
      "Training Epoch 8  40.9% | batch:       316 of       772\t|\tloss: 0.625385\n",
      "Training Epoch 8  41.1% | batch:       317 of       772\t|\tloss: 0.708719\n",
      "Training Epoch 8  41.2% | batch:       318 of       772\t|\tloss: 0.803436\n",
      "Training Epoch 8  41.3% | batch:       319 of       772\t|\tloss: 0.789314\n",
      "Training Epoch 8  41.5% | batch:       320 of       772\t|\tloss: 0.740238\n",
      "Training Epoch 8  41.6% | batch:       321 of       772\t|\tloss: 0.968042\n",
      "Training Epoch 8  41.7% | batch:       322 of       772\t|\tloss: 0.705595\n",
      "Training Epoch 8  41.8% | batch:       323 of       772\t|\tloss: 0.708137\n",
      "Training Epoch 8  42.0% | batch:       324 of       772\t|\tloss: 0.838506\n",
      "Training Epoch 8  42.1% | batch:       325 of       772\t|\tloss: 0.684415\n",
      "Training Epoch 8  42.2% | batch:       326 of       772\t|\tloss: 0.702006\n",
      "Training Epoch 8  42.4% | batch:       327 of       772\t|\tloss: 0.401319\n",
      "Training Epoch 8  42.5% | batch:       328 of       772\t|\tloss: 0.596803\n",
      "Training Epoch 8  42.6% | batch:       329 of       772\t|\tloss: 0.752309\n",
      "Training Epoch 8  42.7% | batch:       330 of       772\t|\tloss: 0.833547\n",
      "Training Epoch 8  42.9% | batch:       331 of       772\t|\tloss: 0.995857\n",
      "Training Epoch 8  43.0% | batch:       332 of       772\t|\tloss: 0.922074\n",
      "Training Epoch 8  43.1% | batch:       333 of       772\t|\tloss: 0.672765\n",
      "Training Epoch 8  43.3% | batch:       334 of       772\t|\tloss: 0.45255\n",
      "Training Epoch 8  43.4% | batch:       335 of       772\t|\tloss: 0.684392\n",
      "Training Epoch 8  43.5% | batch:       336 of       772\t|\tloss: 0.638553\n",
      "Training Epoch 8  43.7% | batch:       337 of       772\t|\tloss: 0.506819\n",
      "Training Epoch 8  43.8% | batch:       338 of       772\t|\tloss: 0.628079\n",
      "Training Epoch 8  43.9% | batch:       339 of       772\t|\tloss: 0.934006\n",
      "Training Epoch 8  44.0% | batch:       340 of       772\t|\tloss: 0.682078\n",
      "Training Epoch 8  44.2% | batch:       341 of       772\t|\tloss: 0.39608\n",
      "Training Epoch 8  44.3% | batch:       342 of       772\t|\tloss: 0.696799\n",
      "Training Epoch 8  44.4% | batch:       343 of       772\t|\tloss: 0.573096\n",
      "Training Epoch 8  44.6% | batch:       344 of       772\t|\tloss: 0.716798\n",
      "Training Epoch 8  44.7% | batch:       345 of       772\t|\tloss: 0.558764\n",
      "Training Epoch 8  44.8% | batch:       346 of       772\t|\tloss: 0.566542\n",
      "Training Epoch 8  44.9% | batch:       347 of       772\t|\tloss: 0.506954\n",
      "Training Epoch 8  45.1% | batch:       348 of       772\t|\tloss: 0.489359\n",
      "Training Epoch 8  45.2% | batch:       349 of       772\t|\tloss: 0.564045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  45.3% | batch:       350 of       772\t|\tloss: 0.54029\n",
      "Training Epoch 8  45.5% | batch:       351 of       772\t|\tloss: 0.567598\n",
      "Training Epoch 8  45.6% | batch:       352 of       772\t|\tloss: 0.661525\n",
      "Training Epoch 8  45.7% | batch:       353 of       772\t|\tloss: 1.03328\n",
      "Training Epoch 8  45.9% | batch:       354 of       772\t|\tloss: 0.815935\n",
      "Training Epoch 8  46.0% | batch:       355 of       772\t|\tloss: 0.510491\n",
      "Training Epoch 8  46.1% | batch:       356 of       772\t|\tloss: 0.861995\n",
      "Training Epoch 8  46.2% | batch:       357 of       772\t|\tloss: 1.28489\n",
      "Training Epoch 8  46.4% | batch:       358 of       772\t|\tloss: 0.878875\n",
      "Training Epoch 8  46.5% | batch:       359 of       772\t|\tloss: 0.85211\n",
      "Training Epoch 8  46.6% | batch:       360 of       772\t|\tloss: 0.719175\n",
      "Training Epoch 8  46.8% | batch:       361 of       772\t|\tloss: 1.04966\n",
      "Training Epoch 8  46.9% | batch:       362 of       772\t|\tloss: 0.899725\n",
      "Training Epoch 8  47.0% | batch:       363 of       772\t|\tloss: 0.749206\n",
      "Training Epoch 8  47.2% | batch:       364 of       772\t|\tloss: 0.530695\n",
      "Training Epoch 8  47.3% | batch:       365 of       772\t|\tloss: 1.02127\n",
      "Training Epoch 8  47.4% | batch:       366 of       772\t|\tloss: 0.904919\n",
      "Training Epoch 8  47.5% | batch:       367 of       772\t|\tloss: 0.615631\n",
      "Training Epoch 8  47.7% | batch:       368 of       772\t|\tloss: 0.784247\n",
      "Training Epoch 8  47.8% | batch:       369 of       772\t|\tloss: 1.2139\n",
      "Training Epoch 8  47.9% | batch:       370 of       772\t|\tloss: 0.728362\n",
      "Training Epoch 8  48.1% | batch:       371 of       772\t|\tloss: 0.415196\n",
      "Training Epoch 8  48.2% | batch:       372 of       772\t|\tloss: 0.840642\n",
      "Training Epoch 8  48.3% | batch:       373 of       772\t|\tloss: 1.07539\n",
      "Training Epoch 8  48.4% | batch:       374 of       772\t|\tloss: 0.995321\n",
      "Training Epoch 8  48.6% | batch:       375 of       772\t|\tloss: 0.365149\n",
      "Training Epoch 8  48.7% | batch:       376 of       772\t|\tloss: 0.501212\n",
      "Training Epoch 8  48.8% | batch:       377 of       772\t|\tloss: 0.62961\n",
      "Training Epoch 8  49.0% | batch:       378 of       772\t|\tloss: 0.477949\n",
      "Training Epoch 8  49.1% | batch:       379 of       772\t|\tloss: 0.489612\n",
      "Training Epoch 8  49.2% | batch:       380 of       772\t|\tloss: 0.643342\n",
      "Training Epoch 8  49.4% | batch:       381 of       772\t|\tloss: 0.829258\n",
      "Training Epoch 8  49.5% | batch:       382 of       772\t|\tloss: 0.60104\n",
      "Training Epoch 8  49.6% | batch:       383 of       772\t|\tloss: 0.941284\n",
      "Training Epoch 8  49.7% | batch:       384 of       772\t|\tloss: 0.743592\n",
      "Training Epoch 8  49.9% | batch:       385 of       772\t|\tloss: 0.823392\n",
      "Training Epoch 8  50.0% | batch:       386 of       772\t|\tloss: 0.821783\n",
      "Training Epoch 8  50.1% | batch:       387 of       772\t|\tloss: 0.631966\n",
      "Training Epoch 8  50.3% | batch:       388 of       772\t|\tloss: 0.603845\n",
      "Training Epoch 8  50.4% | batch:       389 of       772\t|\tloss: 0.610073\n",
      "Training Epoch 8  50.5% | batch:       390 of       772\t|\tloss: 0.522272\n",
      "Training Epoch 8  50.6% | batch:       391 of       772\t|\tloss: 0.62621\n",
      "Training Epoch 8  50.8% | batch:       392 of       772\t|\tloss: 0.663543\n",
      "Training Epoch 8  50.9% | batch:       393 of       772\t|\tloss: 0.640079\n",
      "Training Epoch 8  51.0% | batch:       394 of       772\t|\tloss: 0.900287\n",
      "Training Epoch 8  51.2% | batch:       395 of       772\t|\tloss: 0.606298\n",
      "Training Epoch 8  51.3% | batch:       396 of       772\t|\tloss: 0.477314\n",
      "Training Epoch 8  51.4% | batch:       397 of       772\t|\tloss: 0.708726\n",
      "Training Epoch 8  51.6% | batch:       398 of       772\t|\tloss: 0.719675\n",
      "Training Epoch 8  51.7% | batch:       399 of       772\t|\tloss: 0.688041\n",
      "Training Epoch 8  51.8% | batch:       400 of       772\t|\tloss: 0.548769\n",
      "Training Epoch 8  51.9% | batch:       401 of       772\t|\tloss: 0.795243\n",
      "Training Epoch 8  52.1% | batch:       402 of       772\t|\tloss: 0.888112\n",
      "Training Epoch 8  52.2% | batch:       403 of       772\t|\tloss: 0.85101\n",
      "Training Epoch 8  52.3% | batch:       404 of       772\t|\tloss: 0.798221\n",
      "Training Epoch 8  52.5% | batch:       405 of       772\t|\tloss: 0.820232\n",
      "Training Epoch 8  52.6% | batch:       406 of       772\t|\tloss: 0.594986\n",
      "Training Epoch 8  52.7% | batch:       407 of       772\t|\tloss: 0.601745\n",
      "Training Epoch 8  52.8% | batch:       408 of       772\t|\tloss: 0.89638\n",
      "Training Epoch 8  53.0% | batch:       409 of       772\t|\tloss: 0.903086\n",
      "Training Epoch 8  53.1% | batch:       410 of       772\t|\tloss: 0.54398\n",
      "Training Epoch 8  53.2% | batch:       411 of       772\t|\tloss: 0.856023\n",
      "Training Epoch 8  53.4% | batch:       412 of       772\t|\tloss: 0.782507\n",
      "Training Epoch 8  53.5% | batch:       413 of       772\t|\tloss: 0.533812\n",
      "Training Epoch 8  53.6% | batch:       414 of       772\t|\tloss: 0.666521\n",
      "Training Epoch 8  53.8% | batch:       415 of       772\t|\tloss: 0.975186\n",
      "Training Epoch 8  53.9% | batch:       416 of       772\t|\tloss: 0.716714\n",
      "Training Epoch 8  54.0% | batch:       417 of       772\t|\tloss: 0.623387\n",
      "Training Epoch 8  54.1% | batch:       418 of       772\t|\tloss: 0.791687\n",
      "Training Epoch 8  54.3% | batch:       419 of       772\t|\tloss: 0.739098\n",
      "Training Epoch 8  54.4% | batch:       420 of       772\t|\tloss: 0.58672\n",
      "Training Epoch 8  54.5% | batch:       421 of       772\t|\tloss: 0.696552\n",
      "Training Epoch 8  54.7% | batch:       422 of       772\t|\tloss: 0.717018\n",
      "Training Epoch 8  54.8% | batch:       423 of       772\t|\tloss: 0.743295\n",
      "Training Epoch 8  54.9% | batch:       424 of       772\t|\tloss: 0.913229\n",
      "Training Epoch 8  55.1% | batch:       425 of       772\t|\tloss: 0.631912\n",
      "Training Epoch 8  55.2% | batch:       426 of       772\t|\tloss: 0.422406\n",
      "Training Epoch 8  55.3% | batch:       427 of       772\t|\tloss: 0.713542\n",
      "Training Epoch 8  55.4% | batch:       428 of       772\t|\tloss: 0.862182\n",
      "Training Epoch 8  55.6% | batch:       429 of       772\t|\tloss: 0.724956\n",
      "Training Epoch 8  55.7% | batch:       430 of       772\t|\tloss: 0.92738\n",
      "Training Epoch 8  55.8% | batch:       431 of       772\t|\tloss: 1.01873\n",
      "Training Epoch 8  56.0% | batch:       432 of       772\t|\tloss: 0.599169\n",
      "Training Epoch 8  56.1% | batch:       433 of       772\t|\tloss: 1.05629\n",
      "Training Epoch 8  56.2% | batch:       434 of       772\t|\tloss: 0.85092\n",
      "Training Epoch 8  56.3% | batch:       435 of       772\t|\tloss: 0.806241\n",
      "Training Epoch 8  56.5% | batch:       436 of       772\t|\tloss: 0.75185\n",
      "Training Epoch 8  56.6% | batch:       437 of       772\t|\tloss: 0.668843\n",
      "Training Epoch 8  56.7% | batch:       438 of       772\t|\tloss: 1.40016\n",
      "Training Epoch 8  56.9% | batch:       439 of       772\t|\tloss: 1.054\n",
      "Training Epoch 8  57.0% | batch:       440 of       772\t|\tloss: 0.704505\n",
      "Training Epoch 8  57.1% | batch:       441 of       772\t|\tloss: 0.795854\n",
      "Training Epoch 8  57.3% | batch:       442 of       772\t|\tloss: 0.803988\n",
      "Training Epoch 8  57.4% | batch:       443 of       772\t|\tloss: 0.689429\n",
      "Training Epoch 8  57.5% | batch:       444 of       772\t|\tloss: 0.849713\n",
      "Training Epoch 8  57.6% | batch:       445 of       772\t|\tloss: 0.931006\n",
      "Training Epoch 8  57.8% | batch:       446 of       772\t|\tloss: 0.726049\n",
      "Training Epoch 8  57.9% | batch:       447 of       772\t|\tloss: 0.714034\n",
      "Training Epoch 8  58.0% | batch:       448 of       772\t|\tloss: 0.656838\n",
      "Training Epoch 8  58.2% | batch:       449 of       772\t|\tloss: 0.592476\n",
      "Training Epoch 8  58.3% | batch:       450 of       772\t|\tloss: 0.765043\n",
      "Training Epoch 8  58.4% | batch:       451 of       772\t|\tloss: 0.584355\n",
      "Training Epoch 8  58.5% | batch:       452 of       772\t|\tloss: 0.500329\n",
      "Training Epoch 8  58.7% | batch:       453 of       772\t|\tloss: 0.644872\n",
      "Training Epoch 8  58.8% | batch:       454 of       772\t|\tloss: 0.649566\n",
      "Training Epoch 8  58.9% | batch:       455 of       772\t|\tloss: 0.766867\n",
      "Training Epoch 8  59.1% | batch:       456 of       772\t|\tloss: 0.471122\n",
      "Training Epoch 8  59.2% | batch:       457 of       772\t|\tloss: 0.563854\n",
      "Training Epoch 8  59.3% | batch:       458 of       772\t|\tloss: 0.516915\n",
      "Training Epoch 8  59.5% | batch:       459 of       772\t|\tloss: 0.794277\n",
      "Training Epoch 8  59.6% | batch:       460 of       772\t|\tloss: 0.79556\n",
      "Training Epoch 8  59.7% | batch:       461 of       772\t|\tloss: 0.529096\n",
      "Training Epoch 8  59.8% | batch:       462 of       772\t|\tloss: 0.527809\n",
      "Training Epoch 8  60.0% | batch:       463 of       772\t|\tloss: 0.75703\n",
      "Training Epoch 8  60.1% | batch:       464 of       772\t|\tloss: 0.45404\n",
      "Training Epoch 8  60.2% | batch:       465 of       772\t|\tloss: 0.58713\n",
      "Training Epoch 8  60.4% | batch:       466 of       772\t|\tloss: 0.559869\n",
      "Training Epoch 8  60.5% | batch:       467 of       772\t|\tloss: 0.632806\n",
      "Training Epoch 8  60.6% | batch:       468 of       772\t|\tloss: 0.615183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  60.8% | batch:       469 of       772\t|\tloss: 0.538818\n",
      "Training Epoch 8  60.9% | batch:       470 of       772\t|\tloss: 0.435721\n",
      "Training Epoch 8  61.0% | batch:       471 of       772\t|\tloss: 0.611367\n",
      "Training Epoch 8  61.1% | batch:       472 of       772\t|\tloss: 0.597304\n",
      "Training Epoch 8  61.3% | batch:       473 of       772\t|\tloss: 0.888473\n",
      "Training Epoch 8  61.4% | batch:       474 of       772\t|\tloss: 0.8858\n",
      "Training Epoch 8  61.5% | batch:       475 of       772\t|\tloss: 0.847469\n",
      "Training Epoch 8  61.7% | batch:       476 of       772\t|\tloss: 0.58363\n",
      "Training Epoch 8  61.8% | batch:       477 of       772\t|\tloss: 0.602955\n",
      "Training Epoch 8  61.9% | batch:       478 of       772\t|\tloss: 0.88717\n",
      "Training Epoch 8  62.0% | batch:       479 of       772\t|\tloss: 0.813038\n",
      "Training Epoch 8  62.2% | batch:       480 of       772\t|\tloss: 0.91119\n",
      "Training Epoch 8  62.3% | batch:       481 of       772\t|\tloss: 0.77593\n",
      "Training Epoch 8  62.4% | batch:       482 of       772\t|\tloss: 0.827203\n",
      "Training Epoch 8  62.6% | batch:       483 of       772\t|\tloss: 0.753264\n",
      "Training Epoch 8  62.7% | batch:       484 of       772\t|\tloss: 0.56325\n",
      "Training Epoch 8  62.8% | batch:       485 of       772\t|\tloss: 0.593532\n",
      "Training Epoch 8  63.0% | batch:       486 of       772\t|\tloss: 0.904619\n",
      "Training Epoch 8  63.1% | batch:       487 of       772\t|\tloss: 0.448143\n",
      "Training Epoch 8  63.2% | batch:       488 of       772\t|\tloss: 0.66199\n",
      "Training Epoch 8  63.3% | batch:       489 of       772\t|\tloss: 0.803172\n",
      "Training Epoch 8  63.5% | batch:       490 of       772\t|\tloss: 0.837655\n",
      "Training Epoch 8  63.6% | batch:       491 of       772\t|\tloss: 0.620734\n",
      "Training Epoch 8  63.7% | batch:       492 of       772\t|\tloss: 0.635848\n",
      "Training Epoch 8  63.9% | batch:       493 of       772\t|\tloss: 0.616111\n",
      "Training Epoch 8  64.0% | batch:       494 of       772\t|\tloss: 0.719224\n",
      "Training Epoch 8  64.1% | batch:       495 of       772\t|\tloss: 0.700762\n",
      "Training Epoch 8  64.2% | batch:       496 of       772\t|\tloss: 0.58395\n",
      "Training Epoch 8  64.4% | batch:       497 of       772\t|\tloss: 0.616094\n",
      "Training Epoch 8  64.5% | batch:       498 of       772\t|\tloss: 0.473049\n",
      "Training Epoch 8  64.6% | batch:       499 of       772\t|\tloss: 0.538873\n",
      "Training Epoch 8  64.8% | batch:       500 of       772\t|\tloss: 0.670177\n",
      "Training Epoch 8  64.9% | batch:       501 of       772\t|\tloss: 0.587074\n",
      "Training Epoch 8  65.0% | batch:       502 of       772\t|\tloss: 0.652666\n",
      "Training Epoch 8  65.2% | batch:       503 of       772\t|\tloss: 0.705386\n",
      "Training Epoch 8  65.3% | batch:       504 of       772\t|\tloss: 0.78089\n",
      "Training Epoch 8  65.4% | batch:       505 of       772\t|\tloss: 0.762149\n",
      "Training Epoch 8  65.5% | batch:       506 of       772\t|\tloss: 0.759738\n",
      "Training Epoch 8  65.7% | batch:       507 of       772\t|\tloss: 0.463164\n",
      "Training Epoch 8  65.8% | batch:       508 of       772\t|\tloss: 0.641741\n",
      "Training Epoch 8  65.9% | batch:       509 of       772\t|\tloss: 0.710179\n",
      "Training Epoch 8  66.1% | batch:       510 of       772\t|\tloss: 1.07685\n",
      "Training Epoch 8  66.2% | batch:       511 of       772\t|\tloss: 0.975345\n",
      "Training Epoch 8  66.3% | batch:       512 of       772\t|\tloss: 0.51252\n",
      "Training Epoch 8  66.5% | batch:       513 of       772\t|\tloss: 0.884536\n",
      "Training Epoch 8  66.6% | batch:       514 of       772\t|\tloss: 1.08519\n",
      "Training Epoch 8  66.7% | batch:       515 of       772\t|\tloss: 1.14399\n",
      "Training Epoch 8  66.8% | batch:       516 of       772\t|\tloss: 0.438886\n",
      "Training Epoch 8  67.0% | batch:       517 of       772\t|\tloss: 0.691445\n",
      "Training Epoch 8  67.1% | batch:       518 of       772\t|\tloss: 0.626\n",
      "Training Epoch 8  67.2% | batch:       519 of       772\t|\tloss: 0.69329\n",
      "Training Epoch 8  67.4% | batch:       520 of       772\t|\tloss: 0.434053\n",
      "Training Epoch 8  67.5% | batch:       521 of       772\t|\tloss: 0.7032\n",
      "Training Epoch 8  67.6% | batch:       522 of       772\t|\tloss: 0.867115\n",
      "Training Epoch 8  67.7% | batch:       523 of       772\t|\tloss: 0.995735\n",
      "Training Epoch 8  67.9% | batch:       524 of       772\t|\tloss: 0.560105\n",
      "Training Epoch 8  68.0% | batch:       525 of       772\t|\tloss: 0.653385\n",
      "Training Epoch 8  68.1% | batch:       526 of       772\t|\tloss: 0.681069\n",
      "Training Epoch 8  68.3% | batch:       527 of       772\t|\tloss: 0.571447\n",
      "Training Epoch 8  68.4% | batch:       528 of       772\t|\tloss: 0.670238\n",
      "Training Epoch 8  68.5% | batch:       529 of       772\t|\tloss: 0.706454\n",
      "Training Epoch 8  68.7% | batch:       530 of       772\t|\tloss: 0.480705\n",
      "Training Epoch 8  68.8% | batch:       531 of       772\t|\tloss: 0.590429\n",
      "Training Epoch 8  68.9% | batch:       532 of       772\t|\tloss: 0.531027\n",
      "Training Epoch 8  69.0% | batch:       533 of       772\t|\tloss: 0.783883\n",
      "Training Epoch 8  69.2% | batch:       534 of       772\t|\tloss: 0.60116\n",
      "Training Epoch 8  69.3% | batch:       535 of       772\t|\tloss: 0.507936\n",
      "Training Epoch 8  69.4% | batch:       536 of       772\t|\tloss: 0.793892\n",
      "Training Epoch 8  69.6% | batch:       537 of       772\t|\tloss: 1.08756\n",
      "Training Epoch 8  69.7% | batch:       538 of       772\t|\tloss: 1.04125\n",
      "Training Epoch 8  69.8% | batch:       539 of       772\t|\tloss: 0.628394\n",
      "Training Epoch 8  69.9% | batch:       540 of       772\t|\tloss: 0.896741\n",
      "Training Epoch 8  70.1% | batch:       541 of       772\t|\tloss: 0.970851\n",
      "Training Epoch 8  70.2% | batch:       542 of       772\t|\tloss: 0.81655\n",
      "Training Epoch 8  70.3% | batch:       543 of       772\t|\tloss: 0.477526\n",
      "Training Epoch 8  70.5% | batch:       544 of       772\t|\tloss: 0.798478\n",
      "Training Epoch 8  70.6% | batch:       545 of       772\t|\tloss: 0.776453\n",
      "Training Epoch 8  70.7% | batch:       546 of       772\t|\tloss: 1.06918\n",
      "Training Epoch 8  70.9% | batch:       547 of       772\t|\tloss: 0.515877\n",
      "Training Epoch 8  71.0% | batch:       548 of       772\t|\tloss: 0.596697\n",
      "Training Epoch 8  71.1% | batch:       549 of       772\t|\tloss: 1.01337\n",
      "Training Epoch 8  71.2% | batch:       550 of       772\t|\tloss: 0.694553\n",
      "Training Epoch 8  71.4% | batch:       551 of       772\t|\tloss: 0.672067\n",
      "Training Epoch 8  71.5% | batch:       552 of       772\t|\tloss: 0.662636\n",
      "Training Epoch 8  71.6% | batch:       553 of       772\t|\tloss: 0.69674\n",
      "Training Epoch 8  71.8% | batch:       554 of       772\t|\tloss: 0.528407\n",
      "Training Epoch 8  71.9% | batch:       555 of       772\t|\tloss: 0.703433\n",
      "Training Epoch 8  72.0% | batch:       556 of       772\t|\tloss: 0.752494\n",
      "Training Epoch 8  72.2% | batch:       557 of       772\t|\tloss: 0.573148\n",
      "Training Epoch 8  72.3% | batch:       558 of       772\t|\tloss: 0.684746\n",
      "Training Epoch 8  72.4% | batch:       559 of       772\t|\tloss: 0.46306\n",
      "Training Epoch 8  72.5% | batch:       560 of       772\t|\tloss: 0.545482\n",
      "Training Epoch 8  72.7% | batch:       561 of       772\t|\tloss: 0.702219\n",
      "Training Epoch 8  72.8% | batch:       562 of       772\t|\tloss: 0.725248\n",
      "Training Epoch 8  72.9% | batch:       563 of       772\t|\tloss: 0.597753\n",
      "Training Epoch 8  73.1% | batch:       564 of       772\t|\tloss: 0.612147\n",
      "Training Epoch 8  73.2% | batch:       565 of       772\t|\tloss: 0.555498\n",
      "Training Epoch 8  73.3% | batch:       566 of       772\t|\tloss: 0.682667\n",
      "Training Epoch 8  73.4% | batch:       567 of       772\t|\tloss: 0.671313\n",
      "Training Epoch 8  73.6% | batch:       568 of       772\t|\tloss: 0.483045\n",
      "Training Epoch 8  73.7% | batch:       569 of       772\t|\tloss: 0.711311\n",
      "Training Epoch 8  73.8% | batch:       570 of       772\t|\tloss: 0.464777\n",
      "Training Epoch 8  74.0% | batch:       571 of       772\t|\tloss: 0.548549\n",
      "Training Epoch 8  74.1% | batch:       572 of       772\t|\tloss: 0.796194\n",
      "Training Epoch 8  74.2% | batch:       573 of       772\t|\tloss: 0.806649\n",
      "Training Epoch 8  74.4% | batch:       574 of       772\t|\tloss: 0.427203\n",
      "Training Epoch 8  74.5% | batch:       575 of       772\t|\tloss: 0.741904\n",
      "Training Epoch 8  74.6% | batch:       576 of       772\t|\tloss: 1.15027\n",
      "Training Epoch 8  74.7% | batch:       577 of       772\t|\tloss: 0.958865\n",
      "Training Epoch 8  74.9% | batch:       578 of       772\t|\tloss: 1.18877\n",
      "Training Epoch 8  75.0% | batch:       579 of       772\t|\tloss: 0.539329\n",
      "Training Epoch 8  75.1% | batch:       580 of       772\t|\tloss: 0.682842\n",
      "Training Epoch 8  75.3% | batch:       581 of       772\t|\tloss: 0.704514\n",
      "Training Epoch 8  75.4% | batch:       582 of       772\t|\tloss: 0.715569\n",
      "Training Epoch 8  75.5% | batch:       583 of       772\t|\tloss: 0.515408\n",
      "Training Epoch 8  75.6% | batch:       584 of       772\t|\tloss: 0.65721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  75.8% | batch:       585 of       772\t|\tloss: 0.777429\n",
      "Training Epoch 8  75.9% | batch:       586 of       772\t|\tloss: 0.71346\n",
      "Training Epoch 8  76.0% | batch:       587 of       772\t|\tloss: 0.497442\n",
      "Training Epoch 8  76.2% | batch:       588 of       772\t|\tloss: 0.441345\n",
      "Training Epoch 8  76.3% | batch:       589 of       772\t|\tloss: 0.865738\n",
      "Training Epoch 8  76.4% | batch:       590 of       772\t|\tloss: 0.571875\n",
      "Training Epoch 8  76.6% | batch:       591 of       772\t|\tloss: 0.68545\n",
      "Training Epoch 8  76.7% | batch:       592 of       772\t|\tloss: 0.69765\n",
      "Training Epoch 8  76.8% | batch:       593 of       772\t|\tloss: 0.959321\n",
      "Training Epoch 8  76.9% | batch:       594 of       772\t|\tloss: 0.816982\n",
      "Training Epoch 8  77.1% | batch:       595 of       772\t|\tloss: 0.845435\n",
      "Training Epoch 8  77.2% | batch:       596 of       772\t|\tloss: 0.386596\n",
      "Training Epoch 8  77.3% | batch:       597 of       772\t|\tloss: 0.492562\n",
      "Training Epoch 8  77.5% | batch:       598 of       772\t|\tloss: 0.739592\n",
      "Training Epoch 8  77.6% | batch:       599 of       772\t|\tloss: 0.458811\n",
      "Training Epoch 8  77.7% | batch:       600 of       772\t|\tloss: 0.616709\n",
      "Training Epoch 8  77.8% | batch:       601 of       772\t|\tloss: 0.607946\n",
      "Training Epoch 8  78.0% | batch:       602 of       772\t|\tloss: 0.732267\n",
      "Training Epoch 8  78.1% | batch:       603 of       772\t|\tloss: 0.489942\n",
      "Training Epoch 8  78.2% | batch:       604 of       772\t|\tloss: 0.65827\n",
      "Training Epoch 8  78.4% | batch:       605 of       772\t|\tloss: 0.791252\n",
      "Training Epoch 8  78.5% | batch:       606 of       772\t|\tloss: 0.531546\n",
      "Training Epoch 8  78.6% | batch:       607 of       772\t|\tloss: 0.4548\n",
      "Training Epoch 8  78.8% | batch:       608 of       772\t|\tloss: 0.523989\n",
      "Training Epoch 8  78.9% | batch:       609 of       772\t|\tloss: 0.519626\n",
      "Training Epoch 8  79.0% | batch:       610 of       772\t|\tloss: 0.679343\n",
      "Training Epoch 8  79.1% | batch:       611 of       772\t|\tloss: 0.541147\n",
      "Training Epoch 8  79.3% | batch:       612 of       772\t|\tloss: 0.813052\n",
      "Training Epoch 8  79.4% | batch:       613 of       772\t|\tloss: 0.924236\n",
      "Training Epoch 8  79.5% | batch:       614 of       772\t|\tloss: 0.531257\n",
      "Training Epoch 8  79.7% | batch:       615 of       772\t|\tloss: 0.567235\n",
      "Training Epoch 8  79.8% | batch:       616 of       772\t|\tloss: 0.482268\n",
      "Training Epoch 8  79.9% | batch:       617 of       772\t|\tloss: 0.585794\n",
      "Training Epoch 8  80.1% | batch:       618 of       772\t|\tloss: 0.647309\n",
      "Training Epoch 8  80.2% | batch:       619 of       772\t|\tloss: 0.860568\n",
      "Training Epoch 8  80.3% | batch:       620 of       772\t|\tloss: 0.693413\n",
      "Training Epoch 8  80.4% | batch:       621 of       772\t|\tloss: 0.62143\n",
      "Training Epoch 8  80.6% | batch:       622 of       772\t|\tloss: 0.564028\n",
      "Training Epoch 8  80.7% | batch:       623 of       772\t|\tloss: 0.587743\n",
      "Training Epoch 8  80.8% | batch:       624 of       772\t|\tloss: 0.72591\n",
      "Training Epoch 8  81.0% | batch:       625 of       772\t|\tloss: 0.731412\n",
      "Training Epoch 8  81.1% | batch:       626 of       772\t|\tloss: 0.526185\n",
      "Training Epoch 8  81.2% | batch:       627 of       772\t|\tloss: 0.576929\n",
      "Training Epoch 8  81.3% | batch:       628 of       772\t|\tloss: 0.47227\n",
      "Training Epoch 8  81.5% | batch:       629 of       772\t|\tloss: 0.521844\n",
      "Training Epoch 8  81.6% | batch:       630 of       772\t|\tloss: 0.46309\n",
      "Training Epoch 8  81.7% | batch:       631 of       772\t|\tloss: 0.925438\n",
      "Training Epoch 8  81.9% | batch:       632 of       772\t|\tloss: 0.788569\n",
      "Training Epoch 8  82.0% | batch:       633 of       772\t|\tloss: 0.572515\n",
      "Training Epoch 8  82.1% | batch:       634 of       772\t|\tloss: 0.584296\n",
      "Training Epoch 8  82.3% | batch:       635 of       772\t|\tloss: 0.966771\n",
      "Training Epoch 8  82.4% | batch:       636 of       772\t|\tloss: 0.838102\n",
      "Training Epoch 8  82.5% | batch:       637 of       772\t|\tloss: 0.904689\n",
      "Training Epoch 8  82.6% | batch:       638 of       772\t|\tloss: 0.911096\n",
      "Training Epoch 8  82.8% | batch:       639 of       772\t|\tloss: 1.20664\n",
      "Training Epoch 8  82.9% | batch:       640 of       772\t|\tloss: 1.00178\n",
      "Training Epoch 8  83.0% | batch:       641 of       772\t|\tloss: 0.95673\n",
      "Training Epoch 8  83.2% | batch:       642 of       772\t|\tloss: 0.505388\n",
      "Training Epoch 8  83.3% | batch:       643 of       772\t|\tloss: 0.720979\n",
      "Training Epoch 8  83.4% | batch:       644 of       772\t|\tloss: 0.704829\n",
      "Training Epoch 8  83.5% | batch:       645 of       772\t|\tloss: 0.677544\n",
      "Training Epoch 8  83.7% | batch:       646 of       772\t|\tloss: 0.655822\n",
      "Training Epoch 8  83.8% | batch:       647 of       772\t|\tloss: 0.923707\n",
      "Training Epoch 8  83.9% | batch:       648 of       772\t|\tloss: 0.954093\n",
      "Training Epoch 8  84.1% | batch:       649 of       772\t|\tloss: 0.899155\n",
      "Training Epoch 8  84.2% | batch:       650 of       772\t|\tloss: 0.781736\n",
      "Training Epoch 8  84.3% | batch:       651 of       772\t|\tloss: 0.65212\n",
      "Training Epoch 8  84.5% | batch:       652 of       772\t|\tloss: 0.699708\n",
      "Training Epoch 8  84.6% | batch:       653 of       772\t|\tloss: 0.784435\n",
      "Training Epoch 8  84.7% | batch:       654 of       772\t|\tloss: 0.738142\n",
      "Training Epoch 8  84.8% | batch:       655 of       772\t|\tloss: 0.605514\n",
      "Training Epoch 8  85.0% | batch:       656 of       772\t|\tloss: 0.618942\n",
      "Training Epoch 8  85.1% | batch:       657 of       772\t|\tloss: 0.75376\n",
      "Training Epoch 8  85.2% | batch:       658 of       772\t|\tloss: 0.537208\n",
      "Training Epoch 8  85.4% | batch:       659 of       772\t|\tloss: 0.629632\n",
      "Training Epoch 8  85.5% | batch:       660 of       772\t|\tloss: 0.633748\n",
      "Training Epoch 8  85.6% | batch:       661 of       772\t|\tloss: 0.72946\n",
      "Training Epoch 8  85.8% | batch:       662 of       772\t|\tloss: 0.553214\n",
      "Training Epoch 8  85.9% | batch:       663 of       772\t|\tloss: 0.534253\n",
      "Training Epoch 8  86.0% | batch:       664 of       772\t|\tloss: 0.582306\n",
      "Training Epoch 8  86.1% | batch:       665 of       772\t|\tloss: 0.582558\n",
      "Training Epoch 8  86.3% | batch:       666 of       772\t|\tloss: 0.768292\n",
      "Training Epoch 8  86.4% | batch:       667 of       772\t|\tloss: 0.589452\n",
      "Training Epoch 8  86.5% | batch:       668 of       772\t|\tloss: 0.604411\n",
      "Training Epoch 8  86.7% | batch:       669 of       772\t|\tloss: 0.632324\n",
      "Training Epoch 8  86.8% | batch:       670 of       772\t|\tloss: 0.82125\n",
      "Training Epoch 8  86.9% | batch:       671 of       772\t|\tloss: 0.539511\n",
      "Training Epoch 8  87.0% | batch:       672 of       772\t|\tloss: 0.607046\n",
      "Training Epoch 8  87.2% | batch:       673 of       772\t|\tloss: 0.724903\n",
      "Training Epoch 8  87.3% | batch:       674 of       772\t|\tloss: 0.902554\n",
      "Training Epoch 8  87.4% | batch:       675 of       772\t|\tloss: 0.761089\n",
      "Training Epoch 8  87.6% | batch:       676 of       772\t|\tloss: 0.53158\n",
      "Training Epoch 8  87.7% | batch:       677 of       772\t|\tloss: 0.695428\n",
      "Training Epoch 8  87.8% | batch:       678 of       772\t|\tloss: 0.402748\n",
      "Training Epoch 8  88.0% | batch:       679 of       772\t|\tloss: 0.656134\n",
      "Training Epoch 8  88.1% | batch:       680 of       772\t|\tloss: 0.448023\n",
      "Training Epoch 8  88.2% | batch:       681 of       772\t|\tloss: 0.792625\n",
      "Training Epoch 8  88.3% | batch:       682 of       772\t|\tloss: 0.723191\n",
      "Training Epoch 8  88.5% | batch:       683 of       772\t|\tloss: 1.06585\n",
      "Training Epoch 8  88.6% | batch:       684 of       772\t|\tloss: 0.793915\n",
      "Training Epoch 8  88.7% | batch:       685 of       772\t|\tloss: 0.644436\n",
      "Training Epoch 8  88.9% | batch:       686 of       772\t|\tloss: 0.429214\n",
      "Training Epoch 8  89.0% | batch:       687 of       772\t|\tloss: 0.647434\n",
      "Training Epoch 8  89.1% | batch:       688 of       772\t|\tloss: 0.575958\n",
      "Training Epoch 8  89.2% | batch:       689 of       772\t|\tloss: 0.804788\n",
      "Training Epoch 8  89.4% | batch:       690 of       772\t|\tloss: 1.09255\n",
      "Training Epoch 8  89.5% | batch:       691 of       772\t|\tloss: 0.720144\n",
      "Training Epoch 8  89.6% | batch:       692 of       772\t|\tloss: 0.642729\n",
      "Training Epoch 8  89.8% | batch:       693 of       772\t|\tloss: 0.457674\n",
      "Training Epoch 8  89.9% | batch:       694 of       772\t|\tloss: 0.544499\n",
      "Training Epoch 8  90.0% | batch:       695 of       772\t|\tloss: 0.604382\n",
      "Training Epoch 8  90.2% | batch:       696 of       772\t|\tloss: 0.706086\n",
      "Training Epoch 8  90.3% | batch:       697 of       772\t|\tloss: 0.582888\n",
      "Training Epoch 8  90.4% | batch:       698 of       772\t|\tloss: 0.597136\n",
      "Training Epoch 8  90.5% | batch:       699 of       772\t|\tloss: 0.540999\n",
      "Training Epoch 8  90.7% | batch:       700 of       772\t|\tloss: 0.584933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  90.8% | batch:       701 of       772\t|\tloss: 0.581547\n",
      "Training Epoch 8  90.9% | batch:       702 of       772\t|\tloss: 0.548901\n",
      "Training Epoch 8  91.1% | batch:       703 of       772\t|\tloss: 0.542365\n",
      "Training Epoch 8  91.2% | batch:       704 of       772\t|\tloss: 0.639187\n",
      "Training Epoch 8  91.3% | batch:       705 of       772\t|\tloss: 0.44612\n",
      "Training Epoch 8  91.5% | batch:       706 of       772\t|\tloss: 0.922643\n",
      "Training Epoch 8  91.6% | batch:       707 of       772\t|\tloss: 0.611734\n",
      "Training Epoch 8  91.7% | batch:       708 of       772\t|\tloss: 0.511801\n",
      "Training Epoch 8  91.8% | batch:       709 of       772\t|\tloss: 1.06675\n",
      "Training Epoch 8  92.0% | batch:       710 of       772\t|\tloss: 0.48128\n",
      "Training Epoch 8  92.1% | batch:       711 of       772\t|\tloss: 0.739628\n",
      "Training Epoch 8  92.2% | batch:       712 of       772\t|\tloss: 1.02336\n",
      "Training Epoch 8  92.4% | batch:       713 of       772\t|\tloss: 0.64799\n",
      "Training Epoch 8  92.5% | batch:       714 of       772\t|\tloss: 0.742956\n",
      "Training Epoch 8  92.6% | batch:       715 of       772\t|\tloss: 0.604499\n",
      "Training Epoch 8  92.7% | batch:       716 of       772\t|\tloss: 0.575233\n",
      "Training Epoch 8  92.9% | batch:       717 of       772\t|\tloss: 0.782571\n",
      "Training Epoch 8  93.0% | batch:       718 of       772\t|\tloss: 0.677957\n",
      "Training Epoch 8  93.1% | batch:       719 of       772\t|\tloss: 0.576206\n",
      "Training Epoch 8  93.3% | batch:       720 of       772\t|\tloss: 0.644259\n",
      "Training Epoch 8  93.4% | batch:       721 of       772\t|\tloss: 0.615842\n",
      "Training Epoch 8  93.5% | batch:       722 of       772\t|\tloss: 0.921575\n",
      "Training Epoch 8  93.7% | batch:       723 of       772\t|\tloss: 0.778636\n",
      "Training Epoch 8  93.8% | batch:       724 of       772\t|\tloss: 0.607233\n",
      "Training Epoch 8  93.9% | batch:       725 of       772\t|\tloss: 0.68806\n",
      "Training Epoch 8  94.0% | batch:       726 of       772\t|\tloss: 1.23621\n",
      "Training Epoch 8  94.2% | batch:       727 of       772\t|\tloss: 0.658998\n",
      "Training Epoch 8  94.3% | batch:       728 of       772\t|\tloss: 0.761383\n",
      "Training Epoch 8  94.4% | batch:       729 of       772\t|\tloss: 0.738354\n",
      "Training Epoch 8  94.6% | batch:       730 of       772\t|\tloss: 0.816523\n",
      "Training Epoch 8  94.7% | batch:       731 of       772\t|\tloss: 1.06734\n",
      "Training Epoch 8  94.8% | batch:       732 of       772\t|\tloss: 0.639716\n",
      "Training Epoch 8  94.9% | batch:       733 of       772\t|\tloss: 0.855053\n",
      "Training Epoch 8  95.1% | batch:       734 of       772\t|\tloss: 1.04479\n",
      "Training Epoch 8  95.2% | batch:       735 of       772\t|\tloss: 0.704074\n",
      "Training Epoch 8  95.3% | batch:       736 of       772\t|\tloss: 0.58747\n",
      "Training Epoch 8  95.5% | batch:       737 of       772\t|\tloss: 0.675211\n",
      "Training Epoch 8  95.6% | batch:       738 of       772\t|\tloss: 0.469013\n",
      "Training Epoch 8  95.7% | batch:       739 of       772\t|\tloss: 0.480148\n",
      "Training Epoch 8  95.9% | batch:       740 of       772\t|\tloss: 0.650718\n",
      "Training Epoch 8  96.0% | batch:       741 of       772\t|\tloss: 0.727747\n",
      "Training Epoch 8  96.1% | batch:       742 of       772\t|\tloss: 0.455732\n",
      "Training Epoch 8  96.2% | batch:       743 of       772\t|\tloss: 0.802074\n",
      "Training Epoch 8  96.4% | batch:       744 of       772\t|\tloss: 0.708717\n",
      "Training Epoch 8  96.5% | batch:       745 of       772\t|\tloss: 0.544035\n",
      "Training Epoch 8  96.6% | batch:       746 of       772\t|\tloss: 0.695697\n",
      "Training Epoch 8  96.8% | batch:       747 of       772\t|\tloss: 0.587025\n",
      "Training Epoch 8  96.9% | batch:       748 of       772\t|\tloss: 0.687672\n",
      "Training Epoch 8  97.0% | batch:       749 of       772\t|\tloss: 0.503621\n",
      "Training Epoch 8  97.2% | batch:       750 of       772\t|\tloss: 0.551369\n",
      "Training Epoch 8  97.3% | batch:       751 of       772\t|\tloss: 0.503058\n",
      "Training Epoch 8  97.4% | batch:       752 of       772\t|\tloss: 0.636625\n",
      "Training Epoch 8  97.5% | batch:       753 of       772\t|\tloss: 0.636215\n",
      "Training Epoch 8  97.7% | batch:       754 of       772\t|\tloss: 0.521558\n",
      "Training Epoch 8  97.8% | batch:       755 of       772\t|\tloss: 0.563967\n",
      "Training Epoch 8  97.9% | batch:       756 of       772\t|\tloss: 0.750562\n",
      "Training Epoch 8  98.1% | batch:       757 of       772\t|\tloss: 0.925704\n",
      "Training Epoch 8  98.2% | batch:       758 of       772\t|\tloss: 0.611714\n",
      "Training Epoch 8  98.3% | batch:       759 of       772\t|\tloss: 0.623345\n",
      "Training Epoch 8  98.4% | batch:       760 of       772\t|\tloss: 0.684036\n",
      "Training Epoch 8  98.6% | batch:       761 of       772\t|\tloss: 0.535515\n",
      "Training Epoch 8  98.7% | batch:       762 of       772\t|\tloss: 0.466917\n",
      "Training Epoch 8  98.8% | batch:       763 of       772\t|\tloss: 0.590222\n",
      "Training Epoch 8  99.0% | batch:       764 of       772\t|\tloss: 0.643737\n",
      "Training Epoch 8  99.1% | batch:       765 of       772\t|\tloss: 1.02057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:22:25,981 | INFO : Epoch 8 Training Summary: epoch: 8.000000 | loss: 0.703434 | \n",
      "2023-05-24 10:22:25,982 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 15.06045126914978 seconds\n",
      "\n",
      "2023-05-24 10:22:25,982 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 16.49991887807846 seconds\n",
      "2023-05-24 10:22:25,983 | INFO : Avg batch train. time: 0.021372951914609407 seconds\n",
      "2023-05-24 10:22:25,983 | INFO : Avg sample train. time: 0.00016697956643874816 seconds\n",
      "2023-05-24 10:22:25,984 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  99.2% | batch:       766 of       772\t|\tloss: 0.828867\n",
      "Training Epoch 8  99.4% | batch:       767 of       772\t|\tloss: 0.821865\n",
      "Training Epoch 8  99.5% | batch:       768 of       772\t|\tloss: 0.54635\n",
      "Training Epoch 8  99.6% | batch:       769 of       772\t|\tloss: 0.925279\n",
      "Training Epoch 8  99.7% | batch:       770 of       772\t|\tloss: 1.01645\n",
      "Training Epoch 8  99.9% | batch:       771 of       772\t|\tloss: 0.532805\n",
      "\n",
      "Evaluating Epoch 8   0.0% | batch:         0 of        92\t|\tloss: 1.03835\n",
      "Evaluating Epoch 8   1.1% | batch:         1 of        92\t|\tloss: 8.89648\n",
      "Evaluating Epoch 8   2.2% | batch:         2 of        92\t|\tloss: 2.88539\n",
      "Evaluating Epoch 8   3.3% | batch:         3 of        92\t|\tloss: 6.27855\n",
      "Evaluating Epoch 8   4.3% | batch:         4 of        92\t|\tloss: 3.45928\n",
      "Evaluating Epoch 8   5.4% | batch:         5 of        92\t|\tloss: 7.91718\n",
      "Evaluating Epoch 8   6.5% | batch:         6 of        92\t|\tloss: 4.72726\n",
      "Evaluating Epoch 8   7.6% | batch:         7 of        92\t|\tloss: 2.26157\n",
      "Evaluating Epoch 8   8.7% | batch:         8 of        92\t|\tloss: 6.43085\n",
      "Evaluating Epoch 8   9.8% | batch:         9 of        92\t|\tloss: 4.65136\n",
      "Evaluating Epoch 8  10.9% | batch:        10 of        92\t|\tloss: 5.74179\n",
      "Evaluating Epoch 8  12.0% | batch:        11 of        92\t|\tloss: 3.90101\n",
      "Evaluating Epoch 8  13.0% | batch:        12 of        92\t|\tloss: 7.16272\n",
      "Evaluating Epoch 8  14.1% | batch:        13 of        92\t|\tloss: 6.00128\n",
      "Evaluating Epoch 8  15.2% | batch:        14 of        92\t|\tloss: 1.89497\n",
      "Evaluating Epoch 8  16.3% | batch:        15 of        92\t|\tloss: 0.93808\n",
      "Evaluating Epoch 8  17.4% | batch:        16 of        92\t|\tloss: 3.24148\n",
      "Evaluating Epoch 8  18.5% | batch:        17 of        92\t|\tloss: 1.83288\n",
      "Evaluating Epoch 8  19.6% | batch:        18 of        92\t|\tloss: 3.96049\n",
      "Evaluating Epoch 8  20.7% | batch:        19 of        92\t|\tloss: 5.54779\n",
      "Evaluating Epoch 8  21.7% | batch:        20 of        92\t|\tloss: 3.44716\n",
      "Evaluating Epoch 8  22.8% | batch:        21 of        92\t|\tloss: 3.84213\n",
      "Evaluating Epoch 8  23.9% | batch:        22 of        92\t|\tloss: 5.10481\n",
      "Evaluating Epoch 8  25.0% | batch:        23 of        92\t|\tloss: 6.89278\n",
      "Evaluating Epoch 8  26.1% | batch:        24 of        92\t|\tloss: 2.15184\n",
      "Evaluating Epoch 8  27.2% | batch:        25 of        92\t|\tloss: 0.433155\n",
      "Evaluating Epoch 8  28.3% | batch:        26 of        92\t|\tloss: 1.69432\n",
      "Evaluating Epoch 8  29.3% | batch:        27 of        92\t|\tloss: 3.46558\n",
      "Evaluating Epoch 8  30.4% | batch:        28 of        92\t|\tloss: 3.39023\n",
      "Evaluating Epoch 8  31.5% | batch:        29 of        92\t|\tloss: 3.50514\n",
      "Evaluating Epoch 8  32.6% | batch:        30 of        92\t|\tloss: 2.98443\n",
      "Evaluating Epoch 8  33.7% | batch:        31 of        92\t|\tloss: 4.43174\n",
      "Evaluating Epoch 8  34.8% | batch:        32 of        92\t|\tloss: 2.91536\n",
      "Evaluating Epoch 8  35.9% | batch:        33 of        92\t|\tloss: 5.3806\n",
      "Evaluating Epoch 8  37.0% | batch:        34 of        92\t|\tloss: 3.97672\n",
      "Evaluating Epoch 8  38.0% | batch:        35 of        92\t|\tloss: 2.84057\n",
      "Evaluating Epoch 8  39.1% | batch:        36 of        92\t|\tloss: 1.93642\n",
      "Evaluating Epoch 8  40.2% | batch:        37 of        92\t|\tloss: 2.9877\n",
      "Evaluating Epoch 8  41.3% | batch:        38 of        92\t|\tloss: 2.39755\n",
      "Evaluating Epoch 8  42.4% | batch:        39 of        92\t|\tloss: 7.63813\n",
      "Evaluating Epoch 8  43.5% | batch:        40 of        92\t|\tloss: 3.13954\n",
      "Evaluating Epoch 8  44.6% | batch:        41 of        92\t|\tloss: 5.43391\n",
      "Evaluating Epoch 8  45.7% | batch:        42 of        92\t|\tloss: 4.0505\n",
      "Evaluating Epoch 8  46.7% | batch:        43 of        92\t|\tloss: 7.77947\n",
      "Evaluating Epoch 8  47.8% | batch:        44 of        92\t|\tloss: 2.17258\n",
      "Evaluating Epoch 8  48.9% | batch:        45 of        92\t|\tloss: 1.74171\n",
      "Evaluating Epoch 8  50.0% | batch:        46 of        92\t|\tloss: 2.14616\n",
      "Evaluating Epoch 8  51.1% | batch:        47 of        92\t|\tloss: 4.22811\n",
      "Evaluating Epoch 8  52.2% | batch:        48 of        92\t|\tloss: 6.32828\n",
      "Evaluating Epoch 8  53.3% | batch:        49 of        92\t|\tloss: 4.43489\n",
      "Evaluating Epoch 8  54.3% | batch:        50 of        92\t|\tloss: 5.06936\n",
      "Evaluating Epoch 8  55.4% | batch:        51 of        92\t|\tloss: 6.23229\n",
      "Evaluating Epoch 8  56.5% | batch:        52 of        92\t|\tloss: 6.63969\n",
      "Evaluating Epoch 8  57.6% | batch:        53 of        92\t|\tloss: 1.83748\n",
      "Evaluating Epoch 8  58.7% | batch:        54 of        92\t|\tloss: 1.88166\n",
      "Evaluating Epoch 8  59.8% | batch:        55 of        92\t|\tloss: 5.25384\n",
      "Evaluating Epoch 8  60.9% | batch:        56 of        92\t|\tloss: 6.87303\n",
      "Evaluating Epoch 8  62.0% | batch:        57 of        92\t|\tloss: 4.69195\n",
      "Evaluating Epoch 8  63.0% | batch:        58 of        92\t|\tloss: 4.49613\n",
      "Evaluating Epoch 8  64.1% | batch:        59 of        92\t|\tloss: 6.33304\n",
      "Evaluating Epoch 8  65.2% | batch:        60 of        92\t|\tloss: 6.735\n",
      "Evaluating Epoch 8  66.3% | batch:        61 of        92\t|\tloss: 2.00317\n",
      "Evaluating Epoch 8  67.4% | batch:        62 of        92\t|\tloss: 0.821195\n",
      "Evaluating Epoch 8  68.5% | batch:        63 of        92\t|\tloss: 1.83922\n",
      "Evaluating Epoch 8  69.6% | batch:        64 of        92\t|\tloss: 4.22537\n",
      "Evaluating Epoch 8  70.7% | batch:        65 of        92\t|\tloss: 7.45711\n",
      "Evaluating Epoch 8  71.7% | batch:        66 of        92\t|\tloss: 3.24463\n",
      "Evaluating Epoch 8  72.8% | batch:        67 of        92\t|\tloss: 4.23863\n",
      "Evaluating Epoch 8  73.9% | batch:        68 of        92\t|\tloss: 5.12853\n",
      "Evaluating Epoch 8  75.0% | batch:        69 of        92\t|\tloss: 3.64821\n",
      "Evaluating Epoch 8  76.1% | batch:        70 of        92\t|\tloss: 5.86452\n",
      "Evaluating Epoch 8  77.2% | batch:        71 of        92\t|\tloss: 3.91605\n",
      "Evaluating Epoch 8  78.3% | batch:        72 of        92\t|\tloss: 3.75527\n",
      "Evaluating Epoch 8  79.3% | batch:        73 of        92\t|\tloss: 2.51863\n",
      "Evaluating Epoch 8  80.4% | batch:        74 of        92\t|\tloss: 4.89039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:22:27,095 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.1106715202331543 seconds\n",
      "\n",
      "2023-05-24 10:22:27,095 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.3462597926457722 seconds\n",
      "2023-05-24 10:22:27,096 | INFO : Avg batch val. time: 0.014633258615714916 seconds\n",
      "2023-05-24 10:22:27,096 | INFO : Avg sample val. time: 0.00011540029081482704 seconds\n",
      "2023-05-24 10:22:27,096 | INFO : Epoch 8 Validation Summary: epoch: 8.000000 | loss: 4.168174 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 8  81.5% | batch:        75 of        92\t|\tloss: 1.74715\n",
      "Evaluating Epoch 8  82.6% | batch:        76 of        92\t|\tloss: 3.94566\n",
      "Evaluating Epoch 8  83.7% | batch:        77 of        92\t|\tloss: 5.30144\n",
      "Evaluating Epoch 8  84.8% | batch:        78 of        92\t|\tloss: 4.7244\n",
      "Evaluating Epoch 8  85.9% | batch:        79 of        92\t|\tloss: 4.97486\n",
      "Evaluating Epoch 8  87.0% | batch:        80 of        92\t|\tloss: 4.95725\n",
      "Evaluating Epoch 8  88.0% | batch:        81 of        92\t|\tloss: 6.28125\n",
      "Evaluating Epoch 8  89.1% | batch:        82 of        92\t|\tloss: 1.61766\n",
      "Evaluating Epoch 8  90.2% | batch:        83 of        92\t|\tloss: 1.86861\n",
      "Evaluating Epoch 8  91.3% | batch:        84 of        92\t|\tloss: 4.12088\n",
      "Evaluating Epoch 8  92.4% | batch:        85 of        92\t|\tloss: 6.21369\n",
      "Evaluating Epoch 8  93.5% | batch:        86 of        92\t|\tloss: 4.34919\n",
      "Evaluating Epoch 8  94.6% | batch:        87 of        92\t|\tloss: 4.17388\n",
      "Evaluating Epoch 8  95.7% | batch:        88 of        92\t|\tloss: 5.55625\n",
      "Evaluating Epoch 8  96.7% | batch:        89 of        92\t|\tloss: 6.70182\n",
      "Evaluating Epoch 8  97.8% | batch:        90 of        92\t|\tloss: 2.0617\n",
      "Evaluating Epoch 8  98.9% | batch:        91 of        92\t|\tloss: 0.423952\n",
      "\n",
      "Training Epoch 9   0.0% | batch:         0 of       772\t|\tloss: 0.618906\n",
      "Training Epoch 9   0.1% | batch:         1 of       772\t|\tloss: 0.646901\n",
      "Training Epoch 9   0.3% | batch:         2 of       772\t|\tloss: 0.637319\n",
      "Training Epoch 9   0.4% | batch:         3 of       772\t|\tloss: 0.472993\n",
      "Training Epoch 9   0.5% | batch:         4 of       772\t|\tloss: 0.455316\n",
      "Training Epoch 9   0.6% | batch:         5 of       772\t|\tloss: 0.469128\n",
      "Training Epoch 9   0.8% | batch:         6 of       772\t|\tloss: 0.544911\n",
      "Training Epoch 9   0.9% | batch:         7 of       772\t|\tloss: 0.494473\n",
      "Training Epoch 9   1.0% | batch:         8 of       772\t|\tloss: 0.536776\n",
      "Training Epoch 9   1.2% | batch:         9 of       772\t|\tloss: 0.627247\n",
      "Training Epoch 9   1.3% | batch:        10 of       772\t|\tloss: 0.564762\n",
      "Training Epoch 9   1.4% | batch:        11 of       772\t|\tloss: 0.573026\n",
      "Training Epoch 9   1.6% | batch:        12 of       772\t|\tloss: 0.923864\n",
      "Training Epoch 9   1.7% | batch:        13 of       772\t|\tloss: 0.703119\n",
      "Training Epoch 9   1.8% | batch:        14 of       772\t|\tloss: 0.808073\n",
      "Training Epoch 9   1.9% | batch:        15 of       772\t|\tloss: 0.546417\n",
      "Training Epoch 9   2.1% | batch:        16 of       772\t|\tloss: 0.626608\n",
      "Training Epoch 9   2.2% | batch:        17 of       772\t|\tloss: 0.7141\n",
      "Training Epoch 9   2.3% | batch:        18 of       772\t|\tloss: 0.545515\n",
      "Training Epoch 9   2.5% | batch:        19 of       772\t|\tloss: 0.756226\n",
      "Training Epoch 9   2.6% | batch:        20 of       772\t|\tloss: 0.613431\n",
      "Training Epoch 9   2.7% | batch:        21 of       772\t|\tloss: 0.467207\n",
      "Training Epoch 9   2.8% | batch:        22 of       772\t|\tloss: 0.772879\n",
      "Training Epoch 9   3.0% | batch:        23 of       772\t|\tloss: 0.665915\n",
      "Training Epoch 9   3.1% | batch:        24 of       772\t|\tloss: 0.783345\n",
      "Training Epoch 9   3.2% | batch:        25 of       772\t|\tloss: 0.704286\n",
      "Training Epoch 9   3.4% | batch:        26 of       772\t|\tloss: 0.804777\n",
      "Training Epoch 9   3.5% | batch:        27 of       772\t|\tloss: 0.733952\n",
      "Training Epoch 9   3.6% | batch:        28 of       772\t|\tloss: 0.522561\n",
      "Training Epoch 9   3.8% | batch:        29 of       772\t|\tloss: 0.540771\n",
      "Training Epoch 9   3.9% | batch:        30 of       772\t|\tloss: 0.744307\n",
      "Training Epoch 9   4.0% | batch:        31 of       772\t|\tloss: 0.633567\n",
      "Training Epoch 9   4.1% | batch:        32 of       772\t|\tloss: 0.786992\n",
      "Training Epoch 9   4.3% | batch:        33 of       772\t|\tloss: 0.889624\n",
      "Training Epoch 9   4.4% | batch:        34 of       772\t|\tloss: 0.667509\n",
      "Training Epoch 9   4.5% | batch:        35 of       772\t|\tloss: 1.04112\n",
      "Training Epoch 9   4.7% | batch:        36 of       772\t|\tloss: 0.874741\n",
      "Training Epoch 9   4.8% | batch:        37 of       772\t|\tloss: 0.748653\n",
      "Training Epoch 9   4.9% | batch:        38 of       772\t|\tloss: 0.638754\n",
      "Training Epoch 9   5.1% | batch:        39 of       772\t|\tloss: 0.633713\n",
      "Training Epoch 9   5.2% | batch:        40 of       772\t|\tloss: 0.595343\n",
      "Training Epoch 9   5.3% | batch:        41 of       772\t|\tloss: 0.594404\n",
      "Training Epoch 9   5.4% | batch:        42 of       772\t|\tloss: 0.468103\n",
      "Training Epoch 9   5.6% | batch:        43 of       772\t|\tloss: 0.715106\n",
      "Training Epoch 9   5.7% | batch:        44 of       772\t|\tloss: 0.657341\n",
      "Training Epoch 9   5.8% | batch:        45 of       772\t|\tloss: 0.708432\n",
      "Training Epoch 9   6.0% | batch:        46 of       772\t|\tloss: 0.905545\n",
      "Training Epoch 9   6.1% | batch:        47 of       772\t|\tloss: 0.820484\n",
      "Training Epoch 9   6.2% | batch:        48 of       772\t|\tloss: 0.865118\n",
      "Training Epoch 9   6.3% | batch:        49 of       772\t|\tloss: 0.452612\n",
      "Training Epoch 9   6.5% | batch:        50 of       772\t|\tloss: 1.232\n",
      "Training Epoch 9   6.6% | batch:        51 of       772\t|\tloss: 0.88331\n",
      "Training Epoch 9   6.7% | batch:        52 of       772\t|\tloss: 1.29435\n",
      "Training Epoch 9   6.9% | batch:        53 of       772\t|\tloss: 0.726368\n",
      "Training Epoch 9   7.0% | batch:        54 of       772\t|\tloss: 0.699421\n",
      "Training Epoch 9   7.1% | batch:        55 of       772\t|\tloss: 0.780335\n",
      "Training Epoch 9   7.3% | batch:        56 of       772\t|\tloss: 0.705461\n",
      "Training Epoch 9   7.4% | batch:        57 of       772\t|\tloss: 0.618896\n",
      "Training Epoch 9   7.5% | batch:        58 of       772\t|\tloss: 0.491968\n",
      "Training Epoch 9   7.6% | batch:        59 of       772\t|\tloss: 0.965804\n",
      "Training Epoch 9   7.8% | batch:        60 of       772\t|\tloss: 0.726161\n",
      "Training Epoch 9   7.9% | batch:        61 of       772\t|\tloss: 0.553329\n",
      "Training Epoch 9   8.0% | batch:        62 of       772\t|\tloss: 0.498365\n",
      "Training Epoch 9   8.2% | batch:        63 of       772\t|\tloss: 0.489463\n",
      "Training Epoch 9   8.3% | batch:        64 of       772\t|\tloss: 0.733372\n",
      "Training Epoch 9   8.4% | batch:        65 of       772\t|\tloss: 0.519815\n",
      "Training Epoch 9   8.5% | batch:        66 of       772\t|\tloss: 0.708262\n",
      "Training Epoch 9   8.7% | batch:        67 of       772\t|\tloss: 0.545093\n",
      "Training Epoch 9   8.8% | batch:        68 of       772\t|\tloss: 0.869084\n",
      "Training Epoch 9   8.9% | batch:        69 of       772\t|\tloss: 0.606214\n",
      "Training Epoch 9   9.1% | batch:        70 of       772\t|\tloss: 0.571014\n",
      "Training Epoch 9   9.2% | batch:        71 of       772\t|\tloss: 0.604622\n",
      "Training Epoch 9   9.3% | batch:        72 of       772\t|\tloss: 0.431504\n",
      "Training Epoch 9   9.5% | batch:        73 of       772\t|\tloss: 0.677271\n",
      "Training Epoch 9   9.6% | batch:        74 of       772\t|\tloss: 0.849886\n",
      "Training Epoch 9   9.7% | batch:        75 of       772\t|\tloss: 0.673745\n",
      "Training Epoch 9   9.8% | batch:        76 of       772\t|\tloss: 0.947369\n",
      "Training Epoch 9  10.0% | batch:        77 of       772\t|\tloss: 0.630055\n",
      "Training Epoch 9  10.1% | batch:        78 of       772\t|\tloss: 0.714372\n",
      "Training Epoch 9  10.2% | batch:        79 of       772\t|\tloss: 0.679212\n",
      "Training Epoch 9  10.4% | batch:        80 of       772\t|\tloss: 0.993798\n",
      "Training Epoch 9  10.5% | batch:        81 of       772\t|\tloss: 1.55318\n",
      "Training Epoch 9  10.6% | batch:        82 of       772\t|\tloss: 1.05554\n",
      "Training Epoch 9  10.8% | batch:        83 of       772\t|\tloss: 0.893124\n",
      "Training Epoch 9  10.9% | batch:        84 of       772\t|\tloss: 0.503215\n",
      "Training Epoch 9  11.0% | batch:        85 of       772\t|\tloss: 0.900557\n",
      "Training Epoch 9  11.1% | batch:        86 of       772\t|\tloss: 1.54949\n",
      "Training Epoch 9  11.3% | batch:        87 of       772\t|\tloss: 1.12997\n",
      "Training Epoch 9  11.4% | batch:        88 of       772\t|\tloss: 0.934786\n",
      "Training Epoch 9  11.5% | batch:        89 of       772\t|\tloss: 0.855176\n",
      "Training Epoch 9  11.7% | batch:        90 of       772\t|\tloss: 1.00813\n",
      "Training Epoch 9  11.8% | batch:        91 of       772\t|\tloss: 1.07232\n",
      "Training Epoch 9  11.9% | batch:        92 of       772\t|\tloss: 1.34862\n",
      "Training Epoch 9  12.0% | batch:        93 of       772\t|\tloss: 1.0385\n",
      "Training Epoch 9  12.2% | batch:        94 of       772\t|\tloss: 0.543042\n",
      "Training Epoch 9  12.3% | batch:        95 of       772\t|\tloss: 0.685267\n",
      "Training Epoch 9  12.4% | batch:        96 of       772\t|\tloss: 1.13975\n",
      "Training Epoch 9  12.6% | batch:        97 of       772\t|\tloss: 1.0813\n",
      "Training Epoch 9  12.7% | batch:        98 of       772\t|\tloss: 1.38588\n",
      "Training Epoch 9  12.8% | batch:        99 of       772\t|\tloss: 0.630592\n",
      "Training Epoch 9  13.0% | batch:       100 of       772\t|\tloss: 0.866733\n",
      "Training Epoch 9  13.1% | batch:       101 of       772\t|\tloss: 1.24728\n",
      "Training Epoch 9  13.2% | batch:       102 of       772\t|\tloss: 1.35059\n",
      "Training Epoch 9  13.3% | batch:       103 of       772\t|\tloss: 0.812286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  13.5% | batch:       104 of       772\t|\tloss: 0.54166\n",
      "Training Epoch 9  13.6% | batch:       105 of       772\t|\tloss: 0.935378\n",
      "Training Epoch 9  13.7% | batch:       106 of       772\t|\tloss: 1.14665\n",
      "Training Epoch 9  13.9% | batch:       107 of       772\t|\tloss: 0.758217\n",
      "Training Epoch 9  14.0% | batch:       108 of       772\t|\tloss: 1.00887\n",
      "Training Epoch 9  14.1% | batch:       109 of       772\t|\tloss: 0.658101\n",
      "Training Epoch 9  14.2% | batch:       110 of       772\t|\tloss: 0.812683\n",
      "Training Epoch 9  14.4% | batch:       111 of       772\t|\tloss: 1.70084\n",
      "Training Epoch 9  14.5% | batch:       112 of       772\t|\tloss: 1.04475\n",
      "Training Epoch 9  14.6% | batch:       113 of       772\t|\tloss: 1.06058\n",
      "Training Epoch 9  14.8% | batch:       114 of       772\t|\tloss: 0.566211\n",
      "Training Epoch 9  14.9% | batch:       115 of       772\t|\tloss: 0.757688\n",
      "Training Epoch 9  15.0% | batch:       116 of       772\t|\tloss: 1.15887\n",
      "Training Epoch 9  15.2% | batch:       117 of       772\t|\tloss: 1.34998\n",
      "Training Epoch 9  15.3% | batch:       118 of       772\t|\tloss: 1.14689\n",
      "Training Epoch 9  15.4% | batch:       119 of       772\t|\tloss: 0.588727\n",
      "Training Epoch 9  15.5% | batch:       120 of       772\t|\tloss: 0.822039\n",
      "Training Epoch 9  15.7% | batch:       121 of       772\t|\tloss: 0.876394\n",
      "Training Epoch 9  15.8% | batch:       122 of       772\t|\tloss: 0.912428\n",
      "Training Epoch 9  15.9% | batch:       123 of       772\t|\tloss: 0.672271\n",
      "Training Epoch 9  16.1% | batch:       124 of       772\t|\tloss: 0.752251\n",
      "Training Epoch 9  16.2% | batch:       125 of       772\t|\tloss: 0.795659\n",
      "Training Epoch 9  16.3% | batch:       126 of       772\t|\tloss: 0.710971\n",
      "Training Epoch 9  16.5% | batch:       127 of       772\t|\tloss: 0.565054\n",
      "Training Epoch 9  16.6% | batch:       128 of       772\t|\tloss: 0.606072\n",
      "Training Epoch 9  16.7% | batch:       129 of       772\t|\tloss: 0.703012\n",
      "Training Epoch 9  16.8% | batch:       130 of       772\t|\tloss: 0.825747\n",
      "Training Epoch 9  17.0% | batch:       131 of       772\t|\tloss: 0.631734\n",
      "Training Epoch 9  17.1% | batch:       132 of       772\t|\tloss: 0.565437\n",
      "Training Epoch 9  17.2% | batch:       133 of       772\t|\tloss: 0.850328\n",
      "Training Epoch 9  17.4% | batch:       134 of       772\t|\tloss: 0.574002\n",
      "Training Epoch 9  17.5% | batch:       135 of       772\t|\tloss: 0.749055\n",
      "Training Epoch 9  17.6% | batch:       136 of       772\t|\tloss: 0.918257\n",
      "Training Epoch 9  17.7% | batch:       137 of       772\t|\tloss: 0.684277\n",
      "Training Epoch 9  17.9% | batch:       138 of       772\t|\tloss: 0.549766\n",
      "Training Epoch 9  18.0% | batch:       139 of       772\t|\tloss: 0.499399\n",
      "Training Epoch 9  18.1% | batch:       140 of       772\t|\tloss: 1.09753\n",
      "Training Epoch 9  18.3% | batch:       141 of       772\t|\tloss: 0.919711\n",
      "Training Epoch 9  18.4% | batch:       142 of       772\t|\tloss: 0.595245\n",
      "Training Epoch 9  18.5% | batch:       143 of       772\t|\tloss: 0.719279\n",
      "Training Epoch 9  18.7% | batch:       144 of       772\t|\tloss: 0.561677\n",
      "Training Epoch 9  18.8% | batch:       145 of       772\t|\tloss: 0.564543\n",
      "Training Epoch 9  18.9% | batch:       146 of       772\t|\tloss: 0.852374\n",
      "Training Epoch 9  19.0% | batch:       147 of       772\t|\tloss: 1.15898\n",
      "Training Epoch 9  19.2% | batch:       148 of       772\t|\tloss: 0.717363\n",
      "Training Epoch 9  19.3% | batch:       149 of       772\t|\tloss: 0.840784\n",
      "Training Epoch 9  19.4% | batch:       150 of       772\t|\tloss: 0.710159\n",
      "Training Epoch 9  19.6% | batch:       151 of       772\t|\tloss: 0.781647\n",
      "Training Epoch 9  19.7% | batch:       152 of       772\t|\tloss: 0.664378\n",
      "Training Epoch 9  19.8% | batch:       153 of       772\t|\tloss: 0.74277\n",
      "Training Epoch 9  19.9% | batch:       154 of       772\t|\tloss: 0.711876\n",
      "Training Epoch 9  20.1% | batch:       155 of       772\t|\tloss: 1.07569\n",
      "Training Epoch 9  20.2% | batch:       156 of       772\t|\tloss: 0.619746\n",
      "Training Epoch 9  20.3% | batch:       157 of       772\t|\tloss: 0.438694\n",
      "Training Epoch 9  20.5% | batch:       158 of       772\t|\tloss: 0.67661\n",
      "Training Epoch 9  20.6% | batch:       159 of       772\t|\tloss: 0.620289\n",
      "Training Epoch 9  20.7% | batch:       160 of       772\t|\tloss: 0.818569\n",
      "Training Epoch 9  20.9% | batch:       161 of       772\t|\tloss: 0.674245\n",
      "Training Epoch 9  21.0% | batch:       162 of       772\t|\tloss: 0.668252\n",
      "Training Epoch 9  21.1% | batch:       163 of       772\t|\tloss: 0.920149\n",
      "Training Epoch 9  21.2% | batch:       164 of       772\t|\tloss: 0.983659\n",
      "Training Epoch 9  21.4% | batch:       165 of       772\t|\tloss: 0.7182\n",
      "Training Epoch 9  21.5% | batch:       166 of       772\t|\tloss: 0.701365\n",
      "Training Epoch 9  21.6% | batch:       167 of       772\t|\tloss: 0.533182\n",
      "Training Epoch 9  21.8% | batch:       168 of       772\t|\tloss: 0.503793\n",
      "Training Epoch 9  21.9% | batch:       169 of       772\t|\tloss: 0.649485\n",
      "Training Epoch 9  22.0% | batch:       170 of       772\t|\tloss: 0.576444\n",
      "Training Epoch 9  22.2% | batch:       171 of       772\t|\tloss: 0.934841\n",
      "Training Epoch 9  22.3% | batch:       172 of       772\t|\tloss: 0.624521\n",
      "Training Epoch 9  22.4% | batch:       173 of       772\t|\tloss: 0.891303\n",
      "Training Epoch 9  22.5% | batch:       174 of       772\t|\tloss: 0.782079\n",
      "Training Epoch 9  22.7% | batch:       175 of       772\t|\tloss: 0.6501\n",
      "Training Epoch 9  22.8% | batch:       176 of       772\t|\tloss: 0.515057\n",
      "Training Epoch 9  22.9% | batch:       177 of       772\t|\tloss: 0.493989\n",
      "Training Epoch 9  23.1% | batch:       178 of       772\t|\tloss: 0.721415\n",
      "Training Epoch 9  23.2% | batch:       179 of       772\t|\tloss: 0.631142\n",
      "Training Epoch 9  23.3% | batch:       180 of       772\t|\tloss: 0.499399\n",
      "Training Epoch 9  23.4% | batch:       181 of       772\t|\tloss: 0.503028\n",
      "Training Epoch 9  23.6% | batch:       182 of       772\t|\tloss: 0.497806\n",
      "Training Epoch 9  23.7% | batch:       183 of       772\t|\tloss: 0.619318\n",
      "Training Epoch 9  23.8% | batch:       184 of       772\t|\tloss: 1.11809\n",
      "Training Epoch 9  24.0% | batch:       185 of       772\t|\tloss: 0.831253\n",
      "Training Epoch 9  24.1% | batch:       186 of       772\t|\tloss: 0.511884\n",
      "Training Epoch 9  24.2% | batch:       187 of       772\t|\tloss: 0.76503\n",
      "Training Epoch 9  24.4% | batch:       188 of       772\t|\tloss: 0.613078\n",
      "Training Epoch 9  24.5% | batch:       189 of       772\t|\tloss: 0.544099\n",
      "Training Epoch 9  24.6% | batch:       190 of       772\t|\tloss: 0.592461\n",
      "Training Epoch 9  24.7% | batch:       191 of       772\t|\tloss: 0.467237\n",
      "Training Epoch 9  24.9% | batch:       192 of       772\t|\tloss: 0.483584\n",
      "Training Epoch 9  25.0% | batch:       193 of       772\t|\tloss: 0.65912\n",
      "Training Epoch 9  25.1% | batch:       194 of       772\t|\tloss: 1.0298\n",
      "Training Epoch 9  25.3% | batch:       195 of       772\t|\tloss: 0.764431\n",
      "Training Epoch 9  25.4% | batch:       196 of       772\t|\tloss: 0.528081\n",
      "Training Epoch 9  25.5% | batch:       197 of       772\t|\tloss: 0.473704\n",
      "Training Epoch 9  25.6% | batch:       198 of       772\t|\tloss: 0.722225\n",
      "Training Epoch 9  25.8% | batch:       199 of       772\t|\tloss: 0.602773\n",
      "Training Epoch 9  25.9% | batch:       200 of       772\t|\tloss: 0.403996\n",
      "Training Epoch 9  26.0% | batch:       201 of       772\t|\tloss: 0.675335\n",
      "Training Epoch 9  26.2% | batch:       202 of       772\t|\tloss: 0.671053\n",
      "Training Epoch 9  26.3% | batch:       203 of       772\t|\tloss: 0.789917\n",
      "Training Epoch 9  26.4% | batch:       204 of       772\t|\tloss: 0.415282\n",
      "Training Epoch 9  26.6% | batch:       205 of       772\t|\tloss: 0.728604\n",
      "Training Epoch 9  26.7% | batch:       206 of       772\t|\tloss: 0.566871\n",
      "Training Epoch 9  26.8% | batch:       207 of       772\t|\tloss: 0.483422\n",
      "Training Epoch 9  26.9% | batch:       208 of       772\t|\tloss: 0.568211\n",
      "Training Epoch 9  27.1% | batch:       209 of       772\t|\tloss: 0.439541\n",
      "Training Epoch 9  27.2% | batch:       210 of       772\t|\tloss: 0.486597\n",
      "Training Epoch 9  27.3% | batch:       211 of       772\t|\tloss: 0.739037\n",
      "Training Epoch 9  27.5% | batch:       212 of       772\t|\tloss: 0.640152\n",
      "Training Epoch 9  27.6% | batch:       213 of       772\t|\tloss: 0.744292\n",
      "Training Epoch 9  27.7% | batch:       214 of       772\t|\tloss: 1.1537\n",
      "Training Epoch 9  27.8% | batch:       215 of       772\t|\tloss: 0.800789\n",
      "Training Epoch 9  28.0% | batch:       216 of       772\t|\tloss: 0.779351\n",
      "Training Epoch 9  28.1% | batch:       217 of       772\t|\tloss: 0.749165\n",
      "Training Epoch 9  28.2% | batch:       218 of       772\t|\tloss: 0.454158\n",
      "Training Epoch 9  28.4% | batch:       219 of       772\t|\tloss: 0.779419\n",
      "Training Epoch 9  28.5% | batch:       220 of       772\t|\tloss: 0.916706\n",
      "Training Epoch 9  28.6% | batch:       221 of       772\t|\tloss: 0.659639\n",
      "Training Epoch 9  28.8% | batch:       222 of       772\t|\tloss: 0.548782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  28.9% | batch:       223 of       772\t|\tloss: 0.650134\n",
      "Training Epoch 9  29.0% | batch:       224 of       772\t|\tloss: 0.537684\n",
      "Training Epoch 9  29.1% | batch:       225 of       772\t|\tloss: 0.476928\n",
      "Training Epoch 9  29.3% | batch:       226 of       772\t|\tloss: 0.563866\n",
      "Training Epoch 9  29.4% | batch:       227 of       772\t|\tloss: 0.501845\n",
      "Training Epoch 9  29.5% | batch:       228 of       772\t|\tloss: 0.462951\n",
      "Training Epoch 9  29.7% | batch:       229 of       772\t|\tloss: 0.413667\n",
      "Training Epoch 9  29.8% | batch:       230 of       772\t|\tloss: 0.787279\n",
      "Training Epoch 9  29.9% | batch:       231 of       772\t|\tloss: 0.796858\n",
      "Training Epoch 9  30.1% | batch:       232 of       772\t|\tloss: 0.486879\n",
      "Training Epoch 9  30.2% | batch:       233 of       772\t|\tloss: 0.809373\n",
      "Training Epoch 9  30.3% | batch:       234 of       772\t|\tloss: 0.707304\n",
      "Training Epoch 9  30.4% | batch:       235 of       772\t|\tloss: 0.650341\n",
      "Training Epoch 9  30.6% | batch:       236 of       772\t|\tloss: 0.706911\n",
      "Training Epoch 9  30.7% | batch:       237 of       772\t|\tloss: 0.699956\n",
      "Training Epoch 9  30.8% | batch:       238 of       772\t|\tloss: 0.708001\n",
      "Training Epoch 9  31.0% | batch:       239 of       772\t|\tloss: 0.72619\n",
      "Training Epoch 9  31.1% | batch:       240 of       772\t|\tloss: 0.571901\n",
      "Training Epoch 9  31.2% | batch:       241 of       772\t|\tloss: 0.678329\n",
      "Training Epoch 9  31.3% | batch:       242 of       772\t|\tloss: 1.03338\n",
      "Training Epoch 9  31.5% | batch:       243 of       772\t|\tloss: 0.708461\n",
      "Training Epoch 9  31.6% | batch:       244 of       772\t|\tloss: 0.961482\n",
      "Training Epoch 9  31.7% | batch:       245 of       772\t|\tloss: 0.642968\n",
      "Training Epoch 9  31.9% | batch:       246 of       772\t|\tloss: 0.963274\n",
      "Training Epoch 9  32.0% | batch:       247 of       772\t|\tloss: 1.0585\n",
      "Training Epoch 9  32.1% | batch:       248 of       772\t|\tloss: 0.712038\n",
      "Training Epoch 9  32.3% | batch:       249 of       772\t|\tloss: 0.670578\n",
      "Training Epoch 9  32.4% | batch:       250 of       772\t|\tloss: 0.952229\n",
      "Training Epoch 9  32.5% | batch:       251 of       772\t|\tloss: 0.592392\n",
      "Training Epoch 9  32.6% | batch:       252 of       772\t|\tloss: 0.553537\n",
      "Training Epoch 9  32.8% | batch:       253 of       772\t|\tloss: 0.407289\n",
      "Training Epoch 9  32.9% | batch:       254 of       772\t|\tloss: 0.614369\n",
      "Training Epoch 9  33.0% | batch:       255 of       772\t|\tloss: 0.584243\n",
      "Training Epoch 9  33.2% | batch:       256 of       772\t|\tloss: 0.751202\n",
      "Training Epoch 9  33.3% | batch:       257 of       772\t|\tloss: 0.777571\n",
      "Training Epoch 9  33.4% | batch:       258 of       772\t|\tloss: 0.785189\n",
      "Training Epoch 9  33.5% | batch:       259 of       772\t|\tloss: 0.778483\n",
      "Training Epoch 9  33.7% | batch:       260 of       772\t|\tloss: 0.781681\n",
      "Training Epoch 9  33.8% | batch:       261 of       772\t|\tloss: 0.428717\n",
      "Training Epoch 9  33.9% | batch:       262 of       772\t|\tloss: 0.804167\n",
      "Training Epoch 9  34.1% | batch:       263 of       772\t|\tloss: 1.2808\n",
      "Training Epoch 9  34.2% | batch:       264 of       772\t|\tloss: 0.630367\n",
      "Training Epoch 9  34.3% | batch:       265 of       772\t|\tloss: 0.733539\n",
      "Training Epoch 9  34.5% | batch:       266 of       772\t|\tloss: 0.634159\n",
      "Training Epoch 9  34.6% | batch:       267 of       772\t|\tloss: 0.569576\n",
      "Training Epoch 9  34.7% | batch:       268 of       772\t|\tloss: 0.502809\n",
      "Training Epoch 9  34.8% | batch:       269 of       772\t|\tloss: 0.51142\n",
      "Training Epoch 9  35.0% | batch:       270 of       772\t|\tloss: 0.657106\n",
      "Training Epoch 9  35.1% | batch:       271 of       772\t|\tloss: 0.625129\n",
      "Training Epoch 9  35.2% | batch:       272 of       772\t|\tloss: 0.531671\n",
      "Training Epoch 9  35.4% | batch:       273 of       772\t|\tloss: 0.5575\n",
      "Training Epoch 9  35.5% | batch:       274 of       772\t|\tloss: 0.539615\n",
      "Training Epoch 9  35.6% | batch:       275 of       772\t|\tloss: 0.876642\n",
      "Training Epoch 9  35.8% | batch:       276 of       772\t|\tloss: 0.55254\n",
      "Training Epoch 9  35.9% | batch:       277 of       772\t|\tloss: 0.477652\n",
      "Training Epoch 9  36.0% | batch:       278 of       772\t|\tloss: 0.513199\n",
      "Training Epoch 9  36.1% | batch:       279 of       772\t|\tloss: 0.904643\n",
      "Training Epoch 9  36.3% | batch:       280 of       772\t|\tloss: 0.693665\n",
      "Training Epoch 9  36.4% | batch:       281 of       772\t|\tloss: 0.730894\n",
      "Training Epoch 9  36.5% | batch:       282 of       772\t|\tloss: 0.781446\n",
      "Training Epoch 9  36.7% | batch:       283 of       772\t|\tloss: 0.613039\n",
      "Training Epoch 9  36.8% | batch:       284 of       772\t|\tloss: 0.747792\n",
      "Training Epoch 9  36.9% | batch:       285 of       772\t|\tloss: 0.696192\n",
      "Training Epoch 9  37.0% | batch:       286 of       772\t|\tloss: 0.530243\n",
      "Training Epoch 9  37.2% | batch:       287 of       772\t|\tloss: 0.609952\n",
      "Training Epoch 9  37.3% | batch:       288 of       772\t|\tloss: 0.613671\n",
      "Training Epoch 9  37.4% | batch:       289 of       772\t|\tloss: 0.676744\n",
      "Training Epoch 9  37.6% | batch:       290 of       772\t|\tloss: 0.50992\n",
      "Training Epoch 9  37.7% | batch:       291 of       772\t|\tloss: 0.650967\n",
      "Training Epoch 9  37.8% | batch:       292 of       772\t|\tloss: 0.465143\n",
      "Training Epoch 9  38.0% | batch:       293 of       772\t|\tloss: 0.595123\n",
      "Training Epoch 9  38.1% | batch:       294 of       772\t|\tloss: 0.438572\n",
      "Training Epoch 9  38.2% | batch:       295 of       772\t|\tloss: 0.498064\n",
      "Training Epoch 9  38.3% | batch:       296 of       772\t|\tloss: 0.443212\n",
      "Training Epoch 9  38.5% | batch:       297 of       772\t|\tloss: 0.382319\n",
      "Training Epoch 9  38.6% | batch:       298 of       772\t|\tloss: 0.734948\n",
      "Training Epoch 9  38.7% | batch:       299 of       772\t|\tloss: 0.635648\n",
      "Training Epoch 9  38.9% | batch:       300 of       772\t|\tloss: 0.562288\n",
      "Training Epoch 9  39.0% | batch:       301 of       772\t|\tloss: 0.628359\n",
      "Training Epoch 9  39.1% | batch:       302 of       772\t|\tloss: 0.547193\n",
      "Training Epoch 9  39.2% | batch:       303 of       772\t|\tloss: 0.670303\n",
      "Training Epoch 9  39.4% | batch:       304 of       772\t|\tloss: 0.719724\n",
      "Training Epoch 9  39.5% | batch:       305 of       772\t|\tloss: 0.625759\n",
      "Training Epoch 9  39.6% | batch:       306 of       772\t|\tloss: 0.526785\n",
      "Training Epoch 9  39.8% | batch:       307 of       772\t|\tloss: 0.462828\n",
      "Training Epoch 9  39.9% | batch:       308 of       772\t|\tloss: 0.54479\n",
      "Training Epoch 9  40.0% | batch:       309 of       772\t|\tloss: 0.684556\n",
      "Training Epoch 9  40.2% | batch:       310 of       772\t|\tloss: 0.520713\n",
      "Training Epoch 9  40.3% | batch:       311 of       772\t|\tloss: 0.643926\n",
      "Training Epoch 9  40.4% | batch:       312 of       772\t|\tloss: 0.533612\n",
      "Training Epoch 9  40.5% | batch:       313 of       772\t|\tloss: 0.455439\n",
      "Training Epoch 9  40.7% | batch:       314 of       772\t|\tloss: 0.560734\n",
      "Training Epoch 9  40.8% | batch:       315 of       772\t|\tloss: 0.726702\n",
      "Training Epoch 9  40.9% | batch:       316 of       772\t|\tloss: 0.458443\n",
      "Training Epoch 9  41.1% | batch:       317 of       772\t|\tloss: 0.527254\n",
      "Training Epoch 9  41.2% | batch:       318 of       772\t|\tloss: 0.714596\n",
      "Training Epoch 9  41.3% | batch:       319 of       772\t|\tloss: 0.592438\n",
      "Training Epoch 9  41.5% | batch:       320 of       772\t|\tloss: 0.51764\n",
      "Training Epoch 9  41.6% | batch:       321 of       772\t|\tloss: 0.861462\n",
      "Training Epoch 9  41.7% | batch:       322 of       772\t|\tloss: 0.724278\n",
      "Training Epoch 9  41.8% | batch:       323 of       772\t|\tloss: 0.719248\n",
      "Training Epoch 9  42.0% | batch:       324 of       772\t|\tloss: 0.87812\n",
      "Training Epoch 9  42.1% | batch:       325 of       772\t|\tloss: 0.672749\n",
      "Training Epoch 9  42.2% | batch:       326 of       772\t|\tloss: 0.615777\n",
      "Training Epoch 9  42.4% | batch:       327 of       772\t|\tloss: 0.753289\n",
      "Training Epoch 9  42.5% | batch:       328 of       772\t|\tloss: 0.746136\n",
      "Training Epoch 9  42.6% | batch:       329 of       772\t|\tloss: 0.893326\n",
      "Training Epoch 9  42.7% | batch:       330 of       772\t|\tloss: 0.702544\n",
      "Training Epoch 9  42.9% | batch:       331 of       772\t|\tloss: 0.5347\n",
      "Training Epoch 9  43.0% | batch:       332 of       772\t|\tloss: 0.733794\n",
      "Training Epoch 9  43.1% | batch:       333 of       772\t|\tloss: 0.712298\n",
      "Training Epoch 9  43.3% | batch:       334 of       772\t|\tloss: 0.64734\n",
      "Training Epoch 9  43.4% | batch:       335 of       772\t|\tloss: 0.681345\n",
      "Training Epoch 9  43.5% | batch:       336 of       772\t|\tloss: 0.504508\n",
      "Training Epoch 9  43.7% | batch:       337 of       772\t|\tloss: 0.583716\n",
      "Training Epoch 9  43.8% | batch:       338 of       772\t|\tloss: 0.412681\n",
      "Training Epoch 9  43.9% | batch:       339 of       772\t|\tloss: 0.46641\n",
      "Training Epoch 9  44.0% | batch:       340 of       772\t|\tloss: 0.6084\n",
      "Training Epoch 9  44.2% | batch:       341 of       772\t|\tloss: 0.440373\n",
      "Training Epoch 9  44.3% | batch:       342 of       772\t|\tloss: 0.555398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  44.4% | batch:       343 of       772\t|\tloss: 0.516716\n",
      "Training Epoch 9  44.6% | batch:       344 of       772\t|\tloss: 0.658644\n",
      "Training Epoch 9  44.7% | batch:       345 of       772\t|\tloss: 0.5443\n",
      "Training Epoch 9  44.8% | batch:       346 of       772\t|\tloss: 0.37293\n",
      "Training Epoch 9  44.9% | batch:       347 of       772\t|\tloss: 0.366513\n",
      "Training Epoch 9  45.1% | batch:       348 of       772\t|\tloss: 0.597443\n",
      "Training Epoch 9  45.2% | batch:       349 of       772\t|\tloss: 0.706077\n",
      "Training Epoch 9  45.3% | batch:       350 of       772\t|\tloss: 0.594967\n",
      "Training Epoch 9  45.5% | batch:       351 of       772\t|\tloss: 0.403999\n",
      "Training Epoch 9  45.6% | batch:       352 of       772\t|\tloss: 0.788597\n",
      "Training Epoch 9  45.7% | batch:       353 of       772\t|\tloss: 1.06164\n",
      "Training Epoch 9  45.9% | batch:       354 of       772\t|\tloss: 0.640366\n",
      "Training Epoch 9  46.0% | batch:       355 of       772\t|\tloss: 0.617777\n",
      "Training Epoch 9  46.1% | batch:       356 of       772\t|\tloss: 0.795983\n",
      "Training Epoch 9  46.2% | batch:       357 of       772\t|\tloss: 0.825663\n",
      "Training Epoch 9  46.4% | batch:       358 of       772\t|\tloss: 0.532362\n",
      "Training Epoch 9  46.5% | batch:       359 of       772\t|\tloss: 0.550855\n",
      "Training Epoch 9  46.6% | batch:       360 of       772\t|\tloss: 0.421833\n",
      "Training Epoch 9  46.8% | batch:       361 of       772\t|\tloss: 0.803723\n",
      "Training Epoch 9  46.9% | batch:       362 of       772\t|\tloss: 0.563545\n",
      "Training Epoch 9  47.0% | batch:       363 of       772\t|\tloss: 0.650748\n",
      "Training Epoch 9  47.2% | batch:       364 of       772\t|\tloss: 0.456176\n",
      "Training Epoch 9  47.3% | batch:       365 of       772\t|\tloss: 0.485964\n",
      "Training Epoch 9  47.4% | batch:       366 of       772\t|\tloss: 0.881948\n",
      "Training Epoch 9  47.5% | batch:       367 of       772\t|\tloss: 1.19709\n",
      "Training Epoch 9  47.7% | batch:       368 of       772\t|\tloss: 0.690392\n",
      "Training Epoch 9  47.8% | batch:       369 of       772\t|\tloss: 0.711913\n",
      "Training Epoch 9  47.9% | batch:       370 of       772\t|\tloss: 0.848363\n",
      "Training Epoch 9  48.1% | batch:       371 of       772\t|\tloss: 1.09691\n",
      "Training Epoch 9  48.2% | batch:       372 of       772\t|\tloss: 0.700356\n",
      "Training Epoch 9  48.3% | batch:       373 of       772\t|\tloss: 0.636645\n",
      "Training Epoch 9  48.4% | batch:       374 of       772\t|\tloss: 1.41529\n",
      "Training Epoch 9  48.6% | batch:       375 of       772\t|\tloss: 0.853297\n",
      "Training Epoch 9  48.7% | batch:       376 of       772\t|\tloss: 0.515348\n",
      "Training Epoch 9  48.8% | batch:       377 of       772\t|\tloss: 0.486439\n",
      "Training Epoch 9  49.0% | batch:       378 of       772\t|\tloss: 0.531502\n",
      "Training Epoch 9  49.1% | batch:       379 of       772\t|\tloss: 0.54428\n",
      "Training Epoch 9  49.2% | batch:       380 of       772\t|\tloss: 0.598274\n",
      "Training Epoch 9  49.4% | batch:       381 of       772\t|\tloss: 0.658126\n",
      "Training Epoch 9  49.5% | batch:       382 of       772\t|\tloss: 0.413117\n",
      "Training Epoch 9  49.6% | batch:       383 of       772\t|\tloss: 0.600264\n",
      "Training Epoch 9  49.7% | batch:       384 of       772\t|\tloss: 0.609395\n",
      "Training Epoch 9  49.9% | batch:       385 of       772\t|\tloss: 0.592245\n",
      "Training Epoch 9  50.0% | batch:       386 of       772\t|\tloss: 0.565332\n",
      "Training Epoch 9  50.1% | batch:       387 of       772\t|\tloss: 0.625242\n",
      "Training Epoch 9  50.3% | batch:       388 of       772\t|\tloss: 0.490488\n",
      "Training Epoch 9  50.4% | batch:       389 of       772\t|\tloss: 0.566106\n",
      "Training Epoch 9  50.5% | batch:       390 of       772\t|\tloss: 0.825155\n",
      "Training Epoch 9  50.6% | batch:       391 of       772\t|\tloss: 0.894609\n",
      "Training Epoch 9  50.8% | batch:       392 of       772\t|\tloss: 0.764006\n",
      "Training Epoch 9  50.9% | batch:       393 of       772\t|\tloss: 0.689175\n",
      "Training Epoch 9  51.0% | batch:       394 of       772\t|\tloss: 0.709674\n",
      "Training Epoch 9  51.2% | batch:       395 of       772\t|\tloss: 0.623184\n",
      "Training Epoch 9  51.3% | batch:       396 of       772\t|\tloss: 0.725059\n",
      "Training Epoch 9  51.4% | batch:       397 of       772\t|\tloss: 0.558958\n",
      "Training Epoch 9  51.6% | batch:       398 of       772\t|\tloss: 0.411562\n",
      "Training Epoch 9  51.7% | batch:       399 of       772\t|\tloss: 0.68201\n",
      "Training Epoch 9  51.8% | batch:       400 of       772\t|\tloss: 0.697894\n",
      "Training Epoch 9  51.9% | batch:       401 of       772\t|\tloss: 0.502681\n",
      "Training Epoch 9  52.1% | batch:       402 of       772\t|\tloss: 0.436932\n",
      "Training Epoch 9  52.2% | batch:       403 of       772\t|\tloss: 0.723132\n",
      "Training Epoch 9  52.3% | batch:       404 of       772\t|\tloss: 0.670092\n",
      "Training Epoch 9  52.5% | batch:       405 of       772\t|\tloss: 0.667563\n",
      "Training Epoch 9  52.6% | batch:       406 of       772\t|\tloss: 0.489286\n",
      "Training Epoch 9  52.7% | batch:       407 of       772\t|\tloss: 0.595322\n",
      "Training Epoch 9  52.8% | batch:       408 of       772\t|\tloss: 0.449162\n",
      "Training Epoch 9  53.0% | batch:       409 of       772\t|\tloss: 0.727334\n",
      "Training Epoch 9  53.1% | batch:       410 of       772\t|\tloss: 0.656065\n",
      "Training Epoch 9  53.2% | batch:       411 of       772\t|\tloss: 0.511181\n",
      "Training Epoch 9  53.4% | batch:       412 of       772\t|\tloss: 0.798139\n",
      "Training Epoch 9  53.5% | batch:       413 of       772\t|\tloss: 0.831685\n",
      "Training Epoch 9  53.6% | batch:       414 of       772\t|\tloss: 0.988621\n",
      "Training Epoch 9  53.8% | batch:       415 of       772\t|\tloss: 0.645068\n",
      "Training Epoch 9  53.9% | batch:       416 of       772\t|\tloss: 0.754951\n",
      "Training Epoch 9  54.0% | batch:       417 of       772\t|\tloss: 0.497826\n",
      "Training Epoch 9  54.1% | batch:       418 of       772\t|\tloss: 0.8415\n",
      "Training Epoch 9  54.3% | batch:       419 of       772\t|\tloss: 0.696544\n",
      "Training Epoch 9  54.4% | batch:       420 of       772\t|\tloss: 0.725269\n",
      "Training Epoch 9  54.5% | batch:       421 of       772\t|\tloss: 0.702317\n",
      "Training Epoch 9  54.7% | batch:       422 of       772\t|\tloss: 0.501696\n",
      "Training Epoch 9  54.8% | batch:       423 of       772\t|\tloss: 0.73187\n",
      "Training Epoch 9  54.9% | batch:       424 of       772\t|\tloss: 0.469359\n",
      "Training Epoch 9  55.1% | batch:       425 of       772\t|\tloss: 0.529124\n",
      "Training Epoch 9  55.2% | batch:       426 of       772\t|\tloss: 0.80551\n",
      "Training Epoch 9  55.3% | batch:       427 of       772\t|\tloss: 0.896022\n",
      "Training Epoch 9  55.4% | batch:       428 of       772\t|\tloss: 0.715663\n",
      "Training Epoch 9  55.6% | batch:       429 of       772\t|\tloss: 0.629106\n",
      "Training Epoch 9  55.7% | batch:       430 of       772\t|\tloss: 0.626749\n",
      "Training Epoch 9  55.8% | batch:       431 of       772\t|\tloss: 0.993205\n",
      "Training Epoch 9  56.0% | batch:       432 of       772\t|\tloss: 0.989027\n",
      "Training Epoch 9  56.1% | batch:       433 of       772\t|\tloss: 0.610911\n",
      "Training Epoch 9  56.2% | batch:       434 of       772\t|\tloss: 0.855306\n",
      "Training Epoch 9  56.3% | batch:       435 of       772\t|\tloss: 0.665284\n",
      "Training Epoch 9  56.5% | batch:       436 of       772\t|\tloss: 1.05112\n",
      "Training Epoch 9  56.6% | batch:       437 of       772\t|\tloss: 0.487503\n",
      "Training Epoch 9  56.7% | batch:       438 of       772\t|\tloss: 0.651631\n",
      "Training Epoch 9  56.9% | batch:       439 of       772\t|\tloss: 0.581528\n",
      "Training Epoch 9  57.0% | batch:       440 of       772\t|\tloss: 0.80292\n",
      "Training Epoch 9  57.1% | batch:       441 of       772\t|\tloss: 0.401446\n",
      "Training Epoch 9  57.3% | batch:       442 of       772\t|\tloss: 0.761923\n",
      "Training Epoch 9  57.4% | batch:       443 of       772\t|\tloss: 0.6714\n",
      "Training Epoch 9  57.5% | batch:       444 of       772\t|\tloss: 0.551407\n",
      "Training Epoch 9  57.6% | batch:       445 of       772\t|\tloss: 0.718876\n",
      "Training Epoch 9  57.8% | batch:       446 of       772\t|\tloss: 0.732772\n",
      "Training Epoch 9  57.9% | batch:       447 of       772\t|\tloss: 0.697843\n",
      "Training Epoch 9  58.0% | batch:       448 of       772\t|\tloss: 0.524757\n",
      "Training Epoch 9  58.2% | batch:       449 of       772\t|\tloss: 0.59636\n",
      "Training Epoch 9  58.3% | batch:       450 of       772\t|\tloss: 0.637644\n",
      "Training Epoch 9  58.4% | batch:       451 of       772\t|\tloss: 0.541777\n",
      "Training Epoch 9  58.5% | batch:       452 of       772\t|\tloss: 0.687231\n",
      "Training Epoch 9  58.7% | batch:       453 of       772\t|\tloss: 0.863892\n",
      "Training Epoch 9  58.8% | batch:       454 of       772\t|\tloss: 0.700043\n",
      "Training Epoch 9  58.9% | batch:       455 of       772\t|\tloss: 0.77981\n",
      "Training Epoch 9  59.1% | batch:       456 of       772\t|\tloss: 0.850089\n",
      "Training Epoch 9  59.2% | batch:       457 of       772\t|\tloss: 0.881284\n",
      "Training Epoch 9  59.3% | batch:       458 of       772\t|\tloss: 0.725313\n",
      "Training Epoch 9  59.5% | batch:       459 of       772\t|\tloss: 0.581238\n",
      "Training Epoch 9  59.6% | batch:       460 of       772\t|\tloss: 0.553004\n",
      "Training Epoch 9  59.7% | batch:       461 of       772\t|\tloss: 0.694563\n",
      "Training Epoch 9  59.8% | batch:       462 of       772\t|\tloss: 1.41796\n",
      "Training Epoch 9  60.0% | batch:       463 of       772\t|\tloss: 0.550123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  60.1% | batch:       464 of       772\t|\tloss: 0.629758\n",
      "Training Epoch 9  60.2% | batch:       465 of       772\t|\tloss: 0.719247\n",
      "Training Epoch 9  60.4% | batch:       466 of       772\t|\tloss: 1.2712\n",
      "Training Epoch 9  60.5% | batch:       467 of       772\t|\tloss: 1.20652\n",
      "Training Epoch 9  60.6% | batch:       468 of       772\t|\tloss: 1.12663\n",
      "Training Epoch 9  60.8% | batch:       469 of       772\t|\tloss: 0.990876\n",
      "Training Epoch 9  60.9% | batch:       470 of       772\t|\tloss: 0.571076\n",
      "Training Epoch 9  61.0% | batch:       471 of       772\t|\tloss: 0.988988\n",
      "Training Epoch 9  61.1% | batch:       472 of       772\t|\tloss: 0.85175\n",
      "Training Epoch 9  61.3% | batch:       473 of       772\t|\tloss: 0.624063\n",
      "Training Epoch 9  61.4% | batch:       474 of       772\t|\tloss: 0.76012\n",
      "Training Epoch 9  61.5% | batch:       475 of       772\t|\tloss: 0.827269\n",
      "Training Epoch 9  61.7% | batch:       476 of       772\t|\tloss: 0.842232\n",
      "Training Epoch 9  61.8% | batch:       477 of       772\t|\tloss: 0.505157\n",
      "Training Epoch 9  61.9% | batch:       478 of       772\t|\tloss: 0.748293\n",
      "Training Epoch 9  62.0% | batch:       479 of       772\t|\tloss: 1.55696\n",
      "Training Epoch 9  62.2% | batch:       480 of       772\t|\tloss: 0.818016\n",
      "Training Epoch 9  62.3% | batch:       481 of       772\t|\tloss: 0.871289\n",
      "Training Epoch 9  62.4% | batch:       482 of       772\t|\tloss: 0.561205\n",
      "Training Epoch 9  62.6% | batch:       483 of       772\t|\tloss: 0.68815\n",
      "Training Epoch 9  62.7% | batch:       484 of       772\t|\tloss: 0.805008\n",
      "Training Epoch 9  62.8% | batch:       485 of       772\t|\tloss: 0.521457\n",
      "Training Epoch 9  63.0% | batch:       486 of       772\t|\tloss: 0.430643\n",
      "Training Epoch 9  63.1% | batch:       487 of       772\t|\tloss: 0.673696\n",
      "Training Epoch 9  63.2% | batch:       488 of       772\t|\tloss: 0.546631\n",
      "Training Epoch 9  63.3% | batch:       489 of       772\t|\tloss: 0.538371\n",
      "Training Epoch 9  63.5% | batch:       490 of       772\t|\tloss: 0.563404\n",
      "Training Epoch 9  63.6% | batch:       491 of       772\t|\tloss: 0.739679\n",
      "Training Epoch 9  63.7% | batch:       492 of       772\t|\tloss: 0.575673\n",
      "Training Epoch 9  63.9% | batch:       493 of       772\t|\tloss: 0.641514\n",
      "Training Epoch 9  64.0% | batch:       494 of       772\t|\tloss: 0.524624\n",
      "Training Epoch 9  64.1% | batch:       495 of       772\t|\tloss: 0.559209\n",
      "Training Epoch 9  64.2% | batch:       496 of       772\t|\tloss: 0.51798\n",
      "Training Epoch 9  64.4% | batch:       497 of       772\t|\tloss: 0.51915\n",
      "Training Epoch 9  64.5% | batch:       498 of       772\t|\tloss: 0.665726\n",
      "Training Epoch 9  64.6% | batch:       499 of       772\t|\tloss: 0.556398\n",
      "Training Epoch 9  64.8% | batch:       500 of       772\t|\tloss: 0.620971\n",
      "Training Epoch 9  64.9% | batch:       501 of       772\t|\tloss: 0.711301\n",
      "Training Epoch 9  65.0% | batch:       502 of       772\t|\tloss: 0.686461\n",
      "Training Epoch 9  65.2% | batch:       503 of       772\t|\tloss: 0.827686\n",
      "Training Epoch 9  65.3% | batch:       504 of       772\t|\tloss: 0.756273\n",
      "Training Epoch 9  65.4% | batch:       505 of       772\t|\tloss: 0.425519\n",
      "Training Epoch 9  65.5% | batch:       506 of       772\t|\tloss: 0.734016\n",
      "Training Epoch 9  65.7% | batch:       507 of       772\t|\tloss: 1.42379\n",
      "Training Epoch 9  65.8% | batch:       508 of       772\t|\tloss: 1.30493\n",
      "Training Epoch 9  65.9% | batch:       509 of       772\t|\tloss: 0.718681\n",
      "Training Epoch 9  66.1% | batch:       510 of       772\t|\tloss: 0.707439\n",
      "Training Epoch 9  66.2% | batch:       511 of       772\t|\tloss: 0.59921\n",
      "Training Epoch 9  66.3% | batch:       512 of       772\t|\tloss: 0.549265\n",
      "Training Epoch 9  66.5% | batch:       513 of       772\t|\tloss: 0.595052\n",
      "Training Epoch 9  66.6% | batch:       514 of       772\t|\tloss: 0.831849\n",
      "Training Epoch 9  66.7% | batch:       515 of       772\t|\tloss: 0.590417\n",
      "Training Epoch 9  66.8% | batch:       516 of       772\t|\tloss: 0.550558\n",
      "Training Epoch 9  67.0% | batch:       517 of       772\t|\tloss: 0.704589\n",
      "Training Epoch 9  67.1% | batch:       518 of       772\t|\tloss: 0.704203\n",
      "Training Epoch 9  67.2% | batch:       519 of       772\t|\tloss: 0.454032\n",
      "Training Epoch 9  67.4% | batch:       520 of       772\t|\tloss: 0.435197\n",
      "Training Epoch 9  67.5% | batch:       521 of       772\t|\tloss: 0.57089\n",
      "Training Epoch 9  67.6% | batch:       522 of       772\t|\tloss: 0.598049\n",
      "Training Epoch 9  67.7% | batch:       523 of       772\t|\tloss: 0.821623\n",
      "Training Epoch 9  67.9% | batch:       524 of       772\t|\tloss: 0.440329\n",
      "Training Epoch 9  68.0% | batch:       525 of       772\t|\tloss: 1.04149\n",
      "Training Epoch 9  68.1% | batch:       526 of       772\t|\tloss: 0.569709\n",
      "Training Epoch 9  68.3% | batch:       527 of       772\t|\tloss: 0.591162\n",
      "Training Epoch 9  68.4% | batch:       528 of       772\t|\tloss: 0.643057\n",
      "Training Epoch 9  68.5% | batch:       529 of       772\t|\tloss: 0.548208\n",
      "Training Epoch 9  68.7% | batch:       530 of       772\t|\tloss: 0.712036\n",
      "Training Epoch 9  68.8% | batch:       531 of       772\t|\tloss: 0.448742\n",
      "Training Epoch 9  68.9% | batch:       532 of       772\t|\tloss: 0.701069\n",
      "Training Epoch 9  69.0% | batch:       533 of       772\t|\tloss: 0.685536\n",
      "Training Epoch 9  69.2% | batch:       534 of       772\t|\tloss: 0.630315\n",
      "Training Epoch 9  69.3% | batch:       535 of       772\t|\tloss: 0.669227\n",
      "Training Epoch 9  69.4% | batch:       536 of       772\t|\tloss: 0.848185\n",
      "Training Epoch 9  69.6% | batch:       537 of       772\t|\tloss: 0.900783\n",
      "Training Epoch 9  69.7% | batch:       538 of       772\t|\tloss: 0.75599\n",
      "Training Epoch 9  69.8% | batch:       539 of       772\t|\tloss: 0.563213\n",
      "Training Epoch 9  69.9% | batch:       540 of       772\t|\tloss: 0.716671\n",
      "Training Epoch 9  70.1% | batch:       541 of       772\t|\tloss: 1.08962\n",
      "Training Epoch 9  70.2% | batch:       542 of       772\t|\tloss: 1.02326\n",
      "Training Epoch 9  70.3% | batch:       543 of       772\t|\tloss: 0.783214\n",
      "Training Epoch 9  70.5% | batch:       544 of       772\t|\tloss: 0.439435\n",
      "Training Epoch 9  70.6% | batch:       545 of       772\t|\tloss: 0.879297\n",
      "Training Epoch 9  70.7% | batch:       546 of       772\t|\tloss: 0.659999\n",
      "Training Epoch 9  70.9% | batch:       547 of       772\t|\tloss: 0.56273\n",
      "Training Epoch 9  71.0% | batch:       548 of       772\t|\tloss: 0.511095\n",
      "Training Epoch 9  71.1% | batch:       549 of       772\t|\tloss: 0.617233\n",
      "Training Epoch 9  71.2% | batch:       550 of       772\t|\tloss: 0.558888\n",
      "Training Epoch 9  71.4% | batch:       551 of       772\t|\tloss: 0.485018\n",
      "Training Epoch 9  71.5% | batch:       552 of       772\t|\tloss: 0.803509\n",
      "Training Epoch 9  71.6% | batch:       553 of       772\t|\tloss: 0.687554\n",
      "Training Epoch 9  71.8% | batch:       554 of       772\t|\tloss: 0.622225\n",
      "Training Epoch 9  71.9% | batch:       555 of       772\t|\tloss: 0.998186\n",
      "Training Epoch 9  72.0% | batch:       556 of       772\t|\tloss: 0.41996\n",
      "Training Epoch 9  72.2% | batch:       557 of       772\t|\tloss: 0.534506\n",
      "Training Epoch 9  72.3% | batch:       558 of       772\t|\tloss: 0.543018\n",
      "Training Epoch 9  72.4% | batch:       559 of       772\t|\tloss: 0.450166\n",
      "Training Epoch 9  72.5% | batch:       560 of       772\t|\tloss: 0.532871\n",
      "Training Epoch 9  72.7% | batch:       561 of       772\t|\tloss: 0.651341\n",
      "Training Epoch 9  72.8% | batch:       562 of       772\t|\tloss: 0.440122\n",
      "Training Epoch 9  72.9% | batch:       563 of       772\t|\tloss: 1.01175\n",
      "Training Epoch 9  73.1% | batch:       564 of       772\t|\tloss: 0.706877\n",
      "Training Epoch 9  73.2% | batch:       565 of       772\t|\tloss: 0.517214\n",
      "Training Epoch 9  73.3% | batch:       566 of       772\t|\tloss: 0.694799\n",
      "Training Epoch 9  73.4% | batch:       567 of       772\t|\tloss: 0.714864\n",
      "Training Epoch 9  73.6% | batch:       568 of       772\t|\tloss: 0.622321\n",
      "Training Epoch 9  73.7% | batch:       569 of       772\t|\tloss: 0.659475\n",
      "Training Epoch 9  73.8% | batch:       570 of       772\t|\tloss: 0.853012\n",
      "Training Epoch 9  74.0% | batch:       571 of       772\t|\tloss: 0.676543\n",
      "Training Epoch 9  74.1% | batch:       572 of       772\t|\tloss: 0.630221\n",
      "Training Epoch 9  74.2% | batch:       573 of       772\t|\tloss: 0.797366\n",
      "Training Epoch 9  74.4% | batch:       574 of       772\t|\tloss: 0.823277\n",
      "Training Epoch 9  74.5% | batch:       575 of       772\t|\tloss: 0.774987\n",
      "Training Epoch 9  74.6% | batch:       576 of       772\t|\tloss: 0.605323\n",
      "Training Epoch 9  74.7% | batch:       577 of       772\t|\tloss: 0.96023\n",
      "Training Epoch 9  74.9% | batch:       578 of       772\t|\tloss: 0.61291\n",
      "Training Epoch 9  75.0% | batch:       579 of       772\t|\tloss: 0.755429\n",
      "Training Epoch 9  75.1% | batch:       580 of       772\t|\tloss: 0.624347\n",
      "Training Epoch 9  75.3% | batch:       581 of       772\t|\tloss: 0.867976\n",
      "Training Epoch 9  75.4% | batch:       582 of       772\t|\tloss: 0.819004\n",
      "Training Epoch 9  75.5% | batch:       583 of       772\t|\tloss: 0.563218\n",
      "Training Epoch 9  75.6% | batch:       584 of       772\t|\tloss: 0.467364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  75.8% | batch:       585 of       772\t|\tloss: 1.12998\n",
      "Training Epoch 9  75.9% | batch:       586 of       772\t|\tloss: 0.936828\n",
      "Training Epoch 9  76.0% | batch:       587 of       772\t|\tloss: 0.666213\n",
      "Training Epoch 9  76.2% | batch:       588 of       772\t|\tloss: 0.963657\n",
      "Training Epoch 9  76.3% | batch:       589 of       772\t|\tloss: 0.70655\n",
      "Training Epoch 9  76.4% | batch:       590 of       772\t|\tloss: 0.803125\n",
      "Training Epoch 9  76.6% | batch:       591 of       772\t|\tloss: 0.720176\n",
      "Training Epoch 9  76.7% | batch:       592 of       772\t|\tloss: 0.39701\n",
      "Training Epoch 9  76.8% | batch:       593 of       772\t|\tloss: 0.769066\n",
      "Training Epoch 9  76.9% | batch:       594 of       772\t|\tloss: 0.638466\n",
      "Training Epoch 9  77.1% | batch:       595 of       772\t|\tloss: 0.410013\n",
      "Training Epoch 9  77.2% | batch:       596 of       772\t|\tloss: 0.80803\n",
      "Training Epoch 9  77.3% | batch:       597 of       772\t|\tloss: 0.771859\n",
      "Training Epoch 9  77.5% | batch:       598 of       772\t|\tloss: 0.498815\n",
      "Training Epoch 9  77.6% | batch:       599 of       772\t|\tloss: 0.63102\n",
      "Training Epoch 9  77.7% | batch:       600 of       772\t|\tloss: 0.599328\n",
      "Training Epoch 9  77.8% | batch:       601 of       772\t|\tloss: 0.333309\n",
      "Training Epoch 9  78.0% | batch:       602 of       772\t|\tloss: 0.543949\n",
      "Training Epoch 9  78.1% | batch:       603 of       772\t|\tloss: 0.733518\n",
      "Training Epoch 9  78.2% | batch:       604 of       772\t|\tloss: 0.574566\n",
      "Training Epoch 9  78.4% | batch:       605 of       772\t|\tloss: 0.36159\n",
      "Training Epoch 9  78.5% | batch:       606 of       772\t|\tloss: 0.611184\n",
      "Training Epoch 9  78.6% | batch:       607 of       772\t|\tloss: 0.530825\n",
      "Training Epoch 9  78.8% | batch:       608 of       772\t|\tloss: 0.572415\n",
      "Training Epoch 9  78.9% | batch:       609 of       772\t|\tloss: 0.770783\n",
      "Training Epoch 9  79.0% | batch:       610 of       772\t|\tloss: 0.619379\n",
      "Training Epoch 9  79.1% | batch:       611 of       772\t|\tloss: 0.798031\n",
      "Training Epoch 9  79.3% | batch:       612 of       772\t|\tloss: 0.469366\n",
      "Training Epoch 9  79.4% | batch:       613 of       772\t|\tloss: 0.657646\n",
      "Training Epoch 9  79.5% | batch:       614 of       772\t|\tloss: 0.711622\n",
      "Training Epoch 9  79.7% | batch:       615 of       772\t|\tloss: 0.900833\n",
      "Training Epoch 9  79.8% | batch:       616 of       772\t|\tloss: 0.960507\n",
      "Training Epoch 9  79.9% | batch:       617 of       772\t|\tloss: 0.485458\n",
      "Training Epoch 9  80.1% | batch:       618 of       772\t|\tloss: 0.453915\n",
      "Training Epoch 9  80.2% | batch:       619 of       772\t|\tloss: 0.720648\n",
      "Training Epoch 9  80.3% | batch:       620 of       772\t|\tloss: 0.899577\n",
      "Training Epoch 9  80.4% | batch:       621 of       772\t|\tloss: 0.674623\n",
      "Training Epoch 9  80.6% | batch:       622 of       772\t|\tloss: 0.777588\n",
      "Training Epoch 9  80.7% | batch:       623 of       772\t|\tloss: 0.863056\n",
      "Training Epoch 9  80.8% | batch:       624 of       772\t|\tloss: 0.74211\n",
      "Training Epoch 9  81.0% | batch:       625 of       772\t|\tloss: 0.461178\n",
      "Training Epoch 9  81.1% | batch:       626 of       772\t|\tloss: 0.714327\n",
      "Training Epoch 9  81.2% | batch:       627 of       772\t|\tloss: 0.945791\n",
      "Training Epoch 9  81.3% | batch:       628 of       772\t|\tloss: 1.08002\n",
      "Training Epoch 9  81.5% | batch:       629 of       772\t|\tloss: 1.27918\n",
      "Training Epoch 9  81.6% | batch:       630 of       772\t|\tloss: 0.592068\n",
      "Training Epoch 9  81.7% | batch:       631 of       772\t|\tloss: 0.673527\n",
      "Training Epoch 9  81.9% | batch:       632 of       772\t|\tloss: 0.901637\n",
      "Training Epoch 9  82.0% | batch:       633 of       772\t|\tloss: 0.474641\n",
      "Training Epoch 9  82.1% | batch:       634 of       772\t|\tloss: 0.608854\n",
      "Training Epoch 9  82.3% | batch:       635 of       772\t|\tloss: 0.692848\n",
      "Training Epoch 9  82.4% | batch:       636 of       772\t|\tloss: 0.706566\n",
      "Training Epoch 9  82.5% | batch:       637 of       772\t|\tloss: 0.51076\n",
      "Training Epoch 9  82.6% | batch:       638 of       772\t|\tloss: 0.689503\n",
      "Training Epoch 9  82.8% | batch:       639 of       772\t|\tloss: 0.779222\n",
      "Training Epoch 9  82.9% | batch:       640 of       772\t|\tloss: 0.804308\n",
      "Training Epoch 9  83.0% | batch:       641 of       772\t|\tloss: 0.719253\n",
      "Training Epoch 9  83.2% | batch:       642 of       772\t|\tloss: 0.530879\n",
      "Training Epoch 9  83.3% | batch:       643 of       772\t|\tloss: 0.755455\n",
      "Training Epoch 9  83.4% | batch:       644 of       772\t|\tloss: 0.496043\n",
      "Training Epoch 9  83.5% | batch:       645 of       772\t|\tloss: 0.528353\n",
      "Training Epoch 9  83.7% | batch:       646 of       772\t|\tloss: 0.805064\n",
      "Training Epoch 9  83.8% | batch:       647 of       772\t|\tloss: 1.13098\n",
      "Training Epoch 9  83.9% | batch:       648 of       772\t|\tloss: 1.14715\n",
      "Training Epoch 9  84.1% | batch:       649 of       772\t|\tloss: 0.670947\n",
      "Training Epoch 9  84.2% | batch:       650 of       772\t|\tloss: 0.43928\n",
      "Training Epoch 9  84.3% | batch:       651 of       772\t|\tloss: 0.559418\n",
      "Training Epoch 9  84.5% | batch:       652 of       772\t|\tloss: 1.3192\n",
      "Training Epoch 9  84.6% | batch:       653 of       772\t|\tloss: 0.864442\n",
      "Training Epoch 9  84.7% | batch:       654 of       772\t|\tloss: 0.64319\n",
      "Training Epoch 9  84.8% | batch:       655 of       772\t|\tloss: 0.729857\n",
      "Training Epoch 9  85.0% | batch:       656 of       772\t|\tloss: 0.89948\n",
      "Training Epoch 9  85.1% | batch:       657 of       772\t|\tloss: 0.685189\n",
      "Training Epoch 9  85.2% | batch:       658 of       772\t|\tloss: 0.818664\n",
      "Training Epoch 9  85.4% | batch:       659 of       772\t|\tloss: 0.657657\n",
      "Training Epoch 9  85.5% | batch:       660 of       772\t|\tloss: 0.616791\n",
      "Training Epoch 9  85.6% | batch:       661 of       772\t|\tloss: 0.481007\n",
      "Training Epoch 9  85.8% | batch:       662 of       772\t|\tloss: 0.564454\n",
      "Training Epoch 9  85.9% | batch:       663 of       772\t|\tloss: 0.439739\n",
      "Training Epoch 9  86.0% | batch:       664 of       772\t|\tloss: 0.4551\n",
      "Training Epoch 9  86.1% | batch:       665 of       772\t|\tloss: 0.571931\n",
      "Training Epoch 9  86.3% | batch:       666 of       772\t|\tloss: 0.62028\n",
      "Training Epoch 9  86.4% | batch:       667 of       772\t|\tloss: 0.626972\n",
      "Training Epoch 9  86.5% | batch:       668 of       772\t|\tloss: 0.71843\n",
      "Training Epoch 9  86.7% | batch:       669 of       772\t|\tloss: 0.843686\n",
      "Training Epoch 9  86.8% | batch:       670 of       772\t|\tloss: 0.632466\n",
      "Training Epoch 9  86.9% | batch:       671 of       772\t|\tloss: 0.962056\n",
      "Training Epoch 9  87.0% | batch:       672 of       772\t|\tloss: 0.796679\n",
      "Training Epoch 9  87.2% | batch:       673 of       772\t|\tloss: 0.849116\n",
      "Training Epoch 9  87.3% | batch:       674 of       772\t|\tloss: 0.506589\n",
      "Training Epoch 9  87.4% | batch:       675 of       772\t|\tloss: 0.390768\n",
      "Training Epoch 9  87.6% | batch:       676 of       772\t|\tloss: 0.471444\n",
      "Training Epoch 9  87.7% | batch:       677 of       772\t|\tloss: 0.4893\n",
      "Training Epoch 9  87.8% | batch:       678 of       772\t|\tloss: 0.826815\n",
      "Training Epoch 9  88.0% | batch:       679 of       772\t|\tloss: 0.616855\n",
      "Training Epoch 9  88.1% | batch:       680 of       772\t|\tloss: 0.438119\n",
      "Training Epoch 9  88.2% | batch:       681 of       772\t|\tloss: 0.668255\n",
      "Training Epoch 9  88.3% | batch:       682 of       772\t|\tloss: 0.40699\n",
      "Training Epoch 9  88.5% | batch:       683 of       772\t|\tloss: 0.440991\n",
      "Training Epoch 9  88.6% | batch:       684 of       772\t|\tloss: 0.392954\n",
      "Training Epoch 9  88.7% | batch:       685 of       772\t|\tloss: 0.736842\n",
      "Training Epoch 9  88.9% | batch:       686 of       772\t|\tloss: 0.424904\n",
      "Training Epoch 9  89.0% | batch:       687 of       772\t|\tloss: 0.374777\n",
      "Training Epoch 9  89.1% | batch:       688 of       772\t|\tloss: 0.486197\n",
      "Training Epoch 9  89.2% | batch:       689 of       772\t|\tloss: 0.806832\n",
      "Training Epoch 9  89.4% | batch:       690 of       772\t|\tloss: 0.686441\n",
      "Training Epoch 9  89.5% | batch:       691 of       772\t|\tloss: 0.596867\n",
      "Training Epoch 9  89.6% | batch:       692 of       772\t|\tloss: 0.547448\n",
      "Training Epoch 9  89.8% | batch:       693 of       772\t|\tloss: 0.663965\n",
      "Training Epoch 9  89.9% | batch:       694 of       772\t|\tloss: 0.504425\n",
      "Training Epoch 9  90.0% | batch:       695 of       772\t|\tloss: 0.484233\n",
      "Training Epoch 9  90.2% | batch:       696 of       772\t|\tloss: 0.541658\n",
      "Training Epoch 9  90.3% | batch:       697 of       772\t|\tloss: 0.53049\n",
      "Training Epoch 9  90.4% | batch:       698 of       772\t|\tloss: 0.583495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  90.5% | batch:       699 of       772\t|\tloss: 0.520477\n",
      "Training Epoch 9  90.7% | batch:       700 of       772\t|\tloss: 0.586276\n",
      "Training Epoch 9  90.8% | batch:       701 of       772\t|\tloss: 0.641903\n",
      "Training Epoch 9  90.9% | batch:       702 of       772\t|\tloss: 0.848976\n",
      "Training Epoch 9  91.1% | batch:       703 of       772\t|\tloss: 0.682534\n",
      "Training Epoch 9  91.2% | batch:       704 of       772\t|\tloss: 0.550033\n",
      "Training Epoch 9  91.3% | batch:       705 of       772\t|\tloss: 0.504267\n",
      "Training Epoch 9  91.5% | batch:       706 of       772\t|\tloss: 0.517542\n",
      "Training Epoch 9  91.6% | batch:       707 of       772\t|\tloss: 0.519535\n",
      "Training Epoch 9  91.7% | batch:       708 of       772\t|\tloss: 0.411699\n",
      "Training Epoch 9  91.8% | batch:       709 of       772\t|\tloss: 0.679584\n",
      "Training Epoch 9  92.0% | batch:       710 of       772\t|\tloss: 0.581249\n",
      "Training Epoch 9  92.1% | batch:       711 of       772\t|\tloss: 0.552096\n",
      "Training Epoch 9  92.2% | batch:       712 of       772\t|\tloss: 0.758784\n",
      "Training Epoch 9  92.4% | batch:       713 of       772\t|\tloss: 0.800759\n",
      "Training Epoch 9  92.5% | batch:       714 of       772\t|\tloss: 0.616999\n",
      "Training Epoch 9  92.6% | batch:       715 of       772\t|\tloss: 0.409728\n",
      "Training Epoch 9  92.7% | batch:       716 of       772\t|\tloss: 0.476358\n",
      "Training Epoch 9  92.9% | batch:       717 of       772\t|\tloss: 0.64657\n",
      "Training Epoch 9  93.0% | batch:       718 of       772\t|\tloss: 0.527257\n",
      "Training Epoch 9  93.1% | batch:       719 of       772\t|\tloss: 0.870576\n",
      "Training Epoch 9  93.3% | batch:       720 of       772\t|\tloss: 1.24279\n",
      "Training Epoch 9  93.4% | batch:       721 of       772\t|\tloss: 0.495263\n",
      "Training Epoch 9  93.5% | batch:       722 of       772\t|\tloss: 0.576032\n",
      "Training Epoch 9  93.7% | batch:       723 of       772\t|\tloss: 0.692917\n",
      "Training Epoch 9  93.8% | batch:       724 of       772\t|\tloss: 0.923261\n",
      "Training Epoch 9  93.9% | batch:       725 of       772\t|\tloss: 0.850113\n",
      "Training Epoch 9  94.0% | batch:       726 of       772\t|\tloss: 0.654711\n",
      "Training Epoch 9  94.2% | batch:       727 of       772\t|\tloss: 0.780762\n",
      "Training Epoch 9  94.3% | batch:       728 of       772\t|\tloss: 0.475797\n",
      "Training Epoch 9  94.4% | batch:       729 of       772\t|\tloss: 0.579612\n",
      "Training Epoch 9  94.6% | batch:       730 of       772\t|\tloss: 0.396116\n",
      "Training Epoch 9  94.7% | batch:       731 of       772\t|\tloss: 0.5914\n",
      "Training Epoch 9  94.8% | batch:       732 of       772\t|\tloss: 0.533967\n",
      "Training Epoch 9  94.9% | batch:       733 of       772\t|\tloss: 0.5707\n",
      "Training Epoch 9  95.1% | batch:       734 of       772\t|\tloss: 0.511105\n",
      "Training Epoch 9  95.2% | batch:       735 of       772\t|\tloss: 0.441885\n",
      "Training Epoch 9  95.3% | batch:       736 of       772\t|\tloss: 0.715273\n",
      "Training Epoch 9  95.5% | batch:       737 of       772\t|\tloss: 0.821076\n",
      "Training Epoch 9  95.6% | batch:       738 of       772\t|\tloss: 0.694199\n",
      "Training Epoch 9  95.7% | batch:       739 of       772\t|\tloss: 0.454709\n",
      "Training Epoch 9  95.9% | batch:       740 of       772\t|\tloss: 0.743309\n",
      "Training Epoch 9  96.0% | batch:       741 of       772\t|\tloss: 0.66831\n",
      "Training Epoch 9  96.1% | batch:       742 of       772\t|\tloss: 1.1556\n",
      "Training Epoch 9  96.2% | batch:       743 of       772\t|\tloss: 0.554134\n",
      "Training Epoch 9  96.4% | batch:       744 of       772\t|\tloss: 0.761258\n",
      "Training Epoch 9  96.5% | batch:       745 of       772\t|\tloss: 0.760807\n",
      "Training Epoch 9  96.6% | batch:       746 of       772\t|\tloss: 0.716674\n",
      "Training Epoch 9  96.8% | batch:       747 of       772\t|\tloss: 0.626966\n",
      "Training Epoch 9  96.9% | batch:       748 of       772\t|\tloss: 0.554196\n",
      "Training Epoch 9  97.0% | batch:       749 of       772\t|\tloss: 0.59893\n",
      "Training Epoch 9  97.2% | batch:       750 of       772\t|\tloss: 1.18579\n",
      "Training Epoch 9  97.3% | batch:       751 of       772\t|\tloss: 0.852777\n",
      "Training Epoch 9  97.4% | batch:       752 of       772\t|\tloss: 0.678481\n",
      "Training Epoch 9  97.5% | batch:       753 of       772\t|\tloss: 0.661656\n",
      "Training Epoch 9  97.7% | batch:       754 of       772\t|\tloss: 0.958399\n",
      "Training Epoch 9  97.8% | batch:       755 of       772\t|\tloss: 0.831127\n",
      "Training Epoch 9  97.9% | batch:       756 of       772\t|\tloss: 0.84809\n",
      "Training Epoch 9  98.1% | batch:       757 of       772\t|\tloss: 0.609552\n",
      "Training Epoch 9  98.2% | batch:       758 of       772\t|\tloss: 0.794841\n",
      "Training Epoch 9  98.3% | batch:       759 of       772\t|\tloss: 1.03148\n",
      "Training Epoch 9  98.4% | batch:       760 of       772\t|\tloss: 0.823167\n",
      "Training Epoch 9  98.6% | batch:       761 of       772\t|\tloss: 0.825765\n",
      "Training Epoch 9  98.7% | batch:       762 of       772\t|\tloss: 0.969502\n",
      "Training Epoch 9  98.8% | batch:       763 of       772\t|\tloss: 0.739278\n",
      "Training Epoch 9  99.0% | batch:       764 of       772\t|\tloss: 0.59734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:22:42,120 | INFO : Epoch 9 Training Summary: epoch: 9.000000 | loss: 0.695798 | \n",
      "2023-05-24 10:22:42,120 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 15.013504266738892 seconds\n",
      "\n",
      "2023-05-24 10:22:42,121 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 16.33476169904073 seconds\n",
      "2023-05-24 10:22:42,121 | INFO : Avg batch train. time: 0.02115901774487141 seconds\n",
      "2023-05-24 10:22:42,121 | INFO : Avg sample train. time: 0.00016530817190925102 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  99.1% | batch:       765 of       772\t|\tloss: 0.467326\n",
      "Training Epoch 9  99.2% | batch:       766 of       772\t|\tloss: 0.69933\n",
      "Training Epoch 9  99.4% | batch:       767 of       772\t|\tloss: 0.606863\n",
      "Training Epoch 9  99.5% | batch:       768 of       772\t|\tloss: 1.11044\n",
      "Training Epoch 9  99.6% | batch:       769 of       772\t|\tloss: 0.840023\n",
      "Training Epoch 9  99.7% | batch:       770 of       772\t|\tloss: 0.879788\n",
      "Training Epoch 9  99.9% | batch:       771 of       772\t|\tloss: 0.784853\n",
      "\n",
      "Training Epoch 10   0.0% | batch:         0 of       772\t|\tloss: 0.708482\n",
      "Training Epoch 10   0.1% | batch:         1 of       772\t|\tloss: 0.459663\n",
      "Training Epoch 10   0.3% | batch:         2 of       772\t|\tloss: 0.798493\n",
      "Training Epoch 10   0.4% | batch:         3 of       772\t|\tloss: 1.34414\n",
      "Training Epoch 10   0.5% | batch:         4 of       772\t|\tloss: 0.862967\n",
      "Training Epoch 10   0.6% | batch:         5 of       772\t|\tloss: 0.735626\n",
      "Training Epoch 10   0.8% | batch:         6 of       772\t|\tloss: 0.778836\n",
      "Training Epoch 10   0.9% | batch:         7 of       772\t|\tloss: 0.775267\n",
      "Training Epoch 10   1.0% | batch:         8 of       772\t|\tloss: 1.06061\n",
      "Training Epoch 10   1.2% | batch:         9 of       772\t|\tloss: 0.889048\n",
      "Training Epoch 10   1.3% | batch:        10 of       772\t|\tloss: 0.791756\n",
      "Training Epoch 10   1.4% | batch:        11 of       772\t|\tloss: 0.703137\n",
      "Training Epoch 10   1.6% | batch:        12 of       772\t|\tloss: 0.672984\n",
      "Training Epoch 10   1.7% | batch:        13 of       772\t|\tloss: 0.920515\n",
      "Training Epoch 10   1.8% | batch:        14 of       772\t|\tloss: 0.46852\n",
      "Training Epoch 10   1.9% | batch:        15 of       772\t|\tloss: 0.604866\n",
      "Training Epoch 10   2.1% | batch:        16 of       772\t|\tloss: 0.922837\n",
      "Training Epoch 10   2.2% | batch:        17 of       772\t|\tloss: 1.16538\n",
      "Training Epoch 10   2.3% | batch:        18 of       772\t|\tloss: 0.963397\n",
      "Training Epoch 10   2.5% | batch:        19 of       772\t|\tloss: 0.48662\n",
      "Training Epoch 10   2.6% | batch:        20 of       772\t|\tloss: 0.436582\n",
      "Training Epoch 10   2.7% | batch:        21 of       772\t|\tloss: 0.415979\n",
      "Training Epoch 10   2.8% | batch:        22 of       772\t|\tloss: 0.476814\n",
      "Training Epoch 10   3.0% | batch:        23 of       772\t|\tloss: 0.52485\n",
      "Training Epoch 10   3.1% | batch:        24 of       772\t|\tloss: 0.492356\n",
      "Training Epoch 10   3.2% | batch:        25 of       772\t|\tloss: 0.572079\n",
      "Training Epoch 10   3.4% | batch:        26 of       772\t|\tloss: 0.627478\n",
      "Training Epoch 10   3.5% | batch:        27 of       772\t|\tloss: 0.433142\n",
      "Training Epoch 10   3.6% | batch:        28 of       772\t|\tloss: 0.631358\n",
      "Training Epoch 10   3.8% | batch:        29 of       772\t|\tloss: 0.628592\n",
      "Training Epoch 10   3.9% | batch:        30 of       772\t|\tloss: 0.560776\n",
      "Training Epoch 10   4.0% | batch:        31 of       772\t|\tloss: 0.536526\n",
      "Training Epoch 10   4.1% | batch:        32 of       772\t|\tloss: 0.429747\n",
      "Training Epoch 10   4.3% | batch:        33 of       772\t|\tloss: 0.743996\n",
      "Training Epoch 10   4.4% | batch:        34 of       772\t|\tloss: 0.645778\n",
      "Training Epoch 10   4.5% | batch:        35 of       772\t|\tloss: 0.570347\n",
      "Training Epoch 10   4.7% | batch:        36 of       772\t|\tloss: 0.7323\n",
      "Training Epoch 10   4.8% | batch:        37 of       772\t|\tloss: 0.53369\n",
      "Training Epoch 10   4.9% | batch:        38 of       772\t|\tloss: 0.636138\n",
      "Training Epoch 10   5.1% | batch:        39 of       772\t|\tloss: 0.640627\n",
      "Training Epoch 10   5.2% | batch:        40 of       772\t|\tloss: 0.554717\n",
      "Training Epoch 10   5.3% | batch:        41 of       772\t|\tloss: 0.711522\n",
      "Training Epoch 10   5.4% | batch:        42 of       772\t|\tloss: 0.510701\n",
      "Training Epoch 10   5.6% | batch:        43 of       772\t|\tloss: 0.861476\n",
      "Training Epoch 10   5.7% | batch:        44 of       772\t|\tloss: 0.547428\n",
      "Training Epoch 10   5.8% | batch:        45 of       772\t|\tloss: 0.494198\n",
      "Training Epoch 10   6.0% | batch:        46 of       772\t|\tloss: 0.500783\n",
      "Training Epoch 10   6.1% | batch:        47 of       772\t|\tloss: 0.994303\n",
      "Training Epoch 10   6.2% | batch:        48 of       772\t|\tloss: 0.702038\n",
      "Training Epoch 10   6.3% | batch:        49 of       772\t|\tloss: 0.535155\n",
      "Training Epoch 10   6.5% | batch:        50 of       772\t|\tloss: 0.769245\n",
      "Training Epoch 10   6.6% | batch:        51 of       772\t|\tloss: 0.624707\n",
      "Training Epoch 10   6.7% | batch:        52 of       772\t|\tloss: 0.584908\n",
      "Training Epoch 10   6.9% | batch:        53 of       772\t|\tloss: 0.738177\n",
      "Training Epoch 10   7.0% | batch:        54 of       772\t|\tloss: 0.625542\n",
      "Training Epoch 10   7.1% | batch:        55 of       772\t|\tloss: 0.55589\n",
      "Training Epoch 10   7.3% | batch:        56 of       772\t|\tloss: 0.799025\n",
      "Training Epoch 10   7.4% | batch:        57 of       772\t|\tloss: 0.647682\n",
      "Training Epoch 10   7.5% | batch:        58 of       772\t|\tloss: 0.846026\n",
      "Training Epoch 10   7.6% | batch:        59 of       772\t|\tloss: 0.495215\n",
      "Training Epoch 10   7.8% | batch:        60 of       772\t|\tloss: 0.557346\n",
      "Training Epoch 10   7.9% | batch:        61 of       772\t|\tloss: 0.518012\n",
      "Training Epoch 10   8.0% | batch:        62 of       772\t|\tloss: 0.591102\n",
      "Training Epoch 10   8.2% | batch:        63 of       772\t|\tloss: 0.780257\n",
      "Training Epoch 10   8.3% | batch:        64 of       772\t|\tloss: 0.601\n",
      "Training Epoch 10   8.4% | batch:        65 of       772\t|\tloss: 0.424659\n",
      "Training Epoch 10   8.5% | batch:        66 of       772\t|\tloss: 0.656302\n",
      "Training Epoch 10   8.7% | batch:        67 of       772\t|\tloss: 0.478998\n",
      "Training Epoch 10   8.8% | batch:        68 of       772\t|\tloss: 0.655675\n",
      "Training Epoch 10   8.9% | batch:        69 of       772\t|\tloss: 0.608544\n",
      "Training Epoch 10   9.1% | batch:        70 of       772\t|\tloss: 0.707376\n",
      "Training Epoch 10   9.2% | batch:        71 of       772\t|\tloss: 0.543345\n",
      "Training Epoch 10   9.3% | batch:        72 of       772\t|\tloss: 0.401699\n",
      "Training Epoch 10   9.5% | batch:        73 of       772\t|\tloss: 0.654496\n",
      "Training Epoch 10   9.6% | batch:        74 of       772\t|\tloss: 0.490529\n",
      "Training Epoch 10   9.7% | batch:        75 of       772\t|\tloss: 0.723189\n",
      "Training Epoch 10   9.8% | batch:        76 of       772\t|\tloss: 0.612719\n",
      "Training Epoch 10  10.0% | batch:        77 of       772\t|\tloss: 0.636823\n",
      "Training Epoch 10  10.1% | batch:        78 of       772\t|\tloss: 0.569978\n",
      "Training Epoch 10  10.2% | batch:        79 of       772\t|\tloss: 0.555509\n",
      "Training Epoch 10  10.4% | batch:        80 of       772\t|\tloss: 0.495615\n",
      "Training Epoch 10  10.5% | batch:        81 of       772\t|\tloss: 0.532949\n",
      "Training Epoch 10  10.6% | batch:        82 of       772\t|\tloss: 0.399454\n",
      "Training Epoch 10  10.8% | batch:        83 of       772\t|\tloss: 0.717252\n",
      "Training Epoch 10  10.9% | batch:        84 of       772\t|\tloss: 1.22749\n",
      "Training Epoch 10  11.0% | batch:        85 of       772\t|\tloss: 0.664212\n",
      "Training Epoch 10  11.1% | batch:        86 of       772\t|\tloss: 0.546551\n",
      "Training Epoch 10  11.3% | batch:        87 of       772\t|\tloss: 0.605798\n",
      "Training Epoch 10  11.4% | batch:        88 of       772\t|\tloss: 0.735413\n",
      "Training Epoch 10  11.5% | batch:        89 of       772\t|\tloss: 0.813749\n",
      "Training Epoch 10  11.7% | batch:        90 of       772\t|\tloss: 0.72819\n",
      "Training Epoch 10  11.8% | batch:        91 of       772\t|\tloss: 0.628332\n",
      "Training Epoch 10  11.9% | batch:        92 of       772\t|\tloss: 0.553751\n",
      "Training Epoch 10  12.0% | batch:        93 of       772\t|\tloss: 0.850034\n",
      "Training Epoch 10  12.2% | batch:        94 of       772\t|\tloss: 0.51704\n",
      "Training Epoch 10  12.3% | batch:        95 of       772\t|\tloss: 0.809659\n",
      "Training Epoch 10  12.4% | batch:        96 of       772\t|\tloss: 1.0912\n",
      "Training Epoch 10  12.6% | batch:        97 of       772\t|\tloss: 1.34843\n",
      "Training Epoch 10  12.7% | batch:        98 of       772\t|\tloss: 1.25312\n",
      "Training Epoch 10  12.8% | batch:        99 of       772\t|\tloss: 0.914328\n",
      "Training Epoch 10  13.0% | batch:       100 of       772\t|\tloss: 0.652365\n",
      "Training Epoch 10  13.1% | batch:       101 of       772\t|\tloss: 0.935591\n",
      "Training Epoch 10  13.2% | batch:       102 of       772\t|\tloss: 1.18695\n",
      "Training Epoch 10  13.3% | batch:       103 of       772\t|\tloss: 0.640143\n",
      "Training Epoch 10  13.5% | batch:       104 of       772\t|\tloss: 0.538181\n",
      "Training Epoch 10  13.6% | batch:       105 of       772\t|\tloss: 0.5118\n",
      "Training Epoch 10  13.7% | batch:       106 of       772\t|\tloss: 0.787436\n",
      "Training Epoch 10  13.9% | batch:       107 of       772\t|\tloss: 1.20192\n",
      "Training Epoch 10  14.0% | batch:       108 of       772\t|\tloss: 0.763269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  14.1% | batch:       109 of       772\t|\tloss: 0.636238\n",
      "Training Epoch 10  14.2% | batch:       110 of       772\t|\tloss: 0.513469\n",
      "Training Epoch 10  14.4% | batch:       111 of       772\t|\tloss: 0.412664\n",
      "Training Epoch 10  14.5% | batch:       112 of       772\t|\tloss: 0.40822\n",
      "Training Epoch 10  14.6% | batch:       113 of       772\t|\tloss: 0.717282\n",
      "Training Epoch 10  14.8% | batch:       114 of       772\t|\tloss: 0.884762\n",
      "Training Epoch 10  14.9% | batch:       115 of       772\t|\tloss: 0.968234\n",
      "Training Epoch 10  15.0% | batch:       116 of       772\t|\tloss: 0.99334\n",
      "Training Epoch 10  15.2% | batch:       117 of       772\t|\tloss: 0.736787\n",
      "Training Epoch 10  15.3% | batch:       118 of       772\t|\tloss: 0.581059\n",
      "Training Epoch 10  15.4% | batch:       119 of       772\t|\tloss: 0.739961\n",
      "Training Epoch 10  15.5% | batch:       120 of       772\t|\tloss: 0.848545\n",
      "Training Epoch 10  15.7% | batch:       121 of       772\t|\tloss: 1.12358\n",
      "Training Epoch 10  15.8% | batch:       122 of       772\t|\tloss: 0.703649\n",
      "Training Epoch 10  15.9% | batch:       123 of       772\t|\tloss: 0.631136\n",
      "Training Epoch 10  16.1% | batch:       124 of       772\t|\tloss: 0.529845\n",
      "Training Epoch 10  16.2% | batch:       125 of       772\t|\tloss: 0.595087\n",
      "Training Epoch 10  16.3% | batch:       126 of       772\t|\tloss: 0.468774\n",
      "Training Epoch 10  16.5% | batch:       127 of       772\t|\tloss: 0.771876\n",
      "Training Epoch 10  16.6% | batch:       128 of       772\t|\tloss: 0.797763\n",
      "Training Epoch 10  16.7% | batch:       129 of       772\t|\tloss: 0.820332\n",
      "Training Epoch 10  16.8% | batch:       130 of       772\t|\tloss: 0.596608\n",
      "Training Epoch 10  17.0% | batch:       131 of       772\t|\tloss: 0.722218\n",
      "Training Epoch 10  17.1% | batch:       132 of       772\t|\tloss: 0.513094\n",
      "Training Epoch 10  17.2% | batch:       133 of       772\t|\tloss: 0.667409\n",
      "Training Epoch 10  17.4% | batch:       134 of       772\t|\tloss: 0.473278\n",
      "Training Epoch 10  17.5% | batch:       135 of       772\t|\tloss: 0.733148\n",
      "Training Epoch 10  17.6% | batch:       136 of       772\t|\tloss: 0.451773\n",
      "Training Epoch 10  17.7% | batch:       137 of       772\t|\tloss: 0.663352\n",
      "Training Epoch 10  17.9% | batch:       138 of       772\t|\tloss: 0.787213\n",
      "Training Epoch 10  18.0% | batch:       139 of       772\t|\tloss: 0.603475\n",
      "Training Epoch 10  18.1% | batch:       140 of       772\t|\tloss: 0.57314\n",
      "Training Epoch 10  18.3% | batch:       141 of       772\t|\tloss: 0.654245\n",
      "Training Epoch 10  18.4% | batch:       142 of       772\t|\tloss: 0.711686\n",
      "Training Epoch 10  18.5% | batch:       143 of       772\t|\tloss: 0.497205\n",
      "Training Epoch 10  18.7% | batch:       144 of       772\t|\tloss: 0.447092\n",
      "Training Epoch 10  18.8% | batch:       145 of       772\t|\tloss: 0.550517\n",
      "Training Epoch 10  18.9% | batch:       146 of       772\t|\tloss: 0.679989\n",
      "Training Epoch 10  19.0% | batch:       147 of       772\t|\tloss: 0.55938\n",
      "Training Epoch 10  19.2% | batch:       148 of       772\t|\tloss: 0.70789\n",
      "Training Epoch 10  19.3% | batch:       149 of       772\t|\tloss: 0.575269\n",
      "Training Epoch 10  19.4% | batch:       150 of       772\t|\tloss: 0.976737\n",
      "Training Epoch 10  19.6% | batch:       151 of       772\t|\tloss: 0.484515\n",
      "Training Epoch 10  19.7% | batch:       152 of       772\t|\tloss: 0.576459\n",
      "Training Epoch 10  19.8% | batch:       153 of       772\t|\tloss: 0.691718\n",
      "Training Epoch 10  19.9% | batch:       154 of       772\t|\tloss: 0.454338\n",
      "Training Epoch 10  20.1% | batch:       155 of       772\t|\tloss: 0.750994\n",
      "Training Epoch 10  20.2% | batch:       156 of       772\t|\tloss: 0.634092\n",
      "Training Epoch 10  20.3% | batch:       157 of       772\t|\tloss: 0.672292\n",
      "Training Epoch 10  20.5% | batch:       158 of       772\t|\tloss: 0.625846\n",
      "Training Epoch 10  20.6% | batch:       159 of       772\t|\tloss: 0.609202\n",
      "Training Epoch 10  20.7% | batch:       160 of       772\t|\tloss: 0.504285\n",
      "Training Epoch 10  20.9% | batch:       161 of       772\t|\tloss: 1.01724\n",
      "Training Epoch 10  21.0% | batch:       162 of       772\t|\tloss: 0.675936\n",
      "Training Epoch 10  21.1% | batch:       163 of       772\t|\tloss: 0.819776\n",
      "Training Epoch 10  21.2% | batch:       164 of       772\t|\tloss: 0.650205\n",
      "Training Epoch 10  21.4% | batch:       165 of       772\t|\tloss: 0.894942\n",
      "Training Epoch 10  21.5% | batch:       166 of       772\t|\tloss: 0.619609\n",
      "Training Epoch 10  21.6% | batch:       167 of       772\t|\tloss: 0.582911\n",
      "Training Epoch 10  21.8% | batch:       168 of       772\t|\tloss: 0.918309\n",
      "Training Epoch 10  21.9% | batch:       169 of       772\t|\tloss: 0.595146\n",
      "Training Epoch 10  22.0% | batch:       170 of       772\t|\tloss: 0.635683\n",
      "Training Epoch 10  22.2% | batch:       171 of       772\t|\tloss: 0.530012\n",
      "Training Epoch 10  22.3% | batch:       172 of       772\t|\tloss: 0.583498\n",
      "Training Epoch 10  22.4% | batch:       173 of       772\t|\tloss: 0.713458\n",
      "Training Epoch 10  22.5% | batch:       174 of       772\t|\tloss: 0.646645\n",
      "Training Epoch 10  22.7% | batch:       175 of       772\t|\tloss: 0.612423\n",
      "Training Epoch 10  22.8% | batch:       176 of       772\t|\tloss: 0.729429\n",
      "Training Epoch 10  22.9% | batch:       177 of       772\t|\tloss: 0.638779\n",
      "Training Epoch 10  23.1% | batch:       178 of       772\t|\tloss: 0.62891\n",
      "Training Epoch 10  23.2% | batch:       179 of       772\t|\tloss: 0.431543\n",
      "Training Epoch 10  23.3% | batch:       180 of       772\t|\tloss: 0.750874\n",
      "Training Epoch 10  23.4% | batch:       181 of       772\t|\tloss: 0.474753\n",
      "Training Epoch 10  23.6% | batch:       182 of       772\t|\tloss: 1.49222\n",
      "Training Epoch 10  23.7% | batch:       183 of       772\t|\tloss: 1.14171\n",
      "Training Epoch 10  23.8% | batch:       184 of       772\t|\tloss: 0.45615\n",
      "Training Epoch 10  24.0% | batch:       185 of       772\t|\tloss: 0.554492\n",
      "Training Epoch 10  24.1% | batch:       186 of       772\t|\tloss: 0.641831\n",
      "Training Epoch 10  24.2% | batch:       187 of       772\t|\tloss: 0.955333\n",
      "Training Epoch 10  24.4% | batch:       188 of       772\t|\tloss: 0.962069\n",
      "Training Epoch 10  24.5% | batch:       189 of       772\t|\tloss: 0.566586\n",
      "Training Epoch 10  24.6% | batch:       190 of       772\t|\tloss: 0.646807\n",
      "Training Epoch 10  24.7% | batch:       191 of       772\t|\tloss: 0.666338\n",
      "Training Epoch 10  24.9% | batch:       192 of       772\t|\tloss: 0.459909\n",
      "Training Epoch 10  25.0% | batch:       193 of       772\t|\tloss: 0.496726\n",
      "Training Epoch 10  25.1% | batch:       194 of       772\t|\tloss: 0.745184\n",
      "Training Epoch 10  25.3% | batch:       195 of       772\t|\tloss: 1.00524\n",
      "Training Epoch 10  25.4% | batch:       196 of       772\t|\tloss: 0.733616\n",
      "Training Epoch 10  25.5% | batch:       197 of       772\t|\tloss: 0.762409\n",
      "Training Epoch 10  25.6% | batch:       198 of       772\t|\tloss: 0.481191\n",
      "Training Epoch 10  25.8% | batch:       199 of       772\t|\tloss: 0.624282\n",
      "Training Epoch 10  25.9% | batch:       200 of       772\t|\tloss: 0.922401\n",
      "Training Epoch 10  26.0% | batch:       201 of       772\t|\tloss: 0.5258\n",
      "Training Epoch 10  26.2% | batch:       202 of       772\t|\tloss: 0.751208\n",
      "Training Epoch 10  26.3% | batch:       203 of       772\t|\tloss: 0.62484\n",
      "Training Epoch 10  26.4% | batch:       204 of       772\t|\tloss: 1.11034\n",
      "Training Epoch 10  26.6% | batch:       205 of       772\t|\tloss: 0.72899\n",
      "Training Epoch 10  26.7% | batch:       206 of       772\t|\tloss: 0.692128\n",
      "Training Epoch 10  26.8% | batch:       207 of       772\t|\tloss: 0.556915\n",
      "Training Epoch 10  26.9% | batch:       208 of       772\t|\tloss: 0.890236\n",
      "Training Epoch 10  27.1% | batch:       209 of       772\t|\tloss: 0.583769\n",
      "Training Epoch 10  27.2% | batch:       210 of       772\t|\tloss: 0.48357\n",
      "Training Epoch 10  27.3% | batch:       211 of       772\t|\tloss: 0.501456\n",
      "Training Epoch 10  27.5% | batch:       212 of       772\t|\tloss: 0.643806\n",
      "Training Epoch 10  27.6% | batch:       213 of       772\t|\tloss: 0.576683\n",
      "Training Epoch 10  27.7% | batch:       214 of       772\t|\tloss: 0.60772\n",
      "Training Epoch 10  27.8% | batch:       215 of       772\t|\tloss: 0.474112\n",
      "Training Epoch 10  28.0% | batch:       216 of       772\t|\tloss: 0.584759\n",
      "Training Epoch 10  28.1% | batch:       217 of       772\t|\tloss: 0.383965\n",
      "Training Epoch 10  28.2% | batch:       218 of       772\t|\tloss: 0.5424\n",
      "Training Epoch 10  28.4% | batch:       219 of       772\t|\tloss: 0.80442\n",
      "Training Epoch 10  28.5% | batch:       220 of       772\t|\tloss: 1.01723\n",
      "Training Epoch 10  28.6% | batch:       221 of       772\t|\tloss: 0.706988\n",
      "Training Epoch 10  28.8% | batch:       222 of       772\t|\tloss: 0.516815\n",
      "Training Epoch 10  28.9% | batch:       223 of       772\t|\tloss: 0.700365\n",
      "Training Epoch 10  29.0% | batch:       224 of       772\t|\tloss: 1.11906\n",
      "Training Epoch 10  29.1% | batch:       225 of       772\t|\tloss: 0.879937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  29.3% | batch:       226 of       772\t|\tloss: 0.714452\n",
      "Training Epoch 10  29.4% | batch:       227 of       772\t|\tloss: 0.692352\n",
      "Training Epoch 10  29.5% | batch:       228 of       772\t|\tloss: 1.00945\n",
      "Training Epoch 10  29.7% | batch:       229 of       772\t|\tloss: 1.2967\n",
      "Training Epoch 10  29.8% | batch:       230 of       772\t|\tloss: 0.895952\n",
      "Training Epoch 10  29.9% | batch:       231 of       772\t|\tloss: 0.665796\n",
      "Training Epoch 10  30.1% | batch:       232 of       772\t|\tloss: 0.581882\n",
      "Training Epoch 10  30.2% | batch:       233 of       772\t|\tloss: 0.763718\n",
      "Training Epoch 10  30.3% | batch:       234 of       772\t|\tloss: 1.00228\n",
      "Training Epoch 10  30.4% | batch:       235 of       772\t|\tloss: 1.12053\n",
      "Training Epoch 10  30.6% | batch:       236 of       772\t|\tloss: 0.667829\n",
      "Training Epoch 10  30.7% | batch:       237 of       772\t|\tloss: 0.609942\n",
      "Training Epoch 10  30.8% | batch:       238 of       772\t|\tloss: 1.08008\n",
      "Training Epoch 10  31.0% | batch:       239 of       772\t|\tloss: 1.38696\n",
      "Training Epoch 10  31.1% | batch:       240 of       772\t|\tloss: 0.986289\n",
      "Training Epoch 10  31.2% | batch:       241 of       772\t|\tloss: 0.968964\n",
      "Training Epoch 10  31.3% | batch:       242 of       772\t|\tloss: 0.717207\n",
      "Training Epoch 10  31.5% | batch:       243 of       772\t|\tloss: 0.816361\n",
      "Training Epoch 10  31.6% | batch:       244 of       772\t|\tloss: 0.683897\n",
      "Training Epoch 10  31.7% | batch:       245 of       772\t|\tloss: 1.21179\n",
      "Training Epoch 10  31.9% | batch:       246 of       772\t|\tloss: 0.911315\n",
      "Training Epoch 10  32.0% | batch:       247 of       772\t|\tloss: 0.578458\n",
      "Training Epoch 10  32.1% | batch:       248 of       772\t|\tloss: 0.643022\n",
      "Training Epoch 10  32.3% | batch:       249 of       772\t|\tloss: 0.987843\n",
      "Training Epoch 10  32.4% | batch:       250 of       772\t|\tloss: 0.694564\n",
      "Training Epoch 10  32.5% | batch:       251 of       772\t|\tloss: 0.809297\n",
      "Training Epoch 10  32.6% | batch:       252 of       772\t|\tloss: 0.68919\n",
      "Training Epoch 10  32.8% | batch:       253 of       772\t|\tloss: 0.74327\n",
      "Training Epoch 10  32.9% | batch:       254 of       772\t|\tloss: 0.818389\n",
      "Training Epoch 10  33.0% | batch:       255 of       772\t|\tloss: 0.55358\n",
      "Training Epoch 10  33.2% | batch:       256 of       772\t|\tloss: 0.455591\n",
      "Training Epoch 10  33.3% | batch:       257 of       772\t|\tloss: 0.699904\n",
      "Training Epoch 10  33.4% | batch:       258 of       772\t|\tloss: 0.490414\n",
      "Training Epoch 10  33.5% | batch:       259 of       772\t|\tloss: 0.448013\n",
      "Training Epoch 10  33.7% | batch:       260 of       772\t|\tloss: 1.25663\n",
      "Training Epoch 10  33.8% | batch:       261 of       772\t|\tloss: 0.540784\n",
      "Training Epoch 10  33.9% | batch:       262 of       772\t|\tloss: 0.631738\n",
      "Training Epoch 10  34.1% | batch:       263 of       772\t|\tloss: 0.537744\n",
      "Training Epoch 10  34.2% | batch:       264 of       772\t|\tloss: 0.700962\n",
      "Training Epoch 10  34.3% | batch:       265 of       772\t|\tloss: 0.679933\n",
      "Training Epoch 10  34.5% | batch:       266 of       772\t|\tloss: 0.508635\n",
      "Training Epoch 10  34.6% | batch:       267 of       772\t|\tloss: 0.588232\n",
      "Training Epoch 10  34.7% | batch:       268 of       772\t|\tloss: 0.539674\n",
      "Training Epoch 10  34.8% | batch:       269 of       772\t|\tloss: 0.639358\n",
      "Training Epoch 10  35.0% | batch:       270 of       772\t|\tloss: 0.507545\n",
      "Training Epoch 10  35.1% | batch:       271 of       772\t|\tloss: 0.762925\n",
      "Training Epoch 10  35.2% | batch:       272 of       772\t|\tloss: 0.498301\n",
      "Training Epoch 10  35.4% | batch:       273 of       772\t|\tloss: 0.493661\n",
      "Training Epoch 10  35.5% | batch:       274 of       772\t|\tloss: 0.557572\n",
      "Training Epoch 10  35.6% | batch:       275 of       772\t|\tloss: 0.534017\n",
      "Training Epoch 10  35.8% | batch:       276 of       772\t|\tloss: 0.525753\n",
      "Training Epoch 10  35.9% | batch:       277 of       772\t|\tloss: 0.504291\n",
      "Training Epoch 10  36.0% | batch:       278 of       772\t|\tloss: 0.435103\n",
      "Training Epoch 10  36.1% | batch:       279 of       772\t|\tloss: 0.601218\n",
      "Training Epoch 10  36.3% | batch:       280 of       772\t|\tloss: 0.544022\n",
      "Training Epoch 10  36.4% | batch:       281 of       772\t|\tloss: 0.480483\n",
      "Training Epoch 10  36.5% | batch:       282 of       772\t|\tloss: 0.653464\n",
      "Training Epoch 10  36.7% | batch:       283 of       772\t|\tloss: 0.461466\n",
      "Training Epoch 10  36.8% | batch:       284 of       772\t|\tloss: 0.586236\n",
      "Training Epoch 10  36.9% | batch:       285 of       772\t|\tloss: 0.661068\n",
      "Training Epoch 10  37.0% | batch:       286 of       772\t|\tloss: 0.808015\n",
      "Training Epoch 10  37.2% | batch:       287 of       772\t|\tloss: 0.606099\n",
      "Training Epoch 10  37.3% | batch:       288 of       772\t|\tloss: 0.545665\n",
      "Training Epoch 10  37.4% | batch:       289 of       772\t|\tloss: 0.534593\n",
      "Training Epoch 10  37.6% | batch:       290 of       772\t|\tloss: 0.854509\n",
      "Training Epoch 10  37.7% | batch:       291 of       772\t|\tloss: 0.769894\n",
      "Training Epoch 10  37.8% | batch:       292 of       772\t|\tloss: 0.605473\n",
      "Training Epoch 10  38.0% | batch:       293 of       772\t|\tloss: 1.14295\n",
      "Training Epoch 10  38.1% | batch:       294 of       772\t|\tloss: 0.891008\n",
      "Training Epoch 10  38.2% | batch:       295 of       772\t|\tloss: 1.00252\n",
      "Training Epoch 10  38.3% | batch:       296 of       772\t|\tloss: 0.804294\n",
      "Training Epoch 10  38.5% | batch:       297 of       772\t|\tloss: 0.51047\n",
      "Training Epoch 10  38.6% | batch:       298 of       772\t|\tloss: 1.00668\n",
      "Training Epoch 10  38.7% | batch:       299 of       772\t|\tloss: 0.984901\n",
      "Training Epoch 10  38.9% | batch:       300 of       772\t|\tloss: 0.928219\n",
      "Training Epoch 10  39.0% | batch:       301 of       772\t|\tloss: 0.978404\n",
      "Training Epoch 10  39.1% | batch:       302 of       772\t|\tloss: 0.894401\n",
      "Training Epoch 10  39.2% | batch:       303 of       772\t|\tloss: 0.479001\n",
      "Training Epoch 10  39.4% | batch:       304 of       772\t|\tloss: 0.808095\n",
      "Training Epoch 10  39.5% | batch:       305 of       772\t|\tloss: 0.778259\n",
      "Training Epoch 10  39.6% | batch:       306 of       772\t|\tloss: 0.648442\n",
      "Training Epoch 10  39.8% | batch:       307 of       772\t|\tloss: 0.436874\n",
      "Training Epoch 10  39.9% | batch:       308 of       772\t|\tloss: 0.553607\n",
      "Training Epoch 10  40.0% | batch:       309 of       772\t|\tloss: 0.549622\n",
      "Training Epoch 10  40.2% | batch:       310 of       772\t|\tloss: 0.567648\n",
      "Training Epoch 10  40.3% | batch:       311 of       772\t|\tloss: 0.466696\n",
      "Training Epoch 10  40.4% | batch:       312 of       772\t|\tloss: 0.594588\n",
      "Training Epoch 10  40.5% | batch:       313 of       772\t|\tloss: 0.398184\n",
      "Training Epoch 10  40.7% | batch:       314 of       772\t|\tloss: 0.467619\n",
      "Training Epoch 10  40.8% | batch:       315 of       772\t|\tloss: 0.584405\n",
      "Training Epoch 10  40.9% | batch:       316 of       772\t|\tloss: 0.669656\n",
      "Training Epoch 10  41.1% | batch:       317 of       772\t|\tloss: 0.66358\n",
      "Training Epoch 10  41.2% | batch:       318 of       772\t|\tloss: 0.603751\n",
      "Training Epoch 10  41.3% | batch:       319 of       772\t|\tloss: 0.731389\n",
      "Training Epoch 10  41.5% | batch:       320 of       772\t|\tloss: 0.505867\n",
      "Training Epoch 10  41.6% | batch:       321 of       772\t|\tloss: 0.615983\n",
      "Training Epoch 10  41.7% | batch:       322 of       772\t|\tloss: 0.480111\n",
      "Training Epoch 10  41.8% | batch:       323 of       772\t|\tloss: 0.670216\n",
      "Training Epoch 10  42.0% | batch:       324 of       772\t|\tloss: 0.59045\n",
      "Training Epoch 10  42.1% | batch:       325 of       772\t|\tloss: 0.537403\n",
      "Training Epoch 10  42.2% | batch:       326 of       772\t|\tloss: 0.687191\n",
      "Training Epoch 10  42.4% | batch:       327 of       772\t|\tloss: 0.617051\n",
      "Training Epoch 10  42.5% | batch:       328 of       772\t|\tloss: 0.853273\n",
      "Training Epoch 10  42.6% | batch:       329 of       772\t|\tloss: 0.602738\n",
      "Training Epoch 10  42.7% | batch:       330 of       772\t|\tloss: 0.452345\n",
      "Training Epoch 10  42.9% | batch:       331 of       772\t|\tloss: 0.963234\n",
      "Training Epoch 10  43.0% | batch:       332 of       772\t|\tloss: 0.873497\n",
      "Training Epoch 10  43.1% | batch:       333 of       772\t|\tloss: 0.863267\n",
      "Training Epoch 10  43.3% | batch:       334 of       772\t|\tloss: 0.749133\n",
      "Training Epoch 10  43.4% | batch:       335 of       772\t|\tloss: 0.599558\n",
      "Training Epoch 10  43.5% | batch:       336 of       772\t|\tloss: 0.739361\n",
      "Training Epoch 10  43.7% | batch:       337 of       772\t|\tloss: 1.19139\n",
      "Training Epoch 10  43.8% | batch:       338 of       772\t|\tloss: 1.14537\n",
      "Training Epoch 10  43.9% | batch:       339 of       772\t|\tloss: 0.906375\n",
      "Training Epoch 10  44.0% | batch:       340 of       772\t|\tloss: 0.86436\n",
      "Training Epoch 10  44.2% | batch:       341 of       772\t|\tloss: 0.686562\n",
      "Training Epoch 10  44.3% | batch:       342 of       772\t|\tloss: 0.838608\n",
      "Training Epoch 10  44.4% | batch:       343 of       772\t|\tloss: 0.769151\n",
      "Training Epoch 10  44.6% | batch:       344 of       772\t|\tloss: 0.587498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  44.7% | batch:       345 of       772\t|\tloss: 0.485045\n",
      "Training Epoch 10  44.8% | batch:       346 of       772\t|\tloss: 0.537403\n",
      "Training Epoch 10  44.9% | batch:       347 of       772\t|\tloss: 0.629772\n",
      "Training Epoch 10  45.1% | batch:       348 of       772\t|\tloss: 0.538317\n",
      "Training Epoch 10  45.2% | batch:       349 of       772\t|\tloss: 0.564901\n",
      "Training Epoch 10  45.3% | batch:       350 of       772\t|\tloss: 0.550757\n",
      "Training Epoch 10  45.5% | batch:       351 of       772\t|\tloss: 0.527367\n",
      "Training Epoch 10  45.6% | batch:       352 of       772\t|\tloss: 0.590215\n",
      "Training Epoch 10  45.7% | batch:       353 of       772\t|\tloss: 0.628193\n",
      "Training Epoch 10  45.9% | batch:       354 of       772\t|\tloss: 0.558752\n",
      "Training Epoch 10  46.0% | batch:       355 of       772\t|\tloss: 0.564485\n",
      "Training Epoch 10  46.1% | batch:       356 of       772\t|\tloss: 0.443486\n",
      "Training Epoch 10  46.2% | batch:       357 of       772\t|\tloss: 0.46602\n",
      "Training Epoch 10  46.4% | batch:       358 of       772\t|\tloss: 0.466266\n",
      "Training Epoch 10  46.5% | batch:       359 of       772\t|\tloss: 0.357199\n",
      "Training Epoch 10  46.6% | batch:       360 of       772\t|\tloss: 0.581317\n",
      "Training Epoch 10  46.8% | batch:       361 of       772\t|\tloss: 0.534543\n",
      "Training Epoch 10  46.9% | batch:       362 of       772\t|\tloss: 0.71049\n",
      "Training Epoch 10  47.0% | batch:       363 of       772\t|\tloss: 0.525571\n",
      "Training Epoch 10  47.2% | batch:       364 of       772\t|\tloss: 0.427852\n",
      "Training Epoch 10  47.3% | batch:       365 of       772\t|\tloss: 0.761851\n",
      "Training Epoch 10  47.4% | batch:       366 of       772\t|\tloss: 0.472907\n",
      "Training Epoch 10  47.5% | batch:       367 of       772\t|\tloss: 0.57788\n",
      "Training Epoch 10  47.7% | batch:       368 of       772\t|\tloss: 0.567838\n",
      "Training Epoch 10  47.8% | batch:       369 of       772\t|\tloss: 0.736659\n",
      "Training Epoch 10  47.9% | batch:       370 of       772\t|\tloss: 0.405836\n",
      "Training Epoch 10  48.1% | batch:       371 of       772\t|\tloss: 0.628377\n",
      "Training Epoch 10  48.2% | batch:       372 of       772\t|\tloss: 0.56721\n",
      "Training Epoch 10  48.3% | batch:       373 of       772\t|\tloss: 0.772241\n",
      "Training Epoch 10  48.4% | batch:       374 of       772\t|\tloss: 0.800766\n",
      "Training Epoch 10  48.6% | batch:       375 of       772\t|\tloss: 0.457971\n",
      "Training Epoch 10  48.7% | batch:       376 of       772\t|\tloss: 0.503542\n",
      "Training Epoch 10  48.8% | batch:       377 of       772\t|\tloss: 0.625581\n",
      "Training Epoch 10  49.0% | batch:       378 of       772\t|\tloss: 0.602771\n",
      "Training Epoch 10  49.1% | batch:       379 of       772\t|\tloss: 0.642765\n",
      "Training Epoch 10  49.2% | batch:       380 of       772\t|\tloss: 0.777958\n",
      "Training Epoch 10  49.4% | batch:       381 of       772\t|\tloss: 0.730126\n",
      "Training Epoch 10  49.5% | batch:       382 of       772\t|\tloss: 0.71208\n",
      "Training Epoch 10  49.6% | batch:       383 of       772\t|\tloss: 0.62839\n",
      "Training Epoch 10  49.7% | batch:       384 of       772\t|\tloss: 0.551921\n",
      "Training Epoch 10  49.9% | batch:       385 of       772\t|\tloss: 0.747821\n",
      "Training Epoch 10  50.0% | batch:       386 of       772\t|\tloss: 0.395326\n",
      "Training Epoch 10  50.1% | batch:       387 of       772\t|\tloss: 0.508208\n",
      "Training Epoch 10  50.3% | batch:       388 of       772\t|\tloss: 0.506834\n",
      "Training Epoch 10  50.4% | batch:       389 of       772\t|\tloss: 0.418064\n",
      "Training Epoch 10  50.5% | batch:       390 of       772\t|\tloss: 0.717667\n",
      "Training Epoch 10  50.6% | batch:       391 of       772\t|\tloss: 0.544431\n",
      "Training Epoch 10  50.8% | batch:       392 of       772\t|\tloss: 0.542336\n",
      "Training Epoch 10  50.9% | batch:       393 of       772\t|\tloss: 0.630223\n",
      "Training Epoch 10  51.0% | batch:       394 of       772\t|\tloss: 0.492087\n",
      "Training Epoch 10  51.2% | batch:       395 of       772\t|\tloss: 0.503889\n",
      "Training Epoch 10  51.3% | batch:       396 of       772\t|\tloss: 0.537281\n",
      "Training Epoch 10  51.4% | batch:       397 of       772\t|\tloss: 0.72021\n",
      "Training Epoch 10  51.6% | batch:       398 of       772\t|\tloss: 0.671819\n",
      "Training Epoch 10  51.7% | batch:       399 of       772\t|\tloss: 0.566978\n",
      "Training Epoch 10  51.8% | batch:       400 of       772\t|\tloss: 0.941191\n",
      "Training Epoch 10  51.9% | batch:       401 of       772\t|\tloss: 0.878463\n",
      "Training Epoch 10  52.1% | batch:       402 of       772\t|\tloss: 0.612671\n",
      "Training Epoch 10  52.2% | batch:       403 of       772\t|\tloss: 0.615672\n",
      "Training Epoch 10  52.3% | batch:       404 of       772\t|\tloss: 0.854745\n",
      "Training Epoch 10  52.5% | batch:       405 of       772\t|\tloss: 0.805278\n",
      "Training Epoch 10  52.6% | batch:       406 of       772\t|\tloss: 0.432303\n",
      "Training Epoch 10  52.7% | batch:       407 of       772\t|\tloss: 0.563039\n",
      "Training Epoch 10  52.8% | batch:       408 of       772\t|\tloss: 0.680741\n",
      "Training Epoch 10  53.0% | batch:       409 of       772\t|\tloss: 0.412951\n",
      "Training Epoch 10  53.1% | batch:       410 of       772\t|\tloss: 0.455979\n",
      "Training Epoch 10  53.2% | batch:       411 of       772\t|\tloss: 0.513556\n",
      "Training Epoch 10  53.4% | batch:       412 of       772\t|\tloss: 0.611612\n",
      "Training Epoch 10  53.5% | batch:       413 of       772\t|\tloss: 0.587195\n",
      "Training Epoch 10  53.6% | batch:       414 of       772\t|\tloss: 0.690755\n",
      "Training Epoch 10  53.8% | batch:       415 of       772\t|\tloss: 0.665094\n",
      "Training Epoch 10  53.9% | batch:       416 of       772\t|\tloss: 0.50301\n",
      "Training Epoch 10  54.0% | batch:       417 of       772\t|\tloss: 0.584751\n",
      "Training Epoch 10  54.1% | batch:       418 of       772\t|\tloss: 0.654513\n",
      "Training Epoch 10  54.3% | batch:       419 of       772\t|\tloss: 0.538072\n",
      "Training Epoch 10  54.4% | batch:       420 of       772\t|\tloss: 0.980589\n",
      "Training Epoch 10  54.5% | batch:       421 of       772\t|\tloss: 0.837643\n",
      "Training Epoch 10  54.7% | batch:       422 of       772\t|\tloss: 0.517086\n",
      "Training Epoch 10  54.8% | batch:       423 of       772\t|\tloss: 0.860468\n",
      "Training Epoch 10  54.9% | batch:       424 of       772\t|\tloss: 0.796367\n",
      "Training Epoch 10  55.1% | batch:       425 of       772\t|\tloss: 0.846153\n",
      "Training Epoch 10  55.2% | batch:       426 of       772\t|\tloss: 0.474887\n",
      "Training Epoch 10  55.3% | batch:       427 of       772\t|\tloss: 0.556513\n",
      "Training Epoch 10  55.4% | batch:       428 of       772\t|\tloss: 0.759513\n",
      "Training Epoch 10  55.6% | batch:       429 of       772\t|\tloss: 0.627527\n",
      "Training Epoch 10  55.7% | batch:       430 of       772\t|\tloss: 0.605465\n",
      "Training Epoch 10  55.8% | batch:       431 of       772\t|\tloss: 0.556903\n",
      "Training Epoch 10  56.0% | batch:       432 of       772\t|\tloss: 0.543472\n",
      "Training Epoch 10  56.1% | batch:       433 of       772\t|\tloss: 0.600516\n",
      "Training Epoch 10  56.2% | batch:       434 of       772\t|\tloss: 0.468357\n",
      "Training Epoch 10  56.3% | batch:       435 of       772\t|\tloss: 0.788497\n",
      "Training Epoch 10  56.5% | batch:       436 of       772\t|\tloss: 0.626071\n",
      "Training Epoch 10  56.6% | batch:       437 of       772\t|\tloss: 0.919622\n",
      "Training Epoch 10  56.7% | batch:       438 of       772\t|\tloss: 0.456788\n",
      "Training Epoch 10  56.9% | batch:       439 of       772\t|\tloss: 0.458546\n",
      "Training Epoch 10  57.0% | batch:       440 of       772\t|\tloss: 0.728113\n",
      "Training Epoch 10  57.1% | batch:       441 of       772\t|\tloss: 0.796015\n",
      "Training Epoch 10  57.3% | batch:       442 of       772\t|\tloss: 0.495427\n",
      "Training Epoch 10  57.4% | batch:       443 of       772\t|\tloss: 0.535096\n",
      "Training Epoch 10  57.5% | batch:       444 of       772\t|\tloss: 0.585804\n",
      "Training Epoch 10  57.6% | batch:       445 of       772\t|\tloss: 0.586556\n",
      "Training Epoch 10  57.8% | batch:       446 of       772\t|\tloss: 0.516543\n",
      "Training Epoch 10  57.9% | batch:       447 of       772\t|\tloss: 0.5205\n",
      "Training Epoch 10  58.0% | batch:       448 of       772\t|\tloss: 0.471778\n",
      "Training Epoch 10  58.2% | batch:       449 of       772\t|\tloss: 0.557595\n",
      "Training Epoch 10  58.3% | batch:       450 of       772\t|\tloss: 0.713046\n",
      "Training Epoch 10  58.4% | batch:       451 of       772\t|\tloss: 0.410563\n",
      "Training Epoch 10  58.5% | batch:       452 of       772\t|\tloss: 0.578393\n",
      "Training Epoch 10  58.7% | batch:       453 of       772\t|\tloss: 0.709408\n",
      "Training Epoch 10  58.8% | batch:       454 of       772\t|\tloss: 0.570819\n",
      "Training Epoch 10  58.9% | batch:       455 of       772\t|\tloss: 0.553863\n",
      "Training Epoch 10  59.1% | batch:       456 of       772\t|\tloss: 0.357231\n",
      "Training Epoch 10  59.2% | batch:       457 of       772\t|\tloss: 0.62516\n",
      "Training Epoch 10  59.3% | batch:       458 of       772\t|\tloss: 0.515146\n",
      "Training Epoch 10  59.5% | batch:       459 of       772\t|\tloss: 0.716796\n",
      "Training Epoch 10  59.6% | batch:       460 of       772\t|\tloss: 0.48884\n",
      "Training Epoch 10  59.7% | batch:       461 of       772\t|\tloss: 0.554208\n",
      "Training Epoch 10  59.8% | batch:       462 of       772\t|\tloss: 0.677697\n",
      "Training Epoch 10  60.0% | batch:       463 of       772\t|\tloss: 0.446958\n",
      "Training Epoch 10  60.1% | batch:       464 of       772\t|\tloss: 0.436124\n",
      "Training Epoch 10  60.2% | batch:       465 of       772\t|\tloss: 0.516874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  60.4% | batch:       466 of       772\t|\tloss: 0.501162\n",
      "Training Epoch 10  60.5% | batch:       467 of       772\t|\tloss: 0.522836\n",
      "Training Epoch 10  60.6% | batch:       468 of       772\t|\tloss: 0.398552\n",
      "Training Epoch 10  60.8% | batch:       469 of       772\t|\tloss: 0.598902\n",
      "Training Epoch 10  60.9% | batch:       470 of       772\t|\tloss: 0.49523\n",
      "Training Epoch 10  61.0% | batch:       471 of       772\t|\tloss: 0.373979\n",
      "Training Epoch 10  61.1% | batch:       472 of       772\t|\tloss: 0.646212\n",
      "Training Epoch 10  61.3% | batch:       473 of       772\t|\tloss: 0.804288\n",
      "Training Epoch 10  61.4% | batch:       474 of       772\t|\tloss: 0.620271\n",
      "Training Epoch 10  61.5% | batch:       475 of       772\t|\tloss: 0.534028\n",
      "Training Epoch 10  61.7% | batch:       476 of       772\t|\tloss: 0.823227\n",
      "Training Epoch 10  61.8% | batch:       477 of       772\t|\tloss: 0.624485\n",
      "Training Epoch 10  61.9% | batch:       478 of       772\t|\tloss: 0.429837\n",
      "Training Epoch 10  62.0% | batch:       479 of       772\t|\tloss: 0.443393\n",
      "Training Epoch 10  62.2% | batch:       480 of       772\t|\tloss: 0.559266\n",
      "Training Epoch 10  62.3% | batch:       481 of       772\t|\tloss: 0.534344\n",
      "Training Epoch 10  62.4% | batch:       482 of       772\t|\tloss: 0.549908\n",
      "Training Epoch 10  62.6% | batch:       483 of       772\t|\tloss: 0.532643\n",
      "Training Epoch 10  62.7% | batch:       484 of       772\t|\tloss: 0.704343\n",
      "Training Epoch 10  62.8% | batch:       485 of       772\t|\tloss: 0.645504\n",
      "Training Epoch 10  63.0% | batch:       486 of       772\t|\tloss: 0.711686\n",
      "Training Epoch 10  63.1% | batch:       487 of       772\t|\tloss: 0.932869\n",
      "Training Epoch 10  63.2% | batch:       488 of       772\t|\tloss: 0.897162\n",
      "Training Epoch 10  63.3% | batch:       489 of       772\t|\tloss: 0.619607\n",
      "Training Epoch 10  63.5% | batch:       490 of       772\t|\tloss: 0.536649\n",
      "Training Epoch 10  63.6% | batch:       491 of       772\t|\tloss: 0.622602\n",
      "Training Epoch 10  63.7% | batch:       492 of       772\t|\tloss: 0.745221\n",
      "Training Epoch 10  63.9% | batch:       493 of       772\t|\tloss: 0.726688\n",
      "Training Epoch 10  64.0% | batch:       494 of       772\t|\tloss: 0.686109\n",
      "Training Epoch 10  64.1% | batch:       495 of       772\t|\tloss: 0.665555\n",
      "Training Epoch 10  64.2% | batch:       496 of       772\t|\tloss: 0.864078\n",
      "Training Epoch 10  64.4% | batch:       497 of       772\t|\tloss: 0.805828\n",
      "Training Epoch 10  64.5% | batch:       498 of       772\t|\tloss: 0.492941\n",
      "Training Epoch 10  64.6% | batch:       499 of       772\t|\tloss: 0.633175\n",
      "Training Epoch 10  64.8% | batch:       500 of       772\t|\tloss: 0.650356\n",
      "Training Epoch 10  64.9% | batch:       501 of       772\t|\tloss: 0.756163\n",
      "Training Epoch 10  65.0% | batch:       502 of       772\t|\tloss: 0.801362\n",
      "Training Epoch 10  65.2% | batch:       503 of       772\t|\tloss: 0.668981\n",
      "Training Epoch 10  65.3% | batch:       504 of       772\t|\tloss: 0.524899\n",
      "Training Epoch 10  65.4% | batch:       505 of       772\t|\tloss: 0.651033\n",
      "Training Epoch 10  65.5% | batch:       506 of       772\t|\tloss: 0.509387\n",
      "Training Epoch 10  65.7% | batch:       507 of       772\t|\tloss: 0.56575\n",
      "Training Epoch 10  65.8% | batch:       508 of       772\t|\tloss: 0.809873\n",
      "Training Epoch 10  65.9% | batch:       509 of       772\t|\tloss: 0.880049\n",
      "Training Epoch 10  66.1% | batch:       510 of       772\t|\tloss: 0.601984\n",
      "Training Epoch 10  66.2% | batch:       511 of       772\t|\tloss: 0.600889\n",
      "Training Epoch 10  66.3% | batch:       512 of       772\t|\tloss: 0.56992\n",
      "Training Epoch 10  66.5% | batch:       513 of       772\t|\tloss: 0.483084\n",
      "Training Epoch 10  66.6% | batch:       514 of       772\t|\tloss: 0.540326\n",
      "Training Epoch 10  66.7% | batch:       515 of       772\t|\tloss: 0.475067\n",
      "Training Epoch 10  66.8% | batch:       516 of       772\t|\tloss: 0.601936\n",
      "Training Epoch 10  67.0% | batch:       517 of       772\t|\tloss: 0.559087\n",
      "Training Epoch 10  67.1% | batch:       518 of       772\t|\tloss: 0.503715\n",
      "Training Epoch 10  67.2% | batch:       519 of       772\t|\tloss: 0.52977\n",
      "Training Epoch 10  67.4% | batch:       520 of       772\t|\tloss: 0.84532\n",
      "Training Epoch 10  67.5% | batch:       521 of       772\t|\tloss: 0.785152\n",
      "Training Epoch 10  67.6% | batch:       522 of       772\t|\tloss: 0.654163\n",
      "Training Epoch 10  67.7% | batch:       523 of       772\t|\tloss: 0.418561\n",
      "Training Epoch 10  67.9% | batch:       524 of       772\t|\tloss: 0.399032\n",
      "Training Epoch 10  68.0% | batch:       525 of       772\t|\tloss: 0.683322\n",
      "Training Epoch 10  68.1% | batch:       526 of       772\t|\tloss: 0.462448\n",
      "Training Epoch 10  68.3% | batch:       527 of       772\t|\tloss: 0.422163\n",
      "Training Epoch 10  68.4% | batch:       528 of       772\t|\tloss: 0.557807\n",
      "Training Epoch 10  68.5% | batch:       529 of       772\t|\tloss: 0.526397\n",
      "Training Epoch 10  68.7% | batch:       530 of       772\t|\tloss: 0.77626\n",
      "Training Epoch 10  68.8% | batch:       531 of       772\t|\tloss: 0.466233\n",
      "Training Epoch 10  68.9% | batch:       532 of       772\t|\tloss: 0.798702\n",
      "Training Epoch 10  69.0% | batch:       533 of       772\t|\tloss: 0.636616\n",
      "Training Epoch 10  69.2% | batch:       534 of       772\t|\tloss: 0.52962\n",
      "Training Epoch 10  69.3% | batch:       535 of       772\t|\tloss: 0.449774\n",
      "Training Epoch 10  69.4% | batch:       536 of       772\t|\tloss: 1.04155\n",
      "Training Epoch 10  69.6% | batch:       537 of       772\t|\tloss: 1.26159\n",
      "Training Epoch 10  69.7% | batch:       538 of       772\t|\tloss: 0.829535\n",
      "Training Epoch 10  69.8% | batch:       539 of       772\t|\tloss: 0.492295\n",
      "Training Epoch 10  69.9% | batch:       540 of       772\t|\tloss: 0.47768\n",
      "Training Epoch 10  70.1% | batch:       541 of       772\t|\tloss: 0.689614\n",
      "Training Epoch 10  70.2% | batch:       542 of       772\t|\tloss: 0.714923\n",
      "Training Epoch 10  70.3% | batch:       543 of       772\t|\tloss: 0.471672\n",
      "Training Epoch 10  70.5% | batch:       544 of       772\t|\tloss: 0.669812\n",
      "Training Epoch 10  70.6% | batch:       545 of       772\t|\tloss: 0.797224\n",
      "Training Epoch 10  70.7% | batch:       546 of       772\t|\tloss: 0.615645\n",
      "Training Epoch 10  70.9% | batch:       547 of       772\t|\tloss: 0.617447\n",
      "Training Epoch 10  71.0% | batch:       548 of       772\t|\tloss: 0.810291\n",
      "Training Epoch 10  71.1% | batch:       549 of       772\t|\tloss: 0.544513\n",
      "Training Epoch 10  71.2% | batch:       550 of       772\t|\tloss: 0.500395\n",
      "Training Epoch 10  71.4% | batch:       551 of       772\t|\tloss: 0.741308\n",
      "Training Epoch 10  71.5% | batch:       552 of       772\t|\tloss: 0.574373\n",
      "Training Epoch 10  71.6% | batch:       553 of       772\t|\tloss: 0.468127\n",
      "Training Epoch 10  71.8% | batch:       554 of       772\t|\tloss: 0.561546\n",
      "Training Epoch 10  71.9% | batch:       555 of       772\t|\tloss: 0.949705\n",
      "Training Epoch 10  72.0% | batch:       556 of       772\t|\tloss: 0.539244\n",
      "Training Epoch 10  72.2% | batch:       557 of       772\t|\tloss: 0.566475\n",
      "Training Epoch 10  72.3% | batch:       558 of       772\t|\tloss: 0.585428\n",
      "Training Epoch 10  72.4% | batch:       559 of       772\t|\tloss: 0.5383\n",
      "Training Epoch 10  72.5% | batch:       560 of       772\t|\tloss: 0.617973\n",
      "Training Epoch 10  72.7% | batch:       561 of       772\t|\tloss: 0.434054\n",
      "Training Epoch 10  72.8% | batch:       562 of       772\t|\tloss: 0.450992\n",
      "Training Epoch 10  72.9% | batch:       563 of       772\t|\tloss: 0.714522\n",
      "Training Epoch 10  73.1% | batch:       564 of       772\t|\tloss: 0.594786\n",
      "Training Epoch 10  73.2% | batch:       565 of       772\t|\tloss: 0.671944\n",
      "Training Epoch 10  73.3% | batch:       566 of       772\t|\tloss: 0.764587\n",
      "Training Epoch 10  73.4% | batch:       567 of       772\t|\tloss: 0.739984\n",
      "Training Epoch 10  73.6% | batch:       568 of       772\t|\tloss: 0.627933\n",
      "Training Epoch 10  73.7% | batch:       569 of       772\t|\tloss: 0.532265\n",
      "Training Epoch 10  73.8% | batch:       570 of       772\t|\tloss: 0.674872\n",
      "Training Epoch 10  74.0% | batch:       571 of       772\t|\tloss: 0.567558\n",
      "Training Epoch 10  74.1% | batch:       572 of       772\t|\tloss: 0.623417\n",
      "Training Epoch 10  74.2% | batch:       573 of       772\t|\tloss: 0.656068\n",
      "Training Epoch 10  74.4% | batch:       574 of       772\t|\tloss: 0.541437\n",
      "Training Epoch 10  74.5% | batch:       575 of       772\t|\tloss: 0.593991\n",
      "Training Epoch 10  74.6% | batch:       576 of       772\t|\tloss: 0.485948\n",
      "Training Epoch 10  74.7% | batch:       577 of       772\t|\tloss: 0.850635\n",
      "Training Epoch 10  74.9% | batch:       578 of       772\t|\tloss: 0.689151\n",
      "Training Epoch 10  75.0% | batch:       579 of       772\t|\tloss: 0.835535\n",
      "Training Epoch 10  75.1% | batch:       580 of       772\t|\tloss: 0.566769\n",
      "Training Epoch 10  75.3% | batch:       581 of       772\t|\tloss: 0.476545\n",
      "Training Epoch 10  75.4% | batch:       582 of       772\t|\tloss: 0.534244\n",
      "Training Epoch 10  75.5% | batch:       583 of       772\t|\tloss: 0.691802\n",
      "Training Epoch 10  75.6% | batch:       584 of       772\t|\tloss: 0.479716\n",
      "Training Epoch 10  75.8% | batch:       585 of       772\t|\tloss: 0.606251\n",
      "Training Epoch 10  75.9% | batch:       586 of       772\t|\tloss: 0.490706\n",
      "Training Epoch 10  76.0% | batch:       587 of       772\t|\tloss: 0.487865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  76.2% | batch:       588 of       772\t|\tloss: 0.548032\n",
      "Training Epoch 10  76.3% | batch:       589 of       772\t|\tloss: 0.554707\n",
      "Training Epoch 10  76.4% | batch:       590 of       772\t|\tloss: 0.759479\n",
      "Training Epoch 10  76.6% | batch:       591 of       772\t|\tloss: 0.625773\n",
      "Training Epoch 10  76.7% | batch:       592 of       772\t|\tloss: 0.57997\n",
      "Training Epoch 10  76.8% | batch:       593 of       772\t|\tloss: 0.843694\n",
      "Training Epoch 10  76.9% | batch:       594 of       772\t|\tloss: 0.752682\n",
      "Training Epoch 10  77.1% | batch:       595 of       772\t|\tloss: 0.611602\n",
      "Training Epoch 10  77.2% | batch:       596 of       772\t|\tloss: 0.486983\n",
      "Training Epoch 10  77.3% | batch:       597 of       772\t|\tloss: 0.754167\n",
      "Training Epoch 10  77.5% | batch:       598 of       772\t|\tloss: 0.631921\n",
      "Training Epoch 10  77.6% | batch:       599 of       772\t|\tloss: 0.708795\n",
      "Training Epoch 10  77.7% | batch:       600 of       772\t|\tloss: 0.491748\n",
      "Training Epoch 10  77.8% | batch:       601 of       772\t|\tloss: 0.831624\n",
      "Training Epoch 10  78.0% | batch:       602 of       772\t|\tloss: 0.615527\n",
      "Training Epoch 10  78.1% | batch:       603 of       772\t|\tloss: 0.516384\n",
      "Training Epoch 10  78.2% | batch:       604 of       772\t|\tloss: 0.530939\n",
      "Training Epoch 10  78.4% | batch:       605 of       772\t|\tloss: 0.519981\n",
      "Training Epoch 10  78.5% | batch:       606 of       772\t|\tloss: 0.626437\n",
      "Training Epoch 10  78.6% | batch:       607 of       772\t|\tloss: 0.417707\n",
      "Training Epoch 10  78.8% | batch:       608 of       772\t|\tloss: 0.927936\n",
      "Training Epoch 10  78.9% | batch:       609 of       772\t|\tloss: 0.598343\n",
      "Training Epoch 10  79.0% | batch:       610 of       772\t|\tloss: 0.508544\n",
      "Training Epoch 10  79.1% | batch:       611 of       772\t|\tloss: 0.394924\n",
      "Training Epoch 10  79.3% | batch:       612 of       772\t|\tloss: 0.499625\n",
      "Training Epoch 10  79.4% | batch:       613 of       772\t|\tloss: 0.539123\n",
      "Training Epoch 10  79.5% | batch:       614 of       772\t|\tloss: 0.523757\n",
      "Training Epoch 10  79.7% | batch:       615 of       772\t|\tloss: 0.555463\n",
      "Training Epoch 10  79.8% | batch:       616 of       772\t|\tloss: 0.518593\n",
      "Training Epoch 10  79.9% | batch:       617 of       772\t|\tloss: 0.467658\n",
      "Training Epoch 10  80.1% | batch:       618 of       772\t|\tloss: 0.726752\n",
      "Training Epoch 10  80.2% | batch:       619 of       772\t|\tloss: 0.491186\n",
      "Training Epoch 10  80.3% | batch:       620 of       772\t|\tloss: 0.599888\n",
      "Training Epoch 10  80.4% | batch:       621 of       772\t|\tloss: 0.580086\n",
      "Training Epoch 10  80.6% | batch:       622 of       772\t|\tloss: 0.666449\n",
      "Training Epoch 10  80.7% | batch:       623 of       772\t|\tloss: 0.547299\n",
      "Training Epoch 10  80.8% | batch:       624 of       772\t|\tloss: 0.553665\n",
      "Training Epoch 10  81.0% | batch:       625 of       772\t|\tloss: 0.783232\n",
      "Training Epoch 10  81.1% | batch:       626 of       772\t|\tloss: 0.40977\n",
      "Training Epoch 10  81.2% | batch:       627 of       772\t|\tloss: 0.764991\n",
      "Training Epoch 10  81.3% | batch:       628 of       772\t|\tloss: 0.646056\n",
      "Training Epoch 10  81.5% | batch:       629 of       772\t|\tloss: 0.643966\n",
      "Training Epoch 10  81.6% | batch:       630 of       772\t|\tloss: 0.527354\n",
      "Training Epoch 10  81.7% | batch:       631 of       772\t|\tloss: 0.504122\n",
      "Training Epoch 10  81.9% | batch:       632 of       772\t|\tloss: 0.420932\n",
      "Training Epoch 10  82.0% | batch:       633 of       772\t|\tloss: 0.573071\n",
      "Training Epoch 10  82.1% | batch:       634 of       772\t|\tloss: 0.458805\n",
      "Training Epoch 10  82.3% | batch:       635 of       772\t|\tloss: 0.575949\n",
      "Training Epoch 10  82.4% | batch:       636 of       772\t|\tloss: 0.42468\n",
      "Training Epoch 10  82.5% | batch:       637 of       772\t|\tloss: 0.700469\n",
      "Training Epoch 10  82.6% | batch:       638 of       772\t|\tloss: 0.672778\n",
      "Training Epoch 10  82.8% | batch:       639 of       772\t|\tloss: 0.715255\n",
      "Training Epoch 10  82.9% | batch:       640 of       772\t|\tloss: 0.606942\n",
      "Training Epoch 10  83.0% | batch:       641 of       772\t|\tloss: 0.628651\n",
      "Training Epoch 10  83.2% | batch:       642 of       772\t|\tloss: 0.567582\n",
      "Training Epoch 10  83.3% | batch:       643 of       772\t|\tloss: 0.70706\n",
      "Training Epoch 10  83.4% | batch:       644 of       772\t|\tloss: 0.778101\n",
      "Training Epoch 10  83.5% | batch:       645 of       772\t|\tloss: 0.696275\n",
      "Training Epoch 10  83.7% | batch:       646 of       772\t|\tloss: 0.807179\n",
      "Training Epoch 10  83.8% | batch:       647 of       772\t|\tloss: 0.453407\n",
      "Training Epoch 10  83.9% | batch:       648 of       772\t|\tloss: 0.455914\n",
      "Training Epoch 10  84.1% | batch:       649 of       772\t|\tloss: 0.734365\n",
      "Training Epoch 10  84.2% | batch:       650 of       772\t|\tloss: 0.569388\n",
      "Training Epoch 10  84.3% | batch:       651 of       772\t|\tloss: 0.684618\n",
      "Training Epoch 10  84.5% | batch:       652 of       772\t|\tloss: 0.647752\n",
      "Training Epoch 10  84.6% | batch:       653 of       772\t|\tloss: 0.801134\n",
      "Training Epoch 10  84.7% | batch:       654 of       772\t|\tloss: 0.529305\n",
      "Training Epoch 10  84.8% | batch:       655 of       772\t|\tloss: 0.496045\n",
      "Training Epoch 10  85.0% | batch:       656 of       772\t|\tloss: 0.821478\n",
      "Training Epoch 10  85.1% | batch:       657 of       772\t|\tloss: 0.564531\n",
      "Training Epoch 10  85.2% | batch:       658 of       772\t|\tloss: 0.529101\n",
      "Training Epoch 10  85.4% | batch:       659 of       772\t|\tloss: 0.730745\n",
      "Training Epoch 10  85.5% | batch:       660 of       772\t|\tloss: 0.538942\n",
      "Training Epoch 10  85.6% | batch:       661 of       772\t|\tloss: 0.656278\n",
      "Training Epoch 10  85.8% | batch:       662 of       772\t|\tloss: 0.480275\n",
      "Training Epoch 10  85.9% | batch:       663 of       772\t|\tloss: 0.548555\n",
      "Training Epoch 10  86.0% | batch:       664 of       772\t|\tloss: 0.786915\n",
      "Training Epoch 10  86.1% | batch:       665 of       772\t|\tloss: 0.508139\n",
      "Training Epoch 10  86.3% | batch:       666 of       772\t|\tloss: 0.416792\n",
      "Training Epoch 10  86.4% | batch:       667 of       772\t|\tloss: 0.71843\n",
      "Training Epoch 10  86.5% | batch:       668 of       772\t|\tloss: 0.857288\n",
      "Training Epoch 10  86.7% | batch:       669 of       772\t|\tloss: 0.52699\n",
      "Training Epoch 10  86.8% | batch:       670 of       772\t|\tloss: 0.524142\n",
      "Training Epoch 10  86.9% | batch:       671 of       772\t|\tloss: 0.634259\n",
      "Training Epoch 10  87.0% | batch:       672 of       772\t|\tloss: 0.510172\n",
      "Training Epoch 10  87.2% | batch:       673 of       772\t|\tloss: 0.654471\n",
      "Training Epoch 10  87.3% | batch:       674 of       772\t|\tloss: 0.832351\n",
      "Training Epoch 10  87.4% | batch:       675 of       772\t|\tloss: 0.458246\n",
      "Training Epoch 10  87.6% | batch:       676 of       772\t|\tloss: 0.493776\n",
      "Training Epoch 10  87.7% | batch:       677 of       772\t|\tloss: 0.6066\n",
      "Training Epoch 10  87.8% | batch:       678 of       772\t|\tloss: 0.637567\n",
      "Training Epoch 10  88.0% | batch:       679 of       772\t|\tloss: 0.805677\n",
      "Training Epoch 10  88.1% | batch:       680 of       772\t|\tloss: 0.747702\n",
      "Training Epoch 10  88.2% | batch:       681 of       772\t|\tloss: 0.827872\n",
      "Training Epoch 10  88.3% | batch:       682 of       772\t|\tloss: 0.767662\n",
      "Training Epoch 10  88.5% | batch:       683 of       772\t|\tloss: 0.750134\n",
      "Training Epoch 10  88.6% | batch:       684 of       772\t|\tloss: 0.855814\n",
      "Training Epoch 10  88.7% | batch:       685 of       772\t|\tloss: 0.447141\n",
      "Training Epoch 10  88.9% | batch:       686 of       772\t|\tloss: 0.415767\n",
      "Training Epoch 10  89.0% | batch:       687 of       772\t|\tloss: 0.730818\n",
      "Training Epoch 10  89.1% | batch:       688 of       772\t|\tloss: 0.570897\n",
      "Training Epoch 10  89.2% | batch:       689 of       772\t|\tloss: 0.860158\n",
      "Training Epoch 10  89.4% | batch:       690 of       772\t|\tloss: 0.44031\n",
      "Training Epoch 10  89.5% | batch:       691 of       772\t|\tloss: 0.603779\n",
      "Training Epoch 10  89.6% | batch:       692 of       772\t|\tloss: 0.43382\n",
      "Training Epoch 10  89.8% | batch:       693 of       772\t|\tloss: 0.551203\n",
      "Training Epoch 10  89.9% | batch:       694 of       772\t|\tloss: 0.623483\n",
      "Training Epoch 10  90.0% | batch:       695 of       772\t|\tloss: 0.641982\n",
      "Training Epoch 10  90.2% | batch:       696 of       772\t|\tloss: 0.696956\n",
      "Training Epoch 10  90.3% | batch:       697 of       772\t|\tloss: 0.504914\n",
      "Training Epoch 10  90.4% | batch:       698 of       772\t|\tloss: 0.674656\n",
      "Training Epoch 10  90.5% | batch:       699 of       772\t|\tloss: 0.736336\n",
      "Training Epoch 10  90.7% | batch:       700 of       772\t|\tloss: 0.551438\n",
      "Training Epoch 10  90.8% | batch:       701 of       772\t|\tloss: 0.408688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  90.9% | batch:       702 of       772\t|\tloss: 0.576907\n",
      "Training Epoch 10  91.1% | batch:       703 of       772\t|\tloss: 0.611028\n",
      "Training Epoch 10  91.2% | batch:       704 of       772\t|\tloss: 0.487664\n",
      "Training Epoch 10  91.3% | batch:       705 of       772\t|\tloss: 0.595503\n",
      "Training Epoch 10  91.5% | batch:       706 of       772\t|\tloss: 0.750049\n",
      "Training Epoch 10  91.6% | batch:       707 of       772\t|\tloss: 0.94339\n",
      "Training Epoch 10  91.7% | batch:       708 of       772\t|\tloss: 0.590736\n",
      "Training Epoch 10  91.8% | batch:       709 of       772\t|\tloss: 0.658056\n",
      "Training Epoch 10  92.0% | batch:       710 of       772\t|\tloss: 0.663904\n",
      "Training Epoch 10  92.1% | batch:       711 of       772\t|\tloss: 0.628487\n",
      "Training Epoch 10  92.2% | batch:       712 of       772\t|\tloss: 0.91824\n",
      "Training Epoch 10  92.4% | batch:       713 of       772\t|\tloss: 0.787534\n",
      "Training Epoch 10  92.5% | batch:       714 of       772\t|\tloss: 0.448205\n",
      "Training Epoch 10  92.6% | batch:       715 of       772\t|\tloss: 0.475382\n",
      "Training Epoch 10  92.7% | batch:       716 of       772\t|\tloss: 0.669529\n",
      "Training Epoch 10  92.9% | batch:       717 of       772\t|\tloss: 0.496691\n",
      "Training Epoch 10  93.0% | batch:       718 of       772\t|\tloss: 0.36949\n",
      "Training Epoch 10  93.1% | batch:       719 of       772\t|\tloss: 0.541728\n",
      "Training Epoch 10  93.3% | batch:       720 of       772\t|\tloss: 0.286461\n",
      "Training Epoch 10  93.4% | batch:       721 of       772\t|\tloss: 0.406139\n",
      "Training Epoch 10  93.5% | batch:       722 of       772\t|\tloss: 0.39499\n",
      "Training Epoch 10  93.7% | batch:       723 of       772\t|\tloss: 0.806528\n",
      "Training Epoch 10  93.8% | batch:       724 of       772\t|\tloss: 0.385694\n",
      "Training Epoch 10  93.9% | batch:       725 of       772\t|\tloss: 0.372425\n",
      "Training Epoch 10  94.0% | batch:       726 of       772\t|\tloss: 0.474414\n",
      "Training Epoch 10  94.2% | batch:       727 of       772\t|\tloss: 0.437093\n",
      "Training Epoch 10  94.3% | batch:       728 of       772\t|\tloss: 0.517716\n",
      "Training Epoch 10  94.4% | batch:       729 of       772\t|\tloss: 0.46632\n",
      "Training Epoch 10  94.6% | batch:       730 of       772\t|\tloss: 0.705875\n",
      "Training Epoch 10  94.7% | batch:       731 of       772\t|\tloss: 0.616325\n",
      "Training Epoch 10  94.8% | batch:       732 of       772\t|\tloss: 0.456356\n",
      "Training Epoch 10  94.9% | batch:       733 of       772\t|\tloss: 0.526281\n",
      "Training Epoch 10  95.1% | batch:       734 of       772\t|\tloss: 0.742584\n",
      "Training Epoch 10  95.2% | batch:       735 of       772\t|\tloss: 0.40466\n",
      "Training Epoch 10  95.3% | batch:       736 of       772\t|\tloss: 0.503386\n",
      "Training Epoch 10  95.5% | batch:       737 of       772\t|\tloss: 0.519314\n",
      "Training Epoch 10  95.6% | batch:       738 of       772\t|\tloss: 0.70448\n",
      "Training Epoch 10  95.7% | batch:       739 of       772\t|\tloss: 0.508216\n",
      "Training Epoch 10  95.9% | batch:       740 of       772\t|\tloss: 0.448085\n",
      "Training Epoch 10  96.0% | batch:       741 of       772\t|\tloss: 0.658218\n",
      "Training Epoch 10  96.1% | batch:       742 of       772\t|\tloss: 0.472939\n",
      "Training Epoch 10  96.2% | batch:       743 of       772\t|\tloss: 0.606555\n",
      "Training Epoch 10  96.4% | batch:       744 of       772\t|\tloss: 0.571054\n",
      "Training Epoch 10  96.5% | batch:       745 of       772\t|\tloss: 0.637369\n",
      "Training Epoch 10  96.6% | batch:       746 of       772\t|\tloss: 0.598911\n",
      "Training Epoch 10  96.8% | batch:       747 of       772\t|\tloss: 0.509363\n",
      "Training Epoch 10  96.9% | batch:       748 of       772\t|\tloss: 0.443481\n",
      "Training Epoch 10  97.0% | batch:       749 of       772\t|\tloss: 0.52406\n",
      "Training Epoch 10  97.2% | batch:       750 of       772\t|\tloss: 0.567111\n",
      "Training Epoch 10  97.3% | batch:       751 of       772\t|\tloss: 0.654751\n",
      "Training Epoch 10  97.4% | batch:       752 of       772\t|\tloss: 0.585662\n",
      "Training Epoch 10  97.5% | batch:       753 of       772\t|\tloss: 0.633125\n",
      "Training Epoch 10  97.7% | batch:       754 of       772\t|\tloss: 0.809563\n",
      "Training Epoch 10  97.8% | batch:       755 of       772\t|\tloss: 1.00471\n",
      "Training Epoch 10  97.9% | batch:       756 of       772\t|\tloss: 0.45022\n",
      "Training Epoch 10  98.1% | batch:       757 of       772\t|\tloss: 0.547432\n",
      "Training Epoch 10  98.2% | batch:       758 of       772\t|\tloss: 0.772334\n",
      "Training Epoch 10  98.3% | batch:       759 of       772\t|\tloss: 0.734555\n",
      "Training Epoch 10  98.4% | batch:       760 of       772\t|\tloss: 0.644546\n",
      "Training Epoch 10  98.6% | batch:       761 of       772\t|\tloss: 0.725096\n",
      "Training Epoch 10  98.7% | batch:       762 of       772\t|\tloss: 0.682614\n",
      "Training Epoch 10  98.8% | batch:       763 of       772\t|\tloss: 0.744629\n",
      "Training Epoch 10  99.0% | batch:       764 of       772\t|\tloss: 0.552253\n",
      "Training Epoch 10  99.1% | batch:       765 of       772\t|\tloss: 0.565664\n",
      "Training Epoch 10  99.2% | batch:       766 of       772\t|\tloss: 0.607417\n",
      "Training Epoch 10  99.4% | batch:       767 of       772\t|\tloss: 0.396108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:22:56,824 | INFO : Epoch 10 Training Summary: epoch: 10.000000 | loss: 0.645436 | \n",
      "2023-05-24 10:22:56,825 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 14.691800355911255 seconds\n",
      "\n",
      "2023-05-24 10:22:56,825 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 16.170465564727785 seconds\n",
      "2023-05-24 10:22:56,825 | INFO : Avg batch train. time: 0.02094619891804117 seconds\n",
      "2023-05-24 10:22:56,826 | INFO : Avg sample train. time: 0.0001636454911725847 seconds\n",
      "2023-05-24 10:22:56,826 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  99.5% | batch:       768 of       772\t|\tloss: 0.581628\n",
      "Training Epoch 10  99.6% | batch:       769 of       772\t|\tloss: 0.422266\n",
      "Training Epoch 10  99.7% | batch:       770 of       772\t|\tloss: 0.563466\n",
      "Training Epoch 10  99.9% | batch:       771 of       772\t|\tloss: 0.547744\n",
      "\n",
      "Evaluating Epoch 10   0.0% | batch:         0 of        92\t|\tloss: 0.902251\n",
      "Evaluating Epoch 10   1.1% | batch:         1 of        92\t|\tloss: 8.43731\n",
      "Evaluating Epoch 10   2.2% | batch:         2 of        92\t|\tloss: 2.58634\n",
      "Evaluating Epoch 10   3.3% | batch:         3 of        92\t|\tloss: 5.82693\n",
      "Evaluating Epoch 10   4.3% | batch:         4 of        92\t|\tloss: 3.05182\n",
      "Evaluating Epoch 10   5.4% | batch:         5 of        92\t|\tloss: 8.28578\n",
      "Evaluating Epoch 10   6.5% | batch:         6 of        92\t|\tloss: 4.54147\n",
      "Evaluating Epoch 10   7.6% | batch:         7 of        92\t|\tloss: 1.98958\n",
      "Evaluating Epoch 10   8.7% | batch:         8 of        92\t|\tloss: 6.07706\n",
      "Evaluating Epoch 10   9.8% | batch:         9 of        92\t|\tloss: 4.20879\n",
      "Evaluating Epoch 10  10.9% | batch:        10 of        92\t|\tloss: 5.1407\n",
      "Evaluating Epoch 10  12.0% | batch:        11 of        92\t|\tloss: 3.15781\n",
      "Evaluating Epoch 10  13.0% | batch:        12 of        92\t|\tloss: 7.65355\n",
      "Evaluating Epoch 10  14.1% | batch:        13 of        92\t|\tloss: 5.81194\n",
      "Evaluating Epoch 10  15.2% | batch:        14 of        92\t|\tloss: 1.51429\n",
      "Evaluating Epoch 10  16.3% | batch:        15 of        92\t|\tloss: 0.761507\n",
      "Evaluating Epoch 10  17.4% | batch:        16 of        92\t|\tloss: 2.93325\n",
      "Evaluating Epoch 10  18.5% | batch:        17 of        92\t|\tloss: 1.47529\n",
      "Evaluating Epoch 10  19.6% | batch:        18 of        92\t|\tloss: 3.76461\n",
      "Evaluating Epoch 10  20.7% | batch:        19 of        92\t|\tloss: 5.11157\n",
      "Evaluating Epoch 10  21.7% | batch:        20 of        92\t|\tloss: 2.79927\n",
      "Evaluating Epoch 10  22.8% | batch:        21 of        92\t|\tloss: 3.06391\n",
      "Evaluating Epoch 10  23.9% | batch:        22 of        92\t|\tloss: 5.65433\n",
      "Evaluating Epoch 10  25.0% | batch:        23 of        92\t|\tloss: 6.70766\n",
      "Evaluating Epoch 10  26.1% | batch:        24 of        92\t|\tloss: 1.78659\n",
      "Evaluating Epoch 10  27.2% | batch:        25 of        92\t|\tloss: 0.306841\n",
      "Evaluating Epoch 10  28.3% | batch:        26 of        92\t|\tloss: 1.3995\n",
      "Evaluating Epoch 10  29.3% | batch:        27 of        92\t|\tloss: 3.10874\n",
      "Evaluating Epoch 10  30.4% | batch:        28 of        92\t|\tloss: 3.02508\n",
      "Evaluating Epoch 10  31.5% | batch:        29 of        92\t|\tloss: 3.28108\n",
      "Evaluating Epoch 10  32.6% | batch:        30 of        92\t|\tloss: 2.71609\n",
      "Evaluating Epoch 10  33.7% | batch:        31 of        92\t|\tloss: 3.82927\n",
      "Evaluating Epoch 10  34.8% | batch:        32 of        92\t|\tloss: 2.48438\n",
      "Evaluating Epoch 10  35.9% | batch:        33 of        92\t|\tloss: 5.30456\n",
      "Evaluating Epoch 10  37.0% | batch:        34 of        92\t|\tloss: 3.54002\n",
      "Evaluating Epoch 10  38.0% | batch:        35 of        92\t|\tloss: 2.41554\n",
      "Evaluating Epoch 10  39.1% | batch:        36 of        92\t|\tloss: 1.73262\n",
      "Evaluating Epoch 10  40.2% | batch:        37 of        92\t|\tloss: 2.52651\n",
      "Evaluating Epoch 10  41.3% | batch:        38 of        92\t|\tloss: 2.46399\n",
      "Evaluating Epoch 10  42.4% | batch:        39 of        92\t|\tloss: 6.82927\n",
      "Evaluating Epoch 10  43.5% | batch:        40 of        92\t|\tloss: 2.74518\n",
      "Evaluating Epoch 10  44.6% | batch:        41 of        92\t|\tloss: 4.47926\n",
      "Evaluating Epoch 10  45.7% | batch:        42 of        92\t|\tloss: 4.65121\n",
      "Evaluating Epoch 10  46.7% | batch:        43 of        92\t|\tloss: 7.725\n",
      "Evaluating Epoch 10  47.8% | batch:        44 of        92\t|\tloss: 1.70954\n",
      "Evaluating Epoch 10  48.9% | batch:        45 of        92\t|\tloss: 1.45709\n",
      "Evaluating Epoch 10  50.0% | batch:        46 of        92\t|\tloss: 1.74142\n",
      "Evaluating Epoch 10  51.1% | batch:        47 of        92\t|\tloss: 4.14126\n",
      "Evaluating Epoch 10  52.2% | batch:        48 of        92\t|\tloss: 5.73589\n",
      "Evaluating Epoch 10  53.3% | batch:        49 of        92\t|\tloss: 3.95481\n",
      "Evaluating Epoch 10  54.3% | batch:        50 of        92\t|\tloss: 4.15861\n",
      "Evaluating Epoch 10  55.4% | batch:        51 of        92\t|\tloss: 6.76913\n",
      "Evaluating Epoch 10  56.5% | batch:        52 of        92\t|\tloss: 6.50197\n",
      "Evaluating Epoch 10  57.6% | batch:        53 of        92\t|\tloss: 1.4523\n",
      "Evaluating Epoch 10  58.7% | batch:        54 of        92\t|\tloss: 1.55007\n",
      "Evaluating Epoch 10  59.8% | batch:        55 of        92\t|\tloss: 5.12363\n",
      "Evaluating Epoch 10  60.9% | batch:        56 of        92\t|\tloss: 6.33399\n",
      "Evaluating Epoch 10  62.0% | batch:        57 of        92\t|\tloss: 4.14757\n",
      "Evaluating Epoch 10  63.0% | batch:        58 of        92\t|\tloss: 3.80029\n",
      "Evaluating Epoch 10  64.1% | batch:        59 of        92\t|\tloss: 6.85145\n",
      "Evaluating Epoch 10  65.2% | batch:        60 of        92\t|\tloss: 6.58048\n",
      "Evaluating Epoch 10  66.3% | batch:        61 of        92\t|\tloss: 1.60633\n",
      "Evaluating Epoch 10  67.4% | batch:        62 of        92\t|\tloss: 0.538611\n",
      "Evaluating Epoch 10  68.5% | batch:        63 of        92\t|\tloss: 1.52029\n",
      "Evaluating Epoch 10  69.6% | batch:        64 of        92\t|\tloss: 3.84433\n",
      "Evaluating Epoch 10  70.7% | batch:        65 of        92\t|\tloss: 6.87837\n",
      "Evaluating Epoch 10  71.7% | batch:        66 of        92\t|\tloss: 2.94044\n",
      "Evaluating Epoch 10  72.8% | batch:        67 of        92\t|\tloss: 3.84086\n",
      "Evaluating Epoch 10  73.9% | batch:        68 of        92\t|\tloss: 4.51616\n",
      "Evaluating Epoch 10  75.0% | batch:        69 of        92\t|\tloss: 3.16154\n",
      "Evaluating Epoch 10  76.1% | batch:        70 of        92\t|\tloss: 5.82374\n",
      "Evaluating Epoch 10  77.2% | batch:        71 of        92\t|\tloss: 3.55997\n",
      "Evaluating Epoch 10  78.3% | batch:        72 of        92\t|\tloss: 3.24159\n",
      "Evaluating Epoch 10  79.3% | batch:        73 of        92\t|\tloss: 2.22347\n",
      "Evaluating Epoch 10  80.4% | batch:        74 of        92\t|\tloss: 4.49819\n",
      "Evaluating Epoch 10  81.5% | batch:        75 of        92\t|\tloss: 1.39494\n",
      "Evaluating Epoch 10  82.6% | batch:        76 of        92\t|\tloss: 3.68505\n",
      "Evaluating Epoch 10  83.7% | batch:        77 of        92\t|\tloss: 4.9409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:22:57,940 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.113365650177002 seconds\n",
      "\n",
      "2023-05-24 10:22:57,940 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.3129892008645194 seconds\n",
      "2023-05-24 10:22:57,941 | INFO : Avg batch val. time: 0.014271621748527386 seconds\n",
      "2023-05-24 10:22:57,941 | INFO : Avg sample val. time: 0.0001125483628376924 seconds\n",
      "2023-05-24 10:22:57,941 | INFO : Epoch 10 Validation Summary: epoch: 10.000000 | loss: 3.857020 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 10  84.8% | batch:        78 of        92\t|\tloss: 4.28434\n",
      "Evaluating Epoch 10  85.9% | batch:        79 of        92\t|\tloss: 4.41046\n",
      "Evaluating Epoch 10  87.0% | batch:        80 of        92\t|\tloss: 4.92293\n",
      "Evaluating Epoch 10  88.0% | batch:        81 of        92\t|\tloss: 6.05727\n",
      "Evaluating Epoch 10  89.1% | batch:        82 of        92\t|\tloss: 1.29834\n",
      "Evaluating Epoch 10  90.2% | batch:        83 of        92\t|\tloss: 1.50926\n",
      "Evaluating Epoch 10  91.3% | batch:        84 of        92\t|\tloss: 3.9145\n",
      "Evaluating Epoch 10  92.4% | batch:        85 of        92\t|\tloss: 5.57139\n",
      "Evaluating Epoch 10  93.5% | batch:        86 of        92\t|\tloss: 3.79906\n",
      "Evaluating Epoch 10  94.6% | batch:        87 of        92\t|\tloss: 3.46034\n",
      "Evaluating Epoch 10  95.7% | batch:        88 of        92\t|\tloss: 6.06314\n",
      "Evaluating Epoch 10  96.7% | batch:        89 of        92\t|\tloss: 6.54561\n",
      "Evaluating Epoch 10  97.8% | batch:        90 of        92\t|\tloss: 1.62728\n",
      "Evaluating Epoch 10  98.9% | batch:        91 of        92\t|\tloss: 0.251159\n",
      "\n",
      "Training Epoch 11   0.0% | batch:         0 of       772\t|\tloss: 0.59129\n",
      "Training Epoch 11   0.1% | batch:         1 of       772\t|\tloss: 0.478836\n",
      "Training Epoch 11   0.3% | batch:         2 of       772\t|\tloss: 0.520936\n",
      "Training Epoch 11   0.4% | batch:         3 of       772\t|\tloss: 1.23362\n",
      "Training Epoch 11   0.5% | batch:         4 of       772\t|\tloss: 0.721717\n",
      "Training Epoch 11   0.6% | batch:         5 of       772\t|\tloss: 0.627617\n",
      "Training Epoch 11   0.8% | batch:         6 of       772\t|\tloss: 0.397163\n",
      "Training Epoch 11   0.9% | batch:         7 of       772\t|\tloss: 0.64461\n",
      "Training Epoch 11   1.0% | batch:         8 of       772\t|\tloss: 0.581792\n",
      "Training Epoch 11   1.2% | batch:         9 of       772\t|\tloss: 0.781239\n",
      "Training Epoch 11   1.3% | batch:        10 of       772\t|\tloss: 0.764012\n",
      "Training Epoch 11   1.4% | batch:        11 of       772\t|\tloss: 0.558001\n",
      "Training Epoch 11   1.6% | batch:        12 of       772\t|\tloss: 0.668221\n",
      "Training Epoch 11   1.7% | batch:        13 of       772\t|\tloss: 0.355378\n",
      "Training Epoch 11   1.8% | batch:        14 of       772\t|\tloss: 0.749335\n",
      "Training Epoch 11   1.9% | batch:        15 of       772\t|\tloss: 0.849719\n",
      "Training Epoch 11   2.1% | batch:        16 of       772\t|\tloss: 0.502403\n",
      "Training Epoch 11   2.2% | batch:        17 of       772\t|\tloss: 0.368316\n",
      "Training Epoch 11   2.3% | batch:        18 of       772\t|\tloss: 0.623001\n",
      "Training Epoch 11   2.5% | batch:        19 of       772\t|\tloss: 0.445493\n",
      "Training Epoch 11   2.6% | batch:        20 of       772\t|\tloss: 0.374313\n",
      "Training Epoch 11   2.7% | batch:        21 of       772\t|\tloss: 0.466113\n",
      "Training Epoch 11   2.8% | batch:        22 of       772\t|\tloss: 0.446451\n",
      "Training Epoch 11   3.0% | batch:        23 of       772\t|\tloss: 0.408563\n",
      "Training Epoch 11   3.1% | batch:        24 of       772\t|\tloss: 0.69331\n",
      "Training Epoch 11   3.2% | batch:        25 of       772\t|\tloss: 0.525066\n",
      "Training Epoch 11   3.4% | batch:        26 of       772\t|\tloss: 0.578889\n",
      "Training Epoch 11   3.5% | batch:        27 of       772\t|\tloss: 0.584752\n",
      "Training Epoch 11   3.6% | batch:        28 of       772\t|\tloss: 0.629719\n",
      "Training Epoch 11   3.8% | batch:        29 of       772\t|\tloss: 0.600026\n",
      "Training Epoch 11   3.9% | batch:        30 of       772\t|\tloss: 0.462007\n",
      "Training Epoch 11   4.0% | batch:        31 of       772\t|\tloss: 0.449943\n",
      "Training Epoch 11   4.1% | batch:        32 of       772\t|\tloss: 0.43763\n",
      "Training Epoch 11   4.3% | batch:        33 of       772\t|\tloss: 0.554876\n",
      "Training Epoch 11   4.4% | batch:        34 of       772\t|\tloss: 0.515985\n",
      "Training Epoch 11   4.5% | batch:        35 of       772\t|\tloss: 0.48906\n",
      "Training Epoch 11   4.7% | batch:        36 of       772\t|\tloss: 0.393071\n",
      "Training Epoch 11   4.8% | batch:        37 of       772\t|\tloss: 0.541272\n",
      "Training Epoch 11   4.9% | batch:        38 of       772\t|\tloss: 0.425273\n",
      "Training Epoch 11   5.1% | batch:        39 of       772\t|\tloss: 0.597184\n",
      "Training Epoch 11   5.2% | batch:        40 of       772\t|\tloss: 1.05586\n",
      "Training Epoch 11   5.3% | batch:        41 of       772\t|\tloss: 0.7895\n",
      "Training Epoch 11   5.4% | batch:        42 of       772\t|\tloss: 0.755845\n",
      "Training Epoch 11   5.6% | batch:        43 of       772\t|\tloss: 0.700737\n",
      "Training Epoch 11   5.7% | batch:        44 of       772\t|\tloss: 0.537556\n",
      "Training Epoch 11   5.8% | batch:        45 of       772\t|\tloss: 0.610071\n",
      "Training Epoch 11   6.0% | batch:        46 of       772\t|\tloss: 0.670857\n",
      "Training Epoch 11   6.1% | batch:        47 of       772\t|\tloss: 0.616481\n",
      "Training Epoch 11   6.2% | batch:        48 of       772\t|\tloss: 0.879013\n",
      "Training Epoch 11   6.3% | batch:        49 of       772\t|\tloss: 0.883132\n",
      "Training Epoch 11   6.5% | batch:        50 of       772\t|\tloss: 0.703292\n",
      "Training Epoch 11   6.6% | batch:        51 of       772\t|\tloss: 0.4713\n",
      "Training Epoch 11   6.7% | batch:        52 of       772\t|\tloss: 0.84496\n",
      "Training Epoch 11   6.9% | batch:        53 of       772\t|\tloss: 0.935674\n",
      "Training Epoch 11   7.0% | batch:        54 of       772\t|\tloss: 0.65765\n",
      "Training Epoch 11   7.1% | batch:        55 of       772\t|\tloss: 0.600131\n",
      "Training Epoch 11   7.3% | batch:        56 of       772\t|\tloss: 0.584733\n",
      "Training Epoch 11   7.4% | batch:        57 of       772\t|\tloss: 0.869114\n",
      "Training Epoch 11   7.5% | batch:        58 of       772\t|\tloss: 1.00168\n",
      "Training Epoch 11   7.6% | batch:        59 of       772\t|\tloss: 0.940134\n",
      "Training Epoch 11   7.8% | batch:        60 of       772\t|\tloss: 0.798427\n",
      "Training Epoch 11   7.9% | batch:        61 of       772\t|\tloss: 0.556203\n",
      "Training Epoch 11   8.0% | batch:        62 of       772\t|\tloss: 0.418516\n",
      "Training Epoch 11   8.2% | batch:        63 of       772\t|\tloss: 0.947042\n",
      "Training Epoch 11   8.3% | batch:        64 of       772\t|\tloss: 0.717841\n",
      "Training Epoch 11   8.4% | batch:        65 of       772\t|\tloss: 0.933797\n",
      "Training Epoch 11   8.5% | batch:        66 of       772\t|\tloss: 0.592235\n",
      "Training Epoch 11   8.7% | batch:        67 of       772\t|\tloss: 0.641077\n",
      "Training Epoch 11   8.8% | batch:        68 of       772\t|\tloss: 0.534342\n",
      "Training Epoch 11   8.9% | batch:        69 of       772\t|\tloss: 0.467422\n",
      "Training Epoch 11   9.1% | batch:        70 of       772\t|\tloss: 0.592393\n",
      "Training Epoch 11   9.2% | batch:        71 of       772\t|\tloss: 0.86567\n",
      "Training Epoch 11   9.3% | batch:        72 of       772\t|\tloss: 0.449366\n",
      "Training Epoch 11   9.5% | batch:        73 of       772\t|\tloss: 0.710586\n",
      "Training Epoch 11   9.6% | batch:        74 of       772\t|\tloss: 0.507496\n",
      "Training Epoch 11   9.7% | batch:        75 of       772\t|\tloss: 0.62949\n",
      "Training Epoch 11   9.8% | batch:        76 of       772\t|\tloss: 0.554732\n",
      "Training Epoch 11  10.0% | batch:        77 of       772\t|\tloss: 0.661961\n",
      "Training Epoch 11  10.1% | batch:        78 of       772\t|\tloss: 0.622612\n",
      "Training Epoch 11  10.2% | batch:        79 of       772\t|\tloss: 1.09804\n",
      "Training Epoch 11  10.4% | batch:        80 of       772\t|\tloss: 0.536862\n",
      "Training Epoch 11  10.5% | batch:        81 of       772\t|\tloss: 0.759479\n",
      "Training Epoch 11  10.6% | batch:        82 of       772\t|\tloss: 0.797303\n",
      "Training Epoch 11  10.8% | batch:        83 of       772\t|\tloss: 0.815576\n",
      "Training Epoch 11  10.9% | batch:        84 of       772\t|\tloss: 0.755616\n",
      "Training Epoch 11  11.0% | batch:        85 of       772\t|\tloss: 0.416364\n",
      "Training Epoch 11  11.1% | batch:        86 of       772\t|\tloss: 0.807662\n",
      "Training Epoch 11  11.3% | batch:        87 of       772\t|\tloss: 0.857866\n",
      "Training Epoch 11  11.4% | batch:        88 of       772\t|\tloss: 0.497656\n",
      "Training Epoch 11  11.5% | batch:        89 of       772\t|\tloss: 0.499921\n",
      "Training Epoch 11  11.7% | batch:        90 of       772\t|\tloss: 0.625548\n",
      "Training Epoch 11  11.8% | batch:        91 of       772\t|\tloss: 0.573261\n",
      "Training Epoch 11  11.9% | batch:        92 of       772\t|\tloss: 0.459485\n",
      "Training Epoch 11  12.0% | batch:        93 of       772\t|\tloss: 0.406091\n",
      "Training Epoch 11  12.2% | batch:        94 of       772\t|\tloss: 0.487454\n",
      "Training Epoch 11  12.3% | batch:        95 of       772\t|\tloss: 0.405781\n",
      "Training Epoch 11  12.4% | batch:        96 of       772\t|\tloss: 0.673592\n",
      "Training Epoch 11  12.6% | batch:        97 of       772\t|\tloss: 0.440603\n",
      "Training Epoch 11  12.7% | batch:        98 of       772\t|\tloss: 0.474818\n",
      "Training Epoch 11  12.8% | batch:        99 of       772\t|\tloss: 0.55605\n",
      "Training Epoch 11  13.0% | batch:       100 of       772\t|\tloss: 0.39744\n",
      "Training Epoch 11  13.1% | batch:       101 of       772\t|\tloss: 0.696648\n",
      "Training Epoch 11  13.2% | batch:       102 of       772\t|\tloss: 1.00074\n",
      "Training Epoch 11  13.3% | batch:       103 of       772\t|\tloss: 0.44792\n",
      "Training Epoch 11  13.5% | batch:       104 of       772\t|\tloss: 0.667804\n",
      "Training Epoch 11  13.6% | batch:       105 of       772\t|\tloss: 0.686897\n",
      "Training Epoch 11  13.7% | batch:       106 of       772\t|\tloss: 0.699967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  13.9% | batch:       107 of       772\t|\tloss: 0.555568\n",
      "Training Epoch 11  14.0% | batch:       108 of       772\t|\tloss: 0.462967\n",
      "Training Epoch 11  14.1% | batch:       109 of       772\t|\tloss: 0.39711\n",
      "Training Epoch 11  14.2% | batch:       110 of       772\t|\tloss: 0.800192\n",
      "Training Epoch 11  14.4% | batch:       111 of       772\t|\tloss: 0.71125\n",
      "Training Epoch 11  14.5% | batch:       112 of       772\t|\tloss: 0.534165\n",
      "Training Epoch 11  14.6% | batch:       113 of       772\t|\tloss: 0.599643\n",
      "Training Epoch 11  14.8% | batch:       114 of       772\t|\tloss: 0.728596\n",
      "Training Epoch 11  14.9% | batch:       115 of       772\t|\tloss: 0.538527\n",
      "Training Epoch 11  15.0% | batch:       116 of       772\t|\tloss: 0.539981\n",
      "Training Epoch 11  15.2% | batch:       117 of       772\t|\tloss: 0.575209\n",
      "Training Epoch 11  15.3% | batch:       118 of       772\t|\tloss: 0.705116\n",
      "Training Epoch 11  15.4% | batch:       119 of       772\t|\tloss: 0.525846\n",
      "Training Epoch 11  15.5% | batch:       120 of       772\t|\tloss: 0.482678\n",
      "Training Epoch 11  15.7% | batch:       121 of       772\t|\tloss: 0.707689\n",
      "Training Epoch 11  15.8% | batch:       122 of       772\t|\tloss: 0.867295\n",
      "Training Epoch 11  15.9% | batch:       123 of       772\t|\tloss: 0.592258\n",
      "Training Epoch 11  16.1% | batch:       124 of       772\t|\tloss: 0.939425\n",
      "Training Epoch 11  16.2% | batch:       125 of       772\t|\tloss: 0.647819\n",
      "Training Epoch 11  16.3% | batch:       126 of       772\t|\tloss: 0.527063\n",
      "Training Epoch 11  16.5% | batch:       127 of       772\t|\tloss: 0.447157\n",
      "Training Epoch 11  16.6% | batch:       128 of       772\t|\tloss: 0.642575\n",
      "Training Epoch 11  16.7% | batch:       129 of       772\t|\tloss: 0.659689\n",
      "Training Epoch 11  16.8% | batch:       130 of       772\t|\tloss: 0.549287\n",
      "Training Epoch 11  17.0% | batch:       131 of       772\t|\tloss: 0.577344\n",
      "Training Epoch 11  17.1% | batch:       132 of       772\t|\tloss: 0.476578\n",
      "Training Epoch 11  17.2% | batch:       133 of       772\t|\tloss: 0.828685\n",
      "Training Epoch 11  17.4% | batch:       134 of       772\t|\tloss: 0.824387\n",
      "Training Epoch 11  17.5% | batch:       135 of       772\t|\tloss: 0.472703\n",
      "Training Epoch 11  17.6% | batch:       136 of       772\t|\tloss: 0.666794\n",
      "Training Epoch 11  17.7% | batch:       137 of       772\t|\tloss: 0.730012\n",
      "Training Epoch 11  17.9% | batch:       138 of       772\t|\tloss: 0.706992\n",
      "Training Epoch 11  18.0% | batch:       139 of       772\t|\tloss: 0.763081\n",
      "Training Epoch 11  18.1% | batch:       140 of       772\t|\tloss: 0.770224\n",
      "Training Epoch 11  18.3% | batch:       141 of       772\t|\tloss: 0.889506\n",
      "Training Epoch 11  18.4% | batch:       142 of       772\t|\tloss: 0.690471\n",
      "Training Epoch 11  18.5% | batch:       143 of       772\t|\tloss: 0.715101\n",
      "Training Epoch 11  18.7% | batch:       144 of       772\t|\tloss: 0.595209\n",
      "Training Epoch 11  18.8% | batch:       145 of       772\t|\tloss: 0.957614\n",
      "Training Epoch 11  18.9% | batch:       146 of       772\t|\tloss: 0.795363\n",
      "Training Epoch 11  19.0% | batch:       147 of       772\t|\tloss: 1.02671\n",
      "Training Epoch 11  19.2% | batch:       148 of       772\t|\tloss: 0.947259\n",
      "Training Epoch 11  19.3% | batch:       149 of       772\t|\tloss: 0.965243\n",
      "Training Epoch 11  19.4% | batch:       150 of       772\t|\tloss: 0.677167\n",
      "Training Epoch 11  19.6% | batch:       151 of       772\t|\tloss: 0.57644\n",
      "Training Epoch 11  19.7% | batch:       152 of       772\t|\tloss: 0.779521\n",
      "Training Epoch 11  19.8% | batch:       153 of       772\t|\tloss: 0.672933\n",
      "Training Epoch 11  19.9% | batch:       154 of       772\t|\tloss: 1.0259\n",
      "Training Epoch 11  20.1% | batch:       155 of       772\t|\tloss: 0.691791\n",
      "Training Epoch 11  20.2% | batch:       156 of       772\t|\tloss: 1.03922\n",
      "Training Epoch 11  20.3% | batch:       157 of       772\t|\tloss: 1.02872\n",
      "Training Epoch 11  20.5% | batch:       158 of       772\t|\tloss: 0.858526\n",
      "Training Epoch 11  20.6% | batch:       159 of       772\t|\tloss: 0.690505\n",
      "Training Epoch 11  20.7% | batch:       160 of       772\t|\tloss: 0.759608\n",
      "Training Epoch 11  20.9% | batch:       161 of       772\t|\tloss: 0.388839\n",
      "Training Epoch 11  21.0% | batch:       162 of       772\t|\tloss: 1.0899\n",
      "Training Epoch 11  21.1% | batch:       163 of       772\t|\tloss: 0.500789\n",
      "Training Epoch 11  21.2% | batch:       164 of       772\t|\tloss: 0.645921\n",
      "Training Epoch 11  21.4% | batch:       165 of       772\t|\tloss: 0.677747\n",
      "Training Epoch 11  21.5% | batch:       166 of       772\t|\tloss: 0.631097\n",
      "Training Epoch 11  21.6% | batch:       167 of       772\t|\tloss: 0.448108\n",
      "Training Epoch 11  21.8% | batch:       168 of       772\t|\tloss: 0.695117\n",
      "Training Epoch 11  21.9% | batch:       169 of       772\t|\tloss: 0.368875\n",
      "Training Epoch 11  22.0% | batch:       170 of       772\t|\tloss: 0.425524\n",
      "Training Epoch 11  22.2% | batch:       171 of       772\t|\tloss: 0.718727\n",
      "Training Epoch 11  22.3% | batch:       172 of       772\t|\tloss: 0.676328\n",
      "Training Epoch 11  22.4% | batch:       173 of       772\t|\tloss: 0.695081\n",
      "Training Epoch 11  22.5% | batch:       174 of       772\t|\tloss: 0.666185\n",
      "Training Epoch 11  22.7% | batch:       175 of       772\t|\tloss: 0.591389\n",
      "Training Epoch 11  22.8% | batch:       176 of       772\t|\tloss: 0.495535\n",
      "Training Epoch 11  22.9% | batch:       177 of       772\t|\tloss: 0.831655\n",
      "Training Epoch 11  23.1% | batch:       178 of       772\t|\tloss: 0.745416\n",
      "Training Epoch 11  23.2% | batch:       179 of       772\t|\tloss: 0.640407\n",
      "Training Epoch 11  23.3% | batch:       180 of       772\t|\tloss: 0.759218\n",
      "Training Epoch 11  23.4% | batch:       181 of       772\t|\tloss: 0.831369\n",
      "Training Epoch 11  23.6% | batch:       182 of       772\t|\tloss: 0.930152\n",
      "Training Epoch 11  23.7% | batch:       183 of       772\t|\tloss: 0.489491\n",
      "Training Epoch 11  23.8% | batch:       184 of       772\t|\tloss: 0.596814\n",
      "Training Epoch 11  24.0% | batch:       185 of       772\t|\tloss: 1.11258\n",
      "Training Epoch 11  24.1% | batch:       186 of       772\t|\tloss: 0.944091\n",
      "Training Epoch 11  24.2% | batch:       187 of       772\t|\tloss: 0.763667\n",
      "Training Epoch 11  24.4% | batch:       188 of       772\t|\tloss: 0.932329\n",
      "Training Epoch 11  24.5% | batch:       189 of       772\t|\tloss: 0.799218\n",
      "Training Epoch 11  24.6% | batch:       190 of       772\t|\tloss: 0.963224\n",
      "Training Epoch 11  24.7% | batch:       191 of       772\t|\tloss: 1.34692\n",
      "Training Epoch 11  24.9% | batch:       192 of       772\t|\tloss: 0.778306\n",
      "Training Epoch 11  25.0% | batch:       193 of       772\t|\tloss: 0.530245\n",
      "Training Epoch 11  25.1% | batch:       194 of       772\t|\tloss: 0.539444\n",
      "Training Epoch 11  25.3% | batch:       195 of       772\t|\tloss: 0.572152\n",
      "Training Epoch 11  25.4% | batch:       196 of       772\t|\tloss: 0.530384\n",
      "Training Epoch 11  25.5% | batch:       197 of       772\t|\tloss: 0.421878\n",
      "Training Epoch 11  25.6% | batch:       198 of       772\t|\tloss: 0.451592\n",
      "Training Epoch 11  25.8% | batch:       199 of       772\t|\tloss: 0.680669\n",
      "Training Epoch 11  25.9% | batch:       200 of       772\t|\tloss: 0.546749\n",
      "Training Epoch 11  26.0% | batch:       201 of       772\t|\tloss: 0.713979\n",
      "Training Epoch 11  26.2% | batch:       202 of       772\t|\tloss: 0.646272\n",
      "Training Epoch 11  26.3% | batch:       203 of       772\t|\tloss: 0.539873\n",
      "Training Epoch 11  26.4% | batch:       204 of       772\t|\tloss: 0.563852\n",
      "Training Epoch 11  26.6% | batch:       205 of       772\t|\tloss: 0.459337\n",
      "Training Epoch 11  26.7% | batch:       206 of       772\t|\tloss: 0.734509\n",
      "Training Epoch 11  26.8% | batch:       207 of       772\t|\tloss: 0.524554\n",
      "Training Epoch 11  26.9% | batch:       208 of       772\t|\tloss: 0.356697\n",
      "Training Epoch 11  27.1% | batch:       209 of       772\t|\tloss: 1.46003\n",
      "Training Epoch 11  27.2% | batch:       210 of       772\t|\tloss: 1.24009\n",
      "Training Epoch 11  27.3% | batch:       211 of       772\t|\tloss: 1.42141\n",
      "Training Epoch 11  27.5% | batch:       212 of       772\t|\tloss: 0.959514\n",
      "Training Epoch 11  27.6% | batch:       213 of       772\t|\tloss: 0.824569\n",
      "Training Epoch 11  27.7% | batch:       214 of       772\t|\tloss: 0.865254\n",
      "Training Epoch 11  27.8% | batch:       215 of       772\t|\tloss: 1.65911\n",
      "Training Epoch 11  28.0% | batch:       216 of       772\t|\tloss: 1.33939\n",
      "Training Epoch 11  28.1% | batch:       217 of       772\t|\tloss: 1.18913\n",
      "Training Epoch 11  28.2% | batch:       218 of       772\t|\tloss: 0.837886\n",
      "Training Epoch 11  28.4% | batch:       219 of       772\t|\tloss: 0.637597\n",
      "Training Epoch 11  28.5% | batch:       220 of       772\t|\tloss: 1.07298\n",
      "Training Epoch 11  28.6% | batch:       221 of       772\t|\tloss: 0.746876\n",
      "Training Epoch 11  28.8% | batch:       222 of       772\t|\tloss: 1.22137\n",
      "Training Epoch 11  28.9% | batch:       223 of       772\t|\tloss: 0.795642\n",
      "Training Epoch 11  29.0% | batch:       224 of       772\t|\tloss: 0.604238\n",
      "Training Epoch 11  29.1% | batch:       225 of       772\t|\tloss: 0.697661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  29.3% | batch:       226 of       772\t|\tloss: 0.991799\n",
      "Training Epoch 11  29.4% | batch:       227 of       772\t|\tloss: 0.881003\n",
      "Training Epoch 11  29.5% | batch:       228 of       772\t|\tloss: 0.894997\n",
      "Training Epoch 11  29.7% | batch:       229 of       772\t|\tloss: 0.580078\n",
      "Training Epoch 11  29.8% | batch:       230 of       772\t|\tloss: 0.528328\n",
      "Training Epoch 11  29.9% | batch:       231 of       772\t|\tloss: 0.388659\n",
      "Training Epoch 11  30.1% | batch:       232 of       772\t|\tloss: 0.439581\n",
      "Training Epoch 11  30.2% | batch:       233 of       772\t|\tloss: 0.561199\n",
      "Training Epoch 11  30.3% | batch:       234 of       772\t|\tloss: 0.449066\n",
      "Training Epoch 11  30.4% | batch:       235 of       772\t|\tloss: 0.570915\n",
      "Training Epoch 11  30.6% | batch:       236 of       772\t|\tloss: 0.451046\n",
      "Training Epoch 11  30.7% | batch:       237 of       772\t|\tloss: 0.579937\n",
      "Training Epoch 11  30.8% | batch:       238 of       772\t|\tloss: 0.437582\n",
      "Training Epoch 11  31.0% | batch:       239 of       772\t|\tloss: 0.599063\n",
      "Training Epoch 11  31.1% | batch:       240 of       772\t|\tloss: 0.387402\n",
      "Training Epoch 11  31.2% | batch:       241 of       772\t|\tloss: 0.435509\n",
      "Training Epoch 11  31.3% | batch:       242 of       772\t|\tloss: 0.473788\n",
      "Training Epoch 11  31.5% | batch:       243 of       772\t|\tloss: 0.700967\n",
      "Training Epoch 11  31.6% | batch:       244 of       772\t|\tloss: 0.494435\n",
      "Training Epoch 11  31.7% | batch:       245 of       772\t|\tloss: 0.404573\n",
      "Training Epoch 11  31.9% | batch:       246 of       772\t|\tloss: 0.766346\n",
      "Training Epoch 11  32.0% | batch:       247 of       772\t|\tloss: 0.351419\n",
      "Training Epoch 11  32.1% | batch:       248 of       772\t|\tloss: 0.581263\n",
      "Training Epoch 11  32.3% | batch:       249 of       772\t|\tloss: 0.901137\n",
      "Training Epoch 11  32.4% | batch:       250 of       772\t|\tloss: 0.582867\n",
      "Training Epoch 11  32.5% | batch:       251 of       772\t|\tloss: 0.561788\n",
      "Training Epoch 11  32.6% | batch:       252 of       772\t|\tloss: 0.444708\n",
      "Training Epoch 11  32.8% | batch:       253 of       772\t|\tloss: 0.533445\n",
      "Training Epoch 11  32.9% | batch:       254 of       772\t|\tloss: 0.436763\n",
      "Training Epoch 11  33.0% | batch:       255 of       772\t|\tloss: 0.501025\n",
      "Training Epoch 11  33.2% | batch:       256 of       772\t|\tloss: 0.36938\n",
      "Training Epoch 11  33.3% | batch:       257 of       772\t|\tloss: 0.510827\n",
      "Training Epoch 11  33.4% | batch:       258 of       772\t|\tloss: 0.450929\n",
      "Training Epoch 11  33.5% | batch:       259 of       772\t|\tloss: 0.482761\n",
      "Training Epoch 11  33.7% | batch:       260 of       772\t|\tloss: 0.486798\n",
      "Training Epoch 11  33.8% | batch:       261 of       772\t|\tloss: 0.489592\n",
      "Training Epoch 11  33.9% | batch:       262 of       772\t|\tloss: 0.591583\n",
      "Training Epoch 11  34.1% | batch:       263 of       772\t|\tloss: 0.640378\n",
      "Training Epoch 11  34.2% | batch:       264 of       772\t|\tloss: 0.775367\n",
      "Training Epoch 11  34.3% | batch:       265 of       772\t|\tloss: 0.477149\n",
      "Training Epoch 11  34.5% | batch:       266 of       772\t|\tloss: 0.632172\n",
      "Training Epoch 11  34.6% | batch:       267 of       772\t|\tloss: 0.508987\n",
      "Training Epoch 11  34.7% | batch:       268 of       772\t|\tloss: 0.591613\n",
      "Training Epoch 11  34.8% | batch:       269 of       772\t|\tloss: 0.621124\n",
      "Training Epoch 11  35.0% | batch:       270 of       772\t|\tloss: 0.742598\n",
      "Training Epoch 11  35.1% | batch:       271 of       772\t|\tloss: 0.48279\n",
      "Training Epoch 11  35.2% | batch:       272 of       772\t|\tloss: 0.658715\n",
      "Training Epoch 11  35.4% | batch:       273 of       772\t|\tloss: 0.829065\n",
      "Training Epoch 11  35.5% | batch:       274 of       772\t|\tloss: 0.522409\n",
      "Training Epoch 11  35.6% | batch:       275 of       772\t|\tloss: 0.495646\n",
      "Training Epoch 11  35.8% | batch:       276 of       772\t|\tloss: 0.543054\n",
      "Training Epoch 11  35.9% | batch:       277 of       772\t|\tloss: 0.512557\n",
      "Training Epoch 11  36.0% | batch:       278 of       772\t|\tloss: 0.518494\n",
      "Training Epoch 11  36.1% | batch:       279 of       772\t|\tloss: 0.536102\n",
      "Training Epoch 11  36.3% | batch:       280 of       772\t|\tloss: 0.606803\n",
      "Training Epoch 11  36.4% | batch:       281 of       772\t|\tloss: 0.54119\n",
      "Training Epoch 11  36.5% | batch:       282 of       772\t|\tloss: 0.52416\n",
      "Training Epoch 11  36.7% | batch:       283 of       772\t|\tloss: 0.660194\n",
      "Training Epoch 11  36.8% | batch:       284 of       772\t|\tloss: 0.593829\n",
      "Training Epoch 11  36.9% | batch:       285 of       772\t|\tloss: 0.440352\n",
      "Training Epoch 11  37.0% | batch:       286 of       772\t|\tloss: 0.543132\n",
      "Training Epoch 11  37.2% | batch:       287 of       772\t|\tloss: 0.711512\n",
      "Training Epoch 11  37.3% | batch:       288 of       772\t|\tloss: 0.451885\n",
      "Training Epoch 11  37.4% | batch:       289 of       772\t|\tloss: 0.595101\n",
      "Training Epoch 11  37.6% | batch:       290 of       772\t|\tloss: 0.620527\n",
      "Training Epoch 11  37.7% | batch:       291 of       772\t|\tloss: 0.580388\n",
      "Training Epoch 11  37.8% | batch:       292 of       772\t|\tloss: 0.553654\n",
      "Training Epoch 11  38.0% | batch:       293 of       772\t|\tloss: 0.400901\n",
      "Training Epoch 11  38.1% | batch:       294 of       772\t|\tloss: 0.606379\n",
      "Training Epoch 11  38.2% | batch:       295 of       772\t|\tloss: 0.732088\n",
      "Training Epoch 11  38.3% | batch:       296 of       772\t|\tloss: 0.718772\n",
      "Training Epoch 11  38.5% | batch:       297 of       772\t|\tloss: 0.582099\n",
      "Training Epoch 11  38.6% | batch:       298 of       772\t|\tloss: 0.517381\n",
      "Training Epoch 11  38.7% | batch:       299 of       772\t|\tloss: 0.484544\n",
      "Training Epoch 11  38.9% | batch:       300 of       772\t|\tloss: 0.510349\n",
      "Training Epoch 11  39.0% | batch:       301 of       772\t|\tloss: 0.741685\n",
      "Training Epoch 11  39.1% | batch:       302 of       772\t|\tloss: 0.359528\n",
      "Training Epoch 11  39.2% | batch:       303 of       772\t|\tloss: 0.683634\n",
      "Training Epoch 11  39.4% | batch:       304 of       772\t|\tloss: 0.975875\n",
      "Training Epoch 11  39.5% | batch:       305 of       772\t|\tloss: 0.969717\n",
      "Training Epoch 11  39.6% | batch:       306 of       772\t|\tloss: 0.7783\n",
      "Training Epoch 11  39.8% | batch:       307 of       772\t|\tloss: 0.787542\n",
      "Training Epoch 11  39.9% | batch:       308 of       772\t|\tloss: 0.444219\n",
      "Training Epoch 11  40.0% | batch:       309 of       772\t|\tloss: 1.27443\n",
      "Training Epoch 11  40.2% | batch:       310 of       772\t|\tloss: 1.63807\n",
      "Training Epoch 11  40.3% | batch:       311 of       772\t|\tloss: 0.969087\n",
      "Training Epoch 11  40.4% | batch:       312 of       772\t|\tloss: 1.87272\n",
      "Training Epoch 11  40.5% | batch:       313 of       772\t|\tloss: 1.02028\n",
      "Training Epoch 11  40.7% | batch:       314 of       772\t|\tloss: 0.719792\n",
      "Training Epoch 11  40.8% | batch:       315 of       772\t|\tloss: 0.806979\n",
      "Training Epoch 11  40.9% | batch:       316 of       772\t|\tloss: 1.0546\n",
      "Training Epoch 11  41.1% | batch:       317 of       772\t|\tloss: 1.38317\n",
      "Training Epoch 11  41.2% | batch:       318 of       772\t|\tloss: 1.30217\n",
      "Training Epoch 11  41.3% | batch:       319 of       772\t|\tloss: 0.922953\n",
      "Training Epoch 11  41.5% | batch:       320 of       772\t|\tloss: 0.656004\n",
      "Training Epoch 11  41.6% | batch:       321 of       772\t|\tloss: 0.708923\n",
      "Training Epoch 11  41.7% | batch:       322 of       772\t|\tloss: 1.07958\n",
      "Training Epoch 11  41.8% | batch:       323 of       772\t|\tloss: 1.3437\n",
      "Training Epoch 11  42.0% | batch:       324 of       772\t|\tloss: 0.886601\n",
      "Training Epoch 11  42.1% | batch:       325 of       772\t|\tloss: 0.546741\n",
      "Training Epoch 11  42.2% | batch:       326 of       772\t|\tloss: 0.611805\n",
      "Training Epoch 11  42.4% | batch:       327 of       772\t|\tloss: 0.882681\n",
      "Training Epoch 11  42.5% | batch:       328 of       772\t|\tloss: 0.968275\n",
      "Training Epoch 11  42.6% | batch:       329 of       772\t|\tloss: 0.852965\n",
      "Training Epoch 11  42.7% | batch:       330 of       772\t|\tloss: 0.593305\n",
      "Training Epoch 11  42.9% | batch:       331 of       772\t|\tloss: 0.583036\n",
      "Training Epoch 11  43.0% | batch:       332 of       772\t|\tloss: 0.597373\n",
      "Training Epoch 11  43.1% | batch:       333 of       772\t|\tloss: 0.644342\n",
      "Training Epoch 11  43.3% | batch:       334 of       772\t|\tloss: 0.415102\n",
      "Training Epoch 11  43.4% | batch:       335 of       772\t|\tloss: 0.677236\n",
      "Training Epoch 11  43.5% | batch:       336 of       772\t|\tloss: 0.640741\n",
      "Training Epoch 11  43.7% | batch:       337 of       772\t|\tloss: 0.516289\n",
      "Training Epoch 11  43.8% | batch:       338 of       772\t|\tloss: 0.551856\n",
      "Training Epoch 11  43.9% | batch:       339 of       772\t|\tloss: 0.400438\n",
      "Training Epoch 11  44.0% | batch:       340 of       772\t|\tloss: 0.430376\n",
      "Training Epoch 11  44.2% | batch:       341 of       772\t|\tloss: 0.688684\n",
      "Training Epoch 11  44.3% | batch:       342 of       772\t|\tloss: 0.967835\n",
      "Training Epoch 11  44.4% | batch:       343 of       772\t|\tloss: 0.852571\n",
      "Training Epoch 11  44.6% | batch:       344 of       772\t|\tloss: 0.509889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  44.7% | batch:       345 of       772\t|\tloss: 0.361321\n",
      "Training Epoch 11  44.8% | batch:       346 of       772\t|\tloss: 0.617158\n",
      "Training Epoch 11  44.9% | batch:       347 of       772\t|\tloss: 0.367767\n",
      "Training Epoch 11  45.1% | batch:       348 of       772\t|\tloss: 0.504442\n",
      "Training Epoch 11  45.2% | batch:       349 of       772\t|\tloss: 0.947199\n",
      "Training Epoch 11  45.3% | batch:       350 of       772\t|\tloss: 0.592749\n",
      "Training Epoch 11  45.5% | batch:       351 of       772\t|\tloss: 1.10432\n",
      "Training Epoch 11  45.6% | batch:       352 of       772\t|\tloss: 0.662351\n",
      "Training Epoch 11  45.7% | batch:       353 of       772\t|\tloss: 0.763782\n",
      "Training Epoch 11  45.9% | batch:       354 of       772\t|\tloss: 0.479362\n",
      "Training Epoch 11  46.0% | batch:       355 of       772\t|\tloss: 0.551167\n",
      "Training Epoch 11  46.1% | batch:       356 of       772\t|\tloss: 0.513651\n",
      "Training Epoch 11  46.2% | batch:       357 of       772\t|\tloss: 0.540124\n",
      "Training Epoch 11  46.4% | batch:       358 of       772\t|\tloss: 0.557599\n",
      "Training Epoch 11  46.5% | batch:       359 of       772\t|\tloss: 0.726664\n",
      "Training Epoch 11  46.6% | batch:       360 of       772\t|\tloss: 0.398783\n",
      "Training Epoch 11  46.8% | batch:       361 of       772\t|\tloss: 0.417901\n",
      "Training Epoch 11  46.9% | batch:       362 of       772\t|\tloss: 0.383076\n",
      "Training Epoch 11  47.0% | batch:       363 of       772\t|\tloss: 0.555384\n",
      "Training Epoch 11  47.2% | batch:       364 of       772\t|\tloss: 0.429579\n",
      "Training Epoch 11  47.3% | batch:       365 of       772\t|\tloss: 0.632831\n",
      "Training Epoch 11  47.4% | batch:       366 of       772\t|\tloss: 0.477795\n",
      "Training Epoch 11  47.5% | batch:       367 of       772\t|\tloss: 0.545223\n",
      "Training Epoch 11  47.7% | batch:       368 of       772\t|\tloss: 0.593339\n",
      "Training Epoch 11  47.8% | batch:       369 of       772\t|\tloss: 0.486273\n",
      "Training Epoch 11  47.9% | batch:       370 of       772\t|\tloss: 0.490421\n",
      "Training Epoch 11  48.1% | batch:       371 of       772\t|\tloss: 0.76751\n",
      "Training Epoch 11  48.2% | batch:       372 of       772\t|\tloss: 0.639975\n",
      "Training Epoch 11  48.3% | batch:       373 of       772\t|\tloss: 0.468627\n",
      "Training Epoch 11  48.4% | batch:       374 of       772\t|\tloss: 0.555044\n",
      "Training Epoch 11  48.6% | batch:       375 of       772\t|\tloss: 0.788181\n",
      "Training Epoch 11  48.7% | batch:       376 of       772\t|\tloss: 0.478718\n",
      "Training Epoch 11  48.8% | batch:       377 of       772\t|\tloss: 0.529241\n",
      "Training Epoch 11  49.0% | batch:       378 of       772\t|\tloss: 0.49918\n",
      "Training Epoch 11  49.1% | batch:       379 of       772\t|\tloss: 0.620623\n",
      "Training Epoch 11  49.2% | batch:       380 of       772\t|\tloss: 0.44692\n",
      "Training Epoch 11  49.4% | batch:       381 of       772\t|\tloss: 0.491675\n",
      "Training Epoch 11  49.5% | batch:       382 of       772\t|\tloss: 0.657844\n",
      "Training Epoch 11  49.6% | batch:       383 of       772\t|\tloss: 0.668337\n",
      "Training Epoch 11  49.7% | batch:       384 of       772\t|\tloss: 0.449252\n",
      "Training Epoch 11  49.9% | batch:       385 of       772\t|\tloss: 0.601905\n",
      "Training Epoch 11  50.0% | batch:       386 of       772\t|\tloss: 0.316599\n",
      "Training Epoch 11  50.1% | batch:       387 of       772\t|\tloss: 0.403797\n",
      "Training Epoch 11  50.3% | batch:       388 of       772\t|\tloss: 0.453602\n",
      "Training Epoch 11  50.4% | batch:       389 of       772\t|\tloss: 0.483097\n",
      "Training Epoch 11  50.5% | batch:       390 of       772\t|\tloss: 0.713186\n",
      "Training Epoch 11  50.6% | batch:       391 of       772\t|\tloss: 0.427191\n",
      "Training Epoch 11  50.8% | batch:       392 of       772\t|\tloss: 0.448977\n",
      "Training Epoch 11  50.9% | batch:       393 of       772\t|\tloss: 0.37294\n",
      "Training Epoch 11  51.0% | batch:       394 of       772\t|\tloss: 0.506432\n",
      "Training Epoch 11  51.2% | batch:       395 of       772\t|\tloss: 0.509425\n",
      "Training Epoch 11  51.3% | batch:       396 of       772\t|\tloss: 0.58177\n",
      "Training Epoch 11  51.4% | batch:       397 of       772\t|\tloss: 0.480834\n",
      "Training Epoch 11  51.6% | batch:       398 of       772\t|\tloss: 0.407699\n",
      "Training Epoch 11  51.7% | batch:       399 of       772\t|\tloss: 0.673898\n",
      "Training Epoch 11  51.8% | batch:       400 of       772\t|\tloss: 0.50115\n",
      "Training Epoch 11  51.9% | batch:       401 of       772\t|\tloss: 0.471492\n",
      "Training Epoch 11  52.1% | batch:       402 of       772\t|\tloss: 0.529107\n",
      "Training Epoch 11  52.2% | batch:       403 of       772\t|\tloss: 0.477516\n",
      "Training Epoch 11  52.3% | batch:       404 of       772\t|\tloss: 0.393075\n",
      "Training Epoch 11  52.5% | batch:       405 of       772\t|\tloss: 0.662648\n",
      "Training Epoch 11  52.6% | batch:       406 of       772\t|\tloss: 0.67412\n",
      "Training Epoch 11  52.7% | batch:       407 of       772\t|\tloss: 0.867724\n",
      "Training Epoch 11  52.8% | batch:       408 of       772\t|\tloss: 0.600108\n",
      "Training Epoch 11  53.0% | batch:       409 of       772\t|\tloss: 0.638758\n",
      "Training Epoch 11  53.1% | batch:       410 of       772\t|\tloss: 0.607517\n",
      "Training Epoch 11  53.2% | batch:       411 of       772\t|\tloss: 0.731119\n",
      "Training Epoch 11  53.4% | batch:       412 of       772\t|\tloss: 0.52246\n",
      "Training Epoch 11  53.5% | batch:       413 of       772\t|\tloss: 0.518594\n",
      "Training Epoch 11  53.6% | batch:       414 of       772\t|\tloss: 0.500812\n",
      "Training Epoch 11  53.8% | batch:       415 of       772\t|\tloss: 1.00547\n",
      "Training Epoch 11  53.9% | batch:       416 of       772\t|\tloss: 0.900823\n",
      "Training Epoch 11  54.0% | batch:       417 of       772\t|\tloss: 0.823063\n",
      "Training Epoch 11  54.1% | batch:       418 of       772\t|\tloss: 0.740615\n",
      "Training Epoch 11  54.3% | batch:       419 of       772\t|\tloss: 0.639066\n",
      "Training Epoch 11  54.4% | batch:       420 of       772\t|\tloss: 0.739552\n",
      "Training Epoch 11  54.5% | batch:       421 of       772\t|\tloss: 1.09047\n",
      "Training Epoch 11  54.7% | batch:       422 of       772\t|\tloss: 0.957659\n",
      "Training Epoch 11  54.8% | batch:       423 of       772\t|\tloss: 0.744035\n",
      "Training Epoch 11  54.9% | batch:       424 of       772\t|\tloss: 0.548554\n",
      "Training Epoch 11  55.1% | batch:       425 of       772\t|\tloss: 0.489435\n",
      "Training Epoch 11  55.2% | batch:       426 of       772\t|\tloss: 0.498753\n",
      "Training Epoch 11  55.3% | batch:       427 of       772\t|\tloss: 0.653212\n",
      "Training Epoch 11  55.4% | batch:       428 of       772\t|\tloss: 0.467675\n",
      "Training Epoch 11  55.6% | batch:       429 of       772\t|\tloss: 0.528048\n",
      "Training Epoch 11  55.7% | batch:       430 of       772\t|\tloss: 0.760894\n",
      "Training Epoch 11  55.8% | batch:       431 of       772\t|\tloss: 0.578747\n",
      "Training Epoch 11  56.0% | batch:       432 of       772\t|\tloss: 0.488639\n",
      "Training Epoch 11  56.1% | batch:       433 of       772\t|\tloss: 0.504079\n",
      "Training Epoch 11  56.2% | batch:       434 of       772\t|\tloss: 0.509825\n",
      "Training Epoch 11  56.3% | batch:       435 of       772\t|\tloss: 0.747544\n",
      "Training Epoch 11  56.5% | batch:       436 of       772\t|\tloss: 0.582476\n",
      "Training Epoch 11  56.6% | batch:       437 of       772\t|\tloss: 0.655159\n",
      "Training Epoch 11  56.7% | batch:       438 of       772\t|\tloss: 0.522608\n",
      "Training Epoch 11  56.9% | batch:       439 of       772\t|\tloss: 0.593409\n",
      "Training Epoch 11  57.0% | batch:       440 of       772\t|\tloss: 0.874182\n",
      "Training Epoch 11  57.1% | batch:       441 of       772\t|\tloss: 0.669786\n",
      "Training Epoch 11  57.3% | batch:       442 of       772\t|\tloss: 0.554759\n",
      "Training Epoch 11  57.4% | batch:       443 of       772\t|\tloss: 0.537484\n",
      "Training Epoch 11  57.5% | batch:       444 of       772\t|\tloss: 0.623161\n",
      "Training Epoch 11  57.6% | batch:       445 of       772\t|\tloss: 0.573985\n",
      "Training Epoch 11  57.8% | batch:       446 of       772\t|\tloss: 0.49494\n",
      "Training Epoch 11  57.9% | batch:       447 of       772\t|\tloss: 0.61689\n",
      "Training Epoch 11  58.0% | batch:       448 of       772\t|\tloss: 0.748864\n",
      "Training Epoch 11  58.2% | batch:       449 of       772\t|\tloss: 0.495051\n",
      "Training Epoch 11  58.3% | batch:       450 of       772\t|\tloss: 0.630022\n",
      "Training Epoch 11  58.4% | batch:       451 of       772\t|\tloss: 0.614049\n",
      "Training Epoch 11  58.5% | batch:       452 of       772\t|\tloss: 1.07467\n",
      "Training Epoch 11  58.7% | batch:       453 of       772\t|\tloss: 1.0822\n",
      "Training Epoch 11  58.8% | batch:       454 of       772\t|\tloss: 0.613021\n",
      "Training Epoch 11  58.9% | batch:       455 of       772\t|\tloss: 0.586177\n",
      "Training Epoch 11  59.1% | batch:       456 of       772\t|\tloss: 0.5106\n",
      "Training Epoch 11  59.2% | batch:       457 of       772\t|\tloss: 0.476035\n",
      "Training Epoch 11  59.3% | batch:       458 of       772\t|\tloss: 0.530376\n",
      "Training Epoch 11  59.5% | batch:       459 of       772\t|\tloss: 0.92104\n",
      "Training Epoch 11  59.6% | batch:       460 of       772\t|\tloss: 1.01794\n",
      "Training Epoch 11  59.7% | batch:       461 of       772\t|\tloss: 0.582356\n",
      "Training Epoch 11  59.8% | batch:       462 of       772\t|\tloss: 0.728361\n",
      "Training Epoch 11  60.0% | batch:       463 of       772\t|\tloss: 0.484984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  60.1% | batch:       464 of       772\t|\tloss: 0.894783\n",
      "Training Epoch 11  60.2% | batch:       465 of       772\t|\tloss: 0.409999\n",
      "Training Epoch 11  60.4% | batch:       466 of       772\t|\tloss: 0.641786\n",
      "Training Epoch 11  60.5% | batch:       467 of       772\t|\tloss: 0.463174\n",
      "Training Epoch 11  60.6% | batch:       468 of       772\t|\tloss: 0.65302\n",
      "Training Epoch 11  60.8% | batch:       469 of       772\t|\tloss: 0.695118\n",
      "Training Epoch 11  60.9% | batch:       470 of       772\t|\tloss: 0.680038\n",
      "Training Epoch 11  61.0% | batch:       471 of       772\t|\tloss: 0.579745\n",
      "Training Epoch 11  61.1% | batch:       472 of       772\t|\tloss: 0.430787\n",
      "Training Epoch 11  61.3% | batch:       473 of       772\t|\tloss: 0.604868\n",
      "Training Epoch 11  61.4% | batch:       474 of       772\t|\tloss: 0.55254\n",
      "Training Epoch 11  61.5% | batch:       475 of       772\t|\tloss: 0.749423\n",
      "Training Epoch 11  61.7% | batch:       476 of       772\t|\tloss: 1.0238\n",
      "Training Epoch 11  61.8% | batch:       477 of       772\t|\tloss: 0.694471\n",
      "Training Epoch 11  61.9% | batch:       478 of       772\t|\tloss: 0.626975\n",
      "Training Epoch 11  62.0% | batch:       479 of       772\t|\tloss: 0.575971\n",
      "Training Epoch 11  62.2% | batch:       480 of       772\t|\tloss: 0.486517\n",
      "Training Epoch 11  62.3% | batch:       481 of       772\t|\tloss: 0.510941\n",
      "Training Epoch 11  62.4% | batch:       482 of       772\t|\tloss: 0.500377\n",
      "Training Epoch 11  62.6% | batch:       483 of       772\t|\tloss: 0.686265\n",
      "Training Epoch 11  62.7% | batch:       484 of       772\t|\tloss: 0.862557\n",
      "Training Epoch 11  62.8% | batch:       485 of       772\t|\tloss: 0.59656\n",
      "Training Epoch 11  63.0% | batch:       486 of       772\t|\tloss: 0.627639\n",
      "Training Epoch 11  63.1% | batch:       487 of       772\t|\tloss: 0.385201\n",
      "Training Epoch 11  63.2% | batch:       488 of       772\t|\tloss: 0.610678\n",
      "Training Epoch 11  63.3% | batch:       489 of       772\t|\tloss: 0.578806\n",
      "Training Epoch 11  63.5% | batch:       490 of       772\t|\tloss: 0.47643\n",
      "Training Epoch 11  63.6% | batch:       491 of       772\t|\tloss: 0.672928\n",
      "Training Epoch 11  63.7% | batch:       492 of       772\t|\tloss: 0.677549\n",
      "Training Epoch 11  63.9% | batch:       493 of       772\t|\tloss: 0.633771\n",
      "Training Epoch 11  64.0% | batch:       494 of       772\t|\tloss: 0.647166\n",
      "Training Epoch 11  64.1% | batch:       495 of       772\t|\tloss: 0.5862\n",
      "Training Epoch 11  64.2% | batch:       496 of       772\t|\tloss: 0.646343\n",
      "Training Epoch 11  64.4% | batch:       497 of       772\t|\tloss: 0.48073\n",
      "Training Epoch 11  64.5% | batch:       498 of       772\t|\tloss: 0.604476\n",
      "Training Epoch 11  64.6% | batch:       499 of       772\t|\tloss: 0.458365\n",
      "Training Epoch 11  64.8% | batch:       500 of       772\t|\tloss: 0.720978\n",
      "Training Epoch 11  64.9% | batch:       501 of       772\t|\tloss: 0.729763\n",
      "Training Epoch 11  65.0% | batch:       502 of       772\t|\tloss: 0.639385\n",
      "Training Epoch 11  65.2% | batch:       503 of       772\t|\tloss: 0.550076\n",
      "Training Epoch 11  65.3% | batch:       504 of       772\t|\tloss: 0.507224\n",
      "Training Epoch 11  65.4% | batch:       505 of       772\t|\tloss: 0.468341\n",
      "Training Epoch 11  65.5% | batch:       506 of       772\t|\tloss: 0.356241\n",
      "Training Epoch 11  65.7% | batch:       507 of       772\t|\tloss: 0.853557\n",
      "Training Epoch 11  65.8% | batch:       508 of       772\t|\tloss: 0.787626\n",
      "Training Epoch 11  65.9% | batch:       509 of       772\t|\tloss: 0.477658\n",
      "Training Epoch 11  66.1% | batch:       510 of       772\t|\tloss: 0.638172\n",
      "Training Epoch 11  66.2% | batch:       511 of       772\t|\tloss: 0.784877\n",
      "Training Epoch 11  66.3% | batch:       512 of       772\t|\tloss: 0.575347\n",
      "Training Epoch 11  66.5% | batch:       513 of       772\t|\tloss: 0.490742\n",
      "Training Epoch 11  66.6% | batch:       514 of       772\t|\tloss: 0.623658\n",
      "Training Epoch 11  66.7% | batch:       515 of       772\t|\tloss: 0.608076\n",
      "Training Epoch 11  66.8% | batch:       516 of       772\t|\tloss: 0.859934\n",
      "Training Epoch 11  67.0% | batch:       517 of       772\t|\tloss: 0.519947\n",
      "Training Epoch 11  67.1% | batch:       518 of       772\t|\tloss: 0.521392\n",
      "Training Epoch 11  67.2% | batch:       519 of       772\t|\tloss: 0.576126\n",
      "Training Epoch 11  67.4% | batch:       520 of       772\t|\tloss: 0.589493\n",
      "Training Epoch 11  67.5% | batch:       521 of       772\t|\tloss: 0.5933\n",
      "Training Epoch 11  67.6% | batch:       522 of       772\t|\tloss: 0.596147\n",
      "Training Epoch 11  67.7% | batch:       523 of       772\t|\tloss: 0.481256\n",
      "Training Epoch 11  67.9% | batch:       524 of       772\t|\tloss: 0.517934\n",
      "Training Epoch 11  68.0% | batch:       525 of       772\t|\tloss: 0.571386\n",
      "Training Epoch 11  68.1% | batch:       526 of       772\t|\tloss: 0.356368\n",
      "Training Epoch 11  68.3% | batch:       527 of       772\t|\tloss: 0.778539\n",
      "Training Epoch 11  68.4% | batch:       528 of       772\t|\tloss: 0.546286\n",
      "Training Epoch 11  68.5% | batch:       529 of       772\t|\tloss: 0.510138\n",
      "Training Epoch 11  68.7% | batch:       530 of       772\t|\tloss: 0.585844\n",
      "Training Epoch 11  68.8% | batch:       531 of       772\t|\tloss: 0.48013\n",
      "Training Epoch 11  68.9% | batch:       532 of       772\t|\tloss: 0.459485\n",
      "Training Epoch 11  69.0% | batch:       533 of       772\t|\tloss: 0.657473\n",
      "Training Epoch 11  69.2% | batch:       534 of       772\t|\tloss: 0.630642\n",
      "Training Epoch 11  69.3% | batch:       535 of       772\t|\tloss: 0.592891\n",
      "Training Epoch 11  69.4% | batch:       536 of       772\t|\tloss: 0.541473\n",
      "Training Epoch 11  69.6% | batch:       537 of       772\t|\tloss: 0.704922\n",
      "Training Epoch 11  69.7% | batch:       538 of       772\t|\tloss: 0.640724\n",
      "Training Epoch 11  69.8% | batch:       539 of       772\t|\tloss: 0.793265\n",
      "Training Epoch 11  69.9% | batch:       540 of       772\t|\tloss: 0.430411\n",
      "Training Epoch 11  70.1% | batch:       541 of       772\t|\tloss: 0.623524\n",
      "Training Epoch 11  70.2% | batch:       542 of       772\t|\tloss: 0.560201\n",
      "Training Epoch 11  70.3% | batch:       543 of       772\t|\tloss: 0.43013\n",
      "Training Epoch 11  70.5% | batch:       544 of       772\t|\tloss: 0.622702\n",
      "Training Epoch 11  70.6% | batch:       545 of       772\t|\tloss: 0.400176\n",
      "Training Epoch 11  70.7% | batch:       546 of       772\t|\tloss: 0.775714\n",
      "Training Epoch 11  70.9% | batch:       547 of       772\t|\tloss: 0.594536\n",
      "Training Epoch 11  71.0% | batch:       548 of       772\t|\tloss: 0.628249\n",
      "Training Epoch 11  71.1% | batch:       549 of       772\t|\tloss: 0.7185\n",
      "Training Epoch 11  71.2% | batch:       550 of       772\t|\tloss: 0.812592\n",
      "Training Epoch 11  71.4% | batch:       551 of       772\t|\tloss: 0.927528\n",
      "Training Epoch 11  71.5% | batch:       552 of       772\t|\tloss: 1.08396\n",
      "Training Epoch 11  71.6% | batch:       553 of       772\t|\tloss: 0.674298\n",
      "Training Epoch 11  71.8% | batch:       554 of       772\t|\tloss: 0.567314\n",
      "Training Epoch 11  71.9% | batch:       555 of       772\t|\tloss: 0.445645\n",
      "Training Epoch 11  72.0% | batch:       556 of       772\t|\tloss: 0.772076\n",
      "Training Epoch 11  72.2% | batch:       557 of       772\t|\tloss: 0.611877\n",
      "Training Epoch 11  72.3% | batch:       558 of       772\t|\tloss: 0.47644\n",
      "Training Epoch 11  72.4% | batch:       559 of       772\t|\tloss: 0.449093\n",
      "Training Epoch 11  72.5% | batch:       560 of       772\t|\tloss: 0.655059\n",
      "Training Epoch 11  72.7% | batch:       561 of       772\t|\tloss: 0.4827\n",
      "Training Epoch 11  72.8% | batch:       562 of       772\t|\tloss: 0.407073\n",
      "Training Epoch 11  72.9% | batch:       563 of       772\t|\tloss: 0.460319\n",
      "Training Epoch 11  73.1% | batch:       564 of       772\t|\tloss: 0.532188\n",
      "Training Epoch 11  73.2% | batch:       565 of       772\t|\tloss: 0.516318\n",
      "Training Epoch 11  73.3% | batch:       566 of       772\t|\tloss: 0.49542\n",
      "Training Epoch 11  73.4% | batch:       567 of       772\t|\tloss: 0.494785\n",
      "Training Epoch 11  73.6% | batch:       568 of       772\t|\tloss: 0.619527\n",
      "Training Epoch 11  73.7% | batch:       569 of       772\t|\tloss: 0.504087\n",
      "Training Epoch 11  73.8% | batch:       570 of       772\t|\tloss: 0.493574\n",
      "Training Epoch 11  74.0% | batch:       571 of       772\t|\tloss: 0.350216\n",
      "Training Epoch 11  74.1% | batch:       572 of       772\t|\tloss: 0.543939\n",
      "Training Epoch 11  74.2% | batch:       573 of       772\t|\tloss: 0.67539\n",
      "Training Epoch 11  74.4% | batch:       574 of       772\t|\tloss: 0.727375\n",
      "Training Epoch 11  74.5% | batch:       575 of       772\t|\tloss: 0.909476\n",
      "Training Epoch 11  74.6% | batch:       576 of       772\t|\tloss: 0.511697\n",
      "Training Epoch 11  74.7% | batch:       577 of       772\t|\tloss: 0.563734\n",
      "Training Epoch 11  74.9% | batch:       578 of       772\t|\tloss: 0.577863\n",
      "Training Epoch 11  75.0% | batch:       579 of       772\t|\tloss: 0.42843\n",
      "Training Epoch 11  75.1% | batch:       580 of       772\t|\tloss: 0.502493\n",
      "Training Epoch 11  75.3% | batch:       581 of       772\t|\tloss: 0.617502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  75.4% | batch:       582 of       772\t|\tloss: 0.748384\n",
      "Training Epoch 11  75.5% | batch:       583 of       772\t|\tloss: 0.522175\n",
      "Training Epoch 11  75.6% | batch:       584 of       772\t|\tloss: 0.739183\n",
      "Training Epoch 11  75.8% | batch:       585 of       772\t|\tloss: 0.611237\n",
      "Training Epoch 11  75.9% | batch:       586 of       772\t|\tloss: 0.594499\n",
      "Training Epoch 11  76.0% | batch:       587 of       772\t|\tloss: 0.388138\n",
      "Training Epoch 11  76.2% | batch:       588 of       772\t|\tloss: 0.620837\n",
      "Training Epoch 11  76.3% | batch:       589 of       772\t|\tloss: 0.359726\n",
      "Training Epoch 11  76.4% | batch:       590 of       772\t|\tloss: 0.440942\n",
      "Training Epoch 11  76.6% | batch:       591 of       772\t|\tloss: 0.417135\n",
      "Training Epoch 11  76.7% | batch:       592 of       772\t|\tloss: 0.514189\n",
      "Training Epoch 11  76.8% | batch:       593 of       772\t|\tloss: 0.62877\n",
      "Training Epoch 11  76.9% | batch:       594 of       772\t|\tloss: 0.704972\n",
      "Training Epoch 11  77.1% | batch:       595 of       772\t|\tloss: 0.62889\n",
      "Training Epoch 11  77.2% | batch:       596 of       772\t|\tloss: 0.553893\n",
      "Training Epoch 11  77.3% | batch:       597 of       772\t|\tloss: 0.666823\n",
      "Training Epoch 11  77.5% | batch:       598 of       772\t|\tloss: 0.679373\n",
      "Training Epoch 11  77.6% | batch:       599 of       772\t|\tloss: 0.598558\n",
      "Training Epoch 11  77.7% | batch:       600 of       772\t|\tloss: 0.484921\n",
      "Training Epoch 11  77.8% | batch:       601 of       772\t|\tloss: 0.586422\n",
      "Training Epoch 11  78.0% | batch:       602 of       772\t|\tloss: 0.497353\n",
      "Training Epoch 11  78.1% | batch:       603 of       772\t|\tloss: 0.498708\n",
      "Training Epoch 11  78.2% | batch:       604 of       772\t|\tloss: 0.568704\n",
      "Training Epoch 11  78.4% | batch:       605 of       772\t|\tloss: 0.407329\n",
      "Training Epoch 11  78.5% | batch:       606 of       772\t|\tloss: 0.560949\n",
      "Training Epoch 11  78.6% | batch:       607 of       772\t|\tloss: 0.669419\n",
      "Training Epoch 11  78.8% | batch:       608 of       772\t|\tloss: 0.46789\n",
      "Training Epoch 11  78.9% | batch:       609 of       772\t|\tloss: 0.479854\n",
      "Training Epoch 11  79.0% | batch:       610 of       772\t|\tloss: 0.379132\n",
      "Training Epoch 11  79.1% | batch:       611 of       772\t|\tloss: 0.777145\n",
      "Training Epoch 11  79.3% | batch:       612 of       772\t|\tloss: 0.428781\n",
      "Training Epoch 11  79.4% | batch:       613 of       772\t|\tloss: 0.40737\n",
      "Training Epoch 11  79.5% | batch:       614 of       772\t|\tloss: 0.35846\n",
      "Training Epoch 11  79.7% | batch:       615 of       772\t|\tloss: 0.377876\n",
      "Training Epoch 11  79.8% | batch:       616 of       772\t|\tloss: 0.450344\n",
      "Training Epoch 11  79.9% | batch:       617 of       772\t|\tloss: 0.482991\n",
      "Training Epoch 11  80.1% | batch:       618 of       772\t|\tloss: 0.442062\n",
      "Training Epoch 11  80.2% | batch:       619 of       772\t|\tloss: 0.767536\n",
      "Training Epoch 11  80.3% | batch:       620 of       772\t|\tloss: 0.535529\n",
      "Training Epoch 11  80.4% | batch:       621 of       772\t|\tloss: 0.54918\n",
      "Training Epoch 11  80.6% | batch:       622 of       772\t|\tloss: 0.355427\n",
      "Training Epoch 11  80.7% | batch:       623 of       772\t|\tloss: 0.576782\n",
      "Training Epoch 11  80.8% | batch:       624 of       772\t|\tloss: 0.386683\n",
      "Training Epoch 11  81.0% | batch:       625 of       772\t|\tloss: 0.628386\n",
      "Training Epoch 11  81.1% | batch:       626 of       772\t|\tloss: 0.669588\n",
      "Training Epoch 11  81.2% | batch:       627 of       772\t|\tloss: 0.465193\n",
      "Training Epoch 11  81.3% | batch:       628 of       772\t|\tloss: 0.564903\n",
      "Training Epoch 11  81.5% | batch:       629 of       772\t|\tloss: 0.633972\n",
      "Training Epoch 11  81.6% | batch:       630 of       772\t|\tloss: 0.615378\n",
      "Training Epoch 11  81.7% | batch:       631 of       772\t|\tloss: 0.508187\n",
      "Training Epoch 11  81.9% | batch:       632 of       772\t|\tloss: 0.578455\n",
      "Training Epoch 11  82.0% | batch:       633 of       772\t|\tloss: 0.482949\n",
      "Training Epoch 11  82.1% | batch:       634 of       772\t|\tloss: 0.586193\n",
      "Training Epoch 11  82.3% | batch:       635 of       772\t|\tloss: 0.573616\n",
      "Training Epoch 11  82.4% | batch:       636 of       772\t|\tloss: 0.624222\n",
      "Training Epoch 11  82.5% | batch:       637 of       772\t|\tloss: 0.690644\n",
      "Training Epoch 11  82.6% | batch:       638 of       772\t|\tloss: 0.40989\n",
      "Training Epoch 11  82.8% | batch:       639 of       772\t|\tloss: 0.618134\n",
      "Training Epoch 11  82.9% | batch:       640 of       772\t|\tloss: 0.5581\n",
      "Training Epoch 11  83.0% | batch:       641 of       772\t|\tloss: 0.702796\n",
      "Training Epoch 11  83.2% | batch:       642 of       772\t|\tloss: 0.683536\n",
      "Training Epoch 11  83.3% | batch:       643 of       772\t|\tloss: 0.598296\n",
      "Training Epoch 11  83.4% | batch:       644 of       772\t|\tloss: 0.407893\n",
      "Training Epoch 11  83.5% | batch:       645 of       772\t|\tloss: 0.417723\n",
      "Training Epoch 11  83.7% | batch:       646 of       772\t|\tloss: 0.64707\n",
      "Training Epoch 11  83.8% | batch:       647 of       772\t|\tloss: 0.574783\n",
      "Training Epoch 11  83.9% | batch:       648 of       772\t|\tloss: 0.640705\n",
      "Training Epoch 11  84.1% | batch:       649 of       772\t|\tloss: 0.608506\n",
      "Training Epoch 11  84.2% | batch:       650 of       772\t|\tloss: 0.440573\n",
      "Training Epoch 11  84.3% | batch:       651 of       772\t|\tloss: 0.436754\n",
      "Training Epoch 11  84.5% | batch:       652 of       772\t|\tloss: 0.684708\n",
      "Training Epoch 11  84.6% | batch:       653 of       772\t|\tloss: 0.414355\n",
      "Training Epoch 11  84.7% | batch:       654 of       772\t|\tloss: 0.967458\n",
      "Training Epoch 11  84.8% | batch:       655 of       772\t|\tloss: 0.497125\n",
      "Training Epoch 11  85.0% | batch:       656 of       772\t|\tloss: 0.532371\n",
      "Training Epoch 11  85.1% | batch:       657 of       772\t|\tloss: 0.646963\n",
      "Training Epoch 11  85.2% | batch:       658 of       772\t|\tloss: 0.533144\n",
      "Training Epoch 11  85.4% | batch:       659 of       772\t|\tloss: 0.472277\n",
      "Training Epoch 11  85.5% | batch:       660 of       772\t|\tloss: 0.583424\n",
      "Training Epoch 11  85.6% | batch:       661 of       772\t|\tloss: 0.79556\n",
      "Training Epoch 11  85.8% | batch:       662 of       772\t|\tloss: 0.622555\n",
      "Training Epoch 11  85.9% | batch:       663 of       772\t|\tloss: 0.507562\n",
      "Training Epoch 11  86.0% | batch:       664 of       772\t|\tloss: 0.592067\n",
      "Training Epoch 11  86.1% | batch:       665 of       772\t|\tloss: 0.516418\n",
      "Training Epoch 11  86.3% | batch:       666 of       772\t|\tloss: 0.604079\n",
      "Training Epoch 11  86.4% | batch:       667 of       772\t|\tloss: 0.461624\n",
      "Training Epoch 11  86.5% | batch:       668 of       772\t|\tloss: 0.512259\n",
      "Training Epoch 11  86.7% | batch:       669 of       772\t|\tloss: 0.594002\n",
      "Training Epoch 11  86.8% | batch:       670 of       772\t|\tloss: 0.537377\n",
      "Training Epoch 11  86.9% | batch:       671 of       772\t|\tloss: 0.509195\n",
      "Training Epoch 11  87.0% | batch:       672 of       772\t|\tloss: 0.738679\n",
      "Training Epoch 11  87.2% | batch:       673 of       772\t|\tloss: 0.508751\n",
      "Training Epoch 11  87.3% | batch:       674 of       772\t|\tloss: 0.5883\n",
      "Training Epoch 11  87.4% | batch:       675 of       772\t|\tloss: 0.462651\n",
      "Training Epoch 11  87.6% | batch:       676 of       772\t|\tloss: 0.595842\n",
      "Training Epoch 11  87.7% | batch:       677 of       772\t|\tloss: 0.651796\n",
      "Training Epoch 11  87.8% | batch:       678 of       772\t|\tloss: 0.518314\n",
      "Training Epoch 11  88.0% | batch:       679 of       772\t|\tloss: 0.411564\n",
      "Training Epoch 11  88.1% | batch:       680 of       772\t|\tloss: 0.713495\n",
      "Training Epoch 11  88.2% | batch:       681 of       772\t|\tloss: 0.560763\n",
      "Training Epoch 11  88.3% | batch:       682 of       772\t|\tloss: 0.541127\n",
      "Training Epoch 11  88.5% | batch:       683 of       772\t|\tloss: 0.523241\n",
      "Training Epoch 11  88.6% | batch:       684 of       772\t|\tloss: 0.409791\n",
      "Training Epoch 11  88.7% | batch:       685 of       772\t|\tloss: 0.531823\n",
      "Training Epoch 11  88.9% | batch:       686 of       772\t|\tloss: 0.479332\n",
      "Training Epoch 11  89.0% | batch:       687 of       772\t|\tloss: 0.616812\n",
      "Training Epoch 11  89.1% | batch:       688 of       772\t|\tloss: 0.508131\n",
      "Training Epoch 11  89.2% | batch:       689 of       772\t|\tloss: 0.621389\n",
      "Training Epoch 11  89.4% | batch:       690 of       772\t|\tloss: 0.377817\n",
      "Training Epoch 11  89.5% | batch:       691 of       772\t|\tloss: 0.569291\n",
      "Training Epoch 11  89.6% | batch:       692 of       772\t|\tloss: 0.582245\n",
      "Training Epoch 11  89.8% | batch:       693 of       772\t|\tloss: 0.532392\n",
      "Training Epoch 11  89.9% | batch:       694 of       772\t|\tloss: 0.438134\n",
      "Training Epoch 11  90.0% | batch:       695 of       772\t|\tloss: 0.586743\n",
      "Training Epoch 11  90.2% | batch:       696 of       772\t|\tloss: 0.622465\n",
      "Training Epoch 11  90.3% | batch:       697 of       772\t|\tloss: 0.454826\n",
      "Training Epoch 11  90.4% | batch:       698 of       772\t|\tloss: 0.564684\n",
      "Training Epoch 11  90.5% | batch:       699 of       772\t|\tloss: 0.636533\n",
      "Training Epoch 11  90.7% | batch:       700 of       772\t|\tloss: 0.752989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  90.8% | batch:       701 of       772\t|\tloss: 0.802292\n",
      "Training Epoch 11  90.9% | batch:       702 of       772\t|\tloss: 1.12605\n",
      "Training Epoch 11  91.1% | batch:       703 of       772\t|\tloss: 0.546952\n",
      "Training Epoch 11  91.2% | batch:       704 of       772\t|\tloss: 0.593014\n",
      "Training Epoch 11  91.3% | batch:       705 of       772\t|\tloss: 0.923122\n",
      "Training Epoch 11  91.5% | batch:       706 of       772\t|\tloss: 0.977596\n",
      "Training Epoch 11  91.6% | batch:       707 of       772\t|\tloss: 0.733501\n",
      "Training Epoch 11  91.7% | batch:       708 of       772\t|\tloss: 0.560236\n",
      "Training Epoch 11  91.8% | batch:       709 of       772\t|\tloss: 0.600718\n",
      "Training Epoch 11  92.0% | batch:       710 of       772\t|\tloss: 0.553261\n",
      "Training Epoch 11  92.1% | batch:       711 of       772\t|\tloss: 1.65755\n",
      "Training Epoch 11  92.2% | batch:       712 of       772\t|\tloss: 1.07794\n",
      "Training Epoch 11  92.4% | batch:       713 of       772\t|\tloss: 0.510484\n",
      "Training Epoch 11  92.5% | batch:       714 of       772\t|\tloss: 0.462003\n",
      "Training Epoch 11  92.6% | batch:       715 of       772\t|\tloss: 0.513702\n",
      "Training Epoch 11  92.7% | batch:       716 of       772\t|\tloss: 0.598885\n",
      "Training Epoch 11  92.9% | batch:       717 of       772\t|\tloss: 0.533056\n",
      "Training Epoch 11  93.0% | batch:       718 of       772\t|\tloss: 0.574169\n",
      "Training Epoch 11  93.1% | batch:       719 of       772\t|\tloss: 0.510574\n",
      "Training Epoch 11  93.3% | batch:       720 of       772\t|\tloss: 0.904546\n",
      "Training Epoch 11  93.4% | batch:       721 of       772\t|\tloss: 1.45561\n",
      "Training Epoch 11  93.5% | batch:       722 of       772\t|\tloss: 0.873121\n",
      "Training Epoch 11  93.7% | batch:       723 of       772\t|\tloss: 0.779372\n",
      "Training Epoch 11  93.8% | batch:       724 of       772\t|\tloss: 0.5527\n",
      "Training Epoch 11  93.9% | batch:       725 of       772\t|\tloss: 0.64307\n",
      "Training Epoch 11  94.0% | batch:       726 of       772\t|\tloss: 0.784731\n",
      "Training Epoch 11  94.2% | batch:       727 of       772\t|\tloss: 0.610615\n",
      "Training Epoch 11  94.3% | batch:       728 of       772\t|\tloss: 0.786996\n",
      "Training Epoch 11  94.4% | batch:       729 of       772\t|\tloss: 0.522745\n",
      "Training Epoch 11  94.6% | batch:       730 of       772\t|\tloss: 0.383524\n",
      "Training Epoch 11  94.7% | batch:       731 of       772\t|\tloss: 0.53507\n",
      "Training Epoch 11  94.8% | batch:       732 of       772\t|\tloss: 0.533883\n",
      "Training Epoch 11  94.9% | batch:       733 of       772\t|\tloss: 0.416522\n",
      "Training Epoch 11  95.1% | batch:       734 of       772\t|\tloss: 0.714905\n",
      "Training Epoch 11  95.2% | batch:       735 of       772\t|\tloss: 0.641104\n",
      "Training Epoch 11  95.3% | batch:       736 of       772\t|\tloss: 0.476124\n",
      "Training Epoch 11  95.5% | batch:       737 of       772\t|\tloss: 0.458026\n",
      "Training Epoch 11  95.6% | batch:       738 of       772\t|\tloss: 0.669366\n",
      "Training Epoch 11  95.7% | batch:       739 of       772\t|\tloss: 0.565903\n",
      "Training Epoch 11  95.9% | batch:       740 of       772\t|\tloss: 0.791505\n",
      "Training Epoch 11  96.0% | batch:       741 of       772\t|\tloss: 0.529893\n",
      "Training Epoch 11  96.1% | batch:       742 of       772\t|\tloss: 0.479388\n",
      "Training Epoch 11  96.2% | batch:       743 of       772\t|\tloss: 0.538662\n",
      "Training Epoch 11  96.4% | batch:       744 of       772\t|\tloss: 0.567086\n",
      "Training Epoch 11  96.5% | batch:       745 of       772\t|\tloss: 0.511949\n",
      "Training Epoch 11  96.6% | batch:       746 of       772\t|\tloss: 0.484735\n",
      "Training Epoch 11  96.8% | batch:       747 of       772\t|\tloss: 0.497961\n",
      "Training Epoch 11  96.9% | batch:       748 of       772\t|\tloss: 0.378562\n",
      "Training Epoch 11  97.0% | batch:       749 of       772\t|\tloss: 0.653543\n",
      "Training Epoch 11  97.2% | batch:       750 of       772\t|\tloss: 0.461445\n",
      "Training Epoch 11  97.3% | batch:       751 of       772\t|\tloss: 0.655294\n",
      "Training Epoch 11  97.4% | batch:       752 of       772\t|\tloss: 0.685954\n",
      "Training Epoch 11  97.5% | batch:       753 of       772\t|\tloss: 0.699408\n",
      "Training Epoch 11  97.7% | batch:       754 of       772\t|\tloss: 0.616989\n",
      "Training Epoch 11  97.8% | batch:       755 of       772\t|\tloss: 0.539197\n",
      "Training Epoch 11  97.9% | batch:       756 of       772\t|\tloss: 0.474587\n",
      "Training Epoch 11  98.1% | batch:       757 of       772\t|\tloss: 0.716928\n",
      "Training Epoch 11  98.2% | batch:       758 of       772\t|\tloss: 0.391485\n",
      "Training Epoch 11  98.3% | batch:       759 of       772\t|\tloss: 0.416525\n",
      "Training Epoch 11  98.4% | batch:       760 of       772\t|\tloss: 0.496199\n",
      "Training Epoch 11  98.6% | batch:       761 of       772\t|\tloss: 0.518155\n",
      "Training Epoch 11  98.7% | batch:       762 of       772\t|\tloss: 0.581162\n",
      "Training Epoch 11  98.8% | batch:       763 of       772\t|\tloss: 0.658464\n",
      "Training Epoch 11  99.0% | batch:       764 of       772\t|\tloss: 0.318837\n",
      "Training Epoch 11  99.1% | batch:       765 of       772\t|\tloss: 0.365876\n",
      "Training Epoch 11  99.2% | batch:       766 of       772\t|\tloss: 0.626844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:23:13,082 | INFO : Epoch 11 Training Summary: epoch: 11.000000 | loss: 0.630100 | \n",
      "2023-05-24 10:23:13,083 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 15.12975025177002 seconds\n",
      "\n",
      "2023-05-24 10:23:13,083 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 16.07585508173162 seconds\n",
      "2023-05-24 10:23:13,084 | INFO : Avg batch train. time: 0.020823646478926972 seconds\n",
      "2023-05-24 10:23:13,084 | INFO : Avg sample train. time: 0.0001626880308633556 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  99.4% | batch:       767 of       772\t|\tloss: 0.514338\n",
      "Training Epoch 11  99.5% | batch:       768 of       772\t|\tloss: 0.794106\n",
      "Training Epoch 11  99.6% | batch:       769 of       772\t|\tloss: 0.553429\n",
      "Training Epoch 11  99.7% | batch:       770 of       772\t|\tloss: 0.449928\n",
      "Training Epoch 11  99.9% | batch:       771 of       772\t|\tloss: 0.434121\n",
      "\n",
      "Training Epoch 12   0.0% | batch:         0 of       772\t|\tloss: 0.588053\n",
      "Training Epoch 12   0.1% | batch:         1 of       772\t|\tloss: 0.530365\n",
      "Training Epoch 12   0.3% | batch:         2 of       772\t|\tloss: 0.38199\n",
      "Training Epoch 12   0.4% | batch:         3 of       772\t|\tloss: 0.52044\n",
      "Training Epoch 12   0.5% | batch:         4 of       772\t|\tloss: 0.419838\n",
      "Training Epoch 12   0.6% | batch:         5 of       772\t|\tloss: 0.632469\n",
      "Training Epoch 12   0.8% | batch:         6 of       772\t|\tloss: 0.605774\n",
      "Training Epoch 12   0.9% | batch:         7 of       772\t|\tloss: 0.422625\n",
      "Training Epoch 12   1.0% | batch:         8 of       772\t|\tloss: 0.421166\n",
      "Training Epoch 12   1.2% | batch:         9 of       772\t|\tloss: 0.451079\n",
      "Training Epoch 12   1.3% | batch:        10 of       772\t|\tloss: 0.805036\n",
      "Training Epoch 12   1.4% | batch:        11 of       772\t|\tloss: 0.56117\n",
      "Training Epoch 12   1.6% | batch:        12 of       772\t|\tloss: 0.635654\n",
      "Training Epoch 12   1.7% | batch:        13 of       772\t|\tloss: 0.392622\n",
      "Training Epoch 12   1.8% | batch:        14 of       772\t|\tloss: 0.391735\n",
      "Training Epoch 12   1.9% | batch:        15 of       772\t|\tloss: 0.369105\n",
      "Training Epoch 12   2.1% | batch:        16 of       772\t|\tloss: 1.04851\n",
      "Training Epoch 12   2.2% | batch:        17 of       772\t|\tloss: 0.386414\n",
      "Training Epoch 12   2.3% | batch:        18 of       772\t|\tloss: 0.481731\n",
      "Training Epoch 12   2.5% | batch:        19 of       772\t|\tloss: 0.521151\n",
      "Training Epoch 12   2.6% | batch:        20 of       772\t|\tloss: 0.558275\n",
      "Training Epoch 12   2.7% | batch:        21 of       772\t|\tloss: 0.351319\n",
      "Training Epoch 12   2.8% | batch:        22 of       772\t|\tloss: 0.662203\n",
      "Training Epoch 12   3.0% | batch:        23 of       772\t|\tloss: 0.427505\n",
      "Training Epoch 12   3.1% | batch:        24 of       772\t|\tloss: 0.584898\n",
      "Training Epoch 12   3.2% | batch:        25 of       772\t|\tloss: 0.641862\n",
      "Training Epoch 12   3.4% | batch:        26 of       772\t|\tloss: 0.631423\n",
      "Training Epoch 12   3.5% | batch:        27 of       772\t|\tloss: 0.51945\n",
      "Training Epoch 12   3.6% | batch:        28 of       772\t|\tloss: 0.571157\n",
      "Training Epoch 12   3.8% | batch:        29 of       772\t|\tloss: 0.676945\n",
      "Training Epoch 12   3.9% | batch:        30 of       772\t|\tloss: 0.557439\n",
      "Training Epoch 12   4.0% | batch:        31 of       772\t|\tloss: 0.627061\n",
      "Training Epoch 12   4.1% | batch:        32 of       772\t|\tloss: 0.583154\n",
      "Training Epoch 12   4.3% | batch:        33 of       772\t|\tloss: 0.367914\n",
      "Training Epoch 12   4.4% | batch:        34 of       772\t|\tloss: 0.500631\n",
      "Training Epoch 12   4.5% | batch:        35 of       772\t|\tloss: 0.436774\n",
      "Training Epoch 12   4.7% | batch:        36 of       772\t|\tloss: 0.414009\n",
      "Training Epoch 12   4.8% | batch:        37 of       772\t|\tloss: 0.375115\n",
      "Training Epoch 12   4.9% | batch:        38 of       772\t|\tloss: 0.607058\n",
      "Training Epoch 12   5.1% | batch:        39 of       772\t|\tloss: 0.78196\n",
      "Training Epoch 12   5.2% | batch:        40 of       772\t|\tloss: 0.64339\n",
      "Training Epoch 12   5.3% | batch:        41 of       772\t|\tloss: 0.581966\n",
      "Training Epoch 12   5.4% | batch:        42 of       772\t|\tloss: 0.689655\n",
      "Training Epoch 12   5.6% | batch:        43 of       772\t|\tloss: 0.589735\n",
      "Training Epoch 12   5.7% | batch:        44 of       772\t|\tloss: 0.419262\n",
      "Training Epoch 12   5.8% | batch:        45 of       772\t|\tloss: 0.588845\n",
      "Training Epoch 12   6.0% | batch:        46 of       772\t|\tloss: 0.707199\n",
      "Training Epoch 12   6.1% | batch:        47 of       772\t|\tloss: 0.716737\n",
      "Training Epoch 12   6.2% | batch:        48 of       772\t|\tloss: 0.771439\n",
      "Training Epoch 12   6.3% | batch:        49 of       772\t|\tloss: 0.974491\n",
      "Training Epoch 12   6.5% | batch:        50 of       772\t|\tloss: 0.860311\n",
      "Training Epoch 12   6.6% | batch:        51 of       772\t|\tloss: 0.578061\n",
      "Training Epoch 12   6.7% | batch:        52 of       772\t|\tloss: 0.531488\n",
      "Training Epoch 12   6.9% | batch:        53 of       772\t|\tloss: 0.765219\n",
      "Training Epoch 12   7.0% | batch:        54 of       772\t|\tloss: 0.915617\n",
      "Training Epoch 12   7.1% | batch:        55 of       772\t|\tloss: 0.805397\n",
      "Training Epoch 12   7.3% | batch:        56 of       772\t|\tloss: 0.919437\n",
      "Training Epoch 12   7.4% | batch:        57 of       772\t|\tloss: 0.428593\n",
      "Training Epoch 12   7.5% | batch:        58 of       772\t|\tloss: 0.710354\n",
      "Training Epoch 12   7.6% | batch:        59 of       772\t|\tloss: 0.832447\n",
      "Training Epoch 12   7.8% | batch:        60 of       772\t|\tloss: 0.846678\n",
      "Training Epoch 12   7.9% | batch:        61 of       772\t|\tloss: 0.540851\n",
      "Training Epoch 12   8.0% | batch:        62 of       772\t|\tloss: 0.669525\n",
      "Training Epoch 12   8.2% | batch:        63 of       772\t|\tloss: 0.738063\n",
      "Training Epoch 12   8.3% | batch:        64 of       772\t|\tloss: 0.670604\n",
      "Training Epoch 12   8.4% | batch:        65 of       772\t|\tloss: 0.723836\n",
      "Training Epoch 12   8.5% | batch:        66 of       772\t|\tloss: 0.55992\n",
      "Training Epoch 12   8.7% | batch:        67 of       772\t|\tloss: 0.575007\n",
      "Training Epoch 12   8.8% | batch:        68 of       772\t|\tloss: 0.625975\n",
      "Training Epoch 12   8.9% | batch:        69 of       772\t|\tloss: 0.564845\n",
      "Training Epoch 12   9.1% | batch:        70 of       772\t|\tloss: 0.577989\n",
      "Training Epoch 12   9.2% | batch:        71 of       772\t|\tloss: 0.763845\n",
      "Training Epoch 12   9.3% | batch:        72 of       772\t|\tloss: 0.761272\n",
      "Training Epoch 12   9.5% | batch:        73 of       772\t|\tloss: 0.470126\n",
      "Training Epoch 12   9.6% | batch:        74 of       772\t|\tloss: 0.557105\n",
      "Training Epoch 12   9.7% | batch:        75 of       772\t|\tloss: 0.482732\n",
      "Training Epoch 12   9.8% | batch:        76 of       772\t|\tloss: 0.508799\n",
      "Training Epoch 12  10.0% | batch:        77 of       772\t|\tloss: 0.493414\n",
      "Training Epoch 12  10.1% | batch:        78 of       772\t|\tloss: 0.378183\n",
      "Training Epoch 12  10.2% | batch:        79 of       772\t|\tloss: 0.397057\n",
      "Training Epoch 12  10.4% | batch:        80 of       772\t|\tloss: 0.69221\n",
      "Training Epoch 12  10.5% | batch:        81 of       772\t|\tloss: 0.512022\n",
      "Training Epoch 12  10.6% | batch:        82 of       772\t|\tloss: 0.55549\n",
      "Training Epoch 12  10.8% | batch:        83 of       772\t|\tloss: 0.420213\n",
      "Training Epoch 12  10.9% | batch:        84 of       772\t|\tloss: 0.743131\n",
      "Training Epoch 12  11.0% | batch:        85 of       772\t|\tloss: 0.525552\n",
      "Training Epoch 12  11.1% | batch:        86 of       772\t|\tloss: 0.612869\n",
      "Training Epoch 12  11.3% | batch:        87 of       772\t|\tloss: 0.660055\n",
      "Training Epoch 12  11.4% | batch:        88 of       772\t|\tloss: 0.578038\n",
      "Training Epoch 12  11.5% | batch:        89 of       772\t|\tloss: 0.639222\n",
      "Training Epoch 12  11.7% | batch:        90 of       772\t|\tloss: 0.611151\n",
      "Training Epoch 12  11.8% | batch:        91 of       772\t|\tloss: 0.545451\n",
      "Training Epoch 12  11.9% | batch:        92 of       772\t|\tloss: 0.440333\n",
      "Training Epoch 12  12.0% | batch:        93 of       772\t|\tloss: 0.457291\n",
      "Training Epoch 12  12.2% | batch:        94 of       772\t|\tloss: 0.530961\n",
      "Training Epoch 12  12.3% | batch:        95 of       772\t|\tloss: 0.533343\n",
      "Training Epoch 12  12.4% | batch:        96 of       772\t|\tloss: 0.766328\n",
      "Training Epoch 12  12.6% | batch:        97 of       772\t|\tloss: 0.41543\n",
      "Training Epoch 12  12.7% | batch:        98 of       772\t|\tloss: 0.86738\n",
      "Training Epoch 12  12.8% | batch:        99 of       772\t|\tloss: 0.418597\n",
      "Training Epoch 12  13.0% | batch:       100 of       772\t|\tloss: 0.524797\n",
      "Training Epoch 12  13.1% | batch:       101 of       772\t|\tloss: 0.56323\n",
      "Training Epoch 12  13.2% | batch:       102 of       772\t|\tloss: 0.674512\n",
      "Training Epoch 12  13.3% | batch:       103 of       772\t|\tloss: 0.606364\n",
      "Training Epoch 12  13.5% | batch:       104 of       772\t|\tloss: 0.73954\n",
      "Training Epoch 12  13.6% | batch:       105 of       772\t|\tloss: 0.478989\n",
      "Training Epoch 12  13.7% | batch:       106 of       772\t|\tloss: 0.783643\n",
      "Training Epoch 12  13.9% | batch:       107 of       772\t|\tloss: 0.665585\n",
      "Training Epoch 12  14.0% | batch:       108 of       772\t|\tloss: 0.50483\n",
      "Training Epoch 12  14.1% | batch:       109 of       772\t|\tloss: 0.684062\n",
      "Training Epoch 12  14.2% | batch:       110 of       772\t|\tloss: 0.423548\n",
      "Training Epoch 12  14.4% | batch:       111 of       772\t|\tloss: 0.686585\n",
      "Training Epoch 12  14.5% | batch:       112 of       772\t|\tloss: 0.643936\n",
      "Training Epoch 12  14.6% | batch:       113 of       772\t|\tloss: 0.508017\n",
      "Training Epoch 12  14.8% | batch:       114 of       772\t|\tloss: 0.479221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  14.9% | batch:       115 of       772\t|\tloss: 0.901881\n",
      "Training Epoch 12  15.0% | batch:       116 of       772\t|\tloss: 0.557704\n",
      "Training Epoch 12  15.2% | batch:       117 of       772\t|\tloss: 0.441354\n",
      "Training Epoch 12  15.3% | batch:       118 of       772\t|\tloss: 0.478246\n",
      "Training Epoch 12  15.4% | batch:       119 of       772\t|\tloss: 0.395361\n",
      "Training Epoch 12  15.5% | batch:       120 of       772\t|\tloss: 0.515407\n",
      "Training Epoch 12  15.7% | batch:       121 of       772\t|\tloss: 0.860928\n",
      "Training Epoch 12  15.8% | batch:       122 of       772\t|\tloss: 0.441702\n",
      "Training Epoch 12  15.9% | batch:       123 of       772\t|\tloss: 0.460039\n",
      "Training Epoch 12  16.1% | batch:       124 of       772\t|\tloss: 0.49604\n",
      "Training Epoch 12  16.2% | batch:       125 of       772\t|\tloss: 0.465264\n",
      "Training Epoch 12  16.3% | batch:       126 of       772\t|\tloss: 0.490709\n",
      "Training Epoch 12  16.5% | batch:       127 of       772\t|\tloss: 0.515611\n",
      "Training Epoch 12  16.6% | batch:       128 of       772\t|\tloss: 0.654778\n",
      "Training Epoch 12  16.7% | batch:       129 of       772\t|\tloss: 0.529541\n",
      "Training Epoch 12  16.8% | batch:       130 of       772\t|\tloss: 0.73725\n",
      "Training Epoch 12  17.0% | batch:       131 of       772\t|\tloss: 0.594103\n",
      "Training Epoch 12  17.1% | batch:       132 of       772\t|\tloss: 0.622305\n",
      "Training Epoch 12  17.2% | batch:       133 of       772\t|\tloss: 0.603825\n",
      "Training Epoch 12  17.4% | batch:       134 of       772\t|\tloss: 0.570775\n",
      "Training Epoch 12  17.5% | batch:       135 of       772\t|\tloss: 0.465818\n",
      "Training Epoch 12  17.6% | batch:       136 of       772\t|\tloss: 0.454904\n",
      "Training Epoch 12  17.7% | batch:       137 of       772\t|\tloss: 0.743352\n",
      "Training Epoch 12  17.9% | batch:       138 of       772\t|\tloss: 0.64828\n",
      "Training Epoch 12  18.0% | batch:       139 of       772\t|\tloss: 0.507131\n",
      "Training Epoch 12  18.1% | batch:       140 of       772\t|\tloss: 0.84952\n",
      "Training Epoch 12  18.3% | batch:       141 of       772\t|\tloss: 0.698888\n",
      "Training Epoch 12  18.4% | batch:       142 of       772\t|\tloss: 0.489304\n",
      "Training Epoch 12  18.5% | batch:       143 of       772\t|\tloss: 0.578296\n",
      "Training Epoch 12  18.7% | batch:       144 of       772\t|\tloss: 0.446762\n",
      "Training Epoch 12  18.8% | batch:       145 of       772\t|\tloss: 0.693595\n",
      "Training Epoch 12  18.9% | batch:       146 of       772\t|\tloss: 0.760958\n",
      "Training Epoch 12  19.0% | batch:       147 of       772\t|\tloss: 0.893984\n",
      "Training Epoch 12  19.2% | batch:       148 of       772\t|\tloss: 0.739002\n",
      "Training Epoch 12  19.3% | batch:       149 of       772\t|\tloss: 0.475732\n",
      "Training Epoch 12  19.4% | batch:       150 of       772\t|\tloss: 0.547237\n",
      "Training Epoch 12  19.6% | batch:       151 of       772\t|\tloss: 0.844327\n",
      "Training Epoch 12  19.7% | batch:       152 of       772\t|\tloss: 0.887266\n",
      "Training Epoch 12  19.8% | batch:       153 of       772\t|\tloss: 0.565659\n",
      "Training Epoch 12  19.9% | batch:       154 of       772\t|\tloss: 0.690928\n",
      "Training Epoch 12  20.1% | batch:       155 of       772\t|\tloss: 0.796724\n",
      "Training Epoch 12  20.2% | batch:       156 of       772\t|\tloss: 0.55428\n",
      "Training Epoch 12  20.3% | batch:       157 of       772\t|\tloss: 0.481267\n",
      "Training Epoch 12  20.5% | batch:       158 of       772\t|\tloss: 0.706677\n",
      "Training Epoch 12  20.6% | batch:       159 of       772\t|\tloss: 0.566278\n",
      "Training Epoch 12  20.7% | batch:       160 of       772\t|\tloss: 0.517221\n",
      "Training Epoch 12  20.9% | batch:       161 of       772\t|\tloss: 0.512321\n",
      "Training Epoch 12  21.0% | batch:       162 of       772\t|\tloss: 0.697677\n",
      "Training Epoch 12  21.1% | batch:       163 of       772\t|\tloss: 0.630491\n",
      "Training Epoch 12  21.2% | batch:       164 of       772\t|\tloss: 0.557777\n",
      "Training Epoch 12  21.4% | batch:       165 of       772\t|\tloss: 0.448744\n",
      "Training Epoch 12  21.5% | batch:       166 of       772\t|\tloss: 0.588078\n",
      "Training Epoch 12  21.6% | batch:       167 of       772\t|\tloss: 0.466592\n",
      "Training Epoch 12  21.8% | batch:       168 of       772\t|\tloss: 0.580218\n",
      "Training Epoch 12  21.9% | batch:       169 of       772\t|\tloss: 0.491651\n",
      "Training Epoch 12  22.0% | batch:       170 of       772\t|\tloss: 0.412874\n",
      "Training Epoch 12  22.2% | batch:       171 of       772\t|\tloss: 0.463455\n",
      "Training Epoch 12  22.3% | batch:       172 of       772\t|\tloss: 0.48275\n",
      "Training Epoch 12  22.4% | batch:       173 of       772\t|\tloss: 0.414275\n",
      "Training Epoch 12  22.5% | batch:       174 of       772\t|\tloss: 0.837799\n",
      "Training Epoch 12  22.7% | batch:       175 of       772\t|\tloss: 0.805245\n",
      "Training Epoch 12  22.8% | batch:       176 of       772\t|\tloss: 0.619569\n",
      "Training Epoch 12  22.9% | batch:       177 of       772\t|\tloss: 0.557117\n",
      "Training Epoch 12  23.1% | batch:       178 of       772\t|\tloss: 0.65009\n",
      "Training Epoch 12  23.2% | batch:       179 of       772\t|\tloss: 0.506878\n",
      "Training Epoch 12  23.3% | batch:       180 of       772\t|\tloss: 0.613206\n",
      "Training Epoch 12  23.4% | batch:       181 of       772\t|\tloss: 0.689227\n",
      "Training Epoch 12  23.6% | batch:       182 of       772\t|\tloss: 0.850778\n",
      "Training Epoch 12  23.7% | batch:       183 of       772\t|\tloss: 0.70953\n",
      "Training Epoch 12  23.8% | batch:       184 of       772\t|\tloss: 0.574906\n",
      "Training Epoch 12  24.0% | batch:       185 of       772\t|\tloss: 0.947456\n",
      "Training Epoch 12  24.1% | batch:       186 of       772\t|\tloss: 0.493768\n",
      "Training Epoch 12  24.2% | batch:       187 of       772\t|\tloss: 0.372974\n",
      "Training Epoch 12  24.4% | batch:       188 of       772\t|\tloss: 0.551379\n",
      "Training Epoch 12  24.5% | batch:       189 of       772\t|\tloss: 0.596116\n",
      "Training Epoch 12  24.6% | batch:       190 of       772\t|\tloss: 0.593772\n",
      "Training Epoch 12  24.7% | batch:       191 of       772\t|\tloss: 0.732655\n",
      "Training Epoch 12  24.9% | batch:       192 of       772\t|\tloss: 0.435793\n",
      "Training Epoch 12  25.0% | batch:       193 of       772\t|\tloss: 0.745985\n",
      "Training Epoch 12  25.1% | batch:       194 of       772\t|\tloss: 0.63537\n",
      "Training Epoch 12  25.3% | batch:       195 of       772\t|\tloss: 0.709716\n",
      "Training Epoch 12  25.4% | batch:       196 of       772\t|\tloss: 0.63246\n",
      "Training Epoch 12  25.5% | batch:       197 of       772\t|\tloss: 0.480415\n",
      "Training Epoch 12  25.6% | batch:       198 of       772\t|\tloss: 0.663697\n",
      "Training Epoch 12  25.8% | batch:       199 of       772\t|\tloss: 0.769342\n",
      "Training Epoch 12  25.9% | batch:       200 of       772\t|\tloss: 0.485859\n",
      "Training Epoch 12  26.0% | batch:       201 of       772\t|\tloss: 0.55631\n",
      "Training Epoch 12  26.2% | batch:       202 of       772\t|\tloss: 0.598827\n",
      "Training Epoch 12  26.3% | batch:       203 of       772\t|\tloss: 0.550627\n",
      "Training Epoch 12  26.4% | batch:       204 of       772\t|\tloss: 0.496202\n",
      "Training Epoch 12  26.6% | batch:       205 of       772\t|\tloss: 0.516932\n",
      "Training Epoch 12  26.7% | batch:       206 of       772\t|\tloss: 1.02881\n",
      "Training Epoch 12  26.8% | batch:       207 of       772\t|\tloss: 0.405372\n",
      "Training Epoch 12  26.9% | batch:       208 of       772\t|\tloss: 0.724143\n",
      "Training Epoch 12  27.1% | batch:       209 of       772\t|\tloss: 0.477878\n",
      "Training Epoch 12  27.2% | batch:       210 of       772\t|\tloss: 0.628983\n",
      "Training Epoch 12  27.3% | batch:       211 of       772\t|\tloss: 1.04419\n",
      "Training Epoch 12  27.5% | batch:       212 of       772\t|\tloss: 0.487765\n",
      "Training Epoch 12  27.6% | batch:       213 of       772\t|\tloss: 0.550985\n",
      "Training Epoch 12  27.7% | batch:       214 of       772\t|\tloss: 0.428986\n",
      "Training Epoch 12  27.8% | batch:       215 of       772\t|\tloss: 0.498147\n",
      "Training Epoch 12  28.0% | batch:       216 of       772\t|\tloss: 0.492096\n",
      "Training Epoch 12  28.1% | batch:       217 of       772\t|\tloss: 0.892063\n",
      "Training Epoch 12  28.2% | batch:       218 of       772\t|\tloss: 0.545013\n",
      "Training Epoch 12  28.4% | batch:       219 of       772\t|\tloss: 0.454623\n",
      "Training Epoch 12  28.5% | batch:       220 of       772\t|\tloss: 0.726154\n",
      "Training Epoch 12  28.6% | batch:       221 of       772\t|\tloss: 0.439895\n",
      "Training Epoch 12  28.8% | batch:       222 of       772\t|\tloss: 0.461334\n",
      "Training Epoch 12  28.9% | batch:       223 of       772\t|\tloss: 0.524745\n",
      "Training Epoch 12  29.0% | batch:       224 of       772\t|\tloss: 0.381087\n",
      "Training Epoch 12  29.1% | batch:       225 of       772\t|\tloss: 0.656996\n",
      "Training Epoch 12  29.3% | batch:       226 of       772\t|\tloss: 0.478811\n",
      "Training Epoch 12  29.4% | batch:       227 of       772\t|\tloss: 0.424415\n",
      "Training Epoch 12  29.5% | batch:       228 of       772\t|\tloss: 0.509231\n",
      "Training Epoch 12  29.7% | batch:       229 of       772\t|\tloss: 0.592528\n",
      "Training Epoch 12  29.8% | batch:       230 of       772\t|\tloss: 0.420223\n",
      "Training Epoch 12  29.9% | batch:       231 of       772\t|\tloss: 0.552984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  30.1% | batch:       232 of       772\t|\tloss: 0.402811\n",
      "Training Epoch 12  30.2% | batch:       233 of       772\t|\tloss: 0.569353\n",
      "Training Epoch 12  30.3% | batch:       234 of       772\t|\tloss: 0.495847\n",
      "Training Epoch 12  30.4% | batch:       235 of       772\t|\tloss: 0.577503\n",
      "Training Epoch 12  30.6% | batch:       236 of       772\t|\tloss: 0.563924\n",
      "Training Epoch 12  30.7% | batch:       237 of       772\t|\tloss: 0.515391\n",
      "Training Epoch 12  30.8% | batch:       238 of       772\t|\tloss: 0.816755\n",
      "Training Epoch 12  31.0% | batch:       239 of       772\t|\tloss: 0.865042\n",
      "Training Epoch 12  31.1% | batch:       240 of       772\t|\tloss: 0.714455\n",
      "Training Epoch 12  31.2% | batch:       241 of       772\t|\tloss: 0.52317\n",
      "Training Epoch 12  31.3% | batch:       242 of       772\t|\tloss: 0.536581\n",
      "Training Epoch 12  31.5% | batch:       243 of       772\t|\tloss: 0.699277\n",
      "Training Epoch 12  31.6% | batch:       244 of       772\t|\tloss: 0.422697\n",
      "Training Epoch 12  31.7% | batch:       245 of       772\t|\tloss: 0.646276\n",
      "Training Epoch 12  31.9% | batch:       246 of       772\t|\tloss: 0.871437\n",
      "Training Epoch 12  32.0% | batch:       247 of       772\t|\tloss: 0.826788\n",
      "Training Epoch 12  32.1% | batch:       248 of       772\t|\tloss: 0.646316\n",
      "Training Epoch 12  32.3% | batch:       249 of       772\t|\tloss: 0.43926\n",
      "Training Epoch 12  32.4% | batch:       250 of       772\t|\tloss: 0.617651\n",
      "Training Epoch 12  32.5% | batch:       251 of       772\t|\tloss: 0.485373\n",
      "Training Epoch 12  32.6% | batch:       252 of       772\t|\tloss: 0.403246\n",
      "Training Epoch 12  32.8% | batch:       253 of       772\t|\tloss: 0.446575\n",
      "Training Epoch 12  32.9% | batch:       254 of       772\t|\tloss: 0.592216\n",
      "Training Epoch 12  33.0% | batch:       255 of       772\t|\tloss: 0.603133\n",
      "Training Epoch 12  33.2% | batch:       256 of       772\t|\tloss: 0.532751\n",
      "Training Epoch 12  33.3% | batch:       257 of       772\t|\tloss: 0.698068\n",
      "Training Epoch 12  33.4% | batch:       258 of       772\t|\tloss: 0.647328\n",
      "Training Epoch 12  33.5% | batch:       259 of       772\t|\tloss: 0.529249\n",
      "Training Epoch 12  33.7% | batch:       260 of       772\t|\tloss: 0.481184\n",
      "Training Epoch 12  33.8% | batch:       261 of       772\t|\tloss: 0.453737\n",
      "Training Epoch 12  33.9% | batch:       262 of       772\t|\tloss: 0.511578\n",
      "Training Epoch 12  34.1% | batch:       263 of       772\t|\tloss: 0.487424\n",
      "Training Epoch 12  34.2% | batch:       264 of       772\t|\tloss: 0.644314\n",
      "Training Epoch 12  34.3% | batch:       265 of       772\t|\tloss: 0.549369\n",
      "Training Epoch 12  34.5% | batch:       266 of       772\t|\tloss: 0.597802\n",
      "Training Epoch 12  34.6% | batch:       267 of       772\t|\tloss: 0.626412\n",
      "Training Epoch 12  34.7% | batch:       268 of       772\t|\tloss: 0.470782\n",
      "Training Epoch 12  34.8% | batch:       269 of       772\t|\tloss: 0.613652\n",
      "Training Epoch 12  35.0% | batch:       270 of       772\t|\tloss: 0.327708\n",
      "Training Epoch 12  35.1% | batch:       271 of       772\t|\tloss: 0.482955\n",
      "Training Epoch 12  35.2% | batch:       272 of       772\t|\tloss: 0.772473\n",
      "Training Epoch 12  35.4% | batch:       273 of       772\t|\tloss: 0.431354\n",
      "Training Epoch 12  35.5% | batch:       274 of       772\t|\tloss: 0.860005\n",
      "Training Epoch 12  35.6% | batch:       275 of       772\t|\tloss: 0.620596\n",
      "Training Epoch 12  35.8% | batch:       276 of       772\t|\tloss: 0.604281\n",
      "Training Epoch 12  35.9% | batch:       277 of       772\t|\tloss: 0.486144\n",
      "Training Epoch 12  36.0% | batch:       278 of       772\t|\tloss: 0.705868\n",
      "Training Epoch 12  36.1% | batch:       279 of       772\t|\tloss: 0.589266\n",
      "Training Epoch 12  36.3% | batch:       280 of       772\t|\tloss: 0.511611\n",
      "Training Epoch 12  36.4% | batch:       281 of       772\t|\tloss: 0.578931\n",
      "Training Epoch 12  36.5% | batch:       282 of       772\t|\tloss: 0.632376\n",
      "Training Epoch 12  36.7% | batch:       283 of       772\t|\tloss: 0.829973\n",
      "Training Epoch 12  36.8% | batch:       284 of       772\t|\tloss: 0.601992\n",
      "Training Epoch 12  36.9% | batch:       285 of       772\t|\tloss: 0.609599\n",
      "Training Epoch 12  37.0% | batch:       286 of       772\t|\tloss: 0.921442\n",
      "Training Epoch 12  37.2% | batch:       287 of       772\t|\tloss: 0.655079\n",
      "Training Epoch 12  37.3% | batch:       288 of       772\t|\tloss: 1.07683\n",
      "Training Epoch 12  37.4% | batch:       289 of       772\t|\tloss: 0.582808\n",
      "Training Epoch 12  37.6% | batch:       290 of       772\t|\tloss: 0.850107\n",
      "Training Epoch 12  37.7% | batch:       291 of       772\t|\tloss: 0.599684\n",
      "Training Epoch 12  37.8% | batch:       292 of       772\t|\tloss: 0.493375\n",
      "Training Epoch 12  38.0% | batch:       293 of       772\t|\tloss: 0.459444\n",
      "Training Epoch 12  38.1% | batch:       294 of       772\t|\tloss: 0.59753\n",
      "Training Epoch 12  38.2% | batch:       295 of       772\t|\tloss: 0.45555\n",
      "Training Epoch 12  38.3% | batch:       296 of       772\t|\tloss: 0.594302\n",
      "Training Epoch 12  38.5% | batch:       297 of       772\t|\tloss: 0.469829\n",
      "Training Epoch 12  38.6% | batch:       298 of       772\t|\tloss: 0.617228\n",
      "Training Epoch 12  38.7% | batch:       299 of       772\t|\tloss: 0.481783\n",
      "Training Epoch 12  38.9% | batch:       300 of       772\t|\tloss: 0.46844\n",
      "Training Epoch 12  39.0% | batch:       301 of       772\t|\tloss: 0.665617\n",
      "Training Epoch 12  39.1% | batch:       302 of       772\t|\tloss: 0.556632\n",
      "Training Epoch 12  39.2% | batch:       303 of       772\t|\tloss: 0.580196\n",
      "Training Epoch 12  39.4% | batch:       304 of       772\t|\tloss: 0.427587\n",
      "Training Epoch 12  39.5% | batch:       305 of       772\t|\tloss: 0.596716\n",
      "Training Epoch 12  39.6% | batch:       306 of       772\t|\tloss: 0.493926\n",
      "Training Epoch 12  39.8% | batch:       307 of       772\t|\tloss: 0.4647\n",
      "Training Epoch 12  39.9% | batch:       308 of       772\t|\tloss: 0.667013\n",
      "Training Epoch 12  40.0% | batch:       309 of       772\t|\tloss: 0.885343\n",
      "Training Epoch 12  40.2% | batch:       310 of       772\t|\tloss: 0.727886\n",
      "Training Epoch 12  40.3% | batch:       311 of       772\t|\tloss: 0.744601\n",
      "Training Epoch 12  40.4% | batch:       312 of       772\t|\tloss: 0.604103\n",
      "Training Epoch 12  40.5% | batch:       313 of       772\t|\tloss: 0.698761\n",
      "Training Epoch 12  40.7% | batch:       314 of       772\t|\tloss: 0.433284\n",
      "Training Epoch 12  40.8% | batch:       315 of       772\t|\tloss: 0.50226\n",
      "Training Epoch 12  40.9% | batch:       316 of       772\t|\tloss: 0.985254\n",
      "Training Epoch 12  41.1% | batch:       317 of       772\t|\tloss: 0.733636\n",
      "Training Epoch 12  41.2% | batch:       318 of       772\t|\tloss: 0.642199\n",
      "Training Epoch 12  41.3% | batch:       319 of       772\t|\tloss: 0.683821\n",
      "Training Epoch 12  41.5% | batch:       320 of       772\t|\tloss: 0.679898\n",
      "Training Epoch 12  41.6% | batch:       321 of       772\t|\tloss: 0.874911\n",
      "Training Epoch 12  41.7% | batch:       322 of       772\t|\tloss: 0.833817\n",
      "Training Epoch 12  41.8% | batch:       323 of       772\t|\tloss: 0.649763\n",
      "Training Epoch 12  42.0% | batch:       324 of       772\t|\tloss: 0.552851\n",
      "Training Epoch 12  42.1% | batch:       325 of       772\t|\tloss: 0.472719\n",
      "Training Epoch 12  42.2% | batch:       326 of       772\t|\tloss: 0.914025\n",
      "Training Epoch 12  42.4% | batch:       327 of       772\t|\tloss: 1.13272\n",
      "Training Epoch 12  42.5% | batch:       328 of       772\t|\tloss: 1.42783\n",
      "Training Epoch 12  42.6% | batch:       329 of       772\t|\tloss: 1.09676\n",
      "Training Epoch 12  42.7% | batch:       330 of       772\t|\tloss: 0.66109\n",
      "Training Epoch 12  42.9% | batch:       331 of       772\t|\tloss: 0.816745\n",
      "Training Epoch 12  43.0% | batch:       332 of       772\t|\tloss: 1.06243\n",
      "Training Epoch 12  43.1% | batch:       333 of       772\t|\tloss: 0.903505\n",
      "Training Epoch 12  43.3% | batch:       334 of       772\t|\tloss: 0.440462\n",
      "Training Epoch 12  43.4% | batch:       335 of       772\t|\tloss: 0.591927\n",
      "Training Epoch 12  43.5% | batch:       336 of       772\t|\tloss: 0.685822\n",
      "Training Epoch 12  43.7% | batch:       337 of       772\t|\tloss: 0.969733\n",
      "Training Epoch 12  43.8% | batch:       338 of       772\t|\tloss: 0.481253\n",
      "Training Epoch 12  43.9% | batch:       339 of       772\t|\tloss: 0.407006\n",
      "Training Epoch 12  44.0% | batch:       340 of       772\t|\tloss: 0.471765\n",
      "Training Epoch 12  44.2% | batch:       341 of       772\t|\tloss: 0.429834\n",
      "Training Epoch 12  44.3% | batch:       342 of       772\t|\tloss: 0.64123\n",
      "Training Epoch 12  44.4% | batch:       343 of       772\t|\tloss: 0.465775\n",
      "Training Epoch 12  44.6% | batch:       344 of       772\t|\tloss: 0.737254\n",
      "Training Epoch 12  44.7% | batch:       345 of       772\t|\tloss: 0.544392\n",
      "Training Epoch 12  44.8% | batch:       346 of       772\t|\tloss: 0.666969\n",
      "Training Epoch 12  44.9% | batch:       347 of       772\t|\tloss: 0.78193\n",
      "Training Epoch 12  45.1% | batch:       348 of       772\t|\tloss: 0.732034\n",
      "Training Epoch 12  45.2% | batch:       349 of       772\t|\tloss: 0.539465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  45.3% | batch:       350 of       772\t|\tloss: 0.459633\n",
      "Training Epoch 12  45.5% | batch:       351 of       772\t|\tloss: 0.681245\n",
      "Training Epoch 12  45.6% | batch:       352 of       772\t|\tloss: 0.548631\n",
      "Training Epoch 12  45.7% | batch:       353 of       772\t|\tloss: 0.414851\n",
      "Training Epoch 12  45.9% | batch:       354 of       772\t|\tloss: 0.341023\n",
      "Training Epoch 12  46.0% | batch:       355 of       772\t|\tloss: 0.577122\n",
      "Training Epoch 12  46.1% | batch:       356 of       772\t|\tloss: 0.58436\n",
      "Training Epoch 12  46.2% | batch:       357 of       772\t|\tloss: 0.490981\n",
      "Training Epoch 12  46.4% | batch:       358 of       772\t|\tloss: 0.417609\n",
      "Training Epoch 12  46.5% | batch:       359 of       772\t|\tloss: 0.595374\n",
      "Training Epoch 12  46.6% | batch:       360 of       772\t|\tloss: 0.820985\n",
      "Training Epoch 12  46.8% | batch:       361 of       772\t|\tloss: 0.618833\n",
      "Training Epoch 12  46.9% | batch:       362 of       772\t|\tloss: 0.495817\n",
      "Training Epoch 12  47.0% | batch:       363 of       772\t|\tloss: 0.628815\n",
      "Training Epoch 12  47.2% | batch:       364 of       772\t|\tloss: 0.411111\n",
      "Training Epoch 12  47.3% | batch:       365 of       772\t|\tloss: 0.363597\n",
      "Training Epoch 12  47.4% | batch:       366 of       772\t|\tloss: 0.500876\n",
      "Training Epoch 12  47.5% | batch:       367 of       772\t|\tloss: 0.492084\n",
      "Training Epoch 12  47.7% | batch:       368 of       772\t|\tloss: 0.595495\n",
      "Training Epoch 12  47.8% | batch:       369 of       772\t|\tloss: 0.672726\n",
      "Training Epoch 12  47.9% | batch:       370 of       772\t|\tloss: 0.684105\n",
      "Training Epoch 12  48.1% | batch:       371 of       772\t|\tloss: 0.846786\n",
      "Training Epoch 12  48.2% | batch:       372 of       772\t|\tloss: 1.10386\n",
      "Training Epoch 12  48.3% | batch:       373 of       772\t|\tloss: 0.728063\n",
      "Training Epoch 12  48.4% | batch:       374 of       772\t|\tloss: 0.601387\n",
      "Training Epoch 12  48.6% | batch:       375 of       772\t|\tloss: 0.365808\n",
      "Training Epoch 12  48.7% | batch:       376 of       772\t|\tloss: 0.854644\n",
      "Training Epoch 12  48.8% | batch:       377 of       772\t|\tloss: 0.682791\n",
      "Training Epoch 12  49.0% | batch:       378 of       772\t|\tloss: 1.54587\n",
      "Training Epoch 12  49.1% | batch:       379 of       772\t|\tloss: 0.802096\n",
      "Training Epoch 12  49.2% | batch:       380 of       772\t|\tloss: 0.45908\n",
      "Training Epoch 12  49.4% | batch:       381 of       772\t|\tloss: 0.469559\n",
      "Training Epoch 12  49.5% | batch:       382 of       772\t|\tloss: 0.76115\n",
      "Training Epoch 12  49.6% | batch:       383 of       772\t|\tloss: 0.787667\n",
      "Training Epoch 12  49.7% | batch:       384 of       772\t|\tloss: 0.485987\n",
      "Training Epoch 12  49.9% | batch:       385 of       772\t|\tloss: 0.435131\n",
      "Training Epoch 12  50.0% | batch:       386 of       772\t|\tloss: 0.793806\n",
      "Training Epoch 12  50.1% | batch:       387 of       772\t|\tloss: 0.652217\n",
      "Training Epoch 12  50.3% | batch:       388 of       772\t|\tloss: 0.765856\n",
      "Training Epoch 12  50.4% | batch:       389 of       772\t|\tloss: 0.544803\n",
      "Training Epoch 12  50.5% | batch:       390 of       772\t|\tloss: 0.653821\n",
      "Training Epoch 12  50.6% | batch:       391 of       772\t|\tloss: 0.591886\n",
      "Training Epoch 12  50.8% | batch:       392 of       772\t|\tloss: 0.649695\n",
      "Training Epoch 12  50.9% | batch:       393 of       772\t|\tloss: 0.994511\n",
      "Training Epoch 12  51.0% | batch:       394 of       772\t|\tloss: 0.736338\n",
      "Training Epoch 12  51.2% | batch:       395 of       772\t|\tloss: 0.480321\n",
      "Training Epoch 12  51.3% | batch:       396 of       772\t|\tloss: 0.55294\n",
      "Training Epoch 12  51.4% | batch:       397 of       772\t|\tloss: 0.559408\n",
      "Training Epoch 12  51.6% | batch:       398 of       772\t|\tloss: 0.779326\n",
      "Training Epoch 12  51.7% | batch:       399 of       772\t|\tloss: 0.44104\n",
      "Training Epoch 12  51.8% | batch:       400 of       772\t|\tloss: 0.488169\n",
      "Training Epoch 12  51.9% | batch:       401 of       772\t|\tloss: 0.595323\n",
      "Training Epoch 12  52.1% | batch:       402 of       772\t|\tloss: 0.670322\n",
      "Training Epoch 12  52.2% | batch:       403 of       772\t|\tloss: 0.814286\n",
      "Training Epoch 12  52.3% | batch:       404 of       772\t|\tloss: 0.533666\n",
      "Training Epoch 12  52.5% | batch:       405 of       772\t|\tloss: 0.414908\n",
      "Training Epoch 12  52.6% | batch:       406 of       772\t|\tloss: 0.489675\n",
      "Training Epoch 12  52.7% | batch:       407 of       772\t|\tloss: 0.386661\n",
      "Training Epoch 12  52.8% | batch:       408 of       772\t|\tloss: 0.481753\n",
      "Training Epoch 12  53.0% | batch:       409 of       772\t|\tloss: 0.55832\n",
      "Training Epoch 12  53.1% | batch:       410 of       772\t|\tloss: 0.527549\n",
      "Training Epoch 12  53.2% | batch:       411 of       772\t|\tloss: 0.426681\n",
      "Training Epoch 12  53.4% | batch:       412 of       772\t|\tloss: 0.632818\n",
      "Training Epoch 12  53.5% | batch:       413 of       772\t|\tloss: 0.51291\n",
      "Training Epoch 12  53.6% | batch:       414 of       772\t|\tloss: 0.411889\n",
      "Training Epoch 12  53.8% | batch:       415 of       772\t|\tloss: 0.425034\n",
      "Training Epoch 12  53.9% | batch:       416 of       772\t|\tloss: 0.554204\n",
      "Training Epoch 12  54.0% | batch:       417 of       772\t|\tloss: 0.440802\n",
      "Training Epoch 12  54.1% | batch:       418 of       772\t|\tloss: 0.569286\n",
      "Training Epoch 12  54.3% | batch:       419 of       772\t|\tloss: 0.521963\n",
      "Training Epoch 12  54.4% | batch:       420 of       772\t|\tloss: 0.621219\n",
      "Training Epoch 12  54.5% | batch:       421 of       772\t|\tloss: 0.733655\n",
      "Training Epoch 12  54.7% | batch:       422 of       772\t|\tloss: 0.946605\n",
      "Training Epoch 12  54.8% | batch:       423 of       772\t|\tloss: 1.04573\n",
      "Training Epoch 12  54.9% | batch:       424 of       772\t|\tloss: 0.560689\n",
      "Training Epoch 12  55.1% | batch:       425 of       772\t|\tloss: 0.506566\n",
      "Training Epoch 12  55.2% | batch:       426 of       772\t|\tloss: 0.574359\n",
      "Training Epoch 12  55.3% | batch:       427 of       772\t|\tloss: 0.971202\n",
      "Training Epoch 12  55.4% | batch:       428 of       772\t|\tloss: 0.642432\n",
      "Training Epoch 12  55.6% | batch:       429 of       772\t|\tloss: 0.431427\n",
      "Training Epoch 12  55.7% | batch:       430 of       772\t|\tloss: 0.435745\n",
      "Training Epoch 12  55.8% | batch:       431 of       772\t|\tloss: 0.847416\n",
      "Training Epoch 12  56.0% | batch:       432 of       772\t|\tloss: 0.802385\n",
      "Training Epoch 12  56.1% | batch:       433 of       772\t|\tloss: 0.838141\n",
      "Training Epoch 12  56.2% | batch:       434 of       772\t|\tloss: 0.571785\n",
      "Training Epoch 12  56.3% | batch:       435 of       772\t|\tloss: 0.470881\n",
      "Training Epoch 12  56.5% | batch:       436 of       772\t|\tloss: 0.390763\n",
      "Training Epoch 12  56.6% | batch:       437 of       772\t|\tloss: 0.779659\n",
      "Training Epoch 12  56.7% | batch:       438 of       772\t|\tloss: 1.60362\n",
      "Training Epoch 12  56.9% | batch:       439 of       772\t|\tloss: 0.955737\n",
      "Training Epoch 12  57.0% | batch:       440 of       772\t|\tloss: 0.544883\n",
      "Training Epoch 12  57.1% | batch:       441 of       772\t|\tloss: 0.539428\n",
      "Training Epoch 12  57.3% | batch:       442 of       772\t|\tloss: 0.600679\n",
      "Training Epoch 12  57.4% | batch:       443 of       772\t|\tloss: 0.601519\n",
      "Training Epoch 12  57.5% | batch:       444 of       772\t|\tloss: 0.532322\n",
      "Training Epoch 12  57.6% | batch:       445 of       772\t|\tloss: 0.791014\n",
      "Training Epoch 12  57.8% | batch:       446 of       772\t|\tloss: 0.4879\n",
      "Training Epoch 12  57.9% | batch:       447 of       772\t|\tloss: 0.396151\n",
      "Training Epoch 12  58.0% | batch:       448 of       772\t|\tloss: 0.618716\n",
      "Training Epoch 12  58.2% | batch:       449 of       772\t|\tloss: 0.527688\n",
      "Training Epoch 12  58.3% | batch:       450 of       772\t|\tloss: 0.592668\n",
      "Training Epoch 12  58.4% | batch:       451 of       772\t|\tloss: 0.454518\n",
      "Training Epoch 12  58.5% | batch:       452 of       772\t|\tloss: 0.717135\n",
      "Training Epoch 12  58.7% | batch:       453 of       772\t|\tloss: 0.461091\n",
      "Training Epoch 12  58.8% | batch:       454 of       772\t|\tloss: 0.449155\n",
      "Training Epoch 12  58.9% | batch:       455 of       772\t|\tloss: 0.683257\n",
      "Training Epoch 12  59.1% | batch:       456 of       772\t|\tloss: 0.388421\n",
      "Training Epoch 12  59.2% | batch:       457 of       772\t|\tloss: 0.673004\n",
      "Training Epoch 12  59.3% | batch:       458 of       772\t|\tloss: 0.645247\n",
      "Training Epoch 12  59.5% | batch:       459 of       772\t|\tloss: 0.557137\n",
      "Training Epoch 12  59.6% | batch:       460 of       772\t|\tloss: 0.540948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  59.7% | batch:       461 of       772\t|\tloss: 0.478638\n",
      "Training Epoch 12  59.8% | batch:       462 of       772\t|\tloss: 0.42355\n",
      "Training Epoch 12  60.0% | batch:       463 of       772\t|\tloss: 0.547278\n",
      "Training Epoch 12  60.1% | batch:       464 of       772\t|\tloss: 0.694945\n",
      "Training Epoch 12  60.2% | batch:       465 of       772\t|\tloss: 0.464895\n",
      "Training Epoch 12  60.4% | batch:       466 of       772\t|\tloss: 0.439871\n",
      "Training Epoch 12  60.5% | batch:       467 of       772\t|\tloss: 0.548402\n",
      "Training Epoch 12  60.6% | batch:       468 of       772\t|\tloss: 0.500875\n",
      "Training Epoch 12  60.8% | batch:       469 of       772\t|\tloss: 0.361858\n",
      "Training Epoch 12  60.9% | batch:       470 of       772\t|\tloss: 0.378232\n",
      "Training Epoch 12  61.0% | batch:       471 of       772\t|\tloss: 0.442853\n",
      "Training Epoch 12  61.1% | batch:       472 of       772\t|\tloss: 0.414216\n",
      "Training Epoch 12  61.3% | batch:       473 of       772\t|\tloss: 0.381212\n",
      "Training Epoch 12  61.4% | batch:       474 of       772\t|\tloss: 0.458521\n",
      "Training Epoch 12  61.5% | batch:       475 of       772\t|\tloss: 0.497984\n",
      "Training Epoch 12  61.7% | batch:       476 of       772\t|\tloss: 0.396043\n",
      "Training Epoch 12  61.8% | batch:       477 of       772\t|\tloss: 0.54181\n",
      "Training Epoch 12  61.9% | batch:       478 of       772\t|\tloss: 0.541861\n",
      "Training Epoch 12  62.0% | batch:       479 of       772\t|\tloss: 0.634297\n",
      "Training Epoch 12  62.2% | batch:       480 of       772\t|\tloss: 0.654006\n",
      "Training Epoch 12  62.3% | batch:       481 of       772\t|\tloss: 0.760167\n",
      "Training Epoch 12  62.4% | batch:       482 of       772\t|\tloss: 0.817898\n",
      "Training Epoch 12  62.6% | batch:       483 of       772\t|\tloss: 0.522422\n",
      "Training Epoch 12  62.7% | batch:       484 of       772\t|\tloss: 0.773033\n",
      "Training Epoch 12  62.8% | batch:       485 of       772\t|\tloss: 0.474312\n",
      "Training Epoch 12  63.0% | batch:       486 of       772\t|\tloss: 0.423371\n",
      "Training Epoch 12  63.1% | batch:       487 of       772\t|\tloss: 0.691599\n",
      "Training Epoch 12  63.2% | batch:       488 of       772\t|\tloss: 0.776019\n",
      "Training Epoch 12  63.3% | batch:       489 of       772\t|\tloss: 0.444654\n",
      "Training Epoch 12  63.5% | batch:       490 of       772\t|\tloss: 0.495854\n",
      "Training Epoch 12  63.6% | batch:       491 of       772\t|\tloss: 0.506029\n",
      "Training Epoch 12  63.7% | batch:       492 of       772\t|\tloss: 0.398229\n",
      "Training Epoch 12  63.9% | batch:       493 of       772\t|\tloss: 0.455978\n",
      "Training Epoch 12  64.0% | batch:       494 of       772\t|\tloss: 0.454586\n",
      "Training Epoch 12  64.1% | batch:       495 of       772\t|\tloss: 0.418871\n",
      "Training Epoch 12  64.2% | batch:       496 of       772\t|\tloss: 0.487003\n",
      "Training Epoch 12  64.4% | batch:       497 of       772\t|\tloss: 0.465187\n",
      "Training Epoch 12  64.5% | batch:       498 of       772\t|\tloss: 0.656158\n",
      "Training Epoch 12  64.6% | batch:       499 of       772\t|\tloss: 0.587794\n",
      "Training Epoch 12  64.8% | batch:       500 of       772\t|\tloss: 0.381589\n",
      "Training Epoch 12  64.9% | batch:       501 of       772\t|\tloss: 0.399307\n",
      "Training Epoch 12  65.0% | batch:       502 of       772\t|\tloss: 0.490802\n",
      "Training Epoch 12  65.2% | batch:       503 of       772\t|\tloss: 0.61078\n",
      "Training Epoch 12  65.3% | batch:       504 of       772\t|\tloss: 0.509004\n",
      "Training Epoch 12  65.4% | batch:       505 of       772\t|\tloss: 0.499447\n",
      "Training Epoch 12  65.5% | batch:       506 of       772\t|\tloss: 0.602697\n",
      "Training Epoch 12  65.7% | batch:       507 of       772\t|\tloss: 0.582225\n",
      "Training Epoch 12  65.8% | batch:       508 of       772\t|\tloss: 0.676756\n",
      "Training Epoch 12  65.9% | batch:       509 of       772\t|\tloss: 0.644782\n",
      "Training Epoch 12  66.1% | batch:       510 of       772\t|\tloss: 0.453984\n",
      "Training Epoch 12  66.2% | batch:       511 of       772\t|\tloss: 0.711367\n",
      "Training Epoch 12  66.3% | batch:       512 of       772\t|\tloss: 0.841245\n",
      "Training Epoch 12  66.5% | batch:       513 of       772\t|\tloss: 0.572542\n",
      "Training Epoch 12  66.6% | batch:       514 of       772\t|\tloss: 0.347156\n",
      "Training Epoch 12  66.7% | batch:       515 of       772\t|\tloss: 0.350799\n",
      "Training Epoch 12  66.8% | batch:       516 of       772\t|\tloss: 0.519169\n",
      "Training Epoch 12  67.0% | batch:       517 of       772\t|\tloss: 0.666769\n",
      "Training Epoch 12  67.1% | batch:       518 of       772\t|\tloss: 0.413556\n",
      "Training Epoch 12  67.2% | batch:       519 of       772\t|\tloss: 0.437499\n",
      "Training Epoch 12  67.4% | batch:       520 of       772\t|\tloss: 0.528733\n",
      "Training Epoch 12  67.5% | batch:       521 of       772\t|\tloss: 0.825142\n",
      "Training Epoch 12  67.6% | batch:       522 of       772\t|\tloss: 0.492593\n",
      "Training Epoch 12  67.7% | batch:       523 of       772\t|\tloss: 0.556861\n",
      "Training Epoch 12  67.9% | batch:       524 of       772\t|\tloss: 0.464964\n",
      "Training Epoch 12  68.0% | batch:       525 of       772\t|\tloss: 0.691208\n",
      "Training Epoch 12  68.1% | batch:       526 of       772\t|\tloss: 0.583869\n",
      "Training Epoch 12  68.3% | batch:       527 of       772\t|\tloss: 0.449035\n",
      "Training Epoch 12  68.4% | batch:       528 of       772\t|\tloss: 0.619039\n",
      "Training Epoch 12  68.5% | batch:       529 of       772\t|\tloss: 0.64433\n",
      "Training Epoch 12  68.7% | batch:       530 of       772\t|\tloss: 0.771331\n",
      "Training Epoch 12  68.8% | batch:       531 of       772\t|\tloss: 0.514204\n",
      "Training Epoch 12  68.9% | batch:       532 of       772\t|\tloss: 0.343771\n",
      "Training Epoch 12  69.0% | batch:       533 of       772\t|\tloss: 0.531423\n",
      "Training Epoch 12  69.2% | batch:       534 of       772\t|\tloss: 0.554042\n",
      "Training Epoch 12  69.3% | batch:       535 of       772\t|\tloss: 0.420255\n",
      "Training Epoch 12  69.4% | batch:       536 of       772\t|\tloss: 0.497291\n",
      "Training Epoch 12  69.6% | batch:       537 of       772\t|\tloss: 0.448421\n",
      "Training Epoch 12  69.7% | batch:       538 of       772\t|\tloss: 0.498663\n",
      "Training Epoch 12  69.8% | batch:       539 of       772\t|\tloss: 0.451416\n",
      "Training Epoch 12  69.9% | batch:       540 of       772\t|\tloss: 0.472988\n",
      "Training Epoch 12  70.1% | batch:       541 of       772\t|\tloss: 0.610816\n",
      "Training Epoch 12  70.2% | batch:       542 of       772\t|\tloss: 0.549898\n",
      "Training Epoch 12  70.3% | batch:       543 of       772\t|\tloss: 0.533731\n",
      "Training Epoch 12  70.5% | batch:       544 of       772\t|\tloss: 0.430384\n",
      "Training Epoch 12  70.6% | batch:       545 of       772\t|\tloss: 0.537374\n",
      "Training Epoch 12  70.7% | batch:       546 of       772\t|\tloss: 0.448473\n",
      "Training Epoch 12  70.9% | batch:       547 of       772\t|\tloss: 0.567858\n",
      "Training Epoch 12  71.0% | batch:       548 of       772\t|\tloss: 0.809188\n",
      "Training Epoch 12  71.1% | batch:       549 of       772\t|\tloss: 0.539845\n",
      "Training Epoch 12  71.2% | batch:       550 of       772\t|\tloss: 0.36786\n",
      "Training Epoch 12  71.4% | batch:       551 of       772\t|\tloss: 0.475157\n",
      "Training Epoch 12  71.5% | batch:       552 of       772\t|\tloss: 0.480047\n",
      "Training Epoch 12  71.6% | batch:       553 of       772\t|\tloss: 0.437122\n",
      "Training Epoch 12  71.8% | batch:       554 of       772\t|\tloss: 0.479147\n",
      "Training Epoch 12  71.9% | batch:       555 of       772\t|\tloss: 0.505953\n",
      "Training Epoch 12  72.0% | batch:       556 of       772\t|\tloss: 0.564795\n",
      "Training Epoch 12  72.2% | batch:       557 of       772\t|\tloss: 0.544194\n",
      "Training Epoch 12  72.3% | batch:       558 of       772\t|\tloss: 0.430649\n",
      "Training Epoch 12  72.4% | batch:       559 of       772\t|\tloss: 0.703375\n",
      "Training Epoch 12  72.5% | batch:       560 of       772\t|\tloss: 0.659011\n",
      "Training Epoch 12  72.7% | batch:       561 of       772\t|\tloss: 0.810449\n",
      "Training Epoch 12  72.8% | batch:       562 of       772\t|\tloss: 0.616573\n",
      "Training Epoch 12  72.9% | batch:       563 of       772\t|\tloss: 0.417337\n",
      "Training Epoch 12  73.1% | batch:       564 of       772\t|\tloss: 0.615816\n",
      "Training Epoch 12  73.2% | batch:       565 of       772\t|\tloss: 0.540263\n",
      "Training Epoch 12  73.3% | batch:       566 of       772\t|\tloss: 0.40755\n",
      "Training Epoch 12  73.4% | batch:       567 of       772\t|\tloss: 0.520592\n",
      "Training Epoch 12  73.6% | batch:       568 of       772\t|\tloss: 0.586753\n",
      "Training Epoch 12  73.7% | batch:       569 of       772\t|\tloss: 0.441105\n",
      "Training Epoch 12  73.8% | batch:       570 of       772\t|\tloss: 0.654385\n",
      "Training Epoch 12  74.0% | batch:       571 of       772\t|\tloss: 0.620953\n",
      "Training Epoch 12  74.1% | batch:       572 of       772\t|\tloss: 0.519742\n",
      "Training Epoch 12  74.2% | batch:       573 of       772\t|\tloss: 0.635011\n",
      "Training Epoch 12  74.4% | batch:       574 of       772\t|\tloss: 0.446018\n",
      "Training Epoch 12  74.5% | batch:       575 of       772\t|\tloss: 0.386381\n",
      "Training Epoch 12  74.6% | batch:       576 of       772\t|\tloss: 0.461313\n",
      "Training Epoch 12  74.7% | batch:       577 of       772\t|\tloss: 0.669943\n",
      "Training Epoch 12  74.9% | batch:       578 of       772\t|\tloss: 0.492358\n",
      "Training Epoch 12  75.0% | batch:       579 of       772\t|\tloss: 0.491491\n",
      "Training Epoch 12  75.1% | batch:       580 of       772\t|\tloss: 0.78049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  75.3% | batch:       581 of       772\t|\tloss: 0.606591\n",
      "Training Epoch 12  75.4% | batch:       582 of       772\t|\tloss: 0.538172\n",
      "Training Epoch 12  75.5% | batch:       583 of       772\t|\tloss: 0.405598\n",
      "Training Epoch 12  75.6% | batch:       584 of       772\t|\tloss: 0.54779\n",
      "Training Epoch 12  75.8% | batch:       585 of       772\t|\tloss: 0.551945\n",
      "Training Epoch 12  75.9% | batch:       586 of       772\t|\tloss: 0.39751\n",
      "Training Epoch 12  76.0% | batch:       587 of       772\t|\tloss: 0.966555\n",
      "Training Epoch 12  76.2% | batch:       588 of       772\t|\tloss: 0.593915\n",
      "Training Epoch 12  76.3% | batch:       589 of       772\t|\tloss: 0.429936\n",
      "Training Epoch 12  76.4% | batch:       590 of       772\t|\tloss: 0.50372\n",
      "Training Epoch 12  76.6% | batch:       591 of       772\t|\tloss: 0.749749\n",
      "Training Epoch 12  76.7% | batch:       592 of       772\t|\tloss: 0.605708\n",
      "Training Epoch 12  76.8% | batch:       593 of       772\t|\tloss: 0.495593\n",
      "Training Epoch 12  76.9% | batch:       594 of       772\t|\tloss: 0.382656\n",
      "Training Epoch 12  77.1% | batch:       595 of       772\t|\tloss: 0.377837\n",
      "Training Epoch 12  77.2% | batch:       596 of       772\t|\tloss: 0.467135\n",
      "Training Epoch 12  77.3% | batch:       597 of       772\t|\tloss: 0.855021\n",
      "Training Epoch 12  77.5% | batch:       598 of       772\t|\tloss: 0.332463\n",
      "Training Epoch 12  77.6% | batch:       599 of       772\t|\tloss: 0.421378\n",
      "Training Epoch 12  77.7% | batch:       600 of       772\t|\tloss: 0.50015\n",
      "Training Epoch 12  77.8% | batch:       601 of       772\t|\tloss: 0.646346\n",
      "Training Epoch 12  78.0% | batch:       602 of       772\t|\tloss: 0.428089\n",
      "Training Epoch 12  78.1% | batch:       603 of       772\t|\tloss: 0.47781\n",
      "Training Epoch 12  78.2% | batch:       604 of       772\t|\tloss: 0.673001\n",
      "Training Epoch 12  78.4% | batch:       605 of       772\t|\tloss: 0.421374\n",
      "Training Epoch 12  78.5% | batch:       606 of       772\t|\tloss: 0.456558\n",
      "Training Epoch 12  78.6% | batch:       607 of       772\t|\tloss: 0.517735\n",
      "Training Epoch 12  78.8% | batch:       608 of       772\t|\tloss: 0.43296\n",
      "Training Epoch 12  78.9% | batch:       609 of       772\t|\tloss: 0.475446\n",
      "Training Epoch 12  79.0% | batch:       610 of       772\t|\tloss: 0.431071\n",
      "Training Epoch 12  79.1% | batch:       611 of       772\t|\tloss: 0.61667\n",
      "Training Epoch 12  79.3% | batch:       612 of       772\t|\tloss: 0.514797\n",
      "Training Epoch 12  79.4% | batch:       613 of       772\t|\tloss: 0.496934\n",
      "Training Epoch 12  79.5% | batch:       614 of       772\t|\tloss: 0.440019\n",
      "Training Epoch 12  79.7% | batch:       615 of       772\t|\tloss: 0.324618\n",
      "Training Epoch 12  79.8% | batch:       616 of       772\t|\tloss: 0.624003\n",
      "Training Epoch 12  79.9% | batch:       617 of       772\t|\tloss: 0.307926\n",
      "Training Epoch 12  80.1% | batch:       618 of       772\t|\tloss: 0.405725\n",
      "Training Epoch 12  80.2% | batch:       619 of       772\t|\tloss: 0.429379\n",
      "Training Epoch 12  80.3% | batch:       620 of       772\t|\tloss: 0.556106\n",
      "Training Epoch 12  80.4% | batch:       621 of       772\t|\tloss: 0.498674\n",
      "Training Epoch 12  80.6% | batch:       622 of       772\t|\tloss: 0.47843\n",
      "Training Epoch 12  80.7% | batch:       623 of       772\t|\tloss: 0.542316\n",
      "Training Epoch 12  80.8% | batch:       624 of       772\t|\tloss: 0.438722\n",
      "Training Epoch 12  81.0% | batch:       625 of       772\t|\tloss: 0.454409\n",
      "Training Epoch 12  81.1% | batch:       626 of       772\t|\tloss: 0.586303\n",
      "Training Epoch 12  81.2% | batch:       627 of       772\t|\tloss: 0.593302\n",
      "Training Epoch 12  81.3% | batch:       628 of       772\t|\tloss: 0.589568\n",
      "Training Epoch 12  81.5% | batch:       629 of       772\t|\tloss: 0.566074\n",
      "Training Epoch 12  81.6% | batch:       630 of       772\t|\tloss: 0.536514\n",
      "Training Epoch 12  81.7% | batch:       631 of       772\t|\tloss: 0.487003\n",
      "Training Epoch 12  81.9% | batch:       632 of       772\t|\tloss: 0.433209\n",
      "Training Epoch 12  82.0% | batch:       633 of       772\t|\tloss: 0.87847\n",
      "Training Epoch 12  82.1% | batch:       634 of       772\t|\tloss: 0.500794\n",
      "Training Epoch 12  82.3% | batch:       635 of       772\t|\tloss: 0.56748\n",
      "Training Epoch 12  82.4% | batch:       636 of       772\t|\tloss: 0.450736\n",
      "Training Epoch 12  82.5% | batch:       637 of       772\t|\tloss: 0.505093\n",
      "Training Epoch 12  82.6% | batch:       638 of       772\t|\tloss: 0.658174\n",
      "Training Epoch 12  82.8% | batch:       639 of       772\t|\tloss: 0.700566\n",
      "Training Epoch 12  82.9% | batch:       640 of       772\t|\tloss: 0.403049\n",
      "Training Epoch 12  83.0% | batch:       641 of       772\t|\tloss: 0.667842\n",
      "Training Epoch 12  83.2% | batch:       642 of       772\t|\tloss: 0.492128\n",
      "Training Epoch 12  83.3% | batch:       643 of       772\t|\tloss: 0.509841\n",
      "Training Epoch 12  83.4% | batch:       644 of       772\t|\tloss: 0.632175\n",
      "Training Epoch 12  83.5% | batch:       645 of       772\t|\tloss: 0.507259\n",
      "Training Epoch 12  83.7% | batch:       646 of       772\t|\tloss: 0.482749\n",
      "Training Epoch 12  83.8% | batch:       647 of       772\t|\tloss: 0.89915\n",
      "Training Epoch 12  83.9% | batch:       648 of       772\t|\tloss: 0.897541\n",
      "Training Epoch 12  84.1% | batch:       649 of       772\t|\tloss: 0.883821\n",
      "Training Epoch 12  84.2% | batch:       650 of       772\t|\tloss: 0.722644\n",
      "Training Epoch 12  84.3% | batch:       651 of       772\t|\tloss: 0.402246\n",
      "Training Epoch 12  84.5% | batch:       652 of       772\t|\tloss: 0.714723\n",
      "Training Epoch 12  84.6% | batch:       653 of       772\t|\tloss: 0.56843\n",
      "Training Epoch 12  84.7% | batch:       654 of       772\t|\tloss: 0.62775\n",
      "Training Epoch 12  84.8% | batch:       655 of       772\t|\tloss: 0.552888\n",
      "Training Epoch 12  85.0% | batch:       656 of       772\t|\tloss: 0.487572\n",
      "Training Epoch 12  85.1% | batch:       657 of       772\t|\tloss: 0.975704\n",
      "Training Epoch 12  85.2% | batch:       658 of       772\t|\tloss: 1.00382\n",
      "Training Epoch 12  85.4% | batch:       659 of       772\t|\tloss: 0.728909\n",
      "Training Epoch 12  85.5% | batch:       660 of       772\t|\tloss: 0.465594\n",
      "Training Epoch 12  85.6% | batch:       661 of       772\t|\tloss: 0.461002\n",
      "Training Epoch 12  85.8% | batch:       662 of       772\t|\tloss: 0.410842\n",
      "Training Epoch 12  85.9% | batch:       663 of       772\t|\tloss: 0.342491\n",
      "Training Epoch 12  86.0% | batch:       664 of       772\t|\tloss: 0.510622\n",
      "Training Epoch 12  86.1% | batch:       665 of       772\t|\tloss: 0.41509\n",
      "Training Epoch 12  86.3% | batch:       666 of       772\t|\tloss: 0.491038\n",
      "Training Epoch 12  86.4% | batch:       667 of       772\t|\tloss: 0.440903\n",
      "Training Epoch 12  86.5% | batch:       668 of       772\t|\tloss: 0.649002\n",
      "Training Epoch 12  86.7% | batch:       669 of       772\t|\tloss: 0.959102\n",
      "Training Epoch 12  86.8% | batch:       670 of       772\t|\tloss: 0.405142\n",
      "Training Epoch 12  86.9% | batch:       671 of       772\t|\tloss: 0.536205\n",
      "Training Epoch 12  87.0% | batch:       672 of       772\t|\tloss: 0.599291\n",
      "Training Epoch 12  87.2% | batch:       673 of       772\t|\tloss: 0.641423\n",
      "Training Epoch 12  87.3% | batch:       674 of       772\t|\tloss: 0.614234\n",
      "Training Epoch 12  87.4% | batch:       675 of       772\t|\tloss: 0.668833\n",
      "Training Epoch 12  87.6% | batch:       676 of       772\t|\tloss: 0.545315\n",
      "Training Epoch 12  87.7% | batch:       677 of       772\t|\tloss: 0.365385\n",
      "Training Epoch 12  87.8% | batch:       678 of       772\t|\tloss: 0.309445\n",
      "Training Epoch 12  88.0% | batch:       679 of       772\t|\tloss: 0.479268\n",
      "Training Epoch 12  88.1% | batch:       680 of       772\t|\tloss: 0.509923\n",
      "Training Epoch 12  88.2% | batch:       681 of       772\t|\tloss: 0.667487\n",
      "Training Epoch 12  88.3% | batch:       682 of       772\t|\tloss: 0.582867\n",
      "Training Epoch 12  88.5% | batch:       683 of       772\t|\tloss: 0.582156\n",
      "Training Epoch 12  88.6% | batch:       684 of       772\t|\tloss: 0.350017\n",
      "Training Epoch 12  88.7% | batch:       685 of       772\t|\tloss: 0.461582\n",
      "Training Epoch 12  88.9% | batch:       686 of       772\t|\tloss: 0.601449\n",
      "Training Epoch 12  89.0% | batch:       687 of       772\t|\tloss: 0.552128\n",
      "Training Epoch 12  89.1% | batch:       688 of       772\t|\tloss: 0.553636\n",
      "Training Epoch 12  89.2% | batch:       689 of       772\t|\tloss: 0.299975\n",
      "Training Epoch 12  89.4% | batch:       690 of       772\t|\tloss: 0.791232\n",
      "Training Epoch 12  89.5% | batch:       691 of       772\t|\tloss: 0.575706\n",
      "Training Epoch 12  89.6% | batch:       692 of       772\t|\tloss: 0.561524\n",
      "Training Epoch 12  89.8% | batch:       693 of       772\t|\tloss: 0.47679\n",
      "Training Epoch 12  89.9% | batch:       694 of       772\t|\tloss: 0.520525\n",
      "Training Epoch 12  90.0% | batch:       695 of       772\t|\tloss: 0.463172\n",
      "Training Epoch 12  90.2% | batch:       696 of       772\t|\tloss: 0.631373\n",
      "Training Epoch 12  90.3% | batch:       697 of       772\t|\tloss: 0.637207\n",
      "Training Epoch 12  90.4% | batch:       698 of       772\t|\tloss: 0.43721\n",
      "Training Epoch 12  90.5% | batch:       699 of       772\t|\tloss: 0.494469\n",
      "Training Epoch 12  90.7% | batch:       700 of       772\t|\tloss: 0.482122\n",
      "Training Epoch 12  90.8% | batch:       701 of       772\t|\tloss: 0.418583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  90.9% | batch:       702 of       772\t|\tloss: 1.12254\n",
      "Training Epoch 12  91.1% | batch:       703 of       772\t|\tloss: 0.389192\n",
      "Training Epoch 12  91.2% | batch:       704 of       772\t|\tloss: 0.544583\n",
      "Training Epoch 12  91.3% | batch:       705 of       772\t|\tloss: 0.463\n",
      "Training Epoch 12  91.5% | batch:       706 of       772\t|\tloss: 0.453142\n",
      "Training Epoch 12  91.6% | batch:       707 of       772\t|\tloss: 0.553001\n",
      "Training Epoch 12  91.7% | batch:       708 of       772\t|\tloss: 0.513093\n",
      "Training Epoch 12  91.8% | batch:       709 of       772\t|\tloss: 0.3602\n",
      "Training Epoch 12  92.0% | batch:       710 of       772\t|\tloss: 0.32481\n",
      "Training Epoch 12  92.1% | batch:       711 of       772\t|\tloss: 0.606325\n",
      "Training Epoch 12  92.2% | batch:       712 of       772\t|\tloss: 0.634251\n",
      "Training Epoch 12  92.4% | batch:       713 of       772\t|\tloss: 0.692287\n",
      "Training Epoch 12  92.5% | batch:       714 of       772\t|\tloss: 0.808093\n",
      "Training Epoch 12  92.6% | batch:       715 of       772\t|\tloss: 0.777637\n",
      "Training Epoch 12  92.7% | batch:       716 of       772\t|\tloss: 0.825584\n",
      "Training Epoch 12  92.9% | batch:       717 of       772\t|\tloss: 0.749127\n",
      "Training Epoch 12  93.0% | batch:       718 of       772\t|\tloss: 0.720837\n",
      "Training Epoch 12  93.1% | batch:       719 of       772\t|\tloss: 0.463632\n",
      "Training Epoch 12  93.3% | batch:       720 of       772\t|\tloss: 0.61399\n",
      "Training Epoch 12  93.4% | batch:       721 of       772\t|\tloss: 1.00414\n",
      "Training Epoch 12  93.5% | batch:       722 of       772\t|\tloss: 1.09925\n",
      "Training Epoch 12  93.7% | batch:       723 of       772\t|\tloss: 0.920927\n",
      "Training Epoch 12  93.8% | batch:       724 of       772\t|\tloss: 0.331972\n",
      "Training Epoch 12  93.9% | batch:       725 of       772\t|\tloss: 0.42099\n",
      "Training Epoch 12  94.0% | batch:       726 of       772\t|\tloss: 0.665999\n",
      "Training Epoch 12  94.2% | batch:       727 of       772\t|\tloss: 0.688933\n",
      "Training Epoch 12  94.3% | batch:       728 of       772\t|\tloss: 0.601953\n",
      "Training Epoch 12  94.4% | batch:       729 of       772\t|\tloss: 0.453968\n",
      "Training Epoch 12  94.6% | batch:       730 of       772\t|\tloss: 0.341689\n",
      "Training Epoch 12  94.7% | batch:       731 of       772\t|\tloss: 0.667754\n",
      "Training Epoch 12  94.8% | batch:       732 of       772\t|\tloss: 0.628147\n",
      "Training Epoch 12  94.9% | batch:       733 of       772\t|\tloss: 0.511414\n",
      "Training Epoch 12  95.1% | batch:       734 of       772\t|\tloss: 0.457157\n",
      "Training Epoch 12  95.2% | batch:       735 of       772\t|\tloss: 0.618613\n",
      "Training Epoch 12  95.3% | batch:       736 of       772\t|\tloss: 0.961289\n",
      "Training Epoch 12  95.5% | batch:       737 of       772\t|\tloss: 0.83242\n",
      "Training Epoch 12  95.6% | batch:       738 of       772\t|\tloss: 0.793236\n",
      "Training Epoch 12  95.7% | batch:       739 of       772\t|\tloss: 0.788386\n",
      "Training Epoch 12  95.9% | batch:       740 of       772\t|\tloss: 0.503826\n",
      "Training Epoch 12  96.0% | batch:       741 of       772\t|\tloss: 0.865566\n",
      "Training Epoch 12  96.1% | batch:       742 of       772\t|\tloss: 1.31632\n",
      "Training Epoch 12  96.2% | batch:       743 of       772\t|\tloss: 0.587653\n",
      "Training Epoch 12  96.4% | batch:       744 of       772\t|\tloss: 0.502199\n",
      "Training Epoch 12  96.5% | batch:       745 of       772\t|\tloss: 0.584755\n",
      "Training Epoch 12  96.6% | batch:       746 of       772\t|\tloss: 0.58212\n",
      "Training Epoch 12  96.8% | batch:       747 of       772\t|\tloss: 0.808883\n",
      "Training Epoch 12  96.9% | batch:       748 of       772\t|\tloss: 0.601079\n",
      "Training Epoch 12  97.0% | batch:       749 of       772\t|\tloss: 0.463201\n",
      "Training Epoch 12  97.2% | batch:       750 of       772\t|\tloss: 0.432079\n",
      "Training Epoch 12  97.3% | batch:       751 of       772\t|\tloss: 0.462325\n",
      "Training Epoch 12  97.4% | batch:       752 of       772\t|\tloss: 0.667343\n",
      "Training Epoch 12  97.5% | batch:       753 of       772\t|\tloss: 0.726654\n",
      "Training Epoch 12  97.7% | batch:       754 of       772\t|\tloss: 0.587239\n",
      "Training Epoch 12  97.8% | batch:       755 of       772\t|\tloss: 0.4448\n",
      "Training Epoch 12  97.9% | batch:       756 of       772\t|\tloss: 0.501061\n",
      "Training Epoch 12  98.1% | batch:       757 of       772\t|\tloss: 0.601564\n",
      "Training Epoch 12  98.2% | batch:       758 of       772\t|\tloss: 0.512394\n",
      "Training Epoch 12  98.3% | batch:       759 of       772\t|\tloss: 0.467343\n",
      "Training Epoch 12  98.4% | batch:       760 of       772\t|\tloss: 0.517918\n",
      "Training Epoch 12  98.6% | batch:       761 of       772\t|\tloss: 0.887701\n",
      "Training Epoch 12  98.7% | batch:       762 of       772\t|\tloss: 0.793912\n",
      "Training Epoch 12  98.8% | batch:       763 of       772\t|\tloss: 0.432936\n",
      "Training Epoch 12  99.0% | batch:       764 of       772\t|\tloss: 0.687742\n",
      "Training Epoch 12  99.1% | batch:       765 of       772\t|\tloss: 0.437144\n",
      "Training Epoch 12  99.2% | batch:       766 of       772\t|\tloss: 0.5432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:23:27,865 | INFO : Epoch 12 Training Summary: epoch: 12.000000 | loss: 0.588188 | \n",
      "2023-05-24 10:23:27,865 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 14.77148699760437 seconds\n",
      "\n",
      "2023-05-24 10:23:27,866 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 15.967157741387686 seconds\n",
      "2023-05-24 10:23:27,866 | INFO : Avg batch train. time: 0.020682846815269024 seconds\n",
      "2023-05-24 10:23:27,866 | INFO : Avg sample train. time: 0.00016158801122702942 seconds\n",
      "2023-05-24 10:23:27,867 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  99.4% | batch:       767 of       772\t|\tloss: 0.435103\n",
      "Training Epoch 12  99.5% | batch:       768 of       772\t|\tloss: 0.469164\n",
      "Training Epoch 12  99.6% | batch:       769 of       772\t|\tloss: 0.387646\n",
      "Training Epoch 12  99.7% | batch:       770 of       772\t|\tloss: 0.387923\n",
      "Training Epoch 12  99.9% | batch:       771 of       772\t|\tloss: 0.488992\n",
      "\n",
      "Evaluating Epoch 12   0.0% | batch:         0 of        92\t|\tloss: 1.59188\n",
      "Evaluating Epoch 12   1.1% | batch:         1 of        92\t|\tloss: 8.19028\n",
      "Evaluating Epoch 12   2.2% | batch:         2 of        92\t|\tloss: 3.61451\n",
      "Evaluating Epoch 12   3.3% | batch:         3 of        92\t|\tloss: 5.56909\n",
      "Evaluating Epoch 12   4.3% | batch:         4 of        92\t|\tloss: 4.10157\n",
      "Evaluating Epoch 12   5.4% | batch:         5 of        92\t|\tloss: 7.63488\n",
      "Evaluating Epoch 12   6.5% | batch:         6 of        92\t|\tloss: 4.96942\n",
      "Evaluating Epoch 12   7.6% | batch:         7 of        92\t|\tloss: 2.72791\n",
      "Evaluating Epoch 12   8.7% | batch:         8 of        92\t|\tloss: 6.05675\n",
      "Evaluating Epoch 12   9.8% | batch:         9 of        92\t|\tloss: 5.08031\n",
      "Evaluating Epoch 12  10.9% | batch:        10 of        92\t|\tloss: 5.03268\n",
      "Evaluating Epoch 12  12.0% | batch:        11 of        92\t|\tloss: 4.19901\n",
      "Evaluating Epoch 12  13.0% | batch:        12 of        92\t|\tloss: 7.40856\n",
      "Evaluating Epoch 12  14.1% | batch:        13 of        92\t|\tloss: 5.91779\n",
      "Evaluating Epoch 12  15.2% | batch:        14 of        92\t|\tloss: 2.21923\n",
      "Evaluating Epoch 12  16.3% | batch:        15 of        92\t|\tloss: 1.24788\n",
      "Evaluating Epoch 12  17.4% | batch:        16 of        92\t|\tloss: 3.63345\n",
      "Evaluating Epoch 12  18.5% | batch:        17 of        92\t|\tloss: 2.16983\n",
      "Evaluating Epoch 12  19.6% | batch:        18 of        92\t|\tloss: 3.83249\n",
      "Evaluating Epoch 12  20.7% | batch:        19 of        92\t|\tloss: 5.8231\n",
      "Evaluating Epoch 12  21.7% | batch:        20 of        92\t|\tloss: 2.8739\n",
      "Evaluating Epoch 12  22.8% | batch:        21 of        92\t|\tloss: 4.05722\n",
      "Evaluating Epoch 12  23.9% | batch:        22 of        92\t|\tloss: 5.60312\n",
      "Evaluating Epoch 12  25.0% | batch:        23 of        92\t|\tloss: 6.74755\n",
      "Evaluating Epoch 12  26.1% | batch:        24 of        92\t|\tloss: 2.4771\n",
      "Evaluating Epoch 12  27.2% | batch:        25 of        92\t|\tloss: 0.468867\n",
      "Evaluating Epoch 12  28.3% | batch:        26 of        92\t|\tloss: 1.81421\n",
      "Evaluating Epoch 12  29.3% | batch:        27 of        92\t|\tloss: 3.85481\n",
      "Evaluating Epoch 12  30.4% | batch:        28 of        92\t|\tloss: 3.91219\n",
      "Evaluating Epoch 12  31.5% | batch:        29 of        92\t|\tloss: 3.22982\n",
      "Evaluating Epoch 12  32.6% | batch:        30 of        92\t|\tloss: 3.62059\n",
      "Evaluating Epoch 12  33.7% | batch:        31 of        92\t|\tloss: 3.81167\n",
      "Evaluating Epoch 12  34.8% | batch:        32 of        92\t|\tloss: 3.43911\n",
      "Evaluating Epoch 12  35.9% | batch:        33 of        92\t|\tloss: 5.4899\n",
      "Evaluating Epoch 12  37.0% | batch:        34 of        92\t|\tloss: 4.2078\n",
      "Evaluating Epoch 12  38.0% | batch:        35 of        92\t|\tloss: 3.46348\n",
      "Evaluating Epoch 12  39.1% | batch:        36 of        92\t|\tloss: 2.32196\n",
      "Evaluating Epoch 12  40.2% | batch:        37 of        92\t|\tloss: 3.40994\n",
      "Evaluating Epoch 12  41.3% | batch:        38 of        92\t|\tloss: 3.08348\n",
      "Evaluating Epoch 12  42.4% | batch:        39 of        92\t|\tloss: 7.00506\n",
      "Evaluating Epoch 12  43.5% | batch:        40 of        92\t|\tloss: 3.06073\n",
      "Evaluating Epoch 12  44.6% | batch:        41 of        92\t|\tloss: 5.03613\n",
      "Evaluating Epoch 12  45.7% | batch:        42 of        92\t|\tloss: 5.11782\n",
      "Evaluating Epoch 12  46.7% | batch:        43 of        92\t|\tloss: 7.16106\n",
      "Evaluating Epoch 12  47.8% | batch:        44 of        92\t|\tloss: 2.57823\n",
      "Evaluating Epoch 12  48.9% | batch:        45 of        92\t|\tloss: 2.00055\n",
      "Evaluating Epoch 12  50.0% | batch:        46 of        92\t|\tloss: 2.51243\n",
      "Evaluating Epoch 12  51.1% | batch:        47 of        92\t|\tloss: 4.34334\n",
      "Evaluating Epoch 12  52.2% | batch:        48 of        92\t|\tloss: 6.24624\n",
      "Evaluating Epoch 12  53.3% | batch:        49 of        92\t|\tloss: 3.97459\n",
      "Evaluating Epoch 12  54.3% | batch:        50 of        92\t|\tloss: 4.9672\n",
      "Evaluating Epoch 12  55.4% | batch:        51 of        92\t|\tloss: 6.79429\n",
      "Evaluating Epoch 12  56.5% | batch:        52 of        92\t|\tloss: 6.32631\n",
      "Evaluating Epoch 12  57.6% | batch:        53 of        92\t|\tloss: 2.16174\n",
      "Evaluating Epoch 12  58.7% | batch:        54 of        92\t|\tloss: 2.35946\n",
      "Evaluating Epoch 12  59.8% | batch:        55 of        92\t|\tloss: 5.29706\n",
      "Evaluating Epoch 12  60.9% | batch:        56 of        92\t|\tloss: 6.88739\n",
      "Evaluating Epoch 12  62.0% | batch:        57 of        92\t|\tloss: 4.28981\n",
      "Evaluating Epoch 12  63.0% | batch:        58 of        92\t|\tloss: 4.57449\n",
      "Evaluating Epoch 12  64.1% | batch:        59 of        92\t|\tloss: 6.81599\n",
      "Evaluating Epoch 12  65.2% | batch:        60 of        92\t|\tloss: 6.38907\n",
      "Evaluating Epoch 12  66.3% | batch:        61 of        92\t|\tloss: 2.37314\n",
      "Evaluating Epoch 12  67.4% | batch:        62 of        92\t|\tloss: 0.833732\n",
      "Evaluating Epoch 12  68.5% | batch:        63 of        92\t|\tloss: 1.9799\n",
      "Evaluating Epoch 12  69.6% | batch:        64 of        92\t|\tloss: 4.68224\n",
      "Evaluating Epoch 12  70.7% | batch:        65 of        92\t|\tloss: 8.24431\n",
      "Evaluating Epoch 12  71.7% | batch:        66 of        92\t|\tloss: 3.02302\n",
      "Evaluating Epoch 12  72.8% | batch:        67 of        92\t|\tloss: 4.90787\n",
      "Evaluating Epoch 12  73.9% | batch:        68 of        92\t|\tloss: 4.54619\n",
      "Evaluating Epoch 12  75.0% | batch:        69 of        92\t|\tloss: 4.05944\n",
      "Evaluating Epoch 12  76.1% | batch:        70 of        92\t|\tloss: 6.2267\n",
      "Evaluating Epoch 12  77.2% | batch:        71 of        92\t|\tloss: 4.06442\n",
      "Evaluating Epoch 12  78.3% | batch:        72 of        92\t|\tloss: 4.36062\n",
      "Evaluating Epoch 12  79.3% | batch:        73 of        92\t|\tloss: 3.00039\n",
      "Evaluating Epoch 12  80.4% | batch:        74 of        92\t|\tloss: 5.30908\n",
      "Evaluating Epoch 12  81.5% | batch:        75 of        92\t|\tloss: 2.06849\n",
      "Evaluating Epoch 12  82.6% | batch:        76 of        92\t|\tloss: 3.78072\n",
      "Evaluating Epoch 12  83.7% | batch:        77 of        92\t|\tloss: 5.71667\n",
      "Evaluating Epoch 12  84.8% | batch:        78 of        92\t|\tloss: 4.10441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:23:28,972 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.104506254196167 seconds\n",
      "\n",
      "2023-05-24 10:23:28,972 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.2869288325309753 seconds\n",
      "2023-05-24 10:23:28,972 | INFO : Avg batch val. time: 0.013988356875336689 seconds\n",
      "2023-05-24 10:23:28,973 | INFO : Avg sample val. time: 0.00011031448933061679 seconds\n",
      "2023-05-24 10:23:28,973 | INFO : Epoch 12 Validation Summary: epoch: 12.000000 | loss: 4.307543 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 12  85.9% | batch:        79 of        92\t|\tloss: 5.29055\n",
      "Evaluating Epoch 12  87.0% | batch:        80 of        92\t|\tloss: 5.1877\n",
      "Evaluating Epoch 12  88.0% | batch:        81 of        92\t|\tloss: 6.19263\n",
      "Evaluating Epoch 12  89.1% | batch:        82 of        92\t|\tloss: 1.97303\n",
      "Evaluating Epoch 12  90.2% | batch:        83 of        92\t|\tloss: 2.26339\n",
      "Evaluating Epoch 12  91.3% | batch:        84 of        92\t|\tloss: 4.14302\n",
      "Evaluating Epoch 12  92.4% | batch:        85 of        92\t|\tloss: 6.19046\n",
      "Evaluating Epoch 12  93.5% | batch:        86 of        92\t|\tloss: 3.94745\n",
      "Evaluating Epoch 12  94.6% | batch:        87 of        92\t|\tloss: 4.30146\n",
      "Evaluating Epoch 12  95.7% | batch:        88 of        92\t|\tloss: 6.09488\n",
      "Evaluating Epoch 12  96.7% | batch:        89 of        92\t|\tloss: 6.4531\n",
      "Evaluating Epoch 12  97.8% | batch:        90 of        92\t|\tloss: 2.42761\n",
      "Evaluating Epoch 12  98.9% | batch:        91 of        92\t|\tloss: 0.278697\n",
      "\n",
      "Training Epoch 13   0.0% | batch:         0 of       772\t|\tloss: 0.524694\n",
      "Training Epoch 13   0.1% | batch:         1 of       772\t|\tloss: 0.646026\n",
      "Training Epoch 13   0.3% | batch:         2 of       772\t|\tloss: 0.42907\n",
      "Training Epoch 13   0.4% | batch:         3 of       772\t|\tloss: 0.564317\n",
      "Training Epoch 13   0.5% | batch:         4 of       772\t|\tloss: 0.729716\n",
      "Training Epoch 13   0.6% | batch:         5 of       772\t|\tloss: 0.824705\n",
      "Training Epoch 13   0.8% | batch:         6 of       772\t|\tloss: 0.549616\n",
      "Training Epoch 13   0.9% | batch:         7 of       772\t|\tloss: 0.490332\n",
      "Training Epoch 13   1.0% | batch:         8 of       772\t|\tloss: 0.519383\n",
      "Training Epoch 13   1.2% | batch:         9 of       772\t|\tloss: 0.411552\n",
      "Training Epoch 13   1.3% | batch:        10 of       772\t|\tloss: 0.464768\n",
      "Training Epoch 13   1.4% | batch:        11 of       772\t|\tloss: 0.606597\n",
      "Training Epoch 13   1.6% | batch:        12 of       772\t|\tloss: 0.393342\n",
      "Training Epoch 13   1.7% | batch:        13 of       772\t|\tloss: 0.543048\n",
      "Training Epoch 13   1.8% | batch:        14 of       772\t|\tloss: 0.495137\n",
      "Training Epoch 13   1.9% | batch:        15 of       772\t|\tloss: 0.548982\n",
      "Training Epoch 13   2.1% | batch:        16 of       772\t|\tloss: 0.372725\n",
      "Training Epoch 13   2.2% | batch:        17 of       772\t|\tloss: 0.37353\n",
      "Training Epoch 13   2.3% | batch:        18 of       772\t|\tloss: 0.413398\n",
      "Training Epoch 13   2.5% | batch:        19 of       772\t|\tloss: 0.701538\n",
      "Training Epoch 13   2.6% | batch:        20 of       772\t|\tloss: 0.452931\n",
      "Training Epoch 13   2.7% | batch:        21 of       772\t|\tloss: 0.554965\n",
      "Training Epoch 13   2.8% | batch:        22 of       772\t|\tloss: 0.393329\n",
      "Training Epoch 13   3.0% | batch:        23 of       772\t|\tloss: 0.616284\n",
      "Training Epoch 13   3.1% | batch:        24 of       772\t|\tloss: 0.440471\n",
      "Training Epoch 13   3.2% | batch:        25 of       772\t|\tloss: 0.511035\n",
      "Training Epoch 13   3.4% | batch:        26 of       772\t|\tloss: 0.501339\n",
      "Training Epoch 13   3.5% | batch:        27 of       772\t|\tloss: 0.635713\n",
      "Training Epoch 13   3.6% | batch:        28 of       772\t|\tloss: 0.741508\n",
      "Training Epoch 13   3.8% | batch:        29 of       772\t|\tloss: 0.47011\n",
      "Training Epoch 13   3.9% | batch:        30 of       772\t|\tloss: 0.582837\n",
      "Training Epoch 13   4.0% | batch:        31 of       772\t|\tloss: 0.596439\n",
      "Training Epoch 13   4.1% | batch:        32 of       772\t|\tloss: 0.443142\n",
      "Training Epoch 13   4.3% | batch:        33 of       772\t|\tloss: 0.467025\n",
      "Training Epoch 13   4.4% | batch:        34 of       772\t|\tloss: 0.502797\n",
      "Training Epoch 13   4.5% | batch:        35 of       772\t|\tloss: 0.386486\n",
      "Training Epoch 13   4.7% | batch:        36 of       772\t|\tloss: 1.0447\n",
      "Training Epoch 13   4.8% | batch:        37 of       772\t|\tloss: 0.819504\n",
      "Training Epoch 13   4.9% | batch:        38 of       772\t|\tloss: 0.890431\n",
      "Training Epoch 13   5.1% | batch:        39 of       772\t|\tloss: 0.802002\n",
      "Training Epoch 13   5.2% | batch:        40 of       772\t|\tloss: 0.52975\n",
      "Training Epoch 13   5.3% | batch:        41 of       772\t|\tloss: 0.811921\n",
      "Training Epoch 13   5.4% | batch:        42 of       772\t|\tloss: 0.371749\n",
      "Training Epoch 13   5.6% | batch:        43 of       772\t|\tloss: 0.604706\n",
      "Training Epoch 13   5.7% | batch:        44 of       772\t|\tloss: 0.856433\n",
      "Training Epoch 13   5.8% | batch:        45 of       772\t|\tloss: 0.45787\n",
      "Training Epoch 13   6.0% | batch:        46 of       772\t|\tloss: 0.588273\n",
      "Training Epoch 13   6.1% | batch:        47 of       772\t|\tloss: 0.627511\n",
      "Training Epoch 13   6.2% | batch:        48 of       772\t|\tloss: 0.791421\n",
      "Training Epoch 13   6.3% | batch:        49 of       772\t|\tloss: 0.541321\n",
      "Training Epoch 13   6.5% | batch:        50 of       772\t|\tloss: 0.757184\n",
      "Training Epoch 13   6.6% | batch:        51 of       772\t|\tloss: 0.612752\n",
      "Training Epoch 13   6.7% | batch:        52 of       772\t|\tloss: 0.442259\n",
      "Training Epoch 13   6.9% | batch:        53 of       772\t|\tloss: 0.578412\n",
      "Training Epoch 13   7.0% | batch:        54 of       772\t|\tloss: 0.581119\n",
      "Training Epoch 13   7.1% | batch:        55 of       772\t|\tloss: 0.524585\n",
      "Training Epoch 13   7.3% | batch:        56 of       772\t|\tloss: 0.509403\n",
      "Training Epoch 13   7.4% | batch:        57 of       772\t|\tloss: 0.513666\n",
      "Training Epoch 13   7.5% | batch:        58 of       772\t|\tloss: 0.371266\n",
      "Training Epoch 13   7.6% | batch:        59 of       772\t|\tloss: 1.26894\n",
      "Training Epoch 13   7.8% | batch:        60 of       772\t|\tloss: 0.512506\n",
      "Training Epoch 13   7.9% | batch:        61 of       772\t|\tloss: 0.423548\n",
      "Training Epoch 13   8.0% | batch:        62 of       772\t|\tloss: 0.395608\n",
      "Training Epoch 13   8.2% | batch:        63 of       772\t|\tloss: 0.567737\n",
      "Training Epoch 13   8.3% | batch:        64 of       772\t|\tloss: 0.330191\n",
      "Training Epoch 13   8.4% | batch:        65 of       772\t|\tloss: 0.463181\n",
      "Training Epoch 13   8.5% | batch:        66 of       772\t|\tloss: 0.592975\n",
      "Training Epoch 13   8.7% | batch:        67 of       772\t|\tloss: 0.70276\n",
      "Training Epoch 13   8.8% | batch:        68 of       772\t|\tloss: 0.598953\n",
      "Training Epoch 13   8.9% | batch:        69 of       772\t|\tloss: 0.441994\n",
      "Training Epoch 13   9.1% | batch:        70 of       772\t|\tloss: 0.536145\n",
      "Training Epoch 13   9.2% | batch:        71 of       772\t|\tloss: 0.39039\n",
      "Training Epoch 13   9.3% | batch:        72 of       772\t|\tloss: 0.421259\n",
      "Training Epoch 13   9.5% | batch:        73 of       772\t|\tloss: 0.396239\n",
      "Training Epoch 13   9.6% | batch:        74 of       772\t|\tloss: 0.703003\n",
      "Training Epoch 13   9.7% | batch:        75 of       772\t|\tloss: 0.444708\n",
      "Training Epoch 13   9.8% | batch:        76 of       772\t|\tloss: 0.436793\n",
      "Training Epoch 13  10.0% | batch:        77 of       772\t|\tloss: 0.601843\n",
      "Training Epoch 13  10.1% | batch:        78 of       772\t|\tloss: 0.491282\n",
      "Training Epoch 13  10.2% | batch:        79 of       772\t|\tloss: 0.421465\n",
      "Training Epoch 13  10.4% | batch:        80 of       772\t|\tloss: 0.598212\n",
      "Training Epoch 13  10.5% | batch:        81 of       772\t|\tloss: 0.502707\n",
      "Training Epoch 13  10.6% | batch:        82 of       772\t|\tloss: 0.716588\n",
      "Training Epoch 13  10.8% | batch:        83 of       772\t|\tloss: 0.464479\n",
      "Training Epoch 13  10.9% | batch:        84 of       772\t|\tloss: 0.457683\n",
      "Training Epoch 13  11.0% | batch:        85 of       772\t|\tloss: 0.380009\n",
      "Training Epoch 13  11.1% | batch:        86 of       772\t|\tloss: 0.435359\n",
      "Training Epoch 13  11.3% | batch:        87 of       772\t|\tloss: 0.742872\n",
      "Training Epoch 13  11.4% | batch:        88 of       772\t|\tloss: 0.931685\n",
      "Training Epoch 13  11.5% | batch:        89 of       772\t|\tloss: 0.450244\n",
      "Training Epoch 13  11.7% | batch:        90 of       772\t|\tloss: 0.713303\n",
      "Training Epoch 13  11.8% | batch:        91 of       772\t|\tloss: 0.319134\n",
      "Training Epoch 13  11.9% | batch:        92 of       772\t|\tloss: 0.627283\n",
      "Training Epoch 13  12.0% | batch:        93 of       772\t|\tloss: 0.505197\n",
      "Training Epoch 13  12.2% | batch:        94 of       772\t|\tloss: 0.446366\n",
      "Training Epoch 13  12.3% | batch:        95 of       772\t|\tloss: 0.919301\n",
      "Training Epoch 13  12.4% | batch:        96 of       772\t|\tloss: 0.624776\n",
      "Training Epoch 13  12.6% | batch:        97 of       772\t|\tloss: 0.822303\n",
      "Training Epoch 13  12.7% | batch:        98 of       772\t|\tloss: 0.530825\n",
      "Training Epoch 13  12.8% | batch:        99 of       772\t|\tloss: 0.398983\n",
      "Training Epoch 13  13.0% | batch:       100 of       772\t|\tloss: 0.394326\n",
      "Training Epoch 13  13.1% | batch:       101 of       772\t|\tloss: 0.789652\n",
      "Training Epoch 13  13.2% | batch:       102 of       772\t|\tloss: 0.646574\n",
      "Training Epoch 13  13.3% | batch:       103 of       772\t|\tloss: 0.492349\n",
      "Training Epoch 13  13.5% | batch:       104 of       772\t|\tloss: 0.398429\n",
      "Training Epoch 13  13.6% | batch:       105 of       772\t|\tloss: 0.394254\n",
      "Training Epoch 13  13.7% | batch:       106 of       772\t|\tloss: 0.347778\n",
      "Training Epoch 13  13.9% | batch:       107 of       772\t|\tloss: 0.428714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 13  14.0% | batch:       108 of       772\t|\tloss: 0.465403\n",
      "Training Epoch 13  14.1% | batch:       109 of       772\t|\tloss: 0.410416\n",
      "Training Epoch 13  14.2% | batch:       110 of       772\t|\tloss: 0.445914\n",
      "Training Epoch 13  14.4% | batch:       111 of       772\t|\tloss: 0.399168\n",
      "Training Epoch 13  14.5% | batch:       112 of       772\t|\tloss: 0.496543\n",
      "Training Epoch 13  14.6% | batch:       113 of       772\t|\tloss: 0.445918\n",
      "Training Epoch 13  14.8% | batch:       114 of       772\t|\tloss: 0.633289\n",
      "Training Epoch 13  14.9% | batch:       115 of       772\t|\tloss: 0.490364\n",
      "Training Epoch 13  15.0% | batch:       116 of       772\t|\tloss: 0.403562\n",
      "Training Epoch 13  15.2% | batch:       117 of       772\t|\tloss: 0.526389\n",
      "Training Epoch 13  15.3% | batch:       118 of       772\t|\tloss: 0.569964\n",
      "Training Epoch 13  15.4% | batch:       119 of       772\t|\tloss: 0.520461\n",
      "Training Epoch 13  15.5% | batch:       120 of       772\t|\tloss: 0.507012\n",
      "Training Epoch 13  15.7% | batch:       121 of       772\t|\tloss: 0.491002\n",
      "Training Epoch 13  15.8% | batch:       122 of       772\t|\tloss: 0.546388\n",
      "Training Epoch 13  15.9% | batch:       123 of       772\t|\tloss: 0.637582\n",
      "Training Epoch 13  16.1% | batch:       124 of       772\t|\tloss: 0.407513\n",
      "Training Epoch 13  16.2% | batch:       125 of       772\t|\tloss: 0.468835\n",
      "Training Epoch 13  16.3% | batch:       126 of       772\t|\tloss: 0.457351\n",
      "Training Epoch 13  16.5% | batch:       127 of       772\t|\tloss: 0.427217\n",
      "Training Epoch 13  16.6% | batch:       128 of       772\t|\tloss: 0.589403\n",
      "Training Epoch 13  16.7% | batch:       129 of       772\t|\tloss: 0.623871\n",
      "Training Epoch 13  16.8% | batch:       130 of       772\t|\tloss: 0.583392\n",
      "Training Epoch 13  17.0% | batch:       131 of       772\t|\tloss: 0.420236\n",
      "Training Epoch 13  17.1% | batch:       132 of       772\t|\tloss: 0.502774\n",
      "Training Epoch 13  17.2% | batch:       133 of       772\t|\tloss: 0.540874\n",
      "Training Epoch 13  17.4% | batch:       134 of       772\t|\tloss: 0.436078\n",
      "Training Epoch 13  17.5% | batch:       135 of       772\t|\tloss: 0.430726\n",
      "Training Epoch 13  17.6% | batch:       136 of       772\t|\tloss: 0.607145\n",
      "Training Epoch 13  17.7% | batch:       137 of       772\t|\tloss: 0.433354\n",
      "Training Epoch 13  17.9% | batch:       138 of       772\t|\tloss: 0.535519\n",
      "Training Epoch 13  18.0% | batch:       139 of       772\t|\tloss: 0.398298\n",
      "Training Epoch 13  18.1% | batch:       140 of       772\t|\tloss: 0.731054\n",
      "Training Epoch 13  18.3% | batch:       141 of       772\t|\tloss: 0.530487\n",
      "Training Epoch 13  18.4% | batch:       142 of       772\t|\tloss: 0.526844\n",
      "Training Epoch 13  18.5% | batch:       143 of       772\t|\tloss: 0.650993\n",
      "Training Epoch 13  18.7% | batch:       144 of       772\t|\tloss: 0.608752\n",
      "Training Epoch 13  18.8% | batch:       145 of       772\t|\tloss: 0.629363\n",
      "Training Epoch 13  18.9% | batch:       146 of       772\t|\tloss: 0.40974\n",
      "Training Epoch 13  19.0% | batch:       147 of       772\t|\tloss: 0.521612\n",
      "Training Epoch 13  19.2% | batch:       148 of       772\t|\tloss: 0.401302\n",
      "Training Epoch 13  19.3% | batch:       149 of       772\t|\tloss: 0.431075\n",
      "Training Epoch 13  19.4% | batch:       150 of       772\t|\tloss: 0.965699\n",
      "Training Epoch 13  19.6% | batch:       151 of       772\t|\tloss: 0.550242\n",
      "Training Epoch 13  19.7% | batch:       152 of       772\t|\tloss: 0.521278\n",
      "Training Epoch 13  19.8% | batch:       153 of       772\t|\tloss: 0.393357\n",
      "Training Epoch 13  19.9% | batch:       154 of       772\t|\tloss: 0.418084\n",
      "Training Epoch 13  20.1% | batch:       155 of       772\t|\tloss: 1.06497\n",
      "Training Epoch 13  20.2% | batch:       156 of       772\t|\tloss: 0.844424\n",
      "Training Epoch 13  20.3% | batch:       157 of       772\t|\tloss: 0.639027\n",
      "Training Epoch 13  20.5% | batch:       158 of       772\t|\tloss: 0.678533\n",
      "Training Epoch 13  20.6% | batch:       159 of       772\t|\tloss: 0.346271\n",
      "Training Epoch 13  20.7% | batch:       160 of       772\t|\tloss: 1.07544\n",
      "Training Epoch 13  20.9% | batch:       161 of       772\t|\tloss: 0.460014\n",
      "Training Epoch 13  21.0% | batch:       162 of       772\t|\tloss: 0.636689\n",
      "Training Epoch 13  21.1% | batch:       163 of       772\t|\tloss: 0.475843\n",
      "Training Epoch 13  21.2% | batch:       164 of       772\t|\tloss: 0.567668\n",
      "Training Epoch 13  21.4% | batch:       165 of       772\t|\tloss: 0.44719\n",
      "Training Epoch 13  21.5% | batch:       166 of       772\t|\tloss: 0.430425\n",
      "Training Epoch 13  21.6% | batch:       167 of       772\t|\tloss: 0.570965\n",
      "Training Epoch 13  21.8% | batch:       168 of       772\t|\tloss: 0.76938\n",
      "Training Epoch 13  21.9% | batch:       169 of       772\t|\tloss: 0.669954\n",
      "Training Epoch 13  22.0% | batch:       170 of       772\t|\tloss: 0.448669\n",
      "Training Epoch 13  22.2% | batch:       171 of       772\t|\tloss: 0.39303\n",
      "Training Epoch 13  22.3% | batch:       172 of       772\t|\tloss: 0.580938\n",
      "Training Epoch 13  22.4% | batch:       173 of       772\t|\tloss: 0.559585\n",
      "Training Epoch 13  22.5% | batch:       174 of       772\t|\tloss: 0.557508\n",
      "Training Epoch 13  22.7% | batch:       175 of       772\t|\tloss: 0.444106\n",
      "Training Epoch 13  22.8% | batch:       176 of       772\t|\tloss: 0.4646\n",
      "Training Epoch 13  22.9% | batch:       177 of       772\t|\tloss: 0.616517\n",
      "Training Epoch 13  23.1% | batch:       178 of       772\t|\tloss: 0.50556\n",
      "Training Epoch 13  23.2% | batch:       179 of       772\t|\tloss: 0.606151\n",
      "Training Epoch 13  23.3% | batch:       180 of       772\t|\tloss: 0.602575\n",
      "Training Epoch 13  23.4% | batch:       181 of       772\t|\tloss: 0.5786\n",
      "Training Epoch 13  23.6% | batch:       182 of       772\t|\tloss: 0.76298\n",
      "Training Epoch 13  23.7% | batch:       183 of       772\t|\tloss: 0.457025\n",
      "Training Epoch 13  23.8% | batch:       184 of       772\t|\tloss: 0.459801\n",
      "Training Epoch 13  24.0% | batch:       185 of       772\t|\tloss: 0.391475\n",
      "Training Epoch 13  24.1% | batch:       186 of       772\t|\tloss: 0.558583\n",
      "Training Epoch 13  24.2% | batch:       187 of       772\t|\tloss: 0.859528\n",
      "Training Epoch 13  24.4% | batch:       188 of       772\t|\tloss: 0.881458\n",
      "Training Epoch 13  24.5% | batch:       189 of       772\t|\tloss: 0.598939\n",
      "Training Epoch 13  24.6% | batch:       190 of       772\t|\tloss: 0.488559\n",
      "Training Epoch 13  24.7% | batch:       191 of       772\t|\tloss: 0.548314\n",
      "Training Epoch 13  24.9% | batch:       192 of       772\t|\tloss: 0.539302\n",
      "Training Epoch 13  25.0% | batch:       193 of       772\t|\tloss: 0.659362\n",
      "Training Epoch 13  25.1% | batch:       194 of       772\t|\tloss: 1.06391\n",
      "Training Epoch 13  25.3% | batch:       195 of       772\t|\tloss: 0.499693\n",
      "Training Epoch 13  25.4% | batch:       196 of       772\t|\tloss: 0.555516\n",
      "Training Epoch 13  25.5% | batch:       197 of       772\t|\tloss: 0.407703\n",
      "Training Epoch 13  25.6% | batch:       198 of       772\t|\tloss: 0.439826\n",
      "Training Epoch 13  25.8% | batch:       199 of       772\t|\tloss: 0.428083\n",
      "Training Epoch 13  25.9% | batch:       200 of       772\t|\tloss: 0.644543\n",
      "Training Epoch 13  26.0% | batch:       201 of       772\t|\tloss: 0.426236\n",
      "Training Epoch 13  26.2% | batch:       202 of       772\t|\tloss: 0.361129\n",
      "Training Epoch 13  26.3% | batch:       203 of       772\t|\tloss: 0.562871\n",
      "Training Epoch 13  26.4% | batch:       204 of       772\t|\tloss: 0.559121\n",
      "Training Epoch 13  26.6% | batch:       205 of       772\t|\tloss: 0.698546\n",
      "Training Epoch 13  26.7% | batch:       206 of       772\t|\tloss: 0.641352\n",
      "Training Epoch 13  26.8% | batch:       207 of       772\t|\tloss: 0.611388\n",
      "Training Epoch 13  26.9% | batch:       208 of       772\t|\tloss: 0.506168\n",
      "Training Epoch 13  27.1% | batch:       209 of       772\t|\tloss: 0.554342\n",
      "Training Epoch 13  27.2% | batch:       210 of       772\t|\tloss: 0.788869\n",
      "Training Epoch 13  27.3% | batch:       211 of       772\t|\tloss: 0.393952\n",
      "Training Epoch 13  27.5% | batch:       212 of       772\t|\tloss: 0.502394\n",
      "Training Epoch 13  27.6% | batch:       213 of       772\t|\tloss: 0.371011\n",
      "Training Epoch 13  27.7% | batch:       214 of       772\t|\tloss: 0.476128\n",
      "Training Epoch 13  27.8% | batch:       215 of       772\t|\tloss: 0.398184\n",
      "Training Epoch 13  28.0% | batch:       216 of       772\t|\tloss: 0.490768\n",
      "Training Epoch 13  28.1% | batch:       217 of       772\t|\tloss: 0.713685\n",
      "Training Epoch 13  28.2% | batch:       218 of       772\t|\tloss: 0.561527\n",
      "Training Epoch 13  28.4% | batch:       219 of       772\t|\tloss: 0.520706\n",
      "Training Epoch 13  28.5% | batch:       220 of       772\t|\tloss: 0.558828\n",
      "Training Epoch 13  28.6% | batch:       221 of       772\t|\tloss: 0.369662\n",
      "Training Epoch 13  28.8% | batch:       222 of       772\t|\tloss: 0.442677\n",
      "Training Epoch 13  28.9% | batch:       223 of       772\t|\tloss: 0.734895\n",
      "Training Epoch 13  29.0% | batch:       224 of       772\t|\tloss: 0.377794\n",
      "Training Epoch 13  29.1% | batch:       225 of       772\t|\tloss: 0.549625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 13  29.3% | batch:       226 of       772\t|\tloss: 0.583471\n",
      "Training Epoch 13  29.4% | batch:       227 of       772\t|\tloss: 0.479347\n",
      "Training Epoch 13  29.5% | batch:       228 of       772\t|\tloss: 0.541638\n",
      "Training Epoch 13  29.7% | batch:       229 of       772\t|\tloss: 0.425461\n",
      "Training Epoch 13  29.8% | batch:       230 of       772\t|\tloss: 0.366412\n",
      "Training Epoch 13  29.9% | batch:       231 of       772\t|\tloss: 0.662202\n",
      "Training Epoch 13  30.1% | batch:       232 of       772\t|\tloss: 0.493608\n",
      "Training Epoch 13  30.2% | batch:       233 of       772\t|\tloss: 0.41811\n",
      "Training Epoch 13  30.3% | batch:       234 of       772\t|\tloss: 0.383404\n",
      "Training Epoch 13  30.4% | batch:       235 of       772\t|\tloss: 0.782639\n",
      "Training Epoch 13  30.6% | batch:       236 of       772\t|\tloss: 0.42275\n",
      "Training Epoch 13  30.7% | batch:       237 of       772\t|\tloss: 0.488684\n",
      "Training Epoch 13  30.8% | batch:       238 of       772\t|\tloss: 0.461924\n",
      "Training Epoch 13  31.0% | batch:       239 of       772\t|\tloss: 0.526317\n",
      "Training Epoch 13  31.1% | batch:       240 of       772\t|\tloss: 0.384473\n",
      "Training Epoch 13  31.2% | batch:       241 of       772\t|\tloss: 0.460648\n",
      "Training Epoch 13  31.3% | batch:       242 of       772\t|\tloss: 0.471168\n",
      "Training Epoch 13  31.5% | batch:       243 of       772\t|\tloss: 0.680501\n",
      "Training Epoch 13  31.6% | batch:       244 of       772\t|\tloss: 0.469493\n",
      "Training Epoch 13  31.7% | batch:       245 of       772\t|\tloss: 0.572523\n",
      "Training Epoch 13  31.9% | batch:       246 of       772\t|\tloss: 0.677532\n",
      "Training Epoch 13  32.0% | batch:       247 of       772\t|\tloss: 0.417981\n",
      "Training Epoch 13  32.1% | batch:       248 of       772\t|\tloss: 0.417264\n",
      "Training Epoch 13  32.3% | batch:       249 of       772\t|\tloss: 0.467327\n",
      "Training Epoch 13  32.4% | batch:       250 of       772\t|\tloss: 0.521219\n",
      "Training Epoch 13  32.5% | batch:       251 of       772\t|\tloss: 0.475351\n",
      "Training Epoch 13  32.6% | batch:       252 of       772\t|\tloss: 0.68417\n",
      "Training Epoch 13  32.8% | batch:       253 of       772\t|\tloss: 0.631193\n",
      "Training Epoch 13  32.9% | batch:       254 of       772\t|\tloss: 0.610934\n",
      "Training Epoch 13  33.0% | batch:       255 of       772\t|\tloss: 0.666969\n",
      "Training Epoch 13  33.2% | batch:       256 of       772\t|\tloss: 0.631286\n",
      "Training Epoch 13  33.3% | batch:       257 of       772\t|\tloss: 0.480015\n",
      "Training Epoch 13  33.4% | batch:       258 of       772\t|\tloss: 0.625233\n",
      "Training Epoch 13  33.5% | batch:       259 of       772\t|\tloss: 0.743618\n",
      "Training Epoch 13  33.7% | batch:       260 of       772\t|\tloss: 0.545516\n",
      "Training Epoch 13  33.8% | batch:       261 of       772\t|\tloss: 0.586632\n",
      "Training Epoch 13  33.9% | batch:       262 of       772\t|\tloss: 0.549682\n",
      "Training Epoch 13  34.1% | batch:       263 of       772\t|\tloss: 0.653093\n",
      "Training Epoch 13  34.2% | batch:       264 of       772\t|\tloss: 0.471263\n",
      "Training Epoch 13  34.3% | batch:       265 of       772\t|\tloss: 0.570234\n",
      "Training Epoch 13  34.5% | batch:       266 of       772\t|\tloss: 0.85409\n",
      "Training Epoch 13  34.6% | batch:       267 of       772\t|\tloss: 0.446586\n",
      "Training Epoch 13  34.7% | batch:       268 of       772\t|\tloss: 0.448783\n",
      "Training Epoch 13  34.8% | batch:       269 of       772\t|\tloss: 0.483175\n",
      "Training Epoch 13  35.0% | batch:       270 of       772\t|\tloss: 0.462405\n",
      "Training Epoch 13  35.1% | batch:       271 of       772\t|\tloss: 0.504586\n",
      "Training Epoch 13  35.2% | batch:       272 of       772\t|\tloss: 0.368985\n",
      "Training Epoch 13  35.4% | batch:       273 of       772\t|\tloss: 0.51781\n",
      "Training Epoch 13  35.5% | batch:       274 of       772\t|\tloss: 0.510484\n",
      "Training Epoch 13  35.6% | batch:       275 of       772\t|\tloss: 0.454529\n",
      "Training Epoch 13  35.8% | batch:       276 of       772\t|\tloss: 0.514132\n",
      "Training Epoch 13  35.9% | batch:       277 of       772\t|\tloss: 0.458602\n",
      "Training Epoch 13  36.0% | batch:       278 of       772\t|\tloss: 0.83671\n",
      "Training Epoch 13  36.1% | batch:       279 of       772\t|\tloss: 0.80339\n",
      "Training Epoch 13  36.3% | batch:       280 of       772\t|\tloss: 0.526362\n",
      "Training Epoch 13  36.4% | batch:       281 of       772\t|\tloss: 0.536167\n",
      "Training Epoch 13  36.5% | batch:       282 of       772\t|\tloss: 0.697697\n",
      "Training Epoch 13  36.7% | batch:       283 of       772\t|\tloss: 0.605471\n",
      "Training Epoch 13  36.8% | batch:       284 of       772\t|\tloss: 0.543717\n",
      "Training Epoch 13  36.9% | batch:       285 of       772\t|\tloss: 0.647878\n",
      "Training Epoch 13  37.0% | batch:       286 of       772\t|\tloss: 0.480898\n",
      "Training Epoch 13  37.2% | batch:       287 of       772\t|\tloss: 0.488883\n",
      "Training Epoch 13  37.3% | batch:       288 of       772\t|\tloss: 0.487904\n",
      "Training Epoch 13  37.4% | batch:       289 of       772\t|\tloss: 0.604575\n",
      "Training Epoch 13  37.6% | batch:       290 of       772\t|\tloss: 0.441162\n",
      "Training Epoch 13  37.7% | batch:       291 of       772\t|\tloss: 0.431842\n",
      "Training Epoch 13  37.8% | batch:       292 of       772\t|\tloss: 0.355505\n",
      "Training Epoch 13  38.0% | batch:       293 of       772\t|\tloss: 0.429225\n",
      "Training Epoch 13  38.1% | batch:       294 of       772\t|\tloss: 0.577595\n",
      "Training Epoch 13  38.2% | batch:       295 of       772\t|\tloss: 0.549438\n",
      "Training Epoch 13  38.3% | batch:       296 of       772\t|\tloss: 0.573152\n",
      "Training Epoch 13  38.5% | batch:       297 of       772\t|\tloss: 0.640394\n",
      "Training Epoch 13  38.6% | batch:       298 of       772\t|\tloss: 0.712787\n",
      "Training Epoch 13  38.7% | batch:       299 of       772\t|\tloss: 0.546023\n",
      "Training Epoch 13  38.9% | batch:       300 of       772\t|\tloss: 0.446759\n",
      "Training Epoch 13  39.0% | batch:       301 of       772\t|\tloss: 0.926938\n",
      "Training Epoch 13  39.1% | batch:       302 of       772\t|\tloss: 0.539409\n",
      "Training Epoch 13  39.2% | batch:       303 of       772\t|\tloss: 0.619541\n",
      "Training Epoch 13  39.4% | batch:       304 of       772\t|\tloss: 0.856732\n",
      "Training Epoch 13  39.5% | batch:       305 of       772\t|\tloss: 0.551047\n",
      "Training Epoch 13  39.6% | batch:       306 of       772\t|\tloss: 0.896903\n",
      "Training Epoch 13  39.8% | batch:       307 of       772\t|\tloss: 0.746045\n",
      "Training Epoch 13  39.9% | batch:       308 of       772\t|\tloss: 0.493094\n",
      "Training Epoch 13  40.0% | batch:       309 of       772\t|\tloss: 0.742401\n",
      "Training Epoch 13  40.2% | batch:       310 of       772\t|\tloss: 0.589138\n",
      "Training Epoch 13  40.3% | batch:       311 of       772\t|\tloss: 0.911427\n",
      "Training Epoch 13  40.4% | batch:       312 of       772\t|\tloss: 0.491184\n",
      "Training Epoch 13  40.5% | batch:       313 of       772\t|\tloss: 0.421688\n",
      "Training Epoch 13  40.7% | batch:       314 of       772\t|\tloss: 0.854731\n",
      "Training Epoch 13  40.8% | batch:       315 of       772\t|\tloss: 0.526481\n",
      "Training Epoch 13  40.9% | batch:       316 of       772\t|\tloss: 0.445592\n",
      "Training Epoch 13  41.1% | batch:       317 of       772\t|\tloss: 0.461051\n",
      "Training Epoch 13  41.2% | batch:       318 of       772\t|\tloss: 0.722428\n",
      "Training Epoch 13  41.3% | batch:       319 of       772\t|\tloss: 0.446063\n",
      "Training Epoch 13  41.5% | batch:       320 of       772\t|\tloss: 0.403453\n",
      "Training Epoch 13  41.6% | batch:       321 of       772\t|\tloss: 0.409584\n",
      "Training Epoch 13  41.7% | batch:       322 of       772\t|\tloss: 0.633086\n",
      "Training Epoch 13  41.8% | batch:       323 of       772\t|\tloss: 0.406568\n",
      "Training Epoch 13  42.0% | batch:       324 of       772\t|\tloss: 0.487722\n",
      "Training Epoch 13  42.1% | batch:       325 of       772\t|\tloss: 0.473977\n",
      "Training Epoch 13  42.2% | batch:       326 of       772\t|\tloss: 0.591268\n",
      "Training Epoch 13  42.4% | batch:       327 of       772\t|\tloss: 0.77946\n",
      "Training Epoch 13  42.5% | batch:       328 of       772\t|\tloss: 0.516236\n",
      "Training Epoch 13  42.6% | batch:       329 of       772\t|\tloss: 0.432261\n",
      "Training Epoch 13  42.7% | batch:       330 of       772\t|\tloss: 0.584888\n",
      "Training Epoch 13  42.9% | batch:       331 of       772\t|\tloss: 0.771363\n",
      "Training Epoch 13  43.0% | batch:       332 of       772\t|\tloss: 0.51597\n",
      "Training Epoch 13  43.1% | batch:       333 of       772\t|\tloss: 0.456194\n",
      "Training Epoch 13  43.3% | batch:       334 of       772\t|\tloss: 0.532208\n",
      "Training Epoch 13  43.4% | batch:       335 of       772\t|\tloss: 0.481195\n",
      "Training Epoch 13  43.5% | batch:       336 of       772\t|\tloss: 0.560379\n",
      "Training Epoch 13  43.7% | batch:       337 of       772\t|\tloss: 0.549907\n",
      "Training Epoch 13  43.8% | batch:       338 of       772\t|\tloss: 0.436803\n",
      "Training Epoch 13  43.9% | batch:       339 of       772\t|\tloss: 0.428565\n",
      "Training Epoch 13  44.0% | batch:       340 of       772\t|\tloss: 0.732378\n",
      "Training Epoch 13  44.2% | batch:       341 of       772\t|\tloss: 0.538591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 13  44.3% | batch:       342 of       772\t|\tloss: 0.493555\n",
      "Training Epoch 13  44.4% | batch:       343 of       772\t|\tloss: 0.454757\n",
      "Training Epoch 13  44.6% | batch:       344 of       772\t|\tloss: 0.469336\n",
      "Training Epoch 13  44.7% | batch:       345 of       772\t|\tloss: 0.61576\n",
      "Training Epoch 13  44.8% | batch:       346 of       772\t|\tloss: 0.657864\n",
      "Training Epoch 13  44.9% | batch:       347 of       772\t|\tloss: 0.475778\n",
      "Training Epoch 13  45.1% | batch:       348 of       772\t|\tloss: 0.659907\n",
      "Training Epoch 13  45.2% | batch:       349 of       772\t|\tloss: 0.390553\n",
      "Training Epoch 13  45.3% | batch:       350 of       772\t|\tloss: 0.578506\n",
      "Training Epoch 13  45.5% | batch:       351 of       772\t|\tloss: 0.572704\n",
      "Training Epoch 13  45.6% | batch:       352 of       772\t|\tloss: 0.543027\n",
      "Training Epoch 13  45.7% | batch:       353 of       772\t|\tloss: 0.638868\n",
      "Training Epoch 13  45.9% | batch:       354 of       772\t|\tloss: 0.48437\n",
      "Training Epoch 13  46.0% | batch:       355 of       772\t|\tloss: 0.466065\n",
      "Training Epoch 13  46.1% | batch:       356 of       772\t|\tloss: 0.65047\n",
      "Training Epoch 13  46.2% | batch:       357 of       772\t|\tloss: 0.80599\n",
      "Training Epoch 13  46.4% | batch:       358 of       772\t|\tloss: 0.562891\n",
      "Training Epoch 13  46.5% | batch:       359 of       772\t|\tloss: 0.493019\n",
      "Training Epoch 13  46.6% | batch:       360 of       772\t|\tloss: 0.378955\n",
      "Training Epoch 13  46.8% | batch:       361 of       772\t|\tloss: 0.535087\n",
      "Training Epoch 13  46.9% | batch:       362 of       772\t|\tloss: 0.55029\n",
      "Training Epoch 13  47.0% | batch:       363 of       772\t|\tloss: 0.733445\n"
     ]
    }
   ],
   "source": [
    "for cv_idx in range(len(train_cv_indices)):\n",
    "    print(cv_idx)\n",
    "    train_indices = train_cv_indices[cv_idx]\n",
    "    val_indices = val_cv_indices[cv_idx]\n",
    "    with open(os.path.join(config['output_dir'], 'data_indices.json'), 'w') as f:\n",
    "        try:\n",
    "            json.dump({'train_indices': list(map(int, train_indices)),\n",
    "                       'val_indices': list(map(int, val_indices)),\n",
    "                       'test_indices': list(map(int, test_indices))}, f, indent=4)\n",
    "        except ValueError:  # in case indices are non-integers\n",
    "            json.dump({'train_indices': list(train_indices),\n",
    "                       'val_indices': list(val_indices),\n",
    "                       'test_indices': list(test_indices)}, f, indent=4)\n",
    "    \n",
    "    ## Pre-process features\n",
    "    normalizer = None\n",
    "    if config['norm_from']:\n",
    "        with open(config['norm_from'], 'rb') as f:\n",
    "            norm_dict = pickle.load(f)\n",
    "        normalizer = Normalizer(**norm_dict)\n",
    "    elif config['normalization'] is not None:\n",
    "        normalizer = Normalizer(config['normalization'])\n",
    "        my_data.feature_df.loc[train_indices] = normalizer.normalize(my_data.feature_df.loc[train_indices])\n",
    "        if not config['normalization'].startswith('per_sample'):\n",
    "            # get normalizing values from training set and store for future use\n",
    "            norm_dict = normalizer.__dict__\n",
    "            with open(os.path.join(config['output_dir'], 'normalization.pickle'), 'wb') as f:\n",
    "                pickle.dump(norm_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "    if normalizer is not None:\n",
    "        if len(val_indices):\n",
    "            val_data.feature_df.loc[val_indices] = normalizer.normalize(val_data.feature_df.loc[val_indices])\n",
    "        if len(test_indices):\n",
    "            test_data.feature_df.loc[test_indices] = normalizer.normalize(test_data.feature_df.loc[test_indices])\n",
    "            \n",
    "    ## Create model\n",
    "    logger.info(\"Creating model ...\")\n",
    "    model = model_factory(config, my_data)\n",
    "\n",
    "    if config['freeze']:\n",
    "        for name, param in model.named_parameters():\n",
    "            if name.startswith('output_layer'):\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    logger.info(\"Model:\\n{}\".format(model))\n",
    "    logger.info(\"Total number of parameters: {}\".format(utils.count_parameters(model)))\n",
    "    logger.info(\"Trainable parameters: {}\".format(utils.count_parameters(model, trainable=True)))\n",
    "    \n",
    "    \n",
    "    ## Initialize optimizer\n",
    "    if config['global_reg']:\n",
    "        weight_decay = config['l2_reg']\n",
    "        output_reg = None\n",
    "    else:\n",
    "        weight_decay = 0\n",
    "        output_reg = config['l2_reg']\n",
    "\n",
    "    optim_class = get_optimizer(config['optimizer'])\n",
    "    optimizer = optim_class(model.parameters(), lr=config['lr'], weight_decay=weight_decay)\n",
    "\n",
    "    start_epoch = 0\n",
    "    lr_step = 0  # current step index of `lr_step`\n",
    "    lr = config['lr']  # current learning step\n",
    "    # Load model and optimizer state\n",
    "    if args.load_model:\n",
    "        model, optimizer, start_epoch = utils.load_model(model, config['load_model'], optimizer, config['resume'],\n",
    "                                                         config['change_output'],\n",
    "                                                         config['lr'],\n",
    "                                                         config['lr_step'],\n",
    "                                                         config['lr_factor'])\n",
    "    model.to(device)\n",
    "\n",
    "    loss_module = get_loss_module(config)\n",
    "    \n",
    "    \n",
    "    if config['test_only'] == 'testset':  # Only evaluate and skip training\n",
    "        dataset_class, collate_fn, runner_class = pipeline_factory(config)\n",
    "        test_dataset = dataset_class(test_data, test_indices)\n",
    "\n",
    "        test_loader = DataLoader(dataset=test_dataset,\n",
    "                                 batch_size=config['batch_size'],\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=config['num_workers'],\n",
    "                                 pin_memory=True,\n",
    "                                 collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "        test_evaluator = runner_class(model, test_loader, device, loss_module,\n",
    "                                            print_interval=config['print_interval'], console=config['console'])\n",
    "        aggr_metrics_test, per_batch_test = test_evaluator.evaluate(keep_all=True)\n",
    "        print_str = 'Test Summary: '\n",
    "        for k, v in aggr_metrics_test.items():\n",
    "            print_str += '{}: {:8f} | '.format(k, v)\n",
    "        logger.info(print_str)\n",
    "        #return\n",
    "        \n",
    "    # Initialize data generators\n",
    "    if config['test_only'] != 'testset':  # Only evaluate and skip training\n",
    "        dataset_class, collate_fn, runner_class = pipeline_factory(config)\n",
    "        val_dataset = dataset_class(val_data, val_indices)\n",
    "\n",
    "        val_loader = DataLoader(dataset=val_dataset,\n",
    "                                batch_size=config['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                num_workers=config['num_workers'],\n",
    "                                pin_memory=True,\n",
    "                                collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "\n",
    "        train_dataset = dataset_class(my_data, train_indices)\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=config['batch_size'],\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=config['num_workers'],\n",
    "                                  pin_memory=True,\n",
    "                                  collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "\n",
    "        trainer = runner_class(model, train_loader, device, loss_module, optimizer, l2_reg=output_reg,\n",
    "                                     print_interval=config['print_interval'], console=config['console'])\n",
    "        val_evaluator = runner_class(model, val_loader, device, loss_module,\n",
    "                                           print_interval=config['print_interval'], console=config['console'])\n",
    "\n",
    "        tensorboard_writer = SummaryWriter(config['tensorboard_dir'])\n",
    "\n",
    "        best_value = 1e16 if config['key_metric'] in NEG_METRICS else -1e16  # initialize with +inf or -inf depending on key metric\n",
    "        metrics = []  # (for validation) list of lists: for each epoch, stores metrics like loss, ...\n",
    "        best_metrics = {}\n",
    "        \n",
    "        \n",
    "    ## Evaluate before training\n",
    "    aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, tensorboard_writer, config, best_metrics,\n",
    "                                                      best_value, epoch=0)\n",
    "    metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "    metrics.append(list(metrics_values))\n",
    "    \n",
    "    \n",
    "    ## Start training\n",
    "    logger.info('Starting training...')\n",
    "    for epoch in tqdm(range(start_epoch + 1, config[\"epochs\"] + 1), desc='Training Epoch', leave=False):\n",
    "        mark = epoch if config['save_all'] else 'last'\n",
    "        epoch_start_time = time.time()\n",
    "        # Training\n",
    "        aggr_metrics_train = trainer.train_epoch(epoch)  # dictionary of aggregate epoch metrics\n",
    "        epoch_runtime = time.time() - epoch_start_time\n",
    "        print()\n",
    "        print_str = 'Epoch {} Training Summary: '.format(epoch)\n",
    "        for k, v in aggr_metrics_train.items():\n",
    "            tensorboard_writer.add_scalar('{}/train'.format(k), v, epoch)\n",
    "            print_str += '{}: {:8f} | '.format(k, v)\n",
    "        logger.info(print_str)\n",
    "        logger.info(\"Epoch runtime: {} hours, {} minutes, {} seconds\\n\".format(*utils.readable_time(epoch_runtime)))\n",
    "        total_epoch_time += epoch_runtime\n",
    "        avg_epoch_time = total_epoch_time / (epoch - start_epoch)\n",
    "        avg_batch_time = avg_epoch_time / len(train_loader)\n",
    "        avg_sample_time = avg_epoch_time / len(train_dataset)\n",
    "        logger.info(\"Avg epoch train. time: {} hours, {} minutes, {} seconds\".format(*utils.readable_time(avg_epoch_time)))\n",
    "        logger.info(\"Avg batch train. time: {} seconds\".format(avg_batch_time))\n",
    "        logger.info(\"Avg sample train. time: {} seconds\".format(avg_sample_time))\n",
    "\n",
    "        # evaluate if first or last epoch or at specified interval\n",
    "        if (epoch == config[\"epochs\"]) or (epoch == start_epoch + 1) or (epoch % config['val_interval'] == 0):\n",
    "            aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, tensorboard_writer, config,\n",
    "                                                                  best_metrics, best_value, epoch)\n",
    "            metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "            metrics.append(list(metrics_values))\n",
    "\n",
    "        utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(mark)), epoch, model, optimizer)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        if epoch == config['lr_step'][lr_step]:\n",
    "            utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(epoch)), epoch, model, optimizer)\n",
    "            lr = lr * config['lr_factor'][lr_step]\n",
    "            if lr_step < len(config['lr_step']) - 1:  # so that this index does not get out of bounds\n",
    "                lr_step += 1\n",
    "            logger.info('Learning rate updated to: ', lr)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "        # Difficulty scheduling\n",
    "        if config['harden'] and check_progress(epoch):\n",
    "            train_loader.dataset.update()\n",
    "            val_loader.dataset.update()\n",
    "            \n",
    "    # Export evolution of metrics over epochs\n",
    "    header = metrics_names\n",
    "    metrics_filepath = os.path.join(config[\"output_dir\"], \"metrics_\" + config[\"experiment_name\"] + \".xls\")\n",
    "    book = utils.export_performance_metrics(metrics_filepath, metrics, header, sheet_name=\"metrics\")\n",
    "\n",
    "    # Export record metrics to a file accumulating records from all experiments\n",
    "    utils.register_record(config[\"records_file\"], config[\"initial_timestamp\"], config[\"experiment_name\"],\n",
    "                          best_metrics, aggr_metrics_val, comment=config['comment'])\n",
    "\n",
    "    logger.info('Best {} was {}. Other metrics: {}'.format(config['key_metric'], best_value, best_metrics))\n",
    "    logger.info('All Done!')\n",
    "\n",
    "    total_runtime = time.time() - total_start_time\n",
    "    logger.info(\"Total runtime: {} hours, {} minutes, {} seconds\\n\".format(*utils.readable_time(total_runtime)))\n",
    "\n",
    "    #return best_value\n",
    "    print(best_value)\n",
    "    \n",
    "    source_path = config['pred_dir'] + '/best_predictions.npz'\n",
    "    target_path = config['pred_dir'] + \"/best_predictions_{}.npz\".format(cv_idx)\n",
    "    os.rename(source_path, target_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0993823",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(source_path, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ef48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = config['pred_dir'] + '/best_predictions.npz'\n",
    "target_path = config['pred_dir'] + \"/best_predictions_{}.npz\".format(cv_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b23c0e",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70927512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_path = config['pred_dir'] + \"/best_predictions_{}.npz\".format(idx)\n",
    "total_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['pred_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['output_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ed5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.load(total_path, allow_pickle=True)\n",
    "pred.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd526eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate(pred[\"targets\"], axis=0)\n",
    "y_pred = np.concatenate(pred[\"predictions\"], axis=0)\n",
    "IDs = np.concatenate(pred[\"IDs\"], axis=0)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256fc222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mape(y, y_pred):\n",
    "    err = y - y_pred\n",
    "    return np.mean(np.abs(err)/y)\n",
    "\n",
    "def get_mse(y, y_pred):\n",
    "    err = y - y_pred\n",
    "    return np.mean(np.square(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ce45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mse(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f604a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mape(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172f8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef8e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_output(y, y_pred, title=' '):\n",
    "    fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(14, 6))\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    \n",
    "    ax0.plot(y, color='g', label='truth')\n",
    "    ax0.set_title(title)\n",
    "    ax0.set_xlabel('Step')\n",
    "    ax0.set_ylabel('Error')\n",
    "    ax0.grid()\n",
    "    ax0.legend()\n",
    "    \n",
    "    ax1.plot(y, color='g', label='truth')\n",
    "    ax1.plot(y_pred, color='b', alpha=0.7, label='predict')\n",
    "    mse = get_mse(y, y_pred)\n",
    "    mape = get_mape(y, y_pred)\n",
    "    ax1.set_title(title+\"- mse: {:.5f} | mape: {:.5f}\".format(mse, mape))\n",
    "    ax1.set_xlabel('Step')\n",
    "    ax1.set_ylabel('Error')\n",
    "    ax1.grid()\n",
    "    ax1.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e54b0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_output(y, y_pred, title='Prediction on SenseTime Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2dffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3911d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb206dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c26533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
