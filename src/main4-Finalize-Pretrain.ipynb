{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40db25f3",
   "metadata": {},
   "source": [
    "# Pose Error Project\n",
    "### (Transformer-Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d884a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 21:58:29,668 | INFO : Loading packages ...\n",
      "2023-05-25 21:58:31,386 | INFO : Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-05-25 21:58:31,387 | INFO : NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Loading packages ...\")\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# 3rd party packages\n",
    "\n",
    "#from tqdm import tqdm\n",
    "# since we are using it in jupyter notebook\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Project modules\n",
    "from options import Options\n",
    "from running import setup, pipeline_factory, validate, check_progress, NEG_METRICS\n",
    "from utils import utils\n",
    "from datasets.data import data_factory, Normalizer\n",
    "from datasets.datasplit import split_dataset\n",
    "from models.ts_transformer import model_factory\n",
    "from models.loss import get_loss_module\n",
    "from optimizers import get_optimizer\n",
    "\n",
    "import parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871bc365",
   "metadata": {},
   "source": [
    "# Setup Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036bf287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting 2 - Two Stages\n",
    "# Pretrain\n",
    "# text = \"--output_dir ../experiments/ --comment 'poseErrorPred_pretrain' \\\n",
    "#         --name poseErrorPred_preTrain --records_file Regression_records.xls \\\n",
    "#         --data_dir ../data/Hall_LivingRoom_Pretrain/ --data_class pose \\\n",
    "#         --val_ratio 0.2 --epochs 30 --lr 0.00001 --optimizer RAdam --batch_size 128 \\\n",
    "#         --pos_encoding learnable --task regression --print_interval 1\\\n",
    "#         --num_layers 3  --num_heads 8 --d_model 64 --dim_feedforward 256 --gpu 2 --val_interval 1\"\n",
    "\n",
    "text = \"--output_dir ../experiments/ --comment 'poseErrorPred_pretrain' \\\n",
    "        --name poseErrorPred_preTrain --records_file Regression_records.xls \\\n",
    "        --data_dir ../data/SenseTimeV4_Selective/ --data_class pose \\\n",
    "        --val_ratio 0.2 --epochs 30 --lr 0.00001 --optimizer RAdam --batch_size 128 \\\n",
    "        --pos_encoding learnable --task regression --print_interval 1\\\n",
    "        --num_layers 3  --num_heads 8 --d_model 64 --dim_feedforward 256 --gpu 2 --val_interval 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e99d546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 21:58:39,394 | INFO : Stored configuration file in '../experiments/poseErrorPred_preTrain_2023-05-25_21-58-39_T27'\n"
     ]
    }
   ],
   "source": [
    "# Process the setting string\n",
    "# Generate the config variable\n",
    "input_text = text.split()\n",
    "args = Options().parse(input_text)\n",
    "config = setup(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f6877ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_filepath': None,\n",
       " 'output_dir': '../experiments/poseErrorPred_preTrain_2023-05-25_21-58-39_T27',\n",
       " 'data_dir': '../data/SenseTimeV4_Selective/',\n",
       " 'load_model': None,\n",
       " 'resume': False,\n",
       " 'change_output': False,\n",
       " 'save_all': False,\n",
       " 'experiment_name': 'poseErrorPred_preTrain',\n",
       " 'comment': \"'poseErrorPred_pretrain'\",\n",
       " 'no_timestamp': False,\n",
       " 'records_file': 'Regression_records.xls',\n",
       " 'console': False,\n",
       " 'print_interval': 1,\n",
       " 'gpu': '2',\n",
       " 'n_proc': -1,\n",
       " 'num_workers': 0,\n",
       " 'seed': None,\n",
       " 'limit_size': None,\n",
       " 'test_only': None,\n",
       " 'data_class': 'pose',\n",
       " 'labels': None,\n",
       " 'test_from': None,\n",
       " 'test_ratio': 0,\n",
       " 'val_ratio': 0.2,\n",
       " 'pattern': None,\n",
       " 'val_pattern': None,\n",
       " 'test_pattern': None,\n",
       " 'normalization': 'standardization',\n",
       " 'norm_from': None,\n",
       " 'subsample_factor': None,\n",
       " 'task': 'regression',\n",
       " 'masking_ratio': 0.15,\n",
       " 'mean_mask_length': 3,\n",
       " 'mask_mode': 'separate',\n",
       " 'mask_distribution': 'geometric',\n",
       " 'exclude_feats': None,\n",
       " 'mask_feats': [0, 1],\n",
       " 'start_hint': 0.0,\n",
       " 'end_hint': 0.0,\n",
       " 'harden': False,\n",
       " 'epochs': 30,\n",
       " 'val_interval': 1,\n",
       " 'optimizer': 'RAdam',\n",
       " 'lr': 1e-05,\n",
       " 'lr_step': [1000000],\n",
       " 'lr_factor': [0.1],\n",
       " 'batch_size': 128,\n",
       " 'l2_reg': 0,\n",
       " 'global_reg': False,\n",
       " 'key_metric': 'loss',\n",
       " 'freeze': False,\n",
       " 'model': 'transformer',\n",
       " 'max_seq_len': None,\n",
       " 'data_window_len': None,\n",
       " 'd_model': 64,\n",
       " 'dim_feedforward': 256,\n",
       " 'num_heads': 8,\n",
       " 'num_layers': 3,\n",
       " 'dropout': 0.1,\n",
       " 'pos_encoding': 'learnable',\n",
       " 'activation': 'gelu',\n",
       " 'normalization_layer': 'BatchNorm',\n",
       " 'initial_timestamp': '2023-05-25_21-58-39',\n",
       " 'save_dir': '../experiments/poseErrorPred_preTrain_2023-05-25_21-58-39_T27/checkpoints',\n",
       " 'pred_dir': '../experiments/poseErrorPred_preTrain_2023-05-25_21-58-39_T27/predictions',\n",
       " 'tensorboard_dir': '../experiments/poseErrorPred_preTrain_2023-05-25_21-58-39_T27/tb_summaries'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639d146",
   "metadata": {},
   "source": [
    "# Setup Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552d0b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch_time = 0\n",
    "total_eval_time = 0\n",
    "\n",
    "total_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "386c3baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 21:58:52,421 | INFO : Running:\n",
      "/home/tianyi/anaconda3/envs/transformer/lib/python3.8/site-packages/ipykernel_launcher.py -f /home/tianyi/.local/share/jupyter/runtime/kernel-ab7cf4ec-4309-4e29-829f-276210ab4a13.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add file logging besides stdout\n",
    "file_handler = logging.FileHandler(os.path.join(config['output_dir'], 'output.log'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.info('Running:\\n{}\\n'.format(' '.join(sys.argv)))  # command used to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd3027",
   "metadata": {},
   "source": [
    "# Setup Training Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07459b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 21:58:53,661 | INFO : Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if config['seed'] is not None:\n",
    "    torch.manual_seed(config['seed'])\n",
    "\n",
    "device = torch.device('cuda:0' if (torch.cuda.is_available() and config['gpu'] != '-1') else 'cpu')\n",
    "logger.info(\"Using device: {}\".format(device))\n",
    "if device == 'cuda:0':\n",
    "    logger.info(\"Device index: {}\".format(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d796b",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e8acbf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 21:58:55,358 | INFO : Loading and preprocessing data ...\n",
      "2023-05-25 21:58:55,360 | INFO : Loading 69 datasets files using 32 parallel processes ...\n"
     ]
    }
   ],
   "source": [
    " # Build data\n",
    "logger.info(\"Loading and preprocessing data ...\")\n",
    "data_class = data_factory[config['data_class']]\n",
    "my_data = data_class(config['data_dir'], \n",
    "                     pattern=config['pattern'], \n",
    "                     n_proc=config['n_proc'], \n",
    "                     limit_size=config['limit_size'], \n",
    "                     config=config)\n",
    "feat_dim = my_data.feature_df.shape[1]  # dimensionality of data features\n",
    "if config['task'] == 'classification':\n",
    "    validation_method = 'StratifiedShuffleSplit'\n",
    "    labels = my_data.labels_df.values.flatten()\n",
    "else:\n",
    "    validation_method = 'ShuffleSplit'\n",
    "    labels = None\n",
    "    \n",
    "# Modify for the pose error pred\n",
    "validation_method = 'PoseErrorTimeSplit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "047fe549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_dim = my_data.feature_df.shape[1]\n",
    "feat_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706bd0fa",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "329d7c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "test_data = my_data\n",
    "test_indices = None  # will be converted to empty list in `split_dataset`, if also test_set_ratio == 0\n",
    "val_data = my_data\n",
    "val_indices = []\n",
    "if config['test_pattern']:  # used if test data come from different files / file patterns\n",
    "    test_data = data_class(config['data_dir'], pattern=config['test_pattern'], n_proc=-1, config=config)\n",
    "    test_indices = test_data.all_IDs\n",
    "if config['test_from']:  # load test IDs directly from file, if available, otherwise use `test_set_ratio`. Can work together with `test_pattern`\n",
    "    test_indices = list(set([line.rstrip() for line in open(config['test_from']).readlines()]))\n",
    "    try:\n",
    "        test_indices = [int(ind) for ind in test_indices]  # integer indices\n",
    "    except ValueError:\n",
    "        pass  # in case indices are non-integers\n",
    "    logger.info(\"Loaded {} test IDs from file: '{}'\".format(len(test_indices), config['test_from']))\n",
    "if config['val_pattern']:  # used if val data come from different files / file patterns\n",
    "    val_data = data_class(config['data_dir'], pattern=config['val_pattern'], n_proc=-1, config=config)\n",
    "    val_indices = val_data.all_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1fa9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: currently a validation set must exist, either with `val_pattern` or `val_ratio`\n",
    "# Using a `val_pattern` means that `val_ratio` == 0 and `test_ratio` == 0\n",
    "if config['val_ratio'] > 0:\n",
    "    train_indices, val_indices, test_indices = split_dataset(data_indices=my_data.all_IDs,\n",
    "                                                             validation_method=validation_method,\n",
    "                                                             n_splits=1,\n",
    "                                                             validation_ratio=config['val_ratio'],\n",
    "                                                             test_set_ratio=config['test_ratio'],  # used only if test_indices not explicitly specified\n",
    "                                                             test_indices=test_indices,\n",
    "                                                             random_seed=1337,\n",
    "                                                             labels=labels)\n",
    "    train_indices = train_indices[0]  # `split_dataset` returns a list of indices *per fold/split*\n",
    "    val_indices = val_indices[0]  # `split_dataset` returns a list of indices *per fold/split*\n",
    "else:\n",
    "    train_indices = my_data.all_IDs\n",
    "    if test_indices is None:\n",
    "        test_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "142da4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 21:59:29,046 | INFO : 87695 \t samples may be used for training\n",
      "2023-05-25 21:59:29,048 | INFO : 21957 \t samples will be used for validation\n",
      "2023-05-25 21:59:29,048 | INFO : 0 \t samples will be used for testing\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"{} \\t samples may be used for training\".format(len(train_indices)))\n",
    "logger.info(\"{} \\t samples will be used for validation\".format(len(val_indices)))\n",
    "logger.info(\"{} \\t samples will be used for testing\".format(len(test_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b579872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config['output_dir'], 'data_indices.json'), 'w') as f:\n",
    "    try:\n",
    "        json.dump({'train_indices': list(map(int, train_indices)),\n",
    "                   'val_indices': list(map(int, val_indices)),\n",
    "                   'test_indices': list(map(int, test_indices))}, f, indent=4)\n",
    "    except ValueError:  # in case indices are non-integers\n",
    "        json.dump({'train_indices': list(train_indices),\n",
    "                   'val_indices': list(val_indices),\n",
    "                   'test_indices': list(test_indices)}, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d89d2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process features\n",
    "normalizer = None\n",
    "if config['norm_from']:\n",
    "    with open(config['norm_from'], 'rb') as f:\n",
    "        norm_dict = pickle.load(f)\n",
    "    normalizer = Normalizer(**norm_dict)\n",
    "elif config['normalization'] is not None:\n",
    "    normalizer = Normalizer(config['normalization'])\n",
    "    my_data.feature_df.loc[train_indices] = normalizer.normalize(my_data.feature_df.loc[train_indices])\n",
    "    if not config['normalization'].startswith('per_sample'):\n",
    "        # get normalizing values from training set and store for future use\n",
    "        norm_dict = normalizer.__dict__\n",
    "        with open(os.path.join(config['output_dir'], 'normalization.pickle'), 'wb') as f:\n",
    "            pickle.dump(norm_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "if normalizer is not None:\n",
    "    if len(val_indices):\n",
    "        val_data.feature_df.loc[val_indices] = normalizer.normalize(val_data.feature_df.loc[val_indices])\n",
    "    if len(test_indices):\n",
    "        test_data.feature_df.loc[test_indices] = normalizer.normalize(test_data.feature_df.loc[test_indices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb5e618",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e829b2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 21:59:39,108 | INFO : Creating model ...\n",
      "2023-05-25 21:59:39,132 | INFO : Model:\n",
      "TSTransformerEncoderClassiregressor(\n",
      "  (project_inp): Linear(in_features=13, out_features=64, bias=True)\n",
      "  (pos_enc): LearnablePositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=3840, out_features=1, bias=True)\n",
      ")\n",
      "2023-05-25 21:59:39,133 | INFO : Total number of parameters: 158529\n",
      "2023-05-25 21:59:39,133 | INFO : Trainable parameters: 158529\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "logger.info(\"Creating model ...\")\n",
    "model = model_factory(config, my_data)\n",
    "\n",
    "if config['freeze']:\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith('output_layer'):\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "logger.info(\"Model:\\n{}\".format(model))\n",
    "logger.info(\"Total number of parameters: {}\".format(utils.count_parameters(model)))\n",
    "logger.info(\"Trainable parameters: {}\".format(utils.count_parameters(model, trainable=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb07a9",
   "metadata": {},
   "source": [
    "# Initialize optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2744d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "if config['global_reg']:\n",
    "    weight_decay = config['l2_reg']\n",
    "    output_reg = None\n",
    "else:\n",
    "    weight_decay = 0\n",
    "    output_reg = config['l2_reg']\n",
    "\n",
    "optim_class = get_optimizer(config['optimizer'])\n",
    "optimizer = optim_class(model.parameters(), lr=config['lr'], weight_decay=weight_decay)\n",
    "\n",
    "start_epoch = 0\n",
    "lr_step = 0  # current step index of `lr_step`\n",
    "lr = config['lr']  # current learning step\n",
    "# Load model and optimizer state\n",
    "if args.load_model:\n",
    "    model, optimizer, start_epoch = utils.load_model(model, config['load_model'], optimizer, config['resume'],\n",
    "                                                     config['change_output'],\n",
    "                                                     config['lr'],\n",
    "                                                     config['lr_step'],\n",
    "                                                     config['lr_factor'])\n",
    "model.to(device)\n",
    "\n",
    "loss_module = get_loss_module(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14053daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['test_only'] == 'testset':  # Only evaluate and skip training\n",
    "    dataset_class, collate_fn, runner_class = pipeline_factory(config)\n",
    "    test_dataset = dataset_class(test_data, test_indices)\n",
    "\n",
    "    test_loader = DataLoader(dataset=test_dataset,\n",
    "                             batch_size=config['batch_size'],\n",
    "                             shuffle=False,\n",
    "                             num_workers=config['num_workers'],\n",
    "                             pin_memory=True,\n",
    "                             collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "    test_evaluator = runner_class(model, test_loader, device, loss_module,\n",
    "                                        print_interval=config['print_interval'], console=config['console'])\n",
    "    aggr_metrics_test, per_batch_test = test_evaluator.evaluate(keep_all=True)\n",
    "    print_str = 'Test Summary: '\n",
    "    for k, v in aggr_metrics_test.items():\n",
    "        print_str += '{}: {:8f} | '.format(k, v)\n",
    "    logger.info(print_str)\n",
    "    #return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34b73e40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize data generators\n",
    "if config['test_only'] != 'testset':  # Only evaluate and skip training\n",
    "    dataset_class, collate_fn, runner_class = pipeline_factory(config)\n",
    "    val_dataset = dataset_class(val_data, val_indices)\n",
    "\n",
    "    val_loader = DataLoader(dataset=val_dataset,\n",
    "                            batch_size=config['batch_size'],\n",
    "                            shuffle=False,\n",
    "                            num_workers=config['num_workers'],\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "\n",
    "    train_dataset = dataset_class(my_data, train_indices)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=config['batch_size'],\n",
    "                              shuffle=True,\n",
    "                              num_workers=config['num_workers'],\n",
    "                              pin_memory=True,\n",
    "                              collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "\n",
    "    trainer = runner_class(model, train_loader, device, loss_module, optimizer, l2_reg=output_reg,\n",
    "                                 print_interval=config['print_interval'], console=config['console'])\n",
    "    val_evaluator = runner_class(model, val_loader, device, loss_module,\n",
    "                                       print_interval=config['print_interval'], console=config['console'])\n",
    "\n",
    "    tensorboard_writer = SummaryWriter(config['tensorboard_dir'])\n",
    "\n",
    "    best_value = 1e16 if config['key_metric'] in NEG_METRICS else -1e16  # initialize with +inf or -inf depending on key metric\n",
    "    metrics = []  # (for validation) list of lists: for each epoch, stores metrics like loss, ...\n",
    "    best_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcca71a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(config[\"batch_size\"])\\nfor batch in train_loader:\\n    X, targets, padding_masks, IDs = batch\\n    print(X.shape)\\n    print(X[0])\\n    print(X.dtype)\\n    print(\"-\"*20)\\n    print(targets.shape)\\n    print(targets[0])\\n    print(targets.dtype)\\n    print(\"-\"*20)\\n    print(padding_masks.shape)\\n    print(padding_masks[0])\\n    print(\"-\"*20)\\n    print(IDs)\\n    break\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(config[\"batch_size\"])\n",
    "for batch in train_loader:\n",
    "    X, targets, padding_masks, IDs = batch\n",
    "    print(X.shape)\n",
    "    print(X[0])\n",
    "    print(X.dtype)\n",
    "    print(\"-\"*20)\n",
    "    print(targets.shape)\n",
    "    print(targets[0])\n",
    "    print(targets.dtype)\n",
    "    print(\"-\"*20)\n",
    "    print(padding_masks.shape)\n",
    "    print(padding_masks[0])\n",
    "    print(\"-\"*20)\n",
    "    print(IDs)\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f94a773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "049fe0cf",
   "metadata": {},
   "source": [
    "# Evaluate on validation before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc3f83ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 21:59:47,070 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 0   0.0% | batch:         0 of       172\t|\tloss: 18562.2\n",
      "Evaluating Epoch 0   0.6% | batch:         1 of       172\t|\tloss: 16663.2\n",
      "Evaluating Epoch 0   1.2% | batch:         2 of       172\t|\tloss: 17883.6\n",
      "Evaluating Epoch 0   1.7% | batch:         3 of       172\t|\tloss: 10908.9\n",
      "Evaluating Epoch 0   2.3% | batch:         4 of       172\t|\tloss: 17692.3\n",
      "Evaluating Epoch 0   2.9% | batch:         5 of       172\t|\tloss: 5672.17\n",
      "Evaluating Epoch 0   3.5% | batch:         6 of       172\t|\tloss: 21810.5\n",
      "Evaluating Epoch 0   4.1% | batch:         7 of       172\t|\tloss: 10053.1\n",
      "Evaluating Epoch 0   4.7% | batch:         8 of       172\t|\tloss: 15211.4\n",
      "Evaluating Epoch 0   5.2% | batch:         9 of       172\t|\tloss: 15051.4\n",
      "Evaluating Epoch 0   5.8% | batch:        10 of       172\t|\tloss: 11370.3\n",
      "Evaluating Epoch 0   6.4% | batch:        11 of       172\t|\tloss: 19053.3\n",
      "Evaluating Epoch 0   7.0% | batch:        12 of       172\t|\tloss: 5556.19\n",
      "Evaluating Epoch 0   7.6% | batch:        13 of       172\t|\tloss: 19846.7\n",
      "Evaluating Epoch 0   8.1% | batch:        14 of       172\t|\tloss: 7174.88\n",
      "Evaluating Epoch 0   8.7% | batch:        15 of       172\t|\tloss: 18981.2\n",
      "Evaluating Epoch 0   9.3% | batch:        16 of       172\t|\tloss: 14276.1\n",
      "Evaluating Epoch 0   9.9% | batch:        17 of       172\t|\tloss: 19352.1\n",
      "Evaluating Epoch 0  10.5% | batch:        18 of       172\t|\tloss: 7118.58\n",
      "Evaluating Epoch 0  11.0% | batch:        19 of       172\t|\tloss: 9451.1\n",
      "Evaluating Epoch 0  11.6% | batch:        20 of       172\t|\tloss: 14036.2\n",
      "Evaluating Epoch 0  12.2% | batch:        21 of       172\t|\tloss: 4772.36\n",
      "Evaluating Epoch 0  12.8% | batch:        22 of       172\t|\tloss: 4534.67\n",
      "Evaluating Epoch 0  13.4% | batch:        23 of       172\t|\tloss: 10163.5\n",
      "Evaluating Epoch 0  14.0% | batch:        24 of       172\t|\tloss: 9234.49\n",
      "Evaluating Epoch 0  14.5% | batch:        25 of       172\t|\tloss: 9257.3\n",
      "Evaluating Epoch 0  15.1% | batch:        26 of       172\t|\tloss: 6080.78\n",
      "Evaluating Epoch 0  15.7% | batch:        27 of       172\t|\tloss: 11099.5\n",
      "Evaluating Epoch 0  16.3% | batch:        28 of       172\t|\tloss: 5945.24\n",
      "Evaluating Epoch 0  16.9% | batch:        29 of       172\t|\tloss: 10445.8\n",
      "Evaluating Epoch 0  17.4% | batch:        30 of       172\t|\tloss: 4830.55\n",
      "Evaluating Epoch 0  18.0% | batch:        31 of       172\t|\tloss: 4416.76\n",
      "Evaluating Epoch 0  18.6% | batch:        32 of       172\t|\tloss: 5106.74\n",
      "Evaluating Epoch 0  19.2% | batch:        33 of       172\t|\tloss: 8989.83\n",
      "Evaluating Epoch 0  19.8% | batch:        34 of       172\t|\tloss: 3518.2\n",
      "Evaluating Epoch 0  20.3% | batch:        35 of       172\t|\tloss: 4848.38\n",
      "Evaluating Epoch 0  20.9% | batch:        36 of       172\t|\tloss: 8264.91\n",
      "Evaluating Epoch 0  21.5% | batch:        37 of       172\t|\tloss: 14308.3\n",
      "Evaluating Epoch 0  22.1% | batch:        38 of       172\t|\tloss: 4033.84\n",
      "Evaluating Epoch 0  22.7% | batch:        39 of       172\t|\tloss: 8530.08\n",
      "Evaluating Epoch 0  23.3% | batch:        40 of       172\t|\tloss: 5569.61\n",
      "Evaluating Epoch 0  23.8% | batch:        41 of       172\t|\tloss: 9736.09\n",
      "Evaluating Epoch 0  24.4% | batch:        42 of       172\t|\tloss: 6575.54\n",
      "Evaluating Epoch 0  25.0% | batch:        43 of       172\t|\tloss: 9698.53\n",
      "Evaluating Epoch 0  25.6% | batch:        44 of       172\t|\tloss: 6293.97\n",
      "Evaluating Epoch 0  26.2% | batch:        45 of       172\t|\tloss: 11238.6\n",
      "Evaluating Epoch 0  26.7% | batch:        46 of       172\t|\tloss: 4863.78\n",
      "Evaluating Epoch 0  27.3% | batch:        47 of       172\t|\tloss: 4841.61\n",
      "Evaluating Epoch 0  27.9% | batch:        48 of       172\t|\tloss: 5438.85\n",
      "Evaluating Epoch 0  28.5% | batch:        49 of       172\t|\tloss: 8925.36\n",
      "Evaluating Epoch 0  29.1% | batch:        50 of       172\t|\tloss: 8622.14\n",
      "Evaluating Epoch 0  29.7% | batch:        51 of       172\t|\tloss: 4594.44\n",
      "Evaluating Epoch 0  30.2% | batch:        52 of       172\t|\tloss: 13661.1\n",
      "Evaluating Epoch 0  30.8% | batch:        53 of       172\t|\tloss: 4331.95\n",
      "Evaluating Epoch 0  31.4% | batch:        54 of       172\t|\tloss: 2980.09\n",
      "Evaluating Epoch 0  32.0% | batch:        55 of       172\t|\tloss: 10525.1\n",
      "Evaluating Epoch 0  32.6% | batch:        56 of       172\t|\tloss: 5340.14\n",
      "Evaluating Epoch 0  33.1% | batch:        57 of       172\t|\tloss: 10038.5\n",
      "Evaluating Epoch 0  33.7% | batch:        58 of       172\t|\tloss: 3451.01\n",
      "Evaluating Epoch 0  34.3% | batch:        59 of       172\t|\tloss: 14409.8\n",
      "Evaluating Epoch 0  34.9% | batch:        60 of       172\t|\tloss: 1717.16\n",
      "Evaluating Epoch 0  35.5% | batch:        61 of       172\t|\tloss: 14526.6\n",
      "Evaluating Epoch 0  36.0% | batch:        62 of       172\t|\tloss: 1378.13\n",
      "Evaluating Epoch 0  36.6% | batch:        63 of       172\t|\tloss: 3282.33\n",
      "Evaluating Epoch 0  37.2% | batch:        64 of       172\t|\tloss: 12470.6\n",
      "Evaluating Epoch 0  37.8% | batch:        65 of       172\t|\tloss: 2732.77\n",
      "Evaluating Epoch 0  38.4% | batch:        66 of       172\t|\tloss: 4904.07\n",
      "Evaluating Epoch 0  39.0% | batch:        67 of       172\t|\tloss: 10985.6\n",
      "Evaluating Epoch 0  39.5% | batch:        68 of       172\t|\tloss: 4508.54\n",
      "Evaluating Epoch 0  40.1% | batch:        69 of       172\t|\tloss: 12715.2\n",
      "Evaluating Epoch 0  40.7% | batch:        70 of       172\t|\tloss: 2953.91\n",
      "Evaluating Epoch 0  41.3% | batch:        71 of       172\t|\tloss: 3758.17\n",
      "Evaluating Epoch 0  41.9% | batch:        72 of       172\t|\tloss: 12298\n",
      "Evaluating Epoch 0  42.4% | batch:        73 of       172\t|\tloss: 3581.31\n",
      "Evaluating Epoch 0  43.0% | batch:        74 of       172\t|\tloss: 6100.21\n",
      "Evaluating Epoch 0  43.6% | batch:        75 of       172\t|\tloss: 7353.21\n",
      "Evaluating Epoch 0  44.2% | batch:        76 of       172\t|\tloss: 9400.97\n",
      "Evaluating Epoch 0  44.8% | batch:        77 of       172\t|\tloss: 12177.8\n",
      "Evaluating Epoch 0  45.3% | batch:        78 of       172\t|\tloss: 10070.1\n",
      "Evaluating Epoch 0  45.9% | batch:        79 of       172\t|\tloss: 9110.02\n",
      "Evaluating Epoch 0  46.5% | batch:        80 of       172\t|\tloss: 7141.37\n",
      "Evaluating Epoch 0  47.1% | batch:        81 of       172\t|\tloss: 7995.97\n",
      "Evaluating Epoch 0  47.7% | batch:        82 of       172\t|\tloss: 8894.71\n",
      "Evaluating Epoch 0  48.3% | batch:        83 of       172\t|\tloss: 7612.61\n",
      "Evaluating Epoch 0  48.8% | batch:        84 of       172\t|\tloss: 6562.36\n",
      "Evaluating Epoch 0  49.4% | batch:        85 of       172\t|\tloss: 11805.4\n",
      "Evaluating Epoch 0  50.0% | batch:        86 of       172\t|\tloss: 7479.96\n",
      "Evaluating Epoch 0  50.6% | batch:        87 of       172\t|\tloss: 2406.11\n",
      "Evaluating Epoch 0  51.2% | batch:        88 of       172\t|\tloss: 6245.13\n",
      "Evaluating Epoch 0  51.7% | batch:        89 of       172\t|\tloss: 7126.97\n",
      "Evaluating Epoch 0  52.3% | batch:        90 of       172\t|\tloss: 5227.34\n",
      "Evaluating Epoch 0  52.9% | batch:        91 of       172\t|\tloss: 5847.15\n",
      "Evaluating Epoch 0  53.5% | batch:        92 of       172\t|\tloss: 6221.62\n",
      "Evaluating Epoch 0  54.1% | batch:        93 of       172\t|\tloss: 6908.46\n",
      "Evaluating Epoch 0  54.7% | batch:        94 of       172\t|\tloss: 5788.65\n",
      "Evaluating Epoch 0  55.2% | batch:        95 of       172\t|\tloss: 6517.75\n",
      "Evaluating Epoch 0  55.8% | batch:        96 of       172\t|\tloss: 5099.61\n",
      "Evaluating Epoch 0  56.4% | batch:        97 of       172\t|\tloss: 1796.45\n",
      "Evaluating Epoch 0  57.0% | batch:        98 of       172\t|\tloss: 6136.41\n",
      "Evaluating Epoch 0  57.6% | batch:        99 of       172\t|\tloss: 9227.05\n",
      "Evaluating Epoch 0  58.1% | batch:       100 of       172\t|\tloss: 4613.44\n",
      "Evaluating Epoch 0  58.7% | batch:       101 of       172\t|\tloss: 8337.04\n",
      "Evaluating Epoch 0  59.3% | batch:       102 of       172\t|\tloss: 6605.53\n",
      "Evaluating Epoch 0  59.9% | batch:       103 of       172\t|\tloss: 7356.76\n",
      "Evaluating Epoch 0  60.5% | batch:       104 of       172\t|\tloss: 3821.05\n",
      "Evaluating Epoch 0  61.0% | batch:       105 of       172\t|\tloss: 5737.27\n",
      "Evaluating Epoch 0  61.6% | batch:       106 of       172\t|\tloss: 5736.12\n",
      "Evaluating Epoch 0  62.2% | batch:       107 of       172\t|\tloss: 5887.25\n",
      "Evaluating Epoch 0  62.8% | batch:       108 of       172\t|\tloss: 2254.05\n",
      "Evaluating Epoch 0  63.4% | batch:       109 of       172\t|\tloss: 5017.21\n",
      "Evaluating Epoch 0  64.0% | batch:       110 of       172\t|\tloss: 5999.98\n",
      "Evaluating Epoch 0  64.5% | batch:       111 of       172\t|\tloss: 3417.83\n",
      "Evaluating Epoch 0  65.1% | batch:       112 of       172\t|\tloss: 3525.82\n",
      "Evaluating Epoch 0  65.7% | batch:       113 of       172\t|\tloss: 5696.01\n",
      "Evaluating Epoch 0  66.3% | batch:       114 of       172\t|\tloss: 13186.5\n",
      "Evaluating Epoch 0  66.9% | batch:       115 of       172\t|\tloss: 7372.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 0  67.4% | batch:       116 of       172\t|\tloss: 27538.2\n",
      "Evaluating Epoch 0  68.0% | batch:       117 of       172\t|\tloss: 17829.3\n",
      "Evaluating Epoch 0  68.6% | batch:       118 of       172\t|\tloss: 12735.5\n",
      "Evaluating Epoch 0  69.2% | batch:       119 of       172\t|\tloss: 34265.8\n",
      "Evaluating Epoch 0  69.8% | batch:       120 of       172\t|\tloss: 23691.9\n",
      "Evaluating Epoch 0  70.3% | batch:       121 of       172\t|\tloss: 25640.1\n",
      "Evaluating Epoch 0  70.9% | batch:       122 of       172\t|\tloss: 33486.2\n",
      "Evaluating Epoch 0  71.5% | batch:       123 of       172\t|\tloss: 48895.4\n",
      "Evaluating Epoch 0  72.1% | batch:       124 of       172\t|\tloss: 225596\n",
      "Evaluating Epoch 0  72.7% | batch:       125 of       172\t|\tloss: 43693.4\n",
      "Evaluating Epoch 0  73.3% | batch:       126 of       172\t|\tloss: 15405.9\n",
      "Evaluating Epoch 0  73.8% | batch:       127 of       172\t|\tloss: 9761.76\n",
      "Evaluating Epoch 0  74.4% | batch:       128 of       172\t|\tloss: 34083.4\n",
      "Evaluating Epoch 0  75.0% | batch:       129 of       172\t|\tloss: 33667.1\n",
      "Evaluating Epoch 0  75.6% | batch:       130 of       172\t|\tloss: 12327.9\n",
      "Evaluating Epoch 0  76.2% | batch:       131 of       172\t|\tloss: 36057.2\n",
      "Evaluating Epoch 0  76.7% | batch:       132 of       172\t|\tloss: 1764.63\n",
      "Evaluating Epoch 0  77.3% | batch:       133 of       172\t|\tloss: 1487.33\n",
      "Evaluating Epoch 0  77.9% | batch:       134 of       172\t|\tloss: 1570.53\n",
      "Evaluating Epoch 0  78.5% | batch:       135 of       172\t|\tloss: 1313.17\n",
      "Evaluating Epoch 0  79.1% | batch:       136 of       172\t|\tloss: 356.369\n",
      "Evaluating Epoch 0  79.7% | batch:       137 of       172\t|\tloss: 1883.84\n",
      "Evaluating Epoch 0  80.2% | batch:       138 of       172\t|\tloss: 923.631\n",
      "Evaluating Epoch 0  80.8% | batch:       139 of       172\t|\tloss: 1385.97\n",
      "Evaluating Epoch 0  81.4% | batch:       140 of       172\t|\tloss: 528.692\n",
      "Evaluating Epoch 0  82.0% | batch:       141 of       172\t|\tloss: 149.406\n",
      "Evaluating Epoch 0  82.6% | batch:       142 of       172\t|\tloss: 602.925\n",
      "Evaluating Epoch 0  83.1% | batch:       143 of       172\t|\tloss: 691.707\n",
      "Evaluating Epoch 0  83.7% | batch:       144 of       172\t|\tloss: 1107.94\n",
      "Evaluating Epoch 0  84.3% | batch:       145 of       172\t|\tloss: 1777.94\n",
      "Evaluating Epoch 0  84.9% | batch:       146 of       172\t|\tloss: 514.99\n",
      "Evaluating Epoch 0  85.5% | batch:       147 of       172\t|\tloss: 1761.84\n",
      "Evaluating Epoch 0  86.0% | batch:       148 of       172\t|\tloss: 427.387\n",
      "Evaluating Epoch 0  86.6% | batch:       149 of       172\t|\tloss: 1290.54\n",
      "Evaluating Epoch 0  87.2% | batch:       150 of       172\t|\tloss: 2348.22\n",
      "Evaluating Epoch 0  87.8% | batch:       151 of       172\t|\tloss: 13831.5\n",
      "Evaluating Epoch 0  88.4% | batch:       152 of       172\t|\tloss: 8162.08\n",
      "Evaluating Epoch 0  89.0% | batch:       153 of       172\t|\tloss: 5060.04\n",
      "Evaluating Epoch 0  89.5% | batch:       154 of       172\t|\tloss: 16062.3\n",
      "Evaluating Epoch 0  90.1% | batch:       155 of       172\t|\tloss: 4016.1\n",
      "Evaluating Epoch 0  90.7% | batch:       156 of       172\t|\tloss: 10353.8\n",
      "Evaluating Epoch 0  91.3% | batch:       157 of       172\t|\tloss: 11734.5\n",
      "Evaluating Epoch 0  91.9% | batch:       158 of       172\t|\tloss: 2568.19\n",
      "Evaluating Epoch 0  92.4% | batch:       159 of       172\t|\tloss: 15654.3\n",
      "Evaluating Epoch 0  93.0% | batch:       160 of       172\t|\tloss: 40968.8\n",
      "Evaluating Epoch 0  93.6% | batch:       161 of       172\t|\tloss: 6805.76\n",
      "Evaluating Epoch 0  94.2% | batch:       162 of       172\t|\tloss: 15428.3\n",
      "Evaluating Epoch 0  94.8% | batch:       163 of       172\t|\tloss: 4637.31\n",
      "Evaluating Epoch 0  95.3% | batch:       164 of       172\t|\tloss: 12373.4\n",
      "Evaluating Epoch 0  95.9% | batch:       165 of       172\t|\tloss: 12256\n",
      "Evaluating Epoch 0  96.5% | batch:       166 of       172\t|\tloss: 914.12\n",
      "Evaluating Epoch 0  97.1% | batch:       167 of       172\t|\tloss: 11725.8\n",
      "Evaluating Epoch 0  97.7% | batch:       168 of       172\t|\tloss: 9908.34\n",
      "Evaluating Epoch 0  98.3% | batch:       169 of       172\t|\tloss: 3617.59\n",
      "Evaluating Epoch 0  98.8% | batch:       170 of       172\t|\tloss: 15958.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 21:59:52,783 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 5.713092803955078 seconds\n",
      "\n",
      "2023-05-25 21:59:52,784 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 5.713092803955078 seconds\n",
      "2023-05-25 21:59:52,784 | INFO : Avg batch val. time: 0.033215655836948126 seconds\n",
      "2023-05-25 21:59:52,785 | INFO : Avg sample val. time: 0.00026019459871362566 seconds\n",
      "2023-05-25 21:59:52,785 | INFO : Epoch 0 Validation Summary: epoch: 0.000000 | loss: 10738.389361 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 0  99.4% | batch:       171 of       172\t|\tloss: 10355\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianyi/anaconda3/envs/transformer/lib/python3.8/site-packages/numpy/lib/npyio.py:713: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val = np.asanyarray(val)\n"
     ]
    }
   ],
   "source": [
    "aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, tensorboard_writer, config, best_metrics,\n",
    "                                                      best_value, epoch=0)\n",
    "metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "metrics.append(list(metrics_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf058db",
   "metadata": {},
   "source": [
    "# Starting training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c07b4b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 21:59:52,846 | INFO : Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianyi/Documents/mvts_transformer/src/optimizers.py:69: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1   0.0% | batch:         0 of       686\t|\tloss: 118.869\n",
      "Training Epoch 1   0.1% | batch:         1 of       686\t|\tloss: 114.254\n",
      "Training Epoch 1   0.3% | batch:         2 of       686\t|\tloss: 145.515\n",
      "Training Epoch 1   0.4% | batch:         3 of       686\t|\tloss: 112.183\n",
      "Training Epoch 1   0.6% | batch:         4 of       686\t|\tloss: 139.662\n",
      "Training Epoch 1   0.7% | batch:         5 of       686\t|\tloss: 192.13\n",
      "Training Epoch 1   0.9% | batch:         6 of       686\t|\tloss: 100.377\n",
      "Training Epoch 1   1.0% | batch:         7 of       686\t|\tloss: 121.954\n",
      "Training Epoch 1   1.2% | batch:         8 of       686\t|\tloss: 125.526\n",
      "Training Epoch 1   1.3% | batch:         9 of       686\t|\tloss: 126.244\n",
      "Training Epoch 1   1.5% | batch:        10 of       686\t|\tloss: 143.211\n",
      "Training Epoch 1   1.6% | batch:        11 of       686\t|\tloss: 141.221\n",
      "Training Epoch 1   1.7% | batch:        12 of       686\t|\tloss: 97.1591\n",
      "Training Epoch 1   1.9% | batch:        13 of       686\t|\tloss: 441.118\n",
      "Training Epoch 1   2.0% | batch:        14 of       686\t|\tloss: 122.594\n",
      "Training Epoch 1   2.2% | batch:        15 of       686\t|\tloss: 115.378\n",
      "Training Epoch 1   2.3% | batch:        16 of       686\t|\tloss: 178.282\n",
      "Training Epoch 1   2.5% | batch:        17 of       686\t|\tloss: 149.602\n",
      "Training Epoch 1   2.6% | batch:        18 of       686\t|\tloss: 118.021\n",
      "Training Epoch 1   2.8% | batch:        19 of       686\t|\tloss: 101.131\n",
      "Training Epoch 1   2.9% | batch:        20 of       686\t|\tloss: 122.29\n",
      "Training Epoch 1   3.1% | batch:        21 of       686\t|\tloss: 168.045\n",
      "Training Epoch 1   3.2% | batch:        22 of       686\t|\tloss: 114.548\n",
      "Training Epoch 1   3.4% | batch:        23 of       686\t|\tloss: 132.037\n",
      "Training Epoch 1   3.5% | batch:        24 of       686\t|\tloss: 142.145\n",
      "Training Epoch 1   3.6% | batch:        25 of       686\t|\tloss: 127.552\n",
      "Training Epoch 1   3.8% | batch:        26 of       686\t|\tloss: 115.052\n",
      "Training Epoch 1   3.9% | batch:        27 of       686\t|\tloss: 136.823\n",
      "Training Epoch 1   4.1% | batch:        28 of       686\t|\tloss: 113.312\n",
      "Training Epoch 1   4.2% | batch:        29 of       686\t|\tloss: 103.319\n",
      "Training Epoch 1   4.4% | batch:        30 of       686\t|\tloss: 155.112\n",
      "Training Epoch 1   4.5% | batch:        31 of       686\t|\tloss: 146.732\n",
      "Training Epoch 1   4.7% | batch:        32 of       686\t|\tloss: 149.349\n",
      "Training Epoch 1   4.8% | batch:        33 of       686\t|\tloss: 124.425\n",
      "Training Epoch 1   5.0% | batch:        34 of       686\t|\tloss: 139.257\n",
      "Training Epoch 1   5.1% | batch:        35 of       686\t|\tloss: 146.574\n",
      "Training Epoch 1   5.2% | batch:        36 of       686\t|\tloss: 127.34\n",
      "Training Epoch 1   5.4% | batch:        37 of       686\t|\tloss: 175.679\n",
      "Training Epoch 1   5.5% | batch:        38 of       686\t|\tloss: 151.342\n",
      "Training Epoch 1   5.7% | batch:        39 of       686\t|\tloss: 172.556\n",
      "Training Epoch 1   5.8% | batch:        40 of       686\t|\tloss: 112\n",
      "Training Epoch 1   6.0% | batch:        41 of       686\t|\tloss: 120.871\n",
      "Training Epoch 1   6.1% | batch:        42 of       686\t|\tloss: 123.69\n",
      "Training Epoch 1   6.3% | batch:        43 of       686\t|\tloss: 93.3968\n",
      "Training Epoch 1   6.4% | batch:        44 of       686\t|\tloss: 137.492\n",
      "Training Epoch 1   6.6% | batch:        45 of       686\t|\tloss: 135.285\n",
      "Training Epoch 1   6.7% | batch:        46 of       686\t|\tloss: 105.234\n",
      "Training Epoch 1   6.9% | batch:        47 of       686\t|\tloss: 143.751\n",
      "Training Epoch 1   7.0% | batch:        48 of       686\t|\tloss: 145.886\n",
      "Training Epoch 1   7.1% | batch:        49 of       686\t|\tloss: 91.8833\n",
      "Training Epoch 1   7.3% | batch:        50 of       686\t|\tloss: 93.9824\n",
      "Training Epoch 1   7.4% | batch:        51 of       686\t|\tloss: 113.652\n",
      "Training Epoch 1   7.6% | batch:        52 of       686\t|\tloss: 107.344\n",
      "Training Epoch 1   7.7% | batch:        53 of       686\t|\tloss: 135.34\n",
      "Training Epoch 1   7.9% | batch:        54 of       686\t|\tloss: 148.02\n",
      "Training Epoch 1   8.0% | batch:        55 of       686\t|\tloss: 96.551\n",
      "Training Epoch 1   8.2% | batch:        56 of       686\t|\tloss: 137.132\n",
      "Training Epoch 1   8.3% | batch:        57 of       686\t|\tloss: 116.069\n",
      "Training Epoch 1   8.5% | batch:        58 of       686\t|\tloss: 118.854\n",
      "Training Epoch 1   8.6% | batch:        59 of       686\t|\tloss: 126.369\n",
      "Training Epoch 1   8.7% | batch:        60 of       686\t|\tloss: 92.6464\n",
      "Training Epoch 1   8.9% | batch:        61 of       686\t|\tloss: 115.121\n",
      "Training Epoch 1   9.0% | batch:        62 of       686\t|\tloss: 139.281\n",
      "Training Epoch 1   9.2% | batch:        63 of       686\t|\tloss: 133.315\n",
      "Training Epoch 1   9.3% | batch:        64 of       686\t|\tloss: 118.638\n",
      "Training Epoch 1   9.5% | batch:        65 of       686\t|\tloss: 142.921\n",
      "Training Epoch 1   9.6% | batch:        66 of       686\t|\tloss: 105.024\n",
      "Training Epoch 1   9.8% | batch:        67 of       686\t|\tloss: 118.794\n",
      "Training Epoch 1   9.9% | batch:        68 of       686\t|\tloss: 95.6808\n",
      "Training Epoch 1  10.1% | batch:        69 of       686\t|\tloss: 93.5739\n",
      "Training Epoch 1  10.2% | batch:        70 of       686\t|\tloss: 105.975\n",
      "Training Epoch 1  10.3% | batch:        71 of       686\t|\tloss: 97.1864\n",
      "Training Epoch 1  10.5% | batch:        72 of       686\t|\tloss: 154.848\n",
      "Training Epoch 1  10.6% | batch:        73 of       686\t|\tloss: 89.6248\n",
      "Training Epoch 1  10.8% | batch:        74 of       686\t|\tloss: 99.0085\n",
      "Training Epoch 1  10.9% | batch:        75 of       686\t|\tloss: 112.199\n",
      "Training Epoch 1  11.1% | batch:        76 of       686\t|\tloss: 118.499\n",
      "Training Epoch 1  11.2% | batch:        77 of       686\t|\tloss: 105.517\n",
      "Training Epoch 1  11.4% | batch:        78 of       686\t|\tloss: 79.4814\n",
      "Training Epoch 1  11.5% | batch:        79 of       686\t|\tloss: 106.641\n",
      "Training Epoch 1  11.7% | batch:        80 of       686\t|\tloss: 126.69\n",
      "Training Epoch 1  11.8% | batch:        81 of       686\t|\tloss: 86.4018\n",
      "Training Epoch 1  12.0% | batch:        82 of       686\t|\tloss: 129.718\n",
      "Training Epoch 1  12.1% | batch:        83 of       686\t|\tloss: 98.1312\n",
      "Training Epoch 1  12.2% | batch:        84 of       686\t|\tloss: 101.621\n",
      "Training Epoch 1  12.4% | batch:        85 of       686\t|\tloss: 134.046\n",
      "Training Epoch 1  12.5% | batch:        86 of       686\t|\tloss: 119.194\n",
      "Training Epoch 1  12.7% | batch:        87 of       686\t|\tloss: 109.382\n",
      "Training Epoch 1  12.8% | batch:        88 of       686\t|\tloss: 98.5513\n",
      "Training Epoch 1  13.0% | batch:        89 of       686\t|\tloss: 72.382\n",
      "Training Epoch 1  13.1% | batch:        90 of       686\t|\tloss: 101.895\n",
      "Training Epoch 1  13.3% | batch:        91 of       686\t|\tloss: 147.155\n",
      "Training Epoch 1  13.4% | batch:        92 of       686\t|\tloss: 117.907\n",
      "Training Epoch 1  13.6% | batch:        93 of       686\t|\tloss: 94.248\n",
      "Training Epoch 1  13.7% | batch:        94 of       686\t|\tloss: 178.805\n",
      "Training Epoch 1  13.8% | batch:        95 of       686\t|\tloss: 112.805\n",
      "Training Epoch 1  14.0% | batch:        96 of       686\t|\tloss: 80.1562\n",
      "Training Epoch 1  14.1% | batch:        97 of       686\t|\tloss: 79.1402\n",
      "Training Epoch 1  14.3% | batch:        98 of       686\t|\tloss: 147.929\n",
      "Training Epoch 1  14.4% | batch:        99 of       686\t|\tloss: 105.136\n",
      "Training Epoch 1  14.6% | batch:       100 of       686\t|\tloss: 77.774\n",
      "Training Epoch 1  14.7% | batch:       101 of       686\t|\tloss: 91.4288\n",
      "Training Epoch 1  14.9% | batch:       102 of       686\t|\tloss: 90.2836\n",
      "Training Epoch 1  15.0% | batch:       103 of       686\t|\tloss: 82.4726\n",
      "Training Epoch 1  15.2% | batch:       104 of       686\t|\tloss: 77.2361\n",
      "Training Epoch 1  15.3% | batch:       105 of       686\t|\tloss: 115.359\n",
      "Training Epoch 1  15.5% | batch:       106 of       686\t|\tloss: 116.523\n",
      "Training Epoch 1  15.6% | batch:       107 of       686\t|\tloss: 124.427\n",
      "Training Epoch 1  15.7% | batch:       108 of       686\t|\tloss: 207.697\n",
      "Training Epoch 1  15.9% | batch:       109 of       686\t|\tloss: 94.8338\n",
      "Training Epoch 1  16.0% | batch:       110 of       686\t|\tloss: 114.781\n",
      "Training Epoch 1  16.2% | batch:       111 of       686\t|\tloss: 108.154\n",
      "Training Epoch 1  16.3% | batch:       112 of       686\t|\tloss: 99.5907\n",
      "Training Epoch 1  16.5% | batch:       113 of       686\t|\tloss: 108.667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  16.6% | batch:       114 of       686\t|\tloss: 69.7103\n",
      "Training Epoch 1  16.8% | batch:       115 of       686\t|\tloss: 100.522\n",
      "Training Epoch 1  16.9% | batch:       116 of       686\t|\tloss: 70.4222\n",
      "Training Epoch 1  17.1% | batch:       117 of       686\t|\tloss: 182.101\n",
      "Training Epoch 1  17.2% | batch:       118 of       686\t|\tloss: 116.089\n",
      "Training Epoch 1  17.3% | batch:       119 of       686\t|\tloss: 80.3607\n",
      "Training Epoch 1  17.5% | batch:       120 of       686\t|\tloss: 89.0706\n",
      "Training Epoch 1  17.6% | batch:       121 of       686\t|\tloss: 80.7908\n",
      "Training Epoch 1  17.8% | batch:       122 of       686\t|\tloss: 89.8916\n",
      "Training Epoch 1  17.9% | batch:       123 of       686\t|\tloss: 89.1305\n",
      "Training Epoch 1  18.1% | batch:       124 of       686\t|\tloss: 84.3103\n",
      "Training Epoch 1  18.2% | batch:       125 of       686\t|\tloss: 118.967\n",
      "Training Epoch 1  18.4% | batch:       126 of       686\t|\tloss: 91.0643\n",
      "Training Epoch 1  18.5% | batch:       127 of       686\t|\tloss: 98.4596\n",
      "Training Epoch 1  18.7% | batch:       128 of       686\t|\tloss: 78.0887\n",
      "Training Epoch 1  18.8% | batch:       129 of       686\t|\tloss: 91.964\n",
      "Training Epoch 1  19.0% | batch:       130 of       686\t|\tloss: 191.579\n",
      "Training Epoch 1  19.1% | batch:       131 of       686\t|\tloss: 95.2788\n",
      "Training Epoch 1  19.2% | batch:       132 of       686\t|\tloss: 68.232\n",
      "Training Epoch 1  19.4% | batch:       133 of       686\t|\tloss: 112.002\n",
      "Training Epoch 1  19.5% | batch:       134 of       686\t|\tloss: 91.3618\n",
      "Training Epoch 1  19.7% | batch:       135 of       686\t|\tloss: 80.6344\n",
      "Training Epoch 1  19.8% | batch:       136 of       686\t|\tloss: 97.0892\n",
      "Training Epoch 1  20.0% | batch:       137 of       686\t|\tloss: 83.5586\n",
      "Training Epoch 1  20.1% | batch:       138 of       686\t|\tloss: 93.3571\n",
      "Training Epoch 1  20.3% | batch:       139 of       686\t|\tloss: 93.0847\n",
      "Training Epoch 1  20.4% | batch:       140 of       686\t|\tloss: 87.7373\n",
      "Training Epoch 1  20.6% | batch:       141 of       686\t|\tloss: 93.7264\n",
      "Training Epoch 1  20.7% | batch:       142 of       686\t|\tloss: 85.1411\n",
      "Training Epoch 1  20.8% | batch:       143 of       686\t|\tloss: 118.075\n",
      "Training Epoch 1  21.0% | batch:       144 of       686\t|\tloss: 95.9529\n",
      "Training Epoch 1  21.1% | batch:       145 of       686\t|\tloss: 96.5668\n",
      "Training Epoch 1  21.3% | batch:       146 of       686\t|\tloss: 69.9336\n",
      "Training Epoch 1  21.4% | batch:       147 of       686\t|\tloss: 83.234\n",
      "Training Epoch 1  21.6% | batch:       148 of       686\t|\tloss: 121.704\n",
      "Training Epoch 1  21.7% | batch:       149 of       686\t|\tloss: 88.5643\n",
      "Training Epoch 1  21.9% | batch:       150 of       686\t|\tloss: 92.9349\n",
      "Training Epoch 1  22.0% | batch:       151 of       686\t|\tloss: 98.1437\n",
      "Training Epoch 1  22.2% | batch:       152 of       686\t|\tloss: 105.141\n",
      "Training Epoch 1  22.3% | batch:       153 of       686\t|\tloss: 84.9447\n",
      "Training Epoch 1  22.4% | batch:       154 of       686\t|\tloss: 110.717\n",
      "Training Epoch 1  22.6% | batch:       155 of       686\t|\tloss: 76.473\n",
      "Training Epoch 1  22.7% | batch:       156 of       686\t|\tloss: 95.2363\n",
      "Training Epoch 1  22.9% | batch:       157 of       686\t|\tloss: 99.805\n",
      "Training Epoch 1  23.0% | batch:       158 of       686\t|\tloss: 131.173\n",
      "Training Epoch 1  23.2% | batch:       159 of       686\t|\tloss: 98.504\n",
      "Training Epoch 1  23.3% | batch:       160 of       686\t|\tloss: 106.773\n",
      "Training Epoch 1  23.5% | batch:       161 of       686\t|\tloss: 80.1964\n",
      "Training Epoch 1  23.6% | batch:       162 of       686\t|\tloss: 81.7809\n",
      "Training Epoch 1  23.8% | batch:       163 of       686\t|\tloss: 99.5152\n",
      "Training Epoch 1  23.9% | batch:       164 of       686\t|\tloss: 77.8143\n",
      "Training Epoch 1  24.1% | batch:       165 of       686\t|\tloss: 98.706\n",
      "Training Epoch 1  24.2% | batch:       166 of       686\t|\tloss: 114.939\n",
      "Training Epoch 1  24.3% | batch:       167 of       686\t|\tloss: 59.6904\n",
      "Training Epoch 1  24.5% | batch:       168 of       686\t|\tloss: 81.5706\n",
      "Training Epoch 1  24.6% | batch:       169 of       686\t|\tloss: 72.2193\n",
      "Training Epoch 1  24.8% | batch:       170 of       686\t|\tloss: 120.933\n",
      "Training Epoch 1  24.9% | batch:       171 of       686\t|\tloss: 95.0097\n",
      "Training Epoch 1  25.1% | batch:       172 of       686\t|\tloss: 71.3039\n",
      "Training Epoch 1  25.2% | batch:       173 of       686\t|\tloss: 89.4491\n",
      "Training Epoch 1  25.4% | batch:       174 of       686\t|\tloss: 66.6292\n",
      "Training Epoch 1  25.5% | batch:       175 of       686\t|\tloss: 79.7705\n",
      "Training Epoch 1  25.7% | batch:       176 of       686\t|\tloss: 77.1767\n",
      "Training Epoch 1  25.8% | batch:       177 of       686\t|\tloss: 81.0527\n",
      "Training Epoch 1  25.9% | batch:       178 of       686\t|\tloss: 90.4226\n",
      "Training Epoch 1  26.1% | batch:       179 of       686\t|\tloss: 80.3319\n",
      "Training Epoch 1  26.2% | batch:       180 of       686\t|\tloss: 84.6335\n",
      "Training Epoch 1  26.4% | batch:       181 of       686\t|\tloss: 95.1834\n",
      "Training Epoch 1  26.5% | batch:       182 of       686\t|\tloss: 82.1557\n",
      "Training Epoch 1  26.7% | batch:       183 of       686\t|\tloss: 88.8091\n",
      "Training Epoch 1  26.8% | batch:       184 of       686\t|\tloss: 93.319\n",
      "Training Epoch 1  27.0% | batch:       185 of       686\t|\tloss: 69.2274\n",
      "Training Epoch 1  27.1% | batch:       186 of       686\t|\tloss: 61.7256\n",
      "Training Epoch 1  27.3% | batch:       187 of       686\t|\tloss: 69.9872\n",
      "Training Epoch 1  27.4% | batch:       188 of       686\t|\tloss: 90.6892\n",
      "Training Epoch 1  27.6% | batch:       189 of       686\t|\tloss: 113.264\n",
      "Training Epoch 1  27.7% | batch:       190 of       686\t|\tloss: 79.9138\n",
      "Training Epoch 1  27.8% | batch:       191 of       686\t|\tloss: 57.1673\n",
      "Training Epoch 1  28.0% | batch:       192 of       686\t|\tloss: 87.5552\n",
      "Training Epoch 1  28.1% | batch:       193 of       686\t|\tloss: 76.7433\n",
      "Training Epoch 1  28.3% | batch:       194 of       686\t|\tloss: 74.8779\n",
      "Training Epoch 1  28.4% | batch:       195 of       686\t|\tloss: 64.4814\n",
      "Training Epoch 1  28.6% | batch:       196 of       686\t|\tloss: 56.3747\n",
      "Training Epoch 1  28.7% | batch:       197 of       686\t|\tloss: 96.025\n",
      "Training Epoch 1  28.9% | batch:       198 of       686\t|\tloss: 68.7209\n",
      "Training Epoch 1  29.0% | batch:       199 of       686\t|\tloss: 58.6291\n",
      "Training Epoch 1  29.2% | batch:       200 of       686\t|\tloss: 65.4707\n",
      "Training Epoch 1  29.3% | batch:       201 of       686\t|\tloss: 59.2397\n",
      "Training Epoch 1  29.4% | batch:       202 of       686\t|\tloss: 55.4073\n",
      "Training Epoch 1  29.6% | batch:       203 of       686\t|\tloss: 72.5249\n",
      "Training Epoch 1  29.7% | batch:       204 of       686\t|\tloss: 80.4679\n",
      "Training Epoch 1  29.9% | batch:       205 of       686\t|\tloss: 92.5433\n",
      "Training Epoch 1  30.0% | batch:       206 of       686\t|\tloss: 56.7893\n",
      "Training Epoch 1  30.2% | batch:       207 of       686\t|\tloss: 81.3478\n",
      "Training Epoch 1  30.3% | batch:       208 of       686\t|\tloss: 123.091\n",
      "Training Epoch 1  30.5% | batch:       209 of       686\t|\tloss: 62.9679\n",
      "Training Epoch 1  30.6% | batch:       210 of       686\t|\tloss: 94.2617\n",
      "Training Epoch 1  30.8% | batch:       211 of       686\t|\tloss: 80.0925\n",
      "Training Epoch 1  30.9% | batch:       212 of       686\t|\tloss: 127.259\n",
      "Training Epoch 1  31.0% | batch:       213 of       686\t|\tloss: 73.4872\n",
      "Training Epoch 1  31.2% | batch:       214 of       686\t|\tloss: 61.748\n",
      "Training Epoch 1  31.3% | batch:       215 of       686\t|\tloss: 86.3467\n",
      "Training Epoch 1  31.5% | batch:       216 of       686\t|\tloss: 242.706\n",
      "Training Epoch 1  31.6% | batch:       217 of       686\t|\tloss: 59.9842\n",
      "Training Epoch 1  31.8% | batch:       218 of       686\t|\tloss: 91.0233\n",
      "Training Epoch 1  31.9% | batch:       219 of       686\t|\tloss: 53.4948\n",
      "Training Epoch 1  32.1% | batch:       220 of       686\t|\tloss: 73.697\n",
      "Training Epoch 1  32.2% | batch:       221 of       686\t|\tloss: 85.2533\n",
      "Training Epoch 1  32.4% | batch:       222 of       686\t|\tloss: 65.9869\n",
      "Training Epoch 1  32.5% | batch:       223 of       686\t|\tloss: 71.0979\n",
      "Training Epoch 1  32.7% | batch:       224 of       686\t|\tloss: 74.7992\n",
      "Training Epoch 1  32.8% | batch:       225 of       686\t|\tloss: 67.1076\n",
      "Training Epoch 1  32.9% | batch:       226 of       686\t|\tloss: 98.9239\n",
      "Training Epoch 1  33.1% | batch:       227 of       686\t|\tloss: 64.626\n",
      "Training Epoch 1  33.2% | batch:       228 of       686\t|\tloss: 57.0749\n",
      "Training Epoch 1  33.4% | batch:       229 of       686\t|\tloss: 52.5664\n",
      "Training Epoch 1  33.5% | batch:       230 of       686\t|\tloss: 74.3293\n",
      "Training Epoch 1  33.7% | batch:       231 of       686\t|\tloss: 53.2618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  33.8% | batch:       232 of       686\t|\tloss: 53.2349\n",
      "Training Epoch 1  34.0% | batch:       233 of       686\t|\tloss: 91.2464\n",
      "Training Epoch 1  34.1% | batch:       234 of       686\t|\tloss: 78.0728\n",
      "Training Epoch 1  34.3% | batch:       235 of       686\t|\tloss: 79.1748\n",
      "Training Epoch 1  34.4% | batch:       236 of       686\t|\tloss: 81.108\n",
      "Training Epoch 1  34.5% | batch:       237 of       686\t|\tloss: 87.0924\n",
      "Training Epoch 1  34.7% | batch:       238 of       686\t|\tloss: 52.8321\n",
      "Training Epoch 1  34.8% | batch:       239 of       686\t|\tloss: 114.196\n",
      "Training Epoch 1  35.0% | batch:       240 of       686\t|\tloss: 71.1109\n",
      "Training Epoch 1  35.1% | batch:       241 of       686\t|\tloss: 77.1012\n",
      "Training Epoch 1  35.3% | batch:       242 of       686\t|\tloss: 71.7372\n",
      "Training Epoch 1  35.4% | batch:       243 of       686\t|\tloss: 53.9677\n",
      "Training Epoch 1  35.6% | batch:       244 of       686\t|\tloss: 81.1057\n",
      "Training Epoch 1  35.7% | batch:       245 of       686\t|\tloss: 82.1429\n",
      "Training Epoch 1  35.9% | batch:       246 of       686\t|\tloss: 74.1789\n",
      "Training Epoch 1  36.0% | batch:       247 of       686\t|\tloss: 78.4252\n",
      "Training Epoch 1  36.2% | batch:       248 of       686\t|\tloss: 45.4227\n",
      "Training Epoch 1  36.3% | batch:       249 of       686\t|\tloss: 64.8161\n",
      "Training Epoch 1  36.4% | batch:       250 of       686\t|\tloss: 64.5739\n",
      "Training Epoch 1  36.6% | batch:       251 of       686\t|\tloss: 64.9992\n",
      "Training Epoch 1  36.7% | batch:       252 of       686\t|\tloss: 50.2876\n",
      "Training Epoch 1  36.9% | batch:       253 of       686\t|\tloss: 125.48\n",
      "Training Epoch 1  37.0% | batch:       254 of       686\t|\tloss: 50.6746\n",
      "Training Epoch 1  37.2% | batch:       255 of       686\t|\tloss: 66.9914\n",
      "Training Epoch 1  37.3% | batch:       256 of       686\t|\tloss: 81.634\n",
      "Training Epoch 1  37.5% | batch:       257 of       686\t|\tloss: 65.0825\n",
      "Training Epoch 1  37.6% | batch:       258 of       686\t|\tloss: 84.1848\n",
      "Training Epoch 1  37.8% | batch:       259 of       686\t|\tloss: 84.9754\n",
      "Training Epoch 1  37.9% | batch:       260 of       686\t|\tloss: 53.0291\n",
      "Training Epoch 1  38.0% | batch:       261 of       686\t|\tloss: 92.6353\n",
      "Training Epoch 1  38.2% | batch:       262 of       686\t|\tloss: 57.9516\n",
      "Training Epoch 1  38.3% | batch:       263 of       686\t|\tloss: 62.1455\n",
      "Training Epoch 1  38.5% | batch:       264 of       686\t|\tloss: 55.2919\n",
      "Training Epoch 1  38.6% | batch:       265 of       686\t|\tloss: 39.0552\n",
      "Training Epoch 1  38.8% | batch:       266 of       686\t|\tloss: 71.7423\n",
      "Training Epoch 1  38.9% | batch:       267 of       686\t|\tloss: 65.8038\n",
      "Training Epoch 1  39.1% | batch:       268 of       686\t|\tloss: 51.8917\n",
      "Training Epoch 1  39.2% | batch:       269 of       686\t|\tloss: 53.3405\n",
      "Training Epoch 1  39.4% | batch:       270 of       686\t|\tloss: 103.328\n",
      "Training Epoch 1  39.5% | batch:       271 of       686\t|\tloss: 71.3896\n",
      "Training Epoch 1  39.7% | batch:       272 of       686\t|\tloss: 60.6642\n",
      "Training Epoch 1  39.8% | batch:       273 of       686\t|\tloss: 57.0537\n",
      "Training Epoch 1  39.9% | batch:       274 of       686\t|\tloss: 63.5075\n",
      "Training Epoch 1  40.1% | batch:       275 of       686\t|\tloss: 59.0598\n",
      "Training Epoch 1  40.2% | batch:       276 of       686\t|\tloss: 68.5547\n",
      "Training Epoch 1  40.4% | batch:       277 of       686\t|\tloss: 62.7715\n",
      "Training Epoch 1  40.5% | batch:       278 of       686\t|\tloss: 103.481\n",
      "Training Epoch 1  40.7% | batch:       279 of       686\t|\tloss: 63.4244\n",
      "Training Epoch 1  40.8% | batch:       280 of       686\t|\tloss: 59.2414\n",
      "Training Epoch 1  41.0% | batch:       281 of       686\t|\tloss: 61.5808\n",
      "Training Epoch 1  41.1% | batch:       282 of       686\t|\tloss: 50.4629\n",
      "Training Epoch 1  41.3% | batch:       283 of       686\t|\tloss: 66.7827\n",
      "Training Epoch 1  41.4% | batch:       284 of       686\t|\tloss: 39.218\n",
      "Training Epoch 1  41.5% | batch:       285 of       686\t|\tloss: 52.2839\n",
      "Training Epoch 1  41.7% | batch:       286 of       686\t|\tloss: 77.6857\n",
      "Training Epoch 1  41.8% | batch:       287 of       686\t|\tloss: 90.8331\n",
      "Training Epoch 1  42.0% | batch:       288 of       686\t|\tloss: 61.9279\n",
      "Training Epoch 1  42.1% | batch:       289 of       686\t|\tloss: 92.0712\n",
      "Training Epoch 1  42.3% | batch:       290 of       686\t|\tloss: 60.7608\n",
      "Training Epoch 1  42.4% | batch:       291 of       686\t|\tloss: 87.2745\n",
      "Training Epoch 1  42.6% | batch:       292 of       686\t|\tloss: 82.4099\n",
      "Training Epoch 1  42.7% | batch:       293 of       686\t|\tloss: 57.6252\n",
      "Training Epoch 1  42.9% | batch:       294 of       686\t|\tloss: 57.4841\n",
      "Training Epoch 1  43.0% | batch:       295 of       686\t|\tloss: 61.5431\n",
      "Training Epoch 1  43.1% | batch:       296 of       686\t|\tloss: 56.1225\n",
      "Training Epoch 1  43.3% | batch:       297 of       686\t|\tloss: 63.4654\n",
      "Training Epoch 1  43.4% | batch:       298 of       686\t|\tloss: 36.5955\n",
      "Training Epoch 1  43.6% | batch:       299 of       686\t|\tloss: 62.5668\n",
      "Training Epoch 1  43.7% | batch:       300 of       686\t|\tloss: 73.4745\n",
      "Training Epoch 1  43.9% | batch:       301 of       686\t|\tloss: 54.0329\n",
      "Training Epoch 1  44.0% | batch:       302 of       686\t|\tloss: 66.5085\n",
      "Training Epoch 1  44.2% | batch:       303 of       686\t|\tloss: 56.1633\n",
      "Training Epoch 1  44.3% | batch:       304 of       686\t|\tloss: 42.1329\n",
      "Training Epoch 1  44.5% | batch:       305 of       686\t|\tloss: 76.0756\n",
      "Training Epoch 1  44.6% | batch:       306 of       686\t|\tloss: 60.9931\n",
      "Training Epoch 1  44.8% | batch:       307 of       686\t|\tloss: 56.7864\n",
      "Training Epoch 1  44.9% | batch:       308 of       686\t|\tloss: 70.3879\n",
      "Training Epoch 1  45.0% | batch:       309 of       686\t|\tloss: 71.5575\n",
      "Training Epoch 1  45.2% | batch:       310 of       686\t|\tloss: 57.8282\n",
      "Training Epoch 1  45.3% | batch:       311 of       686\t|\tloss: 44.688\n",
      "Training Epoch 1  45.5% | batch:       312 of       686\t|\tloss: 56.5001\n",
      "Training Epoch 1  45.6% | batch:       313 of       686\t|\tloss: 46.0035\n",
      "Training Epoch 1  45.8% | batch:       314 of       686\t|\tloss: 64.2544\n",
      "Training Epoch 1  45.9% | batch:       315 of       686\t|\tloss: 60.2018\n",
      "Training Epoch 1  46.1% | batch:       316 of       686\t|\tloss: 58.8407\n",
      "Training Epoch 1  46.2% | batch:       317 of       686\t|\tloss: 61.4546\n",
      "Training Epoch 1  46.4% | batch:       318 of       686\t|\tloss: 57.0813\n",
      "Training Epoch 1  46.5% | batch:       319 of       686\t|\tloss: 58.9813\n",
      "Training Epoch 1  46.6% | batch:       320 of       686\t|\tloss: 57.0528\n",
      "Training Epoch 1  46.8% | batch:       321 of       686\t|\tloss: 51.1584\n",
      "Training Epoch 1  46.9% | batch:       322 of       686\t|\tloss: 54.4299\n",
      "Training Epoch 1  47.1% | batch:       323 of       686\t|\tloss: 58.7243\n",
      "Training Epoch 1  47.2% | batch:       324 of       686\t|\tloss: 70.9316\n",
      "Training Epoch 1  47.4% | batch:       325 of       686\t|\tloss: 52.9228\n",
      "Training Epoch 1  47.5% | batch:       326 of       686\t|\tloss: 71.1132\n",
      "Training Epoch 1  47.7% | batch:       327 of       686\t|\tloss: 63.3564\n",
      "Training Epoch 1  47.8% | batch:       328 of       686\t|\tloss: 63.6115\n",
      "Training Epoch 1  48.0% | batch:       329 of       686\t|\tloss: 48.1845\n",
      "Training Epoch 1  48.1% | batch:       330 of       686\t|\tloss: 67.2987\n",
      "Training Epoch 1  48.3% | batch:       331 of       686\t|\tloss: 43.5594\n",
      "Training Epoch 1  48.4% | batch:       332 of       686\t|\tloss: 56.4476\n",
      "Training Epoch 1  48.5% | batch:       333 of       686\t|\tloss: 57.5439\n",
      "Training Epoch 1  48.7% | batch:       334 of       686\t|\tloss: 58.7403\n",
      "Training Epoch 1  48.8% | batch:       335 of       686\t|\tloss: 61.3357\n",
      "Training Epoch 1  49.0% | batch:       336 of       686\t|\tloss: 51.0615\n",
      "Training Epoch 1  49.1% | batch:       337 of       686\t|\tloss: 64.6565\n",
      "Training Epoch 1  49.3% | batch:       338 of       686\t|\tloss: 51.5613\n",
      "Training Epoch 1  49.4% | batch:       339 of       686\t|\tloss: 51.9279\n",
      "Training Epoch 1  49.6% | batch:       340 of       686\t|\tloss: 68.2677\n",
      "Training Epoch 1  49.7% | batch:       341 of       686\t|\tloss: 47.9549\n",
      "Training Epoch 1  49.9% | batch:       342 of       686\t|\tloss: 63.1942\n",
      "Training Epoch 1  50.0% | batch:       343 of       686\t|\tloss: 69.7209\n",
      "Training Epoch 1  50.1% | batch:       344 of       686\t|\tloss: 50.1615\n",
      "Training Epoch 1  50.3% | batch:       345 of       686\t|\tloss: 49.7545\n",
      "Training Epoch 1  50.4% | batch:       346 of       686\t|\tloss: 34.7773\n",
      "Training Epoch 1  50.6% | batch:       347 of       686\t|\tloss: 55.2348\n",
      "Training Epoch 1  50.7% | batch:       348 of       686\t|\tloss: 54.5926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  50.9% | batch:       349 of       686\t|\tloss: 64.1823\n",
      "Training Epoch 1  51.0% | batch:       350 of       686\t|\tloss: 52.5568\n",
      "Training Epoch 1  51.2% | batch:       351 of       686\t|\tloss: 65.9557\n",
      "Training Epoch 1  51.3% | batch:       352 of       686\t|\tloss: 55.0274\n",
      "Training Epoch 1  51.5% | batch:       353 of       686\t|\tloss: 53.6634\n",
      "Training Epoch 1  51.6% | batch:       354 of       686\t|\tloss: 68.222\n",
      "Training Epoch 1  51.7% | batch:       355 of       686\t|\tloss: 60.8056\n",
      "Training Epoch 1  51.9% | batch:       356 of       686\t|\tloss: 63.6235\n",
      "Training Epoch 1  52.0% | batch:       357 of       686\t|\tloss: 125.292\n",
      "Training Epoch 1  52.2% | batch:       358 of       686\t|\tloss: 56.5751\n",
      "Training Epoch 1  52.3% | batch:       359 of       686\t|\tloss: 55.8988\n",
      "Training Epoch 1  52.5% | batch:       360 of       686\t|\tloss: 56.8775\n",
      "Training Epoch 1  52.6% | batch:       361 of       686\t|\tloss: 53.9845\n",
      "Training Epoch 1  52.8% | batch:       362 of       686\t|\tloss: 59.2212\n",
      "Training Epoch 1  52.9% | batch:       363 of       686\t|\tloss: 53.1033\n",
      "Training Epoch 1  53.1% | batch:       364 of       686\t|\tloss: 69.1811\n",
      "Training Epoch 1  53.2% | batch:       365 of       686\t|\tloss: 46.7182\n",
      "Training Epoch 1  53.4% | batch:       366 of       686\t|\tloss: 148.612\n",
      "Training Epoch 1  53.5% | batch:       367 of       686\t|\tloss: 50.0537\n",
      "Training Epoch 1  53.6% | batch:       368 of       686\t|\tloss: 55.2834\n",
      "Training Epoch 1  53.8% | batch:       369 of       686\t|\tloss: 98.4219\n",
      "Training Epoch 1  53.9% | batch:       370 of       686\t|\tloss: 97.6972\n",
      "Training Epoch 1  54.1% | batch:       371 of       686\t|\tloss: 47.3185\n",
      "Training Epoch 1  54.2% | batch:       372 of       686\t|\tloss: 52.5008\n",
      "Training Epoch 1  54.4% | batch:       373 of       686\t|\tloss: 59.759\n",
      "Training Epoch 1  54.5% | batch:       374 of       686\t|\tloss: 60.6086\n",
      "Training Epoch 1  54.7% | batch:       375 of       686\t|\tloss: 65.1093\n",
      "Training Epoch 1  54.8% | batch:       376 of       686\t|\tloss: 50.7945\n",
      "Training Epoch 1  55.0% | batch:       377 of       686\t|\tloss: 62.4438\n",
      "Training Epoch 1  55.1% | batch:       378 of       686\t|\tloss: 53.1807\n",
      "Training Epoch 1  55.2% | batch:       379 of       686\t|\tloss: 37.7397\n",
      "Training Epoch 1  55.4% | batch:       380 of       686\t|\tloss: 65.0279\n",
      "Training Epoch 1  55.5% | batch:       381 of       686\t|\tloss: 66.3863\n",
      "Training Epoch 1  55.7% | batch:       382 of       686\t|\tloss: 53.9259\n",
      "Training Epoch 1  55.8% | batch:       383 of       686\t|\tloss: 63.6361\n",
      "Training Epoch 1  56.0% | batch:       384 of       686\t|\tloss: 54.7954\n",
      "Training Epoch 1  56.1% | batch:       385 of       686\t|\tloss: 46.2323\n",
      "Training Epoch 1  56.3% | batch:       386 of       686\t|\tloss: 52.7711\n",
      "Training Epoch 1  56.4% | batch:       387 of       686\t|\tloss: 62.5149\n",
      "Training Epoch 1  56.6% | batch:       388 of       686\t|\tloss: 52.5327\n",
      "Training Epoch 1  56.7% | batch:       389 of       686\t|\tloss: 59.4654\n",
      "Training Epoch 1  56.9% | batch:       390 of       686\t|\tloss: 52.3669\n",
      "Training Epoch 1  57.0% | batch:       391 of       686\t|\tloss: 47.4628\n",
      "Training Epoch 1  57.1% | batch:       392 of       686\t|\tloss: 70.6853\n",
      "Training Epoch 1  57.3% | batch:       393 of       686\t|\tloss: 56.0908\n",
      "Training Epoch 1  57.4% | batch:       394 of       686\t|\tloss: 53.5092\n",
      "Training Epoch 1  57.6% | batch:       395 of       686\t|\tloss: 64.1141\n",
      "Training Epoch 1  57.7% | batch:       396 of       686\t|\tloss: 48.8972\n",
      "Training Epoch 1  57.9% | batch:       397 of       686\t|\tloss: 59.7227\n",
      "Training Epoch 1  58.0% | batch:       398 of       686\t|\tloss: 62.1185\n",
      "Training Epoch 1  58.2% | batch:       399 of       686\t|\tloss: 52.1601\n",
      "Training Epoch 1  58.3% | batch:       400 of       686\t|\tloss: 54.4305\n",
      "Training Epoch 1  58.5% | batch:       401 of       686\t|\tloss: 37.6618\n",
      "Training Epoch 1  58.6% | batch:       402 of       686\t|\tloss: 66.4413\n",
      "Training Epoch 1  58.7% | batch:       403 of       686\t|\tloss: 51.6612\n",
      "Training Epoch 1  58.9% | batch:       404 of       686\t|\tloss: 55.9398\n",
      "Training Epoch 1  59.0% | batch:       405 of       686\t|\tloss: 33.4425\n",
      "Training Epoch 1  59.2% | batch:       406 of       686\t|\tloss: 55.7234\n",
      "Training Epoch 1  59.3% | batch:       407 of       686\t|\tloss: 49.6606\n",
      "Training Epoch 1  59.5% | batch:       408 of       686\t|\tloss: 55.8526\n",
      "Training Epoch 1  59.6% | batch:       409 of       686\t|\tloss: 84.5837\n",
      "Training Epoch 1  59.8% | batch:       410 of       686\t|\tloss: 52.6159\n",
      "Training Epoch 1  59.9% | batch:       411 of       686\t|\tloss: 71.5518\n",
      "Training Epoch 1  60.1% | batch:       412 of       686\t|\tloss: 40.231\n",
      "Training Epoch 1  60.2% | batch:       413 of       686\t|\tloss: 47.1048\n",
      "Training Epoch 1  60.3% | batch:       414 of       686\t|\tloss: 59.8407\n",
      "Training Epoch 1  60.5% | batch:       415 of       686\t|\tloss: 62.6099\n",
      "Training Epoch 1  60.6% | batch:       416 of       686\t|\tloss: 57.2111\n",
      "Training Epoch 1  60.8% | batch:       417 of       686\t|\tloss: 40.0683\n",
      "Training Epoch 1  60.9% | batch:       418 of       686\t|\tloss: 61.5038\n",
      "Training Epoch 1  61.1% | batch:       419 of       686\t|\tloss: 87.0333\n",
      "Training Epoch 1  61.2% | batch:       420 of       686\t|\tloss: 43.1631\n",
      "Training Epoch 1  61.4% | batch:       421 of       686\t|\tloss: 69.5324\n",
      "Training Epoch 1  61.5% | batch:       422 of       686\t|\tloss: 66.7202\n",
      "Training Epoch 1  61.7% | batch:       423 of       686\t|\tloss: 59.9778\n",
      "Training Epoch 1  61.8% | batch:       424 of       686\t|\tloss: 63.7059\n",
      "Training Epoch 1  62.0% | batch:       425 of       686\t|\tloss: 56.428\n",
      "Training Epoch 1  62.1% | batch:       426 of       686\t|\tloss: 67.134\n",
      "Training Epoch 1  62.2% | batch:       427 of       686\t|\tloss: 64.8476\n",
      "Training Epoch 1  62.4% | batch:       428 of       686\t|\tloss: 56.5748\n",
      "Training Epoch 1  62.5% | batch:       429 of       686\t|\tloss: 53.3725\n",
      "Training Epoch 1  62.7% | batch:       430 of       686\t|\tloss: 43.7325\n",
      "Training Epoch 1  62.8% | batch:       431 of       686\t|\tloss: 59.6388\n",
      "Training Epoch 1  63.0% | batch:       432 of       686\t|\tloss: 43.0843\n",
      "Training Epoch 1  63.1% | batch:       433 of       686\t|\tloss: 76.6086\n",
      "Training Epoch 1  63.3% | batch:       434 of       686\t|\tloss: 60.7284\n",
      "Training Epoch 1  63.4% | batch:       435 of       686\t|\tloss: 46.2032\n",
      "Training Epoch 1  63.6% | batch:       436 of       686\t|\tloss: 48.4381\n",
      "Training Epoch 1  63.7% | batch:       437 of       686\t|\tloss: 45.4407\n",
      "Training Epoch 1  63.8% | batch:       438 of       686\t|\tloss: 48.468\n",
      "Training Epoch 1  64.0% | batch:       439 of       686\t|\tloss: 84.2826\n",
      "Training Epoch 1  64.1% | batch:       440 of       686\t|\tloss: 62.0954\n",
      "Training Epoch 1  64.3% | batch:       441 of       686\t|\tloss: 56.3789\n",
      "Training Epoch 1  64.4% | batch:       442 of       686\t|\tloss: 50.2425\n",
      "Training Epoch 1  64.6% | batch:       443 of       686\t|\tloss: 57.3482\n",
      "Training Epoch 1  64.7% | batch:       444 of       686\t|\tloss: 49.7876\n",
      "Training Epoch 1  64.9% | batch:       445 of       686\t|\tloss: 44.0896\n",
      "Training Epoch 1  65.0% | batch:       446 of       686\t|\tloss: 86.5857\n",
      "Training Epoch 1  65.2% | batch:       447 of       686\t|\tloss: 58.3369\n",
      "Training Epoch 1  65.3% | batch:       448 of       686\t|\tloss: 51.5782\n",
      "Training Epoch 1  65.5% | batch:       449 of       686\t|\tloss: 49.4664\n",
      "Training Epoch 1  65.6% | batch:       450 of       686\t|\tloss: 44.4231\n",
      "Training Epoch 1  65.7% | batch:       451 of       686\t|\tloss: 53.1809\n",
      "Training Epoch 1  65.9% | batch:       452 of       686\t|\tloss: 59.0324\n",
      "Training Epoch 1  66.0% | batch:       453 of       686\t|\tloss: 51.7465\n",
      "Training Epoch 1  66.2% | batch:       454 of       686\t|\tloss: 46.8553\n",
      "Training Epoch 1  66.3% | batch:       455 of       686\t|\tloss: 50.5131\n",
      "Training Epoch 1  66.5% | batch:       456 of       686\t|\tloss: 36.7401\n",
      "Training Epoch 1  66.6% | batch:       457 of       686\t|\tloss: 57.279\n",
      "Training Epoch 1  66.8% | batch:       458 of       686\t|\tloss: 68.4942\n",
      "Training Epoch 1  66.9% | batch:       459 of       686\t|\tloss: 56.1005\n",
      "Training Epoch 1  67.1% | batch:       460 of       686\t|\tloss: 42.401\n",
      "Training Epoch 1  67.2% | batch:       461 of       686\t|\tloss: 62.2667\n",
      "Training Epoch 1  67.3% | batch:       462 of       686\t|\tloss: 60.3371\n",
      "Training Epoch 1  67.5% | batch:       463 of       686\t|\tloss: 51.5419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  67.6% | batch:       464 of       686\t|\tloss: 59.2453\n",
      "Training Epoch 1  67.8% | batch:       465 of       686\t|\tloss: 62.8091\n",
      "Training Epoch 1  67.9% | batch:       466 of       686\t|\tloss: 42.2574\n",
      "Training Epoch 1  68.1% | batch:       467 of       686\t|\tloss: 37.143\n",
      "Training Epoch 1  68.2% | batch:       468 of       686\t|\tloss: 44.3952\n",
      "Training Epoch 1  68.4% | batch:       469 of       686\t|\tloss: 56.4258\n",
      "Training Epoch 1  68.5% | batch:       470 of       686\t|\tloss: 80.3225\n",
      "Training Epoch 1  68.7% | batch:       471 of       686\t|\tloss: 58.8513\n",
      "Training Epoch 1  68.8% | batch:       472 of       686\t|\tloss: 57.3941\n",
      "Training Epoch 1  69.0% | batch:       473 of       686\t|\tloss: 42.8652\n",
      "Training Epoch 1  69.1% | batch:       474 of       686\t|\tloss: 58.5129\n",
      "Training Epoch 1  69.2% | batch:       475 of       686\t|\tloss: 46.7559\n",
      "Training Epoch 1  69.4% | batch:       476 of       686\t|\tloss: 60.5227\n",
      "Training Epoch 1  69.5% | batch:       477 of       686\t|\tloss: 45.028\n",
      "Training Epoch 1  69.7% | batch:       478 of       686\t|\tloss: 58.4491\n",
      "Training Epoch 1  69.8% | batch:       479 of       686\t|\tloss: 42.0629\n",
      "Training Epoch 1  70.0% | batch:       480 of       686\t|\tloss: 57.5313\n",
      "Training Epoch 1  70.1% | batch:       481 of       686\t|\tloss: 45.7121\n",
      "Training Epoch 1  70.3% | batch:       482 of       686\t|\tloss: 43.5124\n",
      "Training Epoch 1  70.4% | batch:       483 of       686\t|\tloss: 56.6247\n",
      "Training Epoch 1  70.6% | batch:       484 of       686\t|\tloss: 55.4769\n",
      "Training Epoch 1  70.7% | batch:       485 of       686\t|\tloss: 60.2318\n",
      "Training Epoch 1  70.8% | batch:       486 of       686\t|\tloss: 47.0628\n",
      "Training Epoch 1  71.0% | batch:       487 of       686\t|\tloss: 41.2046\n",
      "Training Epoch 1  71.1% | batch:       488 of       686\t|\tloss: 44.1304\n",
      "Training Epoch 1  71.3% | batch:       489 of       686\t|\tloss: 53.8331\n",
      "Training Epoch 1  71.4% | batch:       490 of       686\t|\tloss: 62.252\n",
      "Training Epoch 1  71.6% | batch:       491 of       686\t|\tloss: 42.8232\n",
      "Training Epoch 1  71.7% | batch:       492 of       686\t|\tloss: 63.7683\n",
      "Training Epoch 1  71.9% | batch:       493 of       686\t|\tloss: 51.1309\n",
      "Training Epoch 1  72.0% | batch:       494 of       686\t|\tloss: 49.4605\n",
      "Training Epoch 1  72.2% | batch:       495 of       686\t|\tloss: 53.7505\n",
      "Training Epoch 1  72.3% | batch:       496 of       686\t|\tloss: 51.1543\n",
      "Training Epoch 1  72.4% | batch:       497 of       686\t|\tloss: 44.8852\n",
      "Training Epoch 1  72.6% | batch:       498 of       686\t|\tloss: 62.6093\n",
      "Training Epoch 1  72.7% | batch:       499 of       686\t|\tloss: 56.8771\n",
      "Training Epoch 1  72.9% | batch:       500 of       686\t|\tloss: 64.4446\n",
      "Training Epoch 1  73.0% | batch:       501 of       686\t|\tloss: 49.6477\n",
      "Training Epoch 1  73.2% | batch:       502 of       686\t|\tloss: 54.7181\n",
      "Training Epoch 1  73.3% | batch:       503 of       686\t|\tloss: 48.1803\n",
      "Training Epoch 1  73.5% | batch:       504 of       686\t|\tloss: 63.1508\n",
      "Training Epoch 1  73.6% | batch:       505 of       686\t|\tloss: 43.1666\n",
      "Training Epoch 1  73.8% | batch:       506 of       686\t|\tloss: 51.9421\n",
      "Training Epoch 1  73.9% | batch:       507 of       686\t|\tloss: 67.2657\n",
      "Training Epoch 1  74.1% | batch:       508 of       686\t|\tloss: 49.95\n",
      "Training Epoch 1  74.2% | batch:       509 of       686\t|\tloss: 45.838\n",
      "Training Epoch 1  74.3% | batch:       510 of       686\t|\tloss: 43.3842\n",
      "Training Epoch 1  74.5% | batch:       511 of       686\t|\tloss: 60.3371\n",
      "Training Epoch 1  74.6% | batch:       512 of       686\t|\tloss: 54.37\n",
      "Training Epoch 1  74.8% | batch:       513 of       686\t|\tloss: 59.7195\n",
      "Training Epoch 1  74.9% | batch:       514 of       686\t|\tloss: 47.9283\n",
      "Training Epoch 1  75.1% | batch:       515 of       686\t|\tloss: 58.657\n",
      "Training Epoch 1  75.2% | batch:       516 of       686\t|\tloss: 55.2447\n",
      "Training Epoch 1  75.4% | batch:       517 of       686\t|\tloss: 44.4457\n",
      "Training Epoch 1  75.5% | batch:       518 of       686\t|\tloss: 48.8847\n",
      "Training Epoch 1  75.7% | batch:       519 of       686\t|\tloss: 58.8162\n",
      "Training Epoch 1  75.8% | batch:       520 of       686\t|\tloss: 47.6236\n",
      "Training Epoch 1  75.9% | batch:       521 of       686\t|\tloss: 47.1575\n",
      "Training Epoch 1  76.1% | batch:       522 of       686\t|\tloss: 55.7685\n",
      "Training Epoch 1  76.2% | batch:       523 of       686\t|\tloss: 53.3286\n",
      "Training Epoch 1  76.4% | batch:       524 of       686\t|\tloss: 43.906\n",
      "Training Epoch 1  76.5% | batch:       525 of       686\t|\tloss: 57.314\n",
      "Training Epoch 1  76.7% | batch:       526 of       686\t|\tloss: 61.1551\n",
      "Training Epoch 1  76.8% | batch:       527 of       686\t|\tloss: 41.5951\n",
      "Training Epoch 1  77.0% | batch:       528 of       686\t|\tloss: 51.8559\n",
      "Training Epoch 1  77.1% | batch:       529 of       686\t|\tloss: 47.3683\n",
      "Training Epoch 1  77.3% | batch:       530 of       686\t|\tloss: 58.6423\n",
      "Training Epoch 1  77.4% | batch:       531 of       686\t|\tloss: 58.7483\n",
      "Training Epoch 1  77.6% | batch:       532 of       686\t|\tloss: 35.0559\n",
      "Training Epoch 1  77.7% | batch:       533 of       686\t|\tloss: 54.0733\n",
      "Training Epoch 1  77.8% | batch:       534 of       686\t|\tloss: 49.7648\n",
      "Training Epoch 1  78.0% | batch:       535 of       686\t|\tloss: 62.5861\n",
      "Training Epoch 1  78.1% | batch:       536 of       686\t|\tloss: 50.0522\n",
      "Training Epoch 1  78.3% | batch:       537 of       686\t|\tloss: 46.2549\n",
      "Training Epoch 1  78.4% | batch:       538 of       686\t|\tloss: 42.1313\n",
      "Training Epoch 1  78.6% | batch:       539 of       686\t|\tloss: 54.1031\n",
      "Training Epoch 1  78.7% | batch:       540 of       686\t|\tloss: 46.283\n",
      "Training Epoch 1  78.9% | batch:       541 of       686\t|\tloss: 56.4257\n",
      "Training Epoch 1  79.0% | batch:       542 of       686\t|\tloss: 115.613\n",
      "Training Epoch 1  79.2% | batch:       543 of       686\t|\tloss: 52.392\n",
      "Training Epoch 1  79.3% | batch:       544 of       686\t|\tloss: 44.5924\n",
      "Training Epoch 1  79.4% | batch:       545 of       686\t|\tloss: 48.8234\n",
      "Training Epoch 1  79.6% | batch:       546 of       686\t|\tloss: 43.2587\n",
      "Training Epoch 1  79.7% | batch:       547 of       686\t|\tloss: 43.8747\n",
      "Training Epoch 1  79.9% | batch:       548 of       686\t|\tloss: 51.0833\n",
      "Training Epoch 1  80.0% | batch:       549 of       686\t|\tloss: 45.3155\n",
      "Training Epoch 1  80.2% | batch:       550 of       686\t|\tloss: 40.247\n",
      "Training Epoch 1  80.3% | batch:       551 of       686\t|\tloss: 44.8445\n",
      "Training Epoch 1  80.5% | batch:       552 of       686\t|\tloss: 38.096\n",
      "Training Epoch 1  80.6% | batch:       553 of       686\t|\tloss: 44.6278\n",
      "Training Epoch 1  80.8% | batch:       554 of       686\t|\tloss: 58.4769\n",
      "Training Epoch 1  80.9% | batch:       555 of       686\t|\tloss: 61.6295\n",
      "Training Epoch 1  81.0% | batch:       556 of       686\t|\tloss: 59.9628\n",
      "Training Epoch 1  81.2% | batch:       557 of       686\t|\tloss: 46.2766\n",
      "Training Epoch 1  81.3% | batch:       558 of       686\t|\tloss: 56.8741\n",
      "Training Epoch 1  81.5% | batch:       559 of       686\t|\tloss: 40.9481\n",
      "Training Epoch 1  81.6% | batch:       560 of       686\t|\tloss: 50.3612\n",
      "Training Epoch 1  81.8% | batch:       561 of       686\t|\tloss: 38.254\n",
      "Training Epoch 1  81.9% | batch:       562 of       686\t|\tloss: 62.0934\n",
      "Training Epoch 1  82.1% | batch:       563 of       686\t|\tloss: 49.1322\n",
      "Training Epoch 1  82.2% | batch:       564 of       686\t|\tloss: 51.309\n",
      "Training Epoch 1  82.4% | batch:       565 of       686\t|\tloss: 42.4468\n",
      "Training Epoch 1  82.5% | batch:       566 of       686\t|\tloss: 66.743\n",
      "Training Epoch 1  82.7% | batch:       567 of       686\t|\tloss: 42.159\n",
      "Training Epoch 1  82.8% | batch:       568 of       686\t|\tloss: 119.917\n",
      "Training Epoch 1  82.9% | batch:       569 of       686\t|\tloss: 37.9153\n",
      "Training Epoch 1  83.1% | batch:       570 of       686\t|\tloss: 50.3219\n",
      "Training Epoch 1  83.2% | batch:       571 of       686\t|\tloss: 41.861\n",
      "Training Epoch 1  83.4% | batch:       572 of       686\t|\tloss: 55.1933\n",
      "Training Epoch 1  83.5% | batch:       573 of       686\t|\tloss: 69.2855\n",
      "Training Epoch 1  83.7% | batch:       574 of       686\t|\tloss: 48.539\n",
      "Training Epoch 1  83.8% | batch:       575 of       686\t|\tloss: 106.198\n",
      "Training Epoch 1  84.0% | batch:       576 of       686\t|\tloss: 45.3546\n",
      "Training Epoch 1  84.1% | batch:       577 of       686\t|\tloss: 41.0244\n",
      "Training Epoch 1  84.3% | batch:       578 of       686\t|\tloss: 56.5367\n",
      "Training Epoch 1  84.4% | batch:       579 of       686\t|\tloss: 58.8755\n",
      "Training Epoch 1  84.5% | batch:       580 of       686\t|\tloss: 61.5907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  84.7% | batch:       581 of       686\t|\tloss: 37.7471\n",
      "Training Epoch 1  84.8% | batch:       582 of       686\t|\tloss: 54.3973\n",
      "Training Epoch 1  85.0% | batch:       583 of       686\t|\tloss: 53.8628\n",
      "Training Epoch 1  85.1% | batch:       584 of       686\t|\tloss: 45.6549\n",
      "Training Epoch 1  85.3% | batch:       585 of       686\t|\tloss: 34.4818\n",
      "Training Epoch 1  85.4% | batch:       586 of       686\t|\tloss: 51.8084\n",
      "Training Epoch 1  85.6% | batch:       587 of       686\t|\tloss: 50.7863\n",
      "Training Epoch 1  85.7% | batch:       588 of       686\t|\tloss: 42.951\n",
      "Training Epoch 1  85.9% | batch:       589 of       686\t|\tloss: 45.9656\n",
      "Training Epoch 1  86.0% | batch:       590 of       686\t|\tloss: 52.014\n",
      "Training Epoch 1  86.2% | batch:       591 of       686\t|\tloss: 58.7305\n",
      "Training Epoch 1  86.3% | batch:       592 of       686\t|\tloss: 47.4754\n",
      "Training Epoch 1  86.4% | batch:       593 of       686\t|\tloss: 37.7149\n",
      "Training Epoch 1  86.6% | batch:       594 of       686\t|\tloss: 46.1705\n",
      "Training Epoch 1  86.7% | batch:       595 of       686\t|\tloss: 45.0726\n",
      "Training Epoch 1  86.9% | batch:       596 of       686\t|\tloss: 38.0651\n",
      "Training Epoch 1  87.0% | batch:       597 of       686\t|\tloss: 47.4761\n",
      "Training Epoch 1  87.2% | batch:       598 of       686\t|\tloss: 39.0098\n",
      "Training Epoch 1  87.3% | batch:       599 of       686\t|\tloss: 55.3712\n",
      "Training Epoch 1  87.5% | batch:       600 of       686\t|\tloss: 55.4663\n",
      "Training Epoch 1  87.6% | batch:       601 of       686\t|\tloss: 41.2684\n",
      "Training Epoch 1  87.8% | batch:       602 of       686\t|\tloss: 65.9523\n",
      "Training Epoch 1  87.9% | batch:       603 of       686\t|\tloss: 46.2591\n",
      "Training Epoch 1  88.0% | batch:       604 of       686\t|\tloss: 49.2287\n",
      "Training Epoch 1  88.2% | batch:       605 of       686\t|\tloss: 53.6573\n",
      "Training Epoch 1  88.3% | batch:       606 of       686\t|\tloss: 50.7665\n",
      "Training Epoch 1  88.5% | batch:       607 of       686\t|\tloss: 30.777\n",
      "Training Epoch 1  88.6% | batch:       608 of       686\t|\tloss: 46.5962\n",
      "Training Epoch 1  88.8% | batch:       609 of       686\t|\tloss: 27.7819\n",
      "Training Epoch 1  88.9% | batch:       610 of       686\t|\tloss: 47.1943\n",
      "Training Epoch 1  89.1% | batch:       611 of       686\t|\tloss: 56.9167\n",
      "Training Epoch 1  89.2% | batch:       612 of       686\t|\tloss: 49.9904\n",
      "Training Epoch 1  89.4% | batch:       613 of       686\t|\tloss: 51.339\n",
      "Training Epoch 1  89.5% | batch:       614 of       686\t|\tloss: 38.1734\n",
      "Training Epoch 1  89.7% | batch:       615 of       686\t|\tloss: 72.6032\n",
      "Training Epoch 1  89.8% | batch:       616 of       686\t|\tloss: 51.4325\n",
      "Training Epoch 1  89.9% | batch:       617 of       686\t|\tloss: 62.7213\n",
      "Training Epoch 1  90.1% | batch:       618 of       686\t|\tloss: 76.8107\n",
      "Training Epoch 1  90.2% | batch:       619 of       686\t|\tloss: 50.3819\n",
      "Training Epoch 1  90.4% | batch:       620 of       686\t|\tloss: 41.4334\n",
      "Training Epoch 1  90.5% | batch:       621 of       686\t|\tloss: 58.1692\n",
      "Training Epoch 1  90.7% | batch:       622 of       686\t|\tloss: 44.0396\n",
      "Training Epoch 1  90.8% | batch:       623 of       686\t|\tloss: 41.7356\n",
      "Training Epoch 1  91.0% | batch:       624 of       686\t|\tloss: 52.7681\n",
      "Training Epoch 1  91.1% | batch:       625 of       686\t|\tloss: 43.3936\n",
      "Training Epoch 1  91.3% | batch:       626 of       686\t|\tloss: 53.8646\n",
      "Training Epoch 1  91.4% | batch:       627 of       686\t|\tloss: 40.0683\n",
      "Training Epoch 1  91.5% | batch:       628 of       686\t|\tloss: 41.3775\n",
      "Training Epoch 1  91.7% | batch:       629 of       686\t|\tloss: 192.162\n",
      "Training Epoch 1  91.8% | batch:       630 of       686\t|\tloss: 43.113\n",
      "Training Epoch 1  92.0% | batch:       631 of       686\t|\tloss: 41.857\n",
      "Training Epoch 1  92.1% | batch:       632 of       686\t|\tloss: 49.0184\n",
      "Training Epoch 1  92.3% | batch:       633 of       686\t|\tloss: 61.5195\n",
      "Training Epoch 1  92.4% | batch:       634 of       686\t|\tloss: 52.1961\n",
      "Training Epoch 1  92.6% | batch:       635 of       686\t|\tloss: 37.7806\n",
      "Training Epoch 1  92.7% | batch:       636 of       686\t|\tloss: 50.6978\n",
      "Training Epoch 1  92.9% | batch:       637 of       686\t|\tloss: 50.1239\n",
      "Training Epoch 1  93.0% | batch:       638 of       686\t|\tloss: 43.544\n",
      "Training Epoch 1  93.1% | batch:       639 of       686\t|\tloss: 52.817\n",
      "Training Epoch 1  93.3% | batch:       640 of       686\t|\tloss: 52.0524\n",
      "Training Epoch 1  93.4% | batch:       641 of       686\t|\tloss: 42.9661\n",
      "Training Epoch 1  93.6% | batch:       642 of       686\t|\tloss: 52.9819\n",
      "Training Epoch 1  93.7% | batch:       643 of       686\t|\tloss: 48.0032\n",
      "Training Epoch 1  93.9% | batch:       644 of       686\t|\tloss: 43.7116\n",
      "Training Epoch 1  94.0% | batch:       645 of       686\t|\tloss: 47.6834\n",
      "Training Epoch 1  94.2% | batch:       646 of       686\t|\tloss: 53.2397\n",
      "Training Epoch 1  94.3% | batch:       647 of       686\t|\tloss: 37.5336\n",
      "Training Epoch 1  94.5% | batch:       648 of       686\t|\tloss: 60.2181\n",
      "Training Epoch 1  94.6% | batch:       649 of       686\t|\tloss: 37.5659\n",
      "Training Epoch 1  94.8% | batch:       650 of       686\t|\tloss: 44.7272\n",
      "Training Epoch 1  94.9% | batch:       651 of       686\t|\tloss: 43.8446\n",
      "Training Epoch 1  95.0% | batch:       652 of       686\t|\tloss: 48.1947\n",
      "Training Epoch 1  95.2% | batch:       653 of       686\t|\tloss: 37.7525\n",
      "Training Epoch 1  95.3% | batch:       654 of       686\t|\tloss: 46.5747\n",
      "Training Epoch 1  95.5% | batch:       655 of       686\t|\tloss: 56.8643\n",
      "Training Epoch 1  95.6% | batch:       656 of       686\t|\tloss: 50.2093\n",
      "Training Epoch 1  95.8% | batch:       657 of       686\t|\tloss: 43.5132\n",
      "Training Epoch 1  95.9% | batch:       658 of       686\t|\tloss: 56.323\n",
      "Training Epoch 1  96.1% | batch:       659 of       686\t|\tloss: 44.5895\n",
      "Training Epoch 1  96.2% | batch:       660 of       686\t|\tloss: 33.31\n",
      "Training Epoch 1  96.4% | batch:       661 of       686\t|\tloss: 48.2939\n",
      "Training Epoch 1  96.5% | batch:       662 of       686\t|\tloss: 47.514\n",
      "Training Epoch 1  96.6% | batch:       663 of       686\t|\tloss: 42.3436\n",
      "Training Epoch 1  96.8% | batch:       664 of       686\t|\tloss: 36.9738\n",
      "Training Epoch 1  96.9% | batch:       665 of       686\t|\tloss: 46.8486\n",
      "Training Epoch 1  97.1% | batch:       666 of       686\t|\tloss: 37.6995\n",
      "Training Epoch 1  97.2% | batch:       667 of       686\t|\tloss: 29.78\n",
      "Training Epoch 1  97.4% | batch:       668 of       686\t|\tloss: 49.2394\n",
      "Training Epoch 1  97.5% | batch:       669 of       686\t|\tloss: 59.0919\n",
      "Training Epoch 1  97.7% | batch:       670 of       686\t|\tloss: 43.8367\n",
      "Training Epoch 1  97.8% | batch:       671 of       686\t|\tloss: 112.352\n",
      "Training Epoch 1  98.0% | batch:       672 of       686\t|\tloss: 47.6421\n",
      "Training Epoch 1  98.1% | batch:       673 of       686\t|\tloss: 42.1744\n",
      "Training Epoch 1  98.3% | batch:       674 of       686\t|\tloss: 43.5399\n",
      "Training Epoch 1  98.4% | batch:       675 of       686\t|\tloss: 52.8901\n",
      "Training Epoch 1  98.5% | batch:       676 of       686\t|\tloss: 74.001\n",
      "Training Epoch 1  98.7% | batch:       677 of       686\t|\tloss: 62.1953\n",
      "Training Epoch 1  98.8% | batch:       678 of       686\t|\tloss: 37.4282\n",
      "Training Epoch 1  99.0% | batch:       679 of       686\t|\tloss: 45.7653\n",
      "Training Epoch 1  99.1% | batch:       680 of       686\t|\tloss: 64.8717\n",
      "Training Epoch 1  99.3% | batch:       681 of       686\t|\tloss: 54.288\n",
      "Training Epoch 1  99.4% | batch:       682 of       686\t|\tloss: 38.4718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:00:16,934 | INFO : Epoch 1 Training Summary: epoch: 1.000000 | loss: 73.040875 | \n",
      "2023-05-25 22:00:16,935 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 24.069905281066895 seconds\n",
      "\n",
      "2023-05-25 22:00:16,935 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 24.069905281066895 seconds\n",
      "2023-05-25 22:00:16,936 | INFO : Avg batch train. time: 0.035087325482604806 seconds\n",
      "2023-05-25 22:00:16,936 | INFO : Avg sample train. time: 0.00027447294921109405 seconds\n",
      "2023-05-25 22:00:16,937 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  99.6% | batch:       683 of       686\t|\tloss: 39.5323\n",
      "Training Epoch 1  99.7% | batch:       684 of       686\t|\tloss: 51.7404\n",
      "Training Epoch 1  99.9% | batch:       685 of       686\t|\tloss: 95.7157\n",
      "\n",
      "Evaluating Epoch 1   0.0% | batch:         0 of       172\t|\tloss: 2.25972\n",
      "Evaluating Epoch 1   0.6% | batch:         1 of       172\t|\tloss: 19.2985\n",
      "Evaluating Epoch 1   1.2% | batch:         2 of       172\t|\tloss: 26.6075\n",
      "Evaluating Epoch 1   1.7% | batch:         3 of       172\t|\tloss: 6.31715\n",
      "Evaluating Epoch 1   2.3% | batch:         4 of       172\t|\tloss: 22.1248\n",
      "Evaluating Epoch 1   2.9% | batch:         5 of       172\t|\tloss: 7.47437\n",
      "Evaluating Epoch 1   3.5% | batch:         6 of       172\t|\tloss: 14.2713\n",
      "Evaluating Epoch 1   4.1% | batch:         7 of       172\t|\tloss: 8.19986\n",
      "Evaluating Epoch 1   4.7% | batch:         8 of       172\t|\tloss: 15.0125\n",
      "Evaluating Epoch 1   5.2% | batch:         9 of       172\t|\tloss: 4.47494\n",
      "Evaluating Epoch 1   5.8% | batch:        10 of       172\t|\tloss: 18.7021\n",
      "Evaluating Epoch 1   6.4% | batch:        11 of       172\t|\tloss: 16.7344\n",
      "Evaluating Epoch 1   7.0% | batch:        12 of       172\t|\tloss: 16.658\n",
      "Evaluating Epoch 1   7.6% | batch:        13 of       172\t|\tloss: 3.66252\n",
      "Evaluating Epoch 1   8.1% | batch:        14 of       172\t|\tloss: 14.1929\n",
      "Evaluating Epoch 1   8.7% | batch:        15 of       172\t|\tloss: 3.75566\n",
      "Evaluating Epoch 1   9.3% | batch:        16 of       172\t|\tloss: 16.6949\n",
      "Evaluating Epoch 1   9.9% | batch:        17 of       172\t|\tloss: 15.8293\n",
      "Evaluating Epoch 1  10.5% | batch:        18 of       172\t|\tloss: 58.6243\n",
      "Evaluating Epoch 1  11.0% | batch:        19 of       172\t|\tloss: 4.55676\n",
      "Evaluating Epoch 1  11.6% | batch:        20 of       172\t|\tloss: 8.41102\n",
      "Evaluating Epoch 1  12.2% | batch:        21 of       172\t|\tloss: 4.81381\n",
      "Evaluating Epoch 1  12.8% | batch:        22 of       172\t|\tloss: 30.1826\n",
      "Evaluating Epoch 1  13.4% | batch:        23 of       172\t|\tloss: 7.43276\n",
      "Evaluating Epoch 1  14.0% | batch:        24 of       172\t|\tloss: 6.42828\n",
      "Evaluating Epoch 1  14.5% | batch:        25 of       172\t|\tloss: 11.0252\n",
      "Evaluating Epoch 1  15.1% | batch:        26 of       172\t|\tloss: 29.1013\n",
      "Evaluating Epoch 1  15.7% | batch:        27 of       172\t|\tloss: 45.2624\n",
      "Evaluating Epoch 1  16.3% | batch:        28 of       172\t|\tloss: 3.20126\n",
      "Evaluating Epoch 1  16.9% | batch:        29 of       172\t|\tloss: 12.497\n",
      "Evaluating Epoch 1  17.4% | batch:        30 of       172\t|\tloss: 4.5937\n",
      "Evaluating Epoch 1  18.0% | batch:        31 of       172\t|\tloss: 26.4395\n",
      "Evaluating Epoch 1  18.6% | batch:        32 of       172\t|\tloss: 2.91708\n",
      "Evaluating Epoch 1  19.2% | batch:        33 of       172\t|\tloss: 9.66738\n",
      "Evaluating Epoch 1  19.8% | batch:        34 of       172\t|\tloss: 3.54586\n",
      "Evaluating Epoch 1  20.3% | batch:        35 of       172\t|\tloss: 4.47986\n",
      "Evaluating Epoch 1  20.9% | batch:        36 of       172\t|\tloss: 11.1659\n",
      "Evaluating Epoch 1  21.5% | batch:        37 of       172\t|\tloss: 9.26014\n",
      "Evaluating Epoch 1  22.1% | batch:        38 of       172\t|\tloss: 23.3904\n",
      "Evaluating Epoch 1  22.7% | batch:        39 of       172\t|\tloss: 33.874\n",
      "Evaluating Epoch 1  23.3% | batch:        40 of       172\t|\tloss: 3.07391\n",
      "Evaluating Epoch 1  23.8% | batch:        41 of       172\t|\tloss: 10.7686\n",
      "Evaluating Epoch 1  24.4% | batch:        42 of       172\t|\tloss: 2.35923\n",
      "Evaluating Epoch 1  25.0% | batch:        43 of       172\t|\tloss: 66.0425\n",
      "Evaluating Epoch 1  25.6% | batch:        44 of       172\t|\tloss: 4.58681\n",
      "Evaluating Epoch 1  26.2% | batch:        45 of       172\t|\tloss: 6.72908\n",
      "Evaluating Epoch 1  26.7% | batch:        46 of       172\t|\tloss: 4.50158\n",
      "Evaluating Epoch 1  27.3% | batch:        47 of       172\t|\tloss: 33.3295\n",
      "Evaluating Epoch 1  27.9% | batch:        48 of       172\t|\tloss: 5.24311\n",
      "Evaluating Epoch 1  28.5% | batch:        49 of       172\t|\tloss: 7.65374\n",
      "Evaluating Epoch 1  29.1% | batch:        50 of       172\t|\tloss: 6.11889\n",
      "Evaluating Epoch 1  29.7% | batch:        51 of       172\t|\tloss: 4.95675\n",
      "Evaluating Epoch 1  30.2% | batch:        52 of       172\t|\tloss: 92.9597\n",
      "Evaluating Epoch 1  30.8% | batch:        53 of       172\t|\tloss: 6.40919\n",
      "Evaluating Epoch 1  31.4% | batch:        54 of       172\t|\tloss: 65.8901\n",
      "Evaluating Epoch 1  32.0% | batch:        55 of       172\t|\tloss: 18.9601\n",
      "Evaluating Epoch 1  32.6% | batch:        56 of       172\t|\tloss: 17.0391\n",
      "Evaluating Epoch 1  33.1% | batch:        57 of       172\t|\tloss: 21.981\n",
      "Evaluating Epoch 1  33.7% | batch:        58 of       172\t|\tloss: 7.99514\n",
      "Evaluating Epoch 1  34.3% | batch:        59 of       172\t|\tloss: 74.2178\n",
      "Evaluating Epoch 1  34.9% | batch:        60 of       172\t|\tloss: 18.4918\n",
      "Evaluating Epoch 1  35.5% | batch:        61 of       172\t|\tloss: 25.8631\n",
      "Evaluating Epoch 1  36.0% | batch:        62 of       172\t|\tloss: 12.8054\n",
      "Evaluating Epoch 1  36.6% | batch:        63 of       172\t|\tloss: 15.0614\n",
      "Evaluating Epoch 1  37.2% | batch:        64 of       172\t|\tloss: 68.1321\n",
      "Evaluating Epoch 1  37.8% | batch:        65 of       172\t|\tloss: 4.8077\n",
      "Evaluating Epoch 1  38.4% | batch:        66 of       172\t|\tloss: 68.1879\n",
      "Evaluating Epoch 1  39.0% | batch:        67 of       172\t|\tloss: 21.6745\n",
      "Evaluating Epoch 1  39.5% | batch:        68 of       172\t|\tloss: 7.75728\n",
      "Evaluating Epoch 1  40.1% | batch:        69 of       172\t|\tloss: 96.4852\n",
      "Evaluating Epoch 1  40.7% | batch:        70 of       172\t|\tloss: 4.36886\n",
      "Evaluating Epoch 1  41.3% | batch:        71 of       172\t|\tloss: 57.5717\n",
      "Evaluating Epoch 1  41.9% | batch:        72 of       172\t|\tloss: 27.4012\n",
      "Evaluating Epoch 1  42.4% | batch:        73 of       172\t|\tloss: 5.19153\n",
      "Evaluating Epoch 1  43.0% | batch:        74 of       172\t|\tloss: 2.74847\n",
      "Evaluating Epoch 1  43.6% | batch:        75 of       172\t|\tloss: 4.77551\n",
      "Evaluating Epoch 1  44.2% | batch:        76 of       172\t|\tloss: 4.01764\n",
      "Evaluating Epoch 1  44.8% | batch:        77 of       172\t|\tloss: 4.56524\n",
      "Evaluating Epoch 1  45.3% | batch:        78 of       172\t|\tloss: 3.33921\n",
      "Evaluating Epoch 1  45.9% | batch:        79 of       172\t|\tloss: 4.90119\n",
      "Evaluating Epoch 1  46.5% | batch:        80 of       172\t|\tloss: 4.64515\n",
      "Evaluating Epoch 1  47.1% | batch:        81 of       172\t|\tloss: 2.37055\n",
      "Evaluating Epoch 1  47.7% | batch:        82 of       172\t|\tloss: 4.84325\n",
      "Evaluating Epoch 1  48.3% | batch:        83 of       172\t|\tloss: 13.4978\n",
      "Evaluating Epoch 1  48.8% | batch:        84 of       172\t|\tloss: 13.0727\n",
      "Evaluating Epoch 1  49.4% | batch:        85 of       172\t|\tloss: 10.7415\n",
      "Evaluating Epoch 1  50.0% | batch:        86 of       172\t|\tloss: 17.3765\n",
      "Evaluating Epoch 1  50.6% | batch:        87 of       172\t|\tloss: 13.5027\n",
      "Evaluating Epoch 1  51.2% | batch:        88 of       172\t|\tloss: 8.86163\n",
      "Evaluating Epoch 1  51.7% | batch:        89 of       172\t|\tloss: 18.898\n",
      "Evaluating Epoch 1  52.3% | batch:        90 of       172\t|\tloss: 13.7973\n",
      "Evaluating Epoch 1  52.9% | batch:        91 of       172\t|\tloss: 14.0453\n",
      "Evaluating Epoch 1  53.5% | batch:        92 of       172\t|\tloss: 14.9107\n",
      "Evaluating Epoch 1  54.1% | batch:        93 of       172\t|\tloss: 14.1413\n",
      "Evaluating Epoch 1  54.7% | batch:        94 of       172\t|\tloss: 20.1925\n",
      "Evaluating Epoch 1  55.2% | batch:        95 of       172\t|\tloss: 9.05137\n",
      "Evaluating Epoch 1  55.8% | batch:        96 of       172\t|\tloss: 17.3666\n",
      "Evaluating Epoch 1  56.4% | batch:        97 of       172\t|\tloss: 16.8156\n",
      "Evaluating Epoch 1  57.0% | batch:        98 of       172\t|\tloss: 13.8838\n",
      "Evaluating Epoch 1  57.6% | batch:        99 of       172\t|\tloss: 19.2558\n",
      "Evaluating Epoch 1  58.1% | batch:       100 of       172\t|\tloss: 12.7629\n",
      "Evaluating Epoch 1  58.7% | batch:       101 of       172\t|\tloss: 16.9626\n",
      "Evaluating Epoch 1  59.3% | batch:       102 of       172\t|\tloss: 8.77775\n",
      "Evaluating Epoch 1  59.9% | batch:       103 of       172\t|\tloss: 16.968\n",
      "Evaluating Epoch 1  60.5% | batch:       104 of       172\t|\tloss: 20.0275\n",
      "Evaluating Epoch 1  61.0% | batch:       105 of       172\t|\tloss: 9.20032\n",
      "Evaluating Epoch 1  61.6% | batch:       106 of       172\t|\tloss: 19.0871\n",
      "Evaluating Epoch 1  62.2% | batch:       107 of       172\t|\tloss: 13.9113\n",
      "Evaluating Epoch 1  62.8% | batch:       108 of       172\t|\tloss: 13.8571\n",
      "Evaluating Epoch 1  63.4% | batch:       109 of       172\t|\tloss: 11.5585\n",
      "Evaluating Epoch 1  64.0% | batch:       110 of       172\t|\tloss: 20.7655\n",
      "Evaluating Epoch 1  64.5% | batch:       111 of       172\t|\tloss: 19.4228\n",
      "Evaluating Epoch 1  65.1% | batch:       112 of       172\t|\tloss: 7.86998\n",
      "Evaluating Epoch 1  65.7% | batch:       113 of       172\t|\tloss: 20.4137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 1  66.3% | batch:       114 of       172\t|\tloss: 13.2245\n",
      "Evaluating Epoch 1  66.9% | batch:       115 of       172\t|\tloss: 29.2272\n",
      "Evaluating Epoch 1  67.4% | batch:       116 of       172\t|\tloss: 24.7223\n",
      "Evaluating Epoch 1  68.0% | batch:       117 of       172\t|\tloss: 27.6446\n",
      "Evaluating Epoch 1  68.6% | batch:       118 of       172\t|\tloss: 6.13954\n",
      "Evaluating Epoch 1  69.2% | batch:       119 of       172\t|\tloss: 35.5771\n",
      "Evaluating Epoch 1  69.8% | batch:       120 of       172\t|\tloss: 10.7462\n",
      "Evaluating Epoch 1  70.3% | batch:       121 of       172\t|\tloss: 188.43\n",
      "Evaluating Epoch 1  70.9% | batch:       122 of       172\t|\tloss: 28.2015\n",
      "Evaluating Epoch 1  71.5% | batch:       123 of       172\t|\tloss: 124.316\n",
      "Evaluating Epoch 1  72.1% | batch:       124 of       172\t|\tloss: 353.925\n",
      "Evaluating Epoch 1  72.7% | batch:       125 of       172\t|\tloss: 34.1394\n",
      "Evaluating Epoch 1  73.3% | batch:       126 of       172\t|\tloss: 17.3544\n",
      "Evaluating Epoch 1  73.8% | batch:       127 of       172\t|\tloss: 15.6239\n",
      "Evaluating Epoch 1  74.4% | batch:       128 of       172\t|\tloss: 69.0267\n",
      "Evaluating Epoch 1  75.0% | batch:       129 of       172\t|\tloss: 22.8804\n",
      "Evaluating Epoch 1  75.6% | batch:       130 of       172\t|\tloss: 5.37006\n",
      "Evaluating Epoch 1  76.2% | batch:       131 of       172\t|\tloss: 42.6122\n",
      "Evaluating Epoch 1  76.7% | batch:       132 of       172\t|\tloss: 7.37712\n",
      "Evaluating Epoch 1  77.3% | batch:       133 of       172\t|\tloss: 16.0226\n",
      "Evaluating Epoch 1  77.9% | batch:       134 of       172\t|\tloss: 5.55207\n",
      "Evaluating Epoch 1  78.5% | batch:       135 of       172\t|\tloss: 6.74881\n",
      "Evaluating Epoch 1  79.1% | batch:       136 of       172\t|\tloss: 5.83258\n",
      "Evaluating Epoch 1  79.7% | batch:       137 of       172\t|\tloss: 6.00385\n",
      "Evaluating Epoch 1  80.2% | batch:       138 of       172\t|\tloss: 11.5545\n",
      "Evaluating Epoch 1  80.8% | batch:       139 of       172\t|\tloss: 14.4831\n",
      "Evaluating Epoch 1  81.4% | batch:       140 of       172\t|\tloss: 3.21024\n",
      "Evaluating Epoch 1  82.0% | batch:       141 of       172\t|\tloss: 6.22109\n",
      "Evaluating Epoch 1  82.6% | batch:       142 of       172\t|\tloss: 1.74961\n",
      "Evaluating Epoch 1  83.1% | batch:       143 of       172\t|\tloss: 7.79162\n",
      "Evaluating Epoch 1  83.7% | batch:       144 of       172\t|\tloss: 3.99775\n",
      "Evaluating Epoch 1  84.3% | batch:       145 of       172\t|\tloss: 5.78365\n",
      "Evaluating Epoch 1  84.9% | batch:       146 of       172\t|\tloss: 4.5634\n",
      "Evaluating Epoch 1  85.5% | batch:       147 of       172\t|\tloss: 8.96401\n",
      "Evaluating Epoch 1  86.0% | batch:       148 of       172\t|\tloss: 2.38544\n",
      "Evaluating Epoch 1  86.6% | batch:       149 of       172\t|\tloss: 8.68377\n",
      "Evaluating Epoch 1  87.2% | batch:       150 of       172\t|\tloss: 7.25121\n",
      "Evaluating Epoch 1  87.8% | batch:       151 of       172\t|\tloss: 17.8365\n",
      "Evaluating Epoch 1  88.4% | batch:       152 of       172\t|\tloss: 22.7637\n",
      "Evaluating Epoch 1  89.0% | batch:       153 of       172\t|\tloss: 10.1351\n",
      "Evaluating Epoch 1  89.5% | batch:       154 of       172\t|\tloss: 24.0828\n",
      "Evaluating Epoch 1  90.1% | batch:       155 of       172\t|\tloss: 23.8951\n",
      "Evaluating Epoch 1  90.7% | batch:       156 of       172\t|\tloss: 14.7864\n",
      "Evaluating Epoch 1  91.3% | batch:       157 of       172\t|\tloss: 29.2803\n",
      "Evaluating Epoch 1  91.9% | batch:       158 of       172\t|\tloss: 9.03602\n",
      "Evaluating Epoch 1  92.4% | batch:       159 of       172\t|\tloss: 23.0029\n",
      "Evaluating Epoch 1  93.0% | batch:       160 of       172\t|\tloss: 255.946\n",
      "Evaluating Epoch 1  93.6% | batch:       161 of       172\t|\tloss: 145.56\n",
      "Evaluating Epoch 1  94.2% | batch:       162 of       172\t|\tloss: 22.0758\n",
      "Evaluating Epoch 1  94.8% | batch:       163 of       172\t|\tloss: 18.7597\n",
      "Evaluating Epoch 1  95.3% | batch:       164 of       172\t|\tloss: 21.5134\n",
      "Evaluating Epoch 1  95.9% | batch:       165 of       172\t|\tloss: 23.1704\n",
      "Evaluating Epoch 1  96.5% | batch:       166 of       172\t|\tloss: 7.23464\n",
      "Evaluating Epoch 1  97.1% | batch:       167 of       172\t|\tloss: 15.5598\n",
      "Evaluating Epoch 1  97.7% | batch:       168 of       172\t|\tloss: 18.0461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:00:21,260 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 4.323196172714233 seconds\n",
      "\n",
      "2023-05-25 22:00:21,261 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 5.018144488334656 seconds\n",
      "2023-05-25 22:00:21,262 | INFO : Avg batch val. time: 0.029175258653108462 seconds\n",
      "2023-05-25 22:00:21,262 | INFO : Avg sample val. time: 0.0002285441767242636 seconds\n",
      "2023-05-25 22:00:21,263 | INFO : Epoch 1 Validation Summary: epoch: 1.000000 | loss: 22.754293 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 1  98.3% | batch:       169 of       172\t|\tloss: 7.40224\n",
      "Evaluating Epoch 1  98.8% | batch:       170 of       172\t|\tloss: 21.3184\n",
      "Evaluating Epoch 1  99.4% | batch:       171 of       172\t|\tloss: 25.1343\n",
      "\n",
      "Training Epoch 2   0.0% | batch:         0 of       686\t|\tloss: 37.0684\n",
      "Training Epoch 2   0.1% | batch:         1 of       686\t|\tloss: 55.7671\n",
      "Training Epoch 2   0.3% | batch:         2 of       686\t|\tloss: 39.484\n",
      "Training Epoch 2   0.4% | batch:         3 of       686\t|\tloss: 73.1676\n",
      "Training Epoch 2   0.6% | batch:         4 of       686\t|\tloss: 47.7725\n",
      "Training Epoch 2   0.7% | batch:         5 of       686\t|\tloss: 88.402\n",
      "Training Epoch 2   0.9% | batch:         6 of       686\t|\tloss: 47.7109\n",
      "Training Epoch 2   1.0% | batch:         7 of       686\t|\tloss: 41.0777\n",
      "Training Epoch 2   1.2% | batch:         8 of       686\t|\tloss: 55.9931\n",
      "Training Epoch 2   1.3% | batch:         9 of       686\t|\tloss: 39.3272\n",
      "Training Epoch 2   1.5% | batch:        10 of       686\t|\tloss: 43.3882\n",
      "Training Epoch 2   1.6% | batch:        11 of       686\t|\tloss: 41.4266\n",
      "Training Epoch 2   1.7% | batch:        12 of       686\t|\tloss: 60.2382\n",
      "Training Epoch 2   1.9% | batch:        13 of       686\t|\tloss: 36.005\n",
      "Training Epoch 2   2.0% | batch:        14 of       686\t|\tloss: 42.2333\n",
      "Training Epoch 2   2.2% | batch:        15 of       686\t|\tloss: 45.9046\n",
      "Training Epoch 2   2.3% | batch:        16 of       686\t|\tloss: 48.3822\n",
      "Training Epoch 2   2.5% | batch:        17 of       686\t|\tloss: 51.65\n",
      "Training Epoch 2   2.6% | batch:        18 of       686\t|\tloss: 46.7546\n",
      "Training Epoch 2   2.8% | batch:        19 of       686\t|\tloss: 86.1886\n",
      "Training Epoch 2   2.9% | batch:        20 of       686\t|\tloss: 52.8284\n",
      "Training Epoch 2   3.1% | batch:        21 of       686\t|\tloss: 45.7903\n",
      "Training Epoch 2   3.2% | batch:        22 of       686\t|\tloss: 109.29\n",
      "Training Epoch 2   3.4% | batch:        23 of       686\t|\tloss: 46.1057\n",
      "Training Epoch 2   3.5% | batch:        24 of       686\t|\tloss: 58.997\n",
      "Training Epoch 2   3.6% | batch:        25 of       686\t|\tloss: 59.2902\n",
      "Training Epoch 2   3.8% | batch:        26 of       686\t|\tloss: 49.4673\n",
      "Training Epoch 2   3.9% | batch:        27 of       686\t|\tloss: 47.4637\n",
      "Training Epoch 2   4.1% | batch:        28 of       686\t|\tloss: 66.4247\n",
      "Training Epoch 2   4.2% | batch:        29 of       686\t|\tloss: 36.1703\n",
      "Training Epoch 2   4.4% | batch:        30 of       686\t|\tloss: 29.9502\n",
      "Training Epoch 2   4.5% | batch:        31 of       686\t|\tloss: 43.1331\n",
      "Training Epoch 2   4.7% | batch:        32 of       686\t|\tloss: 111.05\n",
      "Training Epoch 2   4.8% | batch:        33 of       686\t|\tloss: 39.8052\n",
      "Training Epoch 2   5.0% | batch:        34 of       686\t|\tloss: 52.436\n",
      "Training Epoch 2   5.1% | batch:        35 of       686\t|\tloss: 45.158\n",
      "Training Epoch 2   5.2% | batch:        36 of       686\t|\tloss: 45.7699\n",
      "Training Epoch 2   5.4% | batch:        37 of       686\t|\tloss: 32.2734\n",
      "Training Epoch 2   5.5% | batch:        38 of       686\t|\tloss: 42.7073\n",
      "Training Epoch 2   5.7% | batch:        39 of       686\t|\tloss: 44.8812\n",
      "Training Epoch 2   5.8% | batch:        40 of       686\t|\tloss: 59.963\n",
      "Training Epoch 2   6.0% | batch:        41 of       686\t|\tloss: 34.2063\n",
      "Training Epoch 2   6.1% | batch:        42 of       686\t|\tloss: 39.0328\n",
      "Training Epoch 2   6.3% | batch:        43 of       686\t|\tloss: 39.1316\n",
      "Training Epoch 2   6.4% | batch:        44 of       686\t|\tloss: 50.5856\n",
      "Training Epoch 2   6.6% | batch:        45 of       686\t|\tloss: 34.0573\n",
      "Training Epoch 2   6.7% | batch:        46 of       686\t|\tloss: 43.9327\n",
      "Training Epoch 2   6.9% | batch:        47 of       686\t|\tloss: 42.3654\n",
      "Training Epoch 2   7.0% | batch:        48 of       686\t|\tloss: 40.8216\n",
      "Training Epoch 2   7.1% | batch:        49 of       686\t|\tloss: 53.1631\n",
      "Training Epoch 2   7.3% | batch:        50 of       686\t|\tloss: 51.1387\n",
      "Training Epoch 2   7.4% | batch:        51 of       686\t|\tloss: 56.9379\n",
      "Training Epoch 2   7.6% | batch:        52 of       686\t|\tloss: 41.9229\n",
      "Training Epoch 2   7.7% | batch:        53 of       686\t|\tloss: 39.7349\n",
      "Training Epoch 2   7.9% | batch:        54 of       686\t|\tloss: 52.309\n",
      "Training Epoch 2   8.0% | batch:        55 of       686\t|\tloss: 57.703\n",
      "Training Epoch 2   8.2% | batch:        56 of       686\t|\tloss: 44.1679\n",
      "Training Epoch 2   8.3% | batch:        57 of       686\t|\tloss: 53.8022\n",
      "Training Epoch 2   8.5% | batch:        58 of       686\t|\tloss: 46.4087\n",
      "Training Epoch 2   8.6% | batch:        59 of       686\t|\tloss: 60.3889\n",
      "Training Epoch 2   8.7% | batch:        60 of       686\t|\tloss: 44.6224\n",
      "Training Epoch 2   8.9% | batch:        61 of       686\t|\tloss: 40.4965\n",
      "Training Epoch 2   9.0% | batch:        62 of       686\t|\tloss: 71.0342\n",
      "Training Epoch 2   9.2% | batch:        63 of       686\t|\tloss: 35\n",
      "Training Epoch 2   9.3% | batch:        64 of       686\t|\tloss: 47.4393\n",
      "Training Epoch 2   9.5% | batch:        65 of       686\t|\tloss: 53.3469\n",
      "Training Epoch 2   9.6% | batch:        66 of       686\t|\tloss: 48.272\n",
      "Training Epoch 2   9.8% | batch:        67 of       686\t|\tloss: 54.8187\n",
      "Training Epoch 2   9.9% | batch:        68 of       686\t|\tloss: 56.0133\n",
      "Training Epoch 2  10.1% | batch:        69 of       686\t|\tloss: 45.2367\n",
      "Training Epoch 2  10.2% | batch:        70 of       686\t|\tloss: 51.2736\n",
      "Training Epoch 2  10.3% | batch:        71 of       686\t|\tloss: 42.7417\n",
      "Training Epoch 2  10.5% | batch:        72 of       686\t|\tloss: 44.9075\n",
      "Training Epoch 2  10.6% | batch:        73 of       686\t|\tloss: 51.9729\n",
      "Training Epoch 2  10.8% | batch:        74 of       686\t|\tloss: 47.8249\n",
      "Training Epoch 2  10.9% | batch:        75 of       686\t|\tloss: 41.1895\n",
      "Training Epoch 2  11.1% | batch:        76 of       686\t|\tloss: 33.5265\n",
      "Training Epoch 2  11.2% | batch:        77 of       686\t|\tloss: 37.6343\n",
      "Training Epoch 2  11.4% | batch:        78 of       686\t|\tloss: 45.2086\n",
      "Training Epoch 2  11.5% | batch:        79 of       686\t|\tloss: 48.8054\n",
      "Training Epoch 2  11.7% | batch:        80 of       686\t|\tloss: 41.8677\n",
      "Training Epoch 2  11.8% | batch:        81 of       686\t|\tloss: 51.6621\n",
      "Training Epoch 2  12.0% | batch:        82 of       686\t|\tloss: 58.3798\n",
      "Training Epoch 2  12.1% | batch:        83 of       686\t|\tloss: 66.7519\n",
      "Training Epoch 2  12.2% | batch:        84 of       686\t|\tloss: 39.2007\n",
      "Training Epoch 2  12.4% | batch:        85 of       686\t|\tloss: 41.2534\n",
      "Training Epoch 2  12.5% | batch:        86 of       686\t|\tloss: 45.2222\n",
      "Training Epoch 2  12.7% | batch:        87 of       686\t|\tloss: 41.0896\n",
      "Training Epoch 2  12.8% | batch:        88 of       686\t|\tloss: 36.1284\n",
      "Training Epoch 2  13.0% | batch:        89 of       686\t|\tloss: 48.3231\n",
      "Training Epoch 2  13.1% | batch:        90 of       686\t|\tloss: 50.2161\n",
      "Training Epoch 2  13.3% | batch:        91 of       686\t|\tloss: 37.2057\n",
      "Training Epoch 2  13.4% | batch:        92 of       686\t|\tloss: 42.3408\n",
      "Training Epoch 2  13.6% | batch:        93 of       686\t|\tloss: 51.1258\n",
      "Training Epoch 2  13.7% | batch:        94 of       686\t|\tloss: 55.9863\n",
      "Training Epoch 2  13.8% | batch:        95 of       686\t|\tloss: 41.5007\n",
      "Training Epoch 2  14.0% | batch:        96 of       686\t|\tloss: 38.3671\n",
      "Training Epoch 2  14.1% | batch:        97 of       686\t|\tloss: 42.9348\n",
      "Training Epoch 2  14.3% | batch:        98 of       686\t|\tloss: 42.6599\n",
      "Training Epoch 2  14.4% | batch:        99 of       686\t|\tloss: 39.6717\n",
      "Training Epoch 2  14.6% | batch:       100 of       686\t|\tloss: 44.1558\n",
      "Training Epoch 2  14.7% | batch:       101 of       686\t|\tloss: 39.2534\n",
      "Training Epoch 2  14.9% | batch:       102 of       686\t|\tloss: 37.0882\n",
      "Training Epoch 2  15.0% | batch:       103 of       686\t|\tloss: 41.5055\n",
      "Training Epoch 2  15.2% | batch:       104 of       686\t|\tloss: 48.8251\n",
      "Training Epoch 2  15.3% | batch:       105 of       686\t|\tloss: 62.1962\n",
      "Training Epoch 2  15.5% | batch:       106 of       686\t|\tloss: 47.5117\n",
      "Training Epoch 2  15.6% | batch:       107 of       686\t|\tloss: 43.44\n",
      "Training Epoch 2  15.7% | batch:       108 of       686\t|\tloss: 43.9215\n",
      "Training Epoch 2  15.9% | batch:       109 of       686\t|\tloss: 49.9593\n",
      "Training Epoch 2  16.0% | batch:       110 of       686\t|\tloss: 58.4544\n",
      "Training Epoch 2  16.2% | batch:       111 of       686\t|\tloss: 36.8623\n",
      "Training Epoch 2  16.3% | batch:       112 of       686\t|\tloss: 52.4211\n",
      "Training Epoch 2  16.5% | batch:       113 of       686\t|\tloss: 33.3304\n",
      "Training Epoch 2  16.6% | batch:       114 of       686\t|\tloss: 43.946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  16.8% | batch:       115 of       686\t|\tloss: 64.1002\n",
      "Training Epoch 2  16.9% | batch:       116 of       686\t|\tloss: 47.2757\n",
      "Training Epoch 2  17.1% | batch:       117 of       686\t|\tloss: 30.3739\n",
      "Training Epoch 2  17.2% | batch:       118 of       686\t|\tloss: 44.4694\n",
      "Training Epoch 2  17.3% | batch:       119 of       686\t|\tloss: 47.6836\n",
      "Training Epoch 2  17.5% | batch:       120 of       686\t|\tloss: 33.6242\n",
      "Training Epoch 2  17.6% | batch:       121 of       686\t|\tloss: 33.6642\n",
      "Training Epoch 2  17.8% | batch:       122 of       686\t|\tloss: 74.0348\n",
      "Training Epoch 2  17.9% | batch:       123 of       686\t|\tloss: 50.1404\n",
      "Training Epoch 2  18.1% | batch:       124 of       686\t|\tloss: 44.2424\n",
      "Training Epoch 2  18.2% | batch:       125 of       686\t|\tloss: 40.9699\n",
      "Training Epoch 2  18.4% | batch:       126 of       686\t|\tloss: 47.6495\n",
      "Training Epoch 2  18.5% | batch:       127 of       686\t|\tloss: 40.84\n",
      "Training Epoch 2  18.7% | batch:       128 of       686\t|\tloss: 34.0282\n",
      "Training Epoch 2  18.8% | batch:       129 of       686\t|\tloss: 40.0166\n",
      "Training Epoch 2  19.0% | batch:       130 of       686\t|\tloss: 41.8971\n",
      "Training Epoch 2  19.1% | batch:       131 of       686\t|\tloss: 57.7222\n",
      "Training Epoch 2  19.2% | batch:       132 of       686\t|\tloss: 26.9946\n",
      "Training Epoch 2  19.4% | batch:       133 of       686\t|\tloss: 44.0567\n",
      "Training Epoch 2  19.5% | batch:       134 of       686\t|\tloss: 42.4164\n",
      "Training Epoch 2  19.7% | batch:       135 of       686\t|\tloss: 43.9202\n",
      "Training Epoch 2  19.8% | batch:       136 of       686\t|\tloss: 49.7641\n",
      "Training Epoch 2  20.0% | batch:       137 of       686\t|\tloss: 35.3939\n",
      "Training Epoch 2  20.1% | batch:       138 of       686\t|\tloss: 32.0603\n",
      "Training Epoch 2  20.3% | batch:       139 of       686\t|\tloss: 60.3739\n",
      "Training Epoch 2  20.4% | batch:       140 of       686\t|\tloss: 31.362\n",
      "Training Epoch 2  20.6% | batch:       141 of       686\t|\tloss: 43.7546\n",
      "Training Epoch 2  20.7% | batch:       142 of       686\t|\tloss: 34.0688\n",
      "Training Epoch 2  20.8% | batch:       143 of       686\t|\tloss: 45.8084\n",
      "Training Epoch 2  21.0% | batch:       144 of       686\t|\tloss: 33.1627\n",
      "Training Epoch 2  21.1% | batch:       145 of       686\t|\tloss: 62.1219\n",
      "Training Epoch 2  21.3% | batch:       146 of       686\t|\tloss: 44.7802\n",
      "Training Epoch 2  21.4% | batch:       147 of       686\t|\tloss: 42.9207\n",
      "Training Epoch 2  21.6% | batch:       148 of       686\t|\tloss: 36.3379\n",
      "Training Epoch 2  21.7% | batch:       149 of       686\t|\tloss: 43.8673\n",
      "Training Epoch 2  21.9% | batch:       150 of       686\t|\tloss: 39.4768\n",
      "Training Epoch 2  22.0% | batch:       151 of       686\t|\tloss: 38.022\n",
      "Training Epoch 2  22.2% | batch:       152 of       686\t|\tloss: 46.6261\n",
      "Training Epoch 2  22.3% | batch:       153 of       686\t|\tloss: 38.0602\n",
      "Training Epoch 2  22.4% | batch:       154 of       686\t|\tloss: 46.9318\n",
      "Training Epoch 2  22.6% | batch:       155 of       686\t|\tloss: 43.3662\n",
      "Training Epoch 2  22.7% | batch:       156 of       686\t|\tloss: 46.3312\n",
      "Training Epoch 2  22.9% | batch:       157 of       686\t|\tloss: 42.7301\n",
      "Training Epoch 2  23.0% | batch:       158 of       686\t|\tloss: 51.4722\n",
      "Training Epoch 2  23.2% | batch:       159 of       686\t|\tloss: 38.8877\n",
      "Training Epoch 2  23.3% | batch:       160 of       686\t|\tloss: 32.5032\n",
      "Training Epoch 2  23.5% | batch:       161 of       686\t|\tloss: 41.4555\n",
      "Training Epoch 2  23.6% | batch:       162 of       686\t|\tloss: 53.3763\n",
      "Training Epoch 2  23.8% | batch:       163 of       686\t|\tloss: 36.9434\n",
      "Training Epoch 2  23.9% | batch:       164 of       686\t|\tloss: 48.9163\n",
      "Training Epoch 2  24.1% | batch:       165 of       686\t|\tloss: 49.7866\n",
      "Training Epoch 2  24.2% | batch:       166 of       686\t|\tloss: 45.8885\n",
      "Training Epoch 2  24.3% | batch:       167 of       686\t|\tloss: 45.3733\n",
      "Training Epoch 2  24.5% | batch:       168 of       686\t|\tloss: 36.0136\n",
      "Training Epoch 2  24.6% | batch:       169 of       686\t|\tloss: 41.0298\n",
      "Training Epoch 2  24.8% | batch:       170 of       686\t|\tloss: 40.9345\n",
      "Training Epoch 2  24.9% | batch:       171 of       686\t|\tloss: 53.5799\n",
      "Training Epoch 2  25.1% | batch:       172 of       686\t|\tloss: 37.6121\n",
      "Training Epoch 2  25.2% | batch:       173 of       686\t|\tloss: 47.895\n",
      "Training Epoch 2  25.4% | batch:       174 of       686\t|\tloss: 46.7993\n",
      "Training Epoch 2  25.5% | batch:       175 of       686\t|\tloss: 48.567\n",
      "Training Epoch 2  25.7% | batch:       176 of       686\t|\tloss: 46.2609\n",
      "Training Epoch 2  25.8% | batch:       177 of       686\t|\tloss: 41.6541\n",
      "Training Epoch 2  25.9% | batch:       178 of       686\t|\tloss: 40.6029\n",
      "Training Epoch 2  26.1% | batch:       179 of       686\t|\tloss: 52.3081\n",
      "Training Epoch 2  26.2% | batch:       180 of       686\t|\tloss: 52.5353\n",
      "Training Epoch 2  26.4% | batch:       181 of       686\t|\tloss: 46.4603\n",
      "Training Epoch 2  26.5% | batch:       182 of       686\t|\tloss: 54.203\n",
      "Training Epoch 2  26.7% | batch:       183 of       686\t|\tloss: 40.0182\n",
      "Training Epoch 2  26.8% | batch:       184 of       686\t|\tloss: 122.794\n",
      "Training Epoch 2  27.0% | batch:       185 of       686\t|\tloss: 40.1543\n",
      "Training Epoch 2  27.1% | batch:       186 of       686\t|\tloss: 55.0231\n",
      "Training Epoch 2  27.3% | batch:       187 of       686\t|\tloss: 42.6163\n",
      "Training Epoch 2  27.4% | batch:       188 of       686\t|\tloss: 38.8445\n",
      "Training Epoch 2  27.6% | batch:       189 of       686\t|\tloss: 35.7458\n",
      "Training Epoch 2  27.7% | batch:       190 of       686\t|\tloss: 50.8849\n",
      "Training Epoch 2  27.8% | batch:       191 of       686\t|\tloss: 44.0525\n",
      "Training Epoch 2  28.0% | batch:       192 of       686\t|\tloss: 31.7855\n",
      "Training Epoch 2  28.1% | batch:       193 of       686\t|\tloss: 59.8887\n",
      "Training Epoch 2  28.3% | batch:       194 of       686\t|\tloss: 44.9914\n",
      "Training Epoch 2  28.4% | batch:       195 of       686\t|\tloss: 47.6822\n",
      "Training Epoch 2  28.6% | batch:       196 of       686\t|\tloss: 42.6146\n",
      "Training Epoch 2  28.7% | batch:       197 of       686\t|\tloss: 40.0698\n",
      "Training Epoch 2  28.9% | batch:       198 of       686\t|\tloss: 40.6748\n",
      "Training Epoch 2  29.0% | batch:       199 of       686\t|\tloss: 50.4943\n",
      "Training Epoch 2  29.2% | batch:       200 of       686\t|\tloss: 48.1425\n",
      "Training Epoch 2  29.3% | batch:       201 of       686\t|\tloss: 40.9791\n",
      "Training Epoch 2  29.4% | batch:       202 of       686\t|\tloss: 44.5621\n",
      "Training Epoch 2  29.6% | batch:       203 of       686\t|\tloss: 39.8989\n",
      "Training Epoch 2  29.7% | batch:       204 of       686\t|\tloss: 44.5041\n",
      "Training Epoch 2  29.9% | batch:       205 of       686\t|\tloss: 27.3547\n",
      "Training Epoch 2  30.0% | batch:       206 of       686\t|\tloss: 50.2935\n",
      "Training Epoch 2  30.2% | batch:       207 of       686\t|\tloss: 56.7972\n",
      "Training Epoch 2  30.3% | batch:       208 of       686\t|\tloss: 60.4811\n",
      "Training Epoch 2  30.5% | batch:       209 of       686\t|\tloss: 35.2912\n",
      "Training Epoch 2  30.6% | batch:       210 of       686\t|\tloss: 41.7825\n",
      "Training Epoch 2  30.8% | batch:       211 of       686\t|\tloss: 51.2295\n",
      "Training Epoch 2  30.9% | batch:       212 of       686\t|\tloss: 51.0245\n",
      "Training Epoch 2  31.0% | batch:       213 of       686\t|\tloss: 54.5162\n",
      "Training Epoch 2  31.2% | batch:       214 of       686\t|\tloss: 53.106\n",
      "Training Epoch 2  31.3% | batch:       215 of       686\t|\tloss: 51.8503\n",
      "Training Epoch 2  31.5% | batch:       216 of       686\t|\tloss: 48.9438\n",
      "Training Epoch 2  31.6% | batch:       217 of       686\t|\tloss: 50.1454\n",
      "Training Epoch 2  31.8% | batch:       218 of       686\t|\tloss: 41.7683\n",
      "Training Epoch 2  31.9% | batch:       219 of       686\t|\tloss: 53.5175\n",
      "Training Epoch 2  32.1% | batch:       220 of       686\t|\tloss: 39.8508\n",
      "Training Epoch 2  32.2% | batch:       221 of       686\t|\tloss: 42.7544\n",
      "Training Epoch 2  32.4% | batch:       222 of       686\t|\tloss: 43.8722\n",
      "Training Epoch 2  32.5% | batch:       223 of       686\t|\tloss: 56.0168\n",
      "Training Epoch 2  32.7% | batch:       224 of       686\t|\tloss: 32.8336\n",
      "Training Epoch 2  32.8% | batch:       225 of       686\t|\tloss: 52.8509\n",
      "Training Epoch 2  32.9% | batch:       226 of       686\t|\tloss: 38.1324\n",
      "Training Epoch 2  33.1% | batch:       227 of       686\t|\tloss: 46.1799\n",
      "Training Epoch 2  33.2% | batch:       228 of       686\t|\tloss: 37.6883\n",
      "Training Epoch 2  33.4% | batch:       229 of       686\t|\tloss: 48.2158\n",
      "Training Epoch 2  33.5% | batch:       230 of       686\t|\tloss: 37.4885\n",
      "Training Epoch 2  33.7% | batch:       231 of       686\t|\tloss: 40.187\n",
      "Training Epoch 2  33.8% | batch:       232 of       686\t|\tloss: 66.4143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  34.0% | batch:       233 of       686\t|\tloss: 37.2252\n",
      "Training Epoch 2  34.1% | batch:       234 of       686\t|\tloss: 33.6616\n",
      "Training Epoch 2  34.3% | batch:       235 of       686\t|\tloss: 35.9827\n",
      "Training Epoch 2  34.4% | batch:       236 of       686\t|\tloss: 34.8477\n",
      "Training Epoch 2  34.5% | batch:       237 of       686\t|\tloss: 40.8243\n",
      "Training Epoch 2  34.7% | batch:       238 of       686\t|\tloss: 49.5752\n",
      "Training Epoch 2  34.8% | batch:       239 of       686\t|\tloss: 53.8851\n",
      "Training Epoch 2  35.0% | batch:       240 of       686\t|\tloss: 51.616\n",
      "Training Epoch 2  35.1% | batch:       241 of       686\t|\tloss: 35.4905\n",
      "Training Epoch 2  35.3% | batch:       242 of       686\t|\tloss: 36.0266\n",
      "Training Epoch 2  35.4% | batch:       243 of       686\t|\tloss: 32.9821\n",
      "Training Epoch 2  35.6% | batch:       244 of       686\t|\tloss: 45.4177\n",
      "Training Epoch 2  35.7% | batch:       245 of       686\t|\tloss: 137.45\n",
      "Training Epoch 2  35.9% | batch:       246 of       686\t|\tloss: 34.4412\n",
      "Training Epoch 2  36.0% | batch:       247 of       686\t|\tloss: 38.3139\n",
      "Training Epoch 2  36.2% | batch:       248 of       686\t|\tloss: 40.7455\n",
      "Training Epoch 2  36.3% | batch:       249 of       686\t|\tloss: 39.9268\n",
      "Training Epoch 2  36.4% | batch:       250 of       686\t|\tloss: 40.5944\n",
      "Training Epoch 2  36.6% | batch:       251 of       686\t|\tloss: 60.9541\n",
      "Training Epoch 2  36.7% | batch:       252 of       686\t|\tloss: 44.8545\n",
      "Training Epoch 2  36.9% | batch:       253 of       686\t|\tloss: 31.2956\n",
      "Training Epoch 2  37.0% | batch:       254 of       686\t|\tloss: 30.886\n",
      "Training Epoch 2  37.2% | batch:       255 of       686\t|\tloss: 44.6266\n",
      "Training Epoch 2  37.3% | batch:       256 of       686\t|\tloss: 36.7437\n",
      "Training Epoch 2  37.5% | batch:       257 of       686\t|\tloss: 52.9659\n",
      "Training Epoch 2  37.6% | batch:       258 of       686\t|\tloss: 36.8183\n",
      "Training Epoch 2  37.8% | batch:       259 of       686\t|\tloss: 48.4978\n",
      "Training Epoch 2  37.9% | batch:       260 of       686\t|\tloss: 34.4906\n",
      "Training Epoch 2  38.0% | batch:       261 of       686\t|\tloss: 46.7959\n",
      "Training Epoch 2  38.2% | batch:       262 of       686\t|\tloss: 43.7001\n",
      "Training Epoch 2  38.3% | batch:       263 of       686\t|\tloss: 40.6514\n",
      "Training Epoch 2  38.5% | batch:       264 of       686\t|\tloss: 50.5749\n",
      "Training Epoch 2  38.6% | batch:       265 of       686\t|\tloss: 91.4241\n",
      "Training Epoch 2  38.8% | batch:       266 of       686\t|\tloss: 23.0737\n",
      "Training Epoch 2  38.9% | batch:       267 of       686\t|\tloss: 41.9546\n",
      "Training Epoch 2  39.1% | batch:       268 of       686\t|\tloss: 34.436\n",
      "Training Epoch 2  39.2% | batch:       269 of       686\t|\tloss: 34.861\n",
      "Training Epoch 2  39.4% | batch:       270 of       686\t|\tloss: 44.0326\n",
      "Training Epoch 2  39.5% | batch:       271 of       686\t|\tloss: 32.0237\n",
      "Training Epoch 2  39.7% | batch:       272 of       686\t|\tloss: 56.6719\n",
      "Training Epoch 2  39.8% | batch:       273 of       686\t|\tloss: 50.5216\n",
      "Training Epoch 2  39.9% | batch:       274 of       686\t|\tloss: 45.878\n",
      "Training Epoch 2  40.1% | batch:       275 of       686\t|\tloss: 42.5712\n",
      "Training Epoch 2  40.2% | batch:       276 of       686\t|\tloss: 32.4199\n",
      "Training Epoch 2  40.4% | batch:       277 of       686\t|\tloss: 36.5092\n",
      "Training Epoch 2  40.5% | batch:       278 of       686\t|\tloss: 34.9991\n",
      "Training Epoch 2  40.7% | batch:       279 of       686\t|\tloss: 36.892\n",
      "Training Epoch 2  40.8% | batch:       280 of       686\t|\tloss: 48.0841\n",
      "Training Epoch 2  41.0% | batch:       281 of       686\t|\tloss: 24.4688\n",
      "Training Epoch 2  41.1% | batch:       282 of       686\t|\tloss: 38.6151\n",
      "Training Epoch 2  41.3% | batch:       283 of       686\t|\tloss: 40.4219\n",
      "Training Epoch 2  41.4% | batch:       284 of       686\t|\tloss: 62.4442\n",
      "Training Epoch 2  41.5% | batch:       285 of       686\t|\tloss: 53.3246\n",
      "Training Epoch 2  41.7% | batch:       286 of       686\t|\tloss: 33.3439\n",
      "Training Epoch 2  41.8% | batch:       287 of       686\t|\tloss: 45.1603\n",
      "Training Epoch 2  42.0% | batch:       288 of       686\t|\tloss: 49.5257\n",
      "Training Epoch 2  42.1% | batch:       289 of       686\t|\tloss: 40.4322\n",
      "Training Epoch 2  42.3% | batch:       290 of       686\t|\tloss: 44.1871\n",
      "Training Epoch 2  42.4% | batch:       291 of       686\t|\tloss: 44.5299\n",
      "Training Epoch 2  42.6% | batch:       292 of       686\t|\tloss: 30.0455\n",
      "Training Epoch 2  42.7% | batch:       293 of       686\t|\tloss: 43.3228\n",
      "Training Epoch 2  42.9% | batch:       294 of       686\t|\tloss: 34.0295\n",
      "Training Epoch 2  43.0% | batch:       295 of       686\t|\tloss: 34.7076\n",
      "Training Epoch 2  43.1% | batch:       296 of       686\t|\tloss: 42.5853\n",
      "Training Epoch 2  43.3% | batch:       297 of       686\t|\tloss: 67.0723\n",
      "Training Epoch 2  43.4% | batch:       298 of       686\t|\tloss: 43.1912\n",
      "Training Epoch 2  43.6% | batch:       299 of       686\t|\tloss: 40.6571\n",
      "Training Epoch 2  43.7% | batch:       300 of       686\t|\tloss: 41.5669\n",
      "Training Epoch 2  43.9% | batch:       301 of       686\t|\tloss: 55.5915\n",
      "Training Epoch 2  44.0% | batch:       302 of       686\t|\tloss: 73.9677\n",
      "Training Epoch 2  44.2% | batch:       303 of       686\t|\tloss: 47.7902\n",
      "Training Epoch 2  44.3% | batch:       304 of       686\t|\tloss: 95.505\n",
      "Training Epoch 2  44.5% | batch:       305 of       686\t|\tloss: 40.6535\n",
      "Training Epoch 2  44.6% | batch:       306 of       686\t|\tloss: 39.7651\n",
      "Training Epoch 2  44.8% | batch:       307 of       686\t|\tloss: 33.5458\n",
      "Training Epoch 2  44.9% | batch:       308 of       686\t|\tloss: 48.6005\n",
      "Training Epoch 2  45.0% | batch:       309 of       686\t|\tloss: 55.2278\n",
      "Training Epoch 2  45.2% | batch:       310 of       686\t|\tloss: 35.2506\n",
      "Training Epoch 2  45.3% | batch:       311 of       686\t|\tloss: 31.9893\n",
      "Training Epoch 2  45.5% | batch:       312 of       686\t|\tloss: 40.1983\n",
      "Training Epoch 2  45.6% | batch:       313 of       686\t|\tloss: 42.426\n",
      "Training Epoch 2  45.8% | batch:       314 of       686\t|\tloss: 40.0743\n",
      "Training Epoch 2  45.9% | batch:       315 of       686\t|\tloss: 39.5712\n",
      "Training Epoch 2  46.1% | batch:       316 of       686\t|\tloss: 43.7599\n",
      "Training Epoch 2  46.2% | batch:       317 of       686\t|\tloss: 46.8767\n",
      "Training Epoch 2  46.4% | batch:       318 of       686\t|\tloss: 46.8073\n",
      "Training Epoch 2  46.5% | batch:       319 of       686\t|\tloss: 40.7847\n",
      "Training Epoch 2  46.6% | batch:       320 of       686\t|\tloss: 41.8913\n",
      "Training Epoch 2  46.8% | batch:       321 of       686\t|\tloss: 37.0333\n",
      "Training Epoch 2  46.9% | batch:       322 of       686\t|\tloss: 33.5031\n",
      "Training Epoch 2  47.1% | batch:       323 of       686\t|\tloss: 48.8393\n",
      "Training Epoch 2  47.2% | batch:       324 of       686\t|\tloss: 45.916\n",
      "Training Epoch 2  47.4% | batch:       325 of       686\t|\tloss: 46.0739\n",
      "Training Epoch 2  47.5% | batch:       326 of       686\t|\tloss: 44.3726\n",
      "Training Epoch 2  47.7% | batch:       327 of       686\t|\tloss: 43.7173\n",
      "Training Epoch 2  47.8% | batch:       328 of       686\t|\tloss: 36.0023\n",
      "Training Epoch 2  48.0% | batch:       329 of       686\t|\tloss: 45.2239\n",
      "Training Epoch 2  48.1% | batch:       330 of       686\t|\tloss: 39.0938\n",
      "Training Epoch 2  48.3% | batch:       331 of       686\t|\tloss: 48.0086\n",
      "Training Epoch 2  48.4% | batch:       332 of       686\t|\tloss: 35.3831\n",
      "Training Epoch 2  48.5% | batch:       333 of       686\t|\tloss: 39.3773\n",
      "Training Epoch 2  48.7% | batch:       334 of       686\t|\tloss: 36.6555\n",
      "Training Epoch 2  48.8% | batch:       335 of       686\t|\tloss: 42.8342\n",
      "Training Epoch 2  49.0% | batch:       336 of       686\t|\tloss: 43.3141\n",
      "Training Epoch 2  49.1% | batch:       337 of       686\t|\tloss: 43.877\n",
      "Training Epoch 2  49.3% | batch:       338 of       686\t|\tloss: 39.9805\n",
      "Training Epoch 2  49.4% | batch:       339 of       686\t|\tloss: 30.8007\n",
      "Training Epoch 2  49.6% | batch:       340 of       686\t|\tloss: 55.0596\n",
      "Training Epoch 2  49.7% | batch:       341 of       686\t|\tloss: 41.3211\n",
      "Training Epoch 2  49.9% | batch:       342 of       686\t|\tloss: 43.0198\n",
      "Training Epoch 2  50.0% | batch:       343 of       686\t|\tloss: 39.3949\n",
      "Training Epoch 2  50.1% | batch:       344 of       686\t|\tloss: 43.7587\n",
      "Training Epoch 2  50.3% | batch:       345 of       686\t|\tloss: 36.0384\n",
      "Training Epoch 2  50.4% | batch:       346 of       686\t|\tloss: 40.8146\n",
      "Training Epoch 2  50.6% | batch:       347 of       686\t|\tloss: 46.4643\n",
      "Training Epoch 2  50.7% | batch:       348 of       686\t|\tloss: 33.6982\n",
      "Training Epoch 2  50.9% | batch:       349 of       686\t|\tloss: 42.2565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  51.0% | batch:       350 of       686\t|\tloss: 45.8941\n",
      "Training Epoch 2  51.2% | batch:       351 of       686\t|\tloss: 39.3536\n",
      "Training Epoch 2  51.3% | batch:       352 of       686\t|\tloss: 41.2199\n",
      "Training Epoch 2  51.5% | batch:       353 of       686\t|\tloss: 39.7351\n",
      "Training Epoch 2  51.6% | batch:       354 of       686\t|\tloss: 43.8347\n",
      "Training Epoch 2  51.7% | batch:       355 of       686\t|\tloss: 49.5322\n",
      "Training Epoch 2  51.9% | batch:       356 of       686\t|\tloss: 44.7704\n",
      "Training Epoch 2  52.0% | batch:       357 of       686\t|\tloss: 40.5903\n",
      "Training Epoch 2  52.2% | batch:       358 of       686\t|\tloss: 39.4792\n",
      "Training Epoch 2  52.3% | batch:       359 of       686\t|\tloss: 46.016\n",
      "Training Epoch 2  52.5% | batch:       360 of       686\t|\tloss: 51.9691\n",
      "Training Epoch 2  52.6% | batch:       361 of       686\t|\tloss: 42.1905\n",
      "Training Epoch 2  52.8% | batch:       362 of       686\t|\tloss: 38.5028\n",
      "Training Epoch 2  52.9% | batch:       363 of       686\t|\tloss: 83.9508\n",
      "Training Epoch 2  53.1% | batch:       364 of       686\t|\tloss: 32.7058\n",
      "Training Epoch 2  53.2% | batch:       365 of       686\t|\tloss: 42.5301\n",
      "Training Epoch 2  53.4% | batch:       366 of       686\t|\tloss: 57.6953\n",
      "Training Epoch 2  53.5% | batch:       367 of       686\t|\tloss: 30.7844\n",
      "Training Epoch 2  53.6% | batch:       368 of       686\t|\tloss: 34.362\n",
      "Training Epoch 2  53.8% | batch:       369 of       686\t|\tloss: 45.585\n",
      "Training Epoch 2  53.9% | batch:       370 of       686\t|\tloss: 41.1082\n",
      "Training Epoch 2  54.1% | batch:       371 of       686\t|\tloss: 59.8273\n",
      "Training Epoch 2  54.2% | batch:       372 of       686\t|\tloss: 36.9348\n",
      "Training Epoch 2  54.4% | batch:       373 of       686\t|\tloss: 43.8379\n",
      "Training Epoch 2  54.5% | batch:       374 of       686\t|\tloss: 48.0797\n",
      "Training Epoch 2  54.7% | batch:       375 of       686\t|\tloss: 39.923\n",
      "Training Epoch 2  54.8% | batch:       376 of       686\t|\tloss: 39.1602\n",
      "Training Epoch 2  55.0% | batch:       377 of       686\t|\tloss: 38.3287\n",
      "Training Epoch 2  55.1% | batch:       378 of       686\t|\tloss: 38.8596\n",
      "Training Epoch 2  55.2% | batch:       379 of       686\t|\tloss: 33.8833\n",
      "Training Epoch 2  55.4% | batch:       380 of       686\t|\tloss: 68.9773\n",
      "Training Epoch 2  55.5% | batch:       381 of       686\t|\tloss: 42.3849\n",
      "Training Epoch 2  55.7% | batch:       382 of       686\t|\tloss: 32.819\n",
      "Training Epoch 2  55.8% | batch:       383 of       686\t|\tloss: 37.5243\n",
      "Training Epoch 2  56.0% | batch:       384 of       686\t|\tloss: 44.1751\n",
      "Training Epoch 2  56.1% | batch:       385 of       686\t|\tloss: 33.2241\n",
      "Training Epoch 2  56.3% | batch:       386 of       686\t|\tloss: 37.2187\n",
      "Training Epoch 2  56.4% | batch:       387 of       686\t|\tloss: 32.2322\n",
      "Training Epoch 2  56.6% | batch:       388 of       686\t|\tloss: 37.2016\n",
      "Training Epoch 2  56.7% | batch:       389 of       686\t|\tloss: 42.8421\n",
      "Training Epoch 2  56.9% | batch:       390 of       686\t|\tloss: 36.0603\n",
      "Training Epoch 2  57.0% | batch:       391 of       686\t|\tloss: 32.1604\n",
      "Training Epoch 2  57.1% | batch:       392 of       686\t|\tloss: 40.9303\n",
      "Training Epoch 2  57.3% | batch:       393 of       686\t|\tloss: 48.4056\n",
      "Training Epoch 2  57.4% | batch:       394 of       686\t|\tloss: 49.0671\n",
      "Training Epoch 2  57.6% | batch:       395 of       686\t|\tloss: 44.6251\n",
      "Training Epoch 2  57.7% | batch:       396 of       686\t|\tloss: 39.8133\n",
      "Training Epoch 2  57.9% | batch:       397 of       686\t|\tloss: 33.6125\n",
      "Training Epoch 2  58.0% | batch:       398 of       686\t|\tloss: 36.6827\n",
      "Training Epoch 2  58.2% | batch:       399 of       686\t|\tloss: 41.7219\n",
      "Training Epoch 2  58.3% | batch:       400 of       686\t|\tloss: 48.4783\n",
      "Training Epoch 2  58.5% | batch:       401 of       686\t|\tloss: 46.4841\n",
      "Training Epoch 2  58.6% | batch:       402 of       686\t|\tloss: 50.8891\n",
      "Training Epoch 2  58.7% | batch:       403 of       686\t|\tloss: 65.7516\n",
      "Training Epoch 2  58.9% | batch:       404 of       686\t|\tloss: 48.3361\n",
      "Training Epoch 2  59.0% | batch:       405 of       686\t|\tloss: 37.2324\n",
      "Training Epoch 2  59.2% | batch:       406 of       686\t|\tloss: 41.3842\n",
      "Training Epoch 2  59.3% | batch:       407 of       686\t|\tloss: 46.6911\n",
      "Training Epoch 2  59.5% | batch:       408 of       686\t|\tloss: 38.8341\n",
      "Training Epoch 2  59.6% | batch:       409 of       686\t|\tloss: 45.6051\n",
      "Training Epoch 2  59.8% | batch:       410 of       686\t|\tloss: 39.2877\n",
      "Training Epoch 2  59.9% | batch:       411 of       686\t|\tloss: 37.3231\n",
      "Training Epoch 2  60.1% | batch:       412 of       686\t|\tloss: 36.6898\n",
      "Training Epoch 2  60.2% | batch:       413 of       686\t|\tloss: 36.8734\n",
      "Training Epoch 2  60.3% | batch:       414 of       686\t|\tloss: 62.1134\n",
      "Training Epoch 2  60.5% | batch:       415 of       686\t|\tloss: 50.8628\n",
      "Training Epoch 2  60.6% | batch:       416 of       686\t|\tloss: 39.0337\n",
      "Training Epoch 2  60.8% | batch:       417 of       686\t|\tloss: 32.4532\n",
      "Training Epoch 2  60.9% | batch:       418 of       686\t|\tloss: 44.7469\n",
      "Training Epoch 2  61.1% | batch:       419 of       686\t|\tloss: 48.8697\n",
      "Training Epoch 2  61.2% | batch:       420 of       686\t|\tloss: 39.5178\n",
      "Training Epoch 2  61.4% | batch:       421 of       686\t|\tloss: 37.0841\n",
      "Training Epoch 2  61.5% | batch:       422 of       686\t|\tloss: 33.054\n",
      "Training Epoch 2  61.7% | batch:       423 of       686\t|\tloss: 40.4491\n",
      "Training Epoch 2  61.8% | batch:       424 of       686\t|\tloss: 45.9672\n",
      "Training Epoch 2  62.0% | batch:       425 of       686\t|\tloss: 36.9286\n",
      "Training Epoch 2  62.1% | batch:       426 of       686\t|\tloss: 44.8447\n",
      "Training Epoch 2  62.2% | batch:       427 of       686\t|\tloss: 42.5174\n",
      "Training Epoch 2  62.4% | batch:       428 of       686\t|\tloss: 55.0055\n",
      "Training Epoch 2  62.5% | batch:       429 of       686\t|\tloss: 29.9258\n",
      "Training Epoch 2  62.7% | batch:       430 of       686\t|\tloss: 29.2608\n",
      "Training Epoch 2  62.8% | batch:       431 of       686\t|\tloss: 41.7493\n",
      "Training Epoch 2  63.0% | batch:       432 of       686\t|\tloss: 27.4497\n",
      "Training Epoch 2  63.1% | batch:       433 of       686\t|\tloss: 35.9455\n",
      "Training Epoch 2  63.3% | batch:       434 of       686\t|\tloss: 46.144\n",
      "Training Epoch 2  63.4% | batch:       435 of       686\t|\tloss: 47.0289\n",
      "Training Epoch 2  63.6% | batch:       436 of       686\t|\tloss: 47.6778\n",
      "Training Epoch 2  63.7% | batch:       437 of       686\t|\tloss: 45.3841\n",
      "Training Epoch 2  63.8% | batch:       438 of       686\t|\tloss: 34.6311\n",
      "Training Epoch 2  64.0% | batch:       439 of       686\t|\tloss: 44.5769\n",
      "Training Epoch 2  64.1% | batch:       440 of       686\t|\tloss: 42.0254\n",
      "Training Epoch 2  64.3% | batch:       441 of       686\t|\tloss: 62.0468\n",
      "Training Epoch 2  64.4% | batch:       442 of       686\t|\tloss: 43.0032\n",
      "Training Epoch 2  64.6% | batch:       443 of       686\t|\tloss: 27.5199\n",
      "Training Epoch 2  64.7% | batch:       444 of       686\t|\tloss: 45.2476\n",
      "Training Epoch 2  64.9% | batch:       445 of       686\t|\tloss: 34.4785\n",
      "Training Epoch 2  65.0% | batch:       446 of       686\t|\tloss: 46.279\n",
      "Training Epoch 2  65.2% | batch:       447 of       686\t|\tloss: 45.2907\n",
      "Training Epoch 2  65.3% | batch:       448 of       686\t|\tloss: 45.5678\n",
      "Training Epoch 2  65.5% | batch:       449 of       686\t|\tloss: 42.6462\n",
      "Training Epoch 2  65.6% | batch:       450 of       686\t|\tloss: 47.0103\n",
      "Training Epoch 2  65.7% | batch:       451 of       686\t|\tloss: 32.0841\n",
      "Training Epoch 2  65.9% | batch:       452 of       686\t|\tloss: 41.0392\n",
      "Training Epoch 2  66.0% | batch:       453 of       686\t|\tloss: 38.3938\n",
      "Training Epoch 2  66.2% | batch:       454 of       686\t|\tloss: 36.2783\n",
      "Training Epoch 2  66.3% | batch:       455 of       686\t|\tloss: 55.9858\n",
      "Training Epoch 2  66.5% | batch:       456 of       686\t|\tloss: 37.7122\n",
      "Training Epoch 2  66.6% | batch:       457 of       686\t|\tloss: 34.5275\n",
      "Training Epoch 2  66.8% | batch:       458 of       686\t|\tloss: 42.4949\n",
      "Training Epoch 2  66.9% | batch:       459 of       686\t|\tloss: 64.2214\n",
      "Training Epoch 2  67.1% | batch:       460 of       686\t|\tloss: 43.4744\n",
      "Training Epoch 2  67.2% | batch:       461 of       686\t|\tloss: 43.1712\n",
      "Training Epoch 2  67.3% | batch:       462 of       686\t|\tloss: 37.6607\n",
      "Training Epoch 2  67.5% | batch:       463 of       686\t|\tloss: 31.5128\n",
      "Training Epoch 2  67.6% | batch:       464 of       686\t|\tloss: 44.7872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  67.8% | batch:       465 of       686\t|\tloss: 42.98\n",
      "Training Epoch 2  67.9% | batch:       466 of       686\t|\tloss: 30.1724\n",
      "Training Epoch 2  68.1% | batch:       467 of       686\t|\tloss: 39.4141\n",
      "Training Epoch 2  68.2% | batch:       468 of       686\t|\tloss: 47.8971\n",
      "Training Epoch 2  68.4% | batch:       469 of       686\t|\tloss: 37.4002\n",
      "Training Epoch 2  68.5% | batch:       470 of       686\t|\tloss: 33.7406\n",
      "Training Epoch 2  68.7% | batch:       471 of       686\t|\tloss: 34.1973\n",
      "Training Epoch 2  68.8% | batch:       472 of       686\t|\tloss: 50.846\n",
      "Training Epoch 2  69.0% | batch:       473 of       686\t|\tloss: 40.6846\n",
      "Training Epoch 2  69.1% | batch:       474 of       686\t|\tloss: 35.9847\n",
      "Training Epoch 2  69.2% | batch:       475 of       686\t|\tloss: 32.5934\n",
      "Training Epoch 2  69.4% | batch:       476 of       686\t|\tloss: 37.7109\n",
      "Training Epoch 2  69.5% | batch:       477 of       686\t|\tloss: 32.0544\n",
      "Training Epoch 2  69.7% | batch:       478 of       686\t|\tloss: 44.2732\n",
      "Training Epoch 2  69.8% | batch:       479 of       686\t|\tloss: 35.623\n",
      "Training Epoch 2  70.0% | batch:       480 of       686\t|\tloss: 46.9915\n",
      "Training Epoch 2  70.1% | batch:       481 of       686\t|\tloss: 43.5208\n",
      "Training Epoch 2  70.3% | batch:       482 of       686\t|\tloss: 33.3344\n",
      "Training Epoch 2  70.4% | batch:       483 of       686\t|\tloss: 38.669\n",
      "Training Epoch 2  70.6% | batch:       484 of       686\t|\tloss: 42.2285\n",
      "Training Epoch 2  70.7% | batch:       485 of       686\t|\tloss: 42.6705\n",
      "Training Epoch 2  70.8% | batch:       486 of       686\t|\tloss: 43.913\n",
      "Training Epoch 2  71.0% | batch:       487 of       686\t|\tloss: 45.4126\n",
      "Training Epoch 2  71.1% | batch:       488 of       686\t|\tloss: 52.4517\n",
      "Training Epoch 2  71.3% | batch:       489 of       686\t|\tloss: 58.621\n",
      "Training Epoch 2  71.4% | batch:       490 of       686\t|\tloss: 47.9024\n",
      "Training Epoch 2  71.6% | batch:       491 of       686\t|\tloss: 55.956\n",
      "Training Epoch 2  71.7% | batch:       492 of       686\t|\tloss: 32.2341\n",
      "Training Epoch 2  71.9% | batch:       493 of       686\t|\tloss: 25.4764\n",
      "Training Epoch 2  72.0% | batch:       494 of       686\t|\tloss: 40.4018\n",
      "Training Epoch 2  72.2% | batch:       495 of       686\t|\tloss: 45.8973\n",
      "Training Epoch 2  72.3% | batch:       496 of       686\t|\tloss: 47.559\n",
      "Training Epoch 2  72.4% | batch:       497 of       686\t|\tloss: 42.6248\n",
      "Training Epoch 2  72.6% | batch:       498 of       686\t|\tloss: 36.7857\n",
      "Training Epoch 2  72.7% | batch:       499 of       686\t|\tloss: 28.5163\n",
      "Training Epoch 2  72.9% | batch:       500 of       686\t|\tloss: 26.4211\n",
      "Training Epoch 2  73.0% | batch:       501 of       686\t|\tloss: 42.6571\n",
      "Training Epoch 2  73.2% | batch:       502 of       686\t|\tloss: 34.3237\n",
      "Training Epoch 2  73.3% | batch:       503 of       686\t|\tloss: 36.3326\n",
      "Training Epoch 2  73.5% | batch:       504 of       686\t|\tloss: 41.345\n",
      "Training Epoch 2  73.6% | batch:       505 of       686\t|\tloss: 39.3054\n",
      "Training Epoch 2  73.8% | batch:       506 of       686\t|\tloss: 45.464\n",
      "Training Epoch 2  73.9% | batch:       507 of       686\t|\tloss: 58.6104\n",
      "Training Epoch 2  74.1% | batch:       508 of       686\t|\tloss: 40.2558\n",
      "Training Epoch 2  74.2% | batch:       509 of       686\t|\tloss: 45.2285\n",
      "Training Epoch 2  74.3% | batch:       510 of       686\t|\tloss: 38.4123\n",
      "Training Epoch 2  74.5% | batch:       511 of       686\t|\tloss: 39.7668\n",
      "Training Epoch 2  74.6% | batch:       512 of       686\t|\tloss: 59.8438\n",
      "Training Epoch 2  74.8% | batch:       513 of       686\t|\tloss: 50.1537\n",
      "Training Epoch 2  74.9% | batch:       514 of       686\t|\tloss: 39.3292\n",
      "Training Epoch 2  75.1% | batch:       515 of       686\t|\tloss: 40.9954\n",
      "Training Epoch 2  75.2% | batch:       516 of       686\t|\tloss: 45.826\n",
      "Training Epoch 2  75.4% | batch:       517 of       686\t|\tloss: 39.8918\n",
      "Training Epoch 2  75.5% | batch:       518 of       686\t|\tloss: 36.5503\n",
      "Training Epoch 2  75.7% | batch:       519 of       686\t|\tloss: 40.9106\n",
      "Training Epoch 2  75.8% | batch:       520 of       686\t|\tloss: 47.4293\n",
      "Training Epoch 2  75.9% | batch:       521 of       686\t|\tloss: 41.77\n",
      "Training Epoch 2  76.1% | batch:       522 of       686\t|\tloss: 42.3258\n",
      "Training Epoch 2  76.2% | batch:       523 of       686\t|\tloss: 40.0023\n",
      "Training Epoch 2  76.4% | batch:       524 of       686\t|\tloss: 36.7358\n",
      "Training Epoch 2  76.5% | batch:       525 of       686\t|\tloss: 32.5675\n",
      "Training Epoch 2  76.7% | batch:       526 of       686\t|\tloss: 41.046\n",
      "Training Epoch 2  76.8% | batch:       527 of       686\t|\tloss: 38.2967\n",
      "Training Epoch 2  77.0% | batch:       528 of       686\t|\tloss: 42.2559\n",
      "Training Epoch 2  77.1% | batch:       529 of       686\t|\tloss: 41.6708\n",
      "Training Epoch 2  77.3% | batch:       530 of       686\t|\tloss: 50.4337\n",
      "Training Epoch 2  77.4% | batch:       531 of       686\t|\tloss: 36.4378\n",
      "Training Epoch 2  77.6% | batch:       532 of       686\t|\tloss: 39.4666\n",
      "Training Epoch 2  77.7% | batch:       533 of       686\t|\tloss: 33.3414\n",
      "Training Epoch 2  77.8% | batch:       534 of       686\t|\tloss: 43.4189\n",
      "Training Epoch 2  78.0% | batch:       535 of       686\t|\tloss: 45.4436\n",
      "Training Epoch 2  78.1% | batch:       536 of       686\t|\tloss: 33.5316\n",
      "Training Epoch 2  78.3% | batch:       537 of       686\t|\tloss: 35.6587\n",
      "Training Epoch 2  78.4% | batch:       538 of       686\t|\tloss: 36.9772\n",
      "Training Epoch 2  78.6% | batch:       539 of       686\t|\tloss: 32.4637\n",
      "Training Epoch 2  78.7% | batch:       540 of       686\t|\tloss: 34.9794\n",
      "Training Epoch 2  78.9% | batch:       541 of       686\t|\tloss: 41.6422\n",
      "Training Epoch 2  79.0% | batch:       542 of       686\t|\tloss: 40.8656\n",
      "Training Epoch 2  79.2% | batch:       543 of       686\t|\tloss: 52.3224\n",
      "Training Epoch 2  79.3% | batch:       544 of       686\t|\tloss: 36.006\n",
      "Training Epoch 2  79.4% | batch:       545 of       686\t|\tloss: 33.7761\n",
      "Training Epoch 2  79.6% | batch:       546 of       686\t|\tloss: 40.7681\n",
      "Training Epoch 2  79.7% | batch:       547 of       686\t|\tloss: 118.547\n",
      "Training Epoch 2  79.9% | batch:       548 of       686\t|\tloss: 56.713\n",
      "Training Epoch 2  80.0% | batch:       549 of       686\t|\tloss: 34.0485\n",
      "Training Epoch 2  80.2% | batch:       550 of       686\t|\tloss: 37.2873\n",
      "Training Epoch 2  80.3% | batch:       551 of       686\t|\tloss: 46.1731\n",
      "Training Epoch 2  80.5% | batch:       552 of       686\t|\tloss: 41.6957\n",
      "Training Epoch 2  80.6% | batch:       553 of       686\t|\tloss: 48.1742\n",
      "Training Epoch 2  80.8% | batch:       554 of       686\t|\tloss: 45.2129\n",
      "Training Epoch 2  80.9% | batch:       555 of       686\t|\tloss: 35.2929\n",
      "Training Epoch 2  81.0% | batch:       556 of       686\t|\tloss: 43.3115\n",
      "Training Epoch 2  81.2% | batch:       557 of       686\t|\tloss: 37.1931\n",
      "Training Epoch 2  81.3% | batch:       558 of       686\t|\tloss: 39.4654\n",
      "Training Epoch 2  81.5% | batch:       559 of       686\t|\tloss: 46.9136\n",
      "Training Epoch 2  81.6% | batch:       560 of       686\t|\tloss: 53.6023\n",
      "Training Epoch 2  81.8% | batch:       561 of       686\t|\tloss: 64.8835\n",
      "Training Epoch 2  81.9% | batch:       562 of       686\t|\tloss: 44.839\n",
      "Training Epoch 2  82.1% | batch:       563 of       686\t|\tloss: 41.2305\n",
      "Training Epoch 2  82.2% | batch:       564 of       686\t|\tloss: 44.8113\n",
      "Training Epoch 2  82.4% | batch:       565 of       686\t|\tloss: 43.9613\n",
      "Training Epoch 2  82.5% | batch:       566 of       686\t|\tloss: 34.5525\n",
      "Training Epoch 2  82.7% | batch:       567 of       686\t|\tloss: 34.0138\n",
      "Training Epoch 2  82.8% | batch:       568 of       686\t|\tloss: 41.712\n",
      "Training Epoch 2  82.9% | batch:       569 of       686\t|\tloss: 34.6524\n",
      "Training Epoch 2  83.1% | batch:       570 of       686\t|\tloss: 50.0267\n",
      "Training Epoch 2  83.2% | batch:       571 of       686\t|\tloss: 37.0879\n",
      "Training Epoch 2  83.4% | batch:       572 of       686\t|\tloss: 39.3463\n",
      "Training Epoch 2  83.5% | batch:       573 of       686\t|\tloss: 34.9279\n",
      "Training Epoch 2  83.7% | batch:       574 of       686\t|\tloss: 43.7951\n",
      "Training Epoch 2  83.8% | batch:       575 of       686\t|\tloss: 44.8047\n",
      "Training Epoch 2  84.0% | batch:       576 of       686\t|\tloss: 43.8482\n",
      "Training Epoch 2  84.1% | batch:       577 of       686\t|\tloss: 78.1023\n",
      "Training Epoch 2  84.3% | batch:       578 of       686\t|\tloss: 25.9268\n",
      "Training Epoch 2  84.4% | batch:       579 of       686\t|\tloss: 35.8601\n",
      "Training Epoch 2  84.5% | batch:       580 of       686\t|\tloss: 37.8561\n",
      "Training Epoch 2  84.7% | batch:       581 of       686\t|\tloss: 33.1952\n",
      "Training Epoch 2  84.8% | batch:       582 of       686\t|\tloss: 32.4867\n",
      "Training Epoch 2  85.0% | batch:       583 of       686\t|\tloss: 38.2961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  85.1% | batch:       584 of       686\t|\tloss: 47.7957\n",
      "Training Epoch 2  85.3% | batch:       585 of       686\t|\tloss: 46.3972\n",
      "Training Epoch 2  85.4% | batch:       586 of       686\t|\tloss: 28.31\n",
      "Training Epoch 2  85.6% | batch:       587 of       686\t|\tloss: 38.1678\n",
      "Training Epoch 2  85.7% | batch:       588 of       686\t|\tloss: 32.0866\n",
      "Training Epoch 2  85.9% | batch:       589 of       686\t|\tloss: 34.0972\n",
      "Training Epoch 2  86.0% | batch:       590 of       686\t|\tloss: 43.2893\n",
      "Training Epoch 2  86.2% | batch:       591 of       686\t|\tloss: 45.7236\n",
      "Training Epoch 2  86.3% | batch:       592 of       686\t|\tloss: 30.5544\n",
      "Training Epoch 2  86.4% | batch:       593 of       686\t|\tloss: 40.294\n",
      "Training Epoch 2  86.6% | batch:       594 of       686\t|\tloss: 38.8416\n",
      "Training Epoch 2  86.7% | batch:       595 of       686\t|\tloss: 43.6756\n",
      "Training Epoch 2  86.9% | batch:       596 of       686\t|\tloss: 39.946\n",
      "Training Epoch 2  87.0% | batch:       597 of       686\t|\tloss: 34.466\n",
      "Training Epoch 2  87.2% | batch:       598 of       686\t|\tloss: 38.5841\n",
      "Training Epoch 2  87.3% | batch:       599 of       686\t|\tloss: 39.4813\n",
      "Training Epoch 2  87.5% | batch:       600 of       686\t|\tloss: 41.9111\n",
      "Training Epoch 2  87.6% | batch:       601 of       686\t|\tloss: 40.9316\n",
      "Training Epoch 2  87.8% | batch:       602 of       686\t|\tloss: 48.5844\n",
      "Training Epoch 2  87.9% | batch:       603 of       686\t|\tloss: 37.3541\n",
      "Training Epoch 2  88.0% | batch:       604 of       686\t|\tloss: 27.4733\n",
      "Training Epoch 2  88.2% | batch:       605 of       686\t|\tloss: 31.8023\n",
      "Training Epoch 2  88.3% | batch:       606 of       686\t|\tloss: 39.5358\n",
      "Training Epoch 2  88.5% | batch:       607 of       686\t|\tloss: 42.7431\n",
      "Training Epoch 2  88.6% | batch:       608 of       686\t|\tloss: 33.5839\n",
      "Training Epoch 2  88.8% | batch:       609 of       686\t|\tloss: 36.6854\n",
      "Training Epoch 2  88.9% | batch:       610 of       686\t|\tloss: 44.3324\n",
      "Training Epoch 2  89.1% | batch:       611 of       686\t|\tloss: 37.7542\n",
      "Training Epoch 2  89.2% | batch:       612 of       686\t|\tloss: 44.8411\n",
      "Training Epoch 2  89.4% | batch:       613 of       686\t|\tloss: 36.7143\n",
      "Training Epoch 2  89.5% | batch:       614 of       686\t|\tloss: 45.9662\n",
      "Training Epoch 2  89.7% | batch:       615 of       686\t|\tloss: 41.9781\n",
      "Training Epoch 2  89.8% | batch:       616 of       686\t|\tloss: 27.3968\n",
      "Training Epoch 2  89.9% | batch:       617 of       686\t|\tloss: 34.2993\n",
      "Training Epoch 2  90.1% | batch:       618 of       686\t|\tloss: 44.2343\n",
      "Training Epoch 2  90.2% | batch:       619 of       686\t|\tloss: 33.6095\n",
      "Training Epoch 2  90.4% | batch:       620 of       686\t|\tloss: 48.3993\n",
      "Training Epoch 2  90.5% | batch:       621 of       686\t|\tloss: 32.024\n",
      "Training Epoch 2  90.7% | batch:       622 of       686\t|\tloss: 39.7108\n",
      "Training Epoch 2  90.8% | batch:       623 of       686\t|\tloss: 34.574\n",
      "Training Epoch 2  91.0% | batch:       624 of       686\t|\tloss: 40.8721\n",
      "Training Epoch 2  91.1% | batch:       625 of       686\t|\tloss: 38.1822\n",
      "Training Epoch 2  91.3% | batch:       626 of       686\t|\tloss: 48.6131\n",
      "Training Epoch 2  91.4% | batch:       627 of       686\t|\tloss: 42.38\n",
      "Training Epoch 2  91.5% | batch:       628 of       686\t|\tloss: 38.3723\n",
      "Training Epoch 2  91.7% | batch:       629 of       686\t|\tloss: 41.0329\n",
      "Training Epoch 2  91.8% | batch:       630 of       686\t|\tloss: 38.0859\n",
      "Training Epoch 2  92.0% | batch:       631 of       686\t|\tloss: 33.3201\n",
      "Training Epoch 2  92.1% | batch:       632 of       686\t|\tloss: 43.5357\n",
      "Training Epoch 2  92.3% | batch:       633 of       686\t|\tloss: 52.8546\n",
      "Training Epoch 2  92.4% | batch:       634 of       686\t|\tloss: 35.0947\n",
      "Training Epoch 2  92.6% | batch:       635 of       686\t|\tloss: 42.5916\n",
      "Training Epoch 2  92.7% | batch:       636 of       686\t|\tloss: 37.5241\n",
      "Training Epoch 2  92.9% | batch:       637 of       686\t|\tloss: 23.9156\n",
      "Training Epoch 2  93.0% | batch:       638 of       686\t|\tloss: 33.5943\n",
      "Training Epoch 2  93.1% | batch:       639 of       686\t|\tloss: 38.7659\n",
      "Training Epoch 2  93.3% | batch:       640 of       686\t|\tloss: 40.444\n",
      "Training Epoch 2  93.4% | batch:       641 of       686\t|\tloss: 33.9481\n",
      "Training Epoch 2  93.6% | batch:       642 of       686\t|\tloss: 29.9354\n",
      "Training Epoch 2  93.7% | batch:       643 of       686\t|\tloss: 28.0472\n",
      "Training Epoch 2  93.9% | batch:       644 of       686\t|\tloss: 31.7446\n",
      "Training Epoch 2  94.0% | batch:       645 of       686\t|\tloss: 34.8834\n",
      "Training Epoch 2  94.2% | batch:       646 of       686\t|\tloss: 36.2993\n",
      "Training Epoch 2  94.3% | batch:       647 of       686\t|\tloss: 67.4438\n",
      "Training Epoch 2  94.5% | batch:       648 of       686\t|\tloss: 34.6402\n",
      "Training Epoch 2  94.6% | batch:       649 of       686\t|\tloss: 40.9488\n",
      "Training Epoch 2  94.8% | batch:       650 of       686\t|\tloss: 48.8369\n",
      "Training Epoch 2  94.9% | batch:       651 of       686\t|\tloss: 27.0844\n",
      "Training Epoch 2  95.0% | batch:       652 of       686\t|\tloss: 50.4542\n",
      "Training Epoch 2  95.2% | batch:       653 of       686\t|\tloss: 38.4455\n",
      "Training Epoch 2  95.3% | batch:       654 of       686\t|\tloss: 31.9263\n",
      "Training Epoch 2  95.5% | batch:       655 of       686\t|\tloss: 38.7314\n",
      "Training Epoch 2  95.6% | batch:       656 of       686\t|\tloss: 31.4374\n",
      "Training Epoch 2  95.8% | batch:       657 of       686\t|\tloss: 29.1355\n",
      "Training Epoch 2  95.9% | batch:       658 of       686\t|\tloss: 42.1604\n",
      "Training Epoch 2  96.1% | batch:       659 of       686\t|\tloss: 31.3402\n",
      "Training Epoch 2  96.2% | batch:       660 of       686\t|\tloss: 35.215\n",
      "Training Epoch 2  96.4% | batch:       661 of       686\t|\tloss: 46.376\n",
      "Training Epoch 2  96.5% | batch:       662 of       686\t|\tloss: 29.7674\n",
      "Training Epoch 2  96.6% | batch:       663 of       686\t|\tloss: 31.3297\n",
      "Training Epoch 2  96.8% | batch:       664 of       686\t|\tloss: 41.7164\n",
      "Training Epoch 2  96.9% | batch:       665 of       686\t|\tloss: 34.8641\n",
      "Training Epoch 2  97.1% | batch:       666 of       686\t|\tloss: 38.8853\n",
      "Training Epoch 2  97.2% | batch:       667 of       686\t|\tloss: 34.8011\n",
      "Training Epoch 2  97.4% | batch:       668 of       686\t|\tloss: 44.4462\n",
      "Training Epoch 2  97.5% | batch:       669 of       686\t|\tloss: 38.844\n",
      "Training Epoch 2  97.7% | batch:       670 of       686\t|\tloss: 28.5954\n",
      "Training Epoch 2  97.8% | batch:       671 of       686\t|\tloss: 34.4024\n",
      "Training Epoch 2  98.0% | batch:       672 of       686\t|\tloss: 24.8043\n",
      "Training Epoch 2  98.1% | batch:       673 of       686\t|\tloss: 40.4684\n",
      "Training Epoch 2  98.3% | batch:       674 of       686\t|\tloss: 42.365\n",
      "Training Epoch 2  98.4% | batch:       675 of       686\t|\tloss: 37.9727\n",
      "Training Epoch 2  98.5% | batch:       676 of       686\t|\tloss: 36.9496\n",
      "Training Epoch 2  98.7% | batch:       677 of       686\t|\tloss: 33.4739\n",
      "Training Epoch 2  98.8% | batch:       678 of       686\t|\tloss: 29.6037\n",
      "Training Epoch 2  99.0% | batch:       679 of       686\t|\tloss: 33.1519\n",
      "Training Epoch 2  99.1% | batch:       680 of       686\t|\tloss: 32.9246\n",
      "Training Epoch 2  99.3% | batch:       681 of       686\t|\tloss: 31.6201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:00:43,970 | INFO : Epoch 2 Training Summary: epoch: 2.000000 | loss: 43.399584 | \n",
      "2023-05-25 22:00:43,973 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 22.64137029647827 seconds\n",
      "\n",
      "2023-05-25 22:00:43,975 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.355637788772583 seconds\n",
      "2023-05-25 22:00:43,976 | INFO : Avg batch train. time: 0.03404611922561601 seconds\n",
      "2023-05-25 22:00:43,978 | INFO : Avg sample train. time: 0.0002663280436601013 seconds\n",
      "2023-05-25 22:00:43,981 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  99.4% | batch:       682 of       686\t|\tloss: 39.2722\n",
      "Training Epoch 2  99.6% | batch:       683 of       686\t|\tloss: 39.3448\n",
      "Training Epoch 2  99.7% | batch:       684 of       686\t|\tloss: 45.6105\n",
      "Training Epoch 2  99.9% | batch:       685 of       686\t|\tloss: 27.5055\n",
      "\n",
      "Evaluating Epoch 2   0.0% | batch:         0 of       172\t|\tloss: 3.26957\n",
      "Evaluating Epoch 2   0.6% | batch:         1 of       172\t|\tloss: 11.4654\n",
      "Evaluating Epoch 2   1.2% | batch:         2 of       172\t|\tloss: 16.459\n",
      "Evaluating Epoch 2   1.7% | batch:         3 of       172\t|\tloss: 5.46653\n",
      "Evaluating Epoch 2   2.3% | batch:         4 of       172\t|\tloss: 15.0568\n",
      "Evaluating Epoch 2   2.9% | batch:         5 of       172\t|\tloss: 5.55943\n",
      "Evaluating Epoch 2   3.5% | batch:         6 of       172\t|\tloss: 12.011\n",
      "Evaluating Epoch 2   4.1% | batch:         7 of       172\t|\tloss: 6.52639\n",
      "Evaluating Epoch 2   4.7% | batch:         8 of       172\t|\tloss: 10.5463\n",
      "Evaluating Epoch 2   5.2% | batch:         9 of       172\t|\tloss: 3.67594\n",
      "Evaluating Epoch 2   5.8% | batch:        10 of       172\t|\tloss: 13.2649\n",
      "Evaluating Epoch 2   6.4% | batch:        11 of       172\t|\tloss: 12.0416\n",
      "Evaluating Epoch 2   7.0% | batch:        12 of       172\t|\tloss: 11.1638\n",
      "Evaluating Epoch 2   7.6% | batch:        13 of       172\t|\tloss: 6.71853\n",
      "Evaluating Epoch 2   8.1% | batch:        14 of       172\t|\tloss: 10.4164\n",
      "Evaluating Epoch 2   8.7% | batch:        15 of       172\t|\tloss: 3.81896\n",
      "Evaluating Epoch 2   9.3% | batch:        16 of       172\t|\tloss: 9.43651\n",
      "Evaluating Epoch 2   9.9% | batch:        17 of       172\t|\tloss: 11.1493\n",
      "Evaluating Epoch 2  10.5% | batch:        18 of       172\t|\tloss: 21.901\n",
      "Evaluating Epoch 2  11.0% | batch:        19 of       172\t|\tloss: 4.80991\n",
      "Evaluating Epoch 2  11.6% | batch:        20 of       172\t|\tloss: 9.6039\n",
      "Evaluating Epoch 2  12.2% | batch:        21 of       172\t|\tloss: 3.54017\n",
      "Evaluating Epoch 2  12.8% | batch:        22 of       172\t|\tloss: 8.46666\n",
      "Evaluating Epoch 2  13.4% | batch:        23 of       172\t|\tloss: 5.709\n",
      "Evaluating Epoch 2  14.0% | batch:        24 of       172\t|\tloss: 7.04168\n",
      "Evaluating Epoch 2  14.5% | batch:        25 of       172\t|\tloss: 10.7523\n",
      "Evaluating Epoch 2  15.1% | batch:        26 of       172\t|\tloss: 20.8008\n",
      "Evaluating Epoch 2  15.7% | batch:        27 of       172\t|\tloss: 14.917\n",
      "Evaluating Epoch 2  16.3% | batch:        28 of       172\t|\tloss: 2.02055\n",
      "Evaluating Epoch 2  16.9% | batch:        29 of       172\t|\tloss: 10.3497\n",
      "Evaluating Epoch 2  17.4% | batch:        30 of       172\t|\tloss: 3.01397\n",
      "Evaluating Epoch 2  18.0% | batch:        31 of       172\t|\tloss: 5.28793\n",
      "Evaluating Epoch 2  18.6% | batch:        32 of       172\t|\tloss: 4.35928\n",
      "Evaluating Epoch 2  19.2% | batch:        33 of       172\t|\tloss: 7.47037\n",
      "Evaluating Epoch 2  19.8% | batch:        34 of       172\t|\tloss: 6.06559\n",
      "Evaluating Epoch 2  20.3% | batch:        35 of       172\t|\tloss: 3.43876\n",
      "Evaluating Epoch 2  20.9% | batch:        36 of       172\t|\tloss: 12.7529\n",
      "Evaluating Epoch 2  21.5% | batch:        37 of       172\t|\tloss: 10.2362\n",
      "Evaluating Epoch 2  22.1% | batch:        38 of       172\t|\tloss: 14.366\n",
      "Evaluating Epoch 2  22.7% | batch:        39 of       172\t|\tloss: 7.92272\n",
      "Evaluating Epoch 2  23.3% | batch:        40 of       172\t|\tloss: 2.7835\n",
      "Evaluating Epoch 2  23.8% | batch:        41 of       172\t|\tloss: 7.94354\n",
      "Evaluating Epoch 2  24.4% | batch:        42 of       172\t|\tloss: 3.69926\n",
      "Evaluating Epoch 2  25.0% | batch:        43 of       172\t|\tloss: 26.6197\n",
      "Evaluating Epoch 2  25.6% | batch:        44 of       172\t|\tloss: 3.63978\n",
      "Evaluating Epoch 2  26.2% | batch:        45 of       172\t|\tloss: 6.89487\n",
      "Evaluating Epoch 2  26.7% | batch:        46 of       172\t|\tloss: 3.90053\n",
      "Evaluating Epoch 2  27.3% | batch:        47 of       172\t|\tloss: 5.24849\n",
      "Evaluating Epoch 2  27.9% | batch:        48 of       172\t|\tloss: 5.29552\n",
      "Evaluating Epoch 2  28.5% | batch:        49 of       172\t|\tloss: 8.82392\n",
      "Evaluating Epoch 2  29.1% | batch:        50 of       172\t|\tloss: 6.00628\n",
      "Evaluating Epoch 2  29.7% | batch:        51 of       172\t|\tloss: 4.08036\n",
      "Evaluating Epoch 2  30.2% | batch:        52 of       172\t|\tloss: 28.9677\n",
      "Evaluating Epoch 2  30.8% | batch:        53 of       172\t|\tloss: 3.63297\n",
      "Evaluating Epoch 2  31.4% | batch:        54 of       172\t|\tloss: 24.104\n",
      "Evaluating Epoch 2  32.0% | batch:        55 of       172\t|\tloss: 7.50421\n",
      "Evaluating Epoch 2  32.6% | batch:        56 of       172\t|\tloss: 4.43509\n",
      "Evaluating Epoch 2  33.1% | batch:        57 of       172\t|\tloss: 8.26206\n",
      "Evaluating Epoch 2  33.7% | batch:        58 of       172\t|\tloss: 5.88727\n",
      "Evaluating Epoch 2  34.3% | batch:        59 of       172\t|\tloss: 22.9664\n",
      "Evaluating Epoch 2  34.9% | batch:        60 of       172\t|\tloss: 8.6933\n",
      "Evaluating Epoch 2  35.5% | batch:        61 of       172\t|\tloss: 7.36337\n",
      "Evaluating Epoch 2  36.0% | batch:        62 of       172\t|\tloss: 6.08498\n",
      "Evaluating Epoch 2  36.6% | batch:        63 of       172\t|\tloss: 9.35962\n",
      "Evaluating Epoch 2  37.2% | batch:        64 of       172\t|\tloss: 21.3068\n",
      "Evaluating Epoch 2  37.8% | batch:        65 of       172\t|\tloss: 3.20505\n",
      "Evaluating Epoch 2  38.4% | batch:        66 of       172\t|\tloss: 23.6668\n",
      "Evaluating Epoch 2  39.0% | batch:        67 of       172\t|\tloss: 10.2739\n",
      "Evaluating Epoch 2  39.5% | batch:        68 of       172\t|\tloss: 4.82491\n",
      "Evaluating Epoch 2  40.1% | batch:        69 of       172\t|\tloss: 28.6704\n",
      "Evaluating Epoch 2  40.7% | batch:        70 of       172\t|\tloss: 2.39443\n",
      "Evaluating Epoch 2  41.3% | batch:        71 of       172\t|\tloss: 19.7529\n",
      "Evaluating Epoch 2  41.9% | batch:        72 of       172\t|\tloss: 10.8252\n",
      "Evaluating Epoch 2  42.4% | batch:        73 of       172\t|\tloss: 3.63166\n",
      "Evaluating Epoch 2  43.0% | batch:        74 of       172\t|\tloss: 3.37544\n",
      "Evaluating Epoch 2  43.6% | batch:        75 of       172\t|\tloss: 4.83197\n",
      "Evaluating Epoch 2  44.2% | batch:        76 of       172\t|\tloss: 4.31475\n",
      "Evaluating Epoch 2  44.8% | batch:        77 of       172\t|\tloss: 4.47528\n",
      "Evaluating Epoch 2  45.3% | batch:        78 of       172\t|\tloss: 5.89224\n",
      "Evaluating Epoch 2  45.9% | batch:        79 of       172\t|\tloss: 4.11676\n",
      "Evaluating Epoch 2  46.5% | batch:        80 of       172\t|\tloss: 3.66195\n",
      "Evaluating Epoch 2  47.1% | batch:        81 of       172\t|\tloss: 4.02305\n",
      "Evaluating Epoch 2  47.7% | batch:        82 of       172\t|\tloss: 5.25061\n",
      "Evaluating Epoch 2  48.3% | batch:        83 of       172\t|\tloss: 4.78403\n",
      "Evaluating Epoch 2  48.8% | batch:        84 of       172\t|\tloss: 5.98456\n",
      "Evaluating Epoch 2  49.4% | batch:        85 of       172\t|\tloss: 5.90619\n",
      "Evaluating Epoch 2  50.0% | batch:        86 of       172\t|\tloss: 9.39987\n",
      "Evaluating Epoch 2  50.6% | batch:        87 of       172\t|\tloss: 5.00288\n",
      "Evaluating Epoch 2  51.2% | batch:        88 of       172\t|\tloss: 4.16566\n",
      "Evaluating Epoch 2  51.7% | batch:        89 of       172\t|\tloss: 10.7678\n",
      "Evaluating Epoch 2  52.3% | batch:        90 of       172\t|\tloss: 6.36778\n",
      "Evaluating Epoch 2  52.9% | batch:        91 of       172\t|\tloss: 7.43908\n",
      "Evaluating Epoch 2  53.5% | batch:        92 of       172\t|\tloss: 7.49322\n",
      "Evaluating Epoch 2  54.1% | batch:        93 of       172\t|\tloss: 7.63085\n",
      "Evaluating Epoch 2  54.7% | batch:        94 of       172\t|\tloss: 10.7323\n",
      "Evaluating Epoch 2  55.2% | batch:        95 of       172\t|\tloss: 4.27588\n",
      "Evaluating Epoch 2  55.8% | batch:        96 of       172\t|\tloss: 10.2375\n",
      "Evaluating Epoch 2  56.4% | batch:        97 of       172\t|\tloss: 6.6091\n",
      "Evaluating Epoch 2  57.0% | batch:        98 of       172\t|\tloss: 7.2195\n",
      "Evaluating Epoch 2  57.6% | batch:        99 of       172\t|\tloss: 10.2989\n",
      "Evaluating Epoch 2  58.1% | batch:       100 of       172\t|\tloss: 6.24688\n",
      "Evaluating Epoch 2  58.7% | batch:       101 of       172\t|\tloss: 7.16532\n",
      "Evaluating Epoch 2  59.3% | batch:       102 of       172\t|\tloss: 4.28803\n",
      "Evaluating Epoch 2  59.9% | batch:       103 of       172\t|\tloss: 9.96204\n",
      "Evaluating Epoch 2  60.5% | batch:       104 of       172\t|\tloss: 7.70263\n",
      "Evaluating Epoch 2  61.0% | batch:       105 of       172\t|\tloss: 5.19711\n",
      "Evaluating Epoch 2  61.6% | batch:       106 of       172\t|\tloss: 10.001\n",
      "Evaluating Epoch 2  62.2% | batch:       107 of       172\t|\tloss: 7.02083\n",
      "Evaluating Epoch 2  62.8% | batch:       108 of       172\t|\tloss: 6.40548\n",
      "Evaluating Epoch 2  63.4% | batch:       109 of       172\t|\tloss: 5.54699\n",
      "Evaluating Epoch 2  64.0% | batch:       110 of       172\t|\tloss: 12.0055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 2  64.5% | batch:       111 of       172\t|\tloss: 8.03832\n",
      "Evaluating Epoch 2  65.1% | batch:       112 of       172\t|\tloss: 4.43161\n",
      "Evaluating Epoch 2  65.7% | batch:       113 of       172\t|\tloss: 11.967\n",
      "Evaluating Epoch 2  66.3% | batch:       114 of       172\t|\tloss: 9.16762\n",
      "Evaluating Epoch 2  66.9% | batch:       115 of       172\t|\tloss: 12.2553\n",
      "Evaluating Epoch 2  67.4% | batch:       116 of       172\t|\tloss: 12.5401\n",
      "Evaluating Epoch 2  68.0% | batch:       117 of       172\t|\tloss: 12.0327\n",
      "Evaluating Epoch 2  68.6% | batch:       118 of       172\t|\tloss: 4.38999\n",
      "Evaluating Epoch 2  69.2% | batch:       119 of       172\t|\tloss: 14.7797\n",
      "Evaluating Epoch 2  69.8% | batch:       120 of       172\t|\tloss: 4.39555\n",
      "Evaluating Epoch 2  70.3% | batch:       121 of       172\t|\tloss: 80.4593\n",
      "Evaluating Epoch 2  70.9% | batch:       122 of       172\t|\tloss: 16.868\n",
      "Evaluating Epoch 2  71.5% | batch:       123 of       172\t|\tloss: 55.783\n",
      "Evaluating Epoch 2  72.1% | batch:       124 of       172\t|\tloss: 154.947\n",
      "Evaluating Epoch 2  72.7% | batch:       125 of       172\t|\tloss: 17.8503\n",
      "Evaluating Epoch 2  73.3% | batch:       126 of       172\t|\tloss: 8.18193\n",
      "Evaluating Epoch 2  73.8% | batch:       127 of       172\t|\tloss: 8.33775\n",
      "Evaluating Epoch 2  74.4% | batch:       128 of       172\t|\tloss: 25.0858\n",
      "Evaluating Epoch 2  75.0% | batch:       129 of       172\t|\tloss: 10.0763\n",
      "Evaluating Epoch 2  75.6% | batch:       130 of       172\t|\tloss: 2.95639\n",
      "Evaluating Epoch 2  76.2% | batch:       131 of       172\t|\tloss: 17.8712\n",
      "Evaluating Epoch 2  76.7% | batch:       132 of       172\t|\tloss: 5.32876\n",
      "Evaluating Epoch 2  77.3% | batch:       133 of       172\t|\tloss: 7.85393\n",
      "Evaluating Epoch 2  77.9% | batch:       134 of       172\t|\tloss: 4.55655\n",
      "Evaluating Epoch 2  78.5% | batch:       135 of       172\t|\tloss: 4.58571\n",
      "Evaluating Epoch 2  79.1% | batch:       136 of       172\t|\tloss: 5.14796\n",
      "Evaluating Epoch 2  79.7% | batch:       137 of       172\t|\tloss: 4.05363\n",
      "Evaluating Epoch 2  80.2% | batch:       138 of       172\t|\tloss: 7.5011\n",
      "Evaluating Epoch 2  80.8% | batch:       139 of       172\t|\tloss: 7.62386\n",
      "Evaluating Epoch 2  81.4% | batch:       140 of       172\t|\tloss: 2.97355\n",
      "Evaluating Epoch 2  82.0% | batch:       141 of       172\t|\tloss: 4.24103\n",
      "Evaluating Epoch 2  82.6% | batch:       142 of       172\t|\tloss: 2.1665\n",
      "Evaluating Epoch 2  83.1% | batch:       143 of       172\t|\tloss: 4.89274\n",
      "Evaluating Epoch 2  83.7% | batch:       144 of       172\t|\tloss: 3.99239\n",
      "Evaluating Epoch 2  84.3% | batch:       145 of       172\t|\tloss: 3.59799\n",
      "Evaluating Epoch 2  84.9% | batch:       146 of       172\t|\tloss: 5.38108\n",
      "Evaluating Epoch 2  85.5% | batch:       147 of       172\t|\tloss: 5.10718\n",
      "Evaluating Epoch 2  86.0% | batch:       148 of       172\t|\tloss: 3.18153\n",
      "Evaluating Epoch 2  86.6% | batch:       149 of       172\t|\tloss: 5.1794\n",
      "Evaluating Epoch 2  87.2% | batch:       150 of       172\t|\tloss: 4.00212\n",
      "Evaluating Epoch 2  87.8% | batch:       151 of       172\t|\tloss: 4.9478\n",
      "Evaluating Epoch 2  88.4% | batch:       152 of       172\t|\tloss: 7.11815\n",
      "Evaluating Epoch 2  89.0% | batch:       153 of       172\t|\tloss: 3.87462\n",
      "Evaluating Epoch 2  89.5% | batch:       154 of       172\t|\tloss: 7.04222\n",
      "Evaluating Epoch 2  90.1% | batch:       155 of       172\t|\tloss: 9.02829\n",
      "Evaluating Epoch 2  90.7% | batch:       156 of       172\t|\tloss: 4.84758\n",
      "Evaluating Epoch 2  91.3% | batch:       157 of       172\t|\tloss: 8.45241\n",
      "Evaluating Epoch 2  91.9% | batch:       158 of       172\t|\tloss: 4.64842\n",
      "Evaluating Epoch 2  92.4% | batch:       159 of       172\t|\tloss: 6.69737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:00:47,422 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.4384636878967285 seconds\n",
      "\n",
      "2023-05-25 22:00:47,424 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.491584221522014 seconds\n",
      "2023-05-25 22:00:47,425 | INFO : Avg batch val. time: 0.026113861753034963 seconds\n",
      "2023-05-25 22:00:47,426 | INFO : Avg sample val. time: 0.0002045627463461317 seconds\n",
      "2023-05-25 22:00:47,427 | INFO : Epoch 2 Validation Summary: epoch: 2.000000 | loss: 10.243031 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 2  93.0% | batch:       160 of       172\t|\tloss: 61.4263\n",
      "Evaluating Epoch 2  93.6% | batch:       161 of       172\t|\tloss: 37.3976\n",
      "Evaluating Epoch 2  94.2% | batch:       162 of       172\t|\tloss: 5.72597\n",
      "Evaluating Epoch 2  94.8% | batch:       163 of       172\t|\tloss: 8.34921\n",
      "Evaluating Epoch 2  95.3% | batch:       164 of       172\t|\tloss: 9.54751\n",
      "Evaluating Epoch 2  95.9% | batch:       165 of       172\t|\tloss: 6.2036\n",
      "Evaluating Epoch 2  96.5% | batch:       166 of       172\t|\tloss: 4.06517\n",
      "Evaluating Epoch 2  97.1% | batch:       167 of       172\t|\tloss: 4.13743\n",
      "Evaluating Epoch 2  97.7% | batch:       168 of       172\t|\tloss: 6.12068\n",
      "Evaluating Epoch 2  98.3% | batch:       169 of       172\t|\tloss: 3.04814\n",
      "Evaluating Epoch 2  98.8% | batch:       170 of       172\t|\tloss: 5.46917\n",
      "Evaluating Epoch 2  99.4% | batch:       171 of       172\t|\tloss: 10.6304\n",
      "\n",
      "Training Epoch 3   0.0% | batch:         0 of       686\t|\tloss: 38.5638\n",
      "Training Epoch 3   0.1% | batch:         1 of       686\t|\tloss: 30.142\n",
      "Training Epoch 3   0.3% | batch:         2 of       686\t|\tloss: 30.832\n",
      "Training Epoch 3   0.4% | batch:         3 of       686\t|\tloss: 25.2768\n",
      "Training Epoch 3   0.6% | batch:         4 of       686\t|\tloss: 40.7092\n",
      "Training Epoch 3   0.7% | batch:         5 of       686\t|\tloss: 44.7417\n",
      "Training Epoch 3   0.9% | batch:         6 of       686\t|\tloss: 41.16\n",
      "Training Epoch 3   1.0% | batch:         7 of       686\t|\tloss: 34.4915\n",
      "Training Epoch 3   1.2% | batch:         8 of       686\t|\tloss: 37.3606\n",
      "Training Epoch 3   1.3% | batch:         9 of       686\t|\tloss: 28.2608\n",
      "Training Epoch 3   1.5% | batch:        10 of       686\t|\tloss: 32.4276\n",
      "Training Epoch 3   1.6% | batch:        11 of       686\t|\tloss: 47.0506\n",
      "Training Epoch 3   1.7% | batch:        12 of       686\t|\tloss: 32.7775\n",
      "Training Epoch 3   1.9% | batch:        13 of       686\t|\tloss: 33.7879\n",
      "Training Epoch 3   2.0% | batch:        14 of       686\t|\tloss: 30.9548\n",
      "Training Epoch 3   2.2% | batch:        15 of       686\t|\tloss: 31.8143\n",
      "Training Epoch 3   2.3% | batch:        16 of       686\t|\tloss: 33.6741\n",
      "Training Epoch 3   2.5% | batch:        17 of       686\t|\tloss: 35.3307\n",
      "Training Epoch 3   2.6% | batch:        18 of       686\t|\tloss: 31.0808\n",
      "Training Epoch 3   2.8% | batch:        19 of       686\t|\tloss: 51.1444\n",
      "Training Epoch 3   2.9% | batch:        20 of       686\t|\tloss: 46.4532\n",
      "Training Epoch 3   3.1% | batch:        21 of       686\t|\tloss: 34.3427\n",
      "Training Epoch 3   3.2% | batch:        22 of       686\t|\tloss: 31.9581\n",
      "Training Epoch 3   3.4% | batch:        23 of       686\t|\tloss: 25.9253\n",
      "Training Epoch 3   3.5% | batch:        24 of       686\t|\tloss: 29.7184\n",
      "Training Epoch 3   3.6% | batch:        25 of       686\t|\tloss: 62.3381\n",
      "Training Epoch 3   3.8% | batch:        26 of       686\t|\tloss: 41.0691\n",
      "Training Epoch 3   3.9% | batch:        27 of       686\t|\tloss: 51.2641\n",
      "Training Epoch 3   4.1% | batch:        28 of       686\t|\tloss: 38.9901\n",
      "Training Epoch 3   4.2% | batch:        29 of       686\t|\tloss: 32.1213\n",
      "Training Epoch 3   4.4% | batch:        30 of       686\t|\tloss: 35.0973\n",
      "Training Epoch 3   4.5% | batch:        31 of       686\t|\tloss: 47.7898\n",
      "Training Epoch 3   4.7% | batch:        32 of       686\t|\tloss: 41.447\n",
      "Training Epoch 3   4.8% | batch:        33 of       686\t|\tloss: 34.9079\n",
      "Training Epoch 3   5.0% | batch:        34 of       686\t|\tloss: 36.0942\n",
      "Training Epoch 3   5.1% | batch:        35 of       686\t|\tloss: 26.92\n",
      "Training Epoch 3   5.2% | batch:        36 of       686\t|\tloss: 42.4761\n",
      "Training Epoch 3   5.4% | batch:        37 of       686\t|\tloss: 33.1645\n",
      "Training Epoch 3   5.5% | batch:        38 of       686\t|\tloss: 41.5003\n",
      "Training Epoch 3   5.7% | batch:        39 of       686\t|\tloss: 33.305\n",
      "Training Epoch 3   5.8% | batch:        40 of       686\t|\tloss: 45.7362\n",
      "Training Epoch 3   6.0% | batch:        41 of       686\t|\tloss: 40.9003\n",
      "Training Epoch 3   6.1% | batch:        42 of       686\t|\tloss: 37.1527\n",
      "Training Epoch 3   6.3% | batch:        43 of       686\t|\tloss: 27.5252\n",
      "Training Epoch 3   6.4% | batch:        44 of       686\t|\tloss: 29.9905\n",
      "Training Epoch 3   6.6% | batch:        45 of       686\t|\tloss: 53.6501\n",
      "Training Epoch 3   6.7% | batch:        46 of       686\t|\tloss: 32.2945\n",
      "Training Epoch 3   6.9% | batch:        47 of       686\t|\tloss: 63.6995\n",
      "Training Epoch 3   7.0% | batch:        48 of       686\t|\tloss: 44.7506\n",
      "Training Epoch 3   7.1% | batch:        49 of       686\t|\tloss: 34.6257\n",
      "Training Epoch 3   7.3% | batch:        50 of       686\t|\tloss: 30.8591\n",
      "Training Epoch 3   7.4% | batch:        51 of       686\t|\tloss: 38.2975\n",
      "Training Epoch 3   7.6% | batch:        52 of       686\t|\tloss: 34.0658\n",
      "Training Epoch 3   7.7% | batch:        53 of       686\t|\tloss: 39.4132\n",
      "Training Epoch 3   7.9% | batch:        54 of       686\t|\tloss: 31.768\n",
      "Training Epoch 3   8.0% | batch:        55 of       686\t|\tloss: 50.291\n",
      "Training Epoch 3   8.2% | batch:        56 of       686\t|\tloss: 37.5471\n",
      "Training Epoch 3   8.3% | batch:        57 of       686\t|\tloss: 45.9755\n",
      "Training Epoch 3   8.5% | batch:        58 of       686\t|\tloss: 40.0354\n",
      "Training Epoch 3   8.6% | batch:        59 of       686\t|\tloss: 38.7384\n",
      "Training Epoch 3   8.7% | batch:        60 of       686\t|\tloss: 34.0067\n",
      "Training Epoch 3   8.9% | batch:        61 of       686\t|\tloss: 30.0572\n",
      "Training Epoch 3   9.0% | batch:        62 of       686\t|\tloss: 41.9535\n",
      "Training Epoch 3   9.2% | batch:        63 of       686\t|\tloss: 36.8712\n",
      "Training Epoch 3   9.3% | batch:        64 of       686\t|\tloss: 32.411\n",
      "Training Epoch 3   9.5% | batch:        65 of       686\t|\tloss: 31.7166\n",
      "Training Epoch 3   9.6% | batch:        66 of       686\t|\tloss: 28.7482\n",
      "Training Epoch 3   9.8% | batch:        67 of       686\t|\tloss: 37.8652\n",
      "Training Epoch 3   9.9% | batch:        68 of       686\t|\tloss: 40.8626\n",
      "Training Epoch 3  10.1% | batch:        69 of       686\t|\tloss: 39.2615\n",
      "Training Epoch 3  10.2% | batch:        70 of       686\t|\tloss: 34.0412\n",
      "Training Epoch 3  10.3% | batch:        71 of       686\t|\tloss: 44.7409\n",
      "Training Epoch 3  10.5% | batch:        72 of       686\t|\tloss: 33.9867\n",
      "Training Epoch 3  10.6% | batch:        73 of       686\t|\tloss: 31.805\n",
      "Training Epoch 3  10.8% | batch:        74 of       686\t|\tloss: 43.9777\n",
      "Training Epoch 3  10.9% | batch:        75 of       686\t|\tloss: 32.7183\n",
      "Training Epoch 3  11.1% | batch:        76 of       686\t|\tloss: 28.0348\n",
      "Training Epoch 3  11.2% | batch:        77 of       686\t|\tloss: 33.3227\n",
      "Training Epoch 3  11.4% | batch:        78 of       686\t|\tloss: 31.4519\n",
      "Training Epoch 3  11.5% | batch:        79 of       686\t|\tloss: 43.8671\n",
      "Training Epoch 3  11.7% | batch:        80 of       686\t|\tloss: 31.1677\n",
      "Training Epoch 3  11.8% | batch:        81 of       686\t|\tloss: 41.0482\n",
      "Training Epoch 3  12.0% | batch:        82 of       686\t|\tloss: 35.5042\n",
      "Training Epoch 3  12.1% | batch:        83 of       686\t|\tloss: 36.0898\n",
      "Training Epoch 3  12.2% | batch:        84 of       686\t|\tloss: 38.7464\n",
      "Training Epoch 3  12.4% | batch:        85 of       686\t|\tloss: 31.8767\n",
      "Training Epoch 3  12.5% | batch:        86 of       686\t|\tloss: 31.1573\n",
      "Training Epoch 3  12.7% | batch:        87 of       686\t|\tloss: 36.1054\n",
      "Training Epoch 3  12.8% | batch:        88 of       686\t|\tloss: 41.7878\n",
      "Training Epoch 3  13.0% | batch:        89 of       686\t|\tloss: 56.5354\n",
      "Training Epoch 3  13.1% | batch:        90 of       686\t|\tloss: 38.709\n",
      "Training Epoch 3  13.3% | batch:        91 of       686\t|\tloss: 37.9697\n",
      "Training Epoch 3  13.4% | batch:        92 of       686\t|\tloss: 45.4612\n",
      "Training Epoch 3  13.6% | batch:        93 of       686\t|\tloss: 44.7699\n",
      "Training Epoch 3  13.7% | batch:        94 of       686\t|\tloss: 46.4548\n",
      "Training Epoch 3  13.8% | batch:        95 of       686\t|\tloss: 37.8425\n",
      "Training Epoch 3  14.0% | batch:        96 of       686\t|\tloss: 54.0505\n",
      "Training Epoch 3  14.1% | batch:        97 of       686\t|\tloss: 42.8\n",
      "Training Epoch 3  14.3% | batch:        98 of       686\t|\tloss: 34.5291\n",
      "Training Epoch 3  14.4% | batch:        99 of       686\t|\tloss: 36.1999\n",
      "Training Epoch 3  14.6% | batch:       100 of       686\t|\tloss: 43.0775\n",
      "Training Epoch 3  14.7% | batch:       101 of       686\t|\tloss: 47.208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  14.9% | batch:       102 of       686\t|\tloss: 27.2884\n",
      "Training Epoch 3  15.0% | batch:       103 of       686\t|\tloss: 39.6539\n",
      "Training Epoch 3  15.2% | batch:       104 of       686\t|\tloss: 40.2868\n",
      "Training Epoch 3  15.3% | batch:       105 of       686\t|\tloss: 35.9323\n",
      "Training Epoch 3  15.5% | batch:       106 of       686\t|\tloss: 30.448\n",
      "Training Epoch 3  15.6% | batch:       107 of       686\t|\tloss: 28.5612\n",
      "Training Epoch 3  15.7% | batch:       108 of       686\t|\tloss: 35.189\n",
      "Training Epoch 3  15.9% | batch:       109 of       686\t|\tloss: 51.7132\n",
      "Training Epoch 3  16.0% | batch:       110 of       686\t|\tloss: 30.2394\n",
      "Training Epoch 3  16.2% | batch:       111 of       686\t|\tloss: 41.3622\n",
      "Training Epoch 3  16.3% | batch:       112 of       686\t|\tloss: 34.0363\n",
      "Training Epoch 3  16.5% | batch:       113 of       686\t|\tloss: 28.2939\n",
      "Training Epoch 3  16.6% | batch:       114 of       686\t|\tloss: 33.2337\n",
      "Training Epoch 3  16.8% | batch:       115 of       686\t|\tloss: 87.3269\n",
      "Training Epoch 3  16.9% | batch:       116 of       686\t|\tloss: 40.6047\n",
      "Training Epoch 3  17.1% | batch:       117 of       686\t|\tloss: 28.1777\n",
      "Training Epoch 3  17.2% | batch:       118 of       686\t|\tloss: 41.7737\n",
      "Training Epoch 3  17.3% | batch:       119 of       686\t|\tloss: 36.9669\n",
      "Training Epoch 3  17.5% | batch:       120 of       686\t|\tloss: 32.9353\n",
      "Training Epoch 3  17.6% | batch:       121 of       686\t|\tloss: 42.3662\n",
      "Training Epoch 3  17.8% | batch:       122 of       686\t|\tloss: 29.0197\n",
      "Training Epoch 3  17.9% | batch:       123 of       686\t|\tloss: 29.7247\n",
      "Training Epoch 3  18.1% | batch:       124 of       686\t|\tloss: 35.8109\n",
      "Training Epoch 3  18.2% | batch:       125 of       686\t|\tloss: 37.8589\n",
      "Training Epoch 3  18.4% | batch:       126 of       686\t|\tloss: 37.6139\n",
      "Training Epoch 3  18.5% | batch:       127 of       686\t|\tloss: 37.6785\n",
      "Training Epoch 3  18.7% | batch:       128 of       686\t|\tloss: 40.7163\n",
      "Training Epoch 3  18.8% | batch:       129 of       686\t|\tloss: 45.0137\n",
      "Training Epoch 3  19.0% | batch:       130 of       686\t|\tloss: 40.0691\n",
      "Training Epoch 3  19.1% | batch:       131 of       686\t|\tloss: 42.348\n",
      "Training Epoch 3  19.2% | batch:       132 of       686\t|\tloss: 32.7354\n",
      "Training Epoch 3  19.4% | batch:       133 of       686\t|\tloss: 43.1859\n",
      "Training Epoch 3  19.5% | batch:       134 of       686\t|\tloss: 36.3488\n",
      "Training Epoch 3  19.7% | batch:       135 of       686\t|\tloss: 36.1236\n",
      "Training Epoch 3  19.8% | batch:       136 of       686\t|\tloss: 46.1526\n",
      "Training Epoch 3  20.0% | batch:       137 of       686\t|\tloss: 42.8576\n",
      "Training Epoch 3  20.1% | batch:       138 of       686\t|\tloss: 37.5576\n",
      "Training Epoch 3  20.3% | batch:       139 of       686\t|\tloss: 35.9319\n",
      "Training Epoch 3  20.4% | batch:       140 of       686\t|\tloss: 47.9371\n",
      "Training Epoch 3  20.6% | batch:       141 of       686\t|\tloss: 28.1821\n",
      "Training Epoch 3  20.7% | batch:       142 of       686\t|\tloss: 39.2721\n",
      "Training Epoch 3  20.8% | batch:       143 of       686\t|\tloss: 43.7054\n",
      "Training Epoch 3  21.0% | batch:       144 of       686\t|\tloss: 39.5584\n",
      "Training Epoch 3  21.1% | batch:       145 of       686\t|\tloss: 24.2338\n",
      "Training Epoch 3  21.3% | batch:       146 of       686\t|\tloss: 23.219\n",
      "Training Epoch 3  21.4% | batch:       147 of       686\t|\tloss: 35.2352\n",
      "Training Epoch 3  21.6% | batch:       148 of       686\t|\tloss: 38.5034\n",
      "Training Epoch 3  21.7% | batch:       149 of       686\t|\tloss: 31.0668\n",
      "Training Epoch 3  21.9% | batch:       150 of       686\t|\tloss: 24.5505\n",
      "Training Epoch 3  22.0% | batch:       151 of       686\t|\tloss: 101.075\n",
      "Training Epoch 3  22.2% | batch:       152 of       686\t|\tloss: 31.6503\n",
      "Training Epoch 3  22.3% | batch:       153 of       686\t|\tloss: 48.3313\n",
      "Training Epoch 3  22.4% | batch:       154 of       686\t|\tloss: 49.7958\n",
      "Training Epoch 3  22.6% | batch:       155 of       686\t|\tloss: 33.3577\n",
      "Training Epoch 3  22.7% | batch:       156 of       686\t|\tloss: 44.996\n",
      "Training Epoch 3  22.9% | batch:       157 of       686\t|\tloss: 42.7058\n",
      "Training Epoch 3  23.0% | batch:       158 of       686\t|\tloss: 30.7083\n",
      "Training Epoch 3  23.2% | batch:       159 of       686\t|\tloss: 49.9277\n",
      "Training Epoch 3  23.3% | batch:       160 of       686\t|\tloss: 48.6006\n",
      "Training Epoch 3  23.5% | batch:       161 of       686\t|\tloss: 31.4479\n",
      "Training Epoch 3  23.6% | batch:       162 of       686\t|\tloss: 35.2636\n",
      "Training Epoch 3  23.8% | batch:       163 of       686\t|\tloss: 33.1668\n",
      "Training Epoch 3  23.9% | batch:       164 of       686\t|\tloss: 29.1937\n",
      "Training Epoch 3  24.1% | batch:       165 of       686\t|\tloss: 42.102\n",
      "Training Epoch 3  24.2% | batch:       166 of       686\t|\tloss: 32.688\n",
      "Training Epoch 3  24.3% | batch:       167 of       686\t|\tloss: 46.8522\n",
      "Training Epoch 3  24.5% | batch:       168 of       686\t|\tloss: 40.3096\n",
      "Training Epoch 3  24.6% | batch:       169 of       686\t|\tloss: 40.8626\n",
      "Training Epoch 3  24.8% | batch:       170 of       686\t|\tloss: 43.1314\n",
      "Training Epoch 3  24.9% | batch:       171 of       686\t|\tloss: 34.0437\n",
      "Training Epoch 3  25.1% | batch:       172 of       686\t|\tloss: 37.1818\n",
      "Training Epoch 3  25.2% | batch:       173 of       686\t|\tloss: 37.1968\n",
      "Training Epoch 3  25.4% | batch:       174 of       686\t|\tloss: 32.0382\n",
      "Training Epoch 3  25.5% | batch:       175 of       686\t|\tloss: 44.8067\n",
      "Training Epoch 3  25.7% | batch:       176 of       686\t|\tloss: 34.189\n",
      "Training Epoch 3  25.8% | batch:       177 of       686\t|\tloss: 35.5975\n",
      "Training Epoch 3  25.9% | batch:       178 of       686\t|\tloss: 25.951\n",
      "Training Epoch 3  26.1% | batch:       179 of       686\t|\tloss: 29.8907\n",
      "Training Epoch 3  26.2% | batch:       180 of       686\t|\tloss: 36.1649\n",
      "Training Epoch 3  26.4% | batch:       181 of       686\t|\tloss: 34.0035\n",
      "Training Epoch 3  26.5% | batch:       182 of       686\t|\tloss: 37.9032\n",
      "Training Epoch 3  26.7% | batch:       183 of       686\t|\tloss: 41.656\n",
      "Training Epoch 3  26.8% | batch:       184 of       686\t|\tloss: 34.3886\n",
      "Training Epoch 3  27.0% | batch:       185 of       686\t|\tloss: 28.8903\n",
      "Training Epoch 3  27.1% | batch:       186 of       686\t|\tloss: 34.8075\n",
      "Training Epoch 3  27.3% | batch:       187 of       686\t|\tloss: 48.8526\n",
      "Training Epoch 3  27.4% | batch:       188 of       686\t|\tloss: 35.3969\n",
      "Training Epoch 3  27.6% | batch:       189 of       686\t|\tloss: 85.0695\n",
      "Training Epoch 3  27.7% | batch:       190 of       686\t|\tloss: 41.2972\n",
      "Training Epoch 3  27.8% | batch:       191 of       686\t|\tloss: 34.1539\n",
      "Training Epoch 3  28.0% | batch:       192 of       686\t|\tloss: 32.9863\n",
      "Training Epoch 3  28.1% | batch:       193 of       686\t|\tloss: 29.1123\n",
      "Training Epoch 3  28.3% | batch:       194 of       686\t|\tloss: 23.1707\n",
      "Training Epoch 3  28.4% | batch:       195 of       686\t|\tloss: 42.023\n",
      "Training Epoch 3  28.6% | batch:       196 of       686\t|\tloss: 25.3103\n",
      "Training Epoch 3  28.7% | batch:       197 of       686\t|\tloss: 40.6098\n",
      "Training Epoch 3  28.9% | batch:       198 of       686\t|\tloss: 31.7509\n",
      "Training Epoch 3  29.0% | batch:       199 of       686\t|\tloss: 34.3053\n",
      "Training Epoch 3  29.2% | batch:       200 of       686\t|\tloss: 30.9666\n",
      "Training Epoch 3  29.3% | batch:       201 of       686\t|\tloss: 33.5777\n",
      "Training Epoch 3  29.4% | batch:       202 of       686\t|\tloss: 29.7448\n",
      "Training Epoch 3  29.6% | batch:       203 of       686\t|\tloss: 29.6533\n",
      "Training Epoch 3  29.7% | batch:       204 of       686\t|\tloss: 29.6971\n",
      "Training Epoch 3  29.9% | batch:       205 of       686\t|\tloss: 32.7561\n",
      "Training Epoch 3  30.0% | batch:       206 of       686\t|\tloss: 41.3999\n",
      "Training Epoch 3  30.2% | batch:       207 of       686\t|\tloss: 31.6501\n",
      "Training Epoch 3  30.3% | batch:       208 of       686\t|\tloss: 27.6212\n",
      "Training Epoch 3  30.5% | batch:       209 of       686\t|\tloss: 40.5686\n",
      "Training Epoch 3  30.6% | batch:       210 of       686\t|\tloss: 40.2574\n",
      "Training Epoch 3  30.8% | batch:       211 of       686\t|\tloss: 35.923\n",
      "Training Epoch 3  30.9% | batch:       212 of       686\t|\tloss: 37.3737\n",
      "Training Epoch 3  31.0% | batch:       213 of       686\t|\tloss: 36.3218\n",
      "Training Epoch 3  31.2% | batch:       214 of       686\t|\tloss: 52.7301\n",
      "Training Epoch 3  31.3% | batch:       215 of       686\t|\tloss: 33.4439\n",
      "Training Epoch 3  31.5% | batch:       216 of       686\t|\tloss: 37.2807\n",
      "Training Epoch 3  31.6% | batch:       217 of       686\t|\tloss: 40.8674\n",
      "Training Epoch 3  31.8% | batch:       218 of       686\t|\tloss: 31.7294\n",
      "Training Epoch 3  31.9% | batch:       219 of       686\t|\tloss: 34.8317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  32.1% | batch:       220 of       686\t|\tloss: 32.9938\n",
      "Training Epoch 3  32.2% | batch:       221 of       686\t|\tloss: 42.1967\n",
      "Training Epoch 3  32.4% | batch:       222 of       686\t|\tloss: 28.0669\n",
      "Training Epoch 3  32.5% | batch:       223 of       686\t|\tloss: 30.7673\n",
      "Training Epoch 3  32.7% | batch:       224 of       686\t|\tloss: 41.1071\n",
      "Training Epoch 3  32.8% | batch:       225 of       686\t|\tloss: 58.6304\n",
      "Training Epoch 3  32.9% | batch:       226 of       686\t|\tloss: 35.7512\n",
      "Training Epoch 3  33.1% | batch:       227 of       686\t|\tloss: 50.2044\n",
      "Training Epoch 3  33.2% | batch:       228 of       686\t|\tloss: 32.265\n",
      "Training Epoch 3  33.4% | batch:       229 of       686\t|\tloss: 36.7492\n",
      "Training Epoch 3  33.5% | batch:       230 of       686\t|\tloss: 35.0363\n",
      "Training Epoch 3  33.7% | batch:       231 of       686\t|\tloss: 31.3672\n",
      "Training Epoch 3  33.8% | batch:       232 of       686\t|\tloss: 33.7965\n",
      "Training Epoch 3  34.0% | batch:       233 of       686\t|\tloss: 45.2809\n",
      "Training Epoch 3  34.1% | batch:       234 of       686\t|\tloss: 31.0797\n",
      "Training Epoch 3  34.3% | batch:       235 of       686\t|\tloss: 33.8288\n",
      "Training Epoch 3  34.4% | batch:       236 of       686\t|\tloss: 30.9542\n",
      "Training Epoch 3  34.5% | batch:       237 of       686\t|\tloss: 30.3462\n",
      "Training Epoch 3  34.7% | batch:       238 of       686\t|\tloss: 25.212\n",
      "Training Epoch 3  34.8% | batch:       239 of       686\t|\tloss: 38.4314\n",
      "Training Epoch 3  35.0% | batch:       240 of       686\t|\tloss: 32.5043\n",
      "Training Epoch 3  35.1% | batch:       241 of       686\t|\tloss: 40.0499\n",
      "Training Epoch 3  35.3% | batch:       242 of       686\t|\tloss: 39.0899\n",
      "Training Epoch 3  35.4% | batch:       243 of       686\t|\tloss: 33.1211\n",
      "Training Epoch 3  35.6% | batch:       244 of       686\t|\tloss: 36.8445\n",
      "Training Epoch 3  35.7% | batch:       245 of       686\t|\tloss: 41.7261\n",
      "Training Epoch 3  35.9% | batch:       246 of       686\t|\tloss: 27.9101\n",
      "Training Epoch 3  36.0% | batch:       247 of       686\t|\tloss: 29.1684\n",
      "Training Epoch 3  36.2% | batch:       248 of       686\t|\tloss: 38.4653\n",
      "Training Epoch 3  36.3% | batch:       249 of       686\t|\tloss: 30.459\n",
      "Training Epoch 3  36.4% | batch:       250 of       686\t|\tloss: 35.0078\n",
      "Training Epoch 3  36.6% | batch:       251 of       686\t|\tloss: 30.0173\n",
      "Training Epoch 3  36.7% | batch:       252 of       686\t|\tloss: 34.4757\n",
      "Training Epoch 3  36.9% | batch:       253 of       686\t|\tloss: 36.3652\n",
      "Training Epoch 3  37.0% | batch:       254 of       686\t|\tloss: 39.509\n",
      "Training Epoch 3  37.2% | batch:       255 of       686\t|\tloss: 29.956\n",
      "Training Epoch 3  37.3% | batch:       256 of       686\t|\tloss: 32.1258\n",
      "Training Epoch 3  37.5% | batch:       257 of       686\t|\tloss: 39.2803\n",
      "Training Epoch 3  37.6% | batch:       258 of       686\t|\tloss: 39.2393\n",
      "Training Epoch 3  37.8% | batch:       259 of       686\t|\tloss: 37.0521\n",
      "Training Epoch 3  37.9% | batch:       260 of       686\t|\tloss: 48.1656\n",
      "Training Epoch 3  38.0% | batch:       261 of       686\t|\tloss: 34.3707\n",
      "Training Epoch 3  38.2% | batch:       262 of       686\t|\tloss: 34.279\n",
      "Training Epoch 3  38.3% | batch:       263 of       686\t|\tloss: 34.2427\n",
      "Training Epoch 3  38.5% | batch:       264 of       686\t|\tloss: 30.2289\n",
      "Training Epoch 3  38.6% | batch:       265 of       686\t|\tloss: 29.2148\n",
      "Training Epoch 3  38.8% | batch:       266 of       686\t|\tloss: 35.1304\n",
      "Training Epoch 3  38.9% | batch:       267 of       686\t|\tloss: 43.1285\n",
      "Training Epoch 3  39.1% | batch:       268 of       686\t|\tloss: 36.2692\n",
      "Training Epoch 3  39.2% | batch:       269 of       686\t|\tloss: 39.5152\n",
      "Training Epoch 3  39.4% | batch:       270 of       686\t|\tloss: 29.0358\n",
      "Training Epoch 3  39.5% | batch:       271 of       686\t|\tloss: 37.1338\n",
      "Training Epoch 3  39.7% | batch:       272 of       686\t|\tloss: 33.1312\n",
      "Training Epoch 3  39.8% | batch:       273 of       686\t|\tloss: 24.2547\n",
      "Training Epoch 3  39.9% | batch:       274 of       686\t|\tloss: 49.9763\n",
      "Training Epoch 3  40.1% | batch:       275 of       686\t|\tloss: 29.3323\n",
      "Training Epoch 3  40.2% | batch:       276 of       686\t|\tloss: 32.3878\n",
      "Training Epoch 3  40.4% | batch:       277 of       686\t|\tloss: 34.0466\n",
      "Training Epoch 3  40.5% | batch:       278 of       686\t|\tloss: 26.441\n",
      "Training Epoch 3  40.7% | batch:       279 of       686\t|\tloss: 29.6679\n",
      "Training Epoch 3  40.8% | batch:       280 of       686\t|\tloss: 37.1308\n",
      "Training Epoch 3  41.0% | batch:       281 of       686\t|\tloss: 60.87\n",
      "Training Epoch 3  41.1% | batch:       282 of       686\t|\tloss: 28.3693\n",
      "Training Epoch 3  41.3% | batch:       283 of       686\t|\tloss: 33.1681\n",
      "Training Epoch 3  41.4% | batch:       284 of       686\t|\tloss: 26.287\n",
      "Training Epoch 3  41.5% | batch:       285 of       686\t|\tloss: 41.4868\n",
      "Training Epoch 3  41.7% | batch:       286 of       686\t|\tloss: 31.1449\n",
      "Training Epoch 3  41.8% | batch:       287 of       686\t|\tloss: 31.7826\n",
      "Training Epoch 3  42.0% | batch:       288 of       686\t|\tloss: 34.4445\n",
      "Training Epoch 3  42.1% | batch:       289 of       686\t|\tloss: 45.2336\n",
      "Training Epoch 3  42.3% | batch:       290 of       686\t|\tloss: 34.2389\n",
      "Training Epoch 3  42.4% | batch:       291 of       686\t|\tloss: 42.4275\n",
      "Training Epoch 3  42.6% | batch:       292 of       686\t|\tloss: 29.4316\n",
      "Training Epoch 3  42.7% | batch:       293 of       686\t|\tloss: 31.6551\n",
      "Training Epoch 3  42.9% | batch:       294 of       686\t|\tloss: 35.2948\n",
      "Training Epoch 3  43.0% | batch:       295 of       686\t|\tloss: 35.717\n",
      "Training Epoch 3  43.1% | batch:       296 of       686\t|\tloss: 75.707\n",
      "Training Epoch 3  43.3% | batch:       297 of       686\t|\tloss: 27.701\n",
      "Training Epoch 3  43.4% | batch:       298 of       686\t|\tloss: 38.1544\n",
      "Training Epoch 3  43.6% | batch:       299 of       686\t|\tloss: 33.573\n",
      "Training Epoch 3  43.7% | batch:       300 of       686\t|\tloss: 34.0724\n",
      "Training Epoch 3  43.9% | batch:       301 of       686\t|\tloss: 26.2137\n",
      "Training Epoch 3  44.0% | batch:       302 of       686\t|\tloss: 31.4037\n",
      "Training Epoch 3  44.2% | batch:       303 of       686\t|\tloss: 29.9566\n",
      "Training Epoch 3  44.3% | batch:       304 of       686\t|\tloss: 39.9106\n",
      "Training Epoch 3  44.5% | batch:       305 of       686\t|\tloss: 33.1011\n",
      "Training Epoch 3  44.6% | batch:       306 of       686\t|\tloss: 32.005\n",
      "Training Epoch 3  44.8% | batch:       307 of       686\t|\tloss: 37.0109\n",
      "Training Epoch 3  44.9% | batch:       308 of       686\t|\tloss: 34.2906\n",
      "Training Epoch 3  45.0% | batch:       309 of       686\t|\tloss: 33.7018\n",
      "Training Epoch 3  45.2% | batch:       310 of       686\t|\tloss: 37.4619\n",
      "Training Epoch 3  45.3% | batch:       311 of       686\t|\tloss: 42.3677\n",
      "Training Epoch 3  45.5% | batch:       312 of       686\t|\tloss: 25.4862\n",
      "Training Epoch 3  45.6% | batch:       313 of       686\t|\tloss: 35.4577\n",
      "Training Epoch 3  45.8% | batch:       314 of       686\t|\tloss: 34.5995\n",
      "Training Epoch 3  45.9% | batch:       315 of       686\t|\tloss: 33.0629\n",
      "Training Epoch 3  46.1% | batch:       316 of       686\t|\tloss: 26.9339\n",
      "Training Epoch 3  46.2% | batch:       317 of       686\t|\tloss: 27.9786\n",
      "Training Epoch 3  46.4% | batch:       318 of       686\t|\tloss: 34.3604\n",
      "Training Epoch 3  46.5% | batch:       319 of       686\t|\tloss: 32.4547\n",
      "Training Epoch 3  46.6% | batch:       320 of       686\t|\tloss: 41.8451\n",
      "Training Epoch 3  46.8% | batch:       321 of       686\t|\tloss: 60.1565\n",
      "Training Epoch 3  46.9% | batch:       322 of       686\t|\tloss: 44.2643\n",
      "Training Epoch 3  47.1% | batch:       323 of       686\t|\tloss: 24.9924\n",
      "Training Epoch 3  47.2% | batch:       324 of       686\t|\tloss: 30.9351\n",
      "Training Epoch 3  47.4% | batch:       325 of       686\t|\tloss: 39.7789\n",
      "Training Epoch 3  47.5% | batch:       326 of       686\t|\tloss: 32.1586\n",
      "Training Epoch 3  47.7% | batch:       327 of       686\t|\tloss: 47.2228\n",
      "Training Epoch 3  47.8% | batch:       328 of       686\t|\tloss: 28.3459\n",
      "Training Epoch 3  48.0% | batch:       329 of       686\t|\tloss: 32.5913\n",
      "Training Epoch 3  48.1% | batch:       330 of       686\t|\tloss: 33.8329\n",
      "Training Epoch 3  48.3% | batch:       331 of       686\t|\tloss: 35.3985\n",
      "Training Epoch 3  48.4% | batch:       332 of       686\t|\tloss: 31.9165\n",
      "Training Epoch 3  48.5% | batch:       333 of       686\t|\tloss: 39.2636\n",
      "Training Epoch 3  48.7% | batch:       334 of       686\t|\tloss: 38.8049\n",
      "Training Epoch 3  48.8% | batch:       335 of       686\t|\tloss: 34.3857\n",
      "Training Epoch 3  49.0% | batch:       336 of       686\t|\tloss: 45.6231\n",
      "Training Epoch 3  49.1% | batch:       337 of       686\t|\tloss: 33.3268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  49.3% | batch:       338 of       686\t|\tloss: 34.8041\n",
      "Training Epoch 3  49.4% | batch:       339 of       686\t|\tloss: 39.1631\n",
      "Training Epoch 3  49.6% | batch:       340 of       686\t|\tloss: 37.1282\n",
      "Training Epoch 3  49.7% | batch:       341 of       686\t|\tloss: 30.4835\n",
      "Training Epoch 3  49.9% | batch:       342 of       686\t|\tloss: 27.2882\n",
      "Training Epoch 3  50.0% | batch:       343 of       686\t|\tloss: 24.2884\n",
      "Training Epoch 3  50.1% | batch:       344 of       686\t|\tloss: 37.0224\n",
      "Training Epoch 3  50.3% | batch:       345 of       686\t|\tloss: 23.0493\n",
      "Training Epoch 3  50.4% | batch:       346 of       686\t|\tloss: 40.3684\n",
      "Training Epoch 3  50.6% | batch:       347 of       686\t|\tloss: 45.4726\n",
      "Training Epoch 3  50.7% | batch:       348 of       686\t|\tloss: 36.5344\n",
      "Training Epoch 3  50.9% | batch:       349 of       686\t|\tloss: 49.7003\n",
      "Training Epoch 3  51.0% | batch:       350 of       686\t|\tloss: 34.743\n",
      "Training Epoch 3  51.2% | batch:       351 of       686\t|\tloss: 33.9208\n",
      "Training Epoch 3  51.3% | batch:       352 of       686\t|\tloss: 24.7719\n",
      "Training Epoch 3  51.5% | batch:       353 of       686\t|\tloss: 35.5549\n",
      "Training Epoch 3  51.6% | batch:       354 of       686\t|\tloss: 31.004\n",
      "Training Epoch 3  51.7% | batch:       355 of       686\t|\tloss: 30.0433\n",
      "Training Epoch 3  51.9% | batch:       356 of       686\t|\tloss: 37.2916\n",
      "Training Epoch 3  52.0% | batch:       357 of       686\t|\tloss: 30.7683\n",
      "Training Epoch 3  52.2% | batch:       358 of       686\t|\tloss: 27.8336\n",
      "Training Epoch 3  52.3% | batch:       359 of       686\t|\tloss: 22.8787\n",
      "Training Epoch 3  52.5% | batch:       360 of       686\t|\tloss: 34.0687\n",
      "Training Epoch 3  52.6% | batch:       361 of       686\t|\tloss: 38.9593\n",
      "Training Epoch 3  52.8% | batch:       362 of       686\t|\tloss: 28.7239\n",
      "Training Epoch 3  52.9% | batch:       363 of       686\t|\tloss: 35.9459\n",
      "Training Epoch 3  53.1% | batch:       364 of       686\t|\tloss: 36.9746\n",
      "Training Epoch 3  53.2% | batch:       365 of       686\t|\tloss: 37.6071\n",
      "Training Epoch 3  53.4% | batch:       366 of       686\t|\tloss: 41.5362\n",
      "Training Epoch 3  53.5% | batch:       367 of       686\t|\tloss: 40.3284\n",
      "Training Epoch 3  53.6% | batch:       368 of       686\t|\tloss: 38.44\n",
      "Training Epoch 3  53.8% | batch:       369 of       686\t|\tloss: 29.4138\n",
      "Training Epoch 3  53.9% | batch:       370 of       686\t|\tloss: 31.1232\n",
      "Training Epoch 3  54.1% | batch:       371 of       686\t|\tloss: 27.4199\n",
      "Training Epoch 3  54.2% | batch:       372 of       686\t|\tloss: 34.4947\n",
      "Training Epoch 3  54.4% | batch:       373 of       686\t|\tloss: 34.0849\n",
      "Training Epoch 3  54.5% | batch:       374 of       686\t|\tloss: 27.711\n",
      "Training Epoch 3  54.7% | batch:       375 of       686\t|\tloss: 40.0065\n",
      "Training Epoch 3  54.8% | batch:       376 of       686\t|\tloss: 44.1858\n",
      "Training Epoch 3  55.0% | batch:       377 of       686\t|\tloss: 26.8243\n",
      "Training Epoch 3  55.1% | batch:       378 of       686\t|\tloss: 48.9177\n",
      "Training Epoch 3  55.2% | batch:       379 of       686\t|\tloss: 29.1207\n",
      "Training Epoch 3  55.4% | batch:       380 of       686\t|\tloss: 41.6855\n",
      "Training Epoch 3  55.5% | batch:       381 of       686\t|\tloss: 31.5619\n",
      "Training Epoch 3  55.7% | batch:       382 of       686\t|\tloss: 33.7748\n",
      "Training Epoch 3  55.8% | batch:       383 of       686\t|\tloss: 24.3633\n",
      "Training Epoch 3  56.0% | batch:       384 of       686\t|\tloss: 38.3963\n",
      "Training Epoch 3  56.1% | batch:       385 of       686\t|\tloss: 30.7449\n",
      "Training Epoch 3  56.3% | batch:       386 of       686\t|\tloss: 34.9323\n",
      "Training Epoch 3  56.4% | batch:       387 of       686\t|\tloss: 36.9187\n",
      "Training Epoch 3  56.6% | batch:       388 of       686\t|\tloss: 34.1038\n",
      "Training Epoch 3  56.7% | batch:       389 of       686\t|\tloss: 38.1959\n",
      "Training Epoch 3  56.9% | batch:       390 of       686\t|\tloss: 38.8155\n",
      "Training Epoch 3  57.0% | batch:       391 of       686\t|\tloss: 26.6509\n",
      "Training Epoch 3  57.1% | batch:       392 of       686\t|\tloss: 27.8593\n",
      "Training Epoch 3  57.3% | batch:       393 of       686\t|\tloss: 27.5954\n",
      "Training Epoch 3  57.4% | batch:       394 of       686\t|\tloss: 43.6847\n",
      "Training Epoch 3  57.6% | batch:       395 of       686\t|\tloss: 36.9877\n",
      "Training Epoch 3  57.7% | batch:       396 of       686\t|\tloss: 38.1747\n",
      "Training Epoch 3  57.9% | batch:       397 of       686\t|\tloss: 44.7509\n",
      "Training Epoch 3  58.0% | batch:       398 of       686\t|\tloss: 39.8124\n",
      "Training Epoch 3  58.2% | batch:       399 of       686\t|\tloss: 30.9677\n",
      "Training Epoch 3  58.3% | batch:       400 of       686\t|\tloss: 28.0369\n",
      "Training Epoch 3  58.5% | batch:       401 of       686\t|\tloss: 31.8835\n",
      "Training Epoch 3  58.6% | batch:       402 of       686\t|\tloss: 32.8625\n",
      "Training Epoch 3  58.7% | batch:       403 of       686\t|\tloss: 26.0453\n",
      "Training Epoch 3  58.9% | batch:       404 of       686\t|\tloss: 23.0546\n",
      "Training Epoch 3  59.0% | batch:       405 of       686\t|\tloss: 32.9341\n",
      "Training Epoch 3  59.2% | batch:       406 of       686\t|\tloss: 41.3248\n",
      "Training Epoch 3  59.3% | batch:       407 of       686\t|\tloss: 29.3102\n",
      "Training Epoch 3  59.5% | batch:       408 of       686\t|\tloss: 38.7512\n",
      "Training Epoch 3  59.6% | batch:       409 of       686\t|\tloss: 31.5852\n",
      "Training Epoch 3  59.8% | batch:       410 of       686\t|\tloss: 44.065\n",
      "Training Epoch 3  59.9% | batch:       411 of       686\t|\tloss: 40.5419\n",
      "Training Epoch 3  60.1% | batch:       412 of       686\t|\tloss: 33.6707\n",
      "Training Epoch 3  60.2% | batch:       413 of       686\t|\tloss: 25.1136\n",
      "Training Epoch 3  60.3% | batch:       414 of       686\t|\tloss: 37.7351\n",
      "Training Epoch 3  60.5% | batch:       415 of       686\t|\tloss: 57.3804\n",
      "Training Epoch 3  60.6% | batch:       416 of       686\t|\tloss: 36.2304\n",
      "Training Epoch 3  60.8% | batch:       417 of       686\t|\tloss: 29.3375\n",
      "Training Epoch 3  60.9% | batch:       418 of       686\t|\tloss: 38.277\n",
      "Training Epoch 3  61.1% | batch:       419 of       686\t|\tloss: 35.0095\n",
      "Training Epoch 3  61.2% | batch:       420 of       686\t|\tloss: 36.773\n",
      "Training Epoch 3  61.4% | batch:       421 of       686\t|\tloss: 41.6218\n",
      "Training Epoch 3  61.5% | batch:       422 of       686\t|\tloss: 30.3049\n",
      "Training Epoch 3  61.7% | batch:       423 of       686\t|\tloss: 27.2427\n",
      "Training Epoch 3  61.8% | batch:       424 of       686\t|\tloss: 46.4022\n",
      "Training Epoch 3  62.0% | batch:       425 of       686\t|\tloss: 37.2688\n",
      "Training Epoch 3  62.1% | batch:       426 of       686\t|\tloss: 32.6721\n",
      "Training Epoch 3  62.2% | batch:       427 of       686\t|\tloss: 31.5939\n",
      "Training Epoch 3  62.4% | batch:       428 of       686\t|\tloss: 28.1091\n",
      "Training Epoch 3  62.5% | batch:       429 of       686\t|\tloss: 42.2108\n",
      "Training Epoch 3  62.7% | batch:       430 of       686\t|\tloss: 42.3261\n",
      "Training Epoch 3  62.8% | batch:       431 of       686\t|\tloss: 35.0665\n",
      "Training Epoch 3  63.0% | batch:       432 of       686\t|\tloss: 29.1527\n",
      "Training Epoch 3  63.1% | batch:       433 of       686\t|\tloss: 36.9252\n",
      "Training Epoch 3  63.3% | batch:       434 of       686\t|\tloss: 32.5499\n",
      "Training Epoch 3  63.4% | batch:       435 of       686\t|\tloss: 33.9357\n",
      "Training Epoch 3  63.6% | batch:       436 of       686\t|\tloss: 44.6345\n",
      "Training Epoch 3  63.7% | batch:       437 of       686\t|\tloss: 39.3195\n",
      "Training Epoch 3  63.8% | batch:       438 of       686\t|\tloss: 36.8616\n",
      "Training Epoch 3  64.0% | batch:       439 of       686\t|\tloss: 38.0192\n",
      "Training Epoch 3  64.1% | batch:       440 of       686\t|\tloss: 35.2065\n",
      "Training Epoch 3  64.3% | batch:       441 of       686\t|\tloss: 37.1036\n",
      "Training Epoch 3  64.4% | batch:       442 of       686\t|\tloss: 59.5961\n",
      "Training Epoch 3  64.6% | batch:       443 of       686\t|\tloss: 45.768\n",
      "Training Epoch 3  64.7% | batch:       444 of       686\t|\tloss: 31.9615\n",
      "Training Epoch 3  64.9% | batch:       445 of       686\t|\tloss: 33.6038\n",
      "Training Epoch 3  65.0% | batch:       446 of       686\t|\tloss: 89.6768\n",
      "Training Epoch 3  65.2% | batch:       447 of       686\t|\tloss: 36.8263\n",
      "Training Epoch 3  65.3% | batch:       448 of       686\t|\tloss: 39.165\n",
      "Training Epoch 3  65.5% | batch:       449 of       686\t|\tloss: 32.1631\n",
      "Training Epoch 3  65.6% | batch:       450 of       686\t|\tloss: 41.7503\n",
      "Training Epoch 3  65.7% | batch:       451 of       686\t|\tloss: 36.5013\n",
      "Training Epoch 3  65.9% | batch:       452 of       686\t|\tloss: 35.9091\n",
      "Training Epoch 3  66.0% | batch:       453 of       686\t|\tloss: 39.4562\n",
      "Training Epoch 3  66.2% | batch:       454 of       686\t|\tloss: 31.1436\n",
      "Training Epoch 3  66.3% | batch:       455 of       686\t|\tloss: 33.6047\n",
      "Training Epoch 3  66.5% | batch:       456 of       686\t|\tloss: 43.3482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  66.6% | batch:       457 of       686\t|\tloss: 27.8971\n",
      "Training Epoch 3  66.8% | batch:       458 of       686\t|\tloss: 46.0604\n",
      "Training Epoch 3  66.9% | batch:       459 of       686\t|\tloss: 35.5944\n",
      "Training Epoch 3  67.1% | batch:       460 of       686\t|\tloss: 31.5709\n",
      "Training Epoch 3  67.2% | batch:       461 of       686\t|\tloss: 46.9141\n",
      "Training Epoch 3  67.3% | batch:       462 of       686\t|\tloss: 39.1874\n",
      "Training Epoch 3  67.5% | batch:       463 of       686\t|\tloss: 47.18\n",
      "Training Epoch 3  67.6% | batch:       464 of       686\t|\tloss: 34.0865\n",
      "Training Epoch 3  67.8% | batch:       465 of       686\t|\tloss: 44.1588\n",
      "Training Epoch 3  67.9% | batch:       466 of       686\t|\tloss: 35.6768\n",
      "Training Epoch 3  68.1% | batch:       467 of       686\t|\tloss: 40.816\n",
      "Training Epoch 3  68.2% | batch:       468 of       686\t|\tloss: 41.3744\n",
      "Training Epoch 3  68.4% | batch:       469 of       686\t|\tloss: 31.5278\n",
      "Training Epoch 3  68.5% | batch:       470 of       686\t|\tloss: 28.9612\n",
      "Training Epoch 3  68.7% | batch:       471 of       686\t|\tloss: 36.9417\n",
      "Training Epoch 3  68.8% | batch:       472 of       686\t|\tloss: 34.6024\n",
      "Training Epoch 3  69.0% | batch:       473 of       686\t|\tloss: 33.1023\n",
      "Training Epoch 3  69.1% | batch:       474 of       686\t|\tloss: 35.2724\n",
      "Training Epoch 3  69.2% | batch:       475 of       686\t|\tloss: 23.3565\n",
      "Training Epoch 3  69.4% | batch:       476 of       686\t|\tloss: 42.2284\n",
      "Training Epoch 3  69.5% | batch:       477 of       686\t|\tloss: 28.2991\n",
      "Training Epoch 3  69.7% | batch:       478 of       686\t|\tloss: 33.9882\n",
      "Training Epoch 3  69.8% | batch:       479 of       686\t|\tloss: 35.1362\n",
      "Training Epoch 3  70.0% | batch:       480 of       686\t|\tloss: 29.5233\n",
      "Training Epoch 3  70.1% | batch:       481 of       686\t|\tloss: 34.7424\n",
      "Training Epoch 3  70.3% | batch:       482 of       686\t|\tloss: 41.3862\n",
      "Training Epoch 3  70.4% | batch:       483 of       686\t|\tloss: 61.079\n",
      "Training Epoch 3  70.6% | batch:       484 of       686\t|\tloss: 37.1915\n",
      "Training Epoch 3  70.7% | batch:       485 of       686\t|\tloss: 35.3085\n",
      "Training Epoch 3  70.8% | batch:       486 of       686\t|\tloss: 40.9057\n",
      "Training Epoch 3  71.0% | batch:       487 of       686\t|\tloss: 37.5042\n",
      "Training Epoch 3  71.1% | batch:       488 of       686\t|\tloss: 33.4913\n",
      "Training Epoch 3  71.3% | batch:       489 of       686\t|\tloss: 28.6786\n",
      "Training Epoch 3  71.4% | batch:       490 of       686\t|\tloss: 32.9532\n",
      "Training Epoch 3  71.6% | batch:       491 of       686\t|\tloss: 30.0615\n",
      "Training Epoch 3  71.7% | batch:       492 of       686\t|\tloss: 29.0728\n",
      "Training Epoch 3  71.9% | batch:       493 of       686\t|\tloss: 37.4165\n",
      "Training Epoch 3  72.0% | batch:       494 of       686\t|\tloss: 32.5476\n",
      "Training Epoch 3  72.2% | batch:       495 of       686\t|\tloss: 35.8292\n",
      "Training Epoch 3  72.3% | batch:       496 of       686\t|\tloss: 33.1251\n",
      "Training Epoch 3  72.4% | batch:       497 of       686\t|\tloss: 51.3188\n",
      "Training Epoch 3  72.6% | batch:       498 of       686\t|\tloss: 39.9355\n",
      "Training Epoch 3  72.7% | batch:       499 of       686\t|\tloss: 31.222\n",
      "Training Epoch 3  72.9% | batch:       500 of       686\t|\tloss: 34.2647\n",
      "Training Epoch 3  73.0% | batch:       501 of       686\t|\tloss: 25.3577\n",
      "Training Epoch 3  73.2% | batch:       502 of       686\t|\tloss: 40.8377\n",
      "Training Epoch 3  73.3% | batch:       503 of       686\t|\tloss: 32.5355\n",
      "Training Epoch 3  73.5% | batch:       504 of       686\t|\tloss: 33.4718\n",
      "Training Epoch 3  73.6% | batch:       505 of       686\t|\tloss: 29.4788\n",
      "Training Epoch 3  73.8% | batch:       506 of       686\t|\tloss: 34.0952\n",
      "Training Epoch 3  73.9% | batch:       507 of       686\t|\tloss: 28.8356\n",
      "Training Epoch 3  74.1% | batch:       508 of       686\t|\tloss: 42.2783\n",
      "Training Epoch 3  74.2% | batch:       509 of       686\t|\tloss: 35.1381\n",
      "Training Epoch 3  74.3% | batch:       510 of       686\t|\tloss: 36.8391\n",
      "Training Epoch 3  74.5% | batch:       511 of       686\t|\tloss: 31.4361\n",
      "Training Epoch 3  74.6% | batch:       512 of       686\t|\tloss: 32.3455\n",
      "Training Epoch 3  74.8% | batch:       513 of       686\t|\tloss: 44.6318\n",
      "Training Epoch 3  74.9% | batch:       514 of       686\t|\tloss: 40.5698\n",
      "Training Epoch 3  75.1% | batch:       515 of       686\t|\tloss: 32.2801\n",
      "Training Epoch 3  75.2% | batch:       516 of       686\t|\tloss: 49.509\n",
      "Training Epoch 3  75.4% | batch:       517 of       686\t|\tloss: 27.0942\n",
      "Training Epoch 3  75.5% | batch:       518 of       686\t|\tloss: 36.6316\n",
      "Training Epoch 3  75.7% | batch:       519 of       686\t|\tloss: 35.2799\n",
      "Training Epoch 3  75.8% | batch:       520 of       686\t|\tloss: 27.2322\n",
      "Training Epoch 3  75.9% | batch:       521 of       686\t|\tloss: 35.3052\n",
      "Training Epoch 3  76.1% | batch:       522 of       686\t|\tloss: 43.034\n",
      "Training Epoch 3  76.2% | batch:       523 of       686\t|\tloss: 37.8502\n",
      "Training Epoch 3  76.4% | batch:       524 of       686\t|\tloss: 33.8276\n",
      "Training Epoch 3  76.5% | batch:       525 of       686\t|\tloss: 35.9408\n",
      "Training Epoch 3  76.7% | batch:       526 of       686\t|\tloss: 30.6671\n",
      "Training Epoch 3  76.8% | batch:       527 of       686\t|\tloss: 30.0973\n",
      "Training Epoch 3  77.0% | batch:       528 of       686\t|\tloss: 33.1243\n",
      "Training Epoch 3  77.1% | batch:       529 of       686\t|\tloss: 38.1675\n",
      "Training Epoch 3  77.3% | batch:       530 of       686\t|\tloss: 40.7698\n",
      "Training Epoch 3  77.4% | batch:       531 of       686\t|\tloss: 32.382\n",
      "Training Epoch 3  77.6% | batch:       532 of       686\t|\tloss: 25.7703\n",
      "Training Epoch 3  77.7% | batch:       533 of       686\t|\tloss: 35.3151\n",
      "Training Epoch 3  77.8% | batch:       534 of       686\t|\tloss: 29.8115\n",
      "Training Epoch 3  78.0% | batch:       535 of       686\t|\tloss: 32.4501\n",
      "Training Epoch 3  78.1% | batch:       536 of       686\t|\tloss: 26.5687\n",
      "Training Epoch 3  78.3% | batch:       537 of       686\t|\tloss: 47.0436\n",
      "Training Epoch 3  78.4% | batch:       538 of       686\t|\tloss: 41.1897\n",
      "Training Epoch 3  78.6% | batch:       539 of       686\t|\tloss: 27.7294\n",
      "Training Epoch 3  78.7% | batch:       540 of       686\t|\tloss: 33.6504\n",
      "Training Epoch 3  78.9% | batch:       541 of       686\t|\tloss: 27.5918\n",
      "Training Epoch 3  79.0% | batch:       542 of       686\t|\tloss: 22.4229\n",
      "Training Epoch 3  79.2% | batch:       543 of       686\t|\tloss: 34.4169\n",
      "Training Epoch 3  79.3% | batch:       544 of       686\t|\tloss: 31.119\n",
      "Training Epoch 3  79.4% | batch:       545 of       686\t|\tloss: 34.3264\n",
      "Training Epoch 3  79.6% | batch:       546 of       686\t|\tloss: 35.4892\n",
      "Training Epoch 3  79.7% | batch:       547 of       686\t|\tloss: 32.8864\n",
      "Training Epoch 3  79.9% | batch:       548 of       686\t|\tloss: 31.8834\n",
      "Training Epoch 3  80.0% | batch:       549 of       686\t|\tloss: 31.9143\n",
      "Training Epoch 3  80.2% | batch:       550 of       686\t|\tloss: 41.3565\n",
      "Training Epoch 3  80.3% | batch:       551 of       686\t|\tloss: 23.5079\n",
      "Training Epoch 3  80.5% | batch:       552 of       686\t|\tloss: 29.4719\n",
      "Training Epoch 3  80.6% | batch:       553 of       686\t|\tloss: 42.5048\n",
      "Training Epoch 3  80.8% | batch:       554 of       686\t|\tloss: 27.3604\n",
      "Training Epoch 3  80.9% | batch:       555 of       686\t|\tloss: 36.3858\n",
      "Training Epoch 3  81.0% | batch:       556 of       686\t|\tloss: 33.8921\n",
      "Training Epoch 3  81.2% | batch:       557 of       686\t|\tloss: 26.7197\n",
      "Training Epoch 3  81.3% | batch:       558 of       686\t|\tloss: 28.9465\n",
      "Training Epoch 3  81.5% | batch:       559 of       686\t|\tloss: 26.6241\n",
      "Training Epoch 3  81.6% | batch:       560 of       686\t|\tloss: 25.7558\n",
      "Training Epoch 3  81.8% | batch:       561 of       686\t|\tloss: 33.5133\n",
      "Training Epoch 3  81.9% | batch:       562 of       686\t|\tloss: 39.2206\n",
      "Training Epoch 3  82.1% | batch:       563 of       686\t|\tloss: 37.7977\n",
      "Training Epoch 3  82.2% | batch:       564 of       686\t|\tloss: 26.3825\n",
      "Training Epoch 3  82.4% | batch:       565 of       686\t|\tloss: 56.4567\n",
      "Training Epoch 3  82.5% | batch:       566 of       686\t|\tloss: 37.5197\n",
      "Training Epoch 3  82.7% | batch:       567 of       686\t|\tloss: 31.6859\n",
      "Training Epoch 3  82.8% | batch:       568 of       686\t|\tloss: 36.6943\n",
      "Training Epoch 3  82.9% | batch:       569 of       686\t|\tloss: 27.6263\n",
      "Training Epoch 3  83.1% | batch:       570 of       686\t|\tloss: 38.8225\n",
      "Training Epoch 3  83.2% | batch:       571 of       686\t|\tloss: 40.5076\n",
      "Training Epoch 3  83.4% | batch:       572 of       686\t|\tloss: 27.106\n",
      "Training Epoch 3  83.5% | batch:       573 of       686\t|\tloss: 34.7874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  83.7% | batch:       574 of       686\t|\tloss: 23.7517\n",
      "Training Epoch 3  83.8% | batch:       575 of       686\t|\tloss: 47.6131\n",
      "Training Epoch 3  84.0% | batch:       576 of       686\t|\tloss: 30.0499\n",
      "Training Epoch 3  84.1% | batch:       577 of       686\t|\tloss: 28.8914\n",
      "Training Epoch 3  84.3% | batch:       578 of       686\t|\tloss: 33.8219\n",
      "Training Epoch 3  84.4% | batch:       579 of       686\t|\tloss: 23.8444\n",
      "Training Epoch 3  84.5% | batch:       580 of       686\t|\tloss: 29.5722\n",
      "Training Epoch 3  84.7% | batch:       581 of       686\t|\tloss: 35.2016\n",
      "Training Epoch 3  84.8% | batch:       582 of       686\t|\tloss: 38.036\n",
      "Training Epoch 3  85.0% | batch:       583 of       686\t|\tloss: 30.236\n",
      "Training Epoch 3  85.1% | batch:       584 of       686\t|\tloss: 23.0891\n",
      "Training Epoch 3  85.3% | batch:       585 of       686\t|\tloss: 25.4107\n",
      "Training Epoch 3  85.4% | batch:       586 of       686\t|\tloss: 24.3538\n",
      "Training Epoch 3  85.6% | batch:       587 of       686\t|\tloss: 31.4446\n",
      "Training Epoch 3  85.7% | batch:       588 of       686\t|\tloss: 29.7924\n",
      "Training Epoch 3  85.9% | batch:       589 of       686\t|\tloss: 27.9797\n",
      "Training Epoch 3  86.0% | batch:       590 of       686\t|\tloss: 34.8865\n",
      "Training Epoch 3  86.2% | batch:       591 of       686\t|\tloss: 30.6492\n",
      "Training Epoch 3  86.3% | batch:       592 of       686\t|\tloss: 30.6777\n",
      "Training Epoch 3  86.4% | batch:       593 of       686\t|\tloss: 26.9006\n",
      "Training Epoch 3  86.6% | batch:       594 of       686\t|\tloss: 26.546\n",
      "Training Epoch 3  86.7% | batch:       595 of       686\t|\tloss: 29.3705\n",
      "Training Epoch 3  86.9% | batch:       596 of       686\t|\tloss: 25.4972\n",
      "Training Epoch 3  87.0% | batch:       597 of       686\t|\tloss: 29.691\n",
      "Training Epoch 3  87.2% | batch:       598 of       686\t|\tloss: 36.9807\n",
      "Training Epoch 3  87.3% | batch:       599 of       686\t|\tloss: 55.9957\n",
      "Training Epoch 3  87.5% | batch:       600 of       686\t|\tloss: 28.9404\n",
      "Training Epoch 3  87.6% | batch:       601 of       686\t|\tloss: 43.8399\n",
      "Training Epoch 3  87.8% | batch:       602 of       686\t|\tloss: 26.5884\n",
      "Training Epoch 3  87.9% | batch:       603 of       686\t|\tloss: 35.8759\n",
      "Training Epoch 3  88.0% | batch:       604 of       686\t|\tloss: 29.2241\n",
      "Training Epoch 3  88.2% | batch:       605 of       686\t|\tloss: 39.6093\n",
      "Training Epoch 3  88.3% | batch:       606 of       686\t|\tloss: 33.1183\n",
      "Training Epoch 3  88.5% | batch:       607 of       686\t|\tloss: 26.438\n",
      "Training Epoch 3  88.6% | batch:       608 of       686\t|\tloss: 46.4937\n",
      "Training Epoch 3  88.8% | batch:       609 of       686\t|\tloss: 34.5592\n",
      "Training Epoch 3  88.9% | batch:       610 of       686\t|\tloss: 26.8848\n",
      "Training Epoch 3  89.1% | batch:       611 of       686\t|\tloss: 46.2936\n",
      "Training Epoch 3  89.2% | batch:       612 of       686\t|\tloss: 24.8622\n",
      "Training Epoch 3  89.4% | batch:       613 of       686\t|\tloss: 33.2111\n",
      "Training Epoch 3  89.5% | batch:       614 of       686\t|\tloss: 20.574\n",
      "Training Epoch 3  89.7% | batch:       615 of       686\t|\tloss: 49.9966\n",
      "Training Epoch 3  89.8% | batch:       616 of       686\t|\tloss: 26.8281\n",
      "Training Epoch 3  89.9% | batch:       617 of       686\t|\tloss: 26.4016\n",
      "Training Epoch 3  90.1% | batch:       618 of       686\t|\tloss: 33.7385\n",
      "Training Epoch 3  90.2% | batch:       619 of       686\t|\tloss: 38.0618\n",
      "Training Epoch 3  90.4% | batch:       620 of       686\t|\tloss: 30.8131\n",
      "Training Epoch 3  90.5% | batch:       621 of       686\t|\tloss: 28.9174\n",
      "Training Epoch 3  90.7% | batch:       622 of       686\t|\tloss: 35.0817\n",
      "Training Epoch 3  90.8% | batch:       623 of       686\t|\tloss: 24.4707\n",
      "Training Epoch 3  91.0% | batch:       624 of       686\t|\tloss: 34.2773\n",
      "Training Epoch 3  91.1% | batch:       625 of       686\t|\tloss: 34.1674\n",
      "Training Epoch 3  91.3% | batch:       626 of       686\t|\tloss: 33.0591\n",
      "Training Epoch 3  91.4% | batch:       627 of       686\t|\tloss: 41.9888\n",
      "Training Epoch 3  91.5% | batch:       628 of       686\t|\tloss: 23.3381\n",
      "Training Epoch 3  91.7% | batch:       629 of       686\t|\tloss: 35.4745\n",
      "Training Epoch 3  91.8% | batch:       630 of       686\t|\tloss: 23.2683\n",
      "Training Epoch 3  92.0% | batch:       631 of       686\t|\tloss: 31.0315\n",
      "Training Epoch 3  92.1% | batch:       632 of       686\t|\tloss: 31.7852\n",
      "Training Epoch 3  92.3% | batch:       633 of       686\t|\tloss: 30.166\n",
      "Training Epoch 3  92.4% | batch:       634 of       686\t|\tloss: 41.876\n",
      "Training Epoch 3  92.6% | batch:       635 of       686\t|\tloss: 30.2414\n",
      "Training Epoch 3  92.7% | batch:       636 of       686\t|\tloss: 33.8686\n",
      "Training Epoch 3  92.9% | batch:       637 of       686\t|\tloss: 36.0625\n",
      "Training Epoch 3  93.0% | batch:       638 of       686\t|\tloss: 30.7454\n",
      "Training Epoch 3  93.1% | batch:       639 of       686\t|\tloss: 33.2798\n",
      "Training Epoch 3  93.3% | batch:       640 of       686\t|\tloss: 30.7041\n",
      "Training Epoch 3  93.4% | batch:       641 of       686\t|\tloss: 31.5921\n",
      "Training Epoch 3  93.6% | batch:       642 of       686\t|\tloss: 36.5923\n",
      "Training Epoch 3  93.7% | batch:       643 of       686\t|\tloss: 29.1763\n",
      "Training Epoch 3  93.9% | batch:       644 of       686\t|\tloss: 28.3138\n",
      "Training Epoch 3  94.0% | batch:       645 of       686\t|\tloss: 24.807\n",
      "Training Epoch 3  94.2% | batch:       646 of       686\t|\tloss: 31.9003\n",
      "Training Epoch 3  94.3% | batch:       647 of       686\t|\tloss: 39.0441\n",
      "Training Epoch 3  94.5% | batch:       648 of       686\t|\tloss: 34.0855\n",
      "Training Epoch 3  94.6% | batch:       649 of       686\t|\tloss: 33.0206\n",
      "Training Epoch 3  94.8% | batch:       650 of       686\t|\tloss: 49.1974\n",
      "Training Epoch 3  94.9% | batch:       651 of       686\t|\tloss: 36.6794\n",
      "Training Epoch 3  95.0% | batch:       652 of       686\t|\tloss: 35.2894\n",
      "Training Epoch 3  95.2% | batch:       653 of       686\t|\tloss: 49.5854\n",
      "Training Epoch 3  95.3% | batch:       654 of       686\t|\tloss: 35.396\n",
      "Training Epoch 3  95.5% | batch:       655 of       686\t|\tloss: 26.5716\n",
      "Training Epoch 3  95.6% | batch:       656 of       686\t|\tloss: 33.7549\n",
      "Training Epoch 3  95.8% | batch:       657 of       686\t|\tloss: 24.7433\n",
      "Training Epoch 3  95.9% | batch:       658 of       686\t|\tloss: 33.981\n",
      "Training Epoch 3  96.1% | batch:       659 of       686\t|\tloss: 27.6936\n",
      "Training Epoch 3  96.2% | batch:       660 of       686\t|\tloss: 27.6069\n",
      "Training Epoch 3  96.4% | batch:       661 of       686\t|\tloss: 29.5583\n",
      "Training Epoch 3  96.5% | batch:       662 of       686\t|\tloss: 36.3445\n",
      "Training Epoch 3  96.6% | batch:       663 of       686\t|\tloss: 35.9013\n",
      "Training Epoch 3  96.8% | batch:       664 of       686\t|\tloss: 23.0276\n",
      "Training Epoch 3  96.9% | batch:       665 of       686\t|\tloss: 29.0654\n",
      "Training Epoch 3  97.1% | batch:       666 of       686\t|\tloss: 26.7693\n",
      "Training Epoch 3  97.2% | batch:       667 of       686\t|\tloss: 22.2736\n",
      "Training Epoch 3  97.4% | batch:       668 of       686\t|\tloss: 35.9031\n",
      "Training Epoch 3  97.5% | batch:       669 of       686\t|\tloss: 31.0458\n",
      "Training Epoch 3  97.7% | batch:       670 of       686\t|\tloss: 29.9209\n",
      "Training Epoch 3  97.8% | batch:       671 of       686\t|\tloss: 28.1615\n",
      "Training Epoch 3  98.0% | batch:       672 of       686\t|\tloss: 28.5738\n",
      "Training Epoch 3  98.1% | batch:       673 of       686\t|\tloss: 38.7558\n",
      "Training Epoch 3  98.3% | batch:       674 of       686\t|\tloss: 39.1494\n",
      "Training Epoch 3  98.4% | batch:       675 of       686\t|\tloss: 22.7944\n",
      "Training Epoch 3  98.5% | batch:       676 of       686\t|\tloss: 23.2514\n",
      "Training Epoch 3  98.7% | batch:       677 of       686\t|\tloss: 25.9857\n",
      "Training Epoch 3  98.8% | batch:       678 of       686\t|\tloss: 49.4504\n",
      "Training Epoch 3  99.0% | batch:       679 of       686\t|\tloss: 27.5241\n",
      "Training Epoch 3  99.1% | batch:       680 of       686\t|\tloss: 40.0392\n",
      "Training Epoch 3  99.3% | batch:       681 of       686\t|\tloss: 32.5837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:01:11,360 | INFO : Epoch 3 Training Summary: epoch: 3.000000 | loss: 35.696928 | \n",
      "2023-05-25 22:01:11,361 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 23.864827632904053 seconds\n",
      "\n",
      "2023-05-25 22:01:11,362 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.525367736816406 seconds\n",
      "2023-05-25 22:01:11,362 | INFO : Avg batch train. time: 0.03429353897495103 seconds\n",
      "2023-05-25 22:01:11,363 | INFO : Avg sample train. time: 0.0002682635011895365 seconds\n",
      "2023-05-25 22:01:11,363 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  99.4% | batch:       682 of       686\t|\tloss: 29.9944\n",
      "Training Epoch 3  99.6% | batch:       683 of       686\t|\tloss: 32.1114\n",
      "Training Epoch 3  99.7% | batch:       684 of       686\t|\tloss: 30.2187\n",
      "Training Epoch 3  99.9% | batch:       685 of       686\t|\tloss: 31.4846\n",
      "\n",
      "Evaluating Epoch 3   0.0% | batch:         0 of       172\t|\tloss: 2.57288\n",
      "Evaluating Epoch 3   0.6% | batch:         1 of       172\t|\tloss: 6.73176\n",
      "Evaluating Epoch 3   1.2% | batch:         2 of       172\t|\tloss: 8.74069\n",
      "Evaluating Epoch 3   1.7% | batch:         3 of       172\t|\tloss: 4.53396\n",
      "Evaluating Epoch 3   2.3% | batch:         4 of       172\t|\tloss: 8.4144\n",
      "Evaluating Epoch 3   2.9% | batch:         5 of       172\t|\tloss: 3.54183\n",
      "Evaluating Epoch 3   3.5% | batch:         6 of       172\t|\tloss: 7.52983\n",
      "Evaluating Epoch 3   4.1% | batch:         7 of       172\t|\tloss: 5.30315\n",
      "Evaluating Epoch 3   4.7% | batch:         8 of       172\t|\tloss: 5.24529\n",
      "Evaluating Epoch 3   5.2% | batch:         9 of       172\t|\tloss: 3.01095\n",
      "Evaluating Epoch 3   5.8% | batch:        10 of       172\t|\tloss: 7.76501\n",
      "Evaluating Epoch 3   6.4% | batch:        11 of       172\t|\tloss: 6.78923\n",
      "Evaluating Epoch 3   7.0% | batch:        12 of       172\t|\tloss: 6.84984\n",
      "Evaluating Epoch 3   7.6% | batch:        13 of       172\t|\tloss: 4.92254\n",
      "Evaluating Epoch 3   8.1% | batch:        14 of       172\t|\tloss: 6.62864\n",
      "Evaluating Epoch 3   8.7% | batch:        15 of       172\t|\tloss: 2.68668\n",
      "Evaluating Epoch 3   9.3% | batch:        16 of       172\t|\tloss: 6.03025\n",
      "Evaluating Epoch 3   9.9% | batch:        17 of       172\t|\tloss: 6.08494\n",
      "Evaluating Epoch 3  10.5% | batch:        18 of       172\t|\tloss: 4.45286\n",
      "Evaluating Epoch 3  11.0% | batch:        19 of       172\t|\tloss: 1.62138\n",
      "Evaluating Epoch 3  11.6% | batch:        20 of       172\t|\tloss: 13.452\n",
      "Evaluating Epoch 3  12.2% | batch:        21 of       172\t|\tloss: 1.90925\n",
      "Evaluating Epoch 3  12.8% | batch:        22 of       172\t|\tloss: 5.60033\n",
      "Evaluating Epoch 3  13.4% | batch:        23 of       172\t|\tloss: 2.77885\n",
      "Evaluating Epoch 3  14.0% | batch:        24 of       172\t|\tloss: 6.13956\n",
      "Evaluating Epoch 3  14.5% | batch:        25 of       172\t|\tloss: 10.7373\n",
      "Evaluating Epoch 3  15.1% | batch:        26 of       172\t|\tloss: 8.22134\n",
      "Evaluating Epoch 3  15.7% | batch:        27 of       172\t|\tloss: 1.96341\n",
      "Evaluating Epoch 3  16.3% | batch:        28 of       172\t|\tloss: 1.08585\n",
      "Evaluating Epoch 3  16.9% | batch:        29 of       172\t|\tloss: 12.3667\n",
      "Evaluating Epoch 3  17.4% | batch:        30 of       172\t|\tloss: 2.3058\n",
      "Evaluating Epoch 3  18.0% | batch:        31 of       172\t|\tloss: 16.4593\n",
      "Evaluating Epoch 3  18.6% | batch:        32 of       172\t|\tloss: 1.07463\n",
      "Evaluating Epoch 3  19.2% | batch:        33 of       172\t|\tloss: 8.18194\n",
      "Evaluating Epoch 3  19.8% | batch:        34 of       172\t|\tloss: 2.86899\n",
      "Evaluating Epoch 3  20.3% | batch:        35 of       172\t|\tloss: 3.12937\n",
      "Evaluating Epoch 3  20.9% | batch:        36 of       172\t|\tloss: 9.58975\n",
      "Evaluating Epoch 3  21.5% | batch:        37 of       172\t|\tloss: 5.8549\n",
      "Evaluating Epoch 3  22.1% | batch:        38 of       172\t|\tloss: 4.97321\n",
      "Evaluating Epoch 3  22.7% | batch:        39 of       172\t|\tloss: 2.16564\n",
      "Evaluating Epoch 3  23.3% | batch:        40 of       172\t|\tloss: 1.5852\n",
      "Evaluating Epoch 3  23.8% | batch:        41 of       172\t|\tloss: 9.81508\n",
      "Evaluating Epoch 3  24.4% | batch:        42 of       172\t|\tloss: 1.38033\n",
      "Evaluating Epoch 3  25.0% | batch:        43 of       172\t|\tloss: 6.4348\n",
      "Evaluating Epoch 3  25.6% | batch:        44 of       172\t|\tloss: 1.30783\n",
      "Evaluating Epoch 3  26.2% | batch:        45 of       172\t|\tloss: 8.69744\n",
      "Evaluating Epoch 3  26.7% | batch:        46 of       172\t|\tloss: 1.58114\n",
      "Evaluating Epoch 3  27.3% | batch:        47 of       172\t|\tloss: 13.5701\n",
      "Evaluating Epoch 3  27.9% | batch:        48 of       172\t|\tloss: 4.38639\n",
      "Evaluating Epoch 3  28.5% | batch:        49 of       172\t|\tloss: 7.32215\n",
      "Evaluating Epoch 3  29.1% | batch:        50 of       172\t|\tloss: 3.78345\n",
      "Evaluating Epoch 3  29.7% | batch:        51 of       172\t|\tloss: 3.15252\n",
      "Evaluating Epoch 3  30.2% | batch:        52 of       172\t|\tloss: 16.8104\n",
      "Evaluating Epoch 3  30.8% | batch:        53 of       172\t|\tloss: 2.65559\n",
      "Evaluating Epoch 3  31.4% | batch:        54 of       172\t|\tloss: 15.28\n",
      "Evaluating Epoch 3  32.0% | batch:        55 of       172\t|\tloss: 6.84828\n",
      "Evaluating Epoch 3  32.6% | batch:        56 of       172\t|\tloss: 4.24321\n",
      "Evaluating Epoch 3  33.1% | batch:        57 of       172\t|\tloss: 8.57973\n",
      "Evaluating Epoch 3  33.7% | batch:        58 of       172\t|\tloss: 5.03021\n",
      "Evaluating Epoch 3  34.3% | batch:        59 of       172\t|\tloss: 17.5585\n",
      "Evaluating Epoch 3  34.9% | batch:        60 of       172\t|\tloss: 5.03583\n",
      "Evaluating Epoch 3  35.5% | batch:        61 of       172\t|\tloss: 7.98876\n",
      "Evaluating Epoch 3  36.0% | batch:        62 of       172\t|\tloss: 4.54875\n",
      "Evaluating Epoch 3  36.6% | batch:        63 of       172\t|\tloss: 8.21377\n",
      "Evaluating Epoch 3  37.2% | batch:        64 of       172\t|\tloss: 13.8423\n",
      "Evaluating Epoch 3  37.8% | batch:        65 of       172\t|\tloss: 2.60415\n",
      "Evaluating Epoch 3  38.4% | batch:        66 of       172\t|\tloss: 17.1207\n",
      "Evaluating Epoch 3  39.0% | batch:        67 of       172\t|\tloss: 6.47397\n",
      "Evaluating Epoch 3  39.5% | batch:        68 of       172\t|\tloss: 3.96718\n",
      "Evaluating Epoch 3  40.1% | batch:        69 of       172\t|\tloss: 19.9839\n",
      "Evaluating Epoch 3  40.7% | batch:        70 of       172\t|\tloss: 1.86036\n",
      "Evaluating Epoch 3  41.3% | batch:        71 of       172\t|\tloss: 13.6414\n",
      "Evaluating Epoch 3  41.9% | batch:        72 of       172\t|\tloss: 6.70365\n",
      "Evaluating Epoch 3  42.4% | batch:        73 of       172\t|\tloss: 2.34087\n",
      "Evaluating Epoch 3  43.0% | batch:        74 of       172\t|\tloss: 1.02918\n",
      "Evaluating Epoch 3  43.6% | batch:        75 of       172\t|\tloss: 2.86819\n",
      "Evaluating Epoch 3  44.2% | batch:        76 of       172\t|\tloss: 2.10338\n",
      "Evaluating Epoch 3  44.8% | batch:        77 of       172\t|\tloss: 2.67727\n",
      "Evaluating Epoch 3  45.3% | batch:        78 of       172\t|\tloss: 2.95164\n",
      "Evaluating Epoch 3  45.9% | batch:        79 of       172\t|\tloss: 2.35853\n",
      "Evaluating Epoch 3  46.5% | batch:        80 of       172\t|\tloss: 2.71768\n",
      "Evaluating Epoch 3  47.1% | batch:        81 of       172\t|\tloss: 1.44122\n",
      "Evaluating Epoch 3  47.7% | batch:        82 of       172\t|\tloss: 2.57054\n",
      "Evaluating Epoch 3  48.3% | batch:        83 of       172\t|\tloss: 3.90325\n",
      "Evaluating Epoch 3  48.8% | batch:        84 of       172\t|\tloss: 4.53492\n",
      "Evaluating Epoch 3  49.4% | batch:        85 of       172\t|\tloss: 5.60067\n",
      "Evaluating Epoch 3  50.0% | batch:        86 of       172\t|\tloss: 7.23432\n",
      "Evaluating Epoch 3  50.6% | batch:        87 of       172\t|\tloss: 3.83091\n",
      "Evaluating Epoch 3  51.2% | batch:        88 of       172\t|\tloss: 3.83069\n",
      "Evaluating Epoch 3  51.7% | batch:        89 of       172\t|\tloss: 9.37053\n",
      "Evaluating Epoch 3  52.3% | batch:        90 of       172\t|\tloss: 5.056\n",
      "Evaluating Epoch 3  52.9% | batch:        91 of       172\t|\tloss: 5.99219\n",
      "Evaluating Epoch 3  53.5% | batch:        92 of       172\t|\tloss: 7.53166\n",
      "Evaluating Epoch 3  54.1% | batch:        93 of       172\t|\tloss: 6.08516\n",
      "Evaluating Epoch 3  54.7% | batch:        94 of       172\t|\tloss: 8.02645\n",
      "Evaluating Epoch 3  55.2% | batch:        95 of       172\t|\tloss: 3.8353\n",
      "Evaluating Epoch 3  55.8% | batch:        96 of       172\t|\tloss: 8.6068\n",
      "Evaluating Epoch 3  56.4% | batch:        97 of       172\t|\tloss: 5.34714\n",
      "Evaluating Epoch 3  57.0% | batch:        98 of       172\t|\tloss: 5.183\n",
      "Evaluating Epoch 3  57.6% | batch:        99 of       172\t|\tloss: 9.80016\n",
      "Evaluating Epoch 3  58.1% | batch:       100 of       172\t|\tloss: 4.59373\n",
      "Evaluating Epoch 3  58.7% | batch:       101 of       172\t|\tloss: 4.78875\n",
      "Evaluating Epoch 3  59.3% | batch:       102 of       172\t|\tloss: 4.12749\n",
      "Evaluating Epoch 3  59.9% | batch:       103 of       172\t|\tloss: 7.91893\n",
      "Evaluating Epoch 3  60.5% | batch:       104 of       172\t|\tloss: 5.76201\n",
      "Evaluating Epoch 3  61.0% | batch:       105 of       172\t|\tloss: 3.84814\n",
      "Evaluating Epoch 3  61.6% | batch:       106 of       172\t|\tloss: 9.05647\n",
      "Evaluating Epoch 3  62.2% | batch:       107 of       172\t|\tloss: 5.59267\n",
      "Evaluating Epoch 3  62.8% | batch:       108 of       172\t|\tloss: 4.5397\n",
      "Evaluating Epoch 3  63.4% | batch:       109 of       172\t|\tloss: 4.85277\n",
      "Evaluating Epoch 3  64.0% | batch:       110 of       172\t|\tloss: 10.019\n",
      "Evaluating Epoch 3  64.5% | batch:       111 of       172\t|\tloss: 5.93979\n",
      "Evaluating Epoch 3  65.1% | batch:       112 of       172\t|\tloss: 3.25409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 3  65.7% | batch:       113 of       172\t|\tloss: 8.25941\n",
      "Evaluating Epoch 3  66.3% | batch:       114 of       172\t|\tloss: 7.86384\n",
      "Evaluating Epoch 3  66.9% | batch:       115 of       172\t|\tloss: 5.688\n",
      "Evaluating Epoch 3  67.4% | batch:       116 of       172\t|\tloss: 8.29514\n",
      "Evaluating Epoch 3  68.0% | batch:       117 of       172\t|\tloss: 6.91994\n",
      "Evaluating Epoch 3  68.6% | batch:       118 of       172\t|\tloss: 3.23218\n",
      "Evaluating Epoch 3  69.2% | batch:       119 of       172\t|\tloss: 9.40077\n",
      "Evaluating Epoch 3  69.8% | batch:       120 of       172\t|\tloss: 3.66561\n",
      "Evaluating Epoch 3  70.3% | batch:       121 of       172\t|\tloss: 26.9689\n",
      "Evaluating Epoch 3  70.9% | batch:       122 of       172\t|\tloss: 10.9372\n",
      "Evaluating Epoch 3  71.5% | batch:       123 of       172\t|\tloss: 31.059\n",
      "Evaluating Epoch 3  72.1% | batch:       124 of       172\t|\tloss: 103.768\n",
      "Evaluating Epoch 3  72.7% | batch:       125 of       172\t|\tloss: 11.422\n",
      "Evaluating Epoch 3  73.3% | batch:       126 of       172\t|\tloss: 4.77392\n",
      "Evaluating Epoch 3  73.8% | batch:       127 of       172\t|\tloss: 5.10833\n",
      "Evaluating Epoch 3  74.4% | batch:       128 of       172\t|\tloss: 8.83484\n",
      "Evaluating Epoch 3  75.0% | batch:       129 of       172\t|\tloss: 4.85652\n",
      "Evaluating Epoch 3  75.6% | batch:       130 of       172\t|\tloss: 2.0726\n",
      "Evaluating Epoch 3  76.2% | batch:       131 of       172\t|\tloss: 11.2124\n",
      "Evaluating Epoch 3  76.7% | batch:       132 of       172\t|\tloss: 2.50558\n",
      "Evaluating Epoch 3  77.3% | batch:       133 of       172\t|\tloss: 4.47686\n",
      "Evaluating Epoch 3  77.9% | batch:       134 of       172\t|\tloss: 1.62757\n",
      "Evaluating Epoch 3  78.5% | batch:       135 of       172\t|\tloss: 1.9093\n",
      "Evaluating Epoch 3  79.1% | batch:       136 of       172\t|\tloss: 3.18006\n",
      "Evaluating Epoch 3  79.7% | batch:       137 of       172\t|\tloss: 1.95021\n",
      "Evaluating Epoch 3  80.2% | batch:       138 of       172\t|\tloss: 4.75219\n",
      "Evaluating Epoch 3  80.8% | batch:       139 of       172\t|\tloss: 5.03642\n",
      "Evaluating Epoch 3  81.4% | batch:       140 of       172\t|\tloss: 2.39833\n",
      "Evaluating Epoch 3  82.0% | batch:       141 of       172\t|\tloss: 2.00115\n",
      "Evaluating Epoch 3  82.6% | batch:       142 of       172\t|\tloss: 1.44703\n",
      "Evaluating Epoch 3  83.1% | batch:       143 of       172\t|\tloss: 3.18375\n",
      "Evaluating Epoch 3  83.7% | batch:       144 of       172\t|\tloss: 1.76403\n",
      "Evaluating Epoch 3  84.3% | batch:       145 of       172\t|\tloss: 2.00384\n",
      "Evaluating Epoch 3  84.9% | batch:       146 of       172\t|\tloss: 3.33197\n",
      "Evaluating Epoch 3  85.5% | batch:       147 of       172\t|\tloss: 2.81353\n",
      "Evaluating Epoch 3  86.0% | batch:       148 of       172\t|\tloss: 1.61609\n",
      "Evaluating Epoch 3  86.6% | batch:       149 of       172\t|\tloss: 2.96109\n",
      "Evaluating Epoch 3  87.2% | batch:       150 of       172\t|\tloss: 3.01405\n",
      "Evaluating Epoch 3  87.8% | batch:       151 of       172\t|\tloss: 3.15249\n",
      "Evaluating Epoch 3  88.4% | batch:       152 of       172\t|\tloss: 4.798\n",
      "Evaluating Epoch 3  89.0% | batch:       153 of       172\t|\tloss: 2.80207\n",
      "Evaluating Epoch 3  89.5% | batch:       154 of       172\t|\tloss: 4.94858\n",
      "Evaluating Epoch 3  90.1% | batch:       155 of       172\t|\tloss: 6.22496\n",
      "Evaluating Epoch 3  90.7% | batch:       156 of       172\t|\tloss: 3.29076\n",
      "Evaluating Epoch 3  91.3% | batch:       157 of       172\t|\tloss: 5.58507\n",
      "Evaluating Epoch 3  91.9% | batch:       158 of       172\t|\tloss: 3.4615\n",
      "Evaluating Epoch 3  92.4% | batch:       159 of       172\t|\tloss: 4.78796\n",
      "Evaluating Epoch 3  93.0% | batch:       160 of       172\t|\tloss: 36.6989\n",
      "Evaluating Epoch 3  93.6% | batch:       161 of       172\t|\tloss: 18.9002\n",
      "Evaluating Epoch 3  94.2% | batch:       162 of       172\t|\tloss: 3.85289\n",
      "Evaluating Epoch 3  94.8% | batch:       163 of       172\t|\tloss: 5.58358\n",
      "Evaluating Epoch 3  95.3% | batch:       164 of       172\t|\tloss: 6.80377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:01:15,048 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.684636116027832 seconds\n",
      "\n",
      "2023-05-25 22:01:15,049 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.289847195148468 seconds\n",
      "2023-05-25 22:01:15,050 | INFO : Avg batch val. time: 0.024940972064816675 seconds\n",
      "2023-05-25 22:01:15,050 | INFO : Avg sample val. time: 0.0001953749234935769 seconds\n",
      "2023-05-25 22:01:15,051 | INFO : Epoch 3 Validation Summary: epoch: 3.000000 | loss: 6.674660 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 3  95.9% | batch:       165 of       172\t|\tloss: 4.01386\n",
      "Evaluating Epoch 3  96.5% | batch:       166 of       172\t|\tloss: 3.01507\n",
      "Evaluating Epoch 3  97.1% | batch:       167 of       172\t|\tloss: 2.95464\n",
      "Evaluating Epoch 3  97.7% | batch:       168 of       172\t|\tloss: 4.22635\n",
      "Evaluating Epoch 3  98.3% | batch:       169 of       172\t|\tloss: 2.11844\n",
      "Evaluating Epoch 3  98.8% | batch:       170 of       172\t|\tloss: 3.94225\n",
      "Evaluating Epoch 3  99.4% | batch:       171 of       172\t|\tloss: 7.51157\n",
      "\n",
      "Training Epoch 4   0.0% | batch:         0 of       686\t|\tloss: 42.0378\n",
      "Training Epoch 4   0.1% | batch:         1 of       686\t|\tloss: 32.4697\n",
      "Training Epoch 4   0.3% | batch:         2 of       686\t|\tloss: 73.1622\n",
      "Training Epoch 4   0.4% | batch:         3 of       686\t|\tloss: 28.0757\n",
      "Training Epoch 4   0.6% | batch:         4 of       686\t|\tloss: 30.027\n",
      "Training Epoch 4   0.7% | batch:         5 of       686\t|\tloss: 32.747\n",
      "Training Epoch 4   0.9% | batch:         6 of       686\t|\tloss: 27.0478\n",
      "Training Epoch 4   1.0% | batch:         7 of       686\t|\tloss: 25.5438\n",
      "Training Epoch 4   1.2% | batch:         8 of       686\t|\tloss: 25.9688\n",
      "Training Epoch 4   1.3% | batch:         9 of       686\t|\tloss: 31.591\n",
      "Training Epoch 4   1.5% | batch:        10 of       686\t|\tloss: 23.103\n",
      "Training Epoch 4   1.6% | batch:        11 of       686\t|\tloss: 30.3481\n",
      "Training Epoch 4   1.7% | batch:        12 of       686\t|\tloss: 26.4513\n",
      "Training Epoch 4   1.9% | batch:        13 of       686\t|\tloss: 31.3548\n",
      "Training Epoch 4   2.0% | batch:        14 of       686\t|\tloss: 23.7434\n",
      "Training Epoch 4   2.2% | batch:        15 of       686\t|\tloss: 27.4631\n",
      "Training Epoch 4   2.3% | batch:        16 of       686\t|\tloss: 35.5567\n",
      "Training Epoch 4   2.5% | batch:        17 of       686\t|\tloss: 39.9915\n",
      "Training Epoch 4   2.6% | batch:        18 of       686\t|\tloss: 25.6749\n",
      "Training Epoch 4   2.8% | batch:        19 of       686\t|\tloss: 30.4843\n",
      "Training Epoch 4   2.9% | batch:        20 of       686\t|\tloss: 34.0316\n",
      "Training Epoch 4   3.1% | batch:        21 of       686\t|\tloss: 38.3289\n",
      "Training Epoch 4   3.2% | batch:        22 of       686\t|\tloss: 36.2569\n",
      "Training Epoch 4   3.4% | batch:        23 of       686\t|\tloss: 34.8963\n",
      "Training Epoch 4   3.5% | batch:        24 of       686\t|\tloss: 29.5973\n",
      "Training Epoch 4   3.6% | batch:        25 of       686\t|\tloss: 36.0672\n",
      "Training Epoch 4   3.8% | batch:        26 of       686\t|\tloss: 29.75\n",
      "Training Epoch 4   3.9% | batch:        27 of       686\t|\tloss: 36.3776\n",
      "Training Epoch 4   4.1% | batch:        28 of       686\t|\tloss: 29.8166\n",
      "Training Epoch 4   4.2% | batch:        29 of       686\t|\tloss: 33.9533\n",
      "Training Epoch 4   4.4% | batch:        30 of       686\t|\tloss: 37.5067\n",
      "Training Epoch 4   4.5% | batch:        31 of       686\t|\tloss: 29.4695\n",
      "Training Epoch 4   4.7% | batch:        32 of       686\t|\tloss: 29.3501\n",
      "Training Epoch 4   4.8% | batch:        33 of       686\t|\tloss: 27.7381\n",
      "Training Epoch 4   5.0% | batch:        34 of       686\t|\tloss: 31.8844\n",
      "Training Epoch 4   5.1% | batch:        35 of       686\t|\tloss: 28.9663\n",
      "Training Epoch 4   5.2% | batch:        36 of       686\t|\tloss: 32.0087\n",
      "Training Epoch 4   5.4% | batch:        37 of       686\t|\tloss: 25.1699\n",
      "Training Epoch 4   5.5% | batch:        38 of       686\t|\tloss: 22.2601\n",
      "Training Epoch 4   5.7% | batch:        39 of       686\t|\tloss: 26.7214\n",
      "Training Epoch 4   5.8% | batch:        40 of       686\t|\tloss: 41.2826\n",
      "Training Epoch 4   6.0% | batch:        41 of       686\t|\tloss: 32.2853\n",
      "Training Epoch 4   6.1% | batch:        42 of       686\t|\tloss: 21.3746\n",
      "Training Epoch 4   6.3% | batch:        43 of       686\t|\tloss: 23.7511\n",
      "Training Epoch 4   6.4% | batch:        44 of       686\t|\tloss: 38.8158\n",
      "Training Epoch 4   6.6% | batch:        45 of       686\t|\tloss: 21.556\n",
      "Training Epoch 4   6.7% | batch:        46 of       686\t|\tloss: 27.7254\n",
      "Training Epoch 4   6.9% | batch:        47 of       686\t|\tloss: 26.1317\n",
      "Training Epoch 4   7.0% | batch:        48 of       686\t|\tloss: 28.4077\n",
      "Training Epoch 4   7.1% | batch:        49 of       686\t|\tloss: 39.3184\n",
      "Training Epoch 4   7.3% | batch:        50 of       686\t|\tloss: 25.8381\n",
      "Training Epoch 4   7.4% | batch:        51 of       686\t|\tloss: 34.3293\n",
      "Training Epoch 4   7.6% | batch:        52 of       686\t|\tloss: 30.8443\n",
      "Training Epoch 4   7.7% | batch:        53 of       686\t|\tloss: 25.3096\n",
      "Training Epoch 4   7.9% | batch:        54 of       686\t|\tloss: 37.1634\n",
      "Training Epoch 4   8.0% | batch:        55 of       686\t|\tloss: 30.3896\n",
      "Training Epoch 4   8.2% | batch:        56 of       686\t|\tloss: 38.966\n",
      "Training Epoch 4   8.3% | batch:        57 of       686\t|\tloss: 25.5059\n",
      "Training Epoch 4   8.5% | batch:        58 of       686\t|\tloss: 30.2026\n",
      "Training Epoch 4   8.6% | batch:        59 of       686\t|\tloss: 39.9756\n",
      "Training Epoch 4   8.7% | batch:        60 of       686\t|\tloss: 32.2848\n",
      "Training Epoch 4   8.9% | batch:        61 of       686\t|\tloss: 30.2098\n",
      "Training Epoch 4   9.0% | batch:        62 of       686\t|\tloss: 29.0901\n",
      "Training Epoch 4   9.2% | batch:        63 of       686\t|\tloss: 35.7974\n",
      "Training Epoch 4   9.3% | batch:        64 of       686\t|\tloss: 32.0864\n",
      "Training Epoch 4   9.5% | batch:        65 of       686\t|\tloss: 34.1608\n",
      "Training Epoch 4   9.6% | batch:        66 of       686\t|\tloss: 30.0144\n",
      "Training Epoch 4   9.8% | batch:        67 of       686\t|\tloss: 33.9343\n",
      "Training Epoch 4   9.9% | batch:        68 of       686\t|\tloss: 35.699\n",
      "Training Epoch 4  10.1% | batch:        69 of       686\t|\tloss: 25.6122\n",
      "Training Epoch 4  10.2% | batch:        70 of       686\t|\tloss: 28.1278\n",
      "Training Epoch 4  10.3% | batch:        71 of       686\t|\tloss: 34.7833\n",
      "Training Epoch 4  10.5% | batch:        72 of       686\t|\tloss: 38.3231\n",
      "Training Epoch 4  10.6% | batch:        73 of       686\t|\tloss: 25.8202\n",
      "Training Epoch 4  10.8% | batch:        74 of       686\t|\tloss: 42.3133\n",
      "Training Epoch 4  10.9% | batch:        75 of       686\t|\tloss: 35.53\n",
      "Training Epoch 4  11.1% | batch:        76 of       686\t|\tloss: 26.8488\n",
      "Training Epoch 4  11.2% | batch:        77 of       686\t|\tloss: 28.2989\n",
      "Training Epoch 4  11.4% | batch:        78 of       686\t|\tloss: 24.3467\n",
      "Training Epoch 4  11.5% | batch:        79 of       686\t|\tloss: 25.5057\n",
      "Training Epoch 4  11.7% | batch:        80 of       686\t|\tloss: 31.3314\n",
      "Training Epoch 4  11.8% | batch:        81 of       686\t|\tloss: 29.427\n",
      "Training Epoch 4  12.0% | batch:        82 of       686\t|\tloss: 34.8323\n",
      "Training Epoch 4  12.1% | batch:        83 of       686\t|\tloss: 39.6918\n",
      "Training Epoch 4  12.2% | batch:        84 of       686\t|\tloss: 30.4663\n",
      "Training Epoch 4  12.4% | batch:        85 of       686\t|\tloss: 26.0819\n",
      "Training Epoch 4  12.5% | batch:        86 of       686\t|\tloss: 24.939\n",
      "Training Epoch 4  12.7% | batch:        87 of       686\t|\tloss: 25.5301\n",
      "Training Epoch 4  12.8% | batch:        88 of       686\t|\tloss: 35.8885\n",
      "Training Epoch 4  13.0% | batch:        89 of       686\t|\tloss: 25.3186\n",
      "Training Epoch 4  13.1% | batch:        90 of       686\t|\tloss: 26.104\n",
      "Training Epoch 4  13.3% | batch:        91 of       686\t|\tloss: 30.0239\n",
      "Training Epoch 4  13.4% | batch:        92 of       686\t|\tloss: 25.2164\n",
      "Training Epoch 4  13.6% | batch:        93 of       686\t|\tloss: 26.2254\n",
      "Training Epoch 4  13.7% | batch:        94 of       686\t|\tloss: 35.7646\n",
      "Training Epoch 4  13.8% | batch:        95 of       686\t|\tloss: 26.8173\n",
      "Training Epoch 4  14.0% | batch:        96 of       686\t|\tloss: 32.747\n",
      "Training Epoch 4  14.1% | batch:        97 of       686\t|\tloss: 35.9528\n",
      "Training Epoch 4  14.3% | batch:        98 of       686\t|\tloss: 32.4008\n",
      "Training Epoch 4  14.4% | batch:        99 of       686\t|\tloss: 31.2548\n",
      "Training Epoch 4  14.6% | batch:       100 of       686\t|\tloss: 30.2117\n",
      "Training Epoch 4  14.7% | batch:       101 of       686\t|\tloss: 28.5613\n",
      "Training Epoch 4  14.9% | batch:       102 of       686\t|\tloss: 40.1352\n",
      "Training Epoch 4  15.0% | batch:       103 of       686\t|\tloss: 37.8881\n",
      "Training Epoch 4  15.2% | batch:       104 of       686\t|\tloss: 27.4013\n",
      "Training Epoch 4  15.3% | batch:       105 of       686\t|\tloss: 31.5003\n",
      "Training Epoch 4  15.5% | batch:       106 of       686\t|\tloss: 30.7069\n",
      "Training Epoch 4  15.6% | batch:       107 of       686\t|\tloss: 32.8863\n",
      "Training Epoch 4  15.7% | batch:       108 of       686\t|\tloss: 39.7004\n",
      "Training Epoch 4  15.9% | batch:       109 of       686\t|\tloss: 32.8721\n",
      "Training Epoch 4  16.0% | batch:       110 of       686\t|\tloss: 37.4952\n",
      "Training Epoch 4  16.2% | batch:       111 of       686\t|\tloss: 33.2901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  16.3% | batch:       112 of       686\t|\tloss: 26.9829\n",
      "Training Epoch 4  16.5% | batch:       113 of       686\t|\tloss: 24.2659\n",
      "Training Epoch 4  16.6% | batch:       114 of       686\t|\tloss: 34.5213\n",
      "Training Epoch 4  16.8% | batch:       115 of       686\t|\tloss: 24.5847\n",
      "Training Epoch 4  16.9% | batch:       116 of       686\t|\tloss: 28.6577\n",
      "Training Epoch 4  17.1% | batch:       117 of       686\t|\tloss: 36.2388\n",
      "Training Epoch 4  17.2% | batch:       118 of       686\t|\tloss: 26.8415\n",
      "Training Epoch 4  17.3% | batch:       119 of       686\t|\tloss: 32.8975\n",
      "Training Epoch 4  17.5% | batch:       120 of       686\t|\tloss: 26.4969\n",
      "Training Epoch 4  17.6% | batch:       121 of       686\t|\tloss: 41.2528\n",
      "Training Epoch 4  17.8% | batch:       122 of       686\t|\tloss: 30.458\n",
      "Training Epoch 4  17.9% | batch:       123 of       686\t|\tloss: 24.4834\n",
      "Training Epoch 4  18.1% | batch:       124 of       686\t|\tloss: 32.7112\n",
      "Training Epoch 4  18.2% | batch:       125 of       686\t|\tloss: 32.159\n",
      "Training Epoch 4  18.4% | batch:       126 of       686\t|\tloss: 25.5823\n",
      "Training Epoch 4  18.5% | batch:       127 of       686\t|\tloss: 32.6831\n",
      "Training Epoch 4  18.7% | batch:       128 of       686\t|\tloss: 27.723\n",
      "Training Epoch 4  18.8% | batch:       129 of       686\t|\tloss: 22.3918\n",
      "Training Epoch 4  19.0% | batch:       130 of       686\t|\tloss: 29.4689\n",
      "Training Epoch 4  19.1% | batch:       131 of       686\t|\tloss: 23.0517\n",
      "Training Epoch 4  19.2% | batch:       132 of       686\t|\tloss: 25.8387\n",
      "Training Epoch 4  19.4% | batch:       133 of       686\t|\tloss: 23.7317\n",
      "Training Epoch 4  19.5% | batch:       134 of       686\t|\tloss: 26.6917\n",
      "Training Epoch 4  19.7% | batch:       135 of       686\t|\tloss: 32.9443\n",
      "Training Epoch 4  19.8% | batch:       136 of       686\t|\tloss: 31.5259\n",
      "Training Epoch 4  20.0% | batch:       137 of       686\t|\tloss: 23.8034\n",
      "Training Epoch 4  20.1% | batch:       138 of       686\t|\tloss: 33.408\n",
      "Training Epoch 4  20.3% | batch:       139 of       686\t|\tloss: 35.549\n",
      "Training Epoch 4  20.4% | batch:       140 of       686\t|\tloss: 26.6215\n",
      "Training Epoch 4  20.6% | batch:       141 of       686\t|\tloss: 28.9718\n",
      "Training Epoch 4  20.7% | batch:       142 of       686\t|\tloss: 31.4087\n",
      "Training Epoch 4  20.8% | batch:       143 of       686\t|\tloss: 30.2267\n",
      "Training Epoch 4  21.0% | batch:       144 of       686\t|\tloss: 33.8091\n",
      "Training Epoch 4  21.1% | batch:       145 of       686\t|\tloss: 18.9968\n",
      "Training Epoch 4  21.3% | batch:       146 of       686\t|\tloss: 27.1084\n",
      "Training Epoch 4  21.4% | batch:       147 of       686\t|\tloss: 22.3231\n",
      "Training Epoch 4  21.6% | batch:       148 of       686\t|\tloss: 24.8906\n",
      "Training Epoch 4  21.7% | batch:       149 of       686\t|\tloss: 31.2362\n",
      "Training Epoch 4  21.9% | batch:       150 of       686\t|\tloss: 30.4186\n",
      "Training Epoch 4  22.0% | batch:       151 of       686\t|\tloss: 33.9207\n",
      "Training Epoch 4  22.2% | batch:       152 of       686\t|\tloss: 35.4716\n",
      "Training Epoch 4  22.3% | batch:       153 of       686\t|\tloss: 72.719\n",
      "Training Epoch 4  22.4% | batch:       154 of       686\t|\tloss: 24.4961\n",
      "Training Epoch 4  22.6% | batch:       155 of       686\t|\tloss: 29.2059\n",
      "Training Epoch 4  22.7% | batch:       156 of       686\t|\tloss: 36.7426\n",
      "Training Epoch 4  22.9% | batch:       157 of       686\t|\tloss: 34.7463\n",
      "Training Epoch 4  23.0% | batch:       158 of       686\t|\tloss: 32.3043\n",
      "Training Epoch 4  23.2% | batch:       159 of       686\t|\tloss: 31.6797\n",
      "Training Epoch 4  23.3% | batch:       160 of       686\t|\tloss: 41.179\n",
      "Training Epoch 4  23.5% | batch:       161 of       686\t|\tloss: 27.8095\n",
      "Training Epoch 4  23.6% | batch:       162 of       686\t|\tloss: 29.5975\n",
      "Training Epoch 4  23.8% | batch:       163 of       686\t|\tloss: 23.4562\n",
      "Training Epoch 4  23.9% | batch:       164 of       686\t|\tloss: 32.3909\n",
      "Training Epoch 4  24.1% | batch:       165 of       686\t|\tloss: 24.9159\n",
      "Training Epoch 4  24.2% | batch:       166 of       686\t|\tloss: 29.672\n",
      "Training Epoch 4  24.3% | batch:       167 of       686\t|\tloss: 40.6187\n",
      "Training Epoch 4  24.5% | batch:       168 of       686\t|\tloss: 29.8846\n",
      "Training Epoch 4  24.6% | batch:       169 of       686\t|\tloss: 33.7388\n",
      "Training Epoch 4  24.8% | batch:       170 of       686\t|\tloss: 26.575\n",
      "Training Epoch 4  24.9% | batch:       171 of       686\t|\tloss: 40.4028\n",
      "Training Epoch 4  25.1% | batch:       172 of       686\t|\tloss: 35.6693\n",
      "Training Epoch 4  25.2% | batch:       173 of       686\t|\tloss: 33.9697\n",
      "Training Epoch 4  25.4% | batch:       174 of       686\t|\tloss: 26.1642\n",
      "Training Epoch 4  25.5% | batch:       175 of       686\t|\tloss: 29.531\n",
      "Training Epoch 4  25.7% | batch:       176 of       686\t|\tloss: 26.6718\n",
      "Training Epoch 4  25.8% | batch:       177 of       686\t|\tloss: 25.047\n",
      "Training Epoch 4  25.9% | batch:       178 of       686\t|\tloss: 39.1181\n",
      "Training Epoch 4  26.1% | batch:       179 of       686\t|\tloss: 23.2404\n",
      "Training Epoch 4  26.2% | batch:       180 of       686\t|\tloss: 29.8502\n",
      "Training Epoch 4  26.4% | batch:       181 of       686\t|\tloss: 23.1989\n",
      "Training Epoch 4  26.5% | batch:       182 of       686\t|\tloss: 30.8921\n",
      "Training Epoch 4  26.7% | batch:       183 of       686\t|\tloss: 27.455\n",
      "Training Epoch 4  26.8% | batch:       184 of       686\t|\tloss: 24.3272\n",
      "Training Epoch 4  27.0% | batch:       185 of       686\t|\tloss: 26.6343\n",
      "Training Epoch 4  27.1% | batch:       186 of       686\t|\tloss: 27.5428\n",
      "Training Epoch 4  27.3% | batch:       187 of       686\t|\tloss: 22.2864\n",
      "Training Epoch 4  27.4% | batch:       188 of       686\t|\tloss: 22.418\n",
      "Training Epoch 4  27.6% | batch:       189 of       686\t|\tloss: 23.1398\n",
      "Training Epoch 4  27.7% | batch:       190 of       686\t|\tloss: 31.3111\n",
      "Training Epoch 4  27.8% | batch:       191 of       686\t|\tloss: 34.44\n",
      "Training Epoch 4  28.0% | batch:       192 of       686\t|\tloss: 27.7322\n",
      "Training Epoch 4  28.1% | batch:       193 of       686\t|\tloss: 36.0934\n",
      "Training Epoch 4  28.3% | batch:       194 of       686\t|\tloss: 25.2674\n",
      "Training Epoch 4  28.4% | batch:       195 of       686\t|\tloss: 43.2902\n",
      "Training Epoch 4  28.6% | batch:       196 of       686\t|\tloss: 43.946\n",
      "Training Epoch 4  28.7% | batch:       197 of       686\t|\tloss: 31.9263\n",
      "Training Epoch 4  28.9% | batch:       198 of       686\t|\tloss: 35.5256\n",
      "Training Epoch 4  29.0% | batch:       199 of       686\t|\tloss: 24.9074\n",
      "Training Epoch 4  29.2% | batch:       200 of       686\t|\tloss: 33.7249\n",
      "Training Epoch 4  29.3% | batch:       201 of       686\t|\tloss: 25.1675\n",
      "Training Epoch 4  29.4% | batch:       202 of       686\t|\tloss: 19.4413\n",
      "Training Epoch 4  29.6% | batch:       203 of       686\t|\tloss: 30.4602\n",
      "Training Epoch 4  29.7% | batch:       204 of       686\t|\tloss: 29.5942\n",
      "Training Epoch 4  29.9% | batch:       205 of       686\t|\tloss: 29.9454\n",
      "Training Epoch 4  30.0% | batch:       206 of       686\t|\tloss: 38.9714\n",
      "Training Epoch 4  30.2% | batch:       207 of       686\t|\tloss: 25.1045\n",
      "Training Epoch 4  30.3% | batch:       208 of       686\t|\tloss: 23.7496\n",
      "Training Epoch 4  30.5% | batch:       209 of       686\t|\tloss: 24.6223\n",
      "Training Epoch 4  30.6% | batch:       210 of       686\t|\tloss: 31.1169\n",
      "Training Epoch 4  30.8% | batch:       211 of       686\t|\tloss: 27.5524\n",
      "Training Epoch 4  30.9% | batch:       212 of       686\t|\tloss: 41.2836\n",
      "Training Epoch 4  31.0% | batch:       213 of       686\t|\tloss: 26.5812\n",
      "Training Epoch 4  31.2% | batch:       214 of       686\t|\tloss: 27.7426\n",
      "Training Epoch 4  31.3% | batch:       215 of       686\t|\tloss: 29.0427\n",
      "Training Epoch 4  31.5% | batch:       216 of       686\t|\tloss: 23.8623\n",
      "Training Epoch 4  31.6% | batch:       217 of       686\t|\tloss: 23.8911\n",
      "Training Epoch 4  31.8% | batch:       218 of       686\t|\tloss: 53.8464\n",
      "Training Epoch 4  31.9% | batch:       219 of       686\t|\tloss: 28.6723\n",
      "Training Epoch 4  32.1% | batch:       220 of       686\t|\tloss: 28.6111\n",
      "Training Epoch 4  32.2% | batch:       221 of       686\t|\tloss: 31.8135\n",
      "Training Epoch 4  32.4% | batch:       222 of       686\t|\tloss: 28.7124\n",
      "Training Epoch 4  32.5% | batch:       223 of       686\t|\tloss: 25.44\n",
      "Training Epoch 4  32.7% | batch:       224 of       686\t|\tloss: 25.3136\n",
      "Training Epoch 4  32.8% | batch:       225 of       686\t|\tloss: 24.6695\n",
      "Training Epoch 4  32.9% | batch:       226 of       686\t|\tloss: 28.356\n",
      "Training Epoch 4  33.1% | batch:       227 of       686\t|\tloss: 25.5607\n",
      "Training Epoch 4  33.2% | batch:       228 of       686\t|\tloss: 29.7359\n",
      "Training Epoch 4  33.4% | batch:       229 of       686\t|\tloss: 28.3623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  33.5% | batch:       230 of       686\t|\tloss: 27.5357\n",
      "Training Epoch 4  33.7% | batch:       231 of       686\t|\tloss: 33.0568\n",
      "Training Epoch 4  33.8% | batch:       232 of       686\t|\tloss: 29.9209\n",
      "Training Epoch 4  34.0% | batch:       233 of       686\t|\tloss: 25.1812\n",
      "Training Epoch 4  34.1% | batch:       234 of       686\t|\tloss: 21.4876\n",
      "Training Epoch 4  34.3% | batch:       235 of       686\t|\tloss: 33.8007\n",
      "Training Epoch 4  34.4% | batch:       236 of       686\t|\tloss: 33.5381\n",
      "Training Epoch 4  34.5% | batch:       237 of       686\t|\tloss: 30.1467\n",
      "Training Epoch 4  34.7% | batch:       238 of       686\t|\tloss: 21.208\n",
      "Training Epoch 4  34.8% | batch:       239 of       686\t|\tloss: 25.6349\n",
      "Training Epoch 4  35.0% | batch:       240 of       686\t|\tloss: 32.0328\n",
      "Training Epoch 4  35.1% | batch:       241 of       686\t|\tloss: 21.3259\n",
      "Training Epoch 4  35.3% | batch:       242 of       686\t|\tloss: 36.9551\n",
      "Training Epoch 4  35.4% | batch:       243 of       686\t|\tloss: 35.4366\n",
      "Training Epoch 4  35.6% | batch:       244 of       686\t|\tloss: 37.6319\n",
      "Training Epoch 4  35.7% | batch:       245 of       686\t|\tloss: 26.5419\n",
      "Training Epoch 4  35.9% | batch:       246 of       686\t|\tloss: 20.1448\n",
      "Training Epoch 4  36.0% | batch:       247 of       686\t|\tloss: 39.4334\n",
      "Training Epoch 4  36.2% | batch:       248 of       686\t|\tloss: 36.0699\n",
      "Training Epoch 4  36.3% | batch:       249 of       686\t|\tloss: 28.1765\n",
      "Training Epoch 4  36.4% | batch:       250 of       686\t|\tloss: 24.8273\n",
      "Training Epoch 4  36.6% | batch:       251 of       686\t|\tloss: 35.5387\n",
      "Training Epoch 4  36.7% | batch:       252 of       686\t|\tloss: 35.977\n",
      "Training Epoch 4  36.9% | batch:       253 of       686\t|\tloss: 33.7016\n",
      "Training Epoch 4  37.0% | batch:       254 of       686\t|\tloss: 28.0717\n",
      "Training Epoch 4  37.2% | batch:       255 of       686\t|\tloss: 32.296\n",
      "Training Epoch 4  37.3% | batch:       256 of       686\t|\tloss: 29.9773\n",
      "Training Epoch 4  37.5% | batch:       257 of       686\t|\tloss: 28.191\n",
      "Training Epoch 4  37.6% | batch:       258 of       686\t|\tloss: 40.1762\n",
      "Training Epoch 4  37.8% | batch:       259 of       686\t|\tloss: 28.1438\n",
      "Training Epoch 4  37.9% | batch:       260 of       686\t|\tloss: 35.2678\n",
      "Training Epoch 4  38.0% | batch:       261 of       686\t|\tloss: 25.1805\n",
      "Training Epoch 4  38.2% | batch:       262 of       686\t|\tloss: 31.723\n",
      "Training Epoch 4  38.3% | batch:       263 of       686\t|\tloss: 23.7672\n",
      "Training Epoch 4  38.5% | batch:       264 of       686\t|\tloss: 28.5569\n",
      "Training Epoch 4  38.6% | batch:       265 of       686\t|\tloss: 25.0555\n",
      "Training Epoch 4  38.8% | batch:       266 of       686\t|\tloss: 25.789\n",
      "Training Epoch 4  38.9% | batch:       267 of       686\t|\tloss: 26.9819\n",
      "Training Epoch 4  39.1% | batch:       268 of       686\t|\tloss: 29.7982\n",
      "Training Epoch 4  39.2% | batch:       269 of       686\t|\tloss: 22.7993\n",
      "Training Epoch 4  39.4% | batch:       270 of       686\t|\tloss: 31.2085\n",
      "Training Epoch 4  39.5% | batch:       271 of       686\t|\tloss: 32.9368\n",
      "Training Epoch 4  39.7% | batch:       272 of       686\t|\tloss: 31.2824\n",
      "Training Epoch 4  39.8% | batch:       273 of       686\t|\tloss: 31.3359\n",
      "Training Epoch 4  39.9% | batch:       274 of       686\t|\tloss: 21.4902\n",
      "Training Epoch 4  40.1% | batch:       275 of       686\t|\tloss: 35.988\n",
      "Training Epoch 4  40.2% | batch:       276 of       686\t|\tloss: 28.143\n",
      "Training Epoch 4  40.4% | batch:       277 of       686\t|\tloss: 29.8812\n",
      "Training Epoch 4  40.5% | batch:       278 of       686\t|\tloss: 76.6159\n",
      "Training Epoch 4  40.7% | batch:       279 of       686\t|\tloss: 35.189\n",
      "Training Epoch 4  40.8% | batch:       280 of       686\t|\tloss: 24.9609\n",
      "Training Epoch 4  41.0% | batch:       281 of       686\t|\tloss: 35.4463\n",
      "Training Epoch 4  41.1% | batch:       282 of       686\t|\tloss: 28.0978\n",
      "Training Epoch 4  41.3% | batch:       283 of       686\t|\tloss: 20.502\n",
      "Training Epoch 4  41.4% | batch:       284 of       686\t|\tloss: 47.1353\n",
      "Training Epoch 4  41.5% | batch:       285 of       686\t|\tloss: 29.5433\n",
      "Training Epoch 4  41.7% | batch:       286 of       686\t|\tloss: 27.8044\n",
      "Training Epoch 4  41.8% | batch:       287 of       686\t|\tloss: 33.4152\n",
      "Training Epoch 4  42.0% | batch:       288 of       686\t|\tloss: 34.3097\n",
      "Training Epoch 4  42.1% | batch:       289 of       686\t|\tloss: 37.1166\n",
      "Training Epoch 4  42.3% | batch:       290 of       686\t|\tloss: 31.1194\n",
      "Training Epoch 4  42.4% | batch:       291 of       686\t|\tloss: 18.0282\n",
      "Training Epoch 4  42.6% | batch:       292 of       686\t|\tloss: 23.8913\n",
      "Training Epoch 4  42.7% | batch:       293 of       686\t|\tloss: 28.6878\n",
      "Training Epoch 4  42.9% | batch:       294 of       686\t|\tloss: 26.3548\n",
      "Training Epoch 4  43.0% | batch:       295 of       686\t|\tloss: 32.6571\n",
      "Training Epoch 4  43.1% | batch:       296 of       686\t|\tloss: 26.8854\n",
      "Training Epoch 4  43.3% | batch:       297 of       686\t|\tloss: 27.3996\n",
      "Training Epoch 4  43.4% | batch:       298 of       686\t|\tloss: 20.6169\n",
      "Training Epoch 4  43.6% | batch:       299 of       686\t|\tloss: 26.3908\n",
      "Training Epoch 4  43.7% | batch:       300 of       686\t|\tloss: 30.8734\n",
      "Training Epoch 4  43.9% | batch:       301 of       686\t|\tloss: 21.7403\n",
      "Training Epoch 4  44.0% | batch:       302 of       686\t|\tloss: 21.3871\n",
      "Training Epoch 4  44.2% | batch:       303 of       686\t|\tloss: 32.6501\n",
      "Training Epoch 4  44.3% | batch:       304 of       686\t|\tloss: 28.7921\n",
      "Training Epoch 4  44.5% | batch:       305 of       686\t|\tloss: 30.2633\n",
      "Training Epoch 4  44.6% | batch:       306 of       686\t|\tloss: 24.2356\n",
      "Training Epoch 4  44.8% | batch:       307 of       686\t|\tloss: 23.5033\n",
      "Training Epoch 4  44.9% | batch:       308 of       686\t|\tloss: 21.9813\n",
      "Training Epoch 4  45.0% | batch:       309 of       686\t|\tloss: 45.1688\n",
      "Training Epoch 4  45.2% | batch:       310 of       686\t|\tloss: 27.4811\n",
      "Training Epoch 4  45.3% | batch:       311 of       686\t|\tloss: 36.5515\n",
      "Training Epoch 4  45.5% | batch:       312 of       686\t|\tloss: 34.129\n",
      "Training Epoch 4  45.6% | batch:       313 of       686\t|\tloss: 33.9457\n",
      "Training Epoch 4  45.8% | batch:       314 of       686\t|\tloss: 32.8809\n",
      "Training Epoch 4  45.9% | batch:       315 of       686\t|\tloss: 30.4352\n",
      "Training Epoch 4  46.1% | batch:       316 of       686\t|\tloss: 37.0651\n",
      "Training Epoch 4  46.2% | batch:       317 of       686\t|\tloss: 24.5678\n",
      "Training Epoch 4  46.4% | batch:       318 of       686\t|\tloss: 24.4437\n",
      "Training Epoch 4  46.5% | batch:       319 of       686\t|\tloss: 33.2239\n",
      "Training Epoch 4  46.6% | batch:       320 of       686\t|\tloss: 38.4748\n",
      "Training Epoch 4  46.8% | batch:       321 of       686\t|\tloss: 40.9114\n",
      "Training Epoch 4  46.9% | batch:       322 of       686\t|\tloss: 23.6153\n",
      "Training Epoch 4  47.1% | batch:       323 of       686\t|\tloss: 35.4859\n",
      "Training Epoch 4  47.2% | batch:       324 of       686\t|\tloss: 28.2205\n",
      "Training Epoch 4  47.4% | batch:       325 of       686\t|\tloss: 32.0335\n",
      "Training Epoch 4  47.5% | batch:       326 of       686\t|\tloss: 37.3409\n",
      "Training Epoch 4  47.7% | batch:       327 of       686\t|\tloss: 28.2949\n",
      "Training Epoch 4  47.8% | batch:       328 of       686\t|\tloss: 24.3193\n",
      "Training Epoch 4  48.0% | batch:       329 of       686\t|\tloss: 36.4386\n",
      "Training Epoch 4  48.1% | batch:       330 of       686\t|\tloss: 30.7743\n",
      "Training Epoch 4  48.3% | batch:       331 of       686\t|\tloss: 27.587\n",
      "Training Epoch 4  48.4% | batch:       332 of       686\t|\tloss: 40.0389\n",
      "Training Epoch 4  48.5% | batch:       333 of       686\t|\tloss: 23.1648\n",
      "Training Epoch 4  48.7% | batch:       334 of       686\t|\tloss: 31.5692\n",
      "Training Epoch 4  48.8% | batch:       335 of       686\t|\tloss: 40.6701\n",
      "Training Epoch 4  49.0% | batch:       336 of       686\t|\tloss: 19.5593\n",
      "Training Epoch 4  49.1% | batch:       337 of       686\t|\tloss: 33.1767\n",
      "Training Epoch 4  49.3% | batch:       338 of       686\t|\tloss: 27.1401\n",
      "Training Epoch 4  49.4% | batch:       339 of       686\t|\tloss: 24.1519\n",
      "Training Epoch 4  49.6% | batch:       340 of       686\t|\tloss: 22.8183\n",
      "Training Epoch 4  49.7% | batch:       341 of       686\t|\tloss: 31.0654\n",
      "Training Epoch 4  49.9% | batch:       342 of       686\t|\tloss: 25.4108\n",
      "Training Epoch 4  50.0% | batch:       343 of       686\t|\tloss: 36.6843\n",
      "Training Epoch 4  50.1% | batch:       344 of       686\t|\tloss: 35.1815\n",
      "Training Epoch 4  50.3% | batch:       345 of       686\t|\tloss: 25.2895\n",
      "Training Epoch 4  50.4% | batch:       346 of       686\t|\tloss: 24.5379\n",
      "Training Epoch 4  50.6% | batch:       347 of       686\t|\tloss: 36.3465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  50.7% | batch:       348 of       686\t|\tloss: 25.8953\n",
      "Training Epoch 4  50.9% | batch:       349 of       686\t|\tloss: 24.166\n",
      "Training Epoch 4  51.0% | batch:       350 of       686\t|\tloss: 30.2375\n",
      "Training Epoch 4  51.2% | batch:       351 of       686\t|\tloss: 25.6287\n",
      "Training Epoch 4  51.3% | batch:       352 of       686\t|\tloss: 23.1292\n",
      "Training Epoch 4  51.5% | batch:       353 of       686\t|\tloss: 31.601\n",
      "Training Epoch 4  51.6% | batch:       354 of       686\t|\tloss: 23.1364\n",
      "Training Epoch 4  51.7% | batch:       355 of       686\t|\tloss: 30.0579\n",
      "Training Epoch 4  51.9% | batch:       356 of       686\t|\tloss: 23.1042\n",
      "Training Epoch 4  52.0% | batch:       357 of       686\t|\tloss: 31.8526\n",
      "Training Epoch 4  52.2% | batch:       358 of       686\t|\tloss: 26.6561\n",
      "Training Epoch 4  52.3% | batch:       359 of       686\t|\tloss: 26.7684\n",
      "Training Epoch 4  52.5% | batch:       360 of       686\t|\tloss: 30.5243\n",
      "Training Epoch 4  52.6% | batch:       361 of       686\t|\tloss: 28.5939\n",
      "Training Epoch 4  52.8% | batch:       362 of       686\t|\tloss: 22.3173\n",
      "Training Epoch 4  52.9% | batch:       363 of       686\t|\tloss: 31.1645\n",
      "Training Epoch 4  53.1% | batch:       364 of       686\t|\tloss: 22.3955\n",
      "Training Epoch 4  53.2% | batch:       365 of       686\t|\tloss: 26.0924\n",
      "Training Epoch 4  53.4% | batch:       366 of       686\t|\tloss: 26.6982\n",
      "Training Epoch 4  53.5% | batch:       367 of       686\t|\tloss: 21.9007\n",
      "Training Epoch 4  53.6% | batch:       368 of       686\t|\tloss: 28.4791\n",
      "Training Epoch 4  53.8% | batch:       369 of       686\t|\tloss: 69.2173\n",
      "Training Epoch 4  53.9% | batch:       370 of       686\t|\tloss: 21.4109\n",
      "Training Epoch 4  54.1% | batch:       371 of       686\t|\tloss: 34.7206\n",
      "Training Epoch 4  54.2% | batch:       372 of       686\t|\tloss: 34.4872\n",
      "Training Epoch 4  54.4% | batch:       373 of       686\t|\tloss: 29.1456\n",
      "Training Epoch 4  54.5% | batch:       374 of       686\t|\tloss: 29.9858\n",
      "Training Epoch 4  54.7% | batch:       375 of       686\t|\tloss: 27.0595\n",
      "Training Epoch 4  54.8% | batch:       376 of       686\t|\tloss: 18.7026\n",
      "Training Epoch 4  55.0% | batch:       377 of       686\t|\tloss: 25.183\n",
      "Training Epoch 4  55.1% | batch:       378 of       686\t|\tloss: 37.3661\n",
      "Training Epoch 4  55.2% | batch:       379 of       686\t|\tloss: 26.4546\n",
      "Training Epoch 4  55.4% | batch:       380 of       686\t|\tloss: 26.883\n",
      "Training Epoch 4  55.5% | batch:       381 of       686\t|\tloss: 31.7326\n",
      "Training Epoch 4  55.7% | batch:       382 of       686\t|\tloss: 26.8765\n",
      "Training Epoch 4  55.8% | batch:       383 of       686\t|\tloss: 34.2571\n",
      "Training Epoch 4  56.0% | batch:       384 of       686\t|\tloss: 28.2676\n",
      "Training Epoch 4  56.1% | batch:       385 of       686\t|\tloss: 28.3728\n",
      "Training Epoch 4  56.3% | batch:       386 of       686\t|\tloss: 24.5547\n",
      "Training Epoch 4  56.4% | batch:       387 of       686\t|\tloss: 26.2526\n",
      "Training Epoch 4  56.6% | batch:       388 of       686\t|\tloss: 29.2372\n",
      "Training Epoch 4  56.7% | batch:       389 of       686\t|\tloss: 28.0355\n",
      "Training Epoch 4  56.9% | batch:       390 of       686\t|\tloss: 29.3374\n",
      "Training Epoch 4  57.0% | batch:       391 of       686\t|\tloss: 31.9493\n",
      "Training Epoch 4  57.1% | batch:       392 of       686\t|\tloss: 27.1485\n",
      "Training Epoch 4  57.3% | batch:       393 of       686\t|\tloss: 26.9447\n",
      "Training Epoch 4  57.4% | batch:       394 of       686\t|\tloss: 25.591\n",
      "Training Epoch 4  57.6% | batch:       395 of       686\t|\tloss: 29.9835\n",
      "Training Epoch 4  57.7% | batch:       396 of       686\t|\tloss: 26.2586\n",
      "Training Epoch 4  57.9% | batch:       397 of       686\t|\tloss: 28.6882\n",
      "Training Epoch 4  58.0% | batch:       398 of       686\t|\tloss: 31.5633\n",
      "Training Epoch 4  58.2% | batch:       399 of       686\t|\tloss: 21.8608\n",
      "Training Epoch 4  58.3% | batch:       400 of       686\t|\tloss: 29.8482\n",
      "Training Epoch 4  58.5% | batch:       401 of       686\t|\tloss: 31.2736\n",
      "Training Epoch 4  58.6% | batch:       402 of       686\t|\tloss: 28.0099\n",
      "Training Epoch 4  58.7% | batch:       403 of       686\t|\tloss: 33.737\n",
      "Training Epoch 4  58.9% | batch:       404 of       686\t|\tloss: 25.5952\n",
      "Training Epoch 4  59.0% | batch:       405 of       686\t|\tloss: 24.6506\n",
      "Training Epoch 4  59.2% | batch:       406 of       686\t|\tloss: 28.404\n",
      "Training Epoch 4  59.3% | batch:       407 of       686\t|\tloss: 40.0522\n",
      "Training Epoch 4  59.5% | batch:       408 of       686\t|\tloss: 25.1731\n",
      "Training Epoch 4  59.6% | batch:       409 of       686\t|\tloss: 22.8709\n",
      "Training Epoch 4  59.8% | batch:       410 of       686\t|\tloss: 21.0747\n",
      "Training Epoch 4  59.9% | batch:       411 of       686\t|\tloss: 36.9239\n",
      "Training Epoch 4  60.1% | batch:       412 of       686\t|\tloss: 23.889\n",
      "Training Epoch 4  60.2% | batch:       413 of       686\t|\tloss: 29.885\n",
      "Training Epoch 4  60.3% | batch:       414 of       686\t|\tloss: 19.9269\n",
      "Training Epoch 4  60.5% | batch:       415 of       686\t|\tloss: 26.6447\n",
      "Training Epoch 4  60.6% | batch:       416 of       686\t|\tloss: 28.7064\n",
      "Training Epoch 4  60.8% | batch:       417 of       686\t|\tloss: 26.3235\n",
      "Training Epoch 4  60.9% | batch:       418 of       686\t|\tloss: 22.1504\n",
      "Training Epoch 4  61.1% | batch:       419 of       686\t|\tloss: 24.8628\n",
      "Training Epoch 4  61.2% | batch:       420 of       686\t|\tloss: 28.4414\n",
      "Training Epoch 4  61.4% | batch:       421 of       686\t|\tloss: 25.7638\n",
      "Training Epoch 4  61.5% | batch:       422 of       686\t|\tloss: 29.3665\n",
      "Training Epoch 4  61.7% | batch:       423 of       686\t|\tloss: 18.8804\n",
      "Training Epoch 4  61.8% | batch:       424 of       686\t|\tloss: 27.2804\n",
      "Training Epoch 4  62.0% | batch:       425 of       686\t|\tloss: 26.0519\n",
      "Training Epoch 4  62.1% | batch:       426 of       686\t|\tloss: 22.4675\n",
      "Training Epoch 4  62.2% | batch:       427 of       686\t|\tloss: 22.5295\n",
      "Training Epoch 4  62.4% | batch:       428 of       686\t|\tloss: 19.0608\n",
      "Training Epoch 4  62.5% | batch:       429 of       686\t|\tloss: 27.719\n",
      "Training Epoch 4  62.7% | batch:       430 of       686\t|\tloss: 20.4449\n",
      "Training Epoch 4  62.8% | batch:       431 of       686\t|\tloss: 24.0595\n",
      "Training Epoch 4  63.0% | batch:       432 of       686\t|\tloss: 20.8271\n",
      "Training Epoch 4  63.1% | batch:       433 of       686\t|\tloss: 28.0832\n",
      "Training Epoch 4  63.3% | batch:       434 of       686\t|\tloss: 37.9355\n",
      "Training Epoch 4  63.4% | batch:       435 of       686\t|\tloss: 30.5052\n",
      "Training Epoch 4  63.6% | batch:       436 of       686\t|\tloss: 25.4868\n",
      "Training Epoch 4  63.7% | batch:       437 of       686\t|\tloss: 28.3301\n",
      "Training Epoch 4  63.8% | batch:       438 of       686\t|\tloss: 32.0191\n",
      "Training Epoch 4  64.0% | batch:       439 of       686\t|\tloss: 17.7435\n",
      "Training Epoch 4  64.1% | batch:       440 of       686\t|\tloss: 39.4455\n",
      "Training Epoch 4  64.3% | batch:       441 of       686\t|\tloss: 31.0632\n",
      "Training Epoch 4  64.4% | batch:       442 of       686\t|\tloss: 23.0956\n",
      "Training Epoch 4  64.6% | batch:       443 of       686\t|\tloss: 22.8366\n",
      "Training Epoch 4  64.7% | batch:       444 of       686\t|\tloss: 36.1335\n",
      "Training Epoch 4  64.9% | batch:       445 of       686\t|\tloss: 26.1238\n",
      "Training Epoch 4  65.0% | batch:       446 of       686\t|\tloss: 33.2234\n",
      "Training Epoch 4  65.2% | batch:       447 of       686\t|\tloss: 27.9835\n",
      "Training Epoch 4  65.3% | batch:       448 of       686\t|\tloss: 23.0085\n",
      "Training Epoch 4  65.5% | batch:       449 of       686\t|\tloss: 25.695\n",
      "Training Epoch 4  65.6% | batch:       450 of       686\t|\tloss: 67.4773\n",
      "Training Epoch 4  65.7% | batch:       451 of       686\t|\tloss: 20.2774\n",
      "Training Epoch 4  65.9% | batch:       452 of       686\t|\tloss: 23.9045\n",
      "Training Epoch 4  66.0% | batch:       453 of       686\t|\tloss: 29.6647\n",
      "Training Epoch 4  66.2% | batch:       454 of       686\t|\tloss: 27.4911\n",
      "Training Epoch 4  66.3% | batch:       455 of       686\t|\tloss: 30.3824\n",
      "Training Epoch 4  66.5% | batch:       456 of       686\t|\tloss: 26.5317\n",
      "Training Epoch 4  66.6% | batch:       457 of       686\t|\tloss: 22.8025\n",
      "Training Epoch 4  66.8% | batch:       458 of       686\t|\tloss: 34.2653\n",
      "Training Epoch 4  66.9% | batch:       459 of       686\t|\tloss: 20.1298\n",
      "Training Epoch 4  67.1% | batch:       460 of       686\t|\tloss: 21.135\n",
      "Training Epoch 4  67.2% | batch:       461 of       686\t|\tloss: 23.2649\n",
      "Training Epoch 4  67.3% | batch:       462 of       686\t|\tloss: 23.6329\n",
      "Training Epoch 4  67.5% | batch:       463 of       686\t|\tloss: 30.0168\n",
      "Training Epoch 4  67.6% | batch:       464 of       686\t|\tloss: 30.1661\n",
      "Training Epoch 4  67.8% | batch:       465 of       686\t|\tloss: 34.1886\n",
      "Training Epoch 4  67.9% | batch:       466 of       686\t|\tloss: 29.5949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  68.1% | batch:       467 of       686\t|\tloss: 30.0016\n",
      "Training Epoch 4  68.2% | batch:       468 of       686\t|\tloss: 27.1479\n",
      "Training Epoch 4  68.4% | batch:       469 of       686\t|\tloss: 29.5156\n",
      "Training Epoch 4  68.5% | batch:       470 of       686\t|\tloss: 28.7509\n",
      "Training Epoch 4  68.7% | batch:       471 of       686\t|\tloss: 23.6936\n",
      "Training Epoch 4  68.8% | batch:       472 of       686\t|\tloss: 39.2211\n",
      "Training Epoch 4  69.0% | batch:       473 of       686\t|\tloss: 22.3835\n",
      "Training Epoch 4  69.1% | batch:       474 of       686\t|\tloss: 26.6272\n",
      "Training Epoch 4  69.2% | batch:       475 of       686\t|\tloss: 27.8164\n",
      "Training Epoch 4  69.4% | batch:       476 of       686\t|\tloss: 27.447\n",
      "Training Epoch 4  69.5% | batch:       477 of       686\t|\tloss: 32.572\n",
      "Training Epoch 4  69.7% | batch:       478 of       686\t|\tloss: 23.2545\n",
      "Training Epoch 4  69.8% | batch:       479 of       686\t|\tloss: 21.4427\n",
      "Training Epoch 4  70.0% | batch:       480 of       686\t|\tloss: 22.7935\n",
      "Training Epoch 4  70.1% | batch:       481 of       686\t|\tloss: 19.7747\n",
      "Training Epoch 4  70.3% | batch:       482 of       686\t|\tloss: 23.4275\n",
      "Training Epoch 4  70.4% | batch:       483 of       686\t|\tloss: 27.9246\n",
      "Training Epoch 4  70.6% | batch:       484 of       686\t|\tloss: 26.4901\n",
      "Training Epoch 4  70.7% | batch:       485 of       686\t|\tloss: 30.8206\n",
      "Training Epoch 4  70.8% | batch:       486 of       686\t|\tloss: 25.5525\n",
      "Training Epoch 4  71.0% | batch:       487 of       686\t|\tloss: 30.8459\n",
      "Training Epoch 4  71.1% | batch:       488 of       686\t|\tloss: 40.8154\n",
      "Training Epoch 4  71.3% | batch:       489 of       686\t|\tloss: 30.2699\n",
      "Training Epoch 4  71.4% | batch:       490 of       686\t|\tloss: 26.3285\n",
      "Training Epoch 4  71.6% | batch:       491 of       686\t|\tloss: 25.8803\n",
      "Training Epoch 4  71.7% | batch:       492 of       686\t|\tloss: 20.6822\n",
      "Training Epoch 4  71.9% | batch:       493 of       686\t|\tloss: 22.4509\n",
      "Training Epoch 4  72.0% | batch:       494 of       686\t|\tloss: 21.79\n",
      "Training Epoch 4  72.2% | batch:       495 of       686\t|\tloss: 42.3856\n",
      "Training Epoch 4  72.3% | batch:       496 of       686\t|\tloss: 27.1355\n",
      "Training Epoch 4  72.4% | batch:       497 of       686\t|\tloss: 29.098\n",
      "Training Epoch 4  72.6% | batch:       498 of       686\t|\tloss: 32.0685\n",
      "Training Epoch 4  72.7% | batch:       499 of       686\t|\tloss: 25.7183\n",
      "Training Epoch 4  72.9% | batch:       500 of       686\t|\tloss: 26.1329\n",
      "Training Epoch 4  73.0% | batch:       501 of       686\t|\tloss: 38.125\n",
      "Training Epoch 4  73.2% | batch:       502 of       686\t|\tloss: 30.5289\n",
      "Training Epoch 4  73.3% | batch:       503 of       686\t|\tloss: 33.3249\n",
      "Training Epoch 4  73.5% | batch:       504 of       686\t|\tloss: 26.34\n",
      "Training Epoch 4  73.6% | batch:       505 of       686\t|\tloss: 26.1708\n",
      "Training Epoch 4  73.8% | batch:       506 of       686\t|\tloss: 34.7543\n",
      "Training Epoch 4  73.9% | batch:       507 of       686\t|\tloss: 32.0018\n",
      "Training Epoch 4  74.1% | batch:       508 of       686\t|\tloss: 32.77\n",
      "Training Epoch 4  74.2% | batch:       509 of       686\t|\tloss: 19.0595\n",
      "Training Epoch 4  74.3% | batch:       510 of       686\t|\tloss: 27.2289\n",
      "Training Epoch 4  74.5% | batch:       511 of       686\t|\tloss: 23.9334\n",
      "Training Epoch 4  74.6% | batch:       512 of       686\t|\tloss: 35.813\n",
      "Training Epoch 4  74.8% | batch:       513 of       686\t|\tloss: 27.1037\n",
      "Training Epoch 4  74.9% | batch:       514 of       686\t|\tloss: 34.7181\n",
      "Training Epoch 4  75.1% | batch:       515 of       686\t|\tloss: 33.0137\n",
      "Training Epoch 4  75.2% | batch:       516 of       686\t|\tloss: 27.6344\n",
      "Training Epoch 4  75.4% | batch:       517 of       686\t|\tloss: 24.2837\n",
      "Training Epoch 4  75.5% | batch:       518 of       686\t|\tloss: 22.532\n",
      "Training Epoch 4  75.7% | batch:       519 of       686\t|\tloss: 22.82\n",
      "Training Epoch 4  75.8% | batch:       520 of       686\t|\tloss: 24.5569\n",
      "Training Epoch 4  75.9% | batch:       521 of       686\t|\tloss: 24.3513\n",
      "Training Epoch 4  76.1% | batch:       522 of       686\t|\tloss: 34.9707\n",
      "Training Epoch 4  76.2% | batch:       523 of       686\t|\tloss: 25.382\n",
      "Training Epoch 4  76.4% | batch:       524 of       686\t|\tloss: 24.1025\n",
      "Training Epoch 4  76.5% | batch:       525 of       686\t|\tloss: 28.642\n",
      "Training Epoch 4  76.7% | batch:       526 of       686\t|\tloss: 20.824\n",
      "Training Epoch 4  76.8% | batch:       527 of       686\t|\tloss: 28.5397\n",
      "Training Epoch 4  77.0% | batch:       528 of       686\t|\tloss: 20.746\n",
      "Training Epoch 4  77.1% | batch:       529 of       686\t|\tloss: 32.4596\n",
      "Training Epoch 4  77.3% | batch:       530 of       686\t|\tloss: 32.0144\n",
      "Training Epoch 4  77.4% | batch:       531 of       686\t|\tloss: 19.7044\n",
      "Training Epoch 4  77.6% | batch:       532 of       686\t|\tloss: 24.8786\n",
      "Training Epoch 4  77.7% | batch:       533 of       686\t|\tloss: 25.49\n",
      "Training Epoch 4  77.8% | batch:       534 of       686\t|\tloss: 32.7393\n",
      "Training Epoch 4  78.0% | batch:       535 of       686\t|\tloss: 24.8016\n",
      "Training Epoch 4  78.1% | batch:       536 of       686\t|\tloss: 35.8961\n",
      "Training Epoch 4  78.3% | batch:       537 of       686\t|\tloss: 28.7933\n",
      "Training Epoch 4  78.4% | batch:       538 of       686\t|\tloss: 31.1809\n",
      "Training Epoch 4  78.6% | batch:       539 of       686\t|\tloss: 29.4432\n",
      "Training Epoch 4  78.7% | batch:       540 of       686\t|\tloss: 29.988\n",
      "Training Epoch 4  78.9% | batch:       541 of       686\t|\tloss: 27.5755\n",
      "Training Epoch 4  79.0% | batch:       542 of       686\t|\tloss: 30.916\n",
      "Training Epoch 4  79.2% | batch:       543 of       686\t|\tloss: 48.7709\n",
      "Training Epoch 4  79.3% | batch:       544 of       686\t|\tloss: 30.0501\n",
      "Training Epoch 4  79.4% | batch:       545 of       686\t|\tloss: 19.4648\n",
      "Training Epoch 4  79.6% | batch:       546 of       686\t|\tloss: 26.9601\n",
      "Training Epoch 4  79.7% | batch:       547 of       686\t|\tloss: 30.6989\n",
      "Training Epoch 4  79.9% | batch:       548 of       686\t|\tloss: 29.8264\n",
      "Training Epoch 4  80.0% | batch:       549 of       686\t|\tloss: 25.0752\n",
      "Training Epoch 4  80.2% | batch:       550 of       686\t|\tloss: 24.5209\n",
      "Training Epoch 4  80.3% | batch:       551 of       686\t|\tloss: 23.3704\n",
      "Training Epoch 4  80.5% | batch:       552 of       686\t|\tloss: 31.8515\n",
      "Training Epoch 4  80.6% | batch:       553 of       686\t|\tloss: 27.7998\n",
      "Training Epoch 4  80.8% | batch:       554 of       686\t|\tloss: 27.8346\n",
      "Training Epoch 4  80.9% | batch:       555 of       686\t|\tloss: 34.9298\n",
      "Training Epoch 4  81.0% | batch:       556 of       686\t|\tloss: 31.6683\n",
      "Training Epoch 4  81.2% | batch:       557 of       686\t|\tloss: 31.5614\n",
      "Training Epoch 4  81.3% | batch:       558 of       686\t|\tloss: 29.3367\n",
      "Training Epoch 4  81.5% | batch:       559 of       686\t|\tloss: 20.7557\n",
      "Training Epoch 4  81.6% | batch:       560 of       686\t|\tloss: 29.9158\n",
      "Training Epoch 4  81.8% | batch:       561 of       686\t|\tloss: 29.1715\n",
      "Training Epoch 4  81.9% | batch:       562 of       686\t|\tloss: 39.2173\n",
      "Training Epoch 4  82.1% | batch:       563 of       686\t|\tloss: 27.009\n",
      "Training Epoch 4  82.2% | batch:       564 of       686\t|\tloss: 37.3844\n",
      "Training Epoch 4  82.4% | batch:       565 of       686\t|\tloss: 33.7585\n",
      "Training Epoch 4  82.5% | batch:       566 of       686\t|\tloss: 32.6397\n",
      "Training Epoch 4  82.7% | batch:       567 of       686\t|\tloss: 29.5959\n",
      "Training Epoch 4  82.8% | batch:       568 of       686\t|\tloss: 19.0907\n",
      "Training Epoch 4  82.9% | batch:       569 of       686\t|\tloss: 21.3874\n",
      "Training Epoch 4  83.1% | batch:       570 of       686\t|\tloss: 24.7928\n",
      "Training Epoch 4  83.2% | batch:       571 of       686\t|\tloss: 26.2435\n",
      "Training Epoch 4  83.4% | batch:       572 of       686\t|\tloss: 25.1196\n",
      "Training Epoch 4  83.5% | batch:       573 of       686\t|\tloss: 22.6586\n",
      "Training Epoch 4  83.7% | batch:       574 of       686\t|\tloss: 30.0911\n",
      "Training Epoch 4  83.8% | batch:       575 of       686\t|\tloss: 28.1655\n",
      "Training Epoch 4  84.0% | batch:       576 of       686\t|\tloss: 27.0873\n",
      "Training Epoch 4  84.1% | batch:       577 of       686\t|\tloss: 22.7965\n",
      "Training Epoch 4  84.3% | batch:       578 of       686\t|\tloss: 23.9951\n",
      "Training Epoch 4  84.4% | batch:       579 of       686\t|\tloss: 19.9287\n",
      "Training Epoch 4  84.5% | batch:       580 of       686\t|\tloss: 46.3183\n",
      "Training Epoch 4  84.7% | batch:       581 of       686\t|\tloss: 33.5433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  84.8% | batch:       582 of       686\t|\tloss: 21.2512\n",
      "Training Epoch 4  85.0% | batch:       583 of       686\t|\tloss: 26.3031\n",
      "Training Epoch 4  85.1% | batch:       584 of       686\t|\tloss: 25.9822\n",
      "Training Epoch 4  85.3% | batch:       585 of       686\t|\tloss: 27.1075\n",
      "Training Epoch 4  85.4% | batch:       586 of       686\t|\tloss: 26.1689\n",
      "Training Epoch 4  85.6% | batch:       587 of       686\t|\tloss: 20.2083\n",
      "Training Epoch 4  85.7% | batch:       588 of       686\t|\tloss: 22.8198\n",
      "Training Epoch 4  85.9% | batch:       589 of       686\t|\tloss: 32.4237\n",
      "Training Epoch 4  86.0% | batch:       590 of       686\t|\tloss: 27.0661\n",
      "Training Epoch 4  86.2% | batch:       591 of       686\t|\tloss: 29.9599\n",
      "Training Epoch 4  86.3% | batch:       592 of       686\t|\tloss: 31.3513\n",
      "Training Epoch 4  86.4% | batch:       593 of       686\t|\tloss: 20.5929\n",
      "Training Epoch 4  86.6% | batch:       594 of       686\t|\tloss: 34.4971\n",
      "Training Epoch 4  86.7% | batch:       595 of       686\t|\tloss: 29.1576\n",
      "Training Epoch 4  86.9% | batch:       596 of       686\t|\tloss: 17.4087\n",
      "Training Epoch 4  87.0% | batch:       597 of       686\t|\tloss: 27.3628\n",
      "Training Epoch 4  87.2% | batch:       598 of       686\t|\tloss: 23.35\n",
      "Training Epoch 4  87.3% | batch:       599 of       686\t|\tloss: 26.3802\n",
      "Training Epoch 4  87.5% | batch:       600 of       686\t|\tloss: 28.8216\n",
      "Training Epoch 4  87.6% | batch:       601 of       686\t|\tloss: 27.5259\n",
      "Training Epoch 4  87.8% | batch:       602 of       686\t|\tloss: 30.8951\n",
      "Training Epoch 4  87.9% | batch:       603 of       686\t|\tloss: 24.1478\n",
      "Training Epoch 4  88.0% | batch:       604 of       686\t|\tloss: 27.1221\n",
      "Training Epoch 4  88.2% | batch:       605 of       686\t|\tloss: 34.2921\n",
      "Training Epoch 4  88.3% | batch:       606 of       686\t|\tloss: 17.6152\n",
      "Training Epoch 4  88.5% | batch:       607 of       686\t|\tloss: 27.8976\n",
      "Training Epoch 4  88.6% | batch:       608 of       686\t|\tloss: 23.5748\n",
      "Training Epoch 4  88.8% | batch:       609 of       686\t|\tloss: 32.8884\n",
      "Training Epoch 4  88.9% | batch:       610 of       686\t|\tloss: 29.3958\n",
      "Training Epoch 4  89.1% | batch:       611 of       686\t|\tloss: 23.2429\n",
      "Training Epoch 4  89.2% | batch:       612 of       686\t|\tloss: 28.3854\n",
      "Training Epoch 4  89.4% | batch:       613 of       686\t|\tloss: 22.6393\n",
      "Training Epoch 4  89.5% | batch:       614 of       686\t|\tloss: 40.3513\n",
      "Training Epoch 4  89.7% | batch:       615 of       686\t|\tloss: 25.9934\n",
      "Training Epoch 4  89.8% | batch:       616 of       686\t|\tloss: 26.7367\n",
      "Training Epoch 4  89.9% | batch:       617 of       686\t|\tloss: 32.0321\n",
      "Training Epoch 4  90.1% | batch:       618 of       686\t|\tloss: 28.1414\n",
      "Training Epoch 4  90.2% | batch:       619 of       686\t|\tloss: 25.6249\n",
      "Training Epoch 4  90.4% | batch:       620 of       686\t|\tloss: 21.6042\n",
      "Training Epoch 4  90.5% | batch:       621 of       686\t|\tloss: 29.017\n",
      "Training Epoch 4  90.7% | batch:       622 of       686\t|\tloss: 18.8548\n",
      "Training Epoch 4  90.8% | batch:       623 of       686\t|\tloss: 19.5181\n",
      "Training Epoch 4  91.0% | batch:       624 of       686\t|\tloss: 26.1078\n",
      "Training Epoch 4  91.1% | batch:       625 of       686\t|\tloss: 28.5627\n",
      "Training Epoch 4  91.3% | batch:       626 of       686\t|\tloss: 32.1604\n",
      "Training Epoch 4  91.4% | batch:       627 of       686\t|\tloss: 25.8842\n",
      "Training Epoch 4  91.5% | batch:       628 of       686\t|\tloss: 28.2339\n",
      "Training Epoch 4  91.7% | batch:       629 of       686\t|\tloss: 32.8501\n",
      "Training Epoch 4  91.8% | batch:       630 of       686\t|\tloss: 23.5596\n",
      "Training Epoch 4  92.0% | batch:       631 of       686\t|\tloss: 27.8194\n",
      "Training Epoch 4  92.1% | batch:       632 of       686\t|\tloss: 34.1957\n",
      "Training Epoch 4  92.3% | batch:       633 of       686\t|\tloss: 58.4664\n",
      "Training Epoch 4  92.4% | batch:       634 of       686\t|\tloss: 28.9376\n",
      "Training Epoch 4  92.6% | batch:       635 of       686\t|\tloss: 22.7775\n",
      "Training Epoch 4  92.7% | batch:       636 of       686\t|\tloss: 23.3239\n",
      "Training Epoch 4  92.9% | batch:       637 of       686\t|\tloss: 30.1806\n",
      "Training Epoch 4  93.0% | batch:       638 of       686\t|\tloss: 27.1024\n",
      "Training Epoch 4  93.1% | batch:       639 of       686\t|\tloss: 20.0472\n",
      "Training Epoch 4  93.3% | batch:       640 of       686\t|\tloss: 20.0674\n",
      "Training Epoch 4  93.4% | batch:       641 of       686\t|\tloss: 28.1183\n",
      "Training Epoch 4  93.6% | batch:       642 of       686\t|\tloss: 23.1582\n",
      "Training Epoch 4  93.7% | batch:       643 of       686\t|\tloss: 22.309\n",
      "Training Epoch 4  93.9% | batch:       644 of       686\t|\tloss: 21.6102\n",
      "Training Epoch 4  94.0% | batch:       645 of       686\t|\tloss: 21.5539\n",
      "Training Epoch 4  94.2% | batch:       646 of       686\t|\tloss: 25.9852\n",
      "Training Epoch 4  94.3% | batch:       647 of       686\t|\tloss: 22.4638\n",
      "Training Epoch 4  94.5% | batch:       648 of       686\t|\tloss: 22.1233\n",
      "Training Epoch 4  94.6% | batch:       649 of       686\t|\tloss: 25.7909\n",
      "Training Epoch 4  94.8% | batch:       650 of       686\t|\tloss: 26.6112\n",
      "Training Epoch 4  94.9% | batch:       651 of       686\t|\tloss: 29.176\n",
      "Training Epoch 4  95.0% | batch:       652 of       686\t|\tloss: 93.3441\n",
      "Training Epoch 4  95.2% | batch:       653 of       686\t|\tloss: 26.0472\n",
      "Training Epoch 4  95.3% | batch:       654 of       686\t|\tloss: 14.2435\n",
      "Training Epoch 4  95.5% | batch:       655 of       686\t|\tloss: 38.3029\n",
      "Training Epoch 4  95.6% | batch:       656 of       686\t|\tloss: 26.5034\n",
      "Training Epoch 4  95.8% | batch:       657 of       686\t|\tloss: 22.613\n",
      "Training Epoch 4  95.9% | batch:       658 of       686\t|\tloss: 24.8932\n",
      "Training Epoch 4  96.1% | batch:       659 of       686\t|\tloss: 23.812\n",
      "Training Epoch 4  96.2% | batch:       660 of       686\t|\tloss: 36.788\n",
      "Training Epoch 4  96.4% | batch:       661 of       686\t|\tloss: 26.3785\n",
      "Training Epoch 4  96.5% | batch:       662 of       686\t|\tloss: 55.0924\n",
      "Training Epoch 4  96.6% | batch:       663 of       686\t|\tloss: 29.4016\n",
      "Training Epoch 4  96.8% | batch:       664 of       686\t|\tloss: 22.5992\n",
      "Training Epoch 4  96.9% | batch:       665 of       686\t|\tloss: 29.5332\n",
      "Training Epoch 4  97.1% | batch:       666 of       686\t|\tloss: 22.8121\n",
      "Training Epoch 4  97.2% | batch:       667 of       686\t|\tloss: 21.6877\n",
      "Training Epoch 4  97.4% | batch:       668 of       686\t|\tloss: 23.6204\n",
      "Training Epoch 4  97.5% | batch:       669 of       686\t|\tloss: 29.8713\n",
      "Training Epoch 4  97.7% | batch:       670 of       686\t|\tloss: 24.8189\n",
      "Training Epoch 4  97.8% | batch:       671 of       686\t|\tloss: 20.984\n",
      "Training Epoch 4  98.0% | batch:       672 of       686\t|\tloss: 16.8847\n",
      "Training Epoch 4  98.1% | batch:       673 of       686\t|\tloss: 51.6448\n",
      "Training Epoch 4  98.3% | batch:       674 of       686\t|\tloss: 24.6226\n",
      "Training Epoch 4  98.4% | batch:       675 of       686\t|\tloss: 26.7419\n",
      "Training Epoch 4  98.5% | batch:       676 of       686\t|\tloss: 24.6573\n",
      "Training Epoch 4  98.7% | batch:       677 of       686\t|\tloss: 29.4484\n",
      "Training Epoch 4  98.8% | batch:       678 of       686\t|\tloss: 21.3602\n",
      "Training Epoch 4  99.0% | batch:       679 of       686\t|\tloss: 27.2455\n",
      "Training Epoch 4  99.1% | batch:       680 of       686\t|\tloss: 29.397\n",
      "Training Epoch 4  99.3% | batch:       681 of       686\t|\tloss: 22.0925\n",
      "Training Epoch 4  99.4% | batch:       682 of       686\t|\tloss: 23.3402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:01:39,955 | INFO : Epoch 4 Training Summary: epoch: 4.000000 | loss: 29.324227 | \n",
      "2023-05-25 22:01:39,956 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 24.839202165603638 seconds\n",
      "\n",
      "2023-05-25 22:01:39,957 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.853826344013214 seconds\n",
      "2023-05-25 22:01:39,957 | INFO : Avg batch train. time: 0.03477234160934871 seconds\n",
      "2023-05-25 22:01:39,957 | INFO : Avg sample train. time: 0.00027200896680555577 seconds\n",
      "2023-05-25 22:01:39,958 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  99.6% | batch:       683 of       686\t|\tloss: 22.4218\n",
      "Training Epoch 4  99.7% | batch:       684 of       686\t|\tloss: 57.045\n",
      "Training Epoch 4  99.9% | batch:       685 of       686\t|\tloss: 37.2406\n",
      "\n",
      "Evaluating Epoch 4   0.0% | batch:         0 of       172\t|\tloss: 2.02263\n",
      "Evaluating Epoch 4   0.6% | batch:         1 of       172\t|\tloss: 4.45273\n",
      "Evaluating Epoch 4   1.2% | batch:         2 of       172\t|\tloss: 4.43617\n",
      "Evaluating Epoch 4   1.7% | batch:         3 of       172\t|\tloss: 4.83412\n",
      "Evaluating Epoch 4   2.3% | batch:         4 of       172\t|\tloss: 4.60422\n",
      "Evaluating Epoch 4   2.9% | batch:         5 of       172\t|\tloss: 2.68383\n",
      "Evaluating Epoch 4   3.5% | batch:         6 of       172\t|\tloss: 3.95389\n",
      "Evaluating Epoch 4   4.1% | batch:         7 of       172\t|\tloss: 5.33373\n",
      "Evaluating Epoch 4   4.7% | batch:         8 of       172\t|\tloss: 3.27659\n",
      "Evaluating Epoch 4   5.2% | batch:         9 of       172\t|\tloss: 3.24671\n",
      "Evaluating Epoch 4   5.8% | batch:        10 of       172\t|\tloss: 5.54455\n",
      "Evaluating Epoch 4   6.4% | batch:        11 of       172\t|\tloss: 3.71606\n",
      "Evaluating Epoch 4   7.0% | batch:        12 of       172\t|\tloss: 4.56763\n",
      "Evaluating Epoch 4   7.6% | batch:        13 of       172\t|\tloss: 2.6378\n",
      "Evaluating Epoch 4   8.1% | batch:        14 of       172\t|\tloss: 5.11455\n",
      "Evaluating Epoch 4   8.7% | batch:        15 of       172\t|\tloss: 1.87565\n",
      "Evaluating Epoch 4   9.3% | batch:        16 of       172\t|\tloss: 4.80588\n",
      "Evaluating Epoch 4   9.9% | batch:        17 of       172\t|\tloss: 3.38573\n",
      "Evaluating Epoch 4  10.5% | batch:        18 of       172\t|\tloss: 11.3954\n",
      "Evaluating Epoch 4  11.0% | batch:        19 of       172\t|\tloss: 1.11835\n",
      "Evaluating Epoch 4  11.6% | batch:        20 of       172\t|\tloss: 5.24384\n",
      "Evaluating Epoch 4  12.2% | batch:        21 of       172\t|\tloss: 1.28066\n",
      "Evaluating Epoch 4  12.8% | batch:        22 of       172\t|\tloss: 2.71296\n",
      "Evaluating Epoch 4  13.4% | batch:        23 of       172\t|\tloss: 1.49217\n",
      "Evaluating Epoch 4  14.0% | batch:        24 of       172\t|\tloss: 4.10549\n",
      "Evaluating Epoch 4  14.5% | batch:        25 of       172\t|\tloss: 4.58747\n",
      "Evaluating Epoch 4  15.1% | batch:        26 of       172\t|\tloss: 10.1609\n",
      "Evaluating Epoch 4  15.7% | batch:        27 of       172\t|\tloss: 7.78706\n",
      "Evaluating Epoch 4  16.3% | batch:        28 of       172\t|\tloss: 1.02652\n",
      "Evaluating Epoch 4  16.9% | batch:        29 of       172\t|\tloss: 5.02588\n",
      "Evaluating Epoch 4  17.4% | batch:        30 of       172\t|\tloss: 0.959514\n",
      "Evaluating Epoch 4  18.0% | batch:        31 of       172\t|\tloss: 4.00072\n",
      "Evaluating Epoch 4  18.6% | batch:        32 of       172\t|\tloss: 1.26868\n",
      "Evaluating Epoch 4  19.2% | batch:        33 of       172\t|\tloss: 5.16863\n",
      "Evaluating Epoch 4  19.8% | batch:        34 of       172\t|\tloss: 1.89904\n",
      "Evaluating Epoch 4  20.3% | batch:        35 of       172\t|\tloss: 0.963855\n",
      "Evaluating Epoch 4  20.9% | batch:        36 of       172\t|\tloss: 9.8417\n",
      "Evaluating Epoch 4  21.5% | batch:        37 of       172\t|\tloss: 5.008\n",
      "Evaluating Epoch 4  22.1% | batch:        38 of       172\t|\tloss: 5.58007\n",
      "Evaluating Epoch 4  22.7% | batch:        39 of       172\t|\tloss: 2.13005\n",
      "Evaluating Epoch 4  23.3% | batch:        40 of       172\t|\tloss: 1.6392\n",
      "Evaluating Epoch 4  23.8% | batch:        41 of       172\t|\tloss: 3.9831\n",
      "Evaluating Epoch 4  24.4% | batch:        42 of       172\t|\tloss: 1.54544\n",
      "Evaluating Epoch 4  25.0% | batch:        43 of       172\t|\tloss: 13.139\n",
      "Evaluating Epoch 4  25.6% | batch:        44 of       172\t|\tloss: 1.20617\n",
      "Evaluating Epoch 4  26.2% | batch:        45 of       172\t|\tloss: 3.40655\n",
      "Evaluating Epoch 4  26.7% | batch:        46 of       172\t|\tloss: 1.30693\n",
      "Evaluating Epoch 4  27.3% | batch:        47 of       172\t|\tloss: 3.17369\n",
      "Evaluating Epoch 4  27.9% | batch:        48 of       172\t|\tloss: 1.53588\n",
      "Evaluating Epoch 4  28.5% | batch:        49 of       172\t|\tloss: 6.12598\n",
      "Evaluating Epoch 4  29.1% | batch:        50 of       172\t|\tloss: 2.30073\n",
      "Evaluating Epoch 4  29.7% | batch:        51 of       172\t|\tloss: 1.73431\n",
      "Evaluating Epoch 4  30.2% | batch:        52 of       172\t|\tloss: 3.98663\n",
      "Evaluating Epoch 4  30.8% | batch:        53 of       172\t|\tloss: 1.73181\n",
      "Evaluating Epoch 4  31.4% | batch:        54 of       172\t|\tloss: 3.82591\n",
      "Evaluating Epoch 4  32.0% | batch:        55 of       172\t|\tloss: 2.4669\n",
      "Evaluating Epoch 4  32.6% | batch:        56 of       172\t|\tloss: 1.72736\n",
      "Evaluating Epoch 4  33.1% | batch:        57 of       172\t|\tloss: 2.82795\n",
      "Evaluating Epoch 4  33.7% | batch:        58 of       172\t|\tloss: 2.62663\n",
      "Evaluating Epoch 4  34.3% | batch:        59 of       172\t|\tloss: 3.91771\n",
      "Evaluating Epoch 4  34.9% | batch:        60 of       172\t|\tloss: 2.39369\n",
      "Evaluating Epoch 4  35.5% | batch:        61 of       172\t|\tloss: 3.03547\n",
      "Evaluating Epoch 4  36.0% | batch:        62 of       172\t|\tloss: 2.1904\n",
      "Evaluating Epoch 4  36.6% | batch:        63 of       172\t|\tloss: 3.42342\n",
      "Evaluating Epoch 4  37.2% | batch:        64 of       172\t|\tloss: 3.4747\n",
      "Evaluating Epoch 4  37.8% | batch:        65 of       172\t|\tloss: 1.71632\n",
      "Evaluating Epoch 4  38.4% | batch:        66 of       172\t|\tloss: 4.65524\n",
      "Evaluating Epoch 4  39.0% | batch:        67 of       172\t|\tloss: 2.58152\n",
      "Evaluating Epoch 4  39.5% | batch:        68 of       172\t|\tloss: 3.22722\n",
      "Evaluating Epoch 4  40.1% | batch:        69 of       172\t|\tloss: 4.27686\n",
      "Evaluating Epoch 4  40.7% | batch:        70 of       172\t|\tloss: 1.15966\n",
      "Evaluating Epoch 4  41.3% | batch:        71 of       172\t|\tloss: 3.53867\n",
      "Evaluating Epoch 4  41.9% | batch:        72 of       172\t|\tloss: 2.50843\n",
      "Evaluating Epoch 4  42.4% | batch:        73 of       172\t|\tloss: 1.63685\n",
      "Evaluating Epoch 4  43.0% | batch:        74 of       172\t|\tloss: 1.65926\n",
      "Evaluating Epoch 4  43.6% | batch:        75 of       172\t|\tloss: 2.31437\n",
      "Evaluating Epoch 4  44.2% | batch:        76 of       172\t|\tloss: 1.99691\n",
      "Evaluating Epoch 4  44.8% | batch:        77 of       172\t|\tloss: 2.45154\n",
      "Evaluating Epoch 4  45.3% | batch:        78 of       172\t|\tloss: 3.62111\n",
      "Evaluating Epoch 4  45.9% | batch:        79 of       172\t|\tloss: 2.14635\n",
      "Evaluating Epoch 4  46.5% | batch:        80 of       172\t|\tloss: 1.63934\n",
      "Evaluating Epoch 4  47.1% | batch:        81 of       172\t|\tloss: 1.6973\n",
      "Evaluating Epoch 4  47.7% | batch:        82 of       172\t|\tloss: 2.07198\n",
      "Evaluating Epoch 4  48.3% | batch:        83 of       172\t|\tloss: 1.54918\n",
      "Evaluating Epoch 4  48.8% | batch:        84 of       172\t|\tloss: 1.12543\n",
      "Evaluating Epoch 4  49.4% | batch:        85 of       172\t|\tloss: 2.6633\n",
      "Evaluating Epoch 4  50.0% | batch:        86 of       172\t|\tloss: 2.70779\n",
      "Evaluating Epoch 4  50.6% | batch:        87 of       172\t|\tloss: 1.48555\n",
      "Evaluating Epoch 4  51.2% | batch:        88 of       172\t|\tloss: 1.21571\n",
      "Evaluating Epoch 4  51.7% | batch:        89 of       172\t|\tloss: 2.59412\n",
      "Evaluating Epoch 4  52.3% | batch:        90 of       172\t|\tloss: 2.88803\n",
      "Evaluating Epoch 4  52.9% | batch:        91 of       172\t|\tloss: 1.65123\n",
      "Evaluating Epoch 4  53.5% | batch:        92 of       172\t|\tloss: 2.16442\n",
      "Evaluating Epoch 4  54.1% | batch:        93 of       172\t|\tloss: 1.67483\n",
      "Evaluating Epoch 4  54.7% | batch:        94 of       172\t|\tloss: 4.16679\n",
      "Evaluating Epoch 4  55.2% | batch:        95 of       172\t|\tloss: 1.11602\n",
      "Evaluating Epoch 4  55.8% | batch:        96 of       172\t|\tloss: 2.53011\n",
      "Evaluating Epoch 4  56.4% | batch:        97 of       172\t|\tloss: 1.33468\n",
      "Evaluating Epoch 4  57.0% | batch:        98 of       172\t|\tloss: 1.90589\n",
      "Evaluating Epoch 4  57.6% | batch:        99 of       172\t|\tloss: 3.00013\n",
      "Evaluating Epoch 4  58.1% | batch:       100 of       172\t|\tloss: 1.36713\n",
      "Evaluating Epoch 4  58.7% | batch:       101 of       172\t|\tloss: 1.21538\n",
      "Evaluating Epoch 4  59.3% | batch:       102 of       172\t|\tloss: 1.56385\n",
      "Evaluating Epoch 4  59.9% | batch:       103 of       172\t|\tloss: 2.11148\n",
      "Evaluating Epoch 4  60.5% | batch:       104 of       172\t|\tloss: 1.53387\n",
      "Evaluating Epoch 4  61.0% | batch:       105 of       172\t|\tloss: 1.37233\n",
      "Evaluating Epoch 4  61.6% | batch:       106 of       172\t|\tloss: 2.8059\n",
      "Evaluating Epoch 4  62.2% | batch:       107 of       172\t|\tloss: 1.65375\n",
      "Evaluating Epoch 4  62.8% | batch:       108 of       172\t|\tloss: 1.44461\n",
      "Evaluating Epoch 4  63.4% | batch:       109 of       172\t|\tloss: 1.70239\n",
      "Evaluating Epoch 4  64.0% | batch:       110 of       172\t|\tloss: 2.84889\n",
      "Evaluating Epoch 4  64.5% | batch:       111 of       172\t|\tloss: 1.56212\n",
      "Evaluating Epoch 4  65.1% | batch:       112 of       172\t|\tloss: 1.27759\n",
      "Evaluating Epoch 4  65.7% | batch:       113 of       172\t|\tloss: 4.32317\n",
      "Evaluating Epoch 4  66.3% | batch:       114 of       172\t|\tloss: 4.69808\n",
      "Evaluating Epoch 4  66.9% | batch:       115 of       172\t|\tloss: 1.32847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 4  67.4% | batch:       116 of       172\t|\tloss: 4.40704\n",
      "Evaluating Epoch 4  68.0% | batch:       117 of       172\t|\tloss: 4.02644\n",
      "Evaluating Epoch 4  68.6% | batch:       118 of       172\t|\tloss: 2.62751\n",
      "Evaluating Epoch 4  69.2% | batch:       119 of       172\t|\tloss: 4.65778\n",
      "Evaluating Epoch 4  69.8% | batch:       120 of       172\t|\tloss: 2.11894\n",
      "Evaluating Epoch 4  70.3% | batch:       121 of       172\t|\tloss: 8.94107\n",
      "Evaluating Epoch 4  70.9% | batch:       122 of       172\t|\tloss: 5.93029\n",
      "Evaluating Epoch 4  71.5% | batch:       123 of       172\t|\tloss: 19.2393\n",
      "Evaluating Epoch 4  72.1% | batch:       124 of       172\t|\tloss: 124.745\n",
      "Evaluating Epoch 4  72.7% | batch:       125 of       172\t|\tloss: 7.50352\n",
      "Evaluating Epoch 4  73.3% | batch:       126 of       172\t|\tloss: 3.13428\n",
      "Evaluating Epoch 4  73.8% | batch:       127 of       172\t|\tloss: 1.70387\n",
      "Evaluating Epoch 4  74.4% | batch:       128 of       172\t|\tloss: 7.99236\n",
      "Evaluating Epoch 4  75.0% | batch:       129 of       172\t|\tloss: 3.67177\n",
      "Evaluating Epoch 4  75.6% | batch:       130 of       172\t|\tloss: 1.76336\n",
      "Evaluating Epoch 4  76.2% | batch:       131 of       172\t|\tloss: 5.1047\n",
      "Evaluating Epoch 4  76.7% | batch:       132 of       172\t|\tloss: 3.04328\n",
      "Evaluating Epoch 4  77.3% | batch:       133 of       172\t|\tloss: 3.65888\n",
      "Evaluating Epoch 4  77.9% | batch:       134 of       172\t|\tloss: 3.49695\n",
      "Evaluating Epoch 4  78.5% | batch:       135 of       172\t|\tloss: 1.71695\n",
      "Evaluating Epoch 4  79.1% | batch:       136 of       172\t|\tloss: 3.06286\n",
      "Evaluating Epoch 4  79.7% | batch:       137 of       172\t|\tloss: 1.5342\n",
      "Evaluating Epoch 4  80.2% | batch:       138 of       172\t|\tloss: 4.14001\n",
      "Evaluating Epoch 4  80.8% | batch:       139 of       172\t|\tloss: 4.16594\n",
      "Evaluating Epoch 4  81.4% | batch:       140 of       172\t|\tloss: 2.05886\n",
      "Evaluating Epoch 4  82.0% | batch:       141 of       172\t|\tloss: 1.97152\n",
      "Evaluating Epoch 4  82.6% | batch:       142 of       172\t|\tloss: 1.4349\n",
      "Evaluating Epoch 4  83.1% | batch:       143 of       172\t|\tloss: 2.34651\n",
      "Evaluating Epoch 4  83.7% | batch:       144 of       172\t|\tloss: 2.77095\n",
      "Evaluating Epoch 4  84.3% | batch:       145 of       172\t|\tloss: 1.88528\n",
      "Evaluating Epoch 4  84.9% | batch:       146 of       172\t|\tloss: 2.76401\n",
      "Evaluating Epoch 4  85.5% | batch:       147 of       172\t|\tloss: 2.30095\n",
      "Evaluating Epoch 4  86.0% | batch:       148 of       172\t|\tloss: 1.57735\n",
      "Evaluating Epoch 4  86.6% | batch:       149 of       172\t|\tloss: 2.37463\n",
      "Evaluating Epoch 4  87.2% | batch:       150 of       172\t|\tloss: 3.43441\n",
      "Evaluating Epoch 4  87.8% | batch:       151 of       172\t|\tloss: 2.45224\n",
      "Evaluating Epoch 4  88.4% | batch:       152 of       172\t|\tloss: 3.65836\n",
      "Evaluating Epoch 4  89.0% | batch:       153 of       172\t|\tloss: 3.13021\n",
      "Evaluating Epoch 4  89.5% | batch:       154 of       172\t|\tloss: 3.21773\n",
      "Evaluating Epoch 4  90.1% | batch:       155 of       172\t|\tloss: 4.73059\n",
      "Evaluating Epoch 4  90.7% | batch:       156 of       172\t|\tloss: 3.4656\n",
      "Evaluating Epoch 4  91.3% | batch:       157 of       172\t|\tloss: 3.39174\n",
      "Evaluating Epoch 4  91.9% | batch:       158 of       172\t|\tloss: 4.62985\n",
      "Evaluating Epoch 4  92.4% | batch:       159 of       172\t|\tloss: 2.80598\n",
      "Evaluating Epoch 4  93.0% | batch:       160 of       172\t|\tloss: 14.6394\n",
      "Evaluating Epoch 4  93.6% | batch:       161 of       172\t|\tloss: 10.533\n",
      "Evaluating Epoch 4  94.2% | batch:       162 of       172\t|\tloss: 2.84918\n",
      "Evaluating Epoch 4  94.8% | batch:       163 of       172\t|\tloss: 3.83879\n",
      "Evaluating Epoch 4  95.3% | batch:       164 of       172\t|\tloss: 4.39982\n",
      "Evaluating Epoch 4  95.9% | batch:       165 of       172\t|\tloss: 2.61595\n",
      "Evaluating Epoch 4  96.5% | batch:       166 of       172\t|\tloss: 3.35111\n",
      "Evaluating Epoch 4  97.1% | batch:       167 of       172\t|\tloss: 2.36308\n",
      "Evaluating Epoch 4  97.7% | batch:       168 of       172\t|\tloss: 2.67863\n",
      "Evaluating Epoch 4  98.3% | batch:       169 of       172\t|\tloss: 3.34555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:01:43,516 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.5579280853271484 seconds\n",
      "\n",
      "2023-05-25 22:01:43,520 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.143463373184204 seconds\n",
      "2023-05-25 22:01:43,521 | INFO : Avg batch val. time: 0.024089903332466304 seconds\n",
      "2023-05-25 22:01:43,523 | INFO : Avg sample val. time: 0.0001887080827610422 seconds\n",
      "2023-05-25 22:01:43,524 | INFO : Epoch 4 Validation Summary: epoch: 4.000000 | loss: 4.080097 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 4  98.8% | batch:       170 of       172\t|\tloss: 2.64513\n",
      "Evaluating Epoch 4  99.4% | batch:       171 of       172\t|\tloss: 4.43853\n",
      "\n",
      "Training Epoch 5   0.0% | batch:         0 of       686\t|\tloss: 25.7904\n",
      "Training Epoch 5   0.1% | batch:         1 of       686\t|\tloss: 29.7352\n",
      "Training Epoch 5   0.3% | batch:         2 of       686\t|\tloss: 21.2505\n",
      "Training Epoch 5   0.4% | batch:         3 of       686\t|\tloss: 21.4477\n",
      "Training Epoch 5   0.6% | batch:         4 of       686\t|\tloss: 22.7672\n",
      "Training Epoch 5   0.7% | batch:         5 of       686\t|\tloss: 20.6393\n",
      "Training Epoch 5   0.9% | batch:         6 of       686\t|\tloss: 23.4433\n",
      "Training Epoch 5   1.0% | batch:         7 of       686\t|\tloss: 26.2804\n",
      "Training Epoch 5   1.2% | batch:         8 of       686\t|\tloss: 29.9956\n",
      "Training Epoch 5   1.3% | batch:         9 of       686\t|\tloss: 22.8598\n",
      "Training Epoch 5   1.5% | batch:        10 of       686\t|\tloss: 22.0268\n",
      "Training Epoch 5   1.6% | batch:        11 of       686\t|\tloss: 25.3187\n",
      "Training Epoch 5   1.7% | batch:        12 of       686\t|\tloss: 25.5645\n",
      "Training Epoch 5   1.9% | batch:        13 of       686\t|\tloss: 24.1953\n",
      "Training Epoch 5   2.0% | batch:        14 of       686\t|\tloss: 25.1116\n",
      "Training Epoch 5   2.2% | batch:        15 of       686\t|\tloss: 26.6488\n",
      "Training Epoch 5   2.3% | batch:        16 of       686\t|\tloss: 25.619\n",
      "Training Epoch 5   2.5% | batch:        17 of       686\t|\tloss: 52.7845\n",
      "Training Epoch 5   2.6% | batch:        18 of       686\t|\tloss: 19.2802\n",
      "Training Epoch 5   2.8% | batch:        19 of       686\t|\tloss: 26.5713\n",
      "Training Epoch 5   2.9% | batch:        20 of       686\t|\tloss: 28.6056\n",
      "Training Epoch 5   3.1% | batch:        21 of       686\t|\tloss: 24.3824\n",
      "Training Epoch 5   3.2% | batch:        22 of       686\t|\tloss: 18.8853\n",
      "Training Epoch 5   3.4% | batch:        23 of       686\t|\tloss: 13.6126\n",
      "Training Epoch 5   3.5% | batch:        24 of       686\t|\tloss: 28.1632\n",
      "Training Epoch 5   3.6% | batch:        25 of       686\t|\tloss: 38.5721\n",
      "Training Epoch 5   3.8% | batch:        26 of       686\t|\tloss: 27.7932\n",
      "Training Epoch 5   3.9% | batch:        27 of       686\t|\tloss: 34.4585\n",
      "Training Epoch 5   4.1% | batch:        28 of       686\t|\tloss: 22.3862\n",
      "Training Epoch 5   4.2% | batch:        29 of       686\t|\tloss: 22.3065\n",
      "Training Epoch 5   4.4% | batch:        30 of       686\t|\tloss: 28.641\n",
      "Training Epoch 5   4.5% | batch:        31 of       686\t|\tloss: 24.8369\n",
      "Training Epoch 5   4.7% | batch:        32 of       686\t|\tloss: 21.4888\n",
      "Training Epoch 5   4.8% | batch:        33 of       686\t|\tloss: 22.6745\n",
      "Training Epoch 5   5.0% | batch:        34 of       686\t|\tloss: 28.8774\n",
      "Training Epoch 5   5.1% | batch:        35 of       686\t|\tloss: 17.2374\n",
      "Training Epoch 5   5.2% | batch:        36 of       686\t|\tloss: 31.5982\n",
      "Training Epoch 5   5.4% | batch:        37 of       686\t|\tloss: 23.3691\n",
      "Training Epoch 5   5.5% | batch:        38 of       686\t|\tloss: 17.3196\n",
      "Training Epoch 5   5.7% | batch:        39 of       686\t|\tloss: 30.0436\n",
      "Training Epoch 5   5.8% | batch:        40 of       686\t|\tloss: 25.1862\n",
      "Training Epoch 5   6.0% | batch:        41 of       686\t|\tloss: 37.4677\n",
      "Training Epoch 5   6.1% | batch:        42 of       686\t|\tloss: 25.807\n",
      "Training Epoch 5   6.3% | batch:        43 of       686\t|\tloss: 28.4682\n",
      "Training Epoch 5   6.4% | batch:        44 of       686\t|\tloss: 23.8633\n",
      "Training Epoch 5   6.6% | batch:        45 of       686\t|\tloss: 27.8511\n",
      "Training Epoch 5   6.7% | batch:        46 of       686\t|\tloss: 29.6733\n",
      "Training Epoch 5   6.9% | batch:        47 of       686\t|\tloss: 21.5262\n",
      "Training Epoch 5   7.0% | batch:        48 of       686\t|\tloss: 24.9956\n",
      "Training Epoch 5   7.1% | batch:        49 of       686\t|\tloss: 26.2587\n",
      "Training Epoch 5   7.3% | batch:        50 of       686\t|\tloss: 26.3068\n",
      "Training Epoch 5   7.4% | batch:        51 of       686\t|\tloss: 29.8081\n",
      "Training Epoch 5   7.6% | batch:        52 of       686\t|\tloss: 22.7296\n",
      "Training Epoch 5   7.7% | batch:        53 of       686\t|\tloss: 20.0557\n",
      "Training Epoch 5   7.9% | batch:        54 of       686\t|\tloss: 30.3132\n",
      "Training Epoch 5   8.0% | batch:        55 of       686\t|\tloss: 24.8629\n",
      "Training Epoch 5   8.2% | batch:        56 of       686\t|\tloss: 22.9041\n",
      "Training Epoch 5   8.3% | batch:        57 of       686\t|\tloss: 25.7999\n",
      "Training Epoch 5   8.5% | batch:        58 of       686\t|\tloss: 18.7217\n",
      "Training Epoch 5   8.6% | batch:        59 of       686\t|\tloss: 23.5418\n",
      "Training Epoch 5   8.7% | batch:        60 of       686\t|\tloss: 23.6577\n",
      "Training Epoch 5   8.9% | batch:        61 of       686\t|\tloss: 24.0124\n",
      "Training Epoch 5   9.0% | batch:        62 of       686\t|\tloss: 23.0763\n",
      "Training Epoch 5   9.2% | batch:        63 of       686\t|\tloss: 16.5611\n",
      "Training Epoch 5   9.3% | batch:        64 of       686\t|\tloss: 20.5869\n",
      "Training Epoch 5   9.5% | batch:        65 of       686\t|\tloss: 30.7987\n",
      "Training Epoch 5   9.6% | batch:        66 of       686\t|\tloss: 22.1471\n",
      "Training Epoch 5   9.8% | batch:        67 of       686\t|\tloss: 25.0146\n",
      "Training Epoch 5   9.9% | batch:        68 of       686\t|\tloss: 22.5508\n",
      "Training Epoch 5  10.1% | batch:        69 of       686\t|\tloss: 31.4087\n",
      "Training Epoch 5  10.2% | batch:        70 of       686\t|\tloss: 18.8698\n",
      "Training Epoch 5  10.3% | batch:        71 of       686\t|\tloss: 24.8187\n",
      "Training Epoch 5  10.5% | batch:        72 of       686\t|\tloss: 31.9004\n",
      "Training Epoch 5  10.6% | batch:        73 of       686\t|\tloss: 20.6624\n",
      "Training Epoch 5  10.8% | batch:        74 of       686\t|\tloss: 26.684\n",
      "Training Epoch 5  10.9% | batch:        75 of       686\t|\tloss: 23.925\n",
      "Training Epoch 5  11.1% | batch:        76 of       686\t|\tloss: 22.0156\n",
      "Training Epoch 5  11.2% | batch:        77 of       686\t|\tloss: 24.6858\n",
      "Training Epoch 5  11.4% | batch:        78 of       686\t|\tloss: 20.458\n",
      "Training Epoch 5  11.5% | batch:        79 of       686\t|\tloss: 21.434\n",
      "Training Epoch 5  11.7% | batch:        80 of       686\t|\tloss: 26.591\n",
      "Training Epoch 5  11.8% | batch:        81 of       686\t|\tloss: 30.5117\n",
      "Training Epoch 5  12.0% | batch:        82 of       686\t|\tloss: 35.7184\n",
      "Training Epoch 5  12.1% | batch:        83 of       686\t|\tloss: 19.7731\n",
      "Training Epoch 5  12.2% | batch:        84 of       686\t|\tloss: 27.3075\n",
      "Training Epoch 5  12.4% | batch:        85 of       686\t|\tloss: 23.9085\n",
      "Training Epoch 5  12.5% | batch:        86 of       686\t|\tloss: 29.1285\n",
      "Training Epoch 5  12.7% | batch:        87 of       686\t|\tloss: 22.5343\n",
      "Training Epoch 5  12.8% | batch:        88 of       686\t|\tloss: 20.0265\n",
      "Training Epoch 5  13.0% | batch:        89 of       686\t|\tloss: 24.5121\n",
      "Training Epoch 5  13.1% | batch:        90 of       686\t|\tloss: 25.204\n",
      "Training Epoch 5  13.3% | batch:        91 of       686\t|\tloss: 27.5443\n",
      "Training Epoch 5  13.4% | batch:        92 of       686\t|\tloss: 26.512\n",
      "Training Epoch 5  13.6% | batch:        93 of       686\t|\tloss: 23.0196\n",
      "Training Epoch 5  13.7% | batch:        94 of       686\t|\tloss: 30.0145\n",
      "Training Epoch 5  13.8% | batch:        95 of       686\t|\tloss: 21.7739\n",
      "Training Epoch 5  14.0% | batch:        96 of       686\t|\tloss: 21.9881\n",
      "Training Epoch 5  14.1% | batch:        97 of       686\t|\tloss: 25.7212\n",
      "Training Epoch 5  14.3% | batch:        98 of       686\t|\tloss: 30.5805\n",
      "Training Epoch 5  14.4% | batch:        99 of       686\t|\tloss: 25.4824\n",
      "Training Epoch 5  14.6% | batch:       100 of       686\t|\tloss: 27.0665\n",
      "Training Epoch 5  14.7% | batch:       101 of       686\t|\tloss: 36.8578\n",
      "Training Epoch 5  14.9% | batch:       102 of       686\t|\tloss: 18.0562\n",
      "Training Epoch 5  15.0% | batch:       103 of       686\t|\tloss: 23.9293\n",
      "Training Epoch 5  15.2% | batch:       104 of       686\t|\tloss: 22.5803\n",
      "Training Epoch 5  15.3% | batch:       105 of       686\t|\tloss: 20.47\n",
      "Training Epoch 5  15.5% | batch:       106 of       686\t|\tloss: 29.5159\n",
      "Training Epoch 5  15.6% | batch:       107 of       686\t|\tloss: 34.0209\n",
      "Training Epoch 5  15.7% | batch:       108 of       686\t|\tloss: 29.5268\n",
      "Training Epoch 5  15.9% | batch:       109 of       686\t|\tloss: 21.9131\n",
      "Training Epoch 5  16.0% | batch:       110 of       686\t|\tloss: 22.6331\n",
      "Training Epoch 5  16.2% | batch:       111 of       686\t|\tloss: 29.1371\n",
      "Training Epoch 5  16.3% | batch:       112 of       686\t|\tloss: 29.2406\n",
      "Training Epoch 5  16.5% | batch:       113 of       686\t|\tloss: 20.5787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  16.6% | batch:       114 of       686\t|\tloss: 28.7444\n",
      "Training Epoch 5  16.8% | batch:       115 of       686\t|\tloss: 18.9943\n",
      "Training Epoch 5  16.9% | batch:       116 of       686\t|\tloss: 30.1638\n",
      "Training Epoch 5  17.1% | batch:       117 of       686\t|\tloss: 27.1594\n",
      "Training Epoch 5  17.2% | batch:       118 of       686\t|\tloss: 41.5427\n",
      "Training Epoch 5  17.3% | batch:       119 of       686\t|\tloss: 19.4961\n",
      "Training Epoch 5  17.5% | batch:       120 of       686\t|\tloss: 26.9521\n",
      "Training Epoch 5  17.6% | batch:       121 of       686\t|\tloss: 21.4092\n",
      "Training Epoch 5  17.8% | batch:       122 of       686\t|\tloss: 27.665\n",
      "Training Epoch 5  17.9% | batch:       123 of       686\t|\tloss: 29.8021\n",
      "Training Epoch 5  18.1% | batch:       124 of       686\t|\tloss: 27.713\n",
      "Training Epoch 5  18.2% | batch:       125 of       686\t|\tloss: 18.6634\n",
      "Training Epoch 5  18.4% | batch:       126 of       686\t|\tloss: 20.666\n",
      "Training Epoch 5  18.5% | batch:       127 of       686\t|\tloss: 28.8865\n",
      "Training Epoch 5  18.7% | batch:       128 of       686\t|\tloss: 20.4767\n",
      "Training Epoch 5  18.8% | batch:       129 of       686\t|\tloss: 21.2252\n",
      "Training Epoch 5  19.0% | batch:       130 of       686\t|\tloss: 28.2666\n",
      "Training Epoch 5  19.1% | batch:       131 of       686\t|\tloss: 23.2778\n",
      "Training Epoch 5  19.2% | batch:       132 of       686\t|\tloss: 25.6834\n",
      "Training Epoch 5  19.4% | batch:       133 of       686\t|\tloss: 64.9029\n",
      "Training Epoch 5  19.5% | batch:       134 of       686\t|\tloss: 33.7995\n",
      "Training Epoch 5  19.7% | batch:       135 of       686\t|\tloss: 22.8425\n",
      "Training Epoch 5  19.8% | batch:       136 of       686\t|\tloss: 21.3556\n",
      "Training Epoch 5  20.0% | batch:       137 of       686\t|\tloss: 22.6758\n",
      "Training Epoch 5  20.1% | batch:       138 of       686\t|\tloss: 32.9051\n",
      "Training Epoch 5  20.3% | batch:       139 of       686\t|\tloss: 27.9148\n",
      "Training Epoch 5  20.4% | batch:       140 of       686\t|\tloss: 30.8512\n",
      "Training Epoch 5  20.6% | batch:       141 of       686\t|\tloss: 25.2268\n",
      "Training Epoch 5  20.7% | batch:       142 of       686\t|\tloss: 28.1686\n",
      "Training Epoch 5  20.8% | batch:       143 of       686\t|\tloss: 19.3172\n",
      "Training Epoch 5  21.0% | batch:       144 of       686\t|\tloss: 26.7828\n",
      "Training Epoch 5  21.1% | batch:       145 of       686\t|\tloss: 22.99\n",
      "Training Epoch 5  21.3% | batch:       146 of       686\t|\tloss: 24.7836\n",
      "Training Epoch 5  21.4% | batch:       147 of       686\t|\tloss: 26.9975\n",
      "Training Epoch 5  21.6% | batch:       148 of       686\t|\tloss: 29.3349\n",
      "Training Epoch 5  21.7% | batch:       149 of       686\t|\tloss: 16.8728\n",
      "Training Epoch 5  21.9% | batch:       150 of       686\t|\tloss: 33.4119\n",
      "Training Epoch 5  22.0% | batch:       151 of       686\t|\tloss: 27.4371\n",
      "Training Epoch 5  22.2% | batch:       152 of       686\t|\tloss: 25.4537\n",
      "Training Epoch 5  22.3% | batch:       153 of       686\t|\tloss: 20.3677\n",
      "Training Epoch 5  22.4% | batch:       154 of       686\t|\tloss: 27.8237\n",
      "Training Epoch 5  22.6% | batch:       155 of       686\t|\tloss: 51.5371\n",
      "Training Epoch 5  22.7% | batch:       156 of       686\t|\tloss: 17.9294\n",
      "Training Epoch 5  22.9% | batch:       157 of       686\t|\tloss: 29.5691\n",
      "Training Epoch 5  23.0% | batch:       158 of       686\t|\tloss: 26.349\n",
      "Training Epoch 5  23.2% | batch:       159 of       686\t|\tloss: 29.2826\n",
      "Training Epoch 5  23.3% | batch:       160 of       686\t|\tloss: 25.5639\n",
      "Training Epoch 5  23.5% | batch:       161 of       686\t|\tloss: 26.3323\n",
      "Training Epoch 5  23.6% | batch:       162 of       686\t|\tloss: 18.0214\n",
      "Training Epoch 5  23.8% | batch:       163 of       686\t|\tloss: 32.2036\n",
      "Training Epoch 5  23.9% | batch:       164 of       686\t|\tloss: 19.4554\n",
      "Training Epoch 5  24.1% | batch:       165 of       686\t|\tloss: 36.9774\n",
      "Training Epoch 5  24.2% | batch:       166 of       686\t|\tloss: 24.2755\n",
      "Training Epoch 5  24.3% | batch:       167 of       686\t|\tloss: 19.8554\n",
      "Training Epoch 5  24.5% | batch:       168 of       686\t|\tloss: 23.5329\n",
      "Training Epoch 5  24.6% | batch:       169 of       686\t|\tloss: 22.0771\n",
      "Training Epoch 5  24.8% | batch:       170 of       686\t|\tloss: 28.3047\n",
      "Training Epoch 5  24.9% | batch:       171 of       686\t|\tloss: 22.3512\n",
      "Training Epoch 5  25.1% | batch:       172 of       686\t|\tloss: 25.1177\n",
      "Training Epoch 5  25.2% | batch:       173 of       686\t|\tloss: 34.2787\n",
      "Training Epoch 5  25.4% | batch:       174 of       686\t|\tloss: 16.7627\n",
      "Training Epoch 5  25.5% | batch:       175 of       686\t|\tloss: 55.8188\n",
      "Training Epoch 5  25.7% | batch:       176 of       686\t|\tloss: 23.1733\n",
      "Training Epoch 5  25.8% | batch:       177 of       686\t|\tloss: 24.7547\n",
      "Training Epoch 5  25.9% | batch:       178 of       686\t|\tloss: 22.4222\n",
      "Training Epoch 5  26.1% | batch:       179 of       686\t|\tloss: 23.0414\n",
      "Training Epoch 5  26.2% | batch:       180 of       686\t|\tloss: 21.9713\n",
      "Training Epoch 5  26.4% | batch:       181 of       686\t|\tloss: 26.2121\n",
      "Training Epoch 5  26.5% | batch:       182 of       686\t|\tloss: 22.4746\n",
      "Training Epoch 5  26.7% | batch:       183 of       686\t|\tloss: 23.7547\n",
      "Training Epoch 5  26.8% | batch:       184 of       686\t|\tloss: 25.7395\n",
      "Training Epoch 5  27.0% | batch:       185 of       686\t|\tloss: 24.1419\n",
      "Training Epoch 5  27.1% | batch:       186 of       686\t|\tloss: 26.43\n",
      "Training Epoch 5  27.3% | batch:       187 of       686\t|\tloss: 23.9313\n",
      "Training Epoch 5  27.4% | batch:       188 of       686\t|\tloss: 25.0973\n",
      "Training Epoch 5  27.6% | batch:       189 of       686\t|\tloss: 24.6065\n",
      "Training Epoch 5  27.7% | batch:       190 of       686\t|\tloss: 29.3607\n",
      "Training Epoch 5  27.8% | batch:       191 of       686\t|\tloss: 32.5267\n",
      "Training Epoch 5  28.0% | batch:       192 of       686\t|\tloss: 21.3655\n",
      "Training Epoch 5  28.1% | batch:       193 of       686\t|\tloss: 28.228\n",
      "Training Epoch 5  28.3% | batch:       194 of       686\t|\tloss: 25.3911\n",
      "Training Epoch 5  28.4% | batch:       195 of       686\t|\tloss: 19.9713\n",
      "Training Epoch 5  28.6% | batch:       196 of       686\t|\tloss: 22.1686\n",
      "Training Epoch 5  28.7% | batch:       197 of       686\t|\tloss: 18.3423\n",
      "Training Epoch 5  28.9% | batch:       198 of       686\t|\tloss: 17.1856\n",
      "Training Epoch 5  29.0% | batch:       199 of       686\t|\tloss: 21.3555\n",
      "Training Epoch 5  29.2% | batch:       200 of       686\t|\tloss: 23.8087\n",
      "Training Epoch 5  29.3% | batch:       201 of       686\t|\tloss: 25.9435\n",
      "Training Epoch 5  29.4% | batch:       202 of       686\t|\tloss: 59.2039\n",
      "Training Epoch 5  29.6% | batch:       203 of       686\t|\tloss: 19.6194\n",
      "Training Epoch 5  29.7% | batch:       204 of       686\t|\tloss: 27.0213\n",
      "Training Epoch 5  29.9% | batch:       205 of       686\t|\tloss: 23.2455\n",
      "Training Epoch 5  30.0% | batch:       206 of       686\t|\tloss: 33.6946\n",
      "Training Epoch 5  30.2% | batch:       207 of       686\t|\tloss: 30.7164\n",
      "Training Epoch 5  30.3% | batch:       208 of       686\t|\tloss: 26.2969\n",
      "Training Epoch 5  30.5% | batch:       209 of       686\t|\tloss: 17.1421\n",
      "Training Epoch 5  30.6% | batch:       210 of       686\t|\tloss: 20.7816\n",
      "Training Epoch 5  30.8% | batch:       211 of       686\t|\tloss: 31.2385\n",
      "Training Epoch 5  30.9% | batch:       212 of       686\t|\tloss: 32.8133\n",
      "Training Epoch 5  31.0% | batch:       213 of       686\t|\tloss: 24.2438\n",
      "Training Epoch 5  31.2% | batch:       214 of       686\t|\tloss: 18.052\n",
      "Training Epoch 5  31.3% | batch:       215 of       686\t|\tloss: 22.4287\n",
      "Training Epoch 5  31.5% | batch:       216 of       686\t|\tloss: 28.4823\n",
      "Training Epoch 5  31.6% | batch:       217 of       686\t|\tloss: 27.8777\n",
      "Training Epoch 5  31.8% | batch:       218 of       686\t|\tloss: 18.4491\n",
      "Training Epoch 5  31.9% | batch:       219 of       686\t|\tloss: 19.7424\n",
      "Training Epoch 5  32.1% | batch:       220 of       686\t|\tloss: 22.6725\n",
      "Training Epoch 5  32.2% | batch:       221 of       686\t|\tloss: 23.0395\n",
      "Training Epoch 5  32.4% | batch:       222 of       686\t|\tloss: 29.5095\n",
      "Training Epoch 5  32.5% | batch:       223 of       686\t|\tloss: 25.3894\n",
      "Training Epoch 5  32.7% | batch:       224 of       686\t|\tloss: 17.9716\n",
      "Training Epoch 5  32.8% | batch:       225 of       686\t|\tloss: 22.5098\n",
      "Training Epoch 5  32.9% | batch:       226 of       686\t|\tloss: 23.6276\n",
      "Training Epoch 5  33.1% | batch:       227 of       686\t|\tloss: 28.0626\n",
      "Training Epoch 5  33.2% | batch:       228 of       686\t|\tloss: 22.9933\n",
      "Training Epoch 5  33.4% | batch:       229 of       686\t|\tloss: 20.9114\n",
      "Training Epoch 5  33.5% | batch:       230 of       686\t|\tloss: 28.8198\n",
      "Training Epoch 5  33.7% | batch:       231 of       686\t|\tloss: 18.6387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  33.8% | batch:       232 of       686\t|\tloss: 25.8538\n",
      "Training Epoch 5  34.0% | batch:       233 of       686\t|\tloss: 29.3767\n",
      "Training Epoch 5  34.1% | batch:       234 of       686\t|\tloss: 23.3019\n",
      "Training Epoch 5  34.3% | batch:       235 of       686\t|\tloss: 25.4827\n",
      "Training Epoch 5  34.4% | batch:       236 of       686\t|\tloss: 24.2907\n",
      "Training Epoch 5  34.5% | batch:       237 of       686\t|\tloss: 22.462\n",
      "Training Epoch 5  34.7% | batch:       238 of       686\t|\tloss: 15.8277\n",
      "Training Epoch 5  34.8% | batch:       239 of       686\t|\tloss: 26.9539\n",
      "Training Epoch 5  35.0% | batch:       240 of       686\t|\tloss: 27.9643\n",
      "Training Epoch 5  35.1% | batch:       241 of       686\t|\tloss: 28.3168\n",
      "Training Epoch 5  35.3% | batch:       242 of       686\t|\tloss: 26.5068\n",
      "Training Epoch 5  35.4% | batch:       243 of       686\t|\tloss: 27.6141\n",
      "Training Epoch 5  35.6% | batch:       244 of       686\t|\tloss: 22.5421\n",
      "Training Epoch 5  35.7% | batch:       245 of       686\t|\tloss: 23.7821\n",
      "Training Epoch 5  35.9% | batch:       246 of       686\t|\tloss: 26.9785\n",
      "Training Epoch 5  36.0% | batch:       247 of       686\t|\tloss: 24.1676\n",
      "Training Epoch 5  36.2% | batch:       248 of       686\t|\tloss: 27.8777\n",
      "Training Epoch 5  36.3% | batch:       249 of       686\t|\tloss: 25.7775\n",
      "Training Epoch 5  36.4% | batch:       250 of       686\t|\tloss: 16.8245\n",
      "Training Epoch 5  36.6% | batch:       251 of       686\t|\tloss: 27.6762\n",
      "Training Epoch 5  36.7% | batch:       252 of       686\t|\tloss: 17.1758\n",
      "Training Epoch 5  36.9% | batch:       253 of       686\t|\tloss: 25.8815\n",
      "Training Epoch 5  37.0% | batch:       254 of       686\t|\tloss: 25.7785\n",
      "Training Epoch 5  37.2% | batch:       255 of       686\t|\tloss: 20.1618\n",
      "Training Epoch 5  37.3% | batch:       256 of       686\t|\tloss: 29.7696\n",
      "Training Epoch 5  37.5% | batch:       257 of       686\t|\tloss: 15.0899\n",
      "Training Epoch 5  37.6% | batch:       258 of       686\t|\tloss: 23.5814\n",
      "Training Epoch 5  37.8% | batch:       259 of       686\t|\tloss: 19.7396\n",
      "Training Epoch 5  37.9% | batch:       260 of       686\t|\tloss: 19.7649\n",
      "Training Epoch 5  38.0% | batch:       261 of       686\t|\tloss: 33.0756\n",
      "Training Epoch 5  38.2% | batch:       262 of       686\t|\tloss: 21.6432\n",
      "Training Epoch 5  38.3% | batch:       263 of       686\t|\tloss: 20.9171\n",
      "Training Epoch 5  38.5% | batch:       264 of       686\t|\tloss: 25.6959\n",
      "Training Epoch 5  38.6% | batch:       265 of       686\t|\tloss: 23.2076\n",
      "Training Epoch 5  38.8% | batch:       266 of       686\t|\tloss: 27.9282\n",
      "Training Epoch 5  38.9% | batch:       267 of       686\t|\tloss: 25.9192\n",
      "Training Epoch 5  39.1% | batch:       268 of       686\t|\tloss: 29.9844\n",
      "Training Epoch 5  39.2% | batch:       269 of       686\t|\tloss: 26.8645\n",
      "Training Epoch 5  39.4% | batch:       270 of       686\t|\tloss: 29.1016\n",
      "Training Epoch 5  39.5% | batch:       271 of       686\t|\tloss: 37.2197\n",
      "Training Epoch 5  39.7% | batch:       272 of       686\t|\tloss: 24.4992\n",
      "Training Epoch 5  39.8% | batch:       273 of       686\t|\tloss: 23.2965\n",
      "Training Epoch 5  39.9% | batch:       274 of       686\t|\tloss: 20.8994\n",
      "Training Epoch 5  40.1% | batch:       275 of       686\t|\tloss: 23.0234\n",
      "Training Epoch 5  40.2% | batch:       276 of       686\t|\tloss: 22.7108\n",
      "Training Epoch 5  40.4% | batch:       277 of       686\t|\tloss: 23.4328\n",
      "Training Epoch 5  40.5% | batch:       278 of       686\t|\tloss: 21.7273\n",
      "Training Epoch 5  40.7% | batch:       279 of       686\t|\tloss: 19.5525\n",
      "Training Epoch 5  40.8% | batch:       280 of       686\t|\tloss: 21.5507\n",
      "Training Epoch 5  41.0% | batch:       281 of       686\t|\tloss: 24.9376\n",
      "Training Epoch 5  41.1% | batch:       282 of       686\t|\tloss: 24.0819\n",
      "Training Epoch 5  41.3% | batch:       283 of       686\t|\tloss: 28.1752\n",
      "Training Epoch 5  41.4% | batch:       284 of       686\t|\tloss: 24.6354\n",
      "Training Epoch 5  41.5% | batch:       285 of       686\t|\tloss: 17.3691\n",
      "Training Epoch 5  41.7% | batch:       286 of       686\t|\tloss: 22.592\n",
      "Training Epoch 5  41.8% | batch:       287 of       686\t|\tloss: 22.4085\n",
      "Training Epoch 5  42.0% | batch:       288 of       686\t|\tloss: 23.0886\n",
      "Training Epoch 5  42.1% | batch:       289 of       686\t|\tloss: 24.0774\n",
      "Training Epoch 5  42.3% | batch:       290 of       686\t|\tloss: 19.2642\n",
      "Training Epoch 5  42.4% | batch:       291 of       686\t|\tloss: 27.9683\n",
      "Training Epoch 5  42.6% | batch:       292 of       686\t|\tloss: 27.0856\n",
      "Training Epoch 5  42.7% | batch:       293 of       686\t|\tloss: 22.1476\n",
      "Training Epoch 5  42.9% | batch:       294 of       686\t|\tloss: 19.8662\n",
      "Training Epoch 5  43.0% | batch:       295 of       686\t|\tloss: 21.2885\n",
      "Training Epoch 5  43.1% | batch:       296 of       686\t|\tloss: 18.338\n",
      "Training Epoch 5  43.3% | batch:       297 of       686\t|\tloss: 23.2633\n",
      "Training Epoch 5  43.4% | batch:       298 of       686\t|\tloss: 19.3622\n",
      "Training Epoch 5  43.6% | batch:       299 of       686\t|\tloss: 22.357\n",
      "Training Epoch 5  43.7% | batch:       300 of       686\t|\tloss: 17.5683\n",
      "Training Epoch 5  43.9% | batch:       301 of       686\t|\tloss: 23.036\n",
      "Training Epoch 5  44.0% | batch:       302 of       686\t|\tloss: 23.3699\n",
      "Training Epoch 5  44.2% | batch:       303 of       686\t|\tloss: 24.9711\n",
      "Training Epoch 5  44.3% | batch:       304 of       686\t|\tloss: 23.7655\n",
      "Training Epoch 5  44.5% | batch:       305 of       686\t|\tloss: 21.5501\n",
      "Training Epoch 5  44.6% | batch:       306 of       686\t|\tloss: 26.0971\n",
      "Training Epoch 5  44.8% | batch:       307 of       686\t|\tloss: 28.2658\n",
      "Training Epoch 5  44.9% | batch:       308 of       686\t|\tloss: 21.7726\n",
      "Training Epoch 5  45.0% | batch:       309 of       686\t|\tloss: 30.9068\n",
      "Training Epoch 5  45.2% | batch:       310 of       686\t|\tloss: 19.4689\n",
      "Training Epoch 5  45.3% | batch:       311 of       686\t|\tloss: 28.2281\n",
      "Training Epoch 5  45.5% | batch:       312 of       686\t|\tloss: 25.5183\n",
      "Training Epoch 5  45.6% | batch:       313 of       686\t|\tloss: 22.7401\n",
      "Training Epoch 5  45.8% | batch:       314 of       686\t|\tloss: 25.9993\n",
      "Training Epoch 5  45.9% | batch:       315 of       686\t|\tloss: 24.5425\n",
      "Training Epoch 5  46.1% | batch:       316 of       686\t|\tloss: 25.4816\n",
      "Training Epoch 5  46.2% | batch:       317 of       686\t|\tloss: 24.4375\n",
      "Training Epoch 5  46.4% | batch:       318 of       686\t|\tloss: 23.4918\n",
      "Training Epoch 5  46.5% | batch:       319 of       686\t|\tloss: 20.1307\n",
      "Training Epoch 5  46.6% | batch:       320 of       686\t|\tloss: 31.4729\n",
      "Training Epoch 5  46.8% | batch:       321 of       686\t|\tloss: 17.4589\n",
      "Training Epoch 5  46.9% | batch:       322 of       686\t|\tloss: 24.3359\n",
      "Training Epoch 5  47.1% | batch:       323 of       686\t|\tloss: 29.7236\n",
      "Training Epoch 5  47.2% | batch:       324 of       686\t|\tloss: 25.8531\n",
      "Training Epoch 5  47.4% | batch:       325 of       686\t|\tloss: 22.8306\n",
      "Training Epoch 5  47.5% | batch:       326 of       686\t|\tloss: 23.4669\n",
      "Training Epoch 5  47.7% | batch:       327 of       686\t|\tloss: 27.2442\n",
      "Training Epoch 5  47.8% | batch:       328 of       686\t|\tloss: 24.1272\n",
      "Training Epoch 5  48.0% | batch:       329 of       686\t|\tloss: 24.2288\n",
      "Training Epoch 5  48.1% | batch:       330 of       686\t|\tloss: 79.2942\n",
      "Training Epoch 5  48.3% | batch:       331 of       686\t|\tloss: 24.376\n",
      "Training Epoch 5  48.4% | batch:       332 of       686\t|\tloss: 23.497\n",
      "Training Epoch 5  48.5% | batch:       333 of       686\t|\tloss: 22.9388\n",
      "Training Epoch 5  48.7% | batch:       334 of       686\t|\tloss: 47.9893\n",
      "Training Epoch 5  48.8% | batch:       335 of       686\t|\tloss: 21.2847\n",
      "Training Epoch 5  49.0% | batch:       336 of       686\t|\tloss: 20.726\n",
      "Training Epoch 5  49.1% | batch:       337 of       686\t|\tloss: 21.1618\n",
      "Training Epoch 5  49.3% | batch:       338 of       686\t|\tloss: 22.4137\n",
      "Training Epoch 5  49.4% | batch:       339 of       686\t|\tloss: 22.1832\n",
      "Training Epoch 5  49.6% | batch:       340 of       686\t|\tloss: 24.5738\n",
      "Training Epoch 5  49.7% | batch:       341 of       686\t|\tloss: 17.9951\n",
      "Training Epoch 5  49.9% | batch:       342 of       686\t|\tloss: 21.5902\n",
      "Training Epoch 5  50.0% | batch:       343 of       686\t|\tloss: 33.5996\n",
      "Training Epoch 5  50.1% | batch:       344 of       686\t|\tloss: 22.1701\n",
      "Training Epoch 5  50.3% | batch:       345 of       686\t|\tloss: 28.3326\n",
      "Training Epoch 5  50.4% | batch:       346 of       686\t|\tloss: 25.9617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  50.6% | batch:       347 of       686\t|\tloss: 32.8392\n",
      "Training Epoch 5  50.7% | batch:       348 of       686\t|\tloss: 24.5075\n",
      "Training Epoch 5  50.9% | batch:       349 of       686\t|\tloss: 22.8053\n",
      "Training Epoch 5  51.0% | batch:       350 of       686\t|\tloss: 18.6293\n",
      "Training Epoch 5  51.2% | batch:       351 of       686\t|\tloss: 22.0517\n",
      "Training Epoch 5  51.3% | batch:       352 of       686\t|\tloss: 22.2482\n",
      "Training Epoch 5  51.5% | batch:       353 of       686\t|\tloss: 33.0774\n",
      "Training Epoch 5  51.6% | batch:       354 of       686\t|\tloss: 20.3593\n",
      "Training Epoch 5  51.7% | batch:       355 of       686\t|\tloss: 22.2065\n",
      "Training Epoch 5  51.9% | batch:       356 of       686\t|\tloss: 34.5089\n",
      "Training Epoch 5  52.0% | batch:       357 of       686\t|\tloss: 21.9702\n",
      "Training Epoch 5  52.2% | batch:       358 of       686\t|\tloss: 17.9011\n",
      "Training Epoch 5  52.3% | batch:       359 of       686\t|\tloss: 17.3212\n",
      "Training Epoch 5  52.5% | batch:       360 of       686\t|\tloss: 21.556\n",
      "Training Epoch 5  52.6% | batch:       361 of       686\t|\tloss: 26.0088\n",
      "Training Epoch 5  52.8% | batch:       362 of       686\t|\tloss: 23.6043\n",
      "Training Epoch 5  52.9% | batch:       363 of       686\t|\tloss: 32.1679\n",
      "Training Epoch 5  53.1% | batch:       364 of       686\t|\tloss: 22.0162\n",
      "Training Epoch 5  53.2% | batch:       365 of       686\t|\tloss: 19.0269\n",
      "Training Epoch 5  53.4% | batch:       366 of       686\t|\tloss: 21.6871\n",
      "Training Epoch 5  53.5% | batch:       367 of       686\t|\tloss: 18.5676\n",
      "Training Epoch 5  53.6% | batch:       368 of       686\t|\tloss: 24.2916\n",
      "Training Epoch 5  53.8% | batch:       369 of       686\t|\tloss: 21.3567\n",
      "Training Epoch 5  53.9% | batch:       370 of       686\t|\tloss: 44.6144\n",
      "Training Epoch 5  54.1% | batch:       371 of       686\t|\tloss: 28.0648\n",
      "Training Epoch 5  54.2% | batch:       372 of       686\t|\tloss: 24.206\n",
      "Training Epoch 5  54.4% | batch:       373 of       686\t|\tloss: 30.4496\n",
      "Training Epoch 5  54.5% | batch:       374 of       686\t|\tloss: 21.9081\n",
      "Training Epoch 5  54.7% | batch:       375 of       686\t|\tloss: 24.8392\n",
      "Training Epoch 5  54.8% | batch:       376 of       686\t|\tloss: 20.286\n",
      "Training Epoch 5  55.0% | batch:       377 of       686\t|\tloss: 22.4327\n",
      "Training Epoch 5  55.1% | batch:       378 of       686\t|\tloss: 20.4442\n",
      "Training Epoch 5  55.2% | batch:       379 of       686\t|\tloss: 27.0861\n",
      "Training Epoch 5  55.4% | batch:       380 of       686\t|\tloss: 20.2079\n",
      "Training Epoch 5  55.5% | batch:       381 of       686\t|\tloss: 21.9517\n",
      "Training Epoch 5  55.7% | batch:       382 of       686\t|\tloss: 21.0635\n",
      "Training Epoch 5  55.8% | batch:       383 of       686\t|\tloss: 16.5072\n",
      "Training Epoch 5  56.0% | batch:       384 of       686\t|\tloss: 20.9401\n",
      "Training Epoch 5  56.1% | batch:       385 of       686\t|\tloss: 21.3818\n",
      "Training Epoch 5  56.3% | batch:       386 of       686\t|\tloss: 24.8522\n",
      "Training Epoch 5  56.4% | batch:       387 of       686\t|\tloss: 24.5822\n",
      "Training Epoch 5  56.6% | batch:       388 of       686\t|\tloss: 27.4887\n",
      "Training Epoch 5  56.7% | batch:       389 of       686\t|\tloss: 21.0537\n",
      "Training Epoch 5  56.9% | batch:       390 of       686\t|\tloss: 28.0339\n",
      "Training Epoch 5  57.0% | batch:       391 of       686\t|\tloss: 31.3711\n",
      "Training Epoch 5  57.1% | batch:       392 of       686\t|\tloss: 23.4587\n",
      "Training Epoch 5  57.3% | batch:       393 of       686\t|\tloss: 22.0685\n",
      "Training Epoch 5  57.4% | batch:       394 of       686\t|\tloss: 20.6728\n",
      "Training Epoch 5  57.6% | batch:       395 of       686\t|\tloss: 17.8783\n",
      "Training Epoch 5  57.7% | batch:       396 of       686\t|\tloss: 21.3731\n",
      "Training Epoch 5  57.9% | batch:       397 of       686\t|\tloss: 17.1334\n",
      "Training Epoch 5  58.0% | batch:       398 of       686\t|\tloss: 19.2657\n",
      "Training Epoch 5  58.2% | batch:       399 of       686\t|\tloss: 18.3055\n",
      "Training Epoch 5  58.3% | batch:       400 of       686\t|\tloss: 23.8265\n",
      "Training Epoch 5  58.5% | batch:       401 of       686\t|\tloss: 21.8204\n",
      "Training Epoch 5  58.6% | batch:       402 of       686\t|\tloss: 18.5121\n",
      "Training Epoch 5  58.7% | batch:       403 of       686\t|\tloss: 31.2086\n",
      "Training Epoch 5  58.9% | batch:       404 of       686\t|\tloss: 17.4573\n",
      "Training Epoch 5  59.0% | batch:       405 of       686\t|\tloss: 18.8071\n",
      "Training Epoch 5  59.2% | batch:       406 of       686\t|\tloss: 31.3087\n",
      "Training Epoch 5  59.3% | batch:       407 of       686\t|\tloss: 20.6285\n",
      "Training Epoch 5  59.5% | batch:       408 of       686\t|\tloss: 28.6138\n",
      "Training Epoch 5  59.6% | batch:       409 of       686\t|\tloss: 31.7409\n",
      "Training Epoch 5  59.8% | batch:       410 of       686\t|\tloss: 17.4502\n",
      "Training Epoch 5  59.9% | batch:       411 of       686\t|\tloss: 23.4379\n",
      "Training Epoch 5  60.1% | batch:       412 of       686\t|\tloss: 27.0425\n",
      "Training Epoch 5  60.2% | batch:       413 of       686\t|\tloss: 25.2118\n",
      "Training Epoch 5  60.3% | batch:       414 of       686\t|\tloss: 19.0376\n",
      "Training Epoch 5  60.5% | batch:       415 of       686\t|\tloss: 16.8206\n",
      "Training Epoch 5  60.6% | batch:       416 of       686\t|\tloss: 20.6711\n",
      "Training Epoch 5  60.8% | batch:       417 of       686\t|\tloss: 26.5055\n",
      "Training Epoch 5  60.9% | batch:       418 of       686\t|\tloss: 23.1108\n",
      "Training Epoch 5  61.1% | batch:       419 of       686\t|\tloss: 14.5775\n",
      "Training Epoch 5  61.2% | batch:       420 of       686\t|\tloss: 25.165\n",
      "Training Epoch 5  61.4% | batch:       421 of       686\t|\tloss: 20.084\n",
      "Training Epoch 5  61.5% | batch:       422 of       686\t|\tloss: 20.3175\n",
      "Training Epoch 5  61.7% | batch:       423 of       686\t|\tloss: 33.1431\n",
      "Training Epoch 5  61.8% | batch:       424 of       686\t|\tloss: 21.61\n",
      "Training Epoch 5  62.0% | batch:       425 of       686\t|\tloss: 23.1569\n",
      "Training Epoch 5  62.1% | batch:       426 of       686\t|\tloss: 20.9669\n",
      "Training Epoch 5  62.2% | batch:       427 of       686\t|\tloss: 34.3922\n",
      "Training Epoch 5  62.4% | batch:       428 of       686\t|\tloss: 27.3396\n",
      "Training Epoch 5  62.5% | batch:       429 of       686\t|\tloss: 22.0358\n",
      "Training Epoch 5  62.7% | batch:       430 of       686\t|\tloss: 22.2109\n",
      "Training Epoch 5  62.8% | batch:       431 of       686\t|\tloss: 26.1254\n",
      "Training Epoch 5  63.0% | batch:       432 of       686\t|\tloss: 21.0222\n",
      "Training Epoch 5  63.1% | batch:       433 of       686\t|\tloss: 22.6422\n",
      "Training Epoch 5  63.3% | batch:       434 of       686\t|\tloss: 21.5898\n",
      "Training Epoch 5  63.4% | batch:       435 of       686\t|\tloss: 21.0708\n",
      "Training Epoch 5  63.6% | batch:       436 of       686\t|\tloss: 21.7264\n",
      "Training Epoch 5  63.7% | batch:       437 of       686\t|\tloss: 28.3325\n",
      "Training Epoch 5  63.8% | batch:       438 of       686\t|\tloss: 17.4308\n",
      "Training Epoch 5  64.0% | batch:       439 of       686\t|\tloss: 20.1843\n",
      "Training Epoch 5  64.1% | batch:       440 of       686\t|\tloss: 24.6272\n",
      "Training Epoch 5  64.3% | batch:       441 of       686\t|\tloss: 20.0262\n",
      "Training Epoch 5  64.4% | batch:       442 of       686\t|\tloss: 22.3755\n",
      "Training Epoch 5  64.6% | batch:       443 of       686\t|\tloss: 20.141\n",
      "Training Epoch 5  64.7% | batch:       444 of       686\t|\tloss: 21.119\n",
      "Training Epoch 5  64.9% | batch:       445 of       686\t|\tloss: 22.4327\n",
      "Training Epoch 5  65.0% | batch:       446 of       686\t|\tloss: 25.8899\n",
      "Training Epoch 5  65.2% | batch:       447 of       686\t|\tloss: 15.701\n",
      "Training Epoch 5  65.3% | batch:       448 of       686\t|\tloss: 31.3458\n",
      "Training Epoch 5  65.5% | batch:       449 of       686\t|\tloss: 27.311\n",
      "Training Epoch 5  65.6% | batch:       450 of       686\t|\tloss: 23.4266\n",
      "Training Epoch 5  65.7% | batch:       451 of       686\t|\tloss: 23.9032\n",
      "Training Epoch 5  65.9% | batch:       452 of       686\t|\tloss: 27.7443\n",
      "Training Epoch 5  66.0% | batch:       453 of       686\t|\tloss: 18.7425\n",
      "Training Epoch 5  66.2% | batch:       454 of       686\t|\tloss: 23.1493\n",
      "Training Epoch 5  66.3% | batch:       455 of       686\t|\tloss: 22.2771\n",
      "Training Epoch 5  66.5% | batch:       456 of       686\t|\tloss: 21.4237\n",
      "Training Epoch 5  66.6% | batch:       457 of       686\t|\tloss: 23.8035\n",
      "Training Epoch 5  66.8% | batch:       458 of       686\t|\tloss: 23.6754\n",
      "Training Epoch 5  66.9% | batch:       459 of       686\t|\tloss: 27.5252\n",
      "Training Epoch 5  67.1% | batch:       460 of       686\t|\tloss: 20.1712\n",
      "Training Epoch 5  67.2% | batch:       461 of       686\t|\tloss: 26.0042\n",
      "Training Epoch 5  67.3% | batch:       462 of       686\t|\tloss: 23.0709\n",
      "Training Epoch 5  67.5% | batch:       463 of       686\t|\tloss: 19.3199\n",
      "Training Epoch 5  67.6% | batch:       464 of       686\t|\tloss: 23.5578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  67.8% | batch:       465 of       686\t|\tloss: 21.0307\n",
      "Training Epoch 5  67.9% | batch:       466 of       686\t|\tloss: 21.1486\n",
      "Training Epoch 5  68.1% | batch:       467 of       686\t|\tloss: 24.2673\n",
      "Training Epoch 5  68.2% | batch:       468 of       686\t|\tloss: 21.3611\n",
      "Training Epoch 5  68.4% | batch:       469 of       686\t|\tloss: 19.2382\n",
      "Training Epoch 5  68.5% | batch:       470 of       686\t|\tloss: 20.1957\n",
      "Training Epoch 5  68.7% | batch:       471 of       686\t|\tloss: 24.3724\n",
      "Training Epoch 5  68.8% | batch:       472 of       686\t|\tloss: 21.0619\n",
      "Training Epoch 5  69.0% | batch:       473 of       686\t|\tloss: 23.9012\n",
      "Training Epoch 5  69.1% | batch:       474 of       686\t|\tloss: 24.7982\n",
      "Training Epoch 5  69.2% | batch:       475 of       686\t|\tloss: 25.4507\n",
      "Training Epoch 5  69.4% | batch:       476 of       686\t|\tloss: 16.0183\n",
      "Training Epoch 5  69.5% | batch:       477 of       686\t|\tloss: 17.9608\n",
      "Training Epoch 5  69.7% | batch:       478 of       686\t|\tloss: 26.7652\n",
      "Training Epoch 5  69.8% | batch:       479 of       686\t|\tloss: 42.6987\n",
      "Training Epoch 5  70.0% | batch:       480 of       686\t|\tloss: 23.6065\n",
      "Training Epoch 5  70.1% | batch:       481 of       686\t|\tloss: 24.161\n",
      "Training Epoch 5  70.3% | batch:       482 of       686\t|\tloss: 27.0183\n",
      "Training Epoch 5  70.4% | batch:       483 of       686\t|\tloss: 21.3855\n",
      "Training Epoch 5  70.6% | batch:       484 of       686\t|\tloss: 15.622\n",
      "Training Epoch 5  70.7% | batch:       485 of       686\t|\tloss: 18.6258\n",
      "Training Epoch 5  70.8% | batch:       486 of       686\t|\tloss: 24.3642\n",
      "Training Epoch 5  71.0% | batch:       487 of       686\t|\tloss: 21.4677\n",
      "Training Epoch 5  71.1% | batch:       488 of       686\t|\tloss: 18.2985\n",
      "Training Epoch 5  71.3% | batch:       489 of       686\t|\tloss: 27.6126\n",
      "Training Epoch 5  71.4% | batch:       490 of       686\t|\tloss: 21.4706\n",
      "Training Epoch 5  71.6% | batch:       491 of       686\t|\tloss: 27.3929\n",
      "Training Epoch 5  71.7% | batch:       492 of       686\t|\tloss: 25.0231\n",
      "Training Epoch 5  71.9% | batch:       493 of       686\t|\tloss: 22.9518\n",
      "Training Epoch 5  72.0% | batch:       494 of       686\t|\tloss: 28.7239\n",
      "Training Epoch 5  72.2% | batch:       495 of       686\t|\tloss: 21.2902\n",
      "Training Epoch 5  72.3% | batch:       496 of       686\t|\tloss: 47.1926\n",
      "Training Epoch 5  72.4% | batch:       497 of       686\t|\tloss: 15.4963\n",
      "Training Epoch 5  72.6% | batch:       498 of       686\t|\tloss: 56.5148\n",
      "Training Epoch 5  72.7% | batch:       499 of       686\t|\tloss: 23.6017\n",
      "Training Epoch 5  72.9% | batch:       500 of       686\t|\tloss: 24.8446\n",
      "Training Epoch 5  73.0% | batch:       501 of       686\t|\tloss: 22.6503\n",
      "Training Epoch 5  73.2% | batch:       502 of       686\t|\tloss: 21.7129\n",
      "Training Epoch 5  73.3% | batch:       503 of       686\t|\tloss: 26.6876\n",
      "Training Epoch 5  73.5% | batch:       504 of       686\t|\tloss: 27.2576\n",
      "Training Epoch 5  73.6% | batch:       505 of       686\t|\tloss: 17.3874\n",
      "Training Epoch 5  73.8% | batch:       506 of       686\t|\tloss: 40.5268\n",
      "Training Epoch 5  73.9% | batch:       507 of       686\t|\tloss: 26.1734\n",
      "Training Epoch 5  74.1% | batch:       508 of       686\t|\tloss: 22.7852\n",
      "Training Epoch 5  74.2% | batch:       509 of       686\t|\tloss: 28.9501\n",
      "Training Epoch 5  74.3% | batch:       510 of       686\t|\tloss: 23.7362\n",
      "Training Epoch 5  74.5% | batch:       511 of       686\t|\tloss: 18.926\n",
      "Training Epoch 5  74.6% | batch:       512 of       686\t|\tloss: 22.3677\n",
      "Training Epoch 5  74.8% | batch:       513 of       686\t|\tloss: 22.0733\n",
      "Training Epoch 5  74.9% | batch:       514 of       686\t|\tloss: 25.7906\n",
      "Training Epoch 5  75.1% | batch:       515 of       686\t|\tloss: 22.3773\n",
      "Training Epoch 5  75.2% | batch:       516 of       686\t|\tloss: 19.7659\n",
      "Training Epoch 5  75.4% | batch:       517 of       686\t|\tloss: 19.8198\n",
      "Training Epoch 5  75.5% | batch:       518 of       686\t|\tloss: 19.803\n",
      "Training Epoch 5  75.7% | batch:       519 of       686\t|\tloss: 19.0899\n",
      "Training Epoch 5  75.8% | batch:       520 of       686\t|\tloss: 16.5594\n",
      "Training Epoch 5  75.9% | batch:       521 of       686\t|\tloss: 16.5034\n",
      "Training Epoch 5  76.1% | batch:       522 of       686\t|\tloss: 26.6375\n",
      "Training Epoch 5  76.2% | batch:       523 of       686\t|\tloss: 22.5752\n",
      "Training Epoch 5  76.4% | batch:       524 of       686\t|\tloss: 23.4388\n",
      "Training Epoch 5  76.5% | batch:       525 of       686\t|\tloss: 25.3236\n",
      "Training Epoch 5  76.7% | batch:       526 of       686\t|\tloss: 26.2968\n",
      "Training Epoch 5  76.8% | batch:       527 of       686\t|\tloss: 19.5444\n",
      "Training Epoch 5  77.0% | batch:       528 of       686\t|\tloss: 26.4484\n",
      "Training Epoch 5  77.1% | batch:       529 of       686\t|\tloss: 19.8168\n",
      "Training Epoch 5  77.3% | batch:       530 of       686\t|\tloss: 20.7912\n",
      "Training Epoch 5  77.4% | batch:       531 of       686\t|\tloss: 32.1037\n",
      "Training Epoch 5  77.6% | batch:       532 of       686\t|\tloss: 21.6473\n",
      "Training Epoch 5  77.7% | batch:       533 of       686\t|\tloss: 24.0147\n",
      "Training Epoch 5  77.8% | batch:       534 of       686\t|\tloss: 22.8541\n",
      "Training Epoch 5  78.0% | batch:       535 of       686\t|\tloss: 17.9873\n",
      "Training Epoch 5  78.1% | batch:       536 of       686\t|\tloss: 24.126\n",
      "Training Epoch 5  78.3% | batch:       537 of       686\t|\tloss: 19.5461\n",
      "Training Epoch 5  78.4% | batch:       538 of       686\t|\tloss: 21.1372\n",
      "Training Epoch 5  78.6% | batch:       539 of       686\t|\tloss: 26.5547\n",
      "Training Epoch 5  78.7% | batch:       540 of       686\t|\tloss: 28.1704\n",
      "Training Epoch 5  78.9% | batch:       541 of       686\t|\tloss: 24.4032\n",
      "Training Epoch 5  79.0% | batch:       542 of       686\t|\tloss: 23.6074\n",
      "Training Epoch 5  79.2% | batch:       543 of       686\t|\tloss: 33.0766\n",
      "Training Epoch 5  79.3% | batch:       544 of       686\t|\tloss: 14.7605\n",
      "Training Epoch 5  79.4% | batch:       545 of       686\t|\tloss: 22.1649\n",
      "Training Epoch 5  79.6% | batch:       546 of       686\t|\tloss: 28.3621\n",
      "Training Epoch 5  79.7% | batch:       547 of       686\t|\tloss: 17.1747\n",
      "Training Epoch 5  79.9% | batch:       548 of       686\t|\tloss: 22.3927\n",
      "Training Epoch 5  80.0% | batch:       549 of       686\t|\tloss: 25.6245\n",
      "Training Epoch 5  80.2% | batch:       550 of       686\t|\tloss: 20.5053\n",
      "Training Epoch 5  80.3% | batch:       551 of       686\t|\tloss: 21.1434\n",
      "Training Epoch 5  80.5% | batch:       552 of       686\t|\tloss: 14.9715\n",
      "Training Epoch 5  80.6% | batch:       553 of       686\t|\tloss: 18.8716\n",
      "Training Epoch 5  80.8% | batch:       554 of       686\t|\tloss: 18.7612\n",
      "Training Epoch 5  80.9% | batch:       555 of       686\t|\tloss: 20.781\n",
      "Training Epoch 5  81.0% | batch:       556 of       686\t|\tloss: 18.8861\n",
      "Training Epoch 5  81.2% | batch:       557 of       686\t|\tloss: 28.5301\n",
      "Training Epoch 5  81.3% | batch:       558 of       686\t|\tloss: 25.863\n",
      "Training Epoch 5  81.5% | batch:       559 of       686\t|\tloss: 21.0618\n",
      "Training Epoch 5  81.6% | batch:       560 of       686\t|\tloss: 19.9246\n",
      "Training Epoch 5  81.8% | batch:       561 of       686\t|\tloss: 20.5836\n",
      "Training Epoch 5  81.9% | batch:       562 of       686\t|\tloss: 25.052\n",
      "Training Epoch 5  82.1% | batch:       563 of       686\t|\tloss: 23.6812\n",
      "Training Epoch 5  82.2% | batch:       564 of       686\t|\tloss: 22.8817\n",
      "Training Epoch 5  82.4% | batch:       565 of       686\t|\tloss: 26.9737\n",
      "Training Epoch 5  82.5% | batch:       566 of       686\t|\tloss: 21.8299\n",
      "Training Epoch 5  82.7% | batch:       567 of       686\t|\tloss: 17.5266\n",
      "Training Epoch 5  82.8% | batch:       568 of       686\t|\tloss: 22.8491\n",
      "Training Epoch 5  82.9% | batch:       569 of       686\t|\tloss: 21.1118\n",
      "Training Epoch 5  83.1% | batch:       570 of       686\t|\tloss: 14.1001\n",
      "Training Epoch 5  83.2% | batch:       571 of       686\t|\tloss: 19.6108\n",
      "Training Epoch 5  83.4% | batch:       572 of       686\t|\tloss: 21.7088\n",
      "Training Epoch 5  83.5% | batch:       573 of       686\t|\tloss: 18.6782\n",
      "Training Epoch 5  83.7% | batch:       574 of       686\t|\tloss: 22.064\n",
      "Training Epoch 5  83.8% | batch:       575 of       686\t|\tloss: 16.0511\n",
      "Training Epoch 5  84.0% | batch:       576 of       686\t|\tloss: 25.9433\n",
      "Training Epoch 5  84.1% | batch:       577 of       686\t|\tloss: 21.5323\n",
      "Training Epoch 5  84.3% | batch:       578 of       686\t|\tloss: 27.3134\n",
      "Training Epoch 5  84.4% | batch:       579 of       686\t|\tloss: 16.5655\n",
      "Training Epoch 5  84.5% | batch:       580 of       686\t|\tloss: 15.5285\n",
      "Training Epoch 5  84.7% | batch:       581 of       686\t|\tloss: 18.3773\n",
      "Training Epoch 5  84.8% | batch:       582 of       686\t|\tloss: 25.1277\n",
      "Training Epoch 5  85.0% | batch:       583 of       686\t|\tloss: 19.6158\n",
      "Training Epoch 5  85.1% | batch:       584 of       686\t|\tloss: 24.0779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  85.3% | batch:       585 of       686\t|\tloss: 19.2593\n",
      "Training Epoch 5  85.4% | batch:       586 of       686\t|\tloss: 23.567\n",
      "Training Epoch 5  85.6% | batch:       587 of       686\t|\tloss: 22.9141\n",
      "Training Epoch 5  85.7% | batch:       588 of       686\t|\tloss: 19.3484\n",
      "Training Epoch 5  85.9% | batch:       589 of       686\t|\tloss: 18.6168\n",
      "Training Epoch 5  86.0% | batch:       590 of       686\t|\tloss: 25.296\n",
      "Training Epoch 5  86.2% | batch:       591 of       686\t|\tloss: 19.6939\n",
      "Training Epoch 5  86.3% | batch:       592 of       686\t|\tloss: 21.7711\n",
      "Training Epoch 5  86.4% | batch:       593 of       686\t|\tloss: 20.3918\n",
      "Training Epoch 5  86.6% | batch:       594 of       686\t|\tloss: 20.8731\n",
      "Training Epoch 5  86.7% | batch:       595 of       686\t|\tloss: 22.5527\n",
      "Training Epoch 5  86.9% | batch:       596 of       686\t|\tloss: 16.9282\n",
      "Training Epoch 5  87.0% | batch:       597 of       686\t|\tloss: 20.5686\n",
      "Training Epoch 5  87.2% | batch:       598 of       686\t|\tloss: 23.6639\n",
      "Training Epoch 5  87.3% | batch:       599 of       686\t|\tloss: 26.9325\n",
      "Training Epoch 5  87.5% | batch:       600 of       686\t|\tloss: 20.1487\n",
      "Training Epoch 5  87.6% | batch:       601 of       686\t|\tloss: 16.8367\n",
      "Training Epoch 5  87.8% | batch:       602 of       686\t|\tloss: 18.0367\n",
      "Training Epoch 5  87.9% | batch:       603 of       686\t|\tloss: 19.0428\n",
      "Training Epoch 5  88.0% | batch:       604 of       686\t|\tloss: 16.9519\n",
      "Training Epoch 5  88.2% | batch:       605 of       686\t|\tloss: 22.4067\n",
      "Training Epoch 5  88.3% | batch:       606 of       686\t|\tloss: 18.963\n",
      "Training Epoch 5  88.5% | batch:       607 of       686\t|\tloss: 18.1469\n",
      "Training Epoch 5  88.6% | batch:       608 of       686\t|\tloss: 22.0734\n",
      "Training Epoch 5  88.8% | batch:       609 of       686\t|\tloss: 19.6454\n",
      "Training Epoch 5  88.9% | batch:       610 of       686\t|\tloss: 24.4332\n",
      "Training Epoch 5  89.1% | batch:       611 of       686\t|\tloss: 19.4905\n",
      "Training Epoch 5  89.2% | batch:       612 of       686\t|\tloss: 20.721\n",
      "Training Epoch 5  89.4% | batch:       613 of       686\t|\tloss: 20.0362\n",
      "Training Epoch 5  89.5% | batch:       614 of       686\t|\tloss: 19.1683\n",
      "Training Epoch 5  89.7% | batch:       615 of       686\t|\tloss: 22.9943\n",
      "Training Epoch 5  89.8% | batch:       616 of       686\t|\tloss: 21.8409\n",
      "Training Epoch 5  89.9% | batch:       617 of       686\t|\tloss: 21.1932\n",
      "Training Epoch 5  90.1% | batch:       618 of       686\t|\tloss: 23.4841\n",
      "Training Epoch 5  90.2% | batch:       619 of       686\t|\tloss: 19.6001\n",
      "Training Epoch 5  90.4% | batch:       620 of       686\t|\tloss: 22.0644\n",
      "Training Epoch 5  90.5% | batch:       621 of       686\t|\tloss: 23.5312\n",
      "Training Epoch 5  90.7% | batch:       622 of       686\t|\tloss: 17.792\n",
      "Training Epoch 5  90.8% | batch:       623 of       686\t|\tloss: 21.5324\n",
      "Training Epoch 5  91.0% | batch:       624 of       686\t|\tloss: 15.3722\n",
      "Training Epoch 5  91.1% | batch:       625 of       686\t|\tloss: 19.9425\n",
      "Training Epoch 5  91.3% | batch:       626 of       686\t|\tloss: 19.6525\n",
      "Training Epoch 5  91.4% | batch:       627 of       686\t|\tloss: 15.1442\n",
      "Training Epoch 5  91.5% | batch:       628 of       686\t|\tloss: 39.4262\n",
      "Training Epoch 5  91.7% | batch:       629 of       686\t|\tloss: 22.265\n",
      "Training Epoch 5  91.8% | batch:       630 of       686\t|\tloss: 23.0438\n",
      "Training Epoch 5  92.0% | batch:       631 of       686\t|\tloss: 24.8277\n",
      "Training Epoch 5  92.1% | batch:       632 of       686\t|\tloss: 24.2305\n",
      "Training Epoch 5  92.3% | batch:       633 of       686\t|\tloss: 49.8924\n",
      "Training Epoch 5  92.4% | batch:       634 of       686\t|\tloss: 22.8116\n",
      "Training Epoch 5  92.6% | batch:       635 of       686\t|\tloss: 23.0284\n",
      "Training Epoch 5  92.7% | batch:       636 of       686\t|\tloss: 22.693\n",
      "Training Epoch 5  92.9% | batch:       637 of       686\t|\tloss: 24.8705\n",
      "Training Epoch 5  93.0% | batch:       638 of       686\t|\tloss: 14.7093\n",
      "Training Epoch 5  93.1% | batch:       639 of       686\t|\tloss: 20.7004\n",
      "Training Epoch 5  93.3% | batch:       640 of       686\t|\tloss: 25.8648\n",
      "Training Epoch 5  93.4% | batch:       641 of       686\t|\tloss: 19.0029\n",
      "Training Epoch 5  93.6% | batch:       642 of       686\t|\tloss: 14.4048\n",
      "Training Epoch 5  93.7% | batch:       643 of       686\t|\tloss: 19.9349\n",
      "Training Epoch 5  93.9% | batch:       644 of       686\t|\tloss: 23.8279\n",
      "Training Epoch 5  94.0% | batch:       645 of       686\t|\tloss: 22.1786\n",
      "Training Epoch 5  94.2% | batch:       646 of       686\t|\tloss: 14.8468\n",
      "Training Epoch 5  94.3% | batch:       647 of       686\t|\tloss: 22.801\n",
      "Training Epoch 5  94.5% | batch:       648 of       686\t|\tloss: 19.787\n",
      "Training Epoch 5  94.6% | batch:       649 of       686\t|\tloss: 20.1049\n",
      "Training Epoch 5  94.8% | batch:       650 of       686\t|\tloss: 23.1319\n",
      "Training Epoch 5  94.9% | batch:       651 of       686\t|\tloss: 26.1125\n",
      "Training Epoch 5  95.0% | batch:       652 of       686\t|\tloss: 22.9774\n",
      "Training Epoch 5  95.2% | batch:       653 of       686\t|\tloss: 13.4196\n",
      "Training Epoch 5  95.3% | batch:       654 of       686\t|\tloss: 27.4336\n",
      "Training Epoch 5  95.5% | batch:       655 of       686\t|\tloss: 23.0054\n",
      "Training Epoch 5  95.6% | batch:       656 of       686\t|\tloss: 17.1588\n",
      "Training Epoch 5  95.8% | batch:       657 of       686\t|\tloss: 17.9692\n",
      "Training Epoch 5  95.9% | batch:       658 of       686\t|\tloss: 23.845\n",
      "Training Epoch 5  96.1% | batch:       659 of       686\t|\tloss: 21.3457\n",
      "Training Epoch 5  96.2% | batch:       660 of       686\t|\tloss: 19.9901\n",
      "Training Epoch 5  96.4% | batch:       661 of       686\t|\tloss: 17.9187\n",
      "Training Epoch 5  96.5% | batch:       662 of       686\t|\tloss: 20.0486\n",
      "Training Epoch 5  96.6% | batch:       663 of       686\t|\tloss: 21.8385\n",
      "Training Epoch 5  96.8% | batch:       664 of       686\t|\tloss: 21.5134\n",
      "Training Epoch 5  96.9% | batch:       665 of       686\t|\tloss: 16.9888\n",
      "Training Epoch 5  97.1% | batch:       666 of       686\t|\tloss: 20.2939\n",
      "Training Epoch 5  97.2% | batch:       667 of       686\t|\tloss: 24.3784\n",
      "Training Epoch 5  97.4% | batch:       668 of       686\t|\tloss: 15.2929\n",
      "Training Epoch 5  97.5% | batch:       669 of       686\t|\tloss: 23.0657\n",
      "Training Epoch 5  97.7% | batch:       670 of       686\t|\tloss: 17.4106\n",
      "Training Epoch 5  97.8% | batch:       671 of       686\t|\tloss: 19.8421\n",
      "Training Epoch 5  98.0% | batch:       672 of       686\t|\tloss: 53.9323\n",
      "Training Epoch 5  98.1% | batch:       673 of       686\t|\tloss: 18.8594\n",
      "Training Epoch 5  98.3% | batch:       674 of       686\t|\tloss: 19.2854\n",
      "Training Epoch 5  98.4% | batch:       675 of       686\t|\tloss: 20.3307\n",
      "Training Epoch 5  98.5% | batch:       676 of       686\t|\tloss: 20.6768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:02:04,683 | INFO : Epoch 5 Training Summary: epoch: 5.000000 | loss: 24.114259 | \n",
      "2023-05-25 22:02:04,685 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 21.0378839969635 seconds\n",
      "\n",
      "2023-05-25 22:02:04,685 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.29063787460327 seconds\n",
      "2023-05-25 22:02:04,686 | INFO : Avg batch train. time: 0.03395136716414471 seconds\n",
      "2023-05-25 22:02:04,687 | INFO : Avg sample train. time: 0.0002655868393249703 seconds\n",
      "2023-05-25 22:02:04,687 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  98.7% | batch:       677 of       686\t|\tloss: 20.487\n",
      "Training Epoch 5  98.8% | batch:       678 of       686\t|\tloss: 26.3693\n",
      "Training Epoch 5  99.0% | batch:       679 of       686\t|\tloss: 37.5254\n",
      "Training Epoch 5  99.1% | batch:       680 of       686\t|\tloss: 20.7348\n",
      "Training Epoch 5  99.3% | batch:       681 of       686\t|\tloss: 19.9036\n",
      "Training Epoch 5  99.4% | batch:       682 of       686\t|\tloss: 20.2935\n",
      "Training Epoch 5  99.6% | batch:       683 of       686\t|\tloss: 16.9445\n",
      "Training Epoch 5  99.7% | batch:       684 of       686\t|\tloss: 39.1421\n",
      "Training Epoch 5  99.9% | batch:       685 of       686\t|\tloss: 14.2341\n",
      "\n",
      "Evaluating Epoch 5   0.0% | batch:         0 of       172\t|\tloss: 1.37335\n",
      "Evaluating Epoch 5   0.6% | batch:         1 of       172\t|\tloss: 3.00703\n",
      "Evaluating Epoch 5   1.2% | batch:         2 of       172\t|\tloss: 2.77003\n",
      "Evaluating Epoch 5   1.7% | batch:         3 of       172\t|\tloss: 3.36494\n",
      "Evaluating Epoch 5   2.3% | batch:         4 of       172\t|\tloss: 3.25486\n",
      "Evaluating Epoch 5   2.9% | batch:         5 of       172\t|\tloss: 2.12139\n",
      "Evaluating Epoch 5   3.5% | batch:         6 of       172\t|\tloss: 2.76453\n",
      "Evaluating Epoch 5   4.1% | batch:         7 of       172\t|\tloss: 3.76294\n",
      "Evaluating Epoch 5   4.7% | batch:         8 of       172\t|\tloss: 1.95626\n",
      "Evaluating Epoch 5   5.2% | batch:         9 of       172\t|\tloss: 2.45853\n",
      "Evaluating Epoch 5   5.8% | batch:        10 of       172\t|\tloss: 3.62418\n",
      "Evaluating Epoch 5   6.4% | batch:        11 of       172\t|\tloss: 2.46199\n",
      "Evaluating Epoch 5   7.0% | batch:        12 of       172\t|\tloss: 3.36241\n",
      "Evaluating Epoch 5   7.6% | batch:        13 of       172\t|\tloss: 1.88851\n",
      "Evaluating Epoch 5   8.1% | batch:        14 of       172\t|\tloss: 3.72645\n",
      "Evaluating Epoch 5   8.7% | batch:        15 of       172\t|\tloss: 1.38369\n",
      "Evaluating Epoch 5   9.3% | batch:        16 of       172\t|\tloss: 3.59167\n",
      "Evaluating Epoch 5   9.9% | batch:        17 of       172\t|\tloss: 2.19097\n",
      "Evaluating Epoch 5  10.5% | batch:        18 of       172\t|\tloss: 7.19946\n",
      "Evaluating Epoch 5  11.0% | batch:        19 of       172\t|\tloss: 0.994122\n",
      "Evaluating Epoch 5  11.6% | batch:        20 of       172\t|\tloss: 5.41007\n",
      "Evaluating Epoch 5  12.2% | batch:        21 of       172\t|\tloss: 0.931121\n",
      "Evaluating Epoch 5  12.8% | batch:        22 of       172\t|\tloss: 1.44029\n",
      "Evaluating Epoch 5  13.4% | batch:        23 of       172\t|\tloss: 1.17241\n",
      "Evaluating Epoch 5  14.0% | batch:        24 of       172\t|\tloss: 3.91124\n",
      "Evaluating Epoch 5  14.5% | batch:        25 of       172\t|\tloss: 4.91006\n",
      "Evaluating Epoch 5  15.1% | batch:        26 of       172\t|\tloss: 7.12805\n",
      "Evaluating Epoch 5  15.7% | batch:        27 of       172\t|\tloss: 6.11264\n",
      "Evaluating Epoch 5  16.3% | batch:        28 of       172\t|\tloss: 0.888265\n",
      "Evaluating Epoch 5  16.9% | batch:        29 of       172\t|\tloss: 4.8155\n",
      "Evaluating Epoch 5  17.4% | batch:        30 of       172\t|\tloss: 1.20966\n",
      "Evaluating Epoch 5  18.0% | batch:        31 of       172\t|\tloss: 6.30478\n",
      "Evaluating Epoch 5  18.6% | batch:        32 of       172\t|\tloss: 1.73055\n",
      "Evaluating Epoch 5  19.2% | batch:        33 of       172\t|\tloss: 3.41949\n",
      "Evaluating Epoch 5  19.8% | batch:        34 of       172\t|\tloss: 2.18556\n",
      "Evaluating Epoch 5  20.3% | batch:        35 of       172\t|\tloss: 1.37454\n",
      "Evaluating Epoch 5  20.9% | batch:        36 of       172\t|\tloss: 8.06432\n",
      "Evaluating Epoch 5  21.5% | batch:        37 of       172\t|\tloss: 3.46488\n",
      "Evaluating Epoch 5  22.1% | batch:        38 of       172\t|\tloss: 3.46895\n",
      "Evaluating Epoch 5  22.7% | batch:        39 of       172\t|\tloss: 1.21206\n",
      "Evaluating Epoch 5  23.3% | batch:        40 of       172\t|\tloss: 1.62489\n",
      "Evaluating Epoch 5  23.8% | batch:        41 of       172\t|\tloss: 3.73395\n",
      "Evaluating Epoch 5  24.4% | batch:        42 of       172\t|\tloss: 1.39986\n",
      "Evaluating Epoch 5  25.0% | batch:        43 of       172\t|\tloss: 8.76543\n",
      "Evaluating Epoch 5  25.6% | batch:        44 of       172\t|\tloss: 1.22524\n",
      "Evaluating Epoch 5  26.2% | batch:        45 of       172\t|\tloss: 3.10011\n",
      "Evaluating Epoch 5  26.7% | batch:        46 of       172\t|\tloss: 0.887657\n",
      "Evaluating Epoch 5  27.3% | batch:        47 of       172\t|\tloss: 5.05365\n",
      "Evaluating Epoch 5  27.9% | batch:        48 of       172\t|\tloss: 2.06451\n",
      "Evaluating Epoch 5  28.5% | batch:        49 of       172\t|\tloss: 4.79917\n",
      "Evaluating Epoch 5  29.1% | batch:        50 of       172\t|\tloss: 1.54401\n",
      "Evaluating Epoch 5  29.7% | batch:        51 of       172\t|\tloss: 1.78401\n",
      "Evaluating Epoch 5  30.2% | batch:        52 of       172\t|\tloss: 2.42\n",
      "Evaluating Epoch 5  30.8% | batch:        53 of       172\t|\tloss: 1.74493\n",
      "Evaluating Epoch 5  31.4% | batch:        54 of       172\t|\tloss: 2.53774\n",
      "Evaluating Epoch 5  32.0% | batch:        55 of       172\t|\tloss: 2.60704\n",
      "Evaluating Epoch 5  32.6% | batch:        56 of       172\t|\tloss: 1.65313\n",
      "Evaluating Epoch 5  33.1% | batch:        57 of       172\t|\tloss: 3.51146\n",
      "Evaluating Epoch 5  33.7% | batch:        58 of       172\t|\tloss: 2.12677\n",
      "Evaluating Epoch 5  34.3% | batch:        59 of       172\t|\tloss: 2.5019\n",
      "Evaluating Epoch 5  34.9% | batch:        60 of       172\t|\tloss: 2.36641\n",
      "Evaluating Epoch 5  35.5% | batch:        61 of       172\t|\tloss: 2.83017\n",
      "Evaluating Epoch 5  36.0% | batch:        62 of       172\t|\tloss: 2.93314\n",
      "Evaluating Epoch 5  36.6% | batch:        63 of       172\t|\tloss: 2.91215\n",
      "Evaluating Epoch 5  37.2% | batch:        64 of       172\t|\tloss: 2.24368\n",
      "Evaluating Epoch 5  37.8% | batch:        65 of       172\t|\tloss: 1.58999\n",
      "Evaluating Epoch 5  38.4% | batch:        66 of       172\t|\tloss: 3.09445\n",
      "Evaluating Epoch 5  39.0% | batch:        67 of       172\t|\tloss: 1.90692\n",
      "Evaluating Epoch 5  39.5% | batch:        68 of       172\t|\tloss: 3.16923\n",
      "Evaluating Epoch 5  40.1% | batch:        69 of       172\t|\tloss: 3.68155\n",
      "Evaluating Epoch 5  40.7% | batch:        70 of       172\t|\tloss: 1.0723\n",
      "Evaluating Epoch 5  41.3% | batch:        71 of       172\t|\tloss: 2.47289\n",
      "Evaluating Epoch 5  41.9% | batch:        72 of       172\t|\tloss: 1.85592\n",
      "Evaluating Epoch 5  42.4% | batch:        73 of       172\t|\tloss: 1.43139\n",
      "Evaluating Epoch 5  43.0% | batch:        74 of       172\t|\tloss: 1.10057\n",
      "Evaluating Epoch 5  43.6% | batch:        75 of       172\t|\tloss: 1.36979\n",
      "Evaluating Epoch 5  44.2% | batch:        76 of       172\t|\tloss: 0.987645\n",
      "Evaluating Epoch 5  44.8% | batch:        77 of       172\t|\tloss: 1.53121\n",
      "Evaluating Epoch 5  45.3% | batch:        78 of       172\t|\tloss: 2.18193\n",
      "Evaluating Epoch 5  45.9% | batch:        79 of       172\t|\tloss: 1.11233\n",
      "Evaluating Epoch 5  46.5% | batch:        80 of       172\t|\tloss: 1.15436\n",
      "Evaluating Epoch 5  47.1% | batch:        81 of       172\t|\tloss: 1.15136\n",
      "Evaluating Epoch 5  47.7% | batch:        82 of       172\t|\tloss: 1.19793\n",
      "Evaluating Epoch 5  48.3% | batch:        83 of       172\t|\tloss: 1.1907\n",
      "Evaluating Epoch 5  48.8% | batch:        84 of       172\t|\tloss: 0.953302\n",
      "Evaluating Epoch 5  49.4% | batch:        85 of       172\t|\tloss: 2.4474\n",
      "Evaluating Epoch 5  50.0% | batch:        86 of       172\t|\tloss: 2.54037\n",
      "Evaluating Epoch 5  50.6% | batch:        87 of       172\t|\tloss: 1.70578\n",
      "Evaluating Epoch 5  51.2% | batch:        88 of       172\t|\tloss: 1.1699\n",
      "Evaluating Epoch 5  51.7% | batch:        89 of       172\t|\tloss: 2.26827\n",
      "Evaluating Epoch 5  52.3% | batch:        90 of       172\t|\tloss: 2.92736\n",
      "Evaluating Epoch 5  52.9% | batch:        91 of       172\t|\tloss: 1.52682\n",
      "Evaluating Epoch 5  53.5% | batch:        92 of       172\t|\tloss: 1.67499\n",
      "Evaluating Epoch 5  54.1% | batch:        93 of       172\t|\tloss: 1.93823\n",
      "Evaluating Epoch 5  54.7% | batch:        94 of       172\t|\tloss: 4.05483\n",
      "Evaluating Epoch 5  55.2% | batch:        95 of       172\t|\tloss: 1.25298\n",
      "Evaluating Epoch 5  55.8% | batch:        96 of       172\t|\tloss: 2.1276\n",
      "Evaluating Epoch 5  56.4% | batch:        97 of       172\t|\tloss: 1.66558\n",
      "Evaluating Epoch 5  57.0% | batch:        98 of       172\t|\tloss: 1.52254\n",
      "Evaluating Epoch 5  57.6% | batch:        99 of       172\t|\tloss: 2.54743\n",
      "Evaluating Epoch 5  58.1% | batch:       100 of       172\t|\tloss: 1.63149\n",
      "Evaluating Epoch 5  58.7% | batch:       101 of       172\t|\tloss: 1.07193\n",
      "Evaluating Epoch 5  59.3% | batch:       102 of       172\t|\tloss: 1.48543\n",
      "Evaluating Epoch 5  59.9% | batch:       103 of       172\t|\tloss: 1.93832\n",
      "Evaluating Epoch 5  60.5% | batch:       104 of       172\t|\tloss: 1.88942\n",
      "Evaluating Epoch 5  61.0% | batch:       105 of       172\t|\tloss: 1.29575\n",
      "Evaluating Epoch 5  61.6% | batch:       106 of       172\t|\tloss: 2.42961\n",
      "Evaluating Epoch 5  62.2% | batch:       107 of       172\t|\tloss: 2.0688\n",
      "Evaluating Epoch 5  62.8% | batch:       108 of       172\t|\tloss: 1.50957\n",
      "Evaluating Epoch 5  63.4% | batch:       109 of       172\t|\tloss: 1.69078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 5  64.0% | batch:       110 of       172\t|\tloss: 2.46524\n",
      "Evaluating Epoch 5  64.5% | batch:       111 of       172\t|\tloss: 1.74919\n",
      "Evaluating Epoch 5  65.1% | batch:       112 of       172\t|\tloss: 1.46734\n",
      "Evaluating Epoch 5  65.7% | batch:       113 of       172\t|\tloss: 4.32465\n",
      "Evaluating Epoch 5  66.3% | batch:       114 of       172\t|\tloss: 3.31672\n",
      "Evaluating Epoch 5  66.9% | batch:       115 of       172\t|\tloss: 1.24819\n",
      "Evaluating Epoch 5  67.4% | batch:       116 of       172\t|\tloss: 3.35955\n",
      "Evaluating Epoch 5  68.0% | batch:       117 of       172\t|\tloss: 2.27083\n",
      "Evaluating Epoch 5  68.6% | batch:       118 of       172\t|\tloss: 2.13929\n",
      "Evaluating Epoch 5  69.2% | batch:       119 of       172\t|\tloss: 3.36662\n",
      "Evaluating Epoch 5  69.8% | batch:       120 of       172\t|\tloss: 2.05705\n",
      "Evaluating Epoch 5  70.3% | batch:       121 of       172\t|\tloss: 11.9791\n",
      "Evaluating Epoch 5  70.9% | batch:       122 of       172\t|\tloss: 4.21237\n",
      "Evaluating Epoch 5  71.5% | batch:       123 of       172\t|\tloss: 13.8884\n",
      "Evaluating Epoch 5  72.1% | batch:       124 of       172\t|\tloss: 55.6207\n",
      "Evaluating Epoch 5  72.7% | batch:       125 of       172\t|\tloss: 4.29384\n",
      "Evaluating Epoch 5  73.3% | batch:       126 of       172\t|\tloss: 2.77698\n",
      "Evaluating Epoch 5  73.8% | batch:       127 of       172\t|\tloss: 1.75159\n",
      "Evaluating Epoch 5  74.4% | batch:       128 of       172\t|\tloss: 3.80379\n",
      "Evaluating Epoch 5  75.0% | batch:       129 of       172\t|\tloss: 2.39334\n",
      "Evaluating Epoch 5  75.6% | batch:       130 of       172\t|\tloss: 1.35225\n",
      "Evaluating Epoch 5  76.2% | batch:       131 of       172\t|\tloss: 4.16071\n",
      "Evaluating Epoch 5  76.7% | batch:       132 of       172\t|\tloss: 1.5881\n",
      "Evaluating Epoch 5  77.3% | batch:       133 of       172\t|\tloss: 1.83828\n",
      "Evaluating Epoch 5  77.9% | batch:       134 of       172\t|\tloss: 2.41802\n",
      "Evaluating Epoch 5  78.5% | batch:       135 of       172\t|\tloss: 0.65139\n",
      "Evaluating Epoch 5  79.1% | batch:       136 of       172\t|\tloss: 1.71825\n",
      "Evaluating Epoch 5  79.7% | batch:       137 of       172\t|\tloss: 0.636136\n",
      "Evaluating Epoch 5  80.2% | batch:       138 of       172\t|\tloss: 2.96943\n",
      "Evaluating Epoch 5  80.8% | batch:       139 of       172\t|\tloss: 3.0559\n",
      "Evaluating Epoch 5  81.4% | batch:       140 of       172\t|\tloss: 1.4154\n",
      "Evaluating Epoch 5  82.0% | batch:       141 of       172\t|\tloss: 1.29622\n",
      "Evaluating Epoch 5  82.6% | batch:       142 of       172\t|\tloss: 1.15996\n",
      "Evaluating Epoch 5  83.1% | batch:       143 of       172\t|\tloss: 1.39731\n",
      "Evaluating Epoch 5  83.7% | batch:       144 of       172\t|\tloss: 1.66784\n",
      "Evaluating Epoch 5  84.3% | batch:       145 of       172\t|\tloss: 0.824544\n",
      "Evaluating Epoch 5  84.9% | batch:       146 of       172\t|\tloss: 1.56595\n",
      "Evaluating Epoch 5  85.5% | batch:       147 of       172\t|\tloss: 1.11089\n",
      "Evaluating Epoch 5  86.0% | batch:       148 of       172\t|\tloss: 0.888789\n",
      "Evaluating Epoch 5  86.6% | batch:       149 of       172\t|\tloss: 1.19214\n",
      "Evaluating Epoch 5  87.2% | batch:       150 of       172\t|\tloss: 2.78648\n",
      "Evaluating Epoch 5  87.8% | batch:       151 of       172\t|\tloss: 1.9167\n",
      "Evaluating Epoch 5  88.4% | batch:       152 of       172\t|\tloss: 3.11567\n",
      "Evaluating Epoch 5  89.0% | batch:       153 of       172\t|\tloss: 2.26899\n",
      "Evaluating Epoch 5  89.5% | batch:       154 of       172\t|\tloss: 2.38541\n",
      "Evaluating Epoch 5  90.1% | batch:       155 of       172\t|\tloss: 3.76136\n",
      "Evaluating Epoch 5  90.7% | batch:       156 of       172\t|\tloss: 2.6543\n",
      "Evaluating Epoch 5  91.3% | batch:       157 of       172\t|\tloss: 3.19483\n",
      "Evaluating Epoch 5  91.9% | batch:       158 of       172\t|\tloss: 3.64593\n",
      "Evaluating Epoch 5  92.4% | batch:       159 of       172\t|\tloss: 2.46382\n",
      "Evaluating Epoch 5  93.0% | batch:       160 of       172\t|\tloss: 20.7903\n",
      "Evaluating Epoch 5  93.6% | batch:       161 of       172\t|\tloss: 11.9295\n",
      "Evaluating Epoch 5  94.2% | batch:       162 of       172\t|\tloss: 1.91613\n",
      "Evaluating Epoch 5  94.8% | batch:       163 of       172\t|\tloss: 3.32647\n",
      "Evaluating Epoch 5  95.3% | batch:       164 of       172\t|\tloss: 3.87019\n",
      "Evaluating Epoch 5  95.9% | batch:       165 of       172\t|\tloss: 2.03229\n",
      "Evaluating Epoch 5  96.5% | batch:       166 of       172\t|\tloss: 2.73426\n",
      "Evaluating Epoch 5  97.1% | batch:       167 of       172\t|\tloss: 2.10569\n",
      "Evaluating Epoch 5  97.7% | batch:       168 of       172\t|\tloss: 2.3364\n",
      "Evaluating Epoch 5  98.3% | batch:       169 of       172\t|\tloss: 2.3989\n",
      "Evaluating Epoch 5  98.8% | batch:       170 of       172\t|\tloss: 2.46248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:02:07,878 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.190314769744873 seconds\n",
      "\n",
      "2023-05-25 22:02:07,880 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 3.9846052726109824 seconds\n",
      "2023-05-25 22:02:07,882 | INFO : Avg batch val. time: 0.023166309724482456 seconds\n",
      "2023-05-25 22:02:07,883 | INFO : Avg sample val. time: 0.00018147311894206779 seconds\n",
      "2023-05-25 22:02:07,884 | INFO : Epoch 5 Validation Summary: epoch: 5.000000 | loss: 3.057575 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 5  99.4% | batch:       171 of       172\t|\tloss: 3.03375\n",
      "\n",
      "Training Epoch 6   0.0% | batch:         0 of       686\t|\tloss: 21.5637\n",
      "Training Epoch 6   0.1% | batch:         1 of       686\t|\tloss: 16.6235\n",
      "Training Epoch 6   0.3% | batch:         2 of       686\t|\tloss: 20.975\n",
      "Training Epoch 6   0.4% | batch:         3 of       686\t|\tloss: 21.9694\n",
      "Training Epoch 6   0.6% | batch:         4 of       686\t|\tloss: 22.0328\n",
      "Training Epoch 6   0.7% | batch:         5 of       686\t|\tloss: 18.75\n",
      "Training Epoch 6   0.9% | batch:         6 of       686\t|\tloss: 21.1939\n",
      "Training Epoch 6   1.0% | batch:         7 of       686\t|\tloss: 22.077\n",
      "Training Epoch 6   1.2% | batch:         8 of       686\t|\tloss: 16.3218\n",
      "Training Epoch 6   1.3% | batch:         9 of       686\t|\tloss: 22.6575\n",
      "Training Epoch 6   1.5% | batch:        10 of       686\t|\tloss: 19.5481\n",
      "Training Epoch 6   1.6% | batch:        11 of       686\t|\tloss: 19.0643\n",
      "Training Epoch 6   1.7% | batch:        12 of       686\t|\tloss: 18.5679\n",
      "Training Epoch 6   1.9% | batch:        13 of       686\t|\tloss: 18.7199\n",
      "Training Epoch 6   2.0% | batch:        14 of       686\t|\tloss: 22.7811\n",
      "Training Epoch 6   2.2% | batch:        15 of       686\t|\tloss: 20.8111\n",
      "Training Epoch 6   2.3% | batch:        16 of       686\t|\tloss: 19.4953\n",
      "Training Epoch 6   2.5% | batch:        17 of       686\t|\tloss: 21.3014\n",
      "Training Epoch 6   2.6% | batch:        18 of       686\t|\tloss: 21.7655\n",
      "Training Epoch 6   2.8% | batch:        19 of       686\t|\tloss: 25.3981\n",
      "Training Epoch 6   2.9% | batch:        20 of       686\t|\tloss: 55.8017\n",
      "Training Epoch 6   3.1% | batch:        21 of       686\t|\tloss: 24.0805\n",
      "Training Epoch 6   3.2% | batch:        22 of       686\t|\tloss: 19.627\n",
      "Training Epoch 6   3.4% | batch:        23 of       686\t|\tloss: 24.6058\n",
      "Training Epoch 6   3.5% | batch:        24 of       686\t|\tloss: 24.8869\n",
      "Training Epoch 6   3.6% | batch:        25 of       686\t|\tloss: 15.7079\n",
      "Training Epoch 6   3.8% | batch:        26 of       686\t|\tloss: 23.4929\n",
      "Training Epoch 6   3.9% | batch:        27 of       686\t|\tloss: 19.91\n",
      "Training Epoch 6   4.1% | batch:        28 of       686\t|\tloss: 18.9576\n",
      "Training Epoch 6   4.2% | batch:        29 of       686\t|\tloss: 30.9564\n",
      "Training Epoch 6   4.4% | batch:        30 of       686\t|\tloss: 18.8172\n",
      "Training Epoch 6   4.5% | batch:        31 of       686\t|\tloss: 21.681\n",
      "Training Epoch 6   4.7% | batch:        32 of       686\t|\tloss: 22.1063\n",
      "Training Epoch 6   4.8% | batch:        33 of       686\t|\tloss: 24.5719\n",
      "Training Epoch 6   5.0% | batch:        34 of       686\t|\tloss: 18.8468\n",
      "Training Epoch 6   5.1% | batch:        35 of       686\t|\tloss: 20.2751\n",
      "Training Epoch 6   5.2% | batch:        36 of       686\t|\tloss: 16.0529\n",
      "Training Epoch 6   5.4% | batch:        37 of       686\t|\tloss: 19.8188\n",
      "Training Epoch 6   5.5% | batch:        38 of       686\t|\tloss: 22.2985\n",
      "Training Epoch 6   5.7% | batch:        39 of       686\t|\tloss: 24.9077\n",
      "Training Epoch 6   5.8% | batch:        40 of       686\t|\tloss: 23.8936\n",
      "Training Epoch 6   6.0% | batch:        41 of       686\t|\tloss: 19.7993\n",
      "Training Epoch 6   6.1% | batch:        42 of       686\t|\tloss: 14.3808\n",
      "Training Epoch 6   6.3% | batch:        43 of       686\t|\tloss: 20.909\n",
      "Training Epoch 6   6.4% | batch:        44 of       686\t|\tloss: 22.0404\n",
      "Training Epoch 6   6.6% | batch:        45 of       686\t|\tloss: 16.7351\n",
      "Training Epoch 6   6.7% | batch:        46 of       686\t|\tloss: 15.2905\n",
      "Training Epoch 6   6.9% | batch:        47 of       686\t|\tloss: 22.5786\n",
      "Training Epoch 6   7.0% | batch:        48 of       686\t|\tloss: 21.5353\n",
      "Training Epoch 6   7.1% | batch:        49 of       686\t|\tloss: 23.0999\n",
      "Training Epoch 6   7.3% | batch:        50 of       686\t|\tloss: 21.2384\n",
      "Training Epoch 6   7.4% | batch:        51 of       686\t|\tloss: 27.2122\n",
      "Training Epoch 6   7.6% | batch:        52 of       686\t|\tloss: 21.107\n",
      "Training Epoch 6   7.7% | batch:        53 of       686\t|\tloss: 19.8649\n",
      "Training Epoch 6   7.9% | batch:        54 of       686\t|\tloss: 17.4528\n",
      "Training Epoch 6   8.0% | batch:        55 of       686\t|\tloss: 20.267\n",
      "Training Epoch 6   8.2% | batch:        56 of       686\t|\tloss: 17.5401\n",
      "Training Epoch 6   8.3% | batch:        57 of       686\t|\tloss: 19.894\n",
      "Training Epoch 6   8.5% | batch:        58 of       686\t|\tloss: 18.8588\n",
      "Training Epoch 6   8.6% | batch:        59 of       686\t|\tloss: 19.5876\n",
      "Training Epoch 6   8.7% | batch:        60 of       686\t|\tloss: 21.001\n",
      "Training Epoch 6   8.9% | batch:        61 of       686\t|\tloss: 21.6433\n",
      "Training Epoch 6   9.0% | batch:        62 of       686\t|\tloss: 22.6756\n",
      "Training Epoch 6   9.2% | batch:        63 of       686\t|\tloss: 18.8579\n",
      "Training Epoch 6   9.3% | batch:        64 of       686\t|\tloss: 18.9581\n",
      "Training Epoch 6   9.5% | batch:        65 of       686\t|\tloss: 31.3205\n",
      "Training Epoch 6   9.6% | batch:        66 of       686\t|\tloss: 22.6374\n",
      "Training Epoch 6   9.8% | batch:        67 of       686\t|\tloss: 21.7678\n",
      "Training Epoch 6   9.9% | batch:        68 of       686\t|\tloss: 17.5446\n",
      "Training Epoch 6  10.1% | batch:        69 of       686\t|\tloss: 12.9342\n",
      "Training Epoch 6  10.2% | batch:        70 of       686\t|\tloss: 20.3992\n",
      "Training Epoch 6  10.3% | batch:        71 of       686\t|\tloss: 17.8774\n",
      "Training Epoch 6  10.5% | batch:        72 of       686\t|\tloss: 18.5134\n",
      "Training Epoch 6  10.6% | batch:        73 of       686\t|\tloss: 23.0701\n",
      "Training Epoch 6  10.8% | batch:        74 of       686\t|\tloss: 17.978\n",
      "Training Epoch 6  10.9% | batch:        75 of       686\t|\tloss: 18.7553\n",
      "Training Epoch 6  11.1% | batch:        76 of       686\t|\tloss: 36.1441\n",
      "Training Epoch 6  11.2% | batch:        77 of       686\t|\tloss: 23.2389\n",
      "Training Epoch 6  11.4% | batch:        78 of       686\t|\tloss: 20.4531\n",
      "Training Epoch 6  11.5% | batch:        79 of       686\t|\tloss: 24.2652\n",
      "Training Epoch 6  11.7% | batch:        80 of       686\t|\tloss: 17.0764\n",
      "Training Epoch 6  11.8% | batch:        81 of       686\t|\tloss: 19.4872\n",
      "Training Epoch 6  12.0% | batch:        82 of       686\t|\tloss: 23.3796\n",
      "Training Epoch 6  12.1% | batch:        83 of       686\t|\tloss: 43.6451\n",
      "Training Epoch 6  12.2% | batch:        84 of       686\t|\tloss: 23.7164\n",
      "Training Epoch 6  12.4% | batch:        85 of       686\t|\tloss: 14.6058\n",
      "Training Epoch 6  12.5% | batch:        86 of       686\t|\tloss: 18.5726\n",
      "Training Epoch 6  12.7% | batch:        87 of       686\t|\tloss: 25.249\n",
      "Training Epoch 6  12.8% | batch:        88 of       686\t|\tloss: 18.5908\n",
      "Training Epoch 6  13.0% | batch:        89 of       686\t|\tloss: 20.277\n",
      "Training Epoch 6  13.1% | batch:        90 of       686\t|\tloss: 19.2253\n",
      "Training Epoch 6  13.3% | batch:        91 of       686\t|\tloss: 18.2995\n",
      "Training Epoch 6  13.4% | batch:        92 of       686\t|\tloss: 19.8613\n",
      "Training Epoch 6  13.6% | batch:        93 of       686\t|\tloss: 16.8869\n",
      "Training Epoch 6  13.7% | batch:        94 of       686\t|\tloss: 25.5645\n",
      "Training Epoch 6  13.8% | batch:        95 of       686\t|\tloss: 20.469\n",
      "Training Epoch 6  14.0% | batch:        96 of       686\t|\tloss: 20.5945\n",
      "Training Epoch 6  14.1% | batch:        97 of       686\t|\tloss: 17.1407\n",
      "Training Epoch 6  14.3% | batch:        98 of       686\t|\tloss: 22.0189\n",
      "Training Epoch 6  14.4% | batch:        99 of       686\t|\tloss: 20.2626\n",
      "Training Epoch 6  14.6% | batch:       100 of       686\t|\tloss: 18.5559\n",
      "Training Epoch 6  14.7% | batch:       101 of       686\t|\tloss: 21.2846\n",
      "Training Epoch 6  14.9% | batch:       102 of       686\t|\tloss: 25.5632\n",
      "Training Epoch 6  15.0% | batch:       103 of       686\t|\tloss: 24.5506\n",
      "Training Epoch 6  15.2% | batch:       104 of       686\t|\tloss: 21.3942\n",
      "Training Epoch 6  15.3% | batch:       105 of       686\t|\tloss: 15.0088\n",
      "Training Epoch 6  15.5% | batch:       106 of       686\t|\tloss: 17.5363\n",
      "Training Epoch 6  15.6% | batch:       107 of       686\t|\tloss: 17.326\n",
      "Training Epoch 6  15.7% | batch:       108 of       686\t|\tloss: 19.7845\n",
      "Training Epoch 6  15.9% | batch:       109 of       686\t|\tloss: 20.8716\n",
      "Training Epoch 6  16.0% | batch:       110 of       686\t|\tloss: 18.4424\n",
      "Training Epoch 6  16.2% | batch:       111 of       686\t|\tloss: 27.1863\n",
      "Training Epoch 6  16.3% | batch:       112 of       686\t|\tloss: 17.1111\n",
      "Training Epoch 6  16.5% | batch:       113 of       686\t|\tloss: 19.5875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  16.6% | batch:       114 of       686\t|\tloss: 14.2116\n",
      "Training Epoch 6  16.8% | batch:       115 of       686\t|\tloss: 19.1747\n",
      "Training Epoch 6  16.9% | batch:       116 of       686\t|\tloss: 41.5765\n",
      "Training Epoch 6  17.1% | batch:       117 of       686\t|\tloss: 29.0331\n",
      "Training Epoch 6  17.2% | batch:       118 of       686\t|\tloss: 20.1525\n",
      "Training Epoch 6  17.3% | batch:       119 of       686\t|\tloss: 17.9713\n",
      "Training Epoch 6  17.5% | batch:       120 of       686\t|\tloss: 19.6108\n",
      "Training Epoch 6  17.6% | batch:       121 of       686\t|\tloss: 24.771\n",
      "Training Epoch 6  17.8% | batch:       122 of       686\t|\tloss: 26.6411\n",
      "Training Epoch 6  17.9% | batch:       123 of       686\t|\tloss: 18.8128\n",
      "Training Epoch 6  18.1% | batch:       124 of       686\t|\tloss: 20.5944\n",
      "Training Epoch 6  18.2% | batch:       125 of       686\t|\tloss: 16.3747\n",
      "Training Epoch 6  18.4% | batch:       126 of       686\t|\tloss: 18.9811\n",
      "Training Epoch 6  18.5% | batch:       127 of       686\t|\tloss: 17.6659\n",
      "Training Epoch 6  18.7% | batch:       128 of       686\t|\tloss: 23.631\n",
      "Training Epoch 6  18.8% | batch:       129 of       686\t|\tloss: 16.7715\n",
      "Training Epoch 6  19.0% | batch:       130 of       686\t|\tloss: 26.9819\n",
      "Training Epoch 6  19.1% | batch:       131 of       686\t|\tloss: 18.9466\n",
      "Training Epoch 6  19.2% | batch:       132 of       686\t|\tloss: 21.0452\n",
      "Training Epoch 6  19.4% | batch:       133 of       686\t|\tloss: 31.8797\n",
      "Training Epoch 6  19.5% | batch:       134 of       686\t|\tloss: 20.9079\n",
      "Training Epoch 6  19.7% | batch:       135 of       686\t|\tloss: 17.1229\n",
      "Training Epoch 6  19.8% | batch:       136 of       686\t|\tloss: 20.6479\n",
      "Training Epoch 6  20.0% | batch:       137 of       686\t|\tloss: 22.4246\n",
      "Training Epoch 6  20.1% | batch:       138 of       686\t|\tloss: 16.5732\n",
      "Training Epoch 6  20.3% | batch:       139 of       686\t|\tloss: 30.3451\n",
      "Training Epoch 6  20.4% | batch:       140 of       686\t|\tloss: 16.0327\n",
      "Training Epoch 6  20.6% | batch:       141 of       686\t|\tloss: 25.1023\n",
      "Training Epoch 6  20.7% | batch:       142 of       686\t|\tloss: 15.119\n",
      "Training Epoch 6  20.8% | batch:       143 of       686\t|\tloss: 20.2607\n",
      "Training Epoch 6  21.0% | batch:       144 of       686\t|\tloss: 24.201\n",
      "Training Epoch 6  21.1% | batch:       145 of       686\t|\tloss: 20.4556\n",
      "Training Epoch 6  21.3% | batch:       146 of       686\t|\tloss: 19.8067\n",
      "Training Epoch 6  21.4% | batch:       147 of       686\t|\tloss: 20.5372\n",
      "Training Epoch 6  21.6% | batch:       148 of       686\t|\tloss: 16.6597\n",
      "Training Epoch 6  21.7% | batch:       149 of       686\t|\tloss: 18.5904\n",
      "Training Epoch 6  21.9% | batch:       150 of       686\t|\tloss: 25.621\n",
      "Training Epoch 6  22.0% | batch:       151 of       686\t|\tloss: 18.9291\n",
      "Training Epoch 6  22.2% | batch:       152 of       686\t|\tloss: 20.2512\n",
      "Training Epoch 6  22.3% | batch:       153 of       686\t|\tloss: 21.1749\n",
      "Training Epoch 6  22.4% | batch:       154 of       686\t|\tloss: 21.5813\n",
      "Training Epoch 6  22.6% | batch:       155 of       686\t|\tloss: 16.884\n",
      "Training Epoch 6  22.7% | batch:       156 of       686\t|\tloss: 20.1086\n",
      "Training Epoch 6  22.9% | batch:       157 of       686\t|\tloss: 18.038\n",
      "Training Epoch 6  23.0% | batch:       158 of       686\t|\tloss: 14.5617\n",
      "Training Epoch 6  23.2% | batch:       159 of       686\t|\tloss: 24.8255\n",
      "Training Epoch 6  23.3% | batch:       160 of       686\t|\tloss: 23.479\n",
      "Training Epoch 6  23.5% | batch:       161 of       686\t|\tloss: 18.3372\n",
      "Training Epoch 6  23.6% | batch:       162 of       686\t|\tloss: 17.0449\n",
      "Training Epoch 6  23.8% | batch:       163 of       686\t|\tloss: 15.6778\n",
      "Training Epoch 6  23.9% | batch:       164 of       686\t|\tloss: 15.8945\n",
      "Training Epoch 6  24.1% | batch:       165 of       686\t|\tloss: 17.2689\n",
      "Training Epoch 6  24.2% | batch:       166 of       686\t|\tloss: 21.1891\n",
      "Training Epoch 6  24.3% | batch:       167 of       686\t|\tloss: 19.653\n",
      "Training Epoch 6  24.5% | batch:       168 of       686\t|\tloss: 17.2917\n",
      "Training Epoch 6  24.6% | batch:       169 of       686\t|\tloss: 18.2094\n",
      "Training Epoch 6  24.8% | batch:       170 of       686\t|\tloss: 16.4418\n",
      "Training Epoch 6  24.9% | batch:       171 of       686\t|\tloss: 17.5511\n",
      "Training Epoch 6  25.1% | batch:       172 of       686\t|\tloss: 16.2037\n",
      "Training Epoch 6  25.2% | batch:       173 of       686\t|\tloss: 18.2882\n",
      "Training Epoch 6  25.4% | batch:       174 of       686\t|\tloss: 18.8736\n",
      "Training Epoch 6  25.5% | batch:       175 of       686\t|\tloss: 20.711\n",
      "Training Epoch 6  25.7% | batch:       176 of       686\t|\tloss: 14.6461\n",
      "Training Epoch 6  25.8% | batch:       177 of       686\t|\tloss: 21.6639\n",
      "Training Epoch 6  25.9% | batch:       178 of       686\t|\tloss: 16.5601\n",
      "Training Epoch 6  26.1% | batch:       179 of       686\t|\tloss: 20.7981\n",
      "Training Epoch 6  26.2% | batch:       180 of       686\t|\tloss: 17.677\n",
      "Training Epoch 6  26.4% | batch:       181 of       686\t|\tloss: 23.1033\n",
      "Training Epoch 6  26.5% | batch:       182 of       686\t|\tloss: 21.943\n",
      "Training Epoch 6  26.7% | batch:       183 of       686\t|\tloss: 23.8909\n",
      "Training Epoch 6  26.8% | batch:       184 of       686\t|\tloss: 20.2045\n",
      "Training Epoch 6  27.0% | batch:       185 of       686\t|\tloss: 14.5656\n",
      "Training Epoch 6  27.1% | batch:       186 of       686\t|\tloss: 19.8647\n",
      "Training Epoch 6  27.3% | batch:       187 of       686\t|\tloss: 15.5831\n",
      "Training Epoch 6  27.4% | batch:       188 of       686\t|\tloss: 23.9173\n",
      "Training Epoch 6  27.6% | batch:       189 of       686\t|\tloss: 21.5181\n",
      "Training Epoch 6  27.7% | batch:       190 of       686\t|\tloss: 18.0807\n",
      "Training Epoch 6  27.8% | batch:       191 of       686\t|\tloss: 19.7489\n",
      "Training Epoch 6  28.0% | batch:       192 of       686\t|\tloss: 17.0339\n",
      "Training Epoch 6  28.1% | batch:       193 of       686\t|\tloss: 17.3817\n",
      "Training Epoch 6  28.3% | batch:       194 of       686\t|\tloss: 22.6537\n",
      "Training Epoch 6  28.4% | batch:       195 of       686\t|\tloss: 16.4216\n",
      "Training Epoch 6  28.6% | batch:       196 of       686\t|\tloss: 26.7616\n",
      "Training Epoch 6  28.7% | batch:       197 of       686\t|\tloss: 14.1375\n",
      "Training Epoch 6  28.9% | batch:       198 of       686\t|\tloss: 23.3948\n",
      "Training Epoch 6  29.0% | batch:       199 of       686\t|\tloss: 20.9291\n",
      "Training Epoch 6  29.2% | batch:       200 of       686\t|\tloss: 19.7229\n",
      "Training Epoch 6  29.3% | batch:       201 of       686\t|\tloss: 15.526\n",
      "Training Epoch 6  29.4% | batch:       202 of       686\t|\tloss: 17.1392\n",
      "Training Epoch 6  29.6% | batch:       203 of       686\t|\tloss: 19.914\n",
      "Training Epoch 6  29.7% | batch:       204 of       686\t|\tloss: 21.7251\n",
      "Training Epoch 6  29.9% | batch:       205 of       686\t|\tloss: 12.3814\n",
      "Training Epoch 6  30.0% | batch:       206 of       686\t|\tloss: 16.1687\n",
      "Training Epoch 6  30.2% | batch:       207 of       686\t|\tloss: 14.0936\n",
      "Training Epoch 6  30.3% | batch:       208 of       686\t|\tloss: 15.3619\n",
      "Training Epoch 6  30.5% | batch:       209 of       686\t|\tloss: 17.2103\n",
      "Training Epoch 6  30.6% | batch:       210 of       686\t|\tloss: 14.1789\n",
      "Training Epoch 6  30.8% | batch:       211 of       686\t|\tloss: 14.7141\n",
      "Training Epoch 6  30.9% | batch:       212 of       686\t|\tloss: 18.4502\n",
      "Training Epoch 6  31.0% | batch:       213 of       686\t|\tloss: 27.0164\n",
      "Training Epoch 6  31.2% | batch:       214 of       686\t|\tloss: 18.6205\n",
      "Training Epoch 6  31.3% | batch:       215 of       686\t|\tloss: 17.5782\n",
      "Training Epoch 6  31.5% | batch:       216 of       686\t|\tloss: 18.4687\n",
      "Training Epoch 6  31.6% | batch:       217 of       686\t|\tloss: 21.847\n",
      "Training Epoch 6  31.8% | batch:       218 of       686\t|\tloss: 18.3505\n",
      "Training Epoch 6  31.9% | batch:       219 of       686\t|\tloss: 17.4212\n",
      "Training Epoch 6  32.1% | batch:       220 of       686\t|\tloss: 19.235\n",
      "Training Epoch 6  32.2% | batch:       221 of       686\t|\tloss: 18.5532\n",
      "Training Epoch 6  32.4% | batch:       222 of       686\t|\tloss: 18.2063\n",
      "Training Epoch 6  32.5% | batch:       223 of       686\t|\tloss: 18.4388\n",
      "Training Epoch 6  32.7% | batch:       224 of       686\t|\tloss: 21.771\n",
      "Training Epoch 6  32.8% | batch:       225 of       686\t|\tloss: 16.7436\n",
      "Training Epoch 6  32.9% | batch:       226 of       686\t|\tloss: 17.9466\n",
      "Training Epoch 6  33.1% | batch:       227 of       686\t|\tloss: 21.9917\n",
      "Training Epoch 6  33.2% | batch:       228 of       686\t|\tloss: 18.5882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  33.4% | batch:       229 of       686\t|\tloss: 17.5238\n",
      "Training Epoch 6  33.5% | batch:       230 of       686\t|\tloss: 25.1198\n",
      "Training Epoch 6  33.7% | batch:       231 of       686\t|\tloss: 21.9158\n",
      "Training Epoch 6  33.8% | batch:       232 of       686\t|\tloss: 19.5681\n",
      "Training Epoch 6  34.0% | batch:       233 of       686\t|\tloss: 18.2937\n",
      "Training Epoch 6  34.1% | batch:       234 of       686\t|\tloss: 23.6602\n",
      "Training Epoch 6  34.3% | batch:       235 of       686\t|\tloss: 29.4486\n",
      "Training Epoch 6  34.4% | batch:       236 of       686\t|\tloss: 23.0801\n",
      "Training Epoch 6  34.5% | batch:       237 of       686\t|\tloss: 17.7883\n",
      "Training Epoch 6  34.7% | batch:       238 of       686\t|\tloss: 18.3659\n",
      "Training Epoch 6  34.8% | batch:       239 of       686\t|\tloss: 23.309\n",
      "Training Epoch 6  35.0% | batch:       240 of       686\t|\tloss: 23.838\n",
      "Training Epoch 6  35.1% | batch:       241 of       686\t|\tloss: 19.924\n",
      "Training Epoch 6  35.3% | batch:       242 of       686\t|\tloss: 17.4676\n",
      "Training Epoch 6  35.4% | batch:       243 of       686\t|\tloss: 21.9957\n",
      "Training Epoch 6  35.6% | batch:       244 of       686\t|\tloss: 16.9333\n",
      "Training Epoch 6  35.7% | batch:       245 of       686\t|\tloss: 18.5643\n",
      "Training Epoch 6  35.9% | batch:       246 of       686\t|\tloss: 21.2611\n",
      "Training Epoch 6  36.0% | batch:       247 of       686\t|\tloss: 14.8781\n",
      "Training Epoch 6  36.2% | batch:       248 of       686\t|\tloss: 23.0607\n",
      "Training Epoch 6  36.3% | batch:       249 of       686\t|\tloss: 21.817\n",
      "Training Epoch 6  36.4% | batch:       250 of       686\t|\tloss: 17.2988\n",
      "Training Epoch 6  36.6% | batch:       251 of       686\t|\tloss: 20.0457\n",
      "Training Epoch 6  36.7% | batch:       252 of       686\t|\tloss: 17.4244\n",
      "Training Epoch 6  36.9% | batch:       253 of       686\t|\tloss: 19.3652\n",
      "Training Epoch 6  37.0% | batch:       254 of       686\t|\tloss: 17.3515\n",
      "Training Epoch 6  37.2% | batch:       255 of       686\t|\tloss: 15.1562\n",
      "Training Epoch 6  37.3% | batch:       256 of       686\t|\tloss: 16.3978\n",
      "Training Epoch 6  37.5% | batch:       257 of       686\t|\tloss: 20.029\n",
      "Training Epoch 6  37.6% | batch:       258 of       686\t|\tloss: 23.7141\n",
      "Training Epoch 6  37.8% | batch:       259 of       686\t|\tloss: 18.0373\n",
      "Training Epoch 6  37.9% | batch:       260 of       686\t|\tloss: 15.8502\n",
      "Training Epoch 6  38.0% | batch:       261 of       686\t|\tloss: 20.278\n",
      "Training Epoch 6  38.2% | batch:       262 of       686\t|\tloss: 28.7306\n",
      "Training Epoch 6  38.3% | batch:       263 of       686\t|\tloss: 23.5335\n",
      "Training Epoch 6  38.5% | batch:       264 of       686\t|\tloss: 12.9621\n",
      "Training Epoch 6  38.6% | batch:       265 of       686\t|\tloss: 18.3347\n",
      "Training Epoch 6  38.8% | batch:       266 of       686\t|\tloss: 17.5858\n",
      "Training Epoch 6  38.9% | batch:       267 of       686\t|\tloss: 20.2012\n",
      "Training Epoch 6  39.1% | batch:       268 of       686\t|\tloss: 26.9846\n",
      "Training Epoch 6  39.2% | batch:       269 of       686\t|\tloss: 17.6381\n",
      "Training Epoch 6  39.4% | batch:       270 of       686\t|\tloss: 19.1846\n",
      "Training Epoch 6  39.5% | batch:       271 of       686\t|\tloss: 18.5225\n",
      "Training Epoch 6  39.7% | batch:       272 of       686\t|\tloss: 18.3175\n",
      "Training Epoch 6  39.8% | batch:       273 of       686\t|\tloss: 34.3591\n",
      "Training Epoch 6  39.9% | batch:       274 of       686\t|\tloss: 14.2615\n",
      "Training Epoch 6  40.1% | batch:       275 of       686\t|\tloss: 19.9272\n",
      "Training Epoch 6  40.2% | batch:       276 of       686\t|\tloss: 16.8718\n",
      "Training Epoch 6  40.4% | batch:       277 of       686\t|\tloss: 14.2646\n",
      "Training Epoch 6  40.5% | batch:       278 of       686\t|\tloss: 21.2785\n",
      "Training Epoch 6  40.7% | batch:       279 of       686\t|\tloss: 18.3535\n",
      "Training Epoch 6  40.8% | batch:       280 of       686\t|\tloss: 25.1966\n",
      "Training Epoch 6  41.0% | batch:       281 of       686\t|\tloss: 19.3439\n",
      "Training Epoch 6  41.1% | batch:       282 of       686\t|\tloss: 21.8161\n",
      "Training Epoch 6  41.3% | batch:       283 of       686\t|\tloss: 14.3323\n",
      "Training Epoch 6  41.4% | batch:       284 of       686\t|\tloss: 19.4093\n",
      "Training Epoch 6  41.5% | batch:       285 of       686\t|\tloss: 21.4135\n",
      "Training Epoch 6  41.7% | batch:       286 of       686\t|\tloss: 17.2235\n",
      "Training Epoch 6  41.8% | batch:       287 of       686\t|\tloss: 19.2013\n",
      "Training Epoch 6  42.0% | batch:       288 of       686\t|\tloss: 21.053\n",
      "Training Epoch 6  42.1% | batch:       289 of       686\t|\tloss: 19.8762\n",
      "Training Epoch 6  42.3% | batch:       290 of       686\t|\tloss: 16.0867\n",
      "Training Epoch 6  42.4% | batch:       291 of       686\t|\tloss: 19.901\n",
      "Training Epoch 6  42.6% | batch:       292 of       686\t|\tloss: 16.197\n",
      "Training Epoch 6  42.7% | batch:       293 of       686\t|\tloss: 21.6328\n",
      "Training Epoch 6  42.9% | batch:       294 of       686\t|\tloss: 21.0639\n",
      "Training Epoch 6  43.0% | batch:       295 of       686\t|\tloss: 18.6843\n",
      "Training Epoch 6  43.1% | batch:       296 of       686\t|\tloss: 15.423\n",
      "Training Epoch 6  43.3% | batch:       297 of       686\t|\tloss: 22.8368\n",
      "Training Epoch 6  43.4% | batch:       298 of       686\t|\tloss: 16.8794\n",
      "Training Epoch 6  43.6% | batch:       299 of       686\t|\tloss: 16.2542\n",
      "Training Epoch 6  43.7% | batch:       300 of       686\t|\tloss: 21.9949\n",
      "Training Epoch 6  43.9% | batch:       301 of       686\t|\tloss: 14.2492\n",
      "Training Epoch 6  44.0% | batch:       302 of       686\t|\tloss: 23.4052\n",
      "Training Epoch 6  44.2% | batch:       303 of       686\t|\tloss: 14.5871\n",
      "Training Epoch 6  44.3% | batch:       304 of       686\t|\tloss: 18.7319\n",
      "Training Epoch 6  44.5% | batch:       305 of       686\t|\tloss: 19.7378\n",
      "Training Epoch 6  44.6% | batch:       306 of       686\t|\tloss: 21.1855\n",
      "Training Epoch 6  44.8% | batch:       307 of       686\t|\tloss: 16.5735\n",
      "Training Epoch 6  44.9% | batch:       308 of       686\t|\tloss: 17.1373\n",
      "Training Epoch 6  45.0% | batch:       309 of       686\t|\tloss: 21.4766\n",
      "Training Epoch 6  45.2% | batch:       310 of       686\t|\tloss: 16.6291\n",
      "Training Epoch 6  45.3% | batch:       311 of       686\t|\tloss: 16.7511\n",
      "Training Epoch 6  45.5% | batch:       312 of       686\t|\tloss: 18.9182\n",
      "Training Epoch 6  45.6% | batch:       313 of       686\t|\tloss: 14.3663\n",
      "Training Epoch 6  45.8% | batch:       314 of       686\t|\tloss: 19.1519\n",
      "Training Epoch 6  45.9% | batch:       315 of       686\t|\tloss: 15.6987\n",
      "Training Epoch 6  46.1% | batch:       316 of       686\t|\tloss: 16.8249\n",
      "Training Epoch 6  46.2% | batch:       317 of       686\t|\tloss: 19.8482\n",
      "Training Epoch 6  46.4% | batch:       318 of       686\t|\tloss: 18.1777\n",
      "Training Epoch 6  46.5% | batch:       319 of       686\t|\tloss: 20.5512\n",
      "Training Epoch 6  46.6% | batch:       320 of       686\t|\tloss: 22.3778\n",
      "Training Epoch 6  46.8% | batch:       321 of       686\t|\tloss: 16.4845\n",
      "Training Epoch 6  46.9% | batch:       322 of       686\t|\tloss: 17.1848\n",
      "Training Epoch 6  47.1% | batch:       323 of       686\t|\tloss: 18.1161\n",
      "Training Epoch 6  47.2% | batch:       324 of       686\t|\tloss: 17.9665\n",
      "Training Epoch 6  47.4% | batch:       325 of       686\t|\tloss: 15.8887\n",
      "Training Epoch 6  47.5% | batch:       326 of       686\t|\tloss: 14.8014\n",
      "Training Epoch 6  47.7% | batch:       327 of       686\t|\tloss: 18.6484\n",
      "Training Epoch 6  47.8% | batch:       328 of       686\t|\tloss: 23.7368\n",
      "Training Epoch 6  48.0% | batch:       329 of       686\t|\tloss: 19.5537\n",
      "Training Epoch 6  48.1% | batch:       330 of       686\t|\tloss: 20.3122\n",
      "Training Epoch 6  48.3% | batch:       331 of       686\t|\tloss: 19.8149\n",
      "Training Epoch 6  48.4% | batch:       332 of       686\t|\tloss: 17.6008\n",
      "Training Epoch 6  48.5% | batch:       333 of       686\t|\tloss: 16.7889\n",
      "Training Epoch 6  48.7% | batch:       334 of       686\t|\tloss: 19.3283\n",
      "Training Epoch 6  48.8% | batch:       335 of       686\t|\tloss: 21.6588\n",
      "Training Epoch 6  49.0% | batch:       336 of       686\t|\tloss: 16.6624\n",
      "Training Epoch 6  49.1% | batch:       337 of       686\t|\tloss: 13.4619\n",
      "Training Epoch 6  49.3% | batch:       338 of       686\t|\tloss: 17.8245\n",
      "Training Epoch 6  49.4% | batch:       339 of       686\t|\tloss: 18.0571\n",
      "Training Epoch 6  49.6% | batch:       340 of       686\t|\tloss: 23.7513\n",
      "Training Epoch 6  49.7% | batch:       341 of       686\t|\tloss: 29.5013\n",
      "Training Epoch 6  49.9% | batch:       342 of       686\t|\tloss: 15.5924\n",
      "Training Epoch 6  50.0% | batch:       343 of       686\t|\tloss: 21.6949\n",
      "Training Epoch 6  50.1% | batch:       344 of       686\t|\tloss: 20.3014\n",
      "Training Epoch 6  50.3% | batch:       345 of       686\t|\tloss: 16.8543\n",
      "Training Epoch 6  50.4% | batch:       346 of       686\t|\tloss: 26.885\n",
      "Training Epoch 6  50.6% | batch:       347 of       686\t|\tloss: 16.8014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  50.7% | batch:       348 of       686\t|\tloss: 21.2729\n",
      "Training Epoch 6  50.9% | batch:       349 of       686\t|\tloss: 22.3767\n",
      "Training Epoch 6  51.0% | batch:       350 of       686\t|\tloss: 20.4732\n",
      "Training Epoch 6  51.2% | batch:       351 of       686\t|\tloss: 19.6436\n",
      "Training Epoch 6  51.3% | batch:       352 of       686\t|\tloss: 15.9021\n",
      "Training Epoch 6  51.5% | batch:       353 of       686\t|\tloss: 17.9256\n",
      "Training Epoch 6  51.6% | batch:       354 of       686\t|\tloss: 17.1439\n",
      "Training Epoch 6  51.7% | batch:       355 of       686\t|\tloss: 16.5289\n",
      "Training Epoch 6  51.9% | batch:       356 of       686\t|\tloss: 13.2323\n",
      "Training Epoch 6  52.0% | batch:       357 of       686\t|\tloss: 15.6519\n",
      "Training Epoch 6  52.2% | batch:       358 of       686\t|\tloss: 16.4966\n",
      "Training Epoch 6  52.3% | batch:       359 of       686\t|\tloss: 21.8038\n",
      "Training Epoch 6  52.5% | batch:       360 of       686\t|\tloss: 16.3716\n",
      "Training Epoch 6  52.6% | batch:       361 of       686\t|\tloss: 17.0155\n",
      "Training Epoch 6  52.8% | batch:       362 of       686\t|\tloss: 15.7655\n",
      "Training Epoch 6  52.9% | batch:       363 of       686\t|\tloss: 13.8232\n",
      "Training Epoch 6  53.1% | batch:       364 of       686\t|\tloss: 21.5446\n",
      "Training Epoch 6  53.2% | batch:       365 of       686\t|\tloss: 23.9749\n",
      "Training Epoch 6  53.4% | batch:       366 of       686\t|\tloss: 16.8451\n",
      "Training Epoch 6  53.5% | batch:       367 of       686\t|\tloss: 24.9489\n",
      "Training Epoch 6  53.6% | batch:       368 of       686\t|\tloss: 16.4624\n",
      "Training Epoch 6  53.8% | batch:       369 of       686\t|\tloss: 23.891\n",
      "Training Epoch 6  53.9% | batch:       370 of       686\t|\tloss: 17.6097\n",
      "Training Epoch 6  54.1% | batch:       371 of       686\t|\tloss: 18.0229\n",
      "Training Epoch 6  54.2% | batch:       372 of       686\t|\tloss: 15.8458\n",
      "Training Epoch 6  54.4% | batch:       373 of       686\t|\tloss: 18.7866\n",
      "Training Epoch 6  54.5% | batch:       374 of       686\t|\tloss: 15.7152\n",
      "Training Epoch 6  54.7% | batch:       375 of       686\t|\tloss: 18.0219\n",
      "Training Epoch 6  54.8% | batch:       376 of       686\t|\tloss: 14.8891\n",
      "Training Epoch 6  55.0% | batch:       377 of       686\t|\tloss: 17.6799\n",
      "Training Epoch 6  55.1% | batch:       378 of       686\t|\tloss: 19.2342\n",
      "Training Epoch 6  55.2% | batch:       379 of       686\t|\tloss: 9.89369\n",
      "Training Epoch 6  55.4% | batch:       380 of       686\t|\tloss: 19.3128\n",
      "Training Epoch 6  55.5% | batch:       381 of       686\t|\tloss: 15.8777\n",
      "Training Epoch 6  55.7% | batch:       382 of       686\t|\tloss: 14.6714\n",
      "Training Epoch 6  55.8% | batch:       383 of       686\t|\tloss: 16.3644\n",
      "Training Epoch 6  56.0% | batch:       384 of       686\t|\tloss: 22.3274\n",
      "Training Epoch 6  56.1% | batch:       385 of       686\t|\tloss: 19.5332\n",
      "Training Epoch 6  56.3% | batch:       386 of       686\t|\tloss: 16.8238\n",
      "Training Epoch 6  56.4% | batch:       387 of       686\t|\tloss: 17.2132\n",
      "Training Epoch 6  56.6% | batch:       388 of       686\t|\tloss: 22.879\n",
      "Training Epoch 6  56.7% | batch:       389 of       686\t|\tloss: 16.8962\n",
      "Training Epoch 6  56.9% | batch:       390 of       686\t|\tloss: 15.8304\n",
      "Training Epoch 6  57.0% | batch:       391 of       686\t|\tloss: 26.9799\n",
      "Training Epoch 6  57.1% | batch:       392 of       686\t|\tloss: 14.6209\n",
      "Training Epoch 6  57.3% | batch:       393 of       686\t|\tloss: 18.6834\n",
      "Training Epoch 6  57.4% | batch:       394 of       686\t|\tloss: 17.8115\n",
      "Training Epoch 6  57.6% | batch:       395 of       686\t|\tloss: 19.5148\n",
      "Training Epoch 6  57.7% | batch:       396 of       686\t|\tloss: 18.42\n",
      "Training Epoch 6  57.9% | batch:       397 of       686\t|\tloss: 18.2884\n",
      "Training Epoch 6  58.0% | batch:       398 of       686\t|\tloss: 15.1032\n",
      "Training Epoch 6  58.2% | batch:       399 of       686\t|\tloss: 14.0773\n",
      "Training Epoch 6  58.3% | batch:       400 of       686\t|\tloss: 20.8475\n",
      "Training Epoch 6  58.5% | batch:       401 of       686\t|\tloss: 12.9608\n",
      "Training Epoch 6  58.6% | batch:       402 of       686\t|\tloss: 18.4235\n",
      "Training Epoch 6  58.7% | batch:       403 of       686\t|\tloss: 17.1179\n",
      "Training Epoch 6  58.9% | batch:       404 of       686\t|\tloss: 16.9006\n",
      "Training Epoch 6  59.0% | batch:       405 of       686\t|\tloss: 17.5785\n",
      "Training Epoch 6  59.2% | batch:       406 of       686\t|\tloss: 27.9653\n",
      "Training Epoch 6  59.3% | batch:       407 of       686\t|\tloss: 22.0808\n",
      "Training Epoch 6  59.5% | batch:       408 of       686\t|\tloss: 24.2255\n",
      "Training Epoch 6  59.6% | batch:       409 of       686\t|\tloss: 18.5206\n",
      "Training Epoch 6  59.8% | batch:       410 of       686\t|\tloss: 18.2113\n",
      "Training Epoch 6  59.9% | batch:       411 of       686\t|\tloss: 20.1225\n",
      "Training Epoch 6  60.1% | batch:       412 of       686\t|\tloss: 17.5688\n",
      "Training Epoch 6  60.2% | batch:       413 of       686\t|\tloss: 16.1467\n",
      "Training Epoch 6  60.3% | batch:       414 of       686\t|\tloss: 19.8714\n",
      "Training Epoch 6  60.5% | batch:       415 of       686\t|\tloss: 19.9895\n",
      "Training Epoch 6  60.6% | batch:       416 of       686\t|\tloss: 23.174\n",
      "Training Epoch 6  60.8% | batch:       417 of       686\t|\tloss: 15.7033\n",
      "Training Epoch 6  60.9% | batch:       418 of       686\t|\tloss: 18.4891\n",
      "Training Epoch 6  61.1% | batch:       419 of       686\t|\tloss: 18.0408\n",
      "Training Epoch 6  61.2% | batch:       420 of       686\t|\tloss: 14.7563\n",
      "Training Epoch 6  61.4% | batch:       421 of       686\t|\tloss: 23.0771\n",
      "Training Epoch 6  61.5% | batch:       422 of       686\t|\tloss: 26.6382\n",
      "Training Epoch 6  61.7% | batch:       423 of       686\t|\tloss: 20.9573\n",
      "Training Epoch 6  61.8% | batch:       424 of       686\t|\tloss: 15.557\n",
      "Training Epoch 6  62.0% | batch:       425 of       686\t|\tloss: 19.2307\n",
      "Training Epoch 6  62.1% | batch:       426 of       686\t|\tloss: 18.3987\n",
      "Training Epoch 6  62.2% | batch:       427 of       686\t|\tloss: 17.2818\n",
      "Training Epoch 6  62.4% | batch:       428 of       686\t|\tloss: 14.6415\n",
      "Training Epoch 6  62.5% | batch:       429 of       686\t|\tloss: 18.5197\n",
      "Training Epoch 6  62.7% | batch:       430 of       686\t|\tloss: 26.5226\n",
      "Training Epoch 6  62.8% | batch:       431 of       686\t|\tloss: 16.5944\n",
      "Training Epoch 6  63.0% | batch:       432 of       686\t|\tloss: 17.3541\n",
      "Training Epoch 6  63.1% | batch:       433 of       686\t|\tloss: 18.0395\n",
      "Training Epoch 6  63.3% | batch:       434 of       686\t|\tloss: 14.1488\n",
      "Training Epoch 6  63.4% | batch:       435 of       686\t|\tloss: 16.3419\n",
      "Training Epoch 6  63.6% | batch:       436 of       686\t|\tloss: 23.7753\n",
      "Training Epoch 6  63.7% | batch:       437 of       686\t|\tloss: 18.6508\n",
      "Training Epoch 6  63.8% | batch:       438 of       686\t|\tloss: 17.9863\n",
      "Training Epoch 6  64.0% | batch:       439 of       686\t|\tloss: 16.6663\n",
      "Training Epoch 6  64.1% | batch:       440 of       686\t|\tloss: 17.5717\n",
      "Training Epoch 6  64.3% | batch:       441 of       686\t|\tloss: 15.9437\n",
      "Training Epoch 6  64.4% | batch:       442 of       686\t|\tloss: 23.4513\n",
      "Training Epoch 6  64.6% | batch:       443 of       686\t|\tloss: 15.2601\n",
      "Training Epoch 6  64.7% | batch:       444 of       686\t|\tloss: 15.3812\n",
      "Training Epoch 6  64.9% | batch:       445 of       686\t|\tloss: 16.1281\n",
      "Training Epoch 6  65.0% | batch:       446 of       686\t|\tloss: 17.7967\n",
      "Training Epoch 6  65.2% | batch:       447 of       686\t|\tloss: 15.1687\n",
      "Training Epoch 6  65.3% | batch:       448 of       686\t|\tloss: 14.3043\n",
      "Training Epoch 6  65.5% | batch:       449 of       686\t|\tloss: 16.899\n",
      "Training Epoch 6  65.6% | batch:       450 of       686\t|\tloss: 19.0071\n",
      "Training Epoch 6  65.7% | batch:       451 of       686\t|\tloss: 19.1311\n",
      "Training Epoch 6  65.9% | batch:       452 of       686\t|\tloss: 26.9379\n",
      "Training Epoch 6  66.0% | batch:       453 of       686\t|\tloss: 15.7206\n",
      "Training Epoch 6  66.2% | batch:       454 of       686\t|\tloss: 20.9611\n",
      "Training Epoch 6  66.3% | batch:       455 of       686\t|\tloss: 14.5654\n",
      "Training Epoch 6  66.5% | batch:       456 of       686\t|\tloss: 17.0482\n",
      "Training Epoch 6  66.6% | batch:       457 of       686\t|\tloss: 18.6293\n",
      "Training Epoch 6  66.8% | batch:       458 of       686\t|\tloss: 18.5134\n",
      "Training Epoch 6  66.9% | batch:       459 of       686\t|\tloss: 14.8577\n",
      "Training Epoch 6  67.1% | batch:       460 of       686\t|\tloss: 20.6738\n",
      "Training Epoch 6  67.2% | batch:       461 of       686\t|\tloss: 16.3578\n",
      "Training Epoch 6  67.3% | batch:       462 of       686\t|\tloss: 17.6118\n",
      "Training Epoch 6  67.5% | batch:       463 of       686\t|\tloss: 15.0449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  67.6% | batch:       464 of       686\t|\tloss: 17.5258\n",
      "Training Epoch 6  67.8% | batch:       465 of       686\t|\tloss: 18.5247\n",
      "Training Epoch 6  67.9% | batch:       466 of       686\t|\tloss: 18.5722\n",
      "Training Epoch 6  68.1% | batch:       467 of       686\t|\tloss: 17.9705\n",
      "Training Epoch 6  68.2% | batch:       468 of       686\t|\tloss: 14.9524\n",
      "Training Epoch 6  68.4% | batch:       469 of       686\t|\tloss: 24.7881\n",
      "Training Epoch 6  68.5% | batch:       470 of       686\t|\tloss: 23.9797\n",
      "Training Epoch 6  68.7% | batch:       471 of       686\t|\tloss: 19.1105\n",
      "Training Epoch 6  68.8% | batch:       472 of       686\t|\tloss: 24.245\n",
      "Training Epoch 6  69.0% | batch:       473 of       686\t|\tloss: 16.7477\n",
      "Training Epoch 6  69.1% | batch:       474 of       686\t|\tloss: 15.6372\n",
      "Training Epoch 6  69.2% | batch:       475 of       686\t|\tloss: 17.7602\n",
      "Training Epoch 6  69.4% | batch:       476 of       686\t|\tloss: 15.3067\n",
      "Training Epoch 6  69.5% | batch:       477 of       686\t|\tloss: 17.8258\n",
      "Training Epoch 6  69.7% | batch:       478 of       686\t|\tloss: 12.3941\n",
      "Training Epoch 6  69.8% | batch:       479 of       686\t|\tloss: 12.8679\n",
      "Training Epoch 6  70.0% | batch:       480 of       686\t|\tloss: 14.0125\n",
      "Training Epoch 6  70.1% | batch:       481 of       686\t|\tloss: 16.9549\n",
      "Training Epoch 6  70.3% | batch:       482 of       686\t|\tloss: 19.9758\n",
      "Training Epoch 6  70.4% | batch:       483 of       686\t|\tloss: 17.2765\n",
      "Training Epoch 6  70.6% | batch:       484 of       686\t|\tloss: 20.0203\n",
      "Training Epoch 6  70.7% | batch:       485 of       686\t|\tloss: 20.6027\n",
      "Training Epoch 6  70.8% | batch:       486 of       686\t|\tloss: 16.604\n",
      "Training Epoch 6  71.0% | batch:       487 of       686\t|\tloss: 18.1948\n",
      "Training Epoch 6  71.1% | batch:       488 of       686\t|\tloss: 17.5045\n",
      "Training Epoch 6  71.3% | batch:       489 of       686\t|\tloss: 17.2376\n",
      "Training Epoch 6  71.4% | batch:       490 of       686\t|\tloss: 17.8139\n",
      "Training Epoch 6  71.6% | batch:       491 of       686\t|\tloss: 30.4383\n",
      "Training Epoch 6  71.7% | batch:       492 of       686\t|\tloss: 12.9122\n",
      "Training Epoch 6  71.9% | batch:       493 of       686\t|\tloss: 18.9182\n",
      "Training Epoch 6  72.0% | batch:       494 of       686\t|\tloss: 13.2899\n",
      "Training Epoch 6  72.2% | batch:       495 of       686\t|\tloss: 19.52\n",
      "Training Epoch 6  72.3% | batch:       496 of       686\t|\tloss: 16.3532\n",
      "Training Epoch 6  72.4% | batch:       497 of       686\t|\tloss: 16.4331\n",
      "Training Epoch 6  72.6% | batch:       498 of       686\t|\tloss: 15.9558\n",
      "Training Epoch 6  72.7% | batch:       499 of       686\t|\tloss: 14.9528\n",
      "Training Epoch 6  72.9% | batch:       500 of       686\t|\tloss: 16.1153\n",
      "Training Epoch 6  73.0% | batch:       501 of       686\t|\tloss: 17.4909\n",
      "Training Epoch 6  73.2% | batch:       502 of       686\t|\tloss: 15.6481\n",
      "Training Epoch 6  73.3% | batch:       503 of       686\t|\tloss: 17.502\n",
      "Training Epoch 6  73.5% | batch:       504 of       686\t|\tloss: 16.9976\n",
      "Training Epoch 6  73.6% | batch:       505 of       686\t|\tloss: 14.9418\n",
      "Training Epoch 6  73.8% | batch:       506 of       686\t|\tloss: 17.3316\n",
      "Training Epoch 6  73.9% | batch:       507 of       686\t|\tloss: 20.4484\n",
      "Training Epoch 6  74.1% | batch:       508 of       686\t|\tloss: 16.7266\n",
      "Training Epoch 6  74.2% | batch:       509 of       686\t|\tloss: 28.0603\n",
      "Training Epoch 6  74.3% | batch:       510 of       686\t|\tloss: 22.0342\n",
      "Training Epoch 6  74.5% | batch:       511 of       686\t|\tloss: 12.5455\n",
      "Training Epoch 6  74.6% | batch:       512 of       686\t|\tloss: 18.3317\n",
      "Training Epoch 6  74.8% | batch:       513 of       686\t|\tloss: 15.0477\n",
      "Training Epoch 6  74.9% | batch:       514 of       686\t|\tloss: 14.9405\n",
      "Training Epoch 6  75.1% | batch:       515 of       686\t|\tloss: 28.1417\n",
      "Training Epoch 6  75.2% | batch:       516 of       686\t|\tloss: 17.8383\n",
      "Training Epoch 6  75.4% | batch:       517 of       686\t|\tloss: 21.6164\n",
      "Training Epoch 6  75.5% | batch:       518 of       686\t|\tloss: 17.075\n",
      "Training Epoch 6  75.7% | batch:       519 of       686\t|\tloss: 20.0238\n",
      "Training Epoch 6  75.8% | batch:       520 of       686\t|\tloss: 17.2818\n",
      "Training Epoch 6  75.9% | batch:       521 of       686\t|\tloss: 29.8917\n",
      "Training Epoch 6  76.1% | batch:       522 of       686\t|\tloss: 17.9604\n",
      "Training Epoch 6  76.2% | batch:       523 of       686\t|\tloss: 20.8681\n",
      "Training Epoch 6  76.4% | batch:       524 of       686\t|\tloss: 12.797\n",
      "Training Epoch 6  76.5% | batch:       525 of       686\t|\tloss: 21.4099\n",
      "Training Epoch 6  76.7% | batch:       526 of       686\t|\tloss: 16.3512\n",
      "Training Epoch 6  76.8% | batch:       527 of       686\t|\tloss: 18.9067\n",
      "Training Epoch 6  77.0% | batch:       528 of       686\t|\tloss: 15.7819\n",
      "Training Epoch 6  77.1% | batch:       529 of       686\t|\tloss: 15.2103\n",
      "Training Epoch 6  77.3% | batch:       530 of       686\t|\tloss: 19.4523\n",
      "Training Epoch 6  77.4% | batch:       531 of       686\t|\tloss: 21.0488\n",
      "Training Epoch 6  77.6% | batch:       532 of       686\t|\tloss: 18.6483\n",
      "Training Epoch 6  77.7% | batch:       533 of       686\t|\tloss: 18.7834\n",
      "Training Epoch 6  77.8% | batch:       534 of       686\t|\tloss: 22.7433\n",
      "Training Epoch 6  78.0% | batch:       535 of       686\t|\tloss: 22.0655\n",
      "Training Epoch 6  78.1% | batch:       536 of       686\t|\tloss: 11.9631\n",
      "Training Epoch 6  78.3% | batch:       537 of       686\t|\tloss: 17.5702\n",
      "Training Epoch 6  78.4% | batch:       538 of       686\t|\tloss: 15.7672\n",
      "Training Epoch 6  78.6% | batch:       539 of       686\t|\tloss: 16.4819\n",
      "Training Epoch 6  78.7% | batch:       540 of       686\t|\tloss: 21.0402\n",
      "Training Epoch 6  78.9% | batch:       541 of       686\t|\tloss: 17.6161\n",
      "Training Epoch 6  79.0% | batch:       542 of       686\t|\tloss: 16.3293\n",
      "Training Epoch 6  79.2% | batch:       543 of       686\t|\tloss: 17.5802\n",
      "Training Epoch 6  79.3% | batch:       544 of       686\t|\tloss: 14.1129\n",
      "Training Epoch 6  79.4% | batch:       545 of       686\t|\tloss: 18.475\n",
      "Training Epoch 6  79.6% | batch:       546 of       686\t|\tloss: 18.3887\n",
      "Training Epoch 6  79.7% | batch:       547 of       686\t|\tloss: 15.6264\n",
      "Training Epoch 6  79.9% | batch:       548 of       686\t|\tloss: 19.794\n",
      "Training Epoch 6  80.0% | batch:       549 of       686\t|\tloss: 13.2738\n",
      "Training Epoch 6  80.2% | batch:       550 of       686\t|\tloss: 15.1403\n",
      "Training Epoch 6  80.3% | batch:       551 of       686\t|\tloss: 14.9943\n",
      "Training Epoch 6  80.5% | batch:       552 of       686\t|\tloss: 20.0176\n",
      "Training Epoch 6  80.6% | batch:       553 of       686\t|\tloss: 19.7927\n",
      "Training Epoch 6  80.8% | batch:       554 of       686\t|\tloss: 23.5089\n",
      "Training Epoch 6  80.9% | batch:       555 of       686\t|\tloss: 15.2744\n",
      "Training Epoch 6  81.0% | batch:       556 of       686\t|\tloss: 19.2599\n",
      "Training Epoch 6  81.2% | batch:       557 of       686\t|\tloss: 17.2398\n",
      "Training Epoch 6  81.3% | batch:       558 of       686\t|\tloss: 20.9066\n",
      "Training Epoch 6  81.5% | batch:       559 of       686\t|\tloss: 17.376\n",
      "Training Epoch 6  81.6% | batch:       560 of       686\t|\tloss: 17.9159\n",
      "Training Epoch 6  81.8% | batch:       561 of       686\t|\tloss: 15.9116\n",
      "Training Epoch 6  81.9% | batch:       562 of       686\t|\tloss: 26.8252\n",
      "Training Epoch 6  82.1% | batch:       563 of       686\t|\tloss: 16.3832\n",
      "Training Epoch 6  82.2% | batch:       564 of       686\t|\tloss: 13.7932\n",
      "Training Epoch 6  82.4% | batch:       565 of       686\t|\tloss: 16.4312\n",
      "Training Epoch 6  82.5% | batch:       566 of       686\t|\tloss: 16.3269\n",
      "Training Epoch 6  82.7% | batch:       567 of       686\t|\tloss: 17.4145\n",
      "Training Epoch 6  82.8% | batch:       568 of       686\t|\tloss: 15.6398\n",
      "Training Epoch 6  82.9% | batch:       569 of       686\t|\tloss: 18.6134\n",
      "Training Epoch 6  83.1% | batch:       570 of       686\t|\tloss: 15.956\n",
      "Training Epoch 6  83.2% | batch:       571 of       686\t|\tloss: 16.3246\n",
      "Training Epoch 6  83.4% | batch:       572 of       686\t|\tloss: 15.5746\n",
      "Training Epoch 6  83.5% | batch:       573 of       686\t|\tloss: 17.5635\n",
      "Training Epoch 6  83.7% | batch:       574 of       686\t|\tloss: 16.6133\n",
      "Training Epoch 6  83.8% | batch:       575 of       686\t|\tloss: 14.8507\n",
      "Training Epoch 6  84.0% | batch:       576 of       686\t|\tloss: 17.8207\n",
      "Training Epoch 6  84.1% | batch:       577 of       686\t|\tloss: 17.441\n",
      "Training Epoch 6  84.3% | batch:       578 of       686\t|\tloss: 25.1285\n",
      "Training Epoch 6  84.4% | batch:       579 of       686\t|\tloss: 17.0569\n",
      "Training Epoch 6  84.5% | batch:       580 of       686\t|\tloss: 19.3623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  84.7% | batch:       581 of       686\t|\tloss: 17.1613\n",
      "Training Epoch 6  84.8% | batch:       582 of       686\t|\tloss: 14.3574\n",
      "Training Epoch 6  85.0% | batch:       583 of       686\t|\tloss: 17.7724\n",
      "Training Epoch 6  85.1% | batch:       584 of       686\t|\tloss: 20.0581\n",
      "Training Epoch 6  85.3% | batch:       585 of       686\t|\tloss: 15.8036\n",
      "Training Epoch 6  85.4% | batch:       586 of       686\t|\tloss: 22.6004\n",
      "Training Epoch 6  85.6% | batch:       587 of       686\t|\tloss: 17.8082\n",
      "Training Epoch 6  85.7% | batch:       588 of       686\t|\tloss: 20.2145\n",
      "Training Epoch 6  85.9% | batch:       589 of       686\t|\tloss: 16.4889\n",
      "Training Epoch 6  86.0% | batch:       590 of       686\t|\tloss: 17.4078\n",
      "Training Epoch 6  86.2% | batch:       591 of       686\t|\tloss: 16.8103\n",
      "Training Epoch 6  86.3% | batch:       592 of       686\t|\tloss: 17.9066\n",
      "Training Epoch 6  86.4% | batch:       593 of       686\t|\tloss: 18.7361\n",
      "Training Epoch 6  86.6% | batch:       594 of       686\t|\tloss: 18.1314\n",
      "Training Epoch 6  86.7% | batch:       595 of       686\t|\tloss: 15.1488\n",
      "Training Epoch 6  86.9% | batch:       596 of       686\t|\tloss: 12.3023\n",
      "Training Epoch 6  87.0% | batch:       597 of       686\t|\tloss: 15.4944\n",
      "Training Epoch 6  87.2% | batch:       598 of       686\t|\tloss: 21.1829\n",
      "Training Epoch 6  87.3% | batch:       599 of       686\t|\tloss: 16.6618\n",
      "Training Epoch 6  87.5% | batch:       600 of       686\t|\tloss: 13.9854\n",
      "Training Epoch 6  87.6% | batch:       601 of       686\t|\tloss: 19.0251\n",
      "Training Epoch 6  87.8% | batch:       602 of       686\t|\tloss: 18.6505\n",
      "Training Epoch 6  87.9% | batch:       603 of       686\t|\tloss: 13.1953\n",
      "Training Epoch 6  88.0% | batch:       604 of       686\t|\tloss: 17.3805\n",
      "Training Epoch 6  88.2% | batch:       605 of       686\t|\tloss: 22.2017\n",
      "Training Epoch 6  88.3% | batch:       606 of       686\t|\tloss: 20.677\n",
      "Training Epoch 6  88.5% | batch:       607 of       686\t|\tloss: 18.3423\n",
      "Training Epoch 6  88.6% | batch:       608 of       686\t|\tloss: 15.1351\n",
      "Training Epoch 6  88.8% | batch:       609 of       686\t|\tloss: 13.1117\n",
      "Training Epoch 6  88.9% | batch:       610 of       686\t|\tloss: 15.5327\n",
      "Training Epoch 6  89.1% | batch:       611 of       686\t|\tloss: 34.3374\n",
      "Training Epoch 6  89.2% | batch:       612 of       686\t|\tloss: 16.5448\n",
      "Training Epoch 6  89.4% | batch:       613 of       686\t|\tloss: 14.2371\n",
      "Training Epoch 6  89.5% | batch:       614 of       686\t|\tloss: 12.0203\n",
      "Training Epoch 6  89.7% | batch:       615 of       686\t|\tloss: 17.1732\n",
      "Training Epoch 6  89.8% | batch:       616 of       686\t|\tloss: 22.2299\n",
      "Training Epoch 6  89.9% | batch:       617 of       686\t|\tloss: 18.6126\n",
      "Training Epoch 6  90.1% | batch:       618 of       686\t|\tloss: 16.9468\n",
      "Training Epoch 6  90.2% | batch:       619 of       686\t|\tloss: 23.8504\n",
      "Training Epoch 6  90.4% | batch:       620 of       686\t|\tloss: 22.4507\n",
      "Training Epoch 6  90.5% | batch:       621 of       686\t|\tloss: 18.0038\n",
      "Training Epoch 6  90.7% | batch:       622 of       686\t|\tloss: 13.5634\n",
      "Training Epoch 6  90.8% | batch:       623 of       686\t|\tloss: 16.1688\n",
      "Training Epoch 6  91.0% | batch:       624 of       686\t|\tloss: 14.8208\n",
      "Training Epoch 6  91.1% | batch:       625 of       686\t|\tloss: 18.0308\n",
      "Training Epoch 6  91.3% | batch:       626 of       686\t|\tloss: 18.9243\n",
      "Training Epoch 6  91.4% | batch:       627 of       686\t|\tloss: 13.2318\n",
      "Training Epoch 6  91.5% | batch:       628 of       686\t|\tloss: 16.316\n",
      "Training Epoch 6  91.7% | batch:       629 of       686\t|\tloss: 15.9763\n",
      "Training Epoch 6  91.8% | batch:       630 of       686\t|\tloss: 19.9105\n",
      "Training Epoch 6  92.0% | batch:       631 of       686\t|\tloss: 13.2299\n",
      "Training Epoch 6  92.1% | batch:       632 of       686\t|\tloss: 16.7064\n",
      "Training Epoch 6  92.3% | batch:       633 of       686\t|\tloss: 14.8273\n",
      "Training Epoch 6  92.4% | batch:       634 of       686\t|\tloss: 12.0609\n",
      "Training Epoch 6  92.6% | batch:       635 of       686\t|\tloss: 16.4045\n",
      "Training Epoch 6  92.7% | batch:       636 of       686\t|\tloss: 19.7686\n",
      "Training Epoch 6  92.9% | batch:       637 of       686\t|\tloss: 17.0611\n",
      "Training Epoch 6  93.0% | batch:       638 of       686\t|\tloss: 19.9632\n",
      "Training Epoch 6  93.1% | batch:       639 of       686\t|\tloss: 18.449\n",
      "Training Epoch 6  93.3% | batch:       640 of       686\t|\tloss: 15.8977\n",
      "Training Epoch 6  93.4% | batch:       641 of       686\t|\tloss: 14.3678\n",
      "Training Epoch 6  93.6% | batch:       642 of       686\t|\tloss: 11.8305\n",
      "Training Epoch 6  93.7% | batch:       643 of       686\t|\tloss: 16.4647\n",
      "Training Epoch 6  93.9% | batch:       644 of       686\t|\tloss: 18.0178\n",
      "Training Epoch 6  94.0% | batch:       645 of       686\t|\tloss: 13.4735\n",
      "Training Epoch 6  94.2% | batch:       646 of       686\t|\tloss: 16.2373\n",
      "Training Epoch 6  94.3% | batch:       647 of       686\t|\tloss: 14.0696\n",
      "Training Epoch 6  94.5% | batch:       648 of       686\t|\tloss: 14.9026\n",
      "Training Epoch 6  94.6% | batch:       649 of       686\t|\tloss: 17.8005\n",
      "Training Epoch 6  94.8% | batch:       650 of       686\t|\tloss: 15.7285\n",
      "Training Epoch 6  94.9% | batch:       651 of       686\t|\tloss: 14.537\n",
      "Training Epoch 6  95.0% | batch:       652 of       686\t|\tloss: 17.3993\n",
      "Training Epoch 6  95.2% | batch:       653 of       686\t|\tloss: 16.5004\n",
      "Training Epoch 6  95.3% | batch:       654 of       686\t|\tloss: 14.6897\n",
      "Training Epoch 6  95.5% | batch:       655 of       686\t|\tloss: 19.047\n",
      "Training Epoch 6  95.6% | batch:       656 of       686\t|\tloss: 18.4839\n",
      "Training Epoch 6  95.8% | batch:       657 of       686\t|\tloss: 17.9981\n",
      "Training Epoch 6  95.9% | batch:       658 of       686\t|\tloss: 11.7558\n",
      "Training Epoch 6  96.1% | batch:       659 of       686\t|\tloss: 16.8689\n",
      "Training Epoch 6  96.2% | batch:       660 of       686\t|\tloss: 17.0002\n",
      "Training Epoch 6  96.4% | batch:       661 of       686\t|\tloss: 17.8016\n",
      "Training Epoch 6  96.5% | batch:       662 of       686\t|\tloss: 19.2274\n",
      "Training Epoch 6  96.6% | batch:       663 of       686\t|\tloss: 10.4543\n",
      "Training Epoch 6  96.8% | batch:       664 of       686\t|\tloss: 13.477\n",
      "Training Epoch 6  96.9% | batch:       665 of       686\t|\tloss: 18.51\n",
      "Training Epoch 6  97.1% | batch:       666 of       686\t|\tloss: 22.2539\n",
      "Training Epoch 6  97.2% | batch:       667 of       686\t|\tloss: 15.81\n",
      "Training Epoch 6  97.4% | batch:       668 of       686\t|\tloss: 19.5431\n",
      "Training Epoch 6  97.5% | batch:       669 of       686\t|\tloss: 13.7103\n",
      "Training Epoch 6  97.7% | batch:       670 of       686\t|\tloss: 29.5676\n",
      "Training Epoch 6  97.8% | batch:       671 of       686\t|\tloss: 15.3179\n",
      "Training Epoch 6  98.0% | batch:       672 of       686\t|\tloss: 14.2682\n",
      "Training Epoch 6  98.1% | batch:       673 of       686\t|\tloss: 14.4342\n",
      "Training Epoch 6  98.3% | batch:       674 of       686\t|\tloss: 15.4495\n",
      "Training Epoch 6  98.4% | batch:       675 of       686\t|\tloss: 14.4484\n",
      "Training Epoch 6  98.5% | batch:       676 of       686\t|\tloss: 17.5413\n",
      "Training Epoch 6  98.7% | batch:       677 of       686\t|\tloss: 12.7022\n",
      "Training Epoch 6  98.8% | batch:       678 of       686\t|\tloss: 14.9691\n",
      "Training Epoch 6  99.0% | batch:       679 of       686\t|\tloss: 14.2096\n",
      "Training Epoch 6  99.1% | batch:       680 of       686\t|\tloss: 19.1366\n",
      "Training Epoch 6  99.3% | batch:       681 of       686\t|\tloss: 19.6088\n",
      "Training Epoch 6  99.4% | batch:       682 of       686\t|\tloss: 22.0159\n",
      "Training Epoch 6  99.6% | batch:       683 of       686\t|\tloss: 11.272\n",
      "Training Epoch 6  99.7% | batch:       684 of       686\t|\tloss: 16.7325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:02:32,816 | INFO : Epoch 6 Training Summary: epoch: 6.000000 | loss: 18.954074 | \n",
      "2023-05-25 22:02:32,817 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 24.85420274734497 seconds\n",
      "\n",
      "2023-05-25 22:02:32,817 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.551232020060223 seconds\n",
      "2023-05-25 22:02:32,818 | INFO : Avg batch train. time: 0.03433124201174959 seconds\n",
      "2023-05-25 22:02:32,818 | INFO : Avg sample train. time: 0.0002685584357153797 seconds\n",
      "2023-05-25 22:02:32,818 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  99.9% | batch:       685 of       686\t|\tloss: 6.74479\n",
      "\n",
      "Evaluating Epoch 6   0.0% | batch:         0 of       172\t|\tloss: 1.87106\n",
      "Evaluating Epoch 6   0.6% | batch:         1 of       172\t|\tloss: 3.06643\n",
      "Evaluating Epoch 6   1.2% | batch:         2 of       172\t|\tloss: 2.46742\n",
      "Evaluating Epoch 6   1.7% | batch:         3 of       172\t|\tloss: 4.08697\n",
      "Evaluating Epoch 6   2.3% | batch:         4 of       172\t|\tloss: 2.78681\n",
      "Evaluating Epoch 6   2.9% | batch:         5 of       172\t|\tloss: 2.23221\n",
      "Evaluating Epoch 6   3.5% | batch:         6 of       172\t|\tloss: 2.86312\n",
      "Evaluating Epoch 6   4.1% | batch:         7 of       172\t|\tloss: 4.31896\n",
      "Evaluating Epoch 6   4.7% | batch:         8 of       172\t|\tloss: 2.1501\n",
      "Evaluating Epoch 6   5.2% | batch:         9 of       172\t|\tloss: 2.75247\n",
      "Evaluating Epoch 6   5.8% | batch:        10 of       172\t|\tloss: 3.91181\n",
      "Evaluating Epoch 6   6.4% | batch:        11 of       172\t|\tloss: 2.38567\n",
      "Evaluating Epoch 6   7.0% | batch:        12 of       172\t|\tloss: 2.73934\n",
      "Evaluating Epoch 6   7.6% | batch:        13 of       172\t|\tloss: 2.80007\n",
      "Evaluating Epoch 6   8.1% | batch:        14 of       172\t|\tloss: 3.64632\n",
      "Evaluating Epoch 6   8.7% | batch:        15 of       172\t|\tloss: 1.9898\n",
      "Evaluating Epoch 6   9.3% | batch:        16 of       172\t|\tloss: 3.59421\n",
      "Evaluating Epoch 6   9.9% | batch:        17 of       172\t|\tloss: 2.26233\n",
      "Evaluating Epoch 6  10.5% | batch:        18 of       172\t|\tloss: 7.46091\n",
      "Evaluating Epoch 6  11.0% | batch:        19 of       172\t|\tloss: 1.03692\n",
      "Evaluating Epoch 6  11.6% | batch:        20 of       172\t|\tloss: 4.45258\n",
      "Evaluating Epoch 6  12.2% | batch:        21 of       172\t|\tloss: 0.592085\n",
      "Evaluating Epoch 6  12.8% | batch:        22 of       172\t|\tloss: 1.37034\n",
      "Evaluating Epoch 6  13.4% | batch:        23 of       172\t|\tloss: 1.18299\n",
      "Evaluating Epoch 6  14.0% | batch:        24 of       172\t|\tloss: 3.00212\n",
      "Evaluating Epoch 6  14.5% | batch:        25 of       172\t|\tloss: 3.95968\n",
      "Evaluating Epoch 6  15.1% | batch:        26 of       172\t|\tloss: 5.93935\n",
      "Evaluating Epoch 6  15.7% | batch:        27 of       172\t|\tloss: 7.08167\n",
      "Evaluating Epoch 6  16.3% | batch:        28 of       172\t|\tloss: 0.611092\n",
      "Evaluating Epoch 6  16.9% | batch:        29 of       172\t|\tloss: 3.68206\n",
      "Evaluating Epoch 6  17.4% | batch:        30 of       172\t|\tloss: 0.772559\n",
      "Evaluating Epoch 6  18.0% | batch:        31 of       172\t|\tloss: 5.57907\n",
      "Evaluating Epoch 6  18.6% | batch:        32 of       172\t|\tloss: 1.15823\n",
      "Evaluating Epoch 6  19.2% | batch:        33 of       172\t|\tloss: 2.56643\n",
      "Evaluating Epoch 6  19.8% | batch:        34 of       172\t|\tloss: 1.20295\n",
      "Evaluating Epoch 6  20.3% | batch:        35 of       172\t|\tloss: 0.806875\n",
      "Evaluating Epoch 6  20.9% | batch:        36 of       172\t|\tloss: 6.57881\n",
      "Evaluating Epoch 6  21.5% | batch:        37 of       172\t|\tloss: 3.29201\n",
      "Evaluating Epoch 6  22.1% | batch:        38 of       172\t|\tloss: 2.64053\n",
      "Evaluating Epoch 6  22.7% | batch:        39 of       172\t|\tloss: 1.78027\n",
      "Evaluating Epoch 6  23.3% | batch:        40 of       172\t|\tloss: 1.13993\n",
      "Evaluating Epoch 6  23.8% | batch:        41 of       172\t|\tloss: 2.89295\n",
      "Evaluating Epoch 6  24.4% | batch:        42 of       172\t|\tloss: 1.01465\n",
      "Evaluating Epoch 6  25.0% | batch:        43 of       172\t|\tloss: 8.74207\n",
      "Evaluating Epoch 6  25.6% | batch:        44 of       172\t|\tloss: 1.15022\n",
      "Evaluating Epoch 6  26.2% | batch:        45 of       172\t|\tloss: 2.37181\n",
      "Evaluating Epoch 6  26.7% | batch:        46 of       172\t|\tloss: 0.528507\n",
      "Evaluating Epoch 6  27.3% | batch:        47 of       172\t|\tloss: 4.67511\n",
      "Evaluating Epoch 6  27.9% | batch:        48 of       172\t|\tloss: 1.06856\n",
      "Evaluating Epoch 6  28.5% | batch:        49 of       172\t|\tloss: 3.61974\n",
      "Evaluating Epoch 6  29.1% | batch:        50 of       172\t|\tloss: 1.23923\n",
      "Evaluating Epoch 6  29.7% | batch:        51 of       172\t|\tloss: 1.3901\n",
      "Evaluating Epoch 6  30.2% | batch:        52 of       172\t|\tloss: 1.97687\n",
      "Evaluating Epoch 6  30.8% | batch:        53 of       172\t|\tloss: 1.14409\n",
      "Evaluating Epoch 6  31.4% | batch:        54 of       172\t|\tloss: 1.72764\n",
      "Evaluating Epoch 6  32.0% | batch:        55 of       172\t|\tloss: 1.95494\n",
      "Evaluating Epoch 6  32.6% | batch:        56 of       172\t|\tloss: 1.33784\n",
      "Evaluating Epoch 6  33.1% | batch:        57 of       172\t|\tloss: 2.44208\n",
      "Evaluating Epoch 6  33.7% | batch:        58 of       172\t|\tloss: 1.27414\n",
      "Evaluating Epoch 6  34.3% | batch:        59 of       172\t|\tloss: 2.77435\n",
      "Evaluating Epoch 6  34.9% | batch:        60 of       172\t|\tloss: 1.20767\n",
      "Evaluating Epoch 6  35.5% | batch:        61 of       172\t|\tloss: 2.79151\n",
      "Evaluating Epoch 6  36.0% | batch:        62 of       172\t|\tloss: 1.24519\n",
      "Evaluating Epoch 6  36.6% | batch:        63 of       172\t|\tloss: 2.07729\n",
      "Evaluating Epoch 6  37.2% | batch:        64 of       172\t|\tloss: 1.95845\n",
      "Evaluating Epoch 6  37.8% | batch:        65 of       172\t|\tloss: 1.08749\n",
      "Evaluating Epoch 6  38.4% | batch:        66 of       172\t|\tloss: 3.18511\n",
      "Evaluating Epoch 6  39.0% | batch:        67 of       172\t|\tloss: 1.28519\n",
      "Evaluating Epoch 6  39.5% | batch:        68 of       172\t|\tloss: 2.86662\n",
      "Evaluating Epoch 6  40.1% | batch:        69 of       172\t|\tloss: 3.32054\n",
      "Evaluating Epoch 6  40.7% | batch:        70 of       172\t|\tloss: 0.626747\n",
      "Evaluating Epoch 6  41.3% | batch:        71 of       172\t|\tloss: 1.93595\n",
      "Evaluating Epoch 6  41.9% | batch:        72 of       172\t|\tloss: 1.66258\n",
      "Evaluating Epoch 6  42.4% | batch:        73 of       172\t|\tloss: 1.16959\n",
      "Evaluating Epoch 6  43.0% | batch:        74 of       172\t|\tloss: 0.435753\n",
      "Evaluating Epoch 6  43.6% | batch:        75 of       172\t|\tloss: 0.685149\n",
      "Evaluating Epoch 6  44.2% | batch:        76 of       172\t|\tloss: 0.828874\n",
      "Evaluating Epoch 6  44.8% | batch:        77 of       172\t|\tloss: 0.774551\n",
      "Evaluating Epoch 6  45.3% | batch:        78 of       172\t|\tloss: 0.493492\n",
      "Evaluating Epoch 6  45.9% | batch:        79 of       172\t|\tloss: 0.703451\n",
      "Evaluating Epoch 6  46.5% | batch:        80 of       172\t|\tloss: 0.901986\n",
      "Evaluating Epoch 6  47.1% | batch:        81 of       172\t|\tloss: 0.623449\n",
      "Evaluating Epoch 6  47.7% | batch:        82 of       172\t|\tloss: 0.77794\n",
      "Evaluating Epoch 6  48.3% | batch:        83 of       172\t|\tloss: 1.03598\n",
      "Evaluating Epoch 6  48.8% | batch:        84 of       172\t|\tloss: 0.946988\n",
      "Evaluating Epoch 6  49.4% | batch:        85 of       172\t|\tloss: 1.92777\n",
      "Evaluating Epoch 6  50.0% | batch:        86 of       172\t|\tloss: 1.65067\n",
      "Evaluating Epoch 6  50.6% | batch:        87 of       172\t|\tloss: 0.861127\n",
      "Evaluating Epoch 6  51.2% | batch:        88 of       172\t|\tloss: 1.22274\n",
      "Evaluating Epoch 6  51.7% | batch:        89 of       172\t|\tloss: 2.39346\n",
      "Evaluating Epoch 6  52.3% | batch:        90 of       172\t|\tloss: 1.6085\n",
      "Evaluating Epoch 6  52.9% | batch:        91 of       172\t|\tloss: 1.66772\n",
      "Evaluating Epoch 6  53.5% | batch:        92 of       172\t|\tloss: 2.15166\n",
      "Evaluating Epoch 6  54.1% | batch:        93 of       172\t|\tloss: 1.6438\n",
      "Evaluating Epoch 6  54.7% | batch:        94 of       172\t|\tloss: 1.93004\n",
      "Evaluating Epoch 6  55.2% | batch:        95 of       172\t|\tloss: 1.4085\n",
      "Evaluating Epoch 6  55.8% | batch:        96 of       172\t|\tloss: 2.03914\n",
      "Evaluating Epoch 6  56.4% | batch:        97 of       172\t|\tloss: 0.980385\n",
      "Evaluating Epoch 6  57.0% | batch:        98 of       172\t|\tloss: 1.18543\n",
      "Evaluating Epoch 6  57.6% | batch:        99 of       172\t|\tloss: 2.66395\n",
      "Evaluating Epoch 6  58.1% | batch:       100 of       172\t|\tloss: 0.965212\n",
      "Evaluating Epoch 6  58.7% | batch:       101 of       172\t|\tloss: 0.961105\n",
      "Evaluating Epoch 6  59.3% | batch:       102 of       172\t|\tloss: 1.6019\n",
      "Evaluating Epoch 6  59.9% | batch:       103 of       172\t|\tloss: 1.90124\n",
      "Evaluating Epoch 6  60.5% | batch:       104 of       172\t|\tloss: 1.06958\n",
      "Evaluating Epoch 6  61.0% | batch:       105 of       172\t|\tloss: 1.02688\n",
      "Evaluating Epoch 6  61.6% | batch:       106 of       172\t|\tloss: 2.56603\n",
      "Evaluating Epoch 6  62.2% | batch:       107 of       172\t|\tloss: 1.57335\n",
      "Evaluating Epoch 6  62.8% | batch:       108 of       172\t|\tloss: 1.31466\n",
      "Evaluating Epoch 6  63.4% | batch:       109 of       172\t|\tloss: 1.8129\n",
      "Evaluating Epoch 6  64.0% | batch:       110 of       172\t|\tloss: 2.37085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 6  64.5% | batch:       111 of       172\t|\tloss: 1.18799\n",
      "Evaluating Epoch 6  65.1% | batch:       112 of       172\t|\tloss: 0.932452\n",
      "Evaluating Epoch 6  65.7% | batch:       113 of       172\t|\tloss: 1.93022\n",
      "Evaluating Epoch 6  66.3% | batch:       114 of       172\t|\tloss: 3.25834\n",
      "Evaluating Epoch 6  66.9% | batch:       115 of       172\t|\tloss: 0.922063\n",
      "Evaluating Epoch 6  67.4% | batch:       116 of       172\t|\tloss: 2.64041\n",
      "Evaluating Epoch 6  68.0% | batch:       117 of       172\t|\tloss: 2.05345\n",
      "Evaluating Epoch 6  68.6% | batch:       118 of       172\t|\tloss: 1.27172\n",
      "Evaluating Epoch 6  69.2% | batch:       119 of       172\t|\tloss: 2.42164\n",
      "Evaluating Epoch 6  69.8% | batch:       120 of       172\t|\tloss: 1.28285\n",
      "Evaluating Epoch 6  70.3% | batch:       121 of       172\t|\tloss: 4.98228\n",
      "Evaluating Epoch 6  70.9% | batch:       122 of       172\t|\tloss: 5.31304\n",
      "Evaluating Epoch 6  71.5% | batch:       123 of       172\t|\tloss: 11.1795\n",
      "Evaluating Epoch 6  72.1% | batch:       124 of       172\t|\tloss: 99.2757\n",
      "Evaluating Epoch 6  72.7% | batch:       125 of       172\t|\tloss: 5.01584\n",
      "Evaluating Epoch 6  73.3% | batch:       126 of       172\t|\tloss: 2.09955\n",
      "Evaluating Epoch 6  73.8% | batch:       127 of       172\t|\tloss: 1.33441\n",
      "Evaluating Epoch 6  74.4% | batch:       128 of       172\t|\tloss: 4.52579\n",
      "Evaluating Epoch 6  75.0% | batch:       129 of       172\t|\tloss: 3.11246\n",
      "Evaluating Epoch 6  75.6% | batch:       130 of       172\t|\tloss: 1.02196\n",
      "Evaluating Epoch 6  76.2% | batch:       131 of       172\t|\tloss: 3.4006\n",
      "Evaluating Epoch 6  76.7% | batch:       132 of       172\t|\tloss: 1.44316\n",
      "Evaluating Epoch 6  77.3% | batch:       133 of       172\t|\tloss: 1.16867\n",
      "Evaluating Epoch 6  77.9% | batch:       134 of       172\t|\tloss: 2.0583\n",
      "Evaluating Epoch 6  78.5% | batch:       135 of       172\t|\tloss: 0.723413\n",
      "Evaluating Epoch 6  79.1% | batch:       136 of       172\t|\tloss: 1.59395\n",
      "Evaluating Epoch 6  79.7% | batch:       137 of       172\t|\tloss: 0.691232\n",
      "Evaluating Epoch 6  80.2% | batch:       138 of       172\t|\tloss: 2.04643\n",
      "Evaluating Epoch 6  80.8% | batch:       139 of       172\t|\tloss: 1.84912\n",
      "Evaluating Epoch 6  81.4% | batch:       140 of       172\t|\tloss: 1.20831\n",
      "Evaluating Epoch 6  82.0% | batch:       141 of       172\t|\tloss: 0.948478\n",
      "Evaluating Epoch 6  82.6% | batch:       142 of       172\t|\tloss: 0.935656\n",
      "Evaluating Epoch 6  83.1% | batch:       143 of       172\t|\tloss: 1.09075\n",
      "Evaluating Epoch 6  83.7% | batch:       144 of       172\t|\tloss: 1.50128\n",
      "Evaluating Epoch 6  84.3% | batch:       145 of       172\t|\tloss: 0.888889\n",
      "Evaluating Epoch 6  84.9% | batch:       146 of       172\t|\tloss: 1.49932\n",
      "Evaluating Epoch 6  85.5% | batch:       147 of       172\t|\tloss: 1.0147\n",
      "Evaluating Epoch 6  86.0% | batch:       148 of       172\t|\tloss: 0.840468\n",
      "Evaluating Epoch 6  86.6% | batch:       149 of       172\t|\tloss: 1.01214\n",
      "Evaluating Epoch 6  87.2% | batch:       150 of       172\t|\tloss: 2.5046\n",
      "Evaluating Epoch 6  87.8% | batch:       151 of       172\t|\tloss: 1.63062\n",
      "Evaluating Epoch 6  88.4% | batch:       152 of       172\t|\tloss: 2.18605\n",
      "Evaluating Epoch 6  89.0% | batch:       153 of       172\t|\tloss: 2.21053\n",
      "Evaluating Epoch 6  89.5% | batch:       154 of       172\t|\tloss: 1.68356\n",
      "Evaluating Epoch 6  90.1% | batch:       155 of       172\t|\tloss: 2.66917\n",
      "Evaluating Epoch 6  90.7% | batch:       156 of       172\t|\tloss: 2.02279\n",
      "Evaluating Epoch 6  91.3% | batch:       157 of       172\t|\tloss: 1.7347\n",
      "Evaluating Epoch 6  91.9% | batch:       158 of       172\t|\tloss: 3.13604\n",
      "Evaluating Epoch 6  92.4% | batch:       159 of       172\t|\tloss: 1.60858\n",
      "Evaluating Epoch 6  93.0% | batch:       160 of       172\t|\tloss: 7.98928\n",
      "Evaluating Epoch 6  93.6% | batch:       161 of       172\t|\tloss: 7.3609\n",
      "Evaluating Epoch 6  94.2% | batch:       162 of       172\t|\tloss: 1.55042\n",
      "Evaluating Epoch 6  94.8% | batch:       163 of       172\t|\tloss: 2.09613\n",
      "Evaluating Epoch 6  95.3% | batch:       164 of       172\t|\tloss: 2.17931\n",
      "Evaluating Epoch 6  95.9% | batch:       165 of       172\t|\tloss: 1.53385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:02:37,702 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 4.883335113525391 seconds\n",
      "\n",
      "2023-05-25 22:02:37,703 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.112995249884469 seconds\n",
      "2023-05-25 22:02:37,704 | INFO : Avg batch val. time: 0.023912763080723657 seconds\n",
      "2023-05-25 22:02:37,704 | INFO : Avg sample val. time: 0.00018732045588579812 seconds\n",
      "2023-05-25 22:02:37,705 | INFO : Epoch 6 Validation Summary: epoch: 6.000000 | loss: 2.774956 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 6  96.5% | batch:       166 of       172\t|\tloss: 2.42342\n",
      "Evaluating Epoch 6  97.1% | batch:       167 of       172\t|\tloss: 1.40872\n",
      "Evaluating Epoch 6  97.7% | batch:       168 of       172\t|\tloss: 1.54527\n",
      "Evaluating Epoch 6  98.3% | batch:       169 of       172\t|\tloss: 2.33144\n",
      "Evaluating Epoch 6  98.8% | batch:       170 of       172\t|\tloss: 1.42017\n",
      "Evaluating Epoch 6  99.4% | batch:       171 of       172\t|\tloss: 1.56858\n",
      "\n",
      "Training Epoch 7   0.0% | batch:         0 of       686\t|\tloss: 18.1729\n",
      "Training Epoch 7   0.1% | batch:         1 of       686\t|\tloss: 15.7165\n",
      "Training Epoch 7   0.3% | batch:         2 of       686\t|\tloss: 12.8781\n",
      "Training Epoch 7   0.4% | batch:         3 of       686\t|\tloss: 16.6932\n",
      "Training Epoch 7   0.6% | batch:         4 of       686\t|\tloss: 15.1265\n",
      "Training Epoch 7   0.7% | batch:         5 of       686\t|\tloss: 16.5468\n",
      "Training Epoch 7   0.9% | batch:         6 of       686\t|\tloss: 13.8066\n",
      "Training Epoch 7   1.0% | batch:         7 of       686\t|\tloss: 14.5792\n",
      "Training Epoch 7   1.2% | batch:         8 of       686\t|\tloss: 13.3412\n",
      "Training Epoch 7   1.3% | batch:         9 of       686\t|\tloss: 14.5612\n",
      "Training Epoch 7   1.5% | batch:        10 of       686\t|\tloss: 15.3253\n",
      "Training Epoch 7   1.6% | batch:        11 of       686\t|\tloss: 21.3248\n",
      "Training Epoch 7   1.7% | batch:        12 of       686\t|\tloss: 14.2003\n",
      "Training Epoch 7   1.9% | batch:        13 of       686\t|\tloss: 14.5731\n",
      "Training Epoch 7   2.0% | batch:        14 of       686\t|\tloss: 15.4587\n",
      "Training Epoch 7   2.2% | batch:        15 of       686\t|\tloss: 15.1055\n",
      "Training Epoch 7   2.3% | batch:        16 of       686\t|\tloss: 12.7168\n",
      "Training Epoch 7   2.5% | batch:        17 of       686\t|\tloss: 22.5437\n",
      "Training Epoch 7   2.6% | batch:        18 of       686\t|\tloss: 15.9091\n",
      "Training Epoch 7   2.8% | batch:        19 of       686\t|\tloss: 16.7976\n",
      "Training Epoch 7   2.9% | batch:        20 of       686\t|\tloss: 16.201\n",
      "Training Epoch 7   3.1% | batch:        21 of       686\t|\tloss: 15.1418\n",
      "Training Epoch 7   3.2% | batch:        22 of       686\t|\tloss: 16.4429\n",
      "Training Epoch 7   3.4% | batch:        23 of       686\t|\tloss: 16.1312\n",
      "Training Epoch 7   3.5% | batch:        24 of       686\t|\tloss: 18.0924\n",
      "Training Epoch 7   3.6% | batch:        25 of       686\t|\tloss: 16.7857\n",
      "Training Epoch 7   3.8% | batch:        26 of       686\t|\tloss: 15.3341\n",
      "Training Epoch 7   3.9% | batch:        27 of       686\t|\tloss: 19.2811\n",
      "Training Epoch 7   4.1% | batch:        28 of       686\t|\tloss: 17.5974\n",
      "Training Epoch 7   4.2% | batch:        29 of       686\t|\tloss: 19.397\n",
      "Training Epoch 7   4.4% | batch:        30 of       686\t|\tloss: 16.1924\n",
      "Training Epoch 7   4.5% | batch:        31 of       686\t|\tloss: 14.3668\n",
      "Training Epoch 7   4.7% | batch:        32 of       686\t|\tloss: 16.1105\n",
      "Training Epoch 7   4.8% | batch:        33 of       686\t|\tloss: 17.7838\n",
      "Training Epoch 7   5.0% | batch:        34 of       686\t|\tloss: 14.2923\n",
      "Training Epoch 7   5.1% | batch:        35 of       686\t|\tloss: 18.1105\n",
      "Training Epoch 7   5.2% | batch:        36 of       686\t|\tloss: 15.6025\n",
      "Training Epoch 7   5.4% | batch:        37 of       686\t|\tloss: 16.3062\n",
      "Training Epoch 7   5.5% | batch:        38 of       686\t|\tloss: 13.0422\n",
      "Training Epoch 7   5.7% | batch:        39 of       686\t|\tloss: 16.9067\n",
      "Training Epoch 7   5.8% | batch:        40 of       686\t|\tloss: 15.2061\n",
      "Training Epoch 7   6.0% | batch:        41 of       686\t|\tloss: 14.0729\n",
      "Training Epoch 7   6.1% | batch:        42 of       686\t|\tloss: 16.3238\n",
      "Training Epoch 7   6.3% | batch:        43 of       686\t|\tloss: 13.551\n",
      "Training Epoch 7   6.4% | batch:        44 of       686\t|\tloss: 16.1525\n",
      "Training Epoch 7   6.6% | batch:        45 of       686\t|\tloss: 17.636\n",
      "Training Epoch 7   6.7% | batch:        46 of       686\t|\tloss: 12.4044\n",
      "Training Epoch 7   6.9% | batch:        47 of       686\t|\tloss: 15.9842\n",
      "Training Epoch 7   7.0% | batch:        48 of       686\t|\tloss: 16.7832\n",
      "Training Epoch 7   7.1% | batch:        49 of       686\t|\tloss: 14.0892\n",
      "Training Epoch 7   7.3% | batch:        50 of       686\t|\tloss: 18.1341\n",
      "Training Epoch 7   7.4% | batch:        51 of       686\t|\tloss: 18.8863\n",
      "Training Epoch 7   7.6% | batch:        52 of       686\t|\tloss: 16.896\n",
      "Training Epoch 7   7.7% | batch:        53 of       686\t|\tloss: 13.7981\n",
      "Training Epoch 7   7.9% | batch:        54 of       686\t|\tloss: 18.0471\n",
      "Training Epoch 7   8.0% | batch:        55 of       686\t|\tloss: 16.3721\n",
      "Training Epoch 7   8.2% | batch:        56 of       686\t|\tloss: 20.5275\n",
      "Training Epoch 7   8.3% | batch:        57 of       686\t|\tloss: 16.7984\n",
      "Training Epoch 7   8.5% | batch:        58 of       686\t|\tloss: 15.7903\n",
      "Training Epoch 7   8.6% | batch:        59 of       686\t|\tloss: 16.1076\n",
      "Training Epoch 7   8.7% | batch:        60 of       686\t|\tloss: 12.5622\n",
      "Training Epoch 7   8.9% | batch:        61 of       686\t|\tloss: 15.0426\n",
      "Training Epoch 7   9.0% | batch:        62 of       686\t|\tloss: 18.2774\n",
      "Training Epoch 7   9.2% | batch:        63 of       686\t|\tloss: 19.4527\n",
      "Training Epoch 7   9.3% | batch:        64 of       686\t|\tloss: 13.1491\n",
      "Training Epoch 7   9.5% | batch:        65 of       686\t|\tloss: 17.7252\n",
      "Training Epoch 7   9.6% | batch:        66 of       686\t|\tloss: 18.4919\n",
      "Training Epoch 7   9.8% | batch:        67 of       686\t|\tloss: 14.408\n",
      "Training Epoch 7   9.9% | batch:        68 of       686\t|\tloss: 18.8752\n",
      "Training Epoch 7  10.1% | batch:        69 of       686\t|\tloss: 17.2704\n",
      "Training Epoch 7  10.2% | batch:        70 of       686\t|\tloss: 17.6341\n",
      "Training Epoch 7  10.3% | batch:        71 of       686\t|\tloss: 18.7593\n",
      "Training Epoch 7  10.5% | batch:        72 of       686\t|\tloss: 12.3671\n",
      "Training Epoch 7  10.6% | batch:        73 of       686\t|\tloss: 17.6958\n",
      "Training Epoch 7  10.8% | batch:        74 of       686\t|\tloss: 13.0476\n",
      "Training Epoch 7  10.9% | batch:        75 of       686\t|\tloss: 15.5258\n",
      "Training Epoch 7  11.1% | batch:        76 of       686\t|\tloss: 18.5686\n",
      "Training Epoch 7  11.2% | batch:        77 of       686\t|\tloss: 14.0881\n",
      "Training Epoch 7  11.4% | batch:        78 of       686\t|\tloss: 14.391\n",
      "Training Epoch 7  11.5% | batch:        79 of       686\t|\tloss: 16.8407\n",
      "Training Epoch 7  11.7% | batch:        80 of       686\t|\tloss: 14.4618\n",
      "Training Epoch 7  11.8% | batch:        81 of       686\t|\tloss: 13.4289\n",
      "Training Epoch 7  12.0% | batch:        82 of       686\t|\tloss: 13.6684\n",
      "Training Epoch 7  12.1% | batch:        83 of       686\t|\tloss: 15.8147\n",
      "Training Epoch 7  12.2% | batch:        84 of       686\t|\tloss: 14.5208\n",
      "Training Epoch 7  12.4% | batch:        85 of       686\t|\tloss: 16.0102\n",
      "Training Epoch 7  12.5% | batch:        86 of       686\t|\tloss: 20.0315\n",
      "Training Epoch 7  12.7% | batch:        87 of       686\t|\tloss: 16.5\n",
      "Training Epoch 7  12.8% | batch:        88 of       686\t|\tloss: 12.7917\n",
      "Training Epoch 7  13.0% | batch:        89 of       686\t|\tloss: 12.0994\n",
      "Training Epoch 7  13.1% | batch:        90 of       686\t|\tloss: 14.9349\n",
      "Training Epoch 7  13.3% | batch:        91 of       686\t|\tloss: 17.5706\n",
      "Training Epoch 7  13.4% | batch:        92 of       686\t|\tloss: 17.2209\n",
      "Training Epoch 7  13.6% | batch:        93 of       686\t|\tloss: 19.8432\n",
      "Training Epoch 7  13.7% | batch:        94 of       686\t|\tloss: 15.2812\n",
      "Training Epoch 7  13.8% | batch:        95 of       686\t|\tloss: 21.309\n",
      "Training Epoch 7  14.0% | batch:        96 of       686\t|\tloss: 19.9058\n",
      "Training Epoch 7  14.1% | batch:        97 of       686\t|\tloss: 18.0123\n",
      "Training Epoch 7  14.3% | batch:        98 of       686\t|\tloss: 16.5648\n",
      "Training Epoch 7  14.4% | batch:        99 of       686\t|\tloss: 11.9455\n",
      "Training Epoch 7  14.6% | batch:       100 of       686\t|\tloss: 15.2012\n",
      "Training Epoch 7  14.7% | batch:       101 of       686\t|\tloss: 17.8159\n",
      "Training Epoch 7  14.9% | batch:       102 of       686\t|\tloss: 14.7389\n",
      "Training Epoch 7  15.0% | batch:       103 of       686\t|\tloss: 14.4176\n",
      "Training Epoch 7  15.2% | batch:       104 of       686\t|\tloss: 17.1411\n",
      "Training Epoch 7  15.3% | batch:       105 of       686\t|\tloss: 14.3356\n",
      "Training Epoch 7  15.5% | batch:       106 of       686\t|\tloss: 18.5212\n",
      "Training Epoch 7  15.6% | batch:       107 of       686\t|\tloss: 20.1726\n",
      "Training Epoch 7  15.7% | batch:       108 of       686\t|\tloss: 15.7949\n",
      "Training Epoch 7  15.9% | batch:       109 of       686\t|\tloss: 15.5375\n",
      "Training Epoch 7  16.0% | batch:       110 of       686\t|\tloss: 19.5329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  16.2% | batch:       111 of       686\t|\tloss: 16.2274\n",
      "Training Epoch 7  16.3% | batch:       112 of       686\t|\tloss: 19.6524\n",
      "Training Epoch 7  16.5% | batch:       113 of       686\t|\tloss: 14.3135\n",
      "Training Epoch 7  16.6% | batch:       114 of       686\t|\tloss: 20.8797\n",
      "Training Epoch 7  16.8% | batch:       115 of       686\t|\tloss: 18.0925\n",
      "Training Epoch 7  16.9% | batch:       116 of       686\t|\tloss: 12.0104\n",
      "Training Epoch 7  17.1% | batch:       117 of       686\t|\tloss: 15.6259\n",
      "Training Epoch 7  17.2% | batch:       118 of       686\t|\tloss: 13.7926\n",
      "Training Epoch 7  17.3% | batch:       119 of       686\t|\tloss: 12.1483\n",
      "Training Epoch 7  17.5% | batch:       120 of       686\t|\tloss: 17.3772\n",
      "Training Epoch 7  17.6% | batch:       121 of       686\t|\tloss: 15.1922\n",
      "Training Epoch 7  17.8% | batch:       122 of       686\t|\tloss: 13.9397\n",
      "Training Epoch 7  17.9% | batch:       123 of       686\t|\tloss: 15.2727\n",
      "Training Epoch 7  18.1% | batch:       124 of       686\t|\tloss: 13.1275\n",
      "Training Epoch 7  18.2% | batch:       125 of       686\t|\tloss: 13.5827\n",
      "Training Epoch 7  18.4% | batch:       126 of       686\t|\tloss: 14.264\n",
      "Training Epoch 7  18.5% | batch:       127 of       686\t|\tloss: 13.3876\n",
      "Training Epoch 7  18.7% | batch:       128 of       686\t|\tloss: 17.6611\n",
      "Training Epoch 7  18.8% | batch:       129 of       686\t|\tloss: 13.9792\n",
      "Training Epoch 7  19.0% | batch:       130 of       686\t|\tloss: 13.8349\n",
      "Training Epoch 7  19.1% | batch:       131 of       686\t|\tloss: 17.1424\n",
      "Training Epoch 7  19.2% | batch:       132 of       686\t|\tloss: 14.8442\n",
      "Training Epoch 7  19.4% | batch:       133 of       686\t|\tloss: 17.1429\n",
      "Training Epoch 7  19.5% | batch:       134 of       686\t|\tloss: 12.7786\n",
      "Training Epoch 7  19.7% | batch:       135 of       686\t|\tloss: 17.4839\n",
      "Training Epoch 7  19.8% | batch:       136 of       686\t|\tloss: 15.9416\n",
      "Training Epoch 7  20.0% | batch:       137 of       686\t|\tloss: 17.4402\n",
      "Training Epoch 7  20.1% | batch:       138 of       686\t|\tloss: 23.9299\n",
      "Training Epoch 7  20.3% | batch:       139 of       686\t|\tloss: 15.7182\n",
      "Training Epoch 7  20.4% | batch:       140 of       686\t|\tloss: 13.2569\n",
      "Training Epoch 7  20.6% | batch:       141 of       686\t|\tloss: 14.8108\n",
      "Training Epoch 7  20.7% | batch:       142 of       686\t|\tloss: 17.1942\n",
      "Training Epoch 7  20.8% | batch:       143 of       686\t|\tloss: 13.7534\n",
      "Training Epoch 7  21.0% | batch:       144 of       686\t|\tloss: 16.0724\n",
      "Training Epoch 7  21.1% | batch:       145 of       686\t|\tloss: 15.9005\n",
      "Training Epoch 7  21.3% | batch:       146 of       686\t|\tloss: 24.0342\n",
      "Training Epoch 7  21.4% | batch:       147 of       686\t|\tloss: 16.0362\n",
      "Training Epoch 7  21.6% | batch:       148 of       686\t|\tloss: 13.9557\n",
      "Training Epoch 7  21.7% | batch:       149 of       686\t|\tloss: 13.8978\n",
      "Training Epoch 7  21.9% | batch:       150 of       686\t|\tloss: 15.809\n",
      "Training Epoch 7  22.0% | batch:       151 of       686\t|\tloss: 12.3408\n",
      "Training Epoch 7  22.2% | batch:       152 of       686\t|\tloss: 14.6114\n",
      "Training Epoch 7  22.3% | batch:       153 of       686\t|\tloss: 15.2748\n",
      "Training Epoch 7  22.4% | batch:       154 of       686\t|\tloss: 24.6915\n",
      "Training Epoch 7  22.6% | batch:       155 of       686\t|\tloss: 13.286\n",
      "Training Epoch 7  22.7% | batch:       156 of       686\t|\tloss: 19.2879\n",
      "Training Epoch 7  22.9% | batch:       157 of       686\t|\tloss: 15.4097\n",
      "Training Epoch 7  23.0% | batch:       158 of       686\t|\tloss: 15.8093\n",
      "Training Epoch 7  23.2% | batch:       159 of       686\t|\tloss: 13.4719\n",
      "Training Epoch 7  23.3% | batch:       160 of       686\t|\tloss: 21.1277\n",
      "Training Epoch 7  23.5% | batch:       161 of       686\t|\tloss: 12.9765\n",
      "Training Epoch 7  23.6% | batch:       162 of       686\t|\tloss: 11.4009\n",
      "Training Epoch 7  23.8% | batch:       163 of       686\t|\tloss: 12.4736\n",
      "Training Epoch 7  23.9% | batch:       164 of       686\t|\tloss: 17.7785\n",
      "Training Epoch 7  24.1% | batch:       165 of       686\t|\tloss: 12.366\n",
      "Training Epoch 7  24.2% | batch:       166 of       686\t|\tloss: 15.2268\n",
      "Training Epoch 7  24.3% | batch:       167 of       686\t|\tloss: 14.8879\n",
      "Training Epoch 7  24.5% | batch:       168 of       686\t|\tloss: 15.9422\n",
      "Training Epoch 7  24.6% | batch:       169 of       686\t|\tloss: 12.9968\n",
      "Training Epoch 7  24.8% | batch:       170 of       686\t|\tloss: 15.0912\n",
      "Training Epoch 7  24.9% | batch:       171 of       686\t|\tloss: 13.803\n",
      "Training Epoch 7  25.1% | batch:       172 of       686\t|\tloss: 15.0588\n",
      "Training Epoch 7  25.2% | batch:       173 of       686\t|\tloss: 19.7003\n",
      "Training Epoch 7  25.4% | batch:       174 of       686\t|\tloss: 12.4154\n",
      "Training Epoch 7  25.5% | batch:       175 of       686\t|\tloss: 16.0761\n",
      "Training Epoch 7  25.7% | batch:       176 of       686\t|\tloss: 14.5236\n",
      "Training Epoch 7  25.8% | batch:       177 of       686\t|\tloss: 15.56\n",
      "Training Epoch 7  25.9% | batch:       178 of       686\t|\tloss: 15.0035\n",
      "Training Epoch 7  26.1% | batch:       179 of       686\t|\tloss: 14.3236\n",
      "Training Epoch 7  26.2% | batch:       180 of       686\t|\tloss: 16.2614\n",
      "Training Epoch 7  26.4% | batch:       181 of       686\t|\tloss: 13.0283\n",
      "Training Epoch 7  26.5% | batch:       182 of       686\t|\tloss: 16.1141\n",
      "Training Epoch 7  26.7% | batch:       183 of       686\t|\tloss: 14.6797\n",
      "Training Epoch 7  26.8% | batch:       184 of       686\t|\tloss: 18.3818\n",
      "Training Epoch 7  27.0% | batch:       185 of       686\t|\tloss: 15.8891\n",
      "Training Epoch 7  27.1% | batch:       186 of       686\t|\tloss: 14.0575\n",
      "Training Epoch 7  27.3% | batch:       187 of       686\t|\tloss: 10.8487\n",
      "Training Epoch 7  27.4% | batch:       188 of       686\t|\tloss: 16.6873\n",
      "Training Epoch 7  27.6% | batch:       189 of       686\t|\tloss: 10.3171\n",
      "Training Epoch 7  27.7% | batch:       190 of       686\t|\tloss: 13.3214\n",
      "Training Epoch 7  27.8% | batch:       191 of       686\t|\tloss: 15.8954\n",
      "Training Epoch 7  28.0% | batch:       192 of       686\t|\tloss: 14.9802\n",
      "Training Epoch 7  28.1% | batch:       193 of       686\t|\tloss: 15.0799\n",
      "Training Epoch 7  28.3% | batch:       194 of       686\t|\tloss: 21.3304\n",
      "Training Epoch 7  28.4% | batch:       195 of       686\t|\tloss: 14.3076\n",
      "Training Epoch 7  28.6% | batch:       196 of       686\t|\tloss: 14.3389\n",
      "Training Epoch 7  28.7% | batch:       197 of       686\t|\tloss: 17.2118\n",
      "Training Epoch 7  28.9% | batch:       198 of       686\t|\tloss: 12.0422\n",
      "Training Epoch 7  29.0% | batch:       199 of       686\t|\tloss: 13.9529\n",
      "Training Epoch 7  29.2% | batch:       200 of       686\t|\tloss: 15.8322\n",
      "Training Epoch 7  29.3% | batch:       201 of       686\t|\tloss: 17.1382\n",
      "Training Epoch 7  29.4% | batch:       202 of       686\t|\tloss: 24.3991\n",
      "Training Epoch 7  29.6% | batch:       203 of       686\t|\tloss: 11.9415\n",
      "Training Epoch 7  29.7% | batch:       204 of       686\t|\tloss: 20.2595\n",
      "Training Epoch 7  29.9% | batch:       205 of       686\t|\tloss: 14.073\n",
      "Training Epoch 7  30.0% | batch:       206 of       686\t|\tloss: 14.5522\n",
      "Training Epoch 7  30.2% | batch:       207 of       686\t|\tloss: 16.529\n",
      "Training Epoch 7  30.3% | batch:       208 of       686\t|\tloss: 16.7555\n",
      "Training Epoch 7  30.5% | batch:       209 of       686\t|\tloss: 17.5746\n",
      "Training Epoch 7  30.6% | batch:       210 of       686\t|\tloss: 16.4025\n",
      "Training Epoch 7  30.8% | batch:       211 of       686\t|\tloss: 14.0088\n",
      "Training Epoch 7  30.9% | batch:       212 of       686\t|\tloss: 13.8795\n",
      "Training Epoch 7  31.0% | batch:       213 of       686\t|\tloss: 16.6666\n",
      "Training Epoch 7  31.2% | batch:       214 of       686\t|\tloss: 15.1549\n",
      "Training Epoch 7  31.3% | batch:       215 of       686\t|\tloss: 15.3501\n",
      "Training Epoch 7  31.5% | batch:       216 of       686\t|\tloss: 16.9686\n",
      "Training Epoch 7  31.6% | batch:       217 of       686\t|\tloss: 16.5796\n",
      "Training Epoch 7  31.8% | batch:       218 of       686\t|\tloss: 13.6793\n",
      "Training Epoch 7  31.9% | batch:       219 of       686\t|\tloss: 15.0167\n",
      "Training Epoch 7  32.1% | batch:       220 of       686\t|\tloss: 13.5423\n",
      "Training Epoch 7  32.2% | batch:       221 of       686\t|\tloss: 11.2493\n",
      "Training Epoch 7  32.4% | batch:       222 of       686\t|\tloss: 14.2506\n",
      "Training Epoch 7  32.5% | batch:       223 of       686\t|\tloss: 13.5655\n",
      "Training Epoch 7  32.7% | batch:       224 of       686\t|\tloss: 12.2192\n",
      "Training Epoch 7  32.8% | batch:       225 of       686\t|\tloss: 21.6254\n",
      "Training Epoch 7  32.9% | batch:       226 of       686\t|\tloss: 14.816\n",
      "Training Epoch 7  33.1% | batch:       227 of       686\t|\tloss: 17.8901\n",
      "Training Epoch 7  33.2% | batch:       228 of       686\t|\tloss: 14.803\n",
      "Training Epoch 7  33.4% | batch:       229 of       686\t|\tloss: 15.0027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  33.5% | batch:       230 of       686\t|\tloss: 13.6855\n",
      "Training Epoch 7  33.7% | batch:       231 of       686\t|\tloss: 15.9135\n",
      "Training Epoch 7  33.8% | batch:       232 of       686\t|\tloss: 18.2256\n",
      "Training Epoch 7  34.0% | batch:       233 of       686\t|\tloss: 14.5456\n",
      "Training Epoch 7  34.1% | batch:       234 of       686\t|\tloss: 14.179\n",
      "Training Epoch 7  34.3% | batch:       235 of       686\t|\tloss: 12.3137\n",
      "Training Epoch 7  34.4% | batch:       236 of       686\t|\tloss: 15.7881\n",
      "Training Epoch 7  34.5% | batch:       237 of       686\t|\tloss: 16.6804\n",
      "Training Epoch 7  34.7% | batch:       238 of       686\t|\tloss: 12.4958\n",
      "Training Epoch 7  34.8% | batch:       239 of       686\t|\tloss: 11.9999\n",
      "Training Epoch 7  35.0% | batch:       240 of       686\t|\tloss: 13.4421\n",
      "Training Epoch 7  35.1% | batch:       241 of       686\t|\tloss: 18.4938\n",
      "Training Epoch 7  35.3% | batch:       242 of       686\t|\tloss: 15.6673\n",
      "Training Epoch 7  35.4% | batch:       243 of       686\t|\tloss: 22.4668\n",
      "Training Epoch 7  35.6% | batch:       244 of       686\t|\tloss: 14.5642\n",
      "Training Epoch 7  35.7% | batch:       245 of       686\t|\tloss: 18.0948\n",
      "Training Epoch 7  35.9% | batch:       246 of       686\t|\tloss: 18.4961\n",
      "Training Epoch 7  36.0% | batch:       247 of       686\t|\tloss: 16.5658\n",
      "Training Epoch 7  36.2% | batch:       248 of       686\t|\tloss: 12.459\n",
      "Training Epoch 7  36.3% | batch:       249 of       686\t|\tloss: 14.8166\n",
      "Training Epoch 7  36.4% | batch:       250 of       686\t|\tloss: 10.0246\n",
      "Training Epoch 7  36.6% | batch:       251 of       686\t|\tloss: 16.0688\n",
      "Training Epoch 7  36.7% | batch:       252 of       686\t|\tloss: 21.1775\n",
      "Training Epoch 7  36.9% | batch:       253 of       686\t|\tloss: 16.4935\n",
      "Training Epoch 7  37.0% | batch:       254 of       686\t|\tloss: 12.0273\n",
      "Training Epoch 7  37.2% | batch:       255 of       686\t|\tloss: 14.5169\n",
      "Training Epoch 7  37.3% | batch:       256 of       686\t|\tloss: 16.7586\n",
      "Training Epoch 7  37.5% | batch:       257 of       686\t|\tloss: 16.1007\n",
      "Training Epoch 7  37.6% | batch:       258 of       686\t|\tloss: 17.7408\n",
      "Training Epoch 7  37.8% | batch:       259 of       686\t|\tloss: 11.9596\n",
      "Training Epoch 7  37.9% | batch:       260 of       686\t|\tloss: 13.6572\n",
      "Training Epoch 7  38.0% | batch:       261 of       686\t|\tloss: 18.4612\n",
      "Training Epoch 7  38.2% | batch:       262 of       686\t|\tloss: 13.2131\n",
      "Training Epoch 7  38.3% | batch:       263 of       686\t|\tloss: 19.554\n",
      "Training Epoch 7  38.5% | batch:       264 of       686\t|\tloss: 15.3801\n",
      "Training Epoch 7  38.6% | batch:       265 of       686\t|\tloss: 23.3313\n",
      "Training Epoch 7  38.8% | batch:       266 of       686\t|\tloss: 11.0764\n",
      "Training Epoch 7  38.9% | batch:       267 of       686\t|\tloss: 13.0872\n",
      "Training Epoch 7  39.1% | batch:       268 of       686\t|\tloss: 14.5622\n",
      "Training Epoch 7  39.2% | batch:       269 of       686\t|\tloss: 10.9313\n",
      "Training Epoch 7  39.4% | batch:       270 of       686\t|\tloss: 16.4648\n",
      "Training Epoch 7  39.5% | batch:       271 of       686\t|\tloss: 14.5885\n",
      "Training Epoch 7  39.7% | batch:       272 of       686\t|\tloss: 16.0486\n",
      "Training Epoch 7  39.8% | batch:       273 of       686\t|\tloss: 12.7894\n",
      "Training Epoch 7  39.9% | batch:       274 of       686\t|\tloss: 14.7444\n",
      "Training Epoch 7  40.1% | batch:       275 of       686\t|\tloss: 14.9912\n",
      "Training Epoch 7  40.2% | batch:       276 of       686\t|\tloss: 14.6282\n",
      "Training Epoch 7  40.4% | batch:       277 of       686\t|\tloss: 10.9841\n",
      "Training Epoch 7  40.5% | batch:       278 of       686\t|\tloss: 13.1913\n",
      "Training Epoch 7  40.7% | batch:       279 of       686\t|\tloss: 14.3329\n",
      "Training Epoch 7  40.8% | batch:       280 of       686\t|\tloss: 13.3254\n",
      "Training Epoch 7  41.0% | batch:       281 of       686\t|\tloss: 16.0894\n",
      "Training Epoch 7  41.1% | batch:       282 of       686\t|\tloss: 12.4832\n",
      "Training Epoch 7  41.3% | batch:       283 of       686\t|\tloss: 13.8895\n",
      "Training Epoch 7  41.4% | batch:       284 of       686\t|\tloss: 13.9613\n",
      "Training Epoch 7  41.5% | batch:       285 of       686\t|\tloss: 15.0452\n",
      "Training Epoch 7  41.7% | batch:       286 of       686\t|\tloss: 24.6281\n",
      "Training Epoch 7  41.8% | batch:       287 of       686\t|\tloss: 16.7882\n",
      "Training Epoch 7  42.0% | batch:       288 of       686\t|\tloss: 14.2799\n",
      "Training Epoch 7  42.1% | batch:       289 of       686\t|\tloss: 20.0354\n",
      "Training Epoch 7  42.3% | batch:       290 of       686\t|\tloss: 14.2932\n",
      "Training Epoch 7  42.4% | batch:       291 of       686\t|\tloss: 11.8836\n",
      "Training Epoch 7  42.6% | batch:       292 of       686\t|\tloss: 13.4291\n",
      "Training Epoch 7  42.7% | batch:       293 of       686\t|\tloss: 13.7002\n",
      "Training Epoch 7  42.9% | batch:       294 of       686\t|\tloss: 14.0735\n",
      "Training Epoch 7  43.0% | batch:       295 of       686\t|\tloss: 13.9095\n",
      "Training Epoch 7  43.1% | batch:       296 of       686\t|\tloss: 13.936\n",
      "Training Epoch 7  43.3% | batch:       297 of       686\t|\tloss: 13.274\n",
      "Training Epoch 7  43.4% | batch:       298 of       686\t|\tloss: 12.4612\n",
      "Training Epoch 7  43.6% | batch:       299 of       686\t|\tloss: 11.7256\n",
      "Training Epoch 7  43.7% | batch:       300 of       686\t|\tloss: 23.3227\n",
      "Training Epoch 7  43.9% | batch:       301 of       686\t|\tloss: 12.7843\n",
      "Training Epoch 7  44.0% | batch:       302 of       686\t|\tloss: 14.9856\n",
      "Training Epoch 7  44.2% | batch:       303 of       686\t|\tloss: 21.8328\n",
      "Training Epoch 7  44.3% | batch:       304 of       686\t|\tloss: 21.921\n",
      "Training Epoch 7  44.5% | batch:       305 of       686\t|\tloss: 13.5687\n",
      "Training Epoch 7  44.6% | batch:       306 of       686\t|\tloss: 14.7092\n",
      "Training Epoch 7  44.8% | batch:       307 of       686\t|\tloss: 10.8467\n",
      "Training Epoch 7  44.9% | batch:       308 of       686\t|\tloss: 13.4064\n",
      "Training Epoch 7  45.0% | batch:       309 of       686\t|\tloss: 13.0654\n",
      "Training Epoch 7  45.2% | batch:       310 of       686\t|\tloss: 14.3094\n",
      "Training Epoch 7  45.3% | batch:       311 of       686\t|\tloss: 13.1201\n",
      "Training Epoch 7  45.5% | batch:       312 of       686\t|\tloss: 14.5193\n",
      "Training Epoch 7  45.6% | batch:       313 of       686\t|\tloss: 22.6357\n",
      "Training Epoch 7  45.8% | batch:       314 of       686\t|\tloss: 13.1553\n",
      "Training Epoch 7  45.9% | batch:       315 of       686\t|\tloss: 13.2432\n",
      "Training Epoch 7  46.1% | batch:       316 of       686\t|\tloss: 14.5411\n",
      "Training Epoch 7  46.2% | batch:       317 of       686\t|\tloss: 17.0252\n",
      "Training Epoch 7  46.4% | batch:       318 of       686\t|\tloss: 15.4391\n",
      "Training Epoch 7  46.5% | batch:       319 of       686\t|\tloss: 11.3046\n",
      "Training Epoch 7  46.6% | batch:       320 of       686\t|\tloss: 9.7957\n",
      "Training Epoch 7  46.8% | batch:       321 of       686\t|\tloss: 17.6042\n",
      "Training Epoch 7  46.9% | batch:       322 of       686\t|\tloss: 13.639\n",
      "Training Epoch 7  47.1% | batch:       323 of       686\t|\tloss: 12.8373\n",
      "Training Epoch 7  47.2% | batch:       324 of       686\t|\tloss: 18.481\n",
      "Training Epoch 7  47.4% | batch:       325 of       686\t|\tloss: 12.5607\n",
      "Training Epoch 7  47.5% | batch:       326 of       686\t|\tloss: 18.4812\n",
      "Training Epoch 7  47.7% | batch:       327 of       686\t|\tloss: 18.4265\n",
      "Training Epoch 7  47.8% | batch:       328 of       686\t|\tloss: 15.6567\n",
      "Training Epoch 7  48.0% | batch:       329 of       686\t|\tloss: 21.5238\n",
      "Training Epoch 7  48.1% | batch:       330 of       686\t|\tloss: 18.1374\n",
      "Training Epoch 7  48.3% | batch:       331 of       686\t|\tloss: 12.7331\n",
      "Training Epoch 7  48.4% | batch:       332 of       686\t|\tloss: 18.515\n",
      "Training Epoch 7  48.5% | batch:       333 of       686\t|\tloss: 15.0743\n",
      "Training Epoch 7  48.7% | batch:       334 of       686\t|\tloss: 16.0785\n",
      "Training Epoch 7  48.8% | batch:       335 of       686\t|\tloss: 16.9903\n",
      "Training Epoch 7  49.0% | batch:       336 of       686\t|\tloss: 23.9006\n",
      "Training Epoch 7  49.1% | batch:       337 of       686\t|\tloss: 14.5165\n",
      "Training Epoch 7  49.3% | batch:       338 of       686\t|\tloss: 22.1501\n",
      "Training Epoch 7  49.4% | batch:       339 of       686\t|\tloss: 14.7307\n",
      "Training Epoch 7  49.6% | batch:       340 of       686\t|\tloss: 14.7796\n",
      "Training Epoch 7  49.7% | batch:       341 of       686\t|\tloss: 15.4891\n",
      "Training Epoch 7  49.9% | batch:       342 of       686\t|\tloss: 16.5297\n",
      "Training Epoch 7  50.0% | batch:       343 of       686\t|\tloss: 13.5135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  50.1% | batch:       344 of       686\t|\tloss: 16.5603\n",
      "Training Epoch 7  50.3% | batch:       345 of       686\t|\tloss: 11.0262\n",
      "Training Epoch 7  50.4% | batch:       346 of       686\t|\tloss: 14.2421\n",
      "Training Epoch 7  50.6% | batch:       347 of       686\t|\tloss: 12.5103\n",
      "Training Epoch 7  50.7% | batch:       348 of       686\t|\tloss: 14.6391\n",
      "Training Epoch 7  50.9% | batch:       349 of       686\t|\tloss: 15.9025\n",
      "Training Epoch 7  51.0% | batch:       350 of       686\t|\tloss: 15.0575\n",
      "Training Epoch 7  51.2% | batch:       351 of       686\t|\tloss: 13.8655\n",
      "Training Epoch 7  51.3% | batch:       352 of       686\t|\tloss: 14.4444\n",
      "Training Epoch 7  51.5% | batch:       353 of       686\t|\tloss: 12.3539\n",
      "Training Epoch 7  51.6% | batch:       354 of       686\t|\tloss: 13.8624\n",
      "Training Epoch 7  51.7% | batch:       355 of       686\t|\tloss: 12.9671\n",
      "Training Epoch 7  51.9% | batch:       356 of       686\t|\tloss: 13.6728\n",
      "Training Epoch 7  52.0% | batch:       357 of       686\t|\tloss: 14.2203\n",
      "Training Epoch 7  52.2% | batch:       358 of       686\t|\tloss: 17.2303\n",
      "Training Epoch 7  52.3% | batch:       359 of       686\t|\tloss: 17.6425\n",
      "Training Epoch 7  52.5% | batch:       360 of       686\t|\tloss: 14.6499\n",
      "Training Epoch 7  52.6% | batch:       361 of       686\t|\tloss: 13.7125\n",
      "Training Epoch 7  52.8% | batch:       362 of       686\t|\tloss: 13.2466\n",
      "Training Epoch 7  52.9% | batch:       363 of       686\t|\tloss: 17.7081\n",
      "Training Epoch 7  53.1% | batch:       364 of       686\t|\tloss: 13.7517\n",
      "Training Epoch 7  53.2% | batch:       365 of       686\t|\tloss: 14.0369\n",
      "Training Epoch 7  53.4% | batch:       366 of       686\t|\tloss: 14.185\n",
      "Training Epoch 7  53.5% | batch:       367 of       686\t|\tloss: 10.8533\n",
      "Training Epoch 7  53.6% | batch:       368 of       686\t|\tloss: 17.7874\n",
      "Training Epoch 7  53.8% | batch:       369 of       686\t|\tloss: 12.9414\n",
      "Training Epoch 7  53.9% | batch:       370 of       686\t|\tloss: 20.6012\n",
      "Training Epoch 7  54.1% | batch:       371 of       686\t|\tloss: 16.44\n",
      "Training Epoch 7  54.2% | batch:       372 of       686\t|\tloss: 12.4018\n",
      "Training Epoch 7  54.4% | batch:       373 of       686\t|\tloss: 14.4433\n",
      "Training Epoch 7  54.5% | batch:       374 of       686\t|\tloss: 16.6288\n",
      "Training Epoch 7  54.7% | batch:       375 of       686\t|\tloss: 13.0655\n",
      "Training Epoch 7  54.8% | batch:       376 of       686\t|\tloss: 15.3862\n",
      "Training Epoch 7  55.0% | batch:       377 of       686\t|\tloss: 13.7898\n",
      "Training Epoch 7  55.1% | batch:       378 of       686\t|\tloss: 15.1041\n",
      "Training Epoch 7  55.2% | batch:       379 of       686\t|\tloss: 10.1941\n",
      "Training Epoch 7  55.4% | batch:       380 of       686\t|\tloss: 13.7775\n",
      "Training Epoch 7  55.5% | batch:       381 of       686\t|\tloss: 13.5205\n",
      "Training Epoch 7  55.7% | batch:       382 of       686\t|\tloss: 14.2157\n",
      "Training Epoch 7  55.8% | batch:       383 of       686\t|\tloss: 15.1254\n",
      "Training Epoch 7  56.0% | batch:       384 of       686\t|\tloss: 17.7727\n",
      "Training Epoch 7  56.1% | batch:       385 of       686\t|\tloss: 16.5235\n",
      "Training Epoch 7  56.3% | batch:       386 of       686\t|\tloss: 10.6222\n",
      "Training Epoch 7  56.4% | batch:       387 of       686\t|\tloss: 12.3739\n",
      "Training Epoch 7  56.6% | batch:       388 of       686\t|\tloss: 16.737\n",
      "Training Epoch 7  56.7% | batch:       389 of       686\t|\tloss: 12.1555\n",
      "Training Epoch 7  56.9% | batch:       390 of       686\t|\tloss: 12.8446\n",
      "Training Epoch 7  57.0% | batch:       391 of       686\t|\tloss: 13.8874\n",
      "Training Epoch 7  57.1% | batch:       392 of       686\t|\tloss: 16.9229\n",
      "Training Epoch 7  57.3% | batch:       393 of       686\t|\tloss: 15.4325\n",
      "Training Epoch 7  57.4% | batch:       394 of       686\t|\tloss: 23.0572\n",
      "Training Epoch 7  57.6% | batch:       395 of       686\t|\tloss: 14.6454\n",
      "Training Epoch 7  57.7% | batch:       396 of       686\t|\tloss: 15.7798\n",
      "Training Epoch 7  57.9% | batch:       397 of       686\t|\tloss: 12.3472\n",
      "Training Epoch 7  58.0% | batch:       398 of       686\t|\tloss: 13.9755\n",
      "Training Epoch 7  58.2% | batch:       399 of       686\t|\tloss: 17.5033\n",
      "Training Epoch 7  58.3% | batch:       400 of       686\t|\tloss: 17.5347\n",
      "Training Epoch 7  58.5% | batch:       401 of       686\t|\tloss: 14.3002\n",
      "Training Epoch 7  58.6% | batch:       402 of       686\t|\tloss: 13.3423\n",
      "Training Epoch 7  58.7% | batch:       403 of       686\t|\tloss: 16.6948\n",
      "Training Epoch 7  58.9% | batch:       404 of       686\t|\tloss: 17.5532\n",
      "Training Epoch 7  59.0% | batch:       405 of       686\t|\tloss: 15.2773\n",
      "Training Epoch 7  59.2% | batch:       406 of       686\t|\tloss: 12.1046\n",
      "Training Epoch 7  59.3% | batch:       407 of       686\t|\tloss: 16.2714\n",
      "Training Epoch 7  59.5% | batch:       408 of       686\t|\tloss: 11.9183\n",
      "Training Epoch 7  59.6% | batch:       409 of       686\t|\tloss: 10.0245\n",
      "Training Epoch 7  59.8% | batch:       410 of       686\t|\tloss: 14.5503\n",
      "Training Epoch 7  59.9% | batch:       411 of       686\t|\tloss: 14.3731\n",
      "Training Epoch 7  60.1% | batch:       412 of       686\t|\tloss: 16.1511\n",
      "Training Epoch 7  60.2% | batch:       413 of       686\t|\tloss: 18.382\n",
      "Training Epoch 7  60.3% | batch:       414 of       686\t|\tloss: 14.7205\n",
      "Training Epoch 7  60.5% | batch:       415 of       686\t|\tloss: 12.4564\n",
      "Training Epoch 7  60.6% | batch:       416 of       686\t|\tloss: 18.0943\n",
      "Training Epoch 7  60.8% | batch:       417 of       686\t|\tloss: 14.9605\n",
      "Training Epoch 7  60.9% | batch:       418 of       686\t|\tloss: 15.6032\n",
      "Training Epoch 7  61.1% | batch:       419 of       686\t|\tloss: 13.4654\n",
      "Training Epoch 7  61.2% | batch:       420 of       686\t|\tloss: 9.67764\n",
      "Training Epoch 7  61.4% | batch:       421 of       686\t|\tloss: 20.6954\n",
      "Training Epoch 7  61.5% | batch:       422 of       686\t|\tloss: 19.9078\n",
      "Training Epoch 7  61.7% | batch:       423 of       686\t|\tloss: 13.674\n",
      "Training Epoch 7  61.8% | batch:       424 of       686\t|\tloss: 15.0399\n",
      "Training Epoch 7  62.0% | batch:       425 of       686\t|\tloss: 12.4302\n",
      "Training Epoch 7  62.1% | batch:       426 of       686\t|\tloss: 19.0223\n",
      "Training Epoch 7  62.2% | batch:       427 of       686\t|\tloss: 12.8919\n",
      "Training Epoch 7  62.4% | batch:       428 of       686\t|\tloss: 10.2419\n",
      "Training Epoch 7  62.5% | batch:       429 of       686\t|\tloss: 13.0092\n",
      "Training Epoch 7  62.7% | batch:       430 of       686\t|\tloss: 13.8832\n",
      "Training Epoch 7  62.8% | batch:       431 of       686\t|\tloss: 12.5749\n",
      "Training Epoch 7  63.0% | batch:       432 of       686\t|\tloss: 17.378\n",
      "Training Epoch 7  63.1% | batch:       433 of       686\t|\tloss: 15.7846\n",
      "Training Epoch 7  63.3% | batch:       434 of       686\t|\tloss: 12.018\n",
      "Training Epoch 7  63.4% | batch:       435 of       686\t|\tloss: 13.836\n",
      "Training Epoch 7  63.6% | batch:       436 of       686\t|\tloss: 16.0667\n",
      "Training Epoch 7  63.7% | batch:       437 of       686\t|\tloss: 17.4884\n",
      "Training Epoch 7  63.8% | batch:       438 of       686\t|\tloss: 15.5965\n",
      "Training Epoch 7  64.0% | batch:       439 of       686\t|\tloss: 14.1238\n",
      "Training Epoch 7  64.1% | batch:       440 of       686\t|\tloss: 16.0889\n",
      "Training Epoch 7  64.3% | batch:       441 of       686\t|\tloss: 16.3257\n",
      "Training Epoch 7  64.4% | batch:       442 of       686\t|\tloss: 12.9325\n",
      "Training Epoch 7  64.6% | batch:       443 of       686\t|\tloss: 15.2076\n",
      "Training Epoch 7  64.7% | batch:       444 of       686\t|\tloss: 13.7779\n",
      "Training Epoch 7  64.9% | batch:       445 of       686\t|\tloss: 13.5712\n",
      "Training Epoch 7  65.0% | batch:       446 of       686\t|\tloss: 15.6297\n",
      "Training Epoch 7  65.2% | batch:       447 of       686\t|\tloss: 16.6748\n",
      "Training Epoch 7  65.3% | batch:       448 of       686\t|\tloss: 13.072\n",
      "Training Epoch 7  65.5% | batch:       449 of       686\t|\tloss: 12.664\n",
      "Training Epoch 7  65.6% | batch:       450 of       686\t|\tloss: 17.0308\n",
      "Training Epoch 7  65.7% | batch:       451 of       686\t|\tloss: 12.5868\n",
      "Training Epoch 7  65.9% | batch:       452 of       686\t|\tloss: 11.4083\n",
      "Training Epoch 7  66.0% | batch:       453 of       686\t|\tloss: 13.4632\n",
      "Training Epoch 7  66.2% | batch:       454 of       686\t|\tloss: 16.0557\n",
      "Training Epoch 7  66.3% | batch:       455 of       686\t|\tloss: 13.6337\n",
      "Training Epoch 7  66.5% | batch:       456 of       686\t|\tloss: 13.3522\n",
      "Training Epoch 7  66.6% | batch:       457 of       686\t|\tloss: 18.124\n",
      "Training Epoch 7  66.8% | batch:       458 of       686\t|\tloss: 10.0615\n",
      "Training Epoch 7  66.9% | batch:       459 of       686\t|\tloss: 18.1108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  67.1% | batch:       460 of       686\t|\tloss: 14.7087\n",
      "Training Epoch 7  67.2% | batch:       461 of       686\t|\tloss: 15.6459\n",
      "Training Epoch 7  67.3% | batch:       462 of       686\t|\tloss: 14.8071\n",
      "Training Epoch 7  67.5% | batch:       463 of       686\t|\tloss: 12.5623\n",
      "Training Epoch 7  67.6% | batch:       464 of       686\t|\tloss: 15.5998\n",
      "Training Epoch 7  67.8% | batch:       465 of       686\t|\tloss: 17.1307\n",
      "Training Epoch 7  67.9% | batch:       466 of       686\t|\tloss: 11.0856\n",
      "Training Epoch 7  68.1% | batch:       467 of       686\t|\tloss: 9.95065\n",
      "Training Epoch 7  68.2% | batch:       468 of       686\t|\tloss: 12.4357\n",
      "Training Epoch 7  68.4% | batch:       469 of       686\t|\tloss: 14.4537\n",
      "Training Epoch 7  68.5% | batch:       470 of       686\t|\tloss: 14.344\n",
      "Training Epoch 7  68.7% | batch:       471 of       686\t|\tloss: 14.4\n",
      "Training Epoch 7  68.8% | batch:       472 of       686\t|\tloss: 14.2159\n",
      "Training Epoch 7  69.0% | batch:       473 of       686\t|\tloss: 21.6807\n",
      "Training Epoch 7  69.1% | batch:       474 of       686\t|\tloss: 13.9014\n",
      "Training Epoch 7  69.2% | batch:       475 of       686\t|\tloss: 14.2082\n",
      "Training Epoch 7  69.4% | batch:       476 of       686\t|\tloss: 17.9829\n",
      "Training Epoch 7  69.5% | batch:       477 of       686\t|\tloss: 12.936\n",
      "Training Epoch 7  69.7% | batch:       478 of       686\t|\tloss: 13.1168\n",
      "Training Epoch 7  69.8% | batch:       479 of       686\t|\tloss: 14.5405\n",
      "Training Epoch 7  70.0% | batch:       480 of       686\t|\tloss: 13.1826\n",
      "Training Epoch 7  70.1% | batch:       481 of       686\t|\tloss: 16.7752\n",
      "Training Epoch 7  70.3% | batch:       482 of       686\t|\tloss: 12.351\n",
      "Training Epoch 7  70.4% | batch:       483 of       686\t|\tloss: 8.63324\n",
      "Training Epoch 7  70.6% | batch:       484 of       686\t|\tloss: 17.7998\n",
      "Training Epoch 7  70.7% | batch:       485 of       686\t|\tloss: 9.8742\n",
      "Training Epoch 7  70.8% | batch:       486 of       686\t|\tloss: 10.7545\n",
      "Training Epoch 7  71.0% | batch:       487 of       686\t|\tloss: 12.6027\n",
      "Training Epoch 7  71.1% | batch:       488 of       686\t|\tloss: 11.9473\n",
      "Training Epoch 7  71.3% | batch:       489 of       686\t|\tloss: 13.2458\n",
      "Training Epoch 7  71.4% | batch:       490 of       686\t|\tloss: 13.337\n",
      "Training Epoch 7  71.6% | batch:       491 of       686\t|\tloss: 13.0833\n",
      "Training Epoch 7  71.7% | batch:       492 of       686\t|\tloss: 13.2891\n",
      "Training Epoch 7  71.9% | batch:       493 of       686\t|\tloss: 17.975\n",
      "Training Epoch 7  72.0% | batch:       494 of       686\t|\tloss: 16.0404\n",
      "Training Epoch 7  72.2% | batch:       495 of       686\t|\tloss: 11.593\n",
      "Training Epoch 7  72.3% | batch:       496 of       686\t|\tloss: 11.7026\n",
      "Training Epoch 7  72.4% | batch:       497 of       686\t|\tloss: 15.0316\n",
      "Training Epoch 7  72.6% | batch:       498 of       686\t|\tloss: 10.231\n",
      "Training Epoch 7  72.7% | batch:       499 of       686\t|\tloss: 11.5695\n",
      "Training Epoch 7  72.9% | batch:       500 of       686\t|\tloss: 13.9974\n",
      "Training Epoch 7  73.0% | batch:       501 of       686\t|\tloss: 13.4827\n",
      "Training Epoch 7  73.2% | batch:       502 of       686\t|\tloss: 15.7036\n",
      "Training Epoch 7  73.3% | batch:       503 of       686\t|\tloss: 16.2149\n",
      "Training Epoch 7  73.5% | batch:       504 of       686\t|\tloss: 13.9496\n",
      "Training Epoch 7  73.6% | batch:       505 of       686\t|\tloss: 13.793\n",
      "Training Epoch 7  73.8% | batch:       506 of       686\t|\tloss: 11.9064\n",
      "Training Epoch 7  73.9% | batch:       507 of       686\t|\tloss: 14.078\n",
      "Training Epoch 7  74.1% | batch:       508 of       686\t|\tloss: 13.8256\n",
      "Training Epoch 7  74.2% | batch:       509 of       686\t|\tloss: 14.5226\n",
      "Training Epoch 7  74.3% | batch:       510 of       686\t|\tloss: 12.3207\n",
      "Training Epoch 7  74.5% | batch:       511 of       686\t|\tloss: 11.1225\n",
      "Training Epoch 7  74.6% | batch:       512 of       686\t|\tloss: 13.2515\n",
      "Training Epoch 7  74.8% | batch:       513 of       686\t|\tloss: 14.861\n",
      "Training Epoch 7  74.9% | batch:       514 of       686\t|\tloss: 14.6597\n",
      "Training Epoch 7  75.1% | batch:       515 of       686\t|\tloss: 15.5309\n",
      "Training Epoch 7  75.2% | batch:       516 of       686\t|\tloss: 11.5033\n",
      "Training Epoch 7  75.4% | batch:       517 of       686\t|\tloss: 12.119\n",
      "Training Epoch 7  75.5% | batch:       518 of       686\t|\tloss: 17.3602\n",
      "Training Epoch 7  75.7% | batch:       519 of       686\t|\tloss: 14.5948\n",
      "Training Epoch 7  75.8% | batch:       520 of       686\t|\tloss: 13.1338\n",
      "Training Epoch 7  75.9% | batch:       521 of       686\t|\tloss: 13.4369\n",
      "Training Epoch 7  76.1% | batch:       522 of       686\t|\tloss: 12.8541\n",
      "Training Epoch 7  76.2% | batch:       523 of       686\t|\tloss: 16.1014\n",
      "Training Epoch 7  76.4% | batch:       524 of       686\t|\tloss: 9.87903\n",
      "Training Epoch 7  76.5% | batch:       525 of       686\t|\tloss: 11.5861\n",
      "Training Epoch 7  76.7% | batch:       526 of       686\t|\tloss: 15.7678\n",
      "Training Epoch 7  76.8% | batch:       527 of       686\t|\tloss: 14.3849\n",
      "Training Epoch 7  77.0% | batch:       528 of       686\t|\tloss: 14.0622\n",
      "Training Epoch 7  77.1% | batch:       529 of       686\t|\tloss: 15.0956\n",
      "Training Epoch 7  77.3% | batch:       530 of       686\t|\tloss: 14.18\n",
      "Training Epoch 7  77.4% | batch:       531 of       686\t|\tloss: 12.9654\n",
      "Training Epoch 7  77.6% | batch:       532 of       686\t|\tloss: 12.0582\n",
      "Training Epoch 7  77.7% | batch:       533 of       686\t|\tloss: 12.7595\n",
      "Training Epoch 7  77.8% | batch:       534 of       686\t|\tloss: 13.0787\n",
      "Training Epoch 7  78.0% | batch:       535 of       686\t|\tloss: 14.191\n",
      "Training Epoch 7  78.1% | batch:       536 of       686\t|\tloss: 13.2588\n",
      "Training Epoch 7  78.3% | batch:       537 of       686\t|\tloss: 13.0116\n",
      "Training Epoch 7  78.4% | batch:       538 of       686\t|\tloss: 11.6748\n",
      "Training Epoch 7  78.6% | batch:       539 of       686\t|\tloss: 11.5334\n",
      "Training Epoch 7  78.7% | batch:       540 of       686\t|\tloss: 16.0277\n",
      "Training Epoch 7  78.9% | batch:       541 of       686\t|\tloss: 8.37566\n",
      "Training Epoch 7  79.0% | batch:       542 of       686\t|\tloss: 15.7743\n",
      "Training Epoch 7  79.2% | batch:       543 of       686\t|\tloss: 11.0729\n",
      "Training Epoch 7  79.3% | batch:       544 of       686\t|\tloss: 14.7545\n",
      "Training Epoch 7  79.4% | batch:       545 of       686\t|\tloss: 12.9977\n",
      "Training Epoch 7  79.6% | batch:       546 of       686\t|\tloss: 13.65\n",
      "Training Epoch 7  79.7% | batch:       547 of       686\t|\tloss: 12.9371\n",
      "Training Epoch 7  79.9% | batch:       548 of       686\t|\tloss: 13.1648\n",
      "Training Epoch 7  80.0% | batch:       549 of       686\t|\tloss: 12.3753\n",
      "Training Epoch 7  80.2% | batch:       550 of       686\t|\tloss: 14.8926\n",
      "Training Epoch 7  80.3% | batch:       551 of       686\t|\tloss: 12.4319\n",
      "Training Epoch 7  80.5% | batch:       552 of       686\t|\tloss: 15.3964\n",
      "Training Epoch 7  80.6% | batch:       553 of       686\t|\tloss: 15.0579\n",
      "Training Epoch 7  80.8% | batch:       554 of       686\t|\tloss: 14.1286\n",
      "Training Epoch 7  80.9% | batch:       555 of       686\t|\tloss: 12.319\n",
      "Training Epoch 7  81.0% | batch:       556 of       686\t|\tloss: 11.6033\n",
      "Training Epoch 7  81.2% | batch:       557 of       686\t|\tloss: 12.8879\n",
      "Training Epoch 7  81.3% | batch:       558 of       686\t|\tloss: 11.698\n",
      "Training Epoch 7  81.5% | batch:       559 of       686\t|\tloss: 12.6077\n",
      "Training Epoch 7  81.6% | batch:       560 of       686\t|\tloss: 11.987\n",
      "Training Epoch 7  81.8% | batch:       561 of       686\t|\tloss: 12.6811\n",
      "Training Epoch 7  81.9% | batch:       562 of       686\t|\tloss: 10.3808\n",
      "Training Epoch 7  82.1% | batch:       563 of       686\t|\tloss: 11.3338\n",
      "Training Epoch 7  82.2% | batch:       564 of       686\t|\tloss: 11.1333\n",
      "Training Epoch 7  82.4% | batch:       565 of       686\t|\tloss: 18.7537\n",
      "Training Epoch 7  82.5% | batch:       566 of       686\t|\tloss: 13.0021\n",
      "Training Epoch 7  82.7% | batch:       567 of       686\t|\tloss: 10.9402\n",
      "Training Epoch 7  82.8% | batch:       568 of       686\t|\tloss: 14.8644\n",
      "Training Epoch 7  82.9% | batch:       569 of       686\t|\tloss: 14.69\n",
      "Training Epoch 7  83.1% | batch:       570 of       686\t|\tloss: 19.01\n",
      "Training Epoch 7  83.2% | batch:       571 of       686\t|\tloss: 20.7221\n",
      "Training Epoch 7  83.4% | batch:       572 of       686\t|\tloss: 13.914\n",
      "Training Epoch 7  83.5% | batch:       573 of       686\t|\tloss: 11.9499\n",
      "Training Epoch 7  83.7% | batch:       574 of       686\t|\tloss: 11.6206\n",
      "Training Epoch 7  83.8% | batch:       575 of       686\t|\tloss: 11.0466\n",
      "Training Epoch 7  84.0% | batch:       576 of       686\t|\tloss: 13.1926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  84.1% | batch:       577 of       686\t|\tloss: 13.7146\n",
      "Training Epoch 7  84.3% | batch:       578 of       686\t|\tloss: 12.182\n",
      "Training Epoch 7  84.4% | batch:       579 of       686\t|\tloss: 14.5039\n",
      "Training Epoch 7  84.5% | batch:       580 of       686\t|\tloss: 16.2534\n",
      "Training Epoch 7  84.7% | batch:       581 of       686\t|\tloss: 14.4347\n",
      "Training Epoch 7  84.8% | batch:       582 of       686\t|\tloss: 10.8161\n",
      "Training Epoch 7  85.0% | batch:       583 of       686\t|\tloss: 13.3428\n",
      "Training Epoch 7  85.1% | batch:       584 of       686\t|\tloss: 16.6125\n",
      "Training Epoch 7  85.3% | batch:       585 of       686\t|\tloss: 9.99462\n",
      "Training Epoch 7  85.4% | batch:       586 of       686\t|\tloss: 15.1242\n",
      "Training Epoch 7  85.6% | batch:       587 of       686\t|\tloss: 11.4881\n",
      "Training Epoch 7  85.7% | batch:       588 of       686\t|\tloss: 13.3571\n",
      "Training Epoch 7  85.9% | batch:       589 of       686\t|\tloss: 15.2459\n",
      "Training Epoch 7  86.0% | batch:       590 of       686\t|\tloss: 12.643\n",
      "Training Epoch 7  86.2% | batch:       591 of       686\t|\tloss: 14.4497\n",
      "Training Epoch 7  86.3% | batch:       592 of       686\t|\tloss: 19.1014\n",
      "Training Epoch 7  86.4% | batch:       593 of       686\t|\tloss: 11.8208\n",
      "Training Epoch 7  86.6% | batch:       594 of       686\t|\tloss: 11.2855\n",
      "Training Epoch 7  86.7% | batch:       595 of       686\t|\tloss: 15.6045\n",
      "Training Epoch 7  86.9% | batch:       596 of       686\t|\tloss: 10.1528\n",
      "Training Epoch 7  87.0% | batch:       597 of       686\t|\tloss: 14.3046\n",
      "Training Epoch 7  87.2% | batch:       598 of       686\t|\tloss: 13.3145\n",
      "Training Epoch 7  87.3% | batch:       599 of       686\t|\tloss: 13.2176\n",
      "Training Epoch 7  87.5% | batch:       600 of       686\t|\tloss: 12.7856\n",
      "Training Epoch 7  87.6% | batch:       601 of       686\t|\tloss: 13.4951\n",
      "Training Epoch 7  87.8% | batch:       602 of       686\t|\tloss: 12.8397\n",
      "Training Epoch 7  87.9% | batch:       603 of       686\t|\tloss: 20.6615\n",
      "Training Epoch 7  88.0% | batch:       604 of       686\t|\tloss: 13.877\n",
      "Training Epoch 7  88.2% | batch:       605 of       686\t|\tloss: 12.1056\n",
      "Training Epoch 7  88.3% | batch:       606 of       686\t|\tloss: 11.0286\n",
      "Training Epoch 7  88.5% | batch:       607 of       686\t|\tloss: 19.046\n",
      "Training Epoch 7  88.6% | batch:       608 of       686\t|\tloss: 13.9882\n",
      "Training Epoch 7  88.8% | batch:       609 of       686\t|\tloss: 11.6192\n",
      "Training Epoch 7  88.9% | batch:       610 of       686\t|\tloss: 12.524\n",
      "Training Epoch 7  89.1% | batch:       611 of       686\t|\tloss: 12.2205\n",
      "Training Epoch 7  89.2% | batch:       612 of       686\t|\tloss: 14.6707\n",
      "Training Epoch 7  89.4% | batch:       613 of       686\t|\tloss: 13.8293\n",
      "Training Epoch 7  89.5% | batch:       614 of       686\t|\tloss: 13.527\n",
      "Training Epoch 7  89.7% | batch:       615 of       686\t|\tloss: 13.5616\n",
      "Training Epoch 7  89.8% | batch:       616 of       686\t|\tloss: 15.2976\n",
      "Training Epoch 7  89.9% | batch:       617 of       686\t|\tloss: 11.2547\n",
      "Training Epoch 7  90.1% | batch:       618 of       686\t|\tloss: 10.7649\n",
      "Training Epoch 7  90.2% | batch:       619 of       686\t|\tloss: 10.5054\n",
      "Training Epoch 7  90.4% | batch:       620 of       686\t|\tloss: 11.561\n",
      "Training Epoch 7  90.5% | batch:       621 of       686\t|\tloss: 14.0221\n",
      "Training Epoch 7  90.7% | batch:       622 of       686\t|\tloss: 12.3765\n",
      "Training Epoch 7  90.8% | batch:       623 of       686\t|\tloss: 13.5102\n",
      "Training Epoch 7  91.0% | batch:       624 of       686\t|\tloss: 12.2015\n",
      "Training Epoch 7  91.1% | batch:       625 of       686\t|\tloss: 13.9538\n",
      "Training Epoch 7  91.3% | batch:       626 of       686\t|\tloss: 18.1274\n",
      "Training Epoch 7  91.4% | batch:       627 of       686\t|\tloss: 11.3063\n",
      "Training Epoch 7  91.5% | batch:       628 of       686\t|\tloss: 12.4693\n",
      "Training Epoch 7  91.7% | batch:       629 of       686\t|\tloss: 12.5245\n",
      "Training Epoch 7  91.8% | batch:       630 of       686\t|\tloss: 16.6792\n",
      "Training Epoch 7  92.0% | batch:       631 of       686\t|\tloss: 9.91029\n",
      "Training Epoch 7  92.1% | batch:       632 of       686\t|\tloss: 14.8854\n",
      "Training Epoch 7  92.3% | batch:       633 of       686\t|\tloss: 15.331\n",
      "Training Epoch 7  92.4% | batch:       634 of       686\t|\tloss: 14.3706\n",
      "Training Epoch 7  92.6% | batch:       635 of       686\t|\tloss: 14.5668\n",
      "Training Epoch 7  92.7% | batch:       636 of       686\t|\tloss: 10.85\n",
      "Training Epoch 7  92.9% | batch:       637 of       686\t|\tloss: 12.2676\n",
      "Training Epoch 7  93.0% | batch:       638 of       686\t|\tloss: 15.1351\n",
      "Training Epoch 7  93.1% | batch:       639 of       686\t|\tloss: 10.6991\n",
      "Training Epoch 7  93.3% | batch:       640 of       686\t|\tloss: 10.4708\n",
      "Training Epoch 7  93.4% | batch:       641 of       686\t|\tloss: 12.7935\n",
      "Training Epoch 7  93.6% | batch:       642 of       686\t|\tloss: 10.9066\n",
      "Training Epoch 7  93.7% | batch:       643 of       686\t|\tloss: 10.2173\n",
      "Training Epoch 7  93.9% | batch:       644 of       686\t|\tloss: 17.1547\n",
      "Training Epoch 7  94.0% | batch:       645 of       686\t|\tloss: 11.14\n",
      "Training Epoch 7  94.2% | batch:       646 of       686\t|\tloss: 14.1205\n",
      "Training Epoch 7  94.3% | batch:       647 of       686\t|\tloss: 10.4315\n",
      "Training Epoch 7  94.5% | batch:       648 of       686\t|\tloss: 13.6261\n",
      "Training Epoch 7  94.6% | batch:       649 of       686\t|\tloss: 11.9733\n",
      "Training Epoch 7  94.8% | batch:       650 of       686\t|\tloss: 19.7865\n",
      "Training Epoch 7  94.9% | batch:       651 of       686\t|\tloss: 15.7396\n",
      "Training Epoch 7  95.0% | batch:       652 of       686\t|\tloss: 11.0912\n",
      "Training Epoch 7  95.2% | batch:       653 of       686\t|\tloss: 18.4779\n",
      "Training Epoch 7  95.3% | batch:       654 of       686\t|\tloss: 12.1217\n",
      "Training Epoch 7  95.5% | batch:       655 of       686\t|\tloss: 13.2068\n",
      "Training Epoch 7  95.6% | batch:       656 of       686\t|\tloss: 12.9053\n",
      "Training Epoch 7  95.8% | batch:       657 of       686\t|\tloss: 17.2719\n",
      "Training Epoch 7  95.9% | batch:       658 of       686\t|\tloss: 11.7599\n",
      "Training Epoch 7  96.1% | batch:       659 of       686\t|\tloss: 11.5639\n",
      "Training Epoch 7  96.2% | batch:       660 of       686\t|\tloss: 12.7675\n",
      "Training Epoch 7  96.4% | batch:       661 of       686\t|\tloss: 11.4171\n",
      "Training Epoch 7  96.5% | batch:       662 of       686\t|\tloss: 19.8523\n",
      "Training Epoch 7  96.6% | batch:       663 of       686\t|\tloss: 12.9506\n",
      "Training Epoch 7  96.8% | batch:       664 of       686\t|\tloss: 8.27013\n",
      "Training Epoch 7  96.9% | batch:       665 of       686\t|\tloss: 12.3485\n",
      "Training Epoch 7  97.1% | batch:       666 of       686\t|\tloss: 12.2342\n",
      "Training Epoch 7  97.2% | batch:       667 of       686\t|\tloss: 11.11\n",
      "Training Epoch 7  97.4% | batch:       668 of       686\t|\tloss: 13.5828\n",
      "Training Epoch 7  97.5% | batch:       669 of       686\t|\tloss: 10.7028\n",
      "Training Epoch 7  97.7% | batch:       670 of       686\t|\tloss: 15.8866\n",
      "Training Epoch 7  97.8% | batch:       671 of       686\t|\tloss: 11.7945\n",
      "Training Epoch 7  98.0% | batch:       672 of       686\t|\tloss: 16.9001\n",
      "Training Epoch 7  98.1% | batch:       673 of       686\t|\tloss: 12.4349\n",
      "Training Epoch 7  98.3% | batch:       674 of       686\t|\tloss: 11.067\n",
      "Training Epoch 7  98.4% | batch:       675 of       686\t|\tloss: 14.1503\n",
      "Training Epoch 7  98.5% | batch:       676 of       686\t|\tloss: 11.4618\n",
      "Training Epoch 7  98.7% | batch:       677 of       686\t|\tloss: 10.4633\n",
      "Training Epoch 7  98.8% | batch:       678 of       686\t|\tloss: 13.0284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:03:02,794 | INFO : Epoch 7 Training Summary: epoch: 7.000000 | loss: 14.786202 | \n",
      "2023-05-25 22:03:02,795 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 25.02272343635559 seconds\n",
      "\n",
      "2023-05-25 22:03:02,795 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.76144507953099 seconds\n",
      "2023-05-25 22:03:02,796 | INFO : Avg batch train. time: 0.03463767504304809 seconds\n",
      "2023-05-25 22:03:02,796 | INFO : Avg sample train. time: 0.0002709555285880722 seconds\n",
      "2023-05-25 22:03:02,797 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  99.0% | batch:       679 of       686\t|\tloss: 16.3408\n",
      "Training Epoch 7  99.1% | batch:       680 of       686\t|\tloss: 14.0227\n",
      "Training Epoch 7  99.3% | batch:       681 of       686\t|\tloss: 15.1208\n",
      "Training Epoch 7  99.4% | batch:       682 of       686\t|\tloss: 9.32667\n",
      "Training Epoch 7  99.6% | batch:       683 of       686\t|\tloss: 11.3988\n",
      "Training Epoch 7  99.7% | batch:       684 of       686\t|\tloss: 12.8027\n",
      "Training Epoch 7  99.9% | batch:       685 of       686\t|\tloss: 21.5845\n",
      "\n",
      "Evaluating Epoch 7   0.0% | batch:         0 of       172\t|\tloss: 2.32781\n",
      "Evaluating Epoch 7   0.6% | batch:         1 of       172\t|\tloss: 2.85461\n",
      "Evaluating Epoch 7   1.2% | batch:         2 of       172\t|\tloss: 2.19362\n",
      "Evaluating Epoch 7   1.7% | batch:         3 of       172\t|\tloss: 4.33815\n",
      "Evaluating Epoch 7   2.3% | batch:         4 of       172\t|\tloss: 2.4428\n",
      "Evaluating Epoch 7   2.9% | batch:         5 of       172\t|\tloss: 1.90875\n",
      "Evaluating Epoch 7   3.5% | batch:         6 of       172\t|\tloss: 2.75212\n",
      "Evaluating Epoch 7   4.1% | batch:         7 of       172\t|\tloss: 4.59207\n",
      "Evaluating Epoch 7   4.7% | batch:         8 of       172\t|\tloss: 1.9208\n",
      "Evaluating Epoch 7   5.2% | batch:         9 of       172\t|\tloss: 3.05607\n",
      "Evaluating Epoch 7   5.8% | batch:        10 of       172\t|\tloss: 3.52425\n",
      "Evaluating Epoch 7   6.4% | batch:        11 of       172\t|\tloss: 2.50551\n",
      "Evaluating Epoch 7   7.0% | batch:        12 of       172\t|\tloss: 2.17166\n",
      "Evaluating Epoch 7   7.6% | batch:        13 of       172\t|\tloss: 3.1025\n",
      "Evaluating Epoch 7   8.1% | batch:        14 of       172\t|\tloss: 3.14565\n",
      "Evaluating Epoch 7   8.7% | batch:        15 of       172\t|\tloss: 2.01696\n",
      "Evaluating Epoch 7   9.3% | batch:        16 of       172\t|\tloss: 3.45365\n",
      "Evaluating Epoch 7   9.9% | batch:        17 of       172\t|\tloss: 2.08024\n",
      "Evaluating Epoch 7  10.5% | batch:        18 of       172\t|\tloss: 12.7768\n",
      "Evaluating Epoch 7  11.0% | batch:        19 of       172\t|\tloss: 1.32594\n",
      "Evaluating Epoch 7  11.6% | batch:        20 of       172\t|\tloss: 2.21204\n",
      "Evaluating Epoch 7  12.2% | batch:        21 of       172\t|\tloss: 0.799862\n",
      "Evaluating Epoch 7  12.8% | batch:        22 of       172\t|\tloss: 2.08377\n",
      "Evaluating Epoch 7  13.4% | batch:        23 of       172\t|\tloss: 1.61039\n",
      "Evaluating Epoch 7  14.0% | batch:        24 of       172\t|\tloss: 1.74714\n",
      "Evaluating Epoch 7  14.5% | batch:        25 of       172\t|\tloss: 2.15527\n",
      "Evaluating Epoch 7  15.1% | batch:        26 of       172\t|\tloss: 6.40965\n",
      "Evaluating Epoch 7  15.7% | batch:        27 of       172\t|\tloss: 12.9771\n",
      "Evaluating Epoch 7  16.3% | batch:        28 of       172\t|\tloss: 0.445911\n",
      "Evaluating Epoch 7  16.9% | batch:        29 of       172\t|\tloss: 1.82336\n",
      "Evaluating Epoch 7  17.4% | batch:        30 of       172\t|\tloss: 1.17688\n",
      "Evaluating Epoch 7  18.0% | batch:        31 of       172\t|\tloss: 1.80267\n",
      "Evaluating Epoch 7  18.6% | batch:        32 of       172\t|\tloss: 0.399483\n",
      "Evaluating Epoch 7  19.2% | batch:        33 of       172\t|\tloss: 2.56088\n",
      "Evaluating Epoch 7  19.8% | batch:        34 of       172\t|\tloss: 0.490936\n",
      "Evaluating Epoch 7  20.3% | batch:        35 of       172\t|\tloss: 0.779616\n",
      "Evaluating Epoch 7  20.9% | batch:        36 of       172\t|\tloss: 6.51276\n",
      "Evaluating Epoch 7  21.5% | batch:        37 of       172\t|\tloss: 3.90764\n",
      "Evaluating Epoch 7  22.1% | batch:        38 of       172\t|\tloss: 2.49609\n",
      "Evaluating Epoch 7  22.7% | batch:        39 of       172\t|\tloss: 4.5296\n",
      "Evaluating Epoch 7  23.3% | batch:        40 of       172\t|\tloss: 0.46817\n",
      "Evaluating Epoch 7  23.8% | batch:        41 of       172\t|\tloss: 1.37807\n",
      "Evaluating Epoch 7  24.4% | batch:        42 of       172\t|\tloss: 0.827598\n",
      "Evaluating Epoch 7  25.0% | batch:        43 of       172\t|\tloss: 14.6963\n",
      "Evaluating Epoch 7  25.6% | batch:        44 of       172\t|\tloss: 1.03047\n",
      "Evaluating Epoch 7  26.2% | batch:        45 of       172\t|\tloss: 1.27308\n",
      "Evaluating Epoch 7  26.7% | batch:        46 of       172\t|\tloss: 0.769211\n",
      "Evaluating Epoch 7  27.3% | batch:        47 of       172\t|\tloss: 1.50317\n",
      "Evaluating Epoch 7  27.9% | batch:        48 of       172\t|\tloss: 0.488784\n",
      "Evaluating Epoch 7  28.5% | batch:        49 of       172\t|\tloss: 3.29735\n",
      "Evaluating Epoch 7  29.1% | batch:        50 of       172\t|\tloss: 0.870014\n",
      "Evaluating Epoch 7  29.7% | batch:        51 of       172\t|\tloss: 0.895895\n",
      "Evaluating Epoch 7  30.2% | batch:        52 of       172\t|\tloss: 1.16779\n",
      "Evaluating Epoch 7  30.8% | batch:        53 of       172\t|\tloss: 1.29061\n",
      "Evaluating Epoch 7  31.4% | batch:        54 of       172\t|\tloss: 1.35101\n",
      "Evaluating Epoch 7  32.0% | batch:        55 of       172\t|\tloss: 0.922179\n",
      "Evaluating Epoch 7  32.6% | batch:        56 of       172\t|\tloss: 1.46978\n",
      "Evaluating Epoch 7  33.1% | batch:        57 of       172\t|\tloss: 1.14463\n",
      "Evaluating Epoch 7  33.7% | batch:        58 of       172\t|\tloss: 1.24324\n",
      "Evaluating Epoch 7  34.3% | batch:        59 of       172\t|\tloss: 1.53562\n",
      "Evaluating Epoch 7  34.9% | batch:        60 of       172\t|\tloss: 0.941732\n",
      "Evaluating Epoch 7  35.5% | batch:        61 of       172\t|\tloss: 1.56448\n",
      "Evaluating Epoch 7  36.0% | batch:        62 of       172\t|\tloss: 1.0277\n",
      "Evaluating Epoch 7  36.6% | batch:        63 of       172\t|\tloss: 1.88495\n",
      "Evaluating Epoch 7  37.2% | batch:        64 of       172\t|\tloss: 1.1817\n",
      "Evaluating Epoch 7  37.8% | batch:        65 of       172\t|\tloss: 1.22806\n",
      "Evaluating Epoch 7  38.4% | batch:        66 of       172\t|\tloss: 1.76209\n",
      "Evaluating Epoch 7  39.0% | batch:        67 of       172\t|\tloss: 0.785308\n",
      "Evaluating Epoch 7  39.5% | batch:        68 of       172\t|\tloss: 2.4312\n",
      "Evaluating Epoch 7  40.1% | batch:        69 of       172\t|\tloss: 1.36674\n",
      "Evaluating Epoch 7  40.7% | batch:        70 of       172\t|\tloss: 0.686451\n",
      "Evaluating Epoch 7  41.3% | batch:        71 of       172\t|\tloss: 1.58871\n",
      "Evaluating Epoch 7  41.9% | batch:        72 of       172\t|\tloss: 0.985131\n",
      "Evaluating Epoch 7  42.4% | batch:        73 of       172\t|\tloss: 1.2434\n",
      "Evaluating Epoch 7  43.0% | batch:        74 of       172\t|\tloss: 0.642519\n",
      "Evaluating Epoch 7  43.6% | batch:        75 of       172\t|\tloss: 0.546097\n",
      "Evaluating Epoch 7  44.2% | batch:        76 of       172\t|\tloss: 0.315668\n",
      "Evaluating Epoch 7  44.8% | batch:        77 of       172\t|\tloss: 0.463647\n",
      "Evaluating Epoch 7  45.3% | batch:        78 of       172\t|\tloss: 0.86222\n",
      "Evaluating Epoch 7  45.9% | batch:        79 of       172\t|\tloss: 0.511114\n",
      "Evaluating Epoch 7  46.5% | batch:        80 of       172\t|\tloss: 0.556163\n",
      "Evaluating Epoch 7  47.1% | batch:        81 of       172\t|\tloss: 0.40724\n",
      "Evaluating Epoch 7  47.7% | batch:        82 of       172\t|\tloss: 0.495518\n",
      "Evaluating Epoch 7  48.3% | batch:        83 of       172\t|\tloss: 1.1757\n",
      "Evaluating Epoch 7  48.8% | batch:        84 of       172\t|\tloss: 0.796523\n",
      "Evaluating Epoch 7  49.4% | batch:        85 of       172\t|\tloss: 2.03362\n",
      "Evaluating Epoch 7  50.0% | batch:        86 of       172\t|\tloss: 2.04217\n",
      "Evaluating Epoch 7  50.6% | batch:        87 of       172\t|\tloss: 2.02827\n",
      "Evaluating Epoch 7  51.2% | batch:        88 of       172\t|\tloss: 1.24469\n",
      "Evaluating Epoch 7  51.7% | batch:        89 of       172\t|\tloss: 1.0523\n",
      "Evaluating Epoch 7  52.3% | batch:        90 of       172\t|\tloss: 2.93383\n",
      "Evaluating Epoch 7  52.9% | batch:        91 of       172\t|\tloss: 0.8408\n",
      "Evaluating Epoch 7  53.5% | batch:        92 of       172\t|\tloss: 1.08677\n",
      "Evaluating Epoch 7  54.1% | batch:        93 of       172\t|\tloss: 1.83358\n",
      "Evaluating Epoch 7  54.7% | batch:        94 of       172\t|\tloss: 3.07001\n",
      "Evaluating Epoch 7  55.2% | batch:        95 of       172\t|\tloss: 1.82313\n",
      "Evaluating Epoch 7  55.8% | batch:        96 of       172\t|\tloss: 1.04688\n",
      "Evaluating Epoch 7  56.4% | batch:        97 of       172\t|\tloss: 2.50656\n",
      "Evaluating Epoch 7  57.0% | batch:        98 of       172\t|\tloss: 1.3193\n",
      "Evaluating Epoch 7  57.6% | batch:        99 of       172\t|\tloss: 1.57198\n",
      "Evaluating Epoch 7  58.1% | batch:       100 of       172\t|\tloss: 2.72204\n",
      "Evaluating Epoch 7  58.7% | batch:       101 of       172\t|\tloss: 1.07494\n",
      "Evaluating Epoch 7  59.3% | batch:       102 of       172\t|\tloss: 1.53883\n",
      "Evaluating Epoch 7  59.9% | batch:       103 of       172\t|\tloss: 0.958177\n",
      "Evaluating Epoch 7  60.5% | batch:       104 of       172\t|\tloss: 2.83164\n",
      "Evaluating Epoch 7  61.0% | batch:       105 of       172\t|\tloss: 1.43088\n",
      "Evaluating Epoch 7  61.6% | batch:       106 of       172\t|\tloss: 1.39835\n",
      "Evaluating Epoch 7  62.2% | batch:       107 of       172\t|\tloss: 2.63014\n",
      "Evaluating Epoch 7  62.8% | batch:       108 of       172\t|\tloss: 1.29966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 7  63.4% | batch:       109 of       172\t|\tloss: 1.74555\n",
      "Evaluating Epoch 7  64.0% | batch:       110 of       172\t|\tloss: 0.901042\n",
      "Evaluating Epoch 7  64.5% | batch:       111 of       172\t|\tloss: 2.97222\n",
      "Evaluating Epoch 7  65.1% | batch:       112 of       172\t|\tloss: 1.70714\n",
      "Evaluating Epoch 7  65.7% | batch:       113 of       172\t|\tloss: 2.99833\n",
      "Evaluating Epoch 7  66.3% | batch:       114 of       172\t|\tloss: 2.32576\n",
      "Evaluating Epoch 7  66.9% | batch:       115 of       172\t|\tloss: 1.12497\n",
      "Evaluating Epoch 7  67.4% | batch:       116 of       172\t|\tloss: 1.98712\n",
      "Evaluating Epoch 7  68.0% | batch:       117 of       172\t|\tloss: 1.91564\n",
      "Evaluating Epoch 7  68.6% | batch:       118 of       172\t|\tloss: 1.52091\n",
      "Evaluating Epoch 7  69.2% | batch:       119 of       172\t|\tloss: 1.89614\n",
      "Evaluating Epoch 7  69.8% | batch:       120 of       172\t|\tloss: 1.11037\n",
      "Evaluating Epoch 7  70.3% | batch:       121 of       172\t|\tloss: 5.65363\n",
      "Evaluating Epoch 7  70.9% | batch:       122 of       172\t|\tloss: 2.92515\n",
      "Evaluating Epoch 7  71.5% | batch:       123 of       172\t|\tloss: 8.09257\n",
      "Evaluating Epoch 7  72.1% | batch:       124 of       172\t|\tloss: 25.2456\n",
      "Evaluating Epoch 7  72.7% | batch:       125 of       172\t|\tloss: 2.6023\n",
      "Evaluating Epoch 7  73.3% | batch:       126 of       172\t|\tloss: 1.3196\n",
      "Evaluating Epoch 7  73.8% | batch:       127 of       172\t|\tloss: 1.84397\n",
      "Evaluating Epoch 7  74.4% | batch:       128 of       172\t|\tloss: 2.82477\n",
      "Evaluating Epoch 7  75.0% | batch:       129 of       172\t|\tloss: 1.56596\n",
      "Evaluating Epoch 7  75.6% | batch:       130 of       172\t|\tloss: 1.50258\n",
      "Evaluating Epoch 7  76.2% | batch:       131 of       172\t|\tloss: 3.22714\n",
      "Evaluating Epoch 7  76.7% | batch:       132 of       172\t|\tloss: 1.56176\n",
      "Evaluating Epoch 7  77.3% | batch:       133 of       172\t|\tloss: 0.782672\n",
      "Evaluating Epoch 7  77.9% | batch:       134 of       172\t|\tloss: 1.45802\n",
      "Evaluating Epoch 7  78.5% | batch:       135 of       172\t|\tloss: 0.41915\n",
      "Evaluating Epoch 7  79.1% | batch:       136 of       172\t|\tloss: 0.991992\n",
      "Evaluating Epoch 7  79.7% | batch:       137 of       172\t|\tloss: 0.38641\n",
      "Evaluating Epoch 7  80.2% | batch:       138 of       172\t|\tloss: 1.60448\n",
      "Evaluating Epoch 7  80.8% | batch:       139 of       172\t|\tloss: 1.36335\n",
      "Evaluating Epoch 7  81.4% | batch:       140 of       172\t|\tloss: 0.866466\n",
      "Evaluating Epoch 7  82.0% | batch:       141 of       172\t|\tloss: 0.783639\n",
      "Evaluating Epoch 7  82.6% | batch:       142 of       172\t|\tloss: 0.766985\n",
      "Evaluating Epoch 7  83.1% | batch:       143 of       172\t|\tloss: 0.778789\n",
      "Evaluating Epoch 7  83.7% | batch:       144 of       172\t|\tloss: 1.08059\n",
      "Evaluating Epoch 7  84.3% | batch:       145 of       172\t|\tloss: 0.585456\n",
      "Evaluating Epoch 7  84.9% | batch:       146 of       172\t|\tloss: 0.831771\n",
      "Evaluating Epoch 7  85.5% | batch:       147 of       172\t|\tloss: 0.618189\n",
      "Evaluating Epoch 7  86.0% | batch:       148 of       172\t|\tloss: 0.562381\n",
      "Evaluating Epoch 7  86.6% | batch:       149 of       172\t|\tloss: 0.653569\n",
      "Evaluating Epoch 7  87.2% | batch:       150 of       172\t|\tloss: 1.0742\n",
      "Evaluating Epoch 7  87.8% | batch:       151 of       172\t|\tloss: 0.663875\n",
      "Evaluating Epoch 7  88.4% | batch:       152 of       172\t|\tloss: 1.07255\n",
      "Evaluating Epoch 7  89.0% | batch:       153 of       172\t|\tloss: 0.805229\n",
      "Evaluating Epoch 7  89.5% | batch:       154 of       172\t|\tloss: 0.742738\n",
      "Evaluating Epoch 7  90.1% | batch:       155 of       172\t|\tloss: 1.39888\n",
      "Evaluating Epoch 7  90.7% | batch:       156 of       172\t|\tloss: 0.764295\n",
      "Evaluating Epoch 7  91.3% | batch:       157 of       172\t|\tloss: 1.05332\n",
      "Evaluating Epoch 7  91.9% | batch:       158 of       172\t|\tloss: 1.17663\n",
      "Evaluating Epoch 7  92.4% | batch:       159 of       172\t|\tloss: 1.13482\n",
      "Evaluating Epoch 7  93.0% | batch:       160 of       172\t|\tloss: 7.18143\n",
      "Evaluating Epoch 7  93.6% | batch:       161 of       172\t|\tloss: 5.02326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:03:06,640 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.8428587913513184 seconds\n",
      "\n",
      "2023-05-25 22:03:06,641 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.079228192567825 seconds\n",
      "2023-05-25 22:03:06,642 | INFO : Avg batch val. time: 0.023716442980045497 seconds\n",
      "2023-05-25 22:03:06,642 | INFO : Avg sample val. time: 0.00018578258380324384 seconds\n",
      "2023-05-25 22:03:06,642 | INFO : Epoch 7 Validation Summary: epoch: 7.000000 | loss: 2.055889 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 7  94.2% | batch:       162 of       172\t|\tloss: 0.702363\n",
      "Evaluating Epoch 7  94.8% | batch:       163 of       172\t|\tloss: 1.14181\n",
      "Evaluating Epoch 7  95.3% | batch:       164 of       172\t|\tloss: 1.38175\n",
      "Evaluating Epoch 7  95.9% | batch:       165 of       172\t|\tloss: 0.640883\n",
      "Evaluating Epoch 7  96.5% | batch:       166 of       172\t|\tloss: 0.997187\n",
      "Evaluating Epoch 7  97.1% | batch:       167 of       172\t|\tloss: 0.872264\n",
      "Evaluating Epoch 7  97.7% | batch:       168 of       172\t|\tloss: 0.704984\n",
      "Evaluating Epoch 7  98.3% | batch:       169 of       172\t|\tloss: 1.04722\n",
      "Evaluating Epoch 7  98.8% | batch:       170 of       172\t|\tloss: 0.710335\n",
      "Evaluating Epoch 7  99.4% | batch:       171 of       172\t|\tloss: 0.869042\n",
      "\n",
      "Training Epoch 8   0.0% | batch:         0 of       686\t|\tloss: 12.0098\n",
      "Training Epoch 8   0.1% | batch:         1 of       686\t|\tloss: 11.2418\n",
      "Training Epoch 8   0.3% | batch:         2 of       686\t|\tloss: 11.0028\n",
      "Training Epoch 8   0.4% | batch:         3 of       686\t|\tloss: 29.1004\n",
      "Training Epoch 8   0.6% | batch:         4 of       686\t|\tloss: 12.242\n",
      "Training Epoch 8   0.7% | batch:         5 of       686\t|\tloss: 10.938\n",
      "Training Epoch 8   0.9% | batch:         6 of       686\t|\tloss: 11.7341\n",
      "Training Epoch 8   1.0% | batch:         7 of       686\t|\tloss: 31.784\n",
      "Training Epoch 8   1.2% | batch:         8 of       686\t|\tloss: 12.7303\n",
      "Training Epoch 8   1.3% | batch:         9 of       686\t|\tloss: 13.3441\n",
      "Training Epoch 8   1.5% | batch:        10 of       686\t|\tloss: 14.9378\n",
      "Training Epoch 8   1.6% | batch:        11 of       686\t|\tloss: 12.9342\n",
      "Training Epoch 8   1.7% | batch:        12 of       686\t|\tloss: 14.83\n",
      "Training Epoch 8   1.9% | batch:        13 of       686\t|\tloss: 15.2442\n",
      "Training Epoch 8   2.0% | batch:        14 of       686\t|\tloss: 12.9409\n",
      "Training Epoch 8   2.2% | batch:        15 of       686\t|\tloss: 12.8925\n",
      "Training Epoch 8   2.3% | batch:        16 of       686\t|\tloss: 14.0201\n",
      "Training Epoch 8   2.5% | batch:        17 of       686\t|\tloss: 12.5927\n",
      "Training Epoch 8   2.6% | batch:        18 of       686\t|\tloss: 13.7867\n",
      "Training Epoch 8   2.8% | batch:        19 of       686\t|\tloss: 11.806\n",
      "Training Epoch 8   2.9% | batch:        20 of       686\t|\tloss: 10.2567\n",
      "Training Epoch 8   3.1% | batch:        21 of       686\t|\tloss: 13.6747\n",
      "Training Epoch 8   3.2% | batch:        22 of       686\t|\tloss: 12.3603\n",
      "Training Epoch 8   3.4% | batch:        23 of       686\t|\tloss: 15.9096\n",
      "Training Epoch 8   3.5% | batch:        24 of       686\t|\tloss: 12.0774\n",
      "Training Epoch 8   3.6% | batch:        25 of       686\t|\tloss: 14.4174\n",
      "Training Epoch 8   3.8% | batch:        26 of       686\t|\tloss: 14.6021\n",
      "Training Epoch 8   3.9% | batch:        27 of       686\t|\tloss: 10.7911\n",
      "Training Epoch 8   4.1% | batch:        28 of       686\t|\tloss: 11.1652\n",
      "Training Epoch 8   4.2% | batch:        29 of       686\t|\tloss: 14.5656\n",
      "Training Epoch 8   4.4% | batch:        30 of       686\t|\tloss: 10.1279\n",
      "Training Epoch 8   4.5% | batch:        31 of       686\t|\tloss: 12.235\n",
      "Training Epoch 8   4.7% | batch:        32 of       686\t|\tloss: 12.936\n",
      "Training Epoch 8   4.8% | batch:        33 of       686\t|\tloss: 15.7651\n",
      "Training Epoch 8   5.0% | batch:        34 of       686\t|\tloss: 12.9091\n",
      "Training Epoch 8   5.1% | batch:        35 of       686\t|\tloss: 12.8044\n",
      "Training Epoch 8   5.2% | batch:        36 of       686\t|\tloss: 13.2072\n",
      "Training Epoch 8   5.4% | batch:        37 of       686\t|\tloss: 14.0764\n",
      "Training Epoch 8   5.5% | batch:        38 of       686\t|\tloss: 10.7438\n",
      "Training Epoch 8   5.7% | batch:        39 of       686\t|\tloss: 11.3712\n",
      "Training Epoch 8   5.8% | batch:        40 of       686\t|\tloss: 11.4089\n",
      "Training Epoch 8   6.0% | batch:        41 of       686\t|\tloss: 10.5579\n",
      "Training Epoch 8   6.1% | batch:        42 of       686\t|\tloss: 19.3151\n",
      "Training Epoch 8   6.3% | batch:        43 of       686\t|\tloss: 13.2548\n",
      "Training Epoch 8   6.4% | batch:        44 of       686\t|\tloss: 12.4011\n",
      "Training Epoch 8   6.6% | batch:        45 of       686\t|\tloss: 18.245\n",
      "Training Epoch 8   6.7% | batch:        46 of       686\t|\tloss: 11.4272\n",
      "Training Epoch 8   6.9% | batch:        47 of       686\t|\tloss: 15.0068\n",
      "Training Epoch 8   7.0% | batch:        48 of       686\t|\tloss: 15.0065\n",
      "Training Epoch 8   7.1% | batch:        49 of       686\t|\tloss: 8.0806\n",
      "Training Epoch 8   7.3% | batch:        50 of       686\t|\tloss: 14.242\n",
      "Training Epoch 8   7.4% | batch:        51 of       686\t|\tloss: 9.34368\n",
      "Training Epoch 8   7.6% | batch:        52 of       686\t|\tloss: 12.3798\n",
      "Training Epoch 8   7.7% | batch:        53 of       686\t|\tloss: 11.033\n",
      "Training Epoch 8   7.9% | batch:        54 of       686\t|\tloss: 9.59877\n",
      "Training Epoch 8   8.0% | batch:        55 of       686\t|\tloss: 11.035\n",
      "Training Epoch 8   8.2% | batch:        56 of       686\t|\tloss: 13.487\n",
      "Training Epoch 8   8.3% | batch:        57 of       686\t|\tloss: 13.1789\n",
      "Training Epoch 8   8.5% | batch:        58 of       686\t|\tloss: 11.1603\n",
      "Training Epoch 8   8.6% | batch:        59 of       686\t|\tloss: 10.6775\n",
      "Training Epoch 8   8.7% | batch:        60 of       686\t|\tloss: 9.93914\n",
      "Training Epoch 8   8.9% | batch:        61 of       686\t|\tloss: 10.8011\n",
      "Training Epoch 8   9.0% | batch:        62 of       686\t|\tloss: 14.1979\n",
      "Training Epoch 8   9.2% | batch:        63 of       686\t|\tloss: 15.0284\n",
      "Training Epoch 8   9.3% | batch:        64 of       686\t|\tloss: 11.8217\n",
      "Training Epoch 8   9.5% | batch:        65 of       686\t|\tloss: 13.8256\n",
      "Training Epoch 8   9.6% | batch:        66 of       686\t|\tloss: 13.8328\n",
      "Training Epoch 8   9.8% | batch:        67 of       686\t|\tloss: 13.7135\n",
      "Training Epoch 8   9.9% | batch:        68 of       686\t|\tloss: 15.727\n",
      "Training Epoch 8  10.1% | batch:        69 of       686\t|\tloss: 8.91456\n",
      "Training Epoch 8  10.2% | batch:        70 of       686\t|\tloss: 12.7417\n",
      "Training Epoch 8  10.3% | batch:        71 of       686\t|\tloss: 9.82779\n",
      "Training Epoch 8  10.5% | batch:        72 of       686\t|\tloss: 19.4465\n",
      "Training Epoch 8  10.6% | batch:        73 of       686\t|\tloss: 12.7573\n",
      "Training Epoch 8  10.8% | batch:        74 of       686\t|\tloss: 10.5221\n",
      "Training Epoch 8  10.9% | batch:        75 of       686\t|\tloss: 12.0648\n",
      "Training Epoch 8  11.1% | batch:        76 of       686\t|\tloss: 16.9814\n",
      "Training Epoch 8  11.2% | batch:        77 of       686\t|\tloss: 13.0955\n",
      "Training Epoch 8  11.4% | batch:        78 of       686\t|\tloss: 12.3963\n",
      "Training Epoch 8  11.5% | batch:        79 of       686\t|\tloss: 11.8459\n",
      "Training Epoch 8  11.7% | batch:        80 of       686\t|\tloss: 11.7742\n",
      "Training Epoch 8  11.8% | batch:        81 of       686\t|\tloss: 14.5632\n",
      "Training Epoch 8  12.0% | batch:        82 of       686\t|\tloss: 14.4877\n",
      "Training Epoch 8  12.1% | batch:        83 of       686\t|\tloss: 14.3728\n",
      "Training Epoch 8  12.2% | batch:        84 of       686\t|\tloss: 13.6821\n",
      "Training Epoch 8  12.4% | batch:        85 of       686\t|\tloss: 11.4279\n",
      "Training Epoch 8  12.5% | batch:        86 of       686\t|\tloss: 12.8252\n",
      "Training Epoch 8  12.7% | batch:        87 of       686\t|\tloss: 14.0383\n",
      "Training Epoch 8  12.8% | batch:        88 of       686\t|\tloss: 17.9492\n",
      "Training Epoch 8  13.0% | batch:        89 of       686\t|\tloss: 10.4283\n",
      "Training Epoch 8  13.1% | batch:        90 of       686\t|\tloss: 8.24467\n",
      "Training Epoch 8  13.3% | batch:        91 of       686\t|\tloss: 12.2965\n",
      "Training Epoch 8  13.4% | batch:        92 of       686\t|\tloss: 11.7487\n",
      "Training Epoch 8  13.6% | batch:        93 of       686\t|\tloss: 16.0364\n",
      "Training Epoch 8  13.7% | batch:        94 of       686\t|\tloss: 11.6116\n",
      "Training Epoch 8  13.8% | batch:        95 of       686\t|\tloss: 15.0745\n",
      "Training Epoch 8  14.0% | batch:        96 of       686\t|\tloss: 12.0054\n",
      "Training Epoch 8  14.1% | batch:        97 of       686\t|\tloss: 15.9121\n",
      "Training Epoch 8  14.3% | batch:        98 of       686\t|\tloss: 9.8223\n",
      "Training Epoch 8  14.4% | batch:        99 of       686\t|\tloss: 11.9418\n",
      "Training Epoch 8  14.6% | batch:       100 of       686\t|\tloss: 11.7773\n",
      "Training Epoch 8  14.7% | batch:       101 of       686\t|\tloss: 12.4248\n",
      "Training Epoch 8  14.9% | batch:       102 of       686\t|\tloss: 15.7549\n",
      "Training Epoch 8  15.0% | batch:       103 of       686\t|\tloss: 10.9969\n",
      "Training Epoch 8  15.2% | batch:       104 of       686\t|\tloss: 16.1136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  15.3% | batch:       105 of       686\t|\tloss: 15.9683\n",
      "Training Epoch 8  15.5% | batch:       106 of       686\t|\tloss: 12.7103\n",
      "Training Epoch 8  15.6% | batch:       107 of       686\t|\tloss: 10.7111\n",
      "Training Epoch 8  15.7% | batch:       108 of       686\t|\tloss: 13.1866\n",
      "Training Epoch 8  15.9% | batch:       109 of       686\t|\tloss: 16.4169\n",
      "Training Epoch 8  16.0% | batch:       110 of       686\t|\tloss: 13.3999\n",
      "Training Epoch 8  16.2% | batch:       111 of       686\t|\tloss: 12.9894\n",
      "Training Epoch 8  16.3% | batch:       112 of       686\t|\tloss: 12.0816\n",
      "Training Epoch 8  16.5% | batch:       113 of       686\t|\tloss: 11.0774\n",
      "Training Epoch 8  16.6% | batch:       114 of       686\t|\tloss: 13.3092\n",
      "Training Epoch 8  16.8% | batch:       115 of       686\t|\tloss: 13.0742\n",
      "Training Epoch 8  16.9% | batch:       116 of       686\t|\tloss: 11.7382\n",
      "Training Epoch 8  17.1% | batch:       117 of       686\t|\tloss: 14.3498\n",
      "Training Epoch 8  17.2% | batch:       118 of       686\t|\tloss: 11.1695\n",
      "Training Epoch 8  17.3% | batch:       119 of       686\t|\tloss: 13.9532\n",
      "Training Epoch 8  17.5% | batch:       120 of       686\t|\tloss: 11.2277\n",
      "Training Epoch 8  17.6% | batch:       121 of       686\t|\tloss: 15.0105\n",
      "Training Epoch 8  17.8% | batch:       122 of       686\t|\tloss: 10.3543\n",
      "Training Epoch 8  17.9% | batch:       123 of       686\t|\tloss: 12.1615\n",
      "Training Epoch 8  18.1% | batch:       124 of       686\t|\tloss: 13.4896\n",
      "Training Epoch 8  18.2% | batch:       125 of       686\t|\tloss: 11.4556\n",
      "Training Epoch 8  18.4% | batch:       126 of       686\t|\tloss: 13.1578\n",
      "Training Epoch 8  18.5% | batch:       127 of       686\t|\tloss: 12.3643\n",
      "Training Epoch 8  18.7% | batch:       128 of       686\t|\tloss: 11.5814\n",
      "Training Epoch 8  18.8% | batch:       129 of       686\t|\tloss: 15.2322\n",
      "Training Epoch 8  19.0% | batch:       130 of       686\t|\tloss: 15.0293\n",
      "Training Epoch 8  19.1% | batch:       131 of       686\t|\tloss: 11.964\n",
      "Training Epoch 8  19.2% | batch:       132 of       686\t|\tloss: 12.8827\n",
      "Training Epoch 8  19.4% | batch:       133 of       686\t|\tloss: 10.3853\n",
      "Training Epoch 8  19.5% | batch:       134 of       686\t|\tloss: 10.9223\n",
      "Training Epoch 8  19.7% | batch:       135 of       686\t|\tloss: 14.1083\n",
      "Training Epoch 8  19.8% | batch:       136 of       686\t|\tloss: 11.3697\n",
      "Training Epoch 8  20.0% | batch:       137 of       686\t|\tloss: 15.3862\n",
      "Training Epoch 8  20.1% | batch:       138 of       686\t|\tloss: 13.2009\n",
      "Training Epoch 8  20.3% | batch:       139 of       686\t|\tloss: 11.0727\n",
      "Training Epoch 8  20.4% | batch:       140 of       686\t|\tloss: 12.5895\n",
      "Training Epoch 8  20.6% | batch:       141 of       686\t|\tloss: 12.1608\n",
      "Training Epoch 8  20.7% | batch:       142 of       686\t|\tloss: 12.3973\n",
      "Training Epoch 8  20.8% | batch:       143 of       686\t|\tloss: 13.5965\n",
      "Training Epoch 8  21.0% | batch:       144 of       686\t|\tloss: 9.54156\n",
      "Training Epoch 8  21.1% | batch:       145 of       686\t|\tloss: 11.5631\n",
      "Training Epoch 8  21.3% | batch:       146 of       686\t|\tloss: 14.8426\n",
      "Training Epoch 8  21.4% | batch:       147 of       686\t|\tloss: 12.0706\n",
      "Training Epoch 8  21.6% | batch:       148 of       686\t|\tloss: 13.8097\n",
      "Training Epoch 8  21.7% | batch:       149 of       686\t|\tloss: 9.19661\n",
      "Training Epoch 8  21.9% | batch:       150 of       686\t|\tloss: 11.8218\n",
      "Training Epoch 8  22.0% | batch:       151 of       686\t|\tloss: 12.4658\n",
      "Training Epoch 8  22.2% | batch:       152 of       686\t|\tloss: 11.2079\n",
      "Training Epoch 8  22.3% | batch:       153 of       686\t|\tloss: 12.2295\n",
      "Training Epoch 8  22.4% | batch:       154 of       686\t|\tloss: 10.3545\n",
      "Training Epoch 8  22.6% | batch:       155 of       686\t|\tloss: 11.3418\n",
      "Training Epoch 8  22.7% | batch:       156 of       686\t|\tloss: 10.437\n",
      "Training Epoch 8  22.9% | batch:       157 of       686\t|\tloss: 9.78346\n",
      "Training Epoch 8  23.0% | batch:       158 of       686\t|\tloss: 11.1168\n",
      "Training Epoch 8  23.2% | batch:       159 of       686\t|\tloss: 16.0495\n",
      "Training Epoch 8  23.3% | batch:       160 of       686\t|\tloss: 9.07775\n",
      "Training Epoch 8  23.5% | batch:       161 of       686\t|\tloss: 11.5759\n",
      "Training Epoch 8  23.6% | batch:       162 of       686\t|\tloss: 10.92\n",
      "Training Epoch 8  23.8% | batch:       163 of       686\t|\tloss: 12.4613\n",
      "Training Epoch 8  23.9% | batch:       164 of       686\t|\tloss: 13.9643\n",
      "Training Epoch 8  24.1% | batch:       165 of       686\t|\tloss: 12.6921\n",
      "Training Epoch 8  24.2% | batch:       166 of       686\t|\tloss: 11.6095\n",
      "Training Epoch 8  24.3% | batch:       167 of       686\t|\tloss: 9.69744\n",
      "Training Epoch 8  24.5% | batch:       168 of       686\t|\tloss: 10.8111\n",
      "Training Epoch 8  24.6% | batch:       169 of       686\t|\tloss: 11.1151\n",
      "Training Epoch 8  24.8% | batch:       170 of       686\t|\tloss: 11.2666\n",
      "Training Epoch 8  24.9% | batch:       171 of       686\t|\tloss: 11.2739\n",
      "Training Epoch 8  25.1% | batch:       172 of       686\t|\tloss: 11.4786\n",
      "Training Epoch 8  25.2% | batch:       173 of       686\t|\tloss: 12.1346\n",
      "Training Epoch 8  25.4% | batch:       174 of       686\t|\tloss: 13.0719\n",
      "Training Epoch 8  25.5% | batch:       175 of       686\t|\tloss: 13.6505\n",
      "Training Epoch 8  25.7% | batch:       176 of       686\t|\tloss: 15.3516\n",
      "Training Epoch 8  25.8% | batch:       177 of       686\t|\tloss: 11.0769\n",
      "Training Epoch 8  25.9% | batch:       178 of       686\t|\tloss: 12.8883\n",
      "Training Epoch 8  26.1% | batch:       179 of       686\t|\tloss: 13.0279\n",
      "Training Epoch 8  26.2% | batch:       180 of       686\t|\tloss: 10.5461\n",
      "Training Epoch 8  26.4% | batch:       181 of       686\t|\tloss: 9.88807\n",
      "Training Epoch 8  26.5% | batch:       182 of       686\t|\tloss: 10.5414\n",
      "Training Epoch 8  26.7% | batch:       183 of       686\t|\tloss: 14.7296\n",
      "Training Epoch 8  26.8% | batch:       184 of       686\t|\tloss: 15.0393\n",
      "Training Epoch 8  27.0% | batch:       185 of       686\t|\tloss: 12.1374\n",
      "Training Epoch 8  27.1% | batch:       186 of       686\t|\tloss: 13.3947\n",
      "Training Epoch 8  27.3% | batch:       187 of       686\t|\tloss: 12.3473\n",
      "Training Epoch 8  27.4% | batch:       188 of       686\t|\tloss: 11.3344\n",
      "Training Epoch 8  27.6% | batch:       189 of       686\t|\tloss: 13.5939\n",
      "Training Epoch 8  27.7% | batch:       190 of       686\t|\tloss: 10.2749\n",
      "Training Epoch 8  27.8% | batch:       191 of       686\t|\tloss: 13.2813\n",
      "Training Epoch 8  28.0% | batch:       192 of       686\t|\tloss: 11.6591\n",
      "Training Epoch 8  28.1% | batch:       193 of       686\t|\tloss: 10.6706\n",
      "Training Epoch 8  28.3% | batch:       194 of       686\t|\tloss: 10.7265\n",
      "Training Epoch 8  28.4% | batch:       195 of       686\t|\tloss: 13.9675\n",
      "Training Epoch 8  28.6% | batch:       196 of       686\t|\tloss: 9.82155\n",
      "Training Epoch 8  28.7% | batch:       197 of       686\t|\tloss: 11.6171\n",
      "Training Epoch 8  28.9% | batch:       198 of       686\t|\tloss: 9.53848\n",
      "Training Epoch 8  29.0% | batch:       199 of       686\t|\tloss: 11.7181\n",
      "Training Epoch 8  29.2% | batch:       200 of       686\t|\tloss: 10.013\n",
      "Training Epoch 8  29.3% | batch:       201 of       686\t|\tloss: 10.8916\n",
      "Training Epoch 8  29.4% | batch:       202 of       686\t|\tloss: 12.5032\n",
      "Training Epoch 8  29.6% | batch:       203 of       686\t|\tloss: 12.3874\n",
      "Training Epoch 8  29.7% | batch:       204 of       686\t|\tloss: 13.7061\n",
      "Training Epoch 8  29.9% | batch:       205 of       686\t|\tloss: 12.5884\n",
      "Training Epoch 8  30.0% | batch:       206 of       686\t|\tloss: 7.89741\n",
      "Training Epoch 8  30.2% | batch:       207 of       686\t|\tloss: 12.949\n",
      "Training Epoch 8  30.3% | batch:       208 of       686\t|\tloss: 12.2802\n",
      "Training Epoch 8  30.5% | batch:       209 of       686\t|\tloss: 14.4753\n",
      "Training Epoch 8  30.6% | batch:       210 of       686\t|\tloss: 16.0282\n",
      "Training Epoch 8  30.8% | batch:       211 of       686\t|\tloss: 8.90584\n",
      "Training Epoch 8  30.9% | batch:       212 of       686\t|\tloss: 13.5756\n",
      "Training Epoch 8  31.0% | batch:       213 of       686\t|\tloss: 14.7256\n",
      "Training Epoch 8  31.2% | batch:       214 of       686\t|\tloss: 13.8622\n",
      "Training Epoch 8  31.3% | batch:       215 of       686\t|\tloss: 11.2205\n",
      "Training Epoch 8  31.5% | batch:       216 of       686\t|\tloss: 11.7468\n",
      "Training Epoch 8  31.6% | batch:       217 of       686\t|\tloss: 12.2959\n",
      "Training Epoch 8  31.8% | batch:       218 of       686\t|\tloss: 10.3752\n",
      "Training Epoch 8  31.9% | batch:       219 of       686\t|\tloss: 10.5401\n",
      "Training Epoch 8  32.1% | batch:       220 of       686\t|\tloss: 11.8261\n",
      "Training Epoch 8  32.2% | batch:       221 of       686\t|\tloss: 11.8298\n",
      "Training Epoch 8  32.4% | batch:       222 of       686\t|\tloss: 14.0887\n",
      "Training Epoch 8  32.5% | batch:       223 of       686\t|\tloss: 13.1614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  32.7% | batch:       224 of       686\t|\tloss: 11.3092\n",
      "Training Epoch 8  32.8% | batch:       225 of       686\t|\tloss: 11.6622\n",
      "Training Epoch 8  32.9% | batch:       226 of       686\t|\tloss: 15.064\n",
      "Training Epoch 8  33.1% | batch:       227 of       686\t|\tloss: 16.4735\n",
      "Training Epoch 8  33.2% | batch:       228 of       686\t|\tloss: 11.4046\n",
      "Training Epoch 8  33.4% | batch:       229 of       686\t|\tloss: 10.5836\n",
      "Training Epoch 8  33.5% | batch:       230 of       686\t|\tloss: 11.0868\n",
      "Training Epoch 8  33.7% | batch:       231 of       686\t|\tloss: 10.5813\n",
      "Training Epoch 8  33.8% | batch:       232 of       686\t|\tloss: 11.8613\n",
      "Training Epoch 8  34.0% | batch:       233 of       686\t|\tloss: 17.8615\n",
      "Training Epoch 8  34.1% | batch:       234 of       686\t|\tloss: 12.8869\n",
      "Training Epoch 8  34.3% | batch:       235 of       686\t|\tloss: 9.9254\n",
      "Training Epoch 8  34.4% | batch:       236 of       686\t|\tloss: 12.7601\n",
      "Training Epoch 8  34.5% | batch:       237 of       686\t|\tloss: 11.5903\n",
      "Training Epoch 8  34.7% | batch:       238 of       686\t|\tloss: 12.3765\n",
      "Training Epoch 8  34.8% | batch:       239 of       686\t|\tloss: 11.6591\n",
      "Training Epoch 8  35.0% | batch:       240 of       686\t|\tloss: 14.4863\n",
      "Training Epoch 8  35.1% | batch:       241 of       686\t|\tloss: 20.969\n",
      "Training Epoch 8  35.3% | batch:       242 of       686\t|\tloss: 10.4993\n",
      "Training Epoch 8  35.4% | batch:       243 of       686\t|\tloss: 13.1215\n",
      "Training Epoch 8  35.6% | batch:       244 of       686\t|\tloss: 11.4454\n",
      "Training Epoch 8  35.7% | batch:       245 of       686\t|\tloss: 15.419\n",
      "Training Epoch 8  35.9% | batch:       246 of       686\t|\tloss: 13.0495\n",
      "Training Epoch 8  36.0% | batch:       247 of       686\t|\tloss: 10.2957\n",
      "Training Epoch 8  36.2% | batch:       248 of       686\t|\tloss: 11.5192\n",
      "Training Epoch 8  36.3% | batch:       249 of       686\t|\tloss: 11.6973\n",
      "Training Epoch 8  36.4% | batch:       250 of       686\t|\tloss: 14.6368\n",
      "Training Epoch 8  36.6% | batch:       251 of       686\t|\tloss: 11.5909\n",
      "Training Epoch 8  36.7% | batch:       252 of       686\t|\tloss: 9.21081\n",
      "Training Epoch 8  36.9% | batch:       253 of       686\t|\tloss: 11.3451\n",
      "Training Epoch 8  37.0% | batch:       254 of       686\t|\tloss: 11.7491\n",
      "Training Epoch 8  37.2% | batch:       255 of       686\t|\tloss: 10.8153\n",
      "Training Epoch 8  37.3% | batch:       256 of       686\t|\tloss: 9.80955\n",
      "Training Epoch 8  37.5% | batch:       257 of       686\t|\tloss: 8.96933\n",
      "Training Epoch 8  37.6% | batch:       258 of       686\t|\tloss: 11.9349\n",
      "Training Epoch 8  37.8% | batch:       259 of       686\t|\tloss: 12.7291\n",
      "Training Epoch 8  37.9% | batch:       260 of       686\t|\tloss: 14.3731\n",
      "Training Epoch 8  38.0% | batch:       261 of       686\t|\tloss: 11.4331\n",
      "Training Epoch 8  38.2% | batch:       262 of       686\t|\tloss: 12.9542\n",
      "Training Epoch 8  38.3% | batch:       263 of       686\t|\tloss: 11.6049\n",
      "Training Epoch 8  38.5% | batch:       264 of       686\t|\tloss: 11.5655\n",
      "Training Epoch 8  38.6% | batch:       265 of       686\t|\tloss: 10.7323\n",
      "Training Epoch 8  38.8% | batch:       266 of       686\t|\tloss: 8.77419\n",
      "Training Epoch 8  38.9% | batch:       267 of       686\t|\tloss: 11.6636\n",
      "Training Epoch 8  39.1% | batch:       268 of       686\t|\tloss: 12.1849\n",
      "Training Epoch 8  39.2% | batch:       269 of       686\t|\tloss: 10.2053\n",
      "Training Epoch 8  39.4% | batch:       270 of       686\t|\tloss: 11.2353\n",
      "Training Epoch 8  39.5% | batch:       271 of       686\t|\tloss: 14.2769\n",
      "Training Epoch 8  39.7% | batch:       272 of       686\t|\tloss: 9.07905\n",
      "Training Epoch 8  39.8% | batch:       273 of       686\t|\tloss: 10.2959\n",
      "Training Epoch 8  39.9% | batch:       274 of       686\t|\tloss: 10.3152\n",
      "Training Epoch 8  40.1% | batch:       275 of       686\t|\tloss: 13.8514\n",
      "Training Epoch 8  40.2% | batch:       276 of       686\t|\tloss: 11.0984\n",
      "Training Epoch 8  40.4% | batch:       277 of       686\t|\tloss: 16.1128\n",
      "Training Epoch 8  40.5% | batch:       278 of       686\t|\tloss: 13.6396\n",
      "Training Epoch 8  40.7% | batch:       279 of       686\t|\tloss: 10.5182\n",
      "Training Epoch 8  40.8% | batch:       280 of       686\t|\tloss: 15.1546\n",
      "Training Epoch 8  41.0% | batch:       281 of       686\t|\tloss: 13.5116\n",
      "Training Epoch 8  41.1% | batch:       282 of       686\t|\tloss: 10.5919\n",
      "Training Epoch 8  41.3% | batch:       283 of       686\t|\tloss: 13.5682\n",
      "Training Epoch 8  41.4% | batch:       284 of       686\t|\tloss: 10.8601\n",
      "Training Epoch 8  41.5% | batch:       285 of       686\t|\tloss: 12.0876\n",
      "Training Epoch 8  41.7% | batch:       286 of       686\t|\tloss: 18.5205\n",
      "Training Epoch 8  41.8% | batch:       287 of       686\t|\tloss: 14.3272\n",
      "Training Epoch 8  42.0% | batch:       288 of       686\t|\tloss: 9.93865\n",
      "Training Epoch 8  42.1% | batch:       289 of       686\t|\tloss: 12.7893\n",
      "Training Epoch 8  42.3% | batch:       290 of       686\t|\tloss: 13.4404\n",
      "Training Epoch 8  42.4% | batch:       291 of       686\t|\tloss: 13.2776\n",
      "Training Epoch 8  42.6% | batch:       292 of       686\t|\tloss: 10.1789\n",
      "Training Epoch 8  42.7% | batch:       293 of       686\t|\tloss: 10.9833\n",
      "Training Epoch 8  42.9% | batch:       294 of       686\t|\tloss: 10.8567\n",
      "Training Epoch 8  43.0% | batch:       295 of       686\t|\tloss: 14.0772\n",
      "Training Epoch 8  43.1% | batch:       296 of       686\t|\tloss: 14.2184\n",
      "Training Epoch 8  43.3% | batch:       297 of       686\t|\tloss: 11.5516\n",
      "Training Epoch 8  43.4% | batch:       298 of       686\t|\tloss: 9.25051\n",
      "Training Epoch 8  43.6% | batch:       299 of       686\t|\tloss: 14.1503\n",
      "Training Epoch 8  43.7% | batch:       300 of       686\t|\tloss: 11.3419\n",
      "Training Epoch 8  43.9% | batch:       301 of       686\t|\tloss: 14.5815\n",
      "Training Epoch 8  44.0% | batch:       302 of       686\t|\tloss: 13.6396\n",
      "Training Epoch 8  44.2% | batch:       303 of       686\t|\tloss: 12.2787\n",
      "Training Epoch 8  44.3% | batch:       304 of       686\t|\tloss: 13.8774\n",
      "Training Epoch 8  44.5% | batch:       305 of       686\t|\tloss: 10.5234\n",
      "Training Epoch 8  44.6% | batch:       306 of       686\t|\tloss: 13.1556\n",
      "Training Epoch 8  44.8% | batch:       307 of       686\t|\tloss: 11.7591\n",
      "Training Epoch 8  44.9% | batch:       308 of       686\t|\tloss: 10.7009\n",
      "Training Epoch 8  45.0% | batch:       309 of       686\t|\tloss: 11.8213\n",
      "Training Epoch 8  45.2% | batch:       310 of       686\t|\tloss: 13.1725\n",
      "Training Epoch 8  45.3% | batch:       311 of       686\t|\tloss: 10.7312\n",
      "Training Epoch 8  45.5% | batch:       312 of       686\t|\tloss: 14.1252\n",
      "Training Epoch 8  45.6% | batch:       313 of       686\t|\tloss: 11.4805\n",
      "Training Epoch 8  45.8% | batch:       314 of       686\t|\tloss: 13.097\n",
      "Training Epoch 8  45.9% | batch:       315 of       686\t|\tloss: 9.01684\n",
      "Training Epoch 8  46.1% | batch:       316 of       686\t|\tloss: 12.8383\n",
      "Training Epoch 8  46.2% | batch:       317 of       686\t|\tloss: 9.30829\n",
      "Training Epoch 8  46.4% | batch:       318 of       686\t|\tloss: 12.4357\n",
      "Training Epoch 8  46.5% | batch:       319 of       686\t|\tloss: 9.82783\n",
      "Training Epoch 8  46.6% | batch:       320 of       686\t|\tloss: 10.2933\n",
      "Training Epoch 8  46.8% | batch:       321 of       686\t|\tloss: 14.5659\n",
      "Training Epoch 8  46.9% | batch:       322 of       686\t|\tloss: 12.2255\n",
      "Training Epoch 8  47.1% | batch:       323 of       686\t|\tloss: 10.2867\n",
      "Training Epoch 8  47.2% | batch:       324 of       686\t|\tloss: 10.4532\n",
      "Training Epoch 8  47.4% | batch:       325 of       686\t|\tloss: 12.5269\n",
      "Training Epoch 8  47.5% | batch:       326 of       686\t|\tloss: 13.255\n",
      "Training Epoch 8  47.7% | batch:       327 of       686\t|\tloss: 10.9494\n",
      "Training Epoch 8  47.8% | batch:       328 of       686\t|\tloss: 14.7063\n",
      "Training Epoch 8  48.0% | batch:       329 of       686\t|\tloss: 11.5451\n",
      "Training Epoch 8  48.1% | batch:       330 of       686\t|\tloss: 14.8761\n",
      "Training Epoch 8  48.3% | batch:       331 of       686\t|\tloss: 11.5731\n",
      "Training Epoch 8  48.4% | batch:       332 of       686\t|\tloss: 11.5478\n",
      "Training Epoch 8  48.5% | batch:       333 of       686\t|\tloss: 11.6352\n",
      "Training Epoch 8  48.7% | batch:       334 of       686\t|\tloss: 14.177\n",
      "Training Epoch 8  48.8% | batch:       335 of       686\t|\tloss: 13.4702\n",
      "Training Epoch 8  49.0% | batch:       336 of       686\t|\tloss: 10.9306\n",
      "Training Epoch 8  49.1% | batch:       337 of       686\t|\tloss: 11.6324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  49.3% | batch:       338 of       686\t|\tloss: 17.2176\n",
      "Training Epoch 8  49.4% | batch:       339 of       686\t|\tloss: 13.8477\n",
      "Training Epoch 8  49.6% | batch:       340 of       686\t|\tloss: 10.8964\n",
      "Training Epoch 8  49.7% | batch:       341 of       686\t|\tloss: 10.6082\n",
      "Training Epoch 8  49.9% | batch:       342 of       686\t|\tloss: 10.0433\n",
      "Training Epoch 8  50.0% | batch:       343 of       686\t|\tloss: 13.8936\n",
      "Training Epoch 8  50.1% | batch:       344 of       686\t|\tloss: 14.6003\n",
      "Training Epoch 8  50.3% | batch:       345 of       686\t|\tloss: 9.72387\n",
      "Training Epoch 8  50.4% | batch:       346 of       686\t|\tloss: 10.2874\n",
      "Training Epoch 8  50.6% | batch:       347 of       686\t|\tloss: 11.5205\n",
      "Training Epoch 8  50.7% | batch:       348 of       686\t|\tloss: 12.6417\n",
      "Training Epoch 8  50.9% | batch:       349 of       686\t|\tloss: 8.54464\n",
      "Training Epoch 8  51.0% | batch:       350 of       686\t|\tloss: 11.9126\n",
      "Training Epoch 8  51.2% | batch:       351 of       686\t|\tloss: 9.74481\n",
      "Training Epoch 8  51.3% | batch:       352 of       686\t|\tloss: 9.58874\n",
      "Training Epoch 8  51.5% | batch:       353 of       686\t|\tloss: 12.0633\n",
      "Training Epoch 8  51.6% | batch:       354 of       686\t|\tloss: 11.8325\n",
      "Training Epoch 8  51.7% | batch:       355 of       686\t|\tloss: 10.5672\n",
      "Training Epoch 8  51.9% | batch:       356 of       686\t|\tloss: 10.6135\n",
      "Training Epoch 8  52.0% | batch:       357 of       686\t|\tloss: 11.3538\n",
      "Training Epoch 8  52.2% | batch:       358 of       686\t|\tloss: 10.2163\n",
      "Training Epoch 8  52.3% | batch:       359 of       686\t|\tloss: 12.1893\n",
      "Training Epoch 8  52.5% | batch:       360 of       686\t|\tloss: 12.6519\n",
      "Training Epoch 8  52.6% | batch:       361 of       686\t|\tloss: 11.2142\n",
      "Training Epoch 8  52.8% | batch:       362 of       686\t|\tloss: 13.0522\n",
      "Training Epoch 8  52.9% | batch:       363 of       686\t|\tloss: 8.96897\n",
      "Training Epoch 8  53.1% | batch:       364 of       686\t|\tloss: 13.6428\n",
      "Training Epoch 8  53.2% | batch:       365 of       686\t|\tloss: 11.2012\n",
      "Training Epoch 8  53.4% | batch:       366 of       686\t|\tloss: 11.9872\n",
      "Training Epoch 8  53.5% | batch:       367 of       686\t|\tloss: 9.62656\n",
      "Training Epoch 8  53.6% | batch:       368 of       686\t|\tloss: 10.9985\n",
      "Training Epoch 8  53.8% | batch:       369 of       686\t|\tloss: 12.0177\n",
      "Training Epoch 8  53.9% | batch:       370 of       686\t|\tloss: 8.44467\n",
      "Training Epoch 8  54.1% | batch:       371 of       686\t|\tloss: 11.6264\n",
      "Training Epoch 8  54.2% | batch:       372 of       686\t|\tloss: 16.0006\n",
      "Training Epoch 8  54.4% | batch:       373 of       686\t|\tloss: 10.4689\n",
      "Training Epoch 8  54.5% | batch:       374 of       686\t|\tloss: 9.20028\n",
      "Training Epoch 8  54.7% | batch:       375 of       686\t|\tloss: 9.96582\n",
      "Training Epoch 8  54.8% | batch:       376 of       686\t|\tloss: 10.2224\n",
      "Training Epoch 8  55.0% | batch:       377 of       686\t|\tloss: 14.4287\n",
      "Training Epoch 8  55.1% | batch:       378 of       686\t|\tloss: 11.2649\n",
      "Training Epoch 8  55.2% | batch:       379 of       686\t|\tloss: 12.9588\n",
      "Training Epoch 8  55.4% | batch:       380 of       686\t|\tloss: 9.257\n",
      "Training Epoch 8  55.5% | batch:       381 of       686\t|\tloss: 14.2838\n",
      "Training Epoch 8  55.7% | batch:       382 of       686\t|\tloss: 10.9526\n",
      "Training Epoch 8  55.8% | batch:       383 of       686\t|\tloss: 11.4361\n",
      "Training Epoch 8  56.0% | batch:       384 of       686\t|\tloss: 11.6651\n",
      "Training Epoch 8  56.1% | batch:       385 of       686\t|\tloss: 9.75348\n",
      "Training Epoch 8  56.3% | batch:       386 of       686\t|\tloss: 10.8153\n",
      "Training Epoch 8  56.4% | batch:       387 of       686\t|\tloss: 13.095\n",
      "Training Epoch 8  56.6% | batch:       388 of       686\t|\tloss: 13.3228\n",
      "Training Epoch 8  56.7% | batch:       389 of       686\t|\tloss: 12.8858\n",
      "Training Epoch 8  56.9% | batch:       390 of       686\t|\tloss: 10.7964\n",
      "Training Epoch 8  57.0% | batch:       391 of       686\t|\tloss: 9.52107\n",
      "Training Epoch 8  57.1% | batch:       392 of       686\t|\tloss: 8.93349\n",
      "Training Epoch 8  57.3% | batch:       393 of       686\t|\tloss: 10.2374\n",
      "Training Epoch 8  57.4% | batch:       394 of       686\t|\tloss: 10.9197\n",
      "Training Epoch 8  57.6% | batch:       395 of       686\t|\tloss: 11.7677\n",
      "Training Epoch 8  57.7% | batch:       396 of       686\t|\tloss: 11.4682\n",
      "Training Epoch 8  57.9% | batch:       397 of       686\t|\tloss: 11.298\n",
      "Training Epoch 8  58.0% | batch:       398 of       686\t|\tloss: 11.7518\n",
      "Training Epoch 8  58.2% | batch:       399 of       686\t|\tloss: 10.7654\n",
      "Training Epoch 8  58.3% | batch:       400 of       686\t|\tloss: 9.7127\n",
      "Training Epoch 8  58.5% | batch:       401 of       686\t|\tloss: 11.9313\n",
      "Training Epoch 8  58.6% | batch:       402 of       686\t|\tloss: 9.15961\n",
      "Training Epoch 8  58.7% | batch:       403 of       686\t|\tloss: 11.0624\n",
      "Training Epoch 8  58.9% | batch:       404 of       686\t|\tloss: 11.2261\n",
      "Training Epoch 8  59.0% | batch:       405 of       686\t|\tloss: 12.3401\n",
      "Training Epoch 8  59.2% | batch:       406 of       686\t|\tloss: 14.0101\n",
      "Training Epoch 8  59.3% | batch:       407 of       686\t|\tloss: 9.87408\n",
      "Training Epoch 8  59.5% | batch:       408 of       686\t|\tloss: 16.3201\n",
      "Training Epoch 8  59.6% | batch:       409 of       686\t|\tloss: 8.87039\n",
      "Training Epoch 8  59.8% | batch:       410 of       686\t|\tloss: 11.4188\n",
      "Training Epoch 8  59.9% | batch:       411 of       686\t|\tloss: 12.7992\n",
      "Training Epoch 8  60.1% | batch:       412 of       686\t|\tloss: 12.7028\n",
      "Training Epoch 8  60.2% | batch:       413 of       686\t|\tloss: 11.6403\n",
      "Training Epoch 8  60.3% | batch:       414 of       686\t|\tloss: 12.9914\n",
      "Training Epoch 8  60.5% | batch:       415 of       686\t|\tloss: 11.9001\n",
      "Training Epoch 8  60.6% | batch:       416 of       686\t|\tloss: 10.0346\n",
      "Training Epoch 8  60.8% | batch:       417 of       686\t|\tloss: 10.9627\n",
      "Training Epoch 8  60.9% | batch:       418 of       686\t|\tloss: 11.8081\n",
      "Training Epoch 8  61.1% | batch:       419 of       686\t|\tloss: 11.9613\n",
      "Training Epoch 8  61.2% | batch:       420 of       686\t|\tloss: 13.3612\n",
      "Training Epoch 8  61.4% | batch:       421 of       686\t|\tloss: 11.1829\n",
      "Training Epoch 8  61.5% | batch:       422 of       686\t|\tloss: 14.4051\n",
      "Training Epoch 8  61.7% | batch:       423 of       686\t|\tloss: 9.98906\n",
      "Training Epoch 8  61.8% | batch:       424 of       686\t|\tloss: 11.9143\n",
      "Training Epoch 8  62.0% | batch:       425 of       686\t|\tloss: 11.7025\n",
      "Training Epoch 8  62.1% | batch:       426 of       686\t|\tloss: 8.46631\n",
      "Training Epoch 8  62.2% | batch:       427 of       686\t|\tloss: 12.6192\n",
      "Training Epoch 8  62.4% | batch:       428 of       686\t|\tloss: 9.64262\n",
      "Training Epoch 8  62.5% | batch:       429 of       686\t|\tloss: 11.0376\n",
      "Training Epoch 8  62.7% | batch:       430 of       686\t|\tloss: 8.99611\n",
      "Training Epoch 8  62.8% | batch:       431 of       686\t|\tloss: 10.8612\n",
      "Training Epoch 8  63.0% | batch:       432 of       686\t|\tloss: 11.7975\n",
      "Training Epoch 8  63.1% | batch:       433 of       686\t|\tloss: 10.2459\n",
      "Training Epoch 8  63.3% | batch:       434 of       686\t|\tloss: 9.95896\n",
      "Training Epoch 8  63.4% | batch:       435 of       686\t|\tloss: 14.2297\n",
      "Training Epoch 8  63.6% | batch:       436 of       686\t|\tloss: 12.2384\n",
      "Training Epoch 8  63.7% | batch:       437 of       686\t|\tloss: 12.9892\n",
      "Training Epoch 8  63.8% | batch:       438 of       686\t|\tloss: 11.5222\n",
      "Training Epoch 8  64.0% | batch:       439 of       686\t|\tloss: 10.962\n",
      "Training Epoch 8  64.1% | batch:       440 of       686\t|\tloss: 20.616\n",
      "Training Epoch 8  64.3% | batch:       441 of       686\t|\tloss: 11.1953\n",
      "Training Epoch 8  64.4% | batch:       442 of       686\t|\tloss: 12.7588\n",
      "Training Epoch 8  64.6% | batch:       443 of       686\t|\tloss: 9.75863\n",
      "Training Epoch 8  64.7% | batch:       444 of       686\t|\tloss: 10.803\n",
      "Training Epoch 8  64.9% | batch:       445 of       686\t|\tloss: 9.07203\n",
      "Training Epoch 8  65.0% | batch:       446 of       686\t|\tloss: 10.2554\n",
      "Training Epoch 8  65.2% | batch:       447 of       686\t|\tloss: 9.8097\n",
      "Training Epoch 8  65.3% | batch:       448 of       686\t|\tloss: 14.885\n",
      "Training Epoch 8  65.5% | batch:       449 of       686\t|\tloss: 13.0896\n",
      "Training Epoch 8  65.6% | batch:       450 of       686\t|\tloss: 13.6282\n",
      "Training Epoch 8  65.7% | batch:       451 of       686\t|\tloss: 10.8496\n",
      "Training Epoch 8  65.9% | batch:       452 of       686\t|\tloss: 11.8618\n",
      "Training Epoch 8  66.0% | batch:       453 of       686\t|\tloss: 13.8925\n",
      "Training Epoch 8  66.2% | batch:       454 of       686\t|\tloss: 10.8291\n",
      "Training Epoch 8  66.3% | batch:       455 of       686\t|\tloss: 13.9021\n",
      "Training Epoch 8  66.5% | batch:       456 of       686\t|\tloss: 14.0102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  66.6% | batch:       457 of       686\t|\tloss: 10.4242\n",
      "Training Epoch 8  66.8% | batch:       458 of       686\t|\tloss: 11.124\n",
      "Training Epoch 8  66.9% | batch:       459 of       686\t|\tloss: 13.1349\n",
      "Training Epoch 8  67.1% | batch:       460 of       686\t|\tloss: 12.883\n",
      "Training Epoch 8  67.2% | batch:       461 of       686\t|\tloss: 12.7768\n",
      "Training Epoch 8  67.3% | batch:       462 of       686\t|\tloss: 11.1412\n",
      "Training Epoch 8  67.5% | batch:       463 of       686\t|\tloss: 10.6129\n",
      "Training Epoch 8  67.6% | batch:       464 of       686\t|\tloss: 10.7902\n",
      "Training Epoch 8  67.8% | batch:       465 of       686\t|\tloss: 12.7066\n",
      "Training Epoch 8  67.9% | batch:       466 of       686\t|\tloss: 8.22557\n",
      "Training Epoch 8  68.1% | batch:       467 of       686\t|\tloss: 12.6794\n",
      "Training Epoch 8  68.2% | batch:       468 of       686\t|\tloss: 11.1871\n",
      "Training Epoch 8  68.4% | batch:       469 of       686\t|\tloss: 9.8025\n",
      "Training Epoch 8  68.5% | batch:       470 of       686\t|\tloss: 7.67027\n",
      "Training Epoch 8  68.7% | batch:       471 of       686\t|\tloss: 14.0728\n",
      "Training Epoch 8  68.8% | batch:       472 of       686\t|\tloss: 7.40095\n",
      "Training Epoch 8  69.0% | batch:       473 of       686\t|\tloss: 12.235\n",
      "Training Epoch 8  69.1% | batch:       474 of       686\t|\tloss: 13.0828\n",
      "Training Epoch 8  69.2% | batch:       475 of       686\t|\tloss: 9.78675\n",
      "Training Epoch 8  69.4% | batch:       476 of       686\t|\tloss: 8.59639\n",
      "Training Epoch 8  69.5% | batch:       477 of       686\t|\tloss: 11.2503\n",
      "Training Epoch 8  69.7% | batch:       478 of       686\t|\tloss: 14.4271\n",
      "Training Epoch 8  69.8% | batch:       479 of       686\t|\tloss: 13.0166\n",
      "Training Epoch 8  70.0% | batch:       480 of       686\t|\tloss: 12.9094\n",
      "Training Epoch 8  70.1% | batch:       481 of       686\t|\tloss: 11.9594\n",
      "Training Epoch 8  70.3% | batch:       482 of       686\t|\tloss: 13.1906\n",
      "Training Epoch 8  70.4% | batch:       483 of       686\t|\tloss: 10.3308\n",
      "Training Epoch 8  70.6% | batch:       484 of       686\t|\tloss: 12.1476\n",
      "Training Epoch 8  70.7% | batch:       485 of       686\t|\tloss: 14.5877\n",
      "Training Epoch 8  70.8% | batch:       486 of       686\t|\tloss: 10.2374\n",
      "Training Epoch 8  71.0% | batch:       487 of       686\t|\tloss: 8.96364\n",
      "Training Epoch 8  71.1% | batch:       488 of       686\t|\tloss: 10.8018\n",
      "Training Epoch 8  71.3% | batch:       489 of       686\t|\tloss: 10.165\n",
      "Training Epoch 8  71.4% | batch:       490 of       686\t|\tloss: 11.1461\n",
      "Training Epoch 8  71.6% | batch:       491 of       686\t|\tloss: 9.52735\n",
      "Training Epoch 8  71.7% | batch:       492 of       686\t|\tloss: 12.568\n",
      "Training Epoch 8  71.9% | batch:       493 of       686\t|\tloss: 11.3867\n",
      "Training Epoch 8  72.0% | batch:       494 of       686\t|\tloss: 11.8394\n",
      "Training Epoch 8  72.2% | batch:       495 of       686\t|\tloss: 11.6869\n",
      "Training Epoch 8  72.3% | batch:       496 of       686\t|\tloss: 12.2401\n",
      "Training Epoch 8  72.4% | batch:       497 of       686\t|\tloss: 13.8447\n",
      "Training Epoch 8  72.6% | batch:       498 of       686\t|\tloss: 13.9909\n",
      "Training Epoch 8  72.7% | batch:       499 of       686\t|\tloss: 11.1645\n",
      "Training Epoch 8  72.9% | batch:       500 of       686\t|\tloss: 10.3859\n",
      "Training Epoch 8  73.0% | batch:       501 of       686\t|\tloss: 20.1065\n",
      "Training Epoch 8  73.2% | batch:       502 of       686\t|\tloss: 9.88015\n",
      "Training Epoch 8  73.3% | batch:       503 of       686\t|\tloss: 11.7035\n",
      "Training Epoch 8  73.5% | batch:       504 of       686\t|\tloss: 11.4906\n",
      "Training Epoch 8  73.6% | batch:       505 of       686\t|\tloss: 12.8577\n",
      "Training Epoch 8  73.8% | batch:       506 of       686\t|\tloss: 12.9521\n",
      "Training Epoch 8  73.9% | batch:       507 of       686\t|\tloss: 6.94756\n",
      "Training Epoch 8  74.1% | batch:       508 of       686\t|\tloss: 13.0552\n",
      "Training Epoch 8  74.2% | batch:       509 of       686\t|\tloss: 11.2147\n",
      "Training Epoch 8  74.3% | batch:       510 of       686\t|\tloss: 9.80924\n",
      "Training Epoch 8  74.5% | batch:       511 of       686\t|\tloss: 11.6613\n",
      "Training Epoch 8  74.6% | batch:       512 of       686\t|\tloss: 9.28479\n",
      "Training Epoch 8  74.8% | batch:       513 of       686\t|\tloss: 12.9741\n",
      "Training Epoch 8  74.9% | batch:       514 of       686\t|\tloss: 10.4902\n",
      "Training Epoch 8  75.1% | batch:       515 of       686\t|\tloss: 11.9825\n",
      "Training Epoch 8  75.2% | batch:       516 of       686\t|\tloss: 10.8504\n",
      "Training Epoch 8  75.4% | batch:       517 of       686\t|\tloss: 10.9496\n",
      "Training Epoch 8  75.5% | batch:       518 of       686\t|\tloss: 8.51147\n",
      "Training Epoch 8  75.7% | batch:       519 of       686\t|\tloss: 10.0368\n",
      "Training Epoch 8  75.8% | batch:       520 of       686\t|\tloss: 11.9541\n",
      "Training Epoch 8  75.9% | batch:       521 of       686\t|\tloss: 15.6064\n",
      "Training Epoch 8  76.1% | batch:       522 of       686\t|\tloss: 13.2245\n",
      "Training Epoch 8  76.2% | batch:       523 of       686\t|\tloss: 10.725\n",
      "Training Epoch 8  76.4% | batch:       524 of       686\t|\tloss: 11.4414\n",
      "Training Epoch 8  76.5% | batch:       525 of       686\t|\tloss: 7.7878\n",
      "Training Epoch 8  76.7% | batch:       526 of       686\t|\tloss: 11.2794\n",
      "Training Epoch 8  76.8% | batch:       527 of       686\t|\tloss: 8.50918\n",
      "Training Epoch 8  77.0% | batch:       528 of       686\t|\tloss: 13.0718\n",
      "Training Epoch 8  77.1% | batch:       529 of       686\t|\tloss: 11.5009\n",
      "Training Epoch 8  77.3% | batch:       530 of       686\t|\tloss: 13.9231\n",
      "Training Epoch 8  77.4% | batch:       531 of       686\t|\tloss: 13.3896\n",
      "Training Epoch 8  77.6% | batch:       532 of       686\t|\tloss: 9.11526\n",
      "Training Epoch 8  77.7% | batch:       533 of       686\t|\tloss: 9.50842\n",
      "Training Epoch 8  77.8% | batch:       534 of       686\t|\tloss: 8.57041\n",
      "Training Epoch 8  78.0% | batch:       535 of       686\t|\tloss: 14.4557\n",
      "Training Epoch 8  78.1% | batch:       536 of       686\t|\tloss: 16.0404\n",
      "Training Epoch 8  78.3% | batch:       537 of       686\t|\tloss: 11.7001\n",
      "Training Epoch 8  78.4% | batch:       538 of       686\t|\tloss: 12.5155\n",
      "Training Epoch 8  78.6% | batch:       539 of       686\t|\tloss: 7.55903\n",
      "Training Epoch 8  78.7% | batch:       540 of       686\t|\tloss: 14.4352\n",
      "Training Epoch 8  78.9% | batch:       541 of       686\t|\tloss: 10.2508\n",
      "Training Epoch 8  79.0% | batch:       542 of       686\t|\tloss: 12.428\n",
      "Training Epoch 8  79.2% | batch:       543 of       686\t|\tloss: 9.25834\n",
      "Training Epoch 8  79.3% | batch:       544 of       686\t|\tloss: 16.8367\n",
      "Training Epoch 8  79.4% | batch:       545 of       686\t|\tloss: 11.3514\n",
      "Training Epoch 8  79.6% | batch:       546 of       686\t|\tloss: 10.1696\n",
      "Training Epoch 8  79.7% | batch:       547 of       686\t|\tloss: 12.4565\n",
      "Training Epoch 8  79.9% | batch:       548 of       686\t|\tloss: 12.4194\n",
      "Training Epoch 8  80.0% | batch:       549 of       686\t|\tloss: 12.4249\n",
      "Training Epoch 8  80.2% | batch:       550 of       686\t|\tloss: 8.08583\n",
      "Training Epoch 8  80.3% | batch:       551 of       686\t|\tloss: 10.3758\n",
      "Training Epoch 8  80.5% | batch:       552 of       686\t|\tloss: 9.76793\n",
      "Training Epoch 8  80.6% | batch:       553 of       686\t|\tloss: 13.4056\n",
      "Training Epoch 8  80.8% | batch:       554 of       686\t|\tloss: 9.20092\n",
      "Training Epoch 8  80.9% | batch:       555 of       686\t|\tloss: 9.49669\n",
      "Training Epoch 8  81.0% | batch:       556 of       686\t|\tloss: 9.46848\n",
      "Training Epoch 8  81.2% | batch:       557 of       686\t|\tloss: 12.5745\n",
      "Training Epoch 8  81.3% | batch:       558 of       686\t|\tloss: 13.2398\n",
      "Training Epoch 8  81.5% | batch:       559 of       686\t|\tloss: 10.3314\n",
      "Training Epoch 8  81.6% | batch:       560 of       686\t|\tloss: 9.98359\n",
      "Training Epoch 8  81.8% | batch:       561 of       686\t|\tloss: 12.3231\n",
      "Training Epoch 8  81.9% | batch:       562 of       686\t|\tloss: 12.023\n",
      "Training Epoch 8  82.1% | batch:       563 of       686\t|\tloss: 10.685\n",
      "Training Epoch 8  82.2% | batch:       564 of       686\t|\tloss: 10.5099\n",
      "Training Epoch 8  82.4% | batch:       565 of       686\t|\tloss: 16.9742\n",
      "Training Epoch 8  82.5% | batch:       566 of       686\t|\tloss: 15.7829\n",
      "Training Epoch 8  82.7% | batch:       567 of       686\t|\tloss: 13.7963\n",
      "Training Epoch 8  82.8% | batch:       568 of       686\t|\tloss: 8.25564\n",
      "Training Epoch 8  82.9% | batch:       569 of       686\t|\tloss: 13.9661\n",
      "Training Epoch 8  83.1% | batch:       570 of       686\t|\tloss: 12.5813\n",
      "Training Epoch 8  83.2% | batch:       571 of       686\t|\tloss: 10.6183\n",
      "Training Epoch 8  83.4% | batch:       572 of       686\t|\tloss: 9.92445\n",
      "Training Epoch 8  83.5% | batch:       573 of       686\t|\tloss: 8.23719\n",
      "Training Epoch 8  83.7% | batch:       574 of       686\t|\tloss: 12.9867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  83.8% | batch:       575 of       686\t|\tloss: 13.189\n",
      "Training Epoch 8  84.0% | batch:       576 of       686\t|\tloss: 10.726\n",
      "Training Epoch 8  84.1% | batch:       577 of       686\t|\tloss: 10.9164\n",
      "Training Epoch 8  84.3% | batch:       578 of       686\t|\tloss: 10.3376\n",
      "Training Epoch 8  84.4% | batch:       579 of       686\t|\tloss: 11.755\n",
      "Training Epoch 8  84.5% | batch:       580 of       686\t|\tloss: 9.68157\n",
      "Training Epoch 8  84.7% | batch:       581 of       686\t|\tloss: 11.7921\n",
      "Training Epoch 8  84.8% | batch:       582 of       686\t|\tloss: 13.2497\n",
      "Training Epoch 8  85.0% | batch:       583 of       686\t|\tloss: 9.55506\n",
      "Training Epoch 8  85.1% | batch:       584 of       686\t|\tloss: 9.61665\n",
      "Training Epoch 8  85.3% | batch:       585 of       686\t|\tloss: 14.1623\n",
      "Training Epoch 8  85.4% | batch:       586 of       686\t|\tloss: 11.4503\n",
      "Training Epoch 8  85.6% | batch:       587 of       686\t|\tloss: 9.47154\n",
      "Training Epoch 8  85.7% | batch:       588 of       686\t|\tloss: 8.36973\n",
      "Training Epoch 8  85.9% | batch:       589 of       686\t|\tloss: 13.7484\n",
      "Training Epoch 8  86.0% | batch:       590 of       686\t|\tloss: 10.1568\n",
      "Training Epoch 8  86.2% | batch:       591 of       686\t|\tloss: 13.0923\n",
      "Training Epoch 8  86.3% | batch:       592 of       686\t|\tloss: 13.2667\n",
      "Training Epoch 8  86.4% | batch:       593 of       686\t|\tloss: 9.78432\n",
      "Training Epoch 8  86.6% | batch:       594 of       686\t|\tloss: 10.0912\n",
      "Training Epoch 8  86.7% | batch:       595 of       686\t|\tloss: 10.9546\n",
      "Training Epoch 8  86.9% | batch:       596 of       686\t|\tloss: 8.95858\n",
      "Training Epoch 8  87.0% | batch:       597 of       686\t|\tloss: 14.2442\n",
      "Training Epoch 8  87.2% | batch:       598 of       686\t|\tloss: 14.6319\n",
      "Training Epoch 8  87.3% | batch:       599 of       686\t|\tloss: 9.81857\n",
      "Training Epoch 8  87.5% | batch:       600 of       686\t|\tloss: 15.9693\n",
      "Training Epoch 8  87.6% | batch:       601 of       686\t|\tloss: 8.9663\n",
      "Training Epoch 8  87.8% | batch:       602 of       686\t|\tloss: 12.7755\n",
      "Training Epoch 8  87.9% | batch:       603 of       686\t|\tloss: 14.1511\n",
      "Training Epoch 8  88.0% | batch:       604 of       686\t|\tloss: 12.4963\n",
      "Training Epoch 8  88.2% | batch:       605 of       686\t|\tloss: 10.4548\n",
      "Training Epoch 8  88.3% | batch:       606 of       686\t|\tloss: 10.0501\n",
      "Training Epoch 8  88.5% | batch:       607 of       686\t|\tloss: 9.81538\n",
      "Training Epoch 8  88.6% | batch:       608 of       686\t|\tloss: 14.2012\n",
      "Training Epoch 8  88.8% | batch:       609 of       686\t|\tloss: 9.92781\n",
      "Training Epoch 8  88.9% | batch:       610 of       686\t|\tloss: 8.97412\n",
      "Training Epoch 8  89.1% | batch:       611 of       686\t|\tloss: 10.1113\n",
      "Training Epoch 8  89.2% | batch:       612 of       686\t|\tloss: 9.0013\n",
      "Training Epoch 8  89.4% | batch:       613 of       686\t|\tloss: 12.1711\n",
      "Training Epoch 8  89.5% | batch:       614 of       686\t|\tloss: 11.3801\n",
      "Training Epoch 8  89.7% | batch:       615 of       686\t|\tloss: 8.79301\n",
      "Training Epoch 8  89.8% | batch:       616 of       686\t|\tloss: 12.6967\n",
      "Training Epoch 8  89.9% | batch:       617 of       686\t|\tloss: 10.1827\n",
      "Training Epoch 8  90.1% | batch:       618 of       686\t|\tloss: 10.2394\n",
      "Training Epoch 8  90.2% | batch:       619 of       686\t|\tloss: 9.84391\n",
      "Training Epoch 8  90.4% | batch:       620 of       686\t|\tloss: 11.086\n",
      "Training Epoch 8  90.5% | batch:       621 of       686\t|\tloss: 12.1337\n",
      "Training Epoch 8  90.7% | batch:       622 of       686\t|\tloss: 8.93597\n",
      "Training Epoch 8  90.8% | batch:       623 of       686\t|\tloss: 10.3612\n",
      "Training Epoch 8  91.0% | batch:       624 of       686\t|\tloss: 12.0927\n",
      "Training Epoch 8  91.1% | batch:       625 of       686\t|\tloss: 7.89785\n",
      "Training Epoch 8  91.3% | batch:       626 of       686\t|\tloss: 9.51378\n",
      "Training Epoch 8  91.4% | batch:       627 of       686\t|\tloss: 8.85509\n",
      "Training Epoch 8  91.5% | batch:       628 of       686\t|\tloss: 8.83621\n",
      "Training Epoch 8  91.7% | batch:       629 of       686\t|\tloss: 9.63487\n",
      "Training Epoch 8  91.8% | batch:       630 of       686\t|\tloss: 7.7249\n",
      "Training Epoch 8  92.0% | batch:       631 of       686\t|\tloss: 9.16234\n",
      "Training Epoch 8  92.1% | batch:       632 of       686\t|\tloss: 11.8794\n",
      "Training Epoch 8  92.3% | batch:       633 of       686\t|\tloss: 10.1728\n",
      "Training Epoch 8  92.4% | batch:       634 of       686\t|\tloss: 9.89242\n",
      "Training Epoch 8  92.6% | batch:       635 of       686\t|\tloss: 9.8264\n",
      "Training Epoch 8  92.7% | batch:       636 of       686\t|\tloss: 10.3534\n",
      "Training Epoch 8  92.9% | batch:       637 of       686\t|\tloss: 11.0393\n",
      "Training Epoch 8  93.0% | batch:       638 of       686\t|\tloss: 16.1752\n",
      "Training Epoch 8  93.1% | batch:       639 of       686\t|\tloss: 10.1011\n",
      "Training Epoch 8  93.3% | batch:       640 of       686\t|\tloss: 10.0129\n",
      "Training Epoch 8  93.4% | batch:       641 of       686\t|\tloss: 9.5321\n",
      "Training Epoch 8  93.6% | batch:       642 of       686\t|\tloss: 10.1375\n",
      "Training Epoch 8  93.7% | batch:       643 of       686\t|\tloss: 8.4858\n",
      "Training Epoch 8  93.9% | batch:       644 of       686\t|\tloss: 8.68567\n",
      "Training Epoch 8  94.0% | batch:       645 of       686\t|\tloss: 11.5547\n",
      "Training Epoch 8  94.2% | batch:       646 of       686\t|\tloss: 14.1615\n",
      "Training Epoch 8  94.3% | batch:       647 of       686\t|\tloss: 11.8793\n",
      "Training Epoch 8  94.5% | batch:       648 of       686\t|\tloss: 8.81358\n",
      "Training Epoch 8  94.6% | batch:       649 of       686\t|\tloss: 8.19428\n",
      "Training Epoch 8  94.8% | batch:       650 of       686\t|\tloss: 11.8222\n",
      "Training Epoch 8  94.9% | batch:       651 of       686\t|\tloss: 11.2142\n",
      "Training Epoch 8  95.0% | batch:       652 of       686\t|\tloss: 12.6609\n",
      "Training Epoch 8  95.2% | batch:       653 of       686\t|\tloss: 11.4722\n",
      "Training Epoch 8  95.3% | batch:       654 of       686\t|\tloss: 15.4999\n",
      "Training Epoch 8  95.5% | batch:       655 of       686\t|\tloss: 13.5453\n",
      "Training Epoch 8  95.6% | batch:       656 of       686\t|\tloss: 9.21937\n",
      "Training Epoch 8  95.8% | batch:       657 of       686\t|\tloss: 10.3141\n",
      "Training Epoch 8  95.9% | batch:       658 of       686\t|\tloss: 9.96438\n",
      "Training Epoch 8  96.1% | batch:       659 of       686\t|\tloss: 11.3078\n",
      "Training Epoch 8  96.2% | batch:       660 of       686\t|\tloss: 11.949\n",
      "Training Epoch 8  96.4% | batch:       661 of       686\t|\tloss: 11.8026\n",
      "Training Epoch 8  96.5% | batch:       662 of       686\t|\tloss: 9.48605\n",
      "Training Epoch 8  96.6% | batch:       663 of       686\t|\tloss: 11.8866\n",
      "Training Epoch 8  96.8% | batch:       664 of       686\t|\tloss: 9.45228\n",
      "Training Epoch 8  96.9% | batch:       665 of       686\t|\tloss: 10.7105\n",
      "Training Epoch 8  97.1% | batch:       666 of       686\t|\tloss: 10.3791\n",
      "Training Epoch 8  97.2% | batch:       667 of       686\t|\tloss: 8.61855\n",
      "Training Epoch 8  97.4% | batch:       668 of       686\t|\tloss: 13.9379\n",
      "Training Epoch 8  97.5% | batch:       669 of       686\t|\tloss: 11.4804\n",
      "Training Epoch 8  97.7% | batch:       670 of       686\t|\tloss: 14.0539\n",
      "Training Epoch 8  97.8% | batch:       671 of       686\t|\tloss: 10.7826\n",
      "Training Epoch 8  98.0% | batch:       672 of       686\t|\tloss: 9.75766\n",
      "Training Epoch 8  98.1% | batch:       673 of       686\t|\tloss: 9.74853\n",
      "Training Epoch 8  98.3% | batch:       674 of       686\t|\tloss: 10.3564\n",
      "Training Epoch 8  98.4% | batch:       675 of       686\t|\tloss: 11.4641\n",
      "Training Epoch 8  98.5% | batch:       676 of       686\t|\tloss: 9.44787\n",
      "Training Epoch 8  98.7% | batch:       677 of       686\t|\tloss: 10.4944\n",
      "Training Epoch 8  98.8% | batch:       678 of       686\t|\tloss: 10.1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:03:29,341 | INFO : Epoch 8 Training Summary: epoch: 8.000000 | loss: 11.940597 | \n",
      "2023-05-25 22:03:29,343 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 22.633482933044434 seconds\n",
      "\n",
      "2023-05-25 22:03:29,345 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.62044981122017 seconds\n",
      "2023-05-25 22:03:29,347 | INFO : Avg batch train. time: 0.0344321425819536 seconds\n",
      "2023-05-25 22:03:29,348 | INFO : Avg sample train. time: 0.0002693477371711063 seconds\n",
      "2023-05-25 22:03:29,350 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  99.0% | batch:       679 of       686\t|\tloss: 12.2372\n",
      "Training Epoch 8  99.1% | batch:       680 of       686\t|\tloss: 9.16765\n",
      "Training Epoch 8  99.3% | batch:       681 of       686\t|\tloss: 11.9498\n",
      "Training Epoch 8  99.4% | batch:       682 of       686\t|\tloss: 11.2866\n",
      "Training Epoch 8  99.6% | batch:       683 of       686\t|\tloss: 12.5223\n",
      "Training Epoch 8  99.7% | batch:       684 of       686\t|\tloss: 10.7132\n",
      "Training Epoch 8  99.9% | batch:       685 of       686\t|\tloss: 5.77953\n",
      "\n",
      "Evaluating Epoch 8   0.0% | batch:         0 of       172\t|\tloss: 1.626\n",
      "Evaluating Epoch 8   0.6% | batch:         1 of       172\t|\tloss: 2.25941\n",
      "Evaluating Epoch 8   1.2% | batch:         2 of       172\t|\tloss: 1.61278\n",
      "Evaluating Epoch 8   1.7% | batch:         3 of       172\t|\tloss: 3.67308\n",
      "Evaluating Epoch 8   2.3% | batch:         4 of       172\t|\tloss: 1.75532\n",
      "Evaluating Epoch 8   2.9% | batch:         5 of       172\t|\tloss: 1.83867\n",
      "Evaluating Epoch 8   3.5% | batch:         6 of       172\t|\tloss: 2.11969\n",
      "Evaluating Epoch 8   4.1% | batch:         7 of       172\t|\tloss: 3.78649\n",
      "Evaluating Epoch 8   4.7% | batch:         8 of       172\t|\tloss: 1.40398\n",
      "Evaluating Epoch 8   5.2% | batch:         9 of       172\t|\tloss: 2.34075\n",
      "Evaluating Epoch 8   5.8% | batch:        10 of       172\t|\tloss: 2.74023\n",
      "Evaluating Epoch 8   6.4% | batch:        11 of       172\t|\tloss: 1.86843\n",
      "Evaluating Epoch 8   7.0% | batch:        12 of       172\t|\tloss: 1.93686\n",
      "Evaluating Epoch 8   7.6% | batch:        13 of       172\t|\tloss: 2.31381\n",
      "Evaluating Epoch 8   8.1% | batch:        14 of       172\t|\tloss: 2.89962\n",
      "Evaluating Epoch 8   8.7% | batch:        15 of       172\t|\tloss: 1.62804\n",
      "Evaluating Epoch 8   9.3% | batch:        16 of       172\t|\tloss: 2.85645\n",
      "Evaluating Epoch 8   9.9% | batch:        17 of       172\t|\tloss: 1.59515\n",
      "Evaluating Epoch 8  10.5% | batch:        18 of       172\t|\tloss: 7.74383\n",
      "Evaluating Epoch 8  11.0% | batch:        19 of       172\t|\tloss: 0.976858\n",
      "Evaluating Epoch 8  11.6% | batch:        20 of       172\t|\tloss: 3.54971\n",
      "Evaluating Epoch 8  12.2% | batch:        21 of       172\t|\tloss: 0.531034\n",
      "Evaluating Epoch 8  12.8% | batch:        22 of       172\t|\tloss: 1.03836\n",
      "Evaluating Epoch 8  13.4% | batch:        23 of       172\t|\tloss: 1.14623\n",
      "Evaluating Epoch 8  14.0% | batch:        24 of       172\t|\tloss: 2.06345\n",
      "Evaluating Epoch 8  14.5% | batch:        25 of       172\t|\tloss: 3.17147\n",
      "Evaluating Epoch 8  15.1% | batch:        26 of       172\t|\tloss: 4.70675\n",
      "Evaluating Epoch 8  15.7% | batch:        27 of       172\t|\tloss: 8.07213\n",
      "Evaluating Epoch 8  16.3% | batch:        28 of       172\t|\tloss: 0.352289\n",
      "Evaluating Epoch 8  16.9% | batch:        29 of       172\t|\tloss: 2.69001\n",
      "Evaluating Epoch 8  17.4% | batch:        30 of       172\t|\tloss: 0.871185\n",
      "Evaluating Epoch 8  18.0% | batch:        31 of       172\t|\tloss: 4.70717\n",
      "Evaluating Epoch 8  18.6% | batch:        32 of       172\t|\tloss: 0.657241\n",
      "Evaluating Epoch 8  19.2% | batch:        33 of       172\t|\tloss: 1.33589\n",
      "Evaluating Epoch 8  19.8% | batch:        34 of       172\t|\tloss: 0.607555\n",
      "Evaluating Epoch 8  20.3% | batch:        35 of       172\t|\tloss: 0.651073\n",
      "Evaluating Epoch 8  20.9% | batch:        36 of       172\t|\tloss: 4.34566\n",
      "Evaluating Epoch 8  21.5% | batch:        37 of       172\t|\tloss: 2.84897\n",
      "Evaluating Epoch 8  22.1% | batch:        38 of       172\t|\tloss: 1.73585\n",
      "Evaluating Epoch 8  22.7% | batch:        39 of       172\t|\tloss: 2.17909\n",
      "Evaluating Epoch 8  23.3% | batch:        40 of       172\t|\tloss: 0.648333\n",
      "Evaluating Epoch 8  23.8% | batch:        41 of       172\t|\tloss: 2.12769\n",
      "Evaluating Epoch 8  24.4% | batch:        42 of       172\t|\tloss: 0.500302\n",
      "Evaluating Epoch 8  25.0% | batch:        43 of       172\t|\tloss: 8.84611\n",
      "Evaluating Epoch 8  25.6% | batch:        44 of       172\t|\tloss: 1.04268\n",
      "Evaluating Epoch 8  26.2% | batch:        45 of       172\t|\tloss: 1.69146\n",
      "Evaluating Epoch 8  26.7% | batch:        46 of       172\t|\tloss: 0.325242\n",
      "Evaluating Epoch 8  27.3% | batch:        47 of       172\t|\tloss: 4.30023\n",
      "Evaluating Epoch 8  27.9% | batch:        48 of       172\t|\tloss: 0.605118\n",
      "Evaluating Epoch 8  28.5% | batch:        49 of       172\t|\tloss: 1.84692\n",
      "Evaluating Epoch 8  29.1% | batch:        50 of       172\t|\tloss: 0.56439\n",
      "Evaluating Epoch 8  29.7% | batch:        51 of       172\t|\tloss: 0.981133\n",
      "Evaluating Epoch 8  30.2% | batch:        52 of       172\t|\tloss: 3.65677\n",
      "Evaluating Epoch 8  30.8% | batch:        53 of       172\t|\tloss: 1.34301\n",
      "Evaluating Epoch 8  31.4% | batch:        54 of       172\t|\tloss: 1.63789\n",
      "Evaluating Epoch 8  32.0% | batch:        55 of       172\t|\tloss: 4.07432\n",
      "Evaluating Epoch 8  32.6% | batch:        56 of       172\t|\tloss: 2.30292\n",
      "Evaluating Epoch 8  33.1% | batch:        57 of       172\t|\tloss: 4.57535\n",
      "Evaluating Epoch 8  33.7% | batch:        58 of       172\t|\tloss: 1.1308\n",
      "Evaluating Epoch 8  34.3% | batch:        59 of       172\t|\tloss: 5.00431\n",
      "Evaluating Epoch 8  34.9% | batch:        60 of       172\t|\tloss: 0.773844\n",
      "Evaluating Epoch 8  35.5% | batch:        61 of       172\t|\tloss: 5.70047\n",
      "Evaluating Epoch 8  36.0% | batch:        62 of       172\t|\tloss: 1.13149\n",
      "Evaluating Epoch 8  36.6% | batch:        63 of       172\t|\tloss: 1.87131\n",
      "Evaluating Epoch 8  37.2% | batch:        64 of       172\t|\tloss: 3.80144\n",
      "Evaluating Epoch 8  37.8% | batch:        65 of       172\t|\tloss: 1.16922\n",
      "Evaluating Epoch 8  38.4% | batch:        66 of       172\t|\tloss: 3.97386\n",
      "Evaluating Epoch 8  39.0% | batch:        67 of       172\t|\tloss: 2.36404\n",
      "Evaluating Epoch 8  39.5% | batch:        68 of       172\t|\tloss: 2.33161\n",
      "Evaluating Epoch 8  40.1% | batch:        69 of       172\t|\tloss: 6.50629\n",
      "Evaluating Epoch 8  40.7% | batch:        70 of       172\t|\tloss: 0.662938\n",
      "Evaluating Epoch 8  41.3% | batch:        71 of       172\t|\tloss: 1.84985\n",
      "Evaluating Epoch 8  41.9% | batch:        72 of       172\t|\tloss: 3.36399\n",
      "Evaluating Epoch 8  42.4% | batch:        73 of       172\t|\tloss: 1.33724\n",
      "Evaluating Epoch 8  43.0% | batch:        74 of       172\t|\tloss: 0.494221\n",
      "Evaluating Epoch 8  43.6% | batch:        75 of       172\t|\tloss: 0.639436\n",
      "Evaluating Epoch 8  44.2% | batch:        76 of       172\t|\tloss: 0.755137\n",
      "Evaluating Epoch 8  44.8% | batch:        77 of       172\t|\tloss: 0.649872\n",
      "Evaluating Epoch 8  45.3% | batch:        78 of       172\t|\tloss: 0.463975\n",
      "Evaluating Epoch 8  45.9% | batch:        79 of       172\t|\tloss: 0.573568\n",
      "Evaluating Epoch 8  46.5% | batch:        80 of       172\t|\tloss: 0.8918\n",
      "Evaluating Epoch 8  47.1% | batch:        81 of       172\t|\tloss: 0.653548\n",
      "Evaluating Epoch 8  47.7% | batch:        82 of       172\t|\tloss: 0.681088\n",
      "Evaluating Epoch 8  48.3% | batch:        83 of       172\t|\tloss: 1.45129\n",
      "Evaluating Epoch 8  48.8% | batch:        84 of       172\t|\tloss: 2.03836\n",
      "Evaluating Epoch 8  49.4% | batch:        85 of       172\t|\tloss: 2.65278\n",
      "Evaluating Epoch 8  50.0% | batch:        86 of       172\t|\tloss: 1.94179\n",
      "Evaluating Epoch 8  50.6% | batch:        87 of       172\t|\tloss: 1.69598\n",
      "Evaluating Epoch 8  51.2% | batch:        88 of       172\t|\tloss: 2.53594\n",
      "Evaluating Epoch 8  51.7% | batch:        89 of       172\t|\tloss: 3.65133\n",
      "Evaluating Epoch 8  52.3% | batch:        90 of       172\t|\tloss: 1.89438\n",
      "Evaluating Epoch 8  52.9% | batch:        91 of       172\t|\tloss: 3.18081\n",
      "Evaluating Epoch 8  53.5% | batch:        92 of       172\t|\tloss: 3.62383\n",
      "Evaluating Epoch 8  54.1% | batch:        93 of       172\t|\tloss: 2.46448\n",
      "Evaluating Epoch 8  54.7% | batch:        94 of       172\t|\tloss: 1.43028\n",
      "Evaluating Epoch 8  55.2% | batch:        95 of       172\t|\tloss: 2.95072\n",
      "Evaluating Epoch 8  55.8% | batch:        96 of       172\t|\tloss: 3.40016\n",
      "Evaluating Epoch 8  56.4% | batch:        97 of       172\t|\tloss: 1.95529\n",
      "Evaluating Epoch 8  57.0% | batch:        98 of       172\t|\tloss: 2.44301\n",
      "Evaluating Epoch 8  57.6% | batch:        99 of       172\t|\tloss: 4.07776\n",
      "Evaluating Epoch 8  58.1% | batch:       100 of       172\t|\tloss: 1.32909\n",
      "Evaluating Epoch 8  58.7% | batch:       101 of       172\t|\tloss: 1.95621\n",
      "Evaluating Epoch 8  59.3% | batch:       102 of       172\t|\tloss: 3.16833\n",
      "Evaluating Epoch 8  59.9% | batch:       103 of       172\t|\tloss: 3.12895\n",
      "Evaluating Epoch 8  60.5% | batch:       104 of       172\t|\tloss: 1.66226\n",
      "Evaluating Epoch 8  61.0% | batch:       105 of       172\t|\tloss: 2.28966\n",
      "Evaluating Epoch 8  61.6% | batch:       106 of       172\t|\tloss: 4.07099\n",
      "Evaluating Epoch 8  62.2% | batch:       107 of       172\t|\tloss: 2.38253\n",
      "Evaluating Epoch 8  62.8% | batch:       108 of       172\t|\tloss: 2.69532\n",
      "Evaluating Epoch 8  63.4% | batch:       109 of       172\t|\tloss: 3.46393\n",
      "Evaluating Epoch 8  64.0% | batch:       110 of       172\t|\tloss: 3.70342\n",
      "Evaluating Epoch 8  64.5% | batch:       111 of       172\t|\tloss: 1.75153\n",
      "Evaluating Epoch 8  65.1% | batch:       112 of       172\t|\tloss: 1.97188\n",
      "Evaluating Epoch 8  65.7% | batch:       113 of       172\t|\tloss: 2.42894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 8  66.3% | batch:       114 of       172\t|\tloss: 2.87558\n",
      "Evaluating Epoch 8  66.9% | batch:       115 of       172\t|\tloss: 1.21141\n",
      "Evaluating Epoch 8  67.4% | batch:       116 of       172\t|\tloss: 1.60127\n",
      "Evaluating Epoch 8  68.0% | batch:       117 of       172\t|\tloss: 1.18452\n",
      "Evaluating Epoch 8  68.6% | batch:       118 of       172\t|\tloss: 1.09174\n",
      "Evaluating Epoch 8  69.2% | batch:       119 of       172\t|\tloss: 1.28817\n",
      "Evaluating Epoch 8  69.8% | batch:       120 of       172\t|\tloss: 1.16967\n",
      "Evaluating Epoch 8  70.3% | batch:       121 of       172\t|\tloss: 3.02165\n",
      "Evaluating Epoch 8  70.9% | batch:       122 of       172\t|\tloss: 1.83232\n",
      "Evaluating Epoch 8  71.5% | batch:       123 of       172\t|\tloss: 4.89224\n",
      "Evaluating Epoch 8  72.1% | batch:       124 of       172\t|\tloss: 17.7497\n",
      "Evaluating Epoch 8  72.7% | batch:       125 of       172\t|\tloss: 1.74487\n",
      "Evaluating Epoch 8  73.3% | batch:       126 of       172\t|\tloss: 1.6359\n",
      "Evaluating Epoch 8  73.8% | batch:       127 of       172\t|\tloss: 0.68821\n",
      "Evaluating Epoch 8  74.4% | batch:       128 of       172\t|\tloss: 1.89759\n",
      "Evaluating Epoch 8  75.0% | batch:       129 of       172\t|\tloss: 1.35793\n",
      "Evaluating Epoch 8  75.6% | batch:       130 of       172\t|\tloss: 0.693236\n",
      "Evaluating Epoch 8  76.2% | batch:       131 of       172\t|\tloss: 1.61145\n",
      "Evaluating Epoch 8  76.7% | batch:       132 of       172\t|\tloss: 0.791064\n",
      "Evaluating Epoch 8  77.3% | batch:       133 of       172\t|\tloss: 0.431562\n",
      "Evaluating Epoch 8  77.9% | batch:       134 of       172\t|\tloss: 0.828723\n",
      "Evaluating Epoch 8  78.5% | batch:       135 of       172\t|\tloss: 0.299905\n",
      "Evaluating Epoch 8  79.1% | batch:       136 of       172\t|\tloss: 0.482968\n",
      "Evaluating Epoch 8  79.7% | batch:       137 of       172\t|\tloss: 0.386341\n",
      "Evaluating Epoch 8  80.2% | batch:       138 of       172\t|\tloss: 0.965959\n",
      "Evaluating Epoch 8  80.8% | batch:       139 of       172\t|\tloss: 0.961456\n",
      "Evaluating Epoch 8  81.4% | batch:       140 of       172\t|\tloss: 0.654259\n",
      "Evaluating Epoch 8  82.0% | batch:       141 of       172\t|\tloss: 0.550617\n",
      "Evaluating Epoch 8  82.6% | batch:       142 of       172\t|\tloss: 0.555924\n",
      "Evaluating Epoch 8  83.1% | batch:       143 of       172\t|\tloss: 0.45233\n",
      "Evaluating Epoch 8  83.7% | batch:       144 of       172\t|\tloss: 0.729971\n",
      "Evaluating Epoch 8  84.3% | batch:       145 of       172\t|\tloss: 0.27327\n",
      "Evaluating Epoch 8  84.9% | batch:       146 of       172\t|\tloss: 0.638077\n",
      "Evaluating Epoch 8  85.5% | batch:       147 of       172\t|\tloss: 0.30395\n",
      "Evaluating Epoch 8  86.0% | batch:       148 of       172\t|\tloss: 0.482392\n",
      "Evaluating Epoch 8  86.6% | batch:       149 of       172\t|\tloss: 0.406728\n",
      "Evaluating Epoch 8  87.2% | batch:       150 of       172\t|\tloss: 0.763702\n",
      "Evaluating Epoch 8  87.8% | batch:       151 of       172\t|\tloss: 0.973202\n",
      "Evaluating Epoch 8  88.4% | batch:       152 of       172\t|\tloss: 0.898172\n",
      "Evaluating Epoch 8  89.0% | batch:       153 of       172\t|\tloss: 0.762812\n",
      "Evaluating Epoch 8  89.5% | batch:       154 of       172\t|\tloss: 1.06474\n",
      "Evaluating Epoch 8  90.1% | batch:       155 of       172\t|\tloss: 1.14822\n",
      "Evaluating Epoch 8  90.7% | batch:       156 of       172\t|\tloss: 1.03065\n",
      "Evaluating Epoch 8  91.3% | batch:       157 of       172\t|\tloss: 1.2488\n",
      "Evaluating Epoch 8  91.9% | batch:       158 of       172\t|\tloss: 0.741635\n",
      "Evaluating Epoch 8  92.4% | batch:       159 of       172\t|\tloss: 1.42025\n",
      "Evaluating Epoch 8  93.0% | batch:       160 of       172\t|\tloss: 4.43799\n",
      "Evaluating Epoch 8  93.6% | batch:       161 of       172\t|\tloss: 3.19491\n",
      "Evaluating Epoch 8  94.2% | batch:       162 of       172\t|\tloss: 1.21822\n",
      "Evaluating Epoch 8  94.8% | batch:       163 of       172\t|\tloss: 0.80036\n",
      "Evaluating Epoch 8  95.3% | batch:       164 of       172\t|\tloss: 1.28897\n",
      "Evaluating Epoch 8  95.9% | batch:       165 of       172\t|\tloss: 1.00164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:03:32,885 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.5329244136810303 seconds\n",
      "\n",
      "2023-05-25 22:03:32,888 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.018527772691515 seconds\n",
      "2023-05-25 22:03:32,889 | INFO : Avg batch val. time: 0.02336353356215997 seconds\n",
      "2023-05-25 22:03:32,889 | INFO : Avg sample val. time: 0.00018301807044184156 seconds\n",
      "2023-05-25 22:03:32,890 | INFO : Epoch 8 Validation Summary: epoch: 8.000000 | loss: 2.042325 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 8  96.5% | batch:       166 of       172\t|\tloss: 0.640327\n",
      "Evaluating Epoch 8  97.1% | batch:       167 of       172\t|\tloss: 1.19364\n",
      "Evaluating Epoch 8  97.7% | batch:       168 of       172\t|\tloss: 0.851228\n",
      "Evaluating Epoch 8  98.3% | batch:       169 of       172\t|\tloss: 0.783122\n",
      "Evaluating Epoch 8  98.8% | batch:       170 of       172\t|\tloss: 1.17462\n",
      "Evaluating Epoch 8  99.4% | batch:       171 of       172\t|\tloss: 0.900366\n",
      "\n",
      "Training Epoch 9   0.0% | batch:         0 of       686\t|\tloss: 8.74308\n",
      "Training Epoch 9   0.1% | batch:         1 of       686\t|\tloss: 9.82033\n",
      "Training Epoch 9   0.3% | batch:         2 of       686\t|\tloss: 10.5944\n",
      "Training Epoch 9   0.4% | batch:         3 of       686\t|\tloss: 8.44442\n",
      "Training Epoch 9   0.6% | batch:         4 of       686\t|\tloss: 12.1749\n",
      "Training Epoch 9   0.7% | batch:         5 of       686\t|\tloss: 13.0765\n",
      "Training Epoch 9   0.9% | batch:         6 of       686\t|\tloss: 10.8824\n",
      "Training Epoch 9   1.0% | batch:         7 of       686\t|\tloss: 14.6433\n",
      "Training Epoch 9   1.2% | batch:         8 of       686\t|\tloss: 7.74223\n",
      "Training Epoch 9   1.3% | batch:         9 of       686\t|\tloss: 10.4387\n",
      "Training Epoch 9   1.5% | batch:        10 of       686\t|\tloss: 9.52857\n",
      "Training Epoch 9   1.6% | batch:        11 of       686\t|\tloss: 8.87829\n",
      "Training Epoch 9   1.7% | batch:        12 of       686\t|\tloss: 13.3261\n",
      "Training Epoch 9   1.9% | batch:        13 of       686\t|\tloss: 13.5919\n",
      "Training Epoch 9   2.0% | batch:        14 of       686\t|\tloss: 10.8378\n",
      "Training Epoch 9   2.2% | batch:        15 of       686\t|\tloss: 14.9661\n",
      "Training Epoch 9   2.3% | batch:        16 of       686\t|\tloss: 12.5724\n",
      "Training Epoch 9   2.5% | batch:        17 of       686\t|\tloss: 7.94031\n",
      "Training Epoch 9   2.6% | batch:        18 of       686\t|\tloss: 8.29958\n",
      "Training Epoch 9   2.8% | batch:        19 of       686\t|\tloss: 9.58606\n",
      "Training Epoch 9   2.9% | batch:        20 of       686\t|\tloss: 13.267\n",
      "Training Epoch 9   3.1% | batch:        21 of       686\t|\tloss: 8.34399\n",
      "Training Epoch 9   3.2% | batch:        22 of       686\t|\tloss: 9.99809\n",
      "Training Epoch 9   3.4% | batch:        23 of       686\t|\tloss: 8.61378\n",
      "Training Epoch 9   3.5% | batch:        24 of       686\t|\tloss: 10.7638\n",
      "Training Epoch 9   3.6% | batch:        25 of       686\t|\tloss: 10.1597\n",
      "Training Epoch 9   3.8% | batch:        26 of       686\t|\tloss: 10.5664\n",
      "Training Epoch 9   3.9% | batch:        27 of       686\t|\tloss: 6.87874\n",
      "Training Epoch 9   4.1% | batch:        28 of       686\t|\tloss: 7.36523\n",
      "Training Epoch 9   4.2% | batch:        29 of       686\t|\tloss: 9.32788\n",
      "Training Epoch 9   4.4% | batch:        30 of       686\t|\tloss: 12.7296\n",
      "Training Epoch 9   4.5% | batch:        31 of       686\t|\tloss: 8.52511\n",
      "Training Epoch 9   4.7% | batch:        32 of       686\t|\tloss: 9.91654\n",
      "Training Epoch 9   4.8% | batch:        33 of       686\t|\tloss: 11.3644\n",
      "Training Epoch 9   5.0% | batch:        34 of       686\t|\tloss: 10.0608\n",
      "Training Epoch 9   5.1% | batch:        35 of       686\t|\tloss: 11.7337\n",
      "Training Epoch 9   5.2% | batch:        36 of       686\t|\tloss: 9.32036\n",
      "Training Epoch 9   5.4% | batch:        37 of       686\t|\tloss: 11.3663\n",
      "Training Epoch 9   5.5% | batch:        38 of       686\t|\tloss: 11.0911\n",
      "Training Epoch 9   5.7% | batch:        39 of       686\t|\tloss: 10.2859\n",
      "Training Epoch 9   5.8% | batch:        40 of       686\t|\tloss: 13.7389\n",
      "Training Epoch 9   6.0% | batch:        41 of       686\t|\tloss: 10.907\n",
      "Training Epoch 9   6.1% | batch:        42 of       686\t|\tloss: 7.68189\n",
      "Training Epoch 9   6.3% | batch:        43 of       686\t|\tloss: 9.9049\n",
      "Training Epoch 9   6.4% | batch:        44 of       686\t|\tloss: 9.02345\n",
      "Training Epoch 9   6.6% | batch:        45 of       686\t|\tloss: 9.87916\n",
      "Training Epoch 9   6.7% | batch:        46 of       686\t|\tloss: 11.8085\n",
      "Training Epoch 9   6.9% | batch:        47 of       686\t|\tloss: 9.50941\n",
      "Training Epoch 9   7.0% | batch:        48 of       686\t|\tloss: 10.0184\n",
      "Training Epoch 9   7.1% | batch:        49 of       686\t|\tloss: 11.0388\n",
      "Training Epoch 9   7.3% | batch:        50 of       686\t|\tloss: 12.7017\n",
      "Training Epoch 9   7.4% | batch:        51 of       686\t|\tloss: 9.84477\n",
      "Training Epoch 9   7.6% | batch:        52 of       686\t|\tloss: 12.4452\n",
      "Training Epoch 9   7.7% | batch:        53 of       686\t|\tloss: 10.5251\n",
      "Training Epoch 9   7.9% | batch:        54 of       686\t|\tloss: 10.1124\n",
      "Training Epoch 9   8.0% | batch:        55 of       686\t|\tloss: 10.4875\n",
      "Training Epoch 9   8.2% | batch:        56 of       686\t|\tloss: 12.0313\n",
      "Training Epoch 9   8.3% | batch:        57 of       686\t|\tloss: 13.0281\n",
      "Training Epoch 9   8.5% | batch:        58 of       686\t|\tloss: 9.73879\n",
      "Training Epoch 9   8.6% | batch:        59 of       686\t|\tloss: 9.8343\n",
      "Training Epoch 9   8.7% | batch:        60 of       686\t|\tloss: 9.5426\n",
      "Training Epoch 9   8.9% | batch:        61 of       686\t|\tloss: 9.75451\n",
      "Training Epoch 9   9.0% | batch:        62 of       686\t|\tloss: 11.1461\n",
      "Training Epoch 9   9.2% | batch:        63 of       686\t|\tloss: 10.6151\n",
      "Training Epoch 9   9.3% | batch:        64 of       686\t|\tloss: 9.89914\n",
      "Training Epoch 9   9.5% | batch:        65 of       686\t|\tloss: 9.26176\n",
      "Training Epoch 9   9.6% | batch:        66 of       686\t|\tloss: 9.18732\n",
      "Training Epoch 9   9.8% | batch:        67 of       686\t|\tloss: 9.48439\n",
      "Training Epoch 9   9.9% | batch:        68 of       686\t|\tloss: 14.8702\n",
      "Training Epoch 9  10.1% | batch:        69 of       686\t|\tloss: 9.84344\n",
      "Training Epoch 9  10.2% | batch:        70 of       686\t|\tloss: 8.78279\n",
      "Training Epoch 9  10.3% | batch:        71 of       686\t|\tloss: 9.87545\n",
      "Training Epoch 9  10.5% | batch:        72 of       686\t|\tloss: 11.7825\n",
      "Training Epoch 9  10.6% | batch:        73 of       686\t|\tloss: 8.85961\n",
      "Training Epoch 9  10.8% | batch:        74 of       686\t|\tloss: 10.1518\n",
      "Training Epoch 9  10.9% | batch:        75 of       686\t|\tloss: 10.8988\n",
      "Training Epoch 9  11.1% | batch:        76 of       686\t|\tloss: 9.93065\n",
      "Training Epoch 9  11.2% | batch:        77 of       686\t|\tloss: 13.5424\n",
      "Training Epoch 9  11.4% | batch:        78 of       686\t|\tloss: 10.8176\n",
      "Training Epoch 9  11.5% | batch:        79 of       686\t|\tloss: 10.0025\n",
      "Training Epoch 9  11.7% | batch:        80 of       686\t|\tloss: 9.77576\n",
      "Training Epoch 9  11.8% | batch:        81 of       686\t|\tloss: 11.636\n",
      "Training Epoch 9  12.0% | batch:        82 of       686\t|\tloss: 10.9088\n",
      "Training Epoch 9  12.1% | batch:        83 of       686\t|\tloss: 11.6553\n",
      "Training Epoch 9  12.2% | batch:        84 of       686\t|\tloss: 11.3334\n",
      "Training Epoch 9  12.4% | batch:        85 of       686\t|\tloss: 8.84426\n",
      "Training Epoch 9  12.5% | batch:        86 of       686\t|\tloss: 9.97963\n",
      "Training Epoch 9  12.7% | batch:        87 of       686\t|\tloss: 10.7066\n",
      "Training Epoch 9  12.8% | batch:        88 of       686\t|\tloss: 12.1972\n",
      "Training Epoch 9  13.0% | batch:        89 of       686\t|\tloss: 10.9373\n",
      "Training Epoch 9  13.1% | batch:        90 of       686\t|\tloss: 10.8984\n",
      "Training Epoch 9  13.3% | batch:        91 of       686\t|\tloss: 10.3972\n",
      "Training Epoch 9  13.4% | batch:        92 of       686\t|\tloss: 10.6854\n",
      "Training Epoch 9  13.6% | batch:        93 of       686\t|\tloss: 8.47826\n",
      "Training Epoch 9  13.7% | batch:        94 of       686\t|\tloss: 7.26534\n",
      "Training Epoch 9  13.8% | batch:        95 of       686\t|\tloss: 11.1424\n",
      "Training Epoch 9  14.0% | batch:        96 of       686\t|\tloss: 8.16751\n",
      "Training Epoch 9  14.1% | batch:        97 of       686\t|\tloss: 9.2767\n",
      "Training Epoch 9  14.3% | batch:        98 of       686\t|\tloss: 8.03867\n",
      "Training Epoch 9  14.4% | batch:        99 of       686\t|\tloss: 11.9971\n",
      "Training Epoch 9  14.6% | batch:       100 of       686\t|\tloss: 13.6585\n",
      "Training Epoch 9  14.7% | batch:       101 of       686\t|\tloss: 9.1097\n",
      "Training Epoch 9  14.9% | batch:       102 of       686\t|\tloss: 10.6469\n",
      "Training Epoch 9  15.0% | batch:       103 of       686\t|\tloss: 10.9202\n",
      "Training Epoch 9  15.2% | batch:       104 of       686\t|\tloss: 7.69809\n",
      "Training Epoch 9  15.3% | batch:       105 of       686\t|\tloss: 11.386\n",
      "Training Epoch 9  15.5% | batch:       106 of       686\t|\tloss: 10.3626\n",
      "Training Epoch 9  15.6% | batch:       107 of       686\t|\tloss: 10.663\n",
      "Training Epoch 9  15.7% | batch:       108 of       686\t|\tloss: 14.2951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  15.9% | batch:       109 of       686\t|\tloss: 8.54343\n",
      "Training Epoch 9  16.0% | batch:       110 of       686\t|\tloss: 11.5538\n",
      "Training Epoch 9  16.2% | batch:       111 of       686\t|\tloss: 7.63274\n",
      "Training Epoch 9  16.3% | batch:       112 of       686\t|\tloss: 10.6203\n",
      "Training Epoch 9  16.5% | batch:       113 of       686\t|\tloss: 9.7723\n",
      "Training Epoch 9  16.6% | batch:       114 of       686\t|\tloss: 10.0185\n",
      "Training Epoch 9  16.8% | batch:       115 of       686\t|\tloss: 8.12198\n",
      "Training Epoch 9  16.9% | batch:       116 of       686\t|\tloss: 13.3766\n",
      "Training Epoch 9  17.1% | batch:       117 of       686\t|\tloss: 9.9223\n",
      "Training Epoch 9  17.2% | batch:       118 of       686\t|\tloss: 10.083\n",
      "Training Epoch 9  17.3% | batch:       119 of       686\t|\tloss: 9.82846\n",
      "Training Epoch 9  17.5% | batch:       120 of       686\t|\tloss: 13.6361\n",
      "Training Epoch 9  17.6% | batch:       121 of       686\t|\tloss: 11.2917\n",
      "Training Epoch 9  17.8% | batch:       122 of       686\t|\tloss: 10.1584\n",
      "Training Epoch 9  17.9% | batch:       123 of       686\t|\tloss: 12.0601\n",
      "Training Epoch 9  18.1% | batch:       124 of       686\t|\tloss: 9.32682\n",
      "Training Epoch 9  18.2% | batch:       125 of       686\t|\tloss: 10.9503\n",
      "Training Epoch 9  18.4% | batch:       126 of       686\t|\tloss: 8.51853\n",
      "Training Epoch 9  18.5% | batch:       127 of       686\t|\tloss: 13.723\n",
      "Training Epoch 9  18.7% | batch:       128 of       686\t|\tloss: 11.5816\n",
      "Training Epoch 9  18.8% | batch:       129 of       686\t|\tloss: 11.5146\n",
      "Training Epoch 9  19.0% | batch:       130 of       686\t|\tloss: 9.38557\n",
      "Training Epoch 9  19.1% | batch:       131 of       686\t|\tloss: 11.4211\n",
      "Training Epoch 9  19.2% | batch:       132 of       686\t|\tloss: 12.7011\n",
      "Training Epoch 9  19.4% | batch:       133 of       686\t|\tloss: 14.4669\n",
      "Training Epoch 9  19.5% | batch:       134 of       686\t|\tloss: 9.83903\n",
      "Training Epoch 9  19.7% | batch:       135 of       686\t|\tloss: 9.2945\n",
      "Training Epoch 9  19.8% | batch:       136 of       686\t|\tloss: 8.65708\n",
      "Training Epoch 9  20.0% | batch:       137 of       686\t|\tloss: 7.89494\n",
      "Training Epoch 9  20.1% | batch:       138 of       686\t|\tloss: 11.5777\n",
      "Training Epoch 9  20.3% | batch:       139 of       686\t|\tloss: 8.86055\n",
      "Training Epoch 9  20.4% | batch:       140 of       686\t|\tloss: 11.4064\n",
      "Training Epoch 9  20.6% | batch:       141 of       686\t|\tloss: 9.80327\n",
      "Training Epoch 9  20.7% | batch:       142 of       686\t|\tloss: 10.9214\n",
      "Training Epoch 9  20.8% | batch:       143 of       686\t|\tloss: 10.1\n",
      "Training Epoch 9  21.0% | batch:       144 of       686\t|\tloss: 8.04958\n",
      "Training Epoch 9  21.1% | batch:       145 of       686\t|\tloss: 8.829\n",
      "Training Epoch 9  21.3% | batch:       146 of       686\t|\tloss: 6.80245\n",
      "Training Epoch 9  21.4% | batch:       147 of       686\t|\tloss: 8.95567\n",
      "Training Epoch 9  21.6% | batch:       148 of       686\t|\tloss: 8.97652\n",
      "Training Epoch 9  21.7% | batch:       149 of       686\t|\tloss: 8.80851\n",
      "Training Epoch 9  21.9% | batch:       150 of       686\t|\tloss: 10.4776\n",
      "Training Epoch 9  22.0% | batch:       151 of       686\t|\tloss: 10.0843\n",
      "Training Epoch 9  22.2% | batch:       152 of       686\t|\tloss: 10.4933\n",
      "Training Epoch 9  22.3% | batch:       153 of       686\t|\tloss: 12.0147\n",
      "Training Epoch 9  22.4% | batch:       154 of       686\t|\tloss: 10.2553\n",
      "Training Epoch 9  22.6% | batch:       155 of       686\t|\tloss: 7.66372\n",
      "Training Epoch 9  22.7% | batch:       156 of       686\t|\tloss: 10.4812\n",
      "Training Epoch 9  22.9% | batch:       157 of       686\t|\tloss: 8.88119\n",
      "Training Epoch 9  23.0% | batch:       158 of       686\t|\tloss: 8.694\n",
      "Training Epoch 9  23.2% | batch:       159 of       686\t|\tloss: 11.3788\n",
      "Training Epoch 9  23.3% | batch:       160 of       686\t|\tloss: 13.5343\n",
      "Training Epoch 9  23.5% | batch:       161 of       686\t|\tloss: 9.49864\n",
      "Training Epoch 9  23.6% | batch:       162 of       686\t|\tloss: 8.41286\n",
      "Training Epoch 9  23.8% | batch:       163 of       686\t|\tloss: 12.0763\n",
      "Training Epoch 9  23.9% | batch:       164 of       686\t|\tloss: 8.34713\n",
      "Training Epoch 9  24.1% | batch:       165 of       686\t|\tloss: 11.2169\n",
      "Training Epoch 9  24.2% | batch:       166 of       686\t|\tloss: 12.5979\n",
      "Training Epoch 9  24.3% | batch:       167 of       686\t|\tloss: 10.5351\n",
      "Training Epoch 9  24.5% | batch:       168 of       686\t|\tloss: 11.3939\n",
      "Training Epoch 9  24.6% | batch:       169 of       686\t|\tloss: 8.80927\n",
      "Training Epoch 9  24.8% | batch:       170 of       686\t|\tloss: 13.4978\n",
      "Training Epoch 9  24.9% | batch:       171 of       686\t|\tloss: 11.0139\n",
      "Training Epoch 9  25.1% | batch:       172 of       686\t|\tloss: 9.47582\n",
      "Training Epoch 9  25.2% | batch:       173 of       686\t|\tloss: 11.3353\n",
      "Training Epoch 9  25.4% | batch:       174 of       686\t|\tloss: 15.3347\n",
      "Training Epoch 9  25.5% | batch:       175 of       686\t|\tloss: 7.68183\n",
      "Training Epoch 9  25.7% | batch:       176 of       686\t|\tloss: 12.2551\n",
      "Training Epoch 9  25.8% | batch:       177 of       686\t|\tloss: 7.23573\n",
      "Training Epoch 9  25.9% | batch:       178 of       686\t|\tloss: 8.43826\n",
      "Training Epoch 9  26.1% | batch:       179 of       686\t|\tloss: 9.62528\n",
      "Training Epoch 9  26.2% | batch:       180 of       686\t|\tloss: 9.52097\n",
      "Training Epoch 9  26.4% | batch:       181 of       686\t|\tloss: 10.4566\n",
      "Training Epoch 9  26.5% | batch:       182 of       686\t|\tloss: 12.6664\n",
      "Training Epoch 9  26.7% | batch:       183 of       686\t|\tloss: 8.44838\n",
      "Training Epoch 9  26.8% | batch:       184 of       686\t|\tloss: 10.4355\n",
      "Training Epoch 9  27.0% | batch:       185 of       686\t|\tloss: 8.93122\n",
      "Training Epoch 9  27.1% | batch:       186 of       686\t|\tloss: 7.79324\n",
      "Training Epoch 9  27.3% | batch:       187 of       686\t|\tloss: 13.2343\n",
      "Training Epoch 9  27.4% | batch:       188 of       686\t|\tloss: 9.85427\n",
      "Training Epoch 9  27.6% | batch:       189 of       686\t|\tloss: 10.8207\n",
      "Training Epoch 9  27.7% | batch:       190 of       686\t|\tloss: 10.777\n",
      "Training Epoch 9  27.8% | batch:       191 of       686\t|\tloss: 11.6151\n",
      "Training Epoch 9  28.0% | batch:       192 of       686\t|\tloss: 12.1308\n",
      "Training Epoch 9  28.1% | batch:       193 of       686\t|\tloss: 10.8181\n",
      "Training Epoch 9  28.3% | batch:       194 of       686\t|\tloss: 12.3919\n",
      "Training Epoch 9  28.4% | batch:       195 of       686\t|\tloss: 13.589\n",
      "Training Epoch 9  28.6% | batch:       196 of       686\t|\tloss: 9.21454\n",
      "Training Epoch 9  28.7% | batch:       197 of       686\t|\tloss: 8.7259\n",
      "Training Epoch 9  28.9% | batch:       198 of       686\t|\tloss: 8.3438\n",
      "Training Epoch 9  29.0% | batch:       199 of       686\t|\tloss: 9.1272\n",
      "Training Epoch 9  29.2% | batch:       200 of       686\t|\tloss: 7.70037\n",
      "Training Epoch 9  29.3% | batch:       201 of       686\t|\tloss: 9.85771\n",
      "Training Epoch 9  29.4% | batch:       202 of       686\t|\tloss: 8.65686\n",
      "Training Epoch 9  29.6% | batch:       203 of       686\t|\tloss: 10.6941\n",
      "Training Epoch 9  29.7% | batch:       204 of       686\t|\tloss: 9.02119\n",
      "Training Epoch 9  29.9% | batch:       205 of       686\t|\tloss: 12.995\n",
      "Training Epoch 9  30.0% | batch:       206 of       686\t|\tloss: 10.1776\n",
      "Training Epoch 9  30.2% | batch:       207 of       686\t|\tloss: 10.0744\n",
      "Training Epoch 9  30.3% | batch:       208 of       686\t|\tloss: 5.4957\n",
      "Training Epoch 9  30.5% | batch:       209 of       686\t|\tloss: 7.83572\n",
      "Training Epoch 9  30.6% | batch:       210 of       686\t|\tloss: 13.3103\n",
      "Training Epoch 9  30.8% | batch:       211 of       686\t|\tloss: 10.3035\n",
      "Training Epoch 9  30.9% | batch:       212 of       686\t|\tloss: 10.2189\n",
      "Training Epoch 9  31.0% | batch:       213 of       686\t|\tloss: 8.95639\n",
      "Training Epoch 9  31.2% | batch:       214 of       686\t|\tloss: 10.0611\n",
      "Training Epoch 9  31.3% | batch:       215 of       686\t|\tloss: 7.01725\n",
      "Training Epoch 9  31.5% | batch:       216 of       686\t|\tloss: 12.1428\n",
      "Training Epoch 9  31.6% | batch:       217 of       686\t|\tloss: 9.56966\n",
      "Training Epoch 9  31.8% | batch:       218 of       686\t|\tloss: 10.3838\n",
      "Training Epoch 9  31.9% | batch:       219 of       686\t|\tloss: 9.71322\n",
      "Training Epoch 9  32.1% | batch:       220 of       686\t|\tloss: 9.11174\n",
      "Training Epoch 9  32.2% | batch:       221 of       686\t|\tloss: 9.29893\n",
      "Training Epoch 9  32.4% | batch:       222 of       686\t|\tloss: 10.6897\n",
      "Training Epoch 9  32.5% | batch:       223 of       686\t|\tloss: 7.35491\n",
      "Training Epoch 9  32.7% | batch:       224 of       686\t|\tloss: 11.209\n",
      "Training Epoch 9  32.8% | batch:       225 of       686\t|\tloss: 9.21437\n",
      "Training Epoch 9  32.9% | batch:       226 of       686\t|\tloss: 9.6045\n",
      "Training Epoch 9  33.1% | batch:       227 of       686\t|\tloss: 11.8757\n",
      "Training Epoch 9  33.2% | batch:       228 of       686\t|\tloss: 8.66155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  33.4% | batch:       229 of       686\t|\tloss: 7.53289\n",
      "Training Epoch 9  33.5% | batch:       230 of       686\t|\tloss: 7.82322\n",
      "Training Epoch 9  33.7% | batch:       231 of       686\t|\tloss: 10.4224\n",
      "Training Epoch 9  33.8% | batch:       232 of       686\t|\tloss: 12.7574\n",
      "Training Epoch 9  34.0% | batch:       233 of       686\t|\tloss: 11.3799\n",
      "Training Epoch 9  34.1% | batch:       234 of       686\t|\tloss: 8.72323\n",
      "Training Epoch 9  34.3% | batch:       235 of       686\t|\tloss: 12.3024\n",
      "Training Epoch 9  34.4% | batch:       236 of       686\t|\tloss: 13.5796\n",
      "Training Epoch 9  34.5% | batch:       237 of       686\t|\tloss: 8.22916\n",
      "Training Epoch 9  34.7% | batch:       238 of       686\t|\tloss: 10.9732\n",
      "Training Epoch 9  34.8% | batch:       239 of       686\t|\tloss: 10.0515\n",
      "Training Epoch 9  35.0% | batch:       240 of       686\t|\tloss: 14.007\n",
      "Training Epoch 9  35.1% | batch:       241 of       686\t|\tloss: 10.2569\n",
      "Training Epoch 9  35.3% | batch:       242 of       686\t|\tloss: 7.50372\n",
      "Training Epoch 9  35.4% | batch:       243 of       686\t|\tloss: 8.26694\n",
      "Training Epoch 9  35.6% | batch:       244 of       686\t|\tloss: 10.4452\n",
      "Training Epoch 9  35.7% | batch:       245 of       686\t|\tloss: 10.9757\n",
      "Training Epoch 9  35.9% | batch:       246 of       686\t|\tloss: 13.284\n",
      "Training Epoch 9  36.0% | batch:       247 of       686\t|\tloss: 9.28303\n",
      "Training Epoch 9  36.2% | batch:       248 of       686\t|\tloss: 10.3121\n",
      "Training Epoch 9  36.3% | batch:       249 of       686\t|\tloss: 16.685\n",
      "Training Epoch 9  36.4% | batch:       250 of       686\t|\tloss: 8.79933\n",
      "Training Epoch 9  36.6% | batch:       251 of       686\t|\tloss: 11.9844\n",
      "Training Epoch 9  36.7% | batch:       252 of       686\t|\tloss: 7.61334\n",
      "Training Epoch 9  36.9% | batch:       253 of       686\t|\tloss: 11.9544\n",
      "Training Epoch 9  37.0% | batch:       254 of       686\t|\tloss: 11.0592\n",
      "Training Epoch 9  37.2% | batch:       255 of       686\t|\tloss: 9.96749\n",
      "Training Epoch 9  37.3% | batch:       256 of       686\t|\tloss: 10.9883\n",
      "Training Epoch 9  37.5% | batch:       257 of       686\t|\tloss: 10.7186\n",
      "Training Epoch 9  37.6% | batch:       258 of       686\t|\tloss: 10.6442\n",
      "Training Epoch 9  37.8% | batch:       259 of       686\t|\tloss: 9.44651\n",
      "Training Epoch 9  37.9% | batch:       260 of       686\t|\tloss: 12.6199\n",
      "Training Epoch 9  38.0% | batch:       261 of       686\t|\tloss: 8.94669\n",
      "Training Epoch 9  38.2% | batch:       262 of       686\t|\tloss: 11.0942\n",
      "Training Epoch 9  38.3% | batch:       263 of       686\t|\tloss: 9.16878\n",
      "Training Epoch 9  38.5% | batch:       264 of       686\t|\tloss: 8.73342\n",
      "Training Epoch 9  38.6% | batch:       265 of       686\t|\tloss: 10.3743\n",
      "Training Epoch 9  38.8% | batch:       266 of       686\t|\tloss: 9.41246\n",
      "Training Epoch 9  38.9% | batch:       267 of       686\t|\tloss: 10.8535\n",
      "Training Epoch 9  39.1% | batch:       268 of       686\t|\tloss: 11.4079\n",
      "Training Epoch 9  39.2% | batch:       269 of       686\t|\tloss: 12.6655\n",
      "Training Epoch 9  39.4% | batch:       270 of       686\t|\tloss: 10.7598\n",
      "Training Epoch 9  39.5% | batch:       271 of       686\t|\tloss: 8.89207\n",
      "Training Epoch 9  39.7% | batch:       272 of       686\t|\tloss: 10.0546\n",
      "Training Epoch 9  39.8% | batch:       273 of       686\t|\tloss: 10.5722\n",
      "Training Epoch 9  39.9% | batch:       274 of       686\t|\tloss: 10.5265\n",
      "Training Epoch 9  40.1% | batch:       275 of       686\t|\tloss: 7.64799\n",
      "Training Epoch 9  40.2% | batch:       276 of       686\t|\tloss: 12.2179\n",
      "Training Epoch 9  40.4% | batch:       277 of       686\t|\tloss: 7.15774\n",
      "Training Epoch 9  40.5% | batch:       278 of       686\t|\tloss: 11.919\n",
      "Training Epoch 9  40.7% | batch:       279 of       686\t|\tloss: 8.01366\n",
      "Training Epoch 9  40.8% | batch:       280 of       686\t|\tloss: 11.2604\n",
      "Training Epoch 9  41.0% | batch:       281 of       686\t|\tloss: 11.7747\n",
      "Training Epoch 9  41.1% | batch:       282 of       686\t|\tloss: 11.2546\n",
      "Training Epoch 9  41.3% | batch:       283 of       686\t|\tloss: 11.5206\n",
      "Training Epoch 9  41.4% | batch:       284 of       686\t|\tloss: 12.9006\n",
      "Training Epoch 9  41.5% | batch:       285 of       686\t|\tloss: 9.9814\n",
      "Training Epoch 9  41.7% | batch:       286 of       686\t|\tloss: 12.7409\n",
      "Training Epoch 9  41.8% | batch:       287 of       686\t|\tloss: 9.92506\n",
      "Training Epoch 9  42.0% | batch:       288 of       686\t|\tloss: 7.30404\n",
      "Training Epoch 9  42.1% | batch:       289 of       686\t|\tloss: 15.8994\n",
      "Training Epoch 9  42.3% | batch:       290 of       686\t|\tloss: 9.98269\n",
      "Training Epoch 9  42.4% | batch:       291 of       686\t|\tloss: 8.47854\n",
      "Training Epoch 9  42.6% | batch:       292 of       686\t|\tloss: 11.4769\n",
      "Training Epoch 9  42.7% | batch:       293 of       686\t|\tloss: 9.16193\n",
      "Training Epoch 9  42.9% | batch:       294 of       686\t|\tloss: 11.2283\n",
      "Training Epoch 9  43.0% | batch:       295 of       686\t|\tloss: 13.3804\n",
      "Training Epoch 9  43.1% | batch:       296 of       686\t|\tloss: 7.61834\n",
      "Training Epoch 9  43.3% | batch:       297 of       686\t|\tloss: 11.5696\n",
      "Training Epoch 9  43.4% | batch:       298 of       686\t|\tloss: 8.10234\n",
      "Training Epoch 9  43.6% | batch:       299 of       686\t|\tloss: 9.66213\n",
      "Training Epoch 9  43.7% | batch:       300 of       686\t|\tloss: 12.5444\n",
      "Training Epoch 9  43.9% | batch:       301 of       686\t|\tloss: 12.713\n",
      "Training Epoch 9  44.0% | batch:       302 of       686\t|\tloss: 12.0254\n",
      "Training Epoch 9  44.2% | batch:       303 of       686\t|\tloss: 11.9815\n",
      "Training Epoch 9  44.3% | batch:       304 of       686\t|\tloss: 9.02529\n",
      "Training Epoch 9  44.5% | batch:       305 of       686\t|\tloss: 8.39833\n",
      "Training Epoch 9  44.6% | batch:       306 of       686\t|\tloss: 9.10625\n",
      "Training Epoch 9  44.8% | batch:       307 of       686\t|\tloss: 13.5138\n",
      "Training Epoch 9  44.9% | batch:       308 of       686\t|\tloss: 12.9516\n",
      "Training Epoch 9  45.0% | batch:       309 of       686\t|\tloss: 10.4953\n",
      "Training Epoch 9  45.2% | batch:       310 of       686\t|\tloss: 9.74501\n",
      "Training Epoch 9  45.3% | batch:       311 of       686\t|\tloss: 8.89092\n",
      "Training Epoch 9  45.5% | batch:       312 of       686\t|\tloss: 10.3466\n",
      "Training Epoch 9  45.6% | batch:       313 of       686\t|\tloss: 10.5157\n",
      "Training Epoch 9  45.8% | batch:       314 of       686\t|\tloss: 8.65862\n",
      "Training Epoch 9  45.9% | batch:       315 of       686\t|\tloss: 9.51798\n",
      "Training Epoch 9  46.1% | batch:       316 of       686\t|\tloss: 9.79902\n",
      "Training Epoch 9  46.2% | batch:       317 of       686\t|\tloss: 8.64791\n",
      "Training Epoch 9  46.4% | batch:       318 of       686\t|\tloss: 12.4869\n",
      "Training Epoch 9  46.5% | batch:       319 of       686\t|\tloss: 9.92149\n",
      "Training Epoch 9  46.6% | batch:       320 of       686\t|\tloss: 8.1803\n",
      "Training Epoch 9  46.8% | batch:       321 of       686\t|\tloss: 10.0984\n",
      "Training Epoch 9  46.9% | batch:       322 of       686\t|\tloss: 7.93069\n",
      "Training Epoch 9  47.1% | batch:       323 of       686\t|\tloss: 11.1556\n",
      "Training Epoch 9  47.2% | batch:       324 of       686\t|\tloss: 10.7439\n",
      "Training Epoch 9  47.4% | batch:       325 of       686\t|\tloss: 9.42444\n",
      "Training Epoch 9  47.5% | batch:       326 of       686\t|\tloss: 10.6446\n",
      "Training Epoch 9  47.7% | batch:       327 of       686\t|\tloss: 9.02291\n",
      "Training Epoch 9  47.8% | batch:       328 of       686\t|\tloss: 8.97847\n",
      "Training Epoch 9  48.0% | batch:       329 of       686\t|\tloss: 9.39239\n",
      "Training Epoch 9  48.1% | batch:       330 of       686\t|\tloss: 8.54432\n",
      "Training Epoch 9  48.3% | batch:       331 of       686\t|\tloss: 16.2577\n",
      "Training Epoch 9  48.4% | batch:       332 of       686\t|\tloss: 10.2777\n",
      "Training Epoch 9  48.5% | batch:       333 of       686\t|\tloss: 10.2393\n",
      "Training Epoch 9  48.7% | batch:       334 of       686\t|\tloss: 8.16908\n",
      "Training Epoch 9  48.8% | batch:       335 of       686\t|\tloss: 8.83154\n",
      "Training Epoch 9  49.0% | batch:       336 of       686\t|\tloss: 10.3387\n",
      "Training Epoch 9  49.1% | batch:       337 of       686\t|\tloss: 10.3713\n",
      "Training Epoch 9  49.3% | batch:       338 of       686\t|\tloss: 8.49088\n",
      "Training Epoch 9  49.4% | batch:       339 of       686\t|\tloss: 11.7922\n",
      "Training Epoch 9  49.6% | batch:       340 of       686\t|\tloss: 7.34621\n",
      "Training Epoch 9  49.7% | batch:       341 of       686\t|\tloss: 10.474\n",
      "Training Epoch 9  49.9% | batch:       342 of       686\t|\tloss: 9.31653\n",
      "Training Epoch 9  50.0% | batch:       343 of       686\t|\tloss: 12.0539\n",
      "Training Epoch 9  50.1% | batch:       344 of       686\t|\tloss: 9.30863\n",
      "Training Epoch 9  50.3% | batch:       345 of       686\t|\tloss: 9.95645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  50.4% | batch:       346 of       686\t|\tloss: 10.2469\n",
      "Training Epoch 9  50.6% | batch:       347 of       686\t|\tloss: 10.3041\n",
      "Training Epoch 9  50.7% | batch:       348 of       686\t|\tloss: 8.34143\n",
      "Training Epoch 9  50.9% | batch:       349 of       686\t|\tloss: 11.3911\n",
      "Training Epoch 9  51.0% | batch:       350 of       686\t|\tloss: 10.3506\n",
      "Training Epoch 9  51.2% | batch:       351 of       686\t|\tloss: 10.4953\n",
      "Training Epoch 9  51.3% | batch:       352 of       686\t|\tloss: 11.073\n",
      "Training Epoch 9  51.5% | batch:       353 of       686\t|\tloss: 14.6568\n",
      "Training Epoch 9  51.6% | batch:       354 of       686\t|\tloss: 16.9528\n",
      "Training Epoch 9  51.7% | batch:       355 of       686\t|\tloss: 9.43412\n",
      "Training Epoch 9  51.9% | batch:       356 of       686\t|\tloss: 11.7336\n",
      "Training Epoch 9  52.0% | batch:       357 of       686\t|\tloss: 9.76877\n",
      "Training Epoch 9  52.2% | batch:       358 of       686\t|\tloss: 8.71537\n",
      "Training Epoch 9  52.3% | batch:       359 of       686\t|\tloss: 12.2291\n",
      "Training Epoch 9  52.5% | batch:       360 of       686\t|\tloss: 10.2946\n",
      "Training Epoch 9  52.6% | batch:       361 of       686\t|\tloss: 7.88102\n",
      "Training Epoch 9  52.8% | batch:       362 of       686\t|\tloss: 12.2351\n",
      "Training Epoch 9  52.9% | batch:       363 of       686\t|\tloss: 9.38389\n",
      "Training Epoch 9  53.1% | batch:       364 of       686\t|\tloss: 8.79765\n",
      "Training Epoch 9  53.2% | batch:       365 of       686\t|\tloss: 10.3035\n",
      "Training Epoch 9  53.4% | batch:       366 of       686\t|\tloss: 7.64471\n",
      "Training Epoch 9  53.5% | batch:       367 of       686\t|\tloss: 8.17718\n",
      "Training Epoch 9  53.6% | batch:       368 of       686\t|\tloss: 8.62096\n",
      "Training Epoch 9  53.8% | batch:       369 of       686\t|\tloss: 12.4707\n",
      "Training Epoch 9  53.9% | batch:       370 of       686\t|\tloss: 8.5247\n",
      "Training Epoch 9  54.1% | batch:       371 of       686\t|\tloss: 7.48897\n",
      "Training Epoch 9  54.2% | batch:       372 of       686\t|\tloss: 7.86844\n",
      "Training Epoch 9  54.4% | batch:       373 of       686\t|\tloss: 11.7605\n",
      "Training Epoch 9  54.5% | batch:       374 of       686\t|\tloss: 16.0216\n",
      "Training Epoch 9  54.7% | batch:       375 of       686\t|\tloss: 8.8034\n",
      "Training Epoch 9  54.8% | batch:       376 of       686\t|\tloss: 8.45848\n",
      "Training Epoch 9  55.0% | batch:       377 of       686\t|\tloss: 10.9273\n",
      "Training Epoch 9  55.1% | batch:       378 of       686\t|\tloss: 9.01753\n",
      "Training Epoch 9  55.2% | batch:       379 of       686\t|\tloss: 7.96266\n",
      "Training Epoch 9  55.4% | batch:       380 of       686\t|\tloss: 9.33906\n",
      "Training Epoch 9  55.5% | batch:       381 of       686\t|\tloss: 7.30671\n",
      "Training Epoch 9  55.7% | batch:       382 of       686\t|\tloss: 10.1199\n",
      "Training Epoch 9  55.8% | batch:       383 of       686\t|\tloss: 8.57787\n",
      "Training Epoch 9  56.0% | batch:       384 of       686\t|\tloss: 7.5309\n",
      "Training Epoch 9  56.1% | batch:       385 of       686\t|\tloss: 8.41636\n",
      "Training Epoch 9  56.3% | batch:       386 of       686\t|\tloss: 11.1115\n",
      "Training Epoch 9  56.4% | batch:       387 of       686\t|\tloss: 12.5239\n",
      "Training Epoch 9  56.6% | batch:       388 of       686\t|\tloss: 8.3554\n",
      "Training Epoch 9  56.7% | batch:       389 of       686\t|\tloss: 10.9507\n",
      "Training Epoch 9  56.9% | batch:       390 of       686\t|\tloss: 9.23591\n",
      "Training Epoch 9  57.0% | batch:       391 of       686\t|\tloss: 8.13576\n",
      "Training Epoch 9  57.1% | batch:       392 of       686\t|\tloss: 7.77767\n",
      "Training Epoch 9  57.3% | batch:       393 of       686\t|\tloss: 10.9342\n",
      "Training Epoch 9  57.4% | batch:       394 of       686\t|\tloss: 9.52103\n",
      "Training Epoch 9  57.6% | batch:       395 of       686\t|\tloss: 10.8929\n",
      "Training Epoch 9  57.7% | batch:       396 of       686\t|\tloss: 7.5323\n",
      "Training Epoch 9  57.9% | batch:       397 of       686\t|\tloss: 7.74786\n",
      "Training Epoch 9  58.0% | batch:       398 of       686\t|\tloss: 9.38824\n",
      "Training Epoch 9  58.2% | batch:       399 of       686\t|\tloss: 10.9349\n",
      "Training Epoch 9  58.3% | batch:       400 of       686\t|\tloss: 9.20466\n",
      "Training Epoch 9  58.5% | batch:       401 of       686\t|\tloss: 7.39172\n",
      "Training Epoch 9  58.6% | batch:       402 of       686\t|\tloss: 7.69144\n",
      "Training Epoch 9  58.7% | batch:       403 of       686\t|\tloss: 10.4892\n",
      "Training Epoch 9  58.9% | batch:       404 of       686\t|\tloss: 8.69934\n",
      "Training Epoch 9  59.0% | batch:       405 of       686\t|\tloss: 11.0661\n",
      "Training Epoch 9  59.2% | batch:       406 of       686\t|\tloss: 12.3653\n",
      "Training Epoch 9  59.3% | batch:       407 of       686\t|\tloss: 9.8332\n",
      "Training Epoch 9  59.5% | batch:       408 of       686\t|\tloss: 12.012\n",
      "Training Epoch 9  59.6% | batch:       409 of       686\t|\tloss: 13.6967\n",
      "Training Epoch 9  59.8% | batch:       410 of       686\t|\tloss: 11.6702\n",
      "Training Epoch 9  59.9% | batch:       411 of       686\t|\tloss: 7.56042\n",
      "Training Epoch 9  60.1% | batch:       412 of       686\t|\tloss: 11.0775\n",
      "Training Epoch 9  60.2% | batch:       413 of       686\t|\tloss: 11.1922\n",
      "Training Epoch 9  60.3% | batch:       414 of       686\t|\tloss: 9.88044\n",
      "Training Epoch 9  60.5% | batch:       415 of       686\t|\tloss: 11.4223\n",
      "Training Epoch 9  60.6% | batch:       416 of       686\t|\tloss: 7.90571\n",
      "Training Epoch 9  60.8% | batch:       417 of       686\t|\tloss: 11.0297\n",
      "Training Epoch 9  60.9% | batch:       418 of       686\t|\tloss: 10.8376\n",
      "Training Epoch 9  61.1% | batch:       419 of       686\t|\tloss: 10.1053\n",
      "Training Epoch 9  61.2% | batch:       420 of       686\t|\tloss: 8.97829\n",
      "Training Epoch 9  61.4% | batch:       421 of       686\t|\tloss: 10.8382\n",
      "Training Epoch 9  61.5% | batch:       422 of       686\t|\tloss: 13.142\n",
      "Training Epoch 9  61.7% | batch:       423 of       686\t|\tloss: 11.4289\n",
      "Training Epoch 9  61.8% | batch:       424 of       686\t|\tloss: 7.36116\n",
      "Training Epoch 9  62.0% | batch:       425 of       686\t|\tloss: 9.35745\n",
      "Training Epoch 9  62.1% | batch:       426 of       686\t|\tloss: 10.2756\n",
      "Training Epoch 9  62.2% | batch:       427 of       686\t|\tloss: 9.79028\n",
      "Training Epoch 9  62.4% | batch:       428 of       686\t|\tloss: 6.5602\n",
      "Training Epoch 9  62.5% | batch:       429 of       686\t|\tloss: 12.311\n",
      "Training Epoch 9  62.7% | batch:       430 of       686\t|\tloss: 8.12296\n",
      "Training Epoch 9  62.8% | batch:       431 of       686\t|\tloss: 7.31751\n",
      "Training Epoch 9  63.0% | batch:       432 of       686\t|\tloss: 8.83494\n",
      "Training Epoch 9  63.1% | batch:       433 of       686\t|\tloss: 16.7464\n",
      "Training Epoch 9  63.3% | batch:       434 of       686\t|\tloss: 8.65258\n",
      "Training Epoch 9  63.4% | batch:       435 of       686\t|\tloss: 9.34595\n",
      "Training Epoch 9  63.6% | batch:       436 of       686\t|\tloss: 10.1823\n",
      "Training Epoch 9  63.7% | batch:       437 of       686\t|\tloss: 10.787\n",
      "Training Epoch 9  63.8% | batch:       438 of       686\t|\tloss: 10.1913\n",
      "Training Epoch 9  64.0% | batch:       439 of       686\t|\tloss: 13.342\n",
      "Training Epoch 9  64.1% | batch:       440 of       686\t|\tloss: 8.20996\n",
      "Training Epoch 9  64.3% | batch:       441 of       686\t|\tloss: 8.79246\n",
      "Training Epoch 9  64.4% | batch:       442 of       686\t|\tloss: 12.9341\n",
      "Training Epoch 9  64.6% | batch:       443 of       686\t|\tloss: 9.89915\n",
      "Training Epoch 9  64.7% | batch:       444 of       686\t|\tloss: 6.82103\n",
      "Training Epoch 9  64.9% | batch:       445 of       686\t|\tloss: 9.68315\n",
      "Training Epoch 9  65.0% | batch:       446 of       686\t|\tloss: 9.26457\n",
      "Training Epoch 9  65.2% | batch:       447 of       686\t|\tloss: 7.99417\n",
      "Training Epoch 9  65.3% | batch:       448 of       686\t|\tloss: 8.47183\n",
      "Training Epoch 9  65.5% | batch:       449 of       686\t|\tloss: 9.71741\n",
      "Training Epoch 9  65.6% | batch:       450 of       686\t|\tloss: 9.58361\n",
      "Training Epoch 9  65.7% | batch:       451 of       686\t|\tloss: 12.209\n",
      "Training Epoch 9  65.9% | batch:       452 of       686\t|\tloss: 8.77301\n",
      "Training Epoch 9  66.0% | batch:       453 of       686\t|\tloss: 6.74293\n",
      "Training Epoch 9  66.2% | batch:       454 of       686\t|\tloss: 9.64705\n",
      "Training Epoch 9  66.3% | batch:       455 of       686\t|\tloss: 11.3846\n",
      "Training Epoch 9  66.5% | batch:       456 of       686\t|\tloss: 8.72827\n",
      "Training Epoch 9  66.6% | batch:       457 of       686\t|\tloss: 8.0447\n",
      "Training Epoch 9  66.8% | batch:       458 of       686\t|\tloss: 9.16994\n",
      "Training Epoch 9  66.9% | batch:       459 of       686\t|\tloss: 10.1469\n",
      "Training Epoch 9  67.1% | batch:       460 of       686\t|\tloss: 11.1241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  67.2% | batch:       461 of       686\t|\tloss: 11.385\n",
      "Training Epoch 9  67.3% | batch:       462 of       686\t|\tloss: 7.37682\n",
      "Training Epoch 9  67.5% | batch:       463 of       686\t|\tloss: 9.26911\n",
      "Training Epoch 9  67.6% | batch:       464 of       686\t|\tloss: 6.11251\n",
      "Training Epoch 9  67.8% | batch:       465 of       686\t|\tloss: 10.0547\n",
      "Training Epoch 9  67.9% | batch:       466 of       686\t|\tloss: 8.87942\n",
      "Training Epoch 9  68.1% | batch:       467 of       686\t|\tloss: 9.14951\n",
      "Training Epoch 9  68.2% | batch:       468 of       686\t|\tloss: 7.67492\n",
      "Training Epoch 9  68.4% | batch:       469 of       686\t|\tloss: 10.8138\n",
      "Training Epoch 9  68.5% | batch:       470 of       686\t|\tloss: 8.83616\n",
      "Training Epoch 9  68.7% | batch:       471 of       686\t|\tloss: 10.2238\n",
      "Training Epoch 9  68.8% | batch:       472 of       686\t|\tloss: 10.9105\n",
      "Training Epoch 9  69.0% | batch:       473 of       686\t|\tloss: 7.00347\n",
      "Training Epoch 9  69.1% | batch:       474 of       686\t|\tloss: 12.9949\n",
      "Training Epoch 9  69.2% | batch:       475 of       686\t|\tloss: 8.16354\n",
      "Training Epoch 9  69.4% | batch:       476 of       686\t|\tloss: 8.55713\n",
      "Training Epoch 9  69.5% | batch:       477 of       686\t|\tloss: 14.0201\n",
      "Training Epoch 9  69.7% | batch:       478 of       686\t|\tloss: 8.21245\n",
      "Training Epoch 9  69.8% | batch:       479 of       686\t|\tloss: 10.0491\n",
      "Training Epoch 9  70.0% | batch:       480 of       686\t|\tloss: 10.4642\n",
      "Training Epoch 9  70.1% | batch:       481 of       686\t|\tloss: 9.63961\n",
      "Training Epoch 9  70.3% | batch:       482 of       686\t|\tloss: 12.2966\n",
      "Training Epoch 9  70.4% | batch:       483 of       686\t|\tloss: 12.1587\n",
      "Training Epoch 9  70.6% | batch:       484 of       686\t|\tloss: 9.10771\n",
      "Training Epoch 9  70.7% | batch:       485 of       686\t|\tloss: 12.8782\n",
      "Training Epoch 9  70.8% | batch:       486 of       686\t|\tloss: 9.63147\n",
      "Training Epoch 9  71.0% | batch:       487 of       686\t|\tloss: 12.0613\n",
      "Training Epoch 9  71.1% | batch:       488 of       686\t|\tloss: 8.30357\n",
      "Training Epoch 9  71.3% | batch:       489 of       686\t|\tloss: 8.99492\n",
      "Training Epoch 9  71.4% | batch:       490 of       686\t|\tloss: 8.68931\n",
      "Training Epoch 9  71.6% | batch:       491 of       686\t|\tloss: 8.658\n",
      "Training Epoch 9  71.7% | batch:       492 of       686\t|\tloss: 7.39739\n",
      "Training Epoch 9  71.9% | batch:       493 of       686\t|\tloss: 10.4065\n",
      "Training Epoch 9  72.0% | batch:       494 of       686\t|\tloss: 8.68526\n",
      "Training Epoch 9  72.2% | batch:       495 of       686\t|\tloss: 8.91057\n",
      "Training Epoch 9  72.3% | batch:       496 of       686\t|\tloss: 8.65473\n",
      "Training Epoch 9  72.4% | batch:       497 of       686\t|\tloss: 9.1378\n",
      "Training Epoch 9  72.6% | batch:       498 of       686\t|\tloss: 8.06526\n",
      "Training Epoch 9  72.7% | batch:       499 of       686\t|\tloss: 8.86044\n",
      "Training Epoch 9  72.9% | batch:       500 of       686\t|\tloss: 9.72947\n",
      "Training Epoch 9  73.0% | batch:       501 of       686\t|\tloss: 8.1703\n",
      "Training Epoch 9  73.2% | batch:       502 of       686\t|\tloss: 10.0124\n",
      "Training Epoch 9  73.3% | batch:       503 of       686\t|\tloss: 10.5402\n",
      "Training Epoch 9  73.5% | batch:       504 of       686\t|\tloss: 8.56823\n",
      "Training Epoch 9  73.6% | batch:       505 of       686\t|\tloss: 11.3412\n",
      "Training Epoch 9  73.8% | batch:       506 of       686\t|\tloss: 10.6868\n",
      "Training Epoch 9  73.9% | batch:       507 of       686\t|\tloss: 9.42877\n",
      "Training Epoch 9  74.1% | batch:       508 of       686\t|\tloss: 8.87944\n",
      "Training Epoch 9  74.2% | batch:       509 of       686\t|\tloss: 11.3469\n",
      "Training Epoch 9  74.3% | batch:       510 of       686\t|\tloss: 10.265\n",
      "Training Epoch 9  74.5% | batch:       511 of       686\t|\tloss: 7.93055\n",
      "Training Epoch 9  74.6% | batch:       512 of       686\t|\tloss: 11.9621\n",
      "Training Epoch 9  74.8% | batch:       513 of       686\t|\tloss: 8.11784\n",
      "Training Epoch 9  74.9% | batch:       514 of       686\t|\tloss: 10.5655\n",
      "Training Epoch 9  75.1% | batch:       515 of       686\t|\tloss: 11.9814\n",
      "Training Epoch 9  75.2% | batch:       516 of       686\t|\tloss: 8.24387\n",
      "Training Epoch 9  75.4% | batch:       517 of       686\t|\tloss: 8.82846\n",
      "Training Epoch 9  75.5% | batch:       518 of       686\t|\tloss: 8.73148\n",
      "Training Epoch 9  75.7% | batch:       519 of       686\t|\tloss: 9.24331\n",
      "Training Epoch 9  75.8% | batch:       520 of       686\t|\tloss: 9.65731\n",
      "Training Epoch 9  75.9% | batch:       521 of       686\t|\tloss: 11.3153\n",
      "Training Epoch 9  76.1% | batch:       522 of       686\t|\tloss: 7.61315\n",
      "Training Epoch 9  76.2% | batch:       523 of       686\t|\tloss: 7.57607\n",
      "Training Epoch 9  76.4% | batch:       524 of       686\t|\tloss: 14.8899\n",
      "Training Epoch 9  76.5% | batch:       525 of       686\t|\tloss: 8.18373\n",
      "Training Epoch 9  76.7% | batch:       526 of       686\t|\tloss: 10.9447\n",
      "Training Epoch 9  76.8% | batch:       527 of       686\t|\tloss: 8.8169\n",
      "Training Epoch 9  77.0% | batch:       528 of       686\t|\tloss: 9.48857\n",
      "Training Epoch 9  77.1% | batch:       529 of       686\t|\tloss: 11.7918\n",
      "Training Epoch 9  77.3% | batch:       530 of       686\t|\tloss: 7.75113\n",
      "Training Epoch 9  77.4% | batch:       531 of       686\t|\tloss: 9.53542\n",
      "Training Epoch 9  77.6% | batch:       532 of       686\t|\tloss: 7.07802\n",
      "Training Epoch 9  77.7% | batch:       533 of       686\t|\tloss: 9.41\n",
      "Training Epoch 9  77.8% | batch:       534 of       686\t|\tloss: 8.89862\n",
      "Training Epoch 9  78.0% | batch:       535 of       686\t|\tloss: 7.35079\n",
      "Training Epoch 9  78.1% | batch:       536 of       686\t|\tloss: 8.92564\n",
      "Training Epoch 9  78.3% | batch:       537 of       686\t|\tloss: 9.53228\n",
      "Training Epoch 9  78.4% | batch:       538 of       686\t|\tloss: 9.45594\n",
      "Training Epoch 9  78.6% | batch:       539 of       686\t|\tloss: 8.64865\n",
      "Training Epoch 9  78.7% | batch:       540 of       686\t|\tloss: 9.24399\n",
      "Training Epoch 9  78.9% | batch:       541 of       686\t|\tloss: 9.90388\n",
      "Training Epoch 9  79.0% | batch:       542 of       686\t|\tloss: 8.52106\n",
      "Training Epoch 9  79.2% | batch:       543 of       686\t|\tloss: 9.36256\n",
      "Training Epoch 9  79.3% | batch:       544 of       686\t|\tloss: 8.90644\n",
      "Training Epoch 9  79.4% | batch:       545 of       686\t|\tloss: 8.27536\n",
      "Training Epoch 9  79.6% | batch:       546 of       686\t|\tloss: 10.2316\n",
      "Training Epoch 9  79.7% | batch:       547 of       686\t|\tloss: 11.4681\n",
      "Training Epoch 9  79.9% | batch:       548 of       686\t|\tloss: 8.58376\n",
      "Training Epoch 9  80.0% | batch:       549 of       686\t|\tloss: 15.9894\n",
      "Training Epoch 9  80.2% | batch:       550 of       686\t|\tloss: 7.96134\n",
      "Training Epoch 9  80.3% | batch:       551 of       686\t|\tloss: 8.56238\n",
      "Training Epoch 9  80.5% | batch:       552 of       686\t|\tloss: 8.41279\n",
      "Training Epoch 9  80.6% | batch:       553 of       686\t|\tloss: 9.79941\n",
      "Training Epoch 9  80.8% | batch:       554 of       686\t|\tloss: 8.1546\n",
      "Training Epoch 9  80.9% | batch:       555 of       686\t|\tloss: 8.90022\n",
      "Training Epoch 9  81.0% | batch:       556 of       686\t|\tloss: 9.52261\n",
      "Training Epoch 9  81.2% | batch:       557 of       686\t|\tloss: 8.42101\n",
      "Training Epoch 9  81.3% | batch:       558 of       686\t|\tloss: 8.1013\n",
      "Training Epoch 9  81.5% | batch:       559 of       686\t|\tloss: 9.49056\n",
      "Training Epoch 9  81.6% | batch:       560 of       686\t|\tloss: 9.80005\n",
      "Training Epoch 9  81.8% | batch:       561 of       686\t|\tloss: 9.10475\n",
      "Training Epoch 9  81.9% | batch:       562 of       686\t|\tloss: 10.8506\n",
      "Training Epoch 9  82.1% | batch:       563 of       686\t|\tloss: 7.88451\n",
      "Training Epoch 9  82.2% | batch:       564 of       686\t|\tloss: 10.3466\n",
      "Training Epoch 9  82.4% | batch:       565 of       686\t|\tloss: 5.75073\n",
      "Training Epoch 9  82.5% | batch:       566 of       686\t|\tloss: 8.65923\n",
      "Training Epoch 9  82.7% | batch:       567 of       686\t|\tloss: 9.82878\n",
      "Training Epoch 9  82.8% | batch:       568 of       686\t|\tloss: 10.236\n",
      "Training Epoch 9  82.9% | batch:       569 of       686\t|\tloss: 8.88873\n",
      "Training Epoch 9  83.1% | batch:       570 of       686\t|\tloss: 6.59886\n",
      "Training Epoch 9  83.2% | batch:       571 of       686\t|\tloss: 10.4278\n",
      "Training Epoch 9  83.4% | batch:       572 of       686\t|\tloss: 9.31318\n",
      "Training Epoch 9  83.5% | batch:       573 of       686\t|\tloss: 8.77936\n",
      "Training Epoch 9  83.7% | batch:       574 of       686\t|\tloss: 9.17745\n",
      "Training Epoch 9  83.8% | batch:       575 of       686\t|\tloss: 8.79547\n",
      "Training Epoch 9  84.0% | batch:       576 of       686\t|\tloss: 6.06357\n",
      "Training Epoch 9  84.1% | batch:       577 of       686\t|\tloss: 9.46272\n",
      "Training Epoch 9  84.3% | batch:       578 of       686\t|\tloss: 8.77858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  84.4% | batch:       579 of       686\t|\tloss: 8.53794\n",
      "Training Epoch 9  84.5% | batch:       580 of       686\t|\tloss: 6.36918\n",
      "Training Epoch 9  84.7% | batch:       581 of       686\t|\tloss: 7.19428\n",
      "Training Epoch 9  84.8% | batch:       582 of       686\t|\tloss: 9.07984\n",
      "Training Epoch 9  85.0% | batch:       583 of       686\t|\tloss: 9.55974\n",
      "Training Epoch 9  85.1% | batch:       584 of       686\t|\tloss: 8.0026\n",
      "Training Epoch 9  85.3% | batch:       585 of       686\t|\tloss: 10.3322\n",
      "Training Epoch 9  85.4% | batch:       586 of       686\t|\tloss: 10.2277\n",
      "Training Epoch 9  85.6% | batch:       587 of       686\t|\tloss: 6.80441\n",
      "Training Epoch 9  85.7% | batch:       588 of       686\t|\tloss: 7.31165\n",
      "Training Epoch 9  85.9% | batch:       589 of       686\t|\tloss: 6.63027\n",
      "Training Epoch 9  86.0% | batch:       590 of       686\t|\tloss: 8.08505\n",
      "Training Epoch 9  86.2% | batch:       591 of       686\t|\tloss: 9.52431\n",
      "Training Epoch 9  86.3% | batch:       592 of       686\t|\tloss: 9.11747\n",
      "Training Epoch 9  86.4% | batch:       593 of       686\t|\tloss: 9.0267\n",
      "Training Epoch 9  86.6% | batch:       594 of       686\t|\tloss: 8.95588\n",
      "Training Epoch 9  86.7% | batch:       595 of       686\t|\tloss: 10.7303\n",
      "Training Epoch 9  86.9% | batch:       596 of       686\t|\tloss: 9.18911\n",
      "Training Epoch 9  87.0% | batch:       597 of       686\t|\tloss: 10.9168\n",
      "Training Epoch 9  87.2% | batch:       598 of       686\t|\tloss: 7.69216\n",
      "Training Epoch 9  87.3% | batch:       599 of       686\t|\tloss: 8.07891\n",
      "Training Epoch 9  87.5% | batch:       600 of       686\t|\tloss: 11.4057\n",
      "Training Epoch 9  87.6% | batch:       601 of       686\t|\tloss: 7.26291\n",
      "Training Epoch 9  87.8% | batch:       602 of       686\t|\tloss: 8.81506\n",
      "Training Epoch 9  87.9% | batch:       603 of       686\t|\tloss: 8.95755\n",
      "Training Epoch 9  88.0% | batch:       604 of       686\t|\tloss: 10.0657\n",
      "Training Epoch 9  88.2% | batch:       605 of       686\t|\tloss: 8.65548\n",
      "Training Epoch 9  88.3% | batch:       606 of       686\t|\tloss: 8.98916\n",
      "Training Epoch 9  88.5% | batch:       607 of       686\t|\tloss: 9.08307\n",
      "Training Epoch 9  88.6% | batch:       608 of       686\t|\tloss: 8.2225\n",
      "Training Epoch 9  88.8% | batch:       609 of       686\t|\tloss: 12.8174\n",
      "Training Epoch 9  88.9% | batch:       610 of       686\t|\tloss: 9.20718\n",
      "Training Epoch 9  89.1% | batch:       611 of       686\t|\tloss: 10.8148\n",
      "Training Epoch 9  89.2% | batch:       612 of       686\t|\tloss: 7.00586\n",
      "Training Epoch 9  89.4% | batch:       613 of       686\t|\tloss: 15.0709\n",
      "Training Epoch 9  89.5% | batch:       614 of       686\t|\tloss: 9.24857\n",
      "Training Epoch 9  89.7% | batch:       615 of       686\t|\tloss: 6.92727\n",
      "Training Epoch 9  89.8% | batch:       616 of       686\t|\tloss: 8.96768\n",
      "Training Epoch 9  89.9% | batch:       617 of       686\t|\tloss: 9.87267\n",
      "Training Epoch 9  90.1% | batch:       618 of       686\t|\tloss: 7.87126\n",
      "Training Epoch 9  90.2% | batch:       619 of       686\t|\tloss: 10.3056\n",
      "Training Epoch 9  90.4% | batch:       620 of       686\t|\tloss: 8.65899\n",
      "Training Epoch 9  90.5% | batch:       621 of       686\t|\tloss: 8.19181\n",
      "Training Epoch 9  90.7% | batch:       622 of       686\t|\tloss: 9.3033\n",
      "Training Epoch 9  90.8% | batch:       623 of       686\t|\tloss: 9.29983\n",
      "Training Epoch 9  91.0% | batch:       624 of       686\t|\tloss: 12.3692\n",
      "Training Epoch 9  91.1% | batch:       625 of       686\t|\tloss: 7.98182\n",
      "Training Epoch 9  91.3% | batch:       626 of       686\t|\tloss: 7.86306\n",
      "Training Epoch 9  91.4% | batch:       627 of       686\t|\tloss: 9.83542\n",
      "Training Epoch 9  91.5% | batch:       628 of       686\t|\tloss: 8.81823\n",
      "Training Epoch 9  91.7% | batch:       629 of       686\t|\tloss: 7.95134\n",
      "Training Epoch 9  91.8% | batch:       630 of       686\t|\tloss: 11.6639\n",
      "Training Epoch 9  92.0% | batch:       631 of       686\t|\tloss: 5.57303\n",
      "Training Epoch 9  92.1% | batch:       632 of       686\t|\tloss: 11.2976\n",
      "Training Epoch 9  92.3% | batch:       633 of       686\t|\tloss: 8.80774\n",
      "Training Epoch 9  92.4% | batch:       634 of       686\t|\tloss: 8.65106\n",
      "Training Epoch 9  92.6% | batch:       635 of       686\t|\tloss: 9.77407\n",
      "Training Epoch 9  92.7% | batch:       636 of       686\t|\tloss: 8.31351\n",
      "Training Epoch 9  92.9% | batch:       637 of       686\t|\tloss: 8.85513\n",
      "Training Epoch 9  93.0% | batch:       638 of       686\t|\tloss: 11.8264\n",
      "Training Epoch 9  93.1% | batch:       639 of       686\t|\tloss: 8.07014\n",
      "Training Epoch 9  93.3% | batch:       640 of       686\t|\tloss: 10.6948\n",
      "Training Epoch 9  93.4% | batch:       641 of       686\t|\tloss: 7.29023\n",
      "Training Epoch 9  93.6% | batch:       642 of       686\t|\tloss: 11.3903\n",
      "Training Epoch 9  93.7% | batch:       643 of       686\t|\tloss: 9.69482\n",
      "Training Epoch 9  93.9% | batch:       644 of       686\t|\tloss: 6.66033\n",
      "Training Epoch 9  94.0% | batch:       645 of       686\t|\tloss: 8.94743\n",
      "Training Epoch 9  94.2% | batch:       646 of       686\t|\tloss: 12.9243\n",
      "Training Epoch 9  94.3% | batch:       647 of       686\t|\tloss: 10.0346\n",
      "Training Epoch 9  94.5% | batch:       648 of       686\t|\tloss: 8.20101\n",
      "Training Epoch 9  94.6% | batch:       649 of       686\t|\tloss: 11.4859\n",
      "Training Epoch 9  94.8% | batch:       650 of       686\t|\tloss: 10.0132\n",
      "Training Epoch 9  94.9% | batch:       651 of       686\t|\tloss: 8.67182\n",
      "Training Epoch 9  95.0% | batch:       652 of       686\t|\tloss: 8.86193\n",
      "Training Epoch 9  95.2% | batch:       653 of       686\t|\tloss: 9.36848\n",
      "Training Epoch 9  95.3% | batch:       654 of       686\t|\tloss: 9.70732\n",
      "Training Epoch 9  95.5% | batch:       655 of       686\t|\tloss: 11.4705\n",
      "Training Epoch 9  95.6% | batch:       656 of       686\t|\tloss: 10.4427\n",
      "Training Epoch 9  95.8% | batch:       657 of       686\t|\tloss: 9.99171\n",
      "Training Epoch 9  95.9% | batch:       658 of       686\t|\tloss: 10.161\n",
      "Training Epoch 9  96.1% | batch:       659 of       686\t|\tloss: 9.58958\n",
      "Training Epoch 9  96.2% | batch:       660 of       686\t|\tloss: 17.2004\n",
      "Training Epoch 9  96.4% | batch:       661 of       686\t|\tloss: 9.32236\n",
      "Training Epoch 9  96.5% | batch:       662 of       686\t|\tloss: 7.39329\n",
      "Training Epoch 9  96.6% | batch:       663 of       686\t|\tloss: 10.4762\n",
      "Training Epoch 9  96.8% | batch:       664 of       686\t|\tloss: 8.25457\n",
      "Training Epoch 9  96.9% | batch:       665 of       686\t|\tloss: 7.61906\n",
      "Training Epoch 9  97.1% | batch:       666 of       686\t|\tloss: 9.38006\n",
      "Training Epoch 9  97.2% | batch:       667 of       686\t|\tloss: 7.30577\n",
      "Training Epoch 9  97.4% | batch:       668 of       686\t|\tloss: 9.57583\n",
      "Training Epoch 9  97.5% | batch:       669 of       686\t|\tloss: 8.80536\n",
      "Training Epoch 9  97.7% | batch:       670 of       686\t|\tloss: 7.21485\n",
      "Training Epoch 9  97.8% | batch:       671 of       686\t|\tloss: 12.0399\n",
      "Training Epoch 9  98.0% | batch:       672 of       686\t|\tloss: 9.77964\n",
      "Training Epoch 9  98.1% | batch:       673 of       686\t|\tloss: 6.17311\n",
      "Training Epoch 9  98.3% | batch:       674 of       686\t|\tloss: 8.71735\n",
      "Training Epoch 9  98.4% | batch:       675 of       686\t|\tloss: 8.08846\n",
      "Training Epoch 9  98.5% | batch:       676 of       686\t|\tloss: 8.13726\n",
      "Training Epoch 9  98.7% | batch:       677 of       686\t|\tloss: 8.00799\n",
      "Training Epoch 9  98.8% | batch:       678 of       686\t|\tloss: 9.7584\n",
      "Training Epoch 9  99.0% | batch:       679 of       686\t|\tloss: 6.60465\n",
      "Training Epoch 9  99.1% | batch:       680 of       686\t|\tloss: 9.84964\n",
      "Training Epoch 9  99.3% | batch:       681 of       686\t|\tloss: 8.78332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:03:57,002 | INFO : Epoch 9 Training Summary: epoch: 9.000000 | loss: 9.938496 | \n",
      "2023-05-25 22:03:57,004 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 24.007866144180298 seconds\n",
      "\n",
      "2023-05-25 22:03:57,005 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.66349607043796 seconds\n",
      "2023-05-25 22:03:57,005 | INFO : Avg batch train. time: 0.034494892230959126 seconds\n",
      "2023-05-25 22:03:57,008 | INFO : Avg sample train. time: 0.00026983860049532995 seconds\n",
      "2023-05-25 22:03:57,011 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  99.4% | batch:       682 of       686\t|\tloss: 9.03123\n",
      "Training Epoch 9  99.6% | batch:       683 of       686\t|\tloss: 8.53054\n",
      "Training Epoch 9  99.7% | batch:       684 of       686\t|\tloss: 10.5693\n",
      "Training Epoch 9  99.9% | batch:       685 of       686\t|\tloss: 11.6379\n",
      "\n",
      "Evaluating Epoch 9   0.0% | batch:         0 of       172\t|\tloss: 2.11278\n",
      "Evaluating Epoch 9   0.6% | batch:         1 of       172\t|\tloss: 2.73821\n",
      "Evaluating Epoch 9   1.2% | batch:         2 of       172\t|\tloss: 2.15109\n",
      "Evaluating Epoch 9   1.7% | batch:         3 of       172\t|\tloss: 3.90279\n",
      "Evaluating Epoch 9   2.3% | batch:         4 of       172\t|\tloss: 2.26576\n",
      "Evaluating Epoch 9   2.9% | batch:         5 of       172\t|\tloss: 1.70883\n",
      "Evaluating Epoch 9   3.5% | batch:         6 of       172\t|\tloss: 2.7057\n",
      "Evaluating Epoch 9   4.1% | batch:         7 of       172\t|\tloss: 4.00518\n",
      "Evaluating Epoch 9   4.7% | batch:         8 of       172\t|\tloss: 2.00676\n",
      "Evaluating Epoch 9   5.2% | batch:         9 of       172\t|\tloss: 2.6318\n",
      "Evaluating Epoch 9   5.8% | batch:        10 of       172\t|\tloss: 3.1861\n",
      "Evaluating Epoch 9   6.4% | batch:        11 of       172\t|\tloss: 2.52801\n",
      "Evaluating Epoch 9   7.0% | batch:        12 of       172\t|\tloss: 1.82542\n",
      "Evaluating Epoch 9   7.6% | batch:        13 of       172\t|\tloss: 2.93714\n",
      "Evaluating Epoch 9   8.1% | batch:        14 of       172\t|\tloss: 2.89887\n",
      "Evaluating Epoch 9   8.7% | batch:        15 of       172\t|\tloss: 2.08828\n",
      "Evaluating Epoch 9   9.3% | batch:        16 of       172\t|\tloss: 3.14038\n",
      "Evaluating Epoch 9   9.9% | batch:        17 of       172\t|\tloss: 2.10932\n",
      "Evaluating Epoch 9  10.5% | batch:        18 of       172\t|\tloss: 10.9419\n",
      "Evaluating Epoch 9  11.0% | batch:        19 of       172\t|\tloss: 1.36745\n",
      "Evaluating Epoch 9  11.6% | batch:        20 of       172\t|\tloss: 2.2764\n",
      "Evaluating Epoch 9  12.2% | batch:        21 of       172\t|\tloss: 0.759238\n",
      "Evaluating Epoch 9  12.8% | batch:        22 of       172\t|\tloss: 1.50175\n",
      "Evaluating Epoch 9  13.4% | batch:        23 of       172\t|\tloss: 1.59501\n",
      "Evaluating Epoch 9  14.0% | batch:        24 of       172\t|\tloss: 1.3688\n",
      "Evaluating Epoch 9  14.5% | batch:        25 of       172\t|\tloss: 2.30354\n",
      "Evaluating Epoch 9  15.1% | batch:        26 of       172\t|\tloss: 5.42152\n",
      "Evaluating Epoch 9  15.7% | batch:        27 of       172\t|\tloss: 11.4641\n",
      "Evaluating Epoch 9  16.3% | batch:        28 of       172\t|\tloss: 0.257088\n",
      "Evaluating Epoch 9  16.9% | batch:        29 of       172\t|\tloss: 1.73633\n",
      "Evaluating Epoch 9  17.4% | batch:        30 of       172\t|\tloss: 1.17893\n",
      "Evaluating Epoch 9  18.0% | batch:        31 of       172\t|\tloss: 2.49477\n",
      "Evaluating Epoch 9  18.6% | batch:        32 of       172\t|\tloss: 0.467341\n",
      "Evaluating Epoch 9  19.2% | batch:        33 of       172\t|\tloss: 1.21309\n",
      "Evaluating Epoch 9  19.8% | batch:        34 of       172\t|\tloss: 0.395512\n",
      "Evaluating Epoch 9  20.3% | batch:        35 of       172\t|\tloss: 0.642361\n",
      "Evaluating Epoch 9  20.9% | batch:        36 of       172\t|\tloss: 4.51209\n",
      "Evaluating Epoch 9  21.5% | batch:        37 of       172\t|\tloss: 3.44798\n",
      "Evaluating Epoch 9  22.1% | batch:        38 of       172\t|\tloss: 1.92774\n",
      "Evaluating Epoch 9  22.7% | batch:        39 of       172\t|\tloss: 3.7838\n",
      "Evaluating Epoch 9  23.3% | batch:        40 of       172\t|\tloss: 0.344085\n",
      "Evaluating Epoch 9  23.8% | batch:        41 of       172\t|\tloss: 1.27731\n",
      "Evaluating Epoch 9  24.4% | batch:        42 of       172\t|\tloss: 0.622214\n",
      "Evaluating Epoch 9  25.0% | batch:        43 of       172\t|\tloss: 12.1999\n",
      "Evaluating Epoch 9  25.6% | batch:        44 of       172\t|\tloss: 1.23495\n",
      "Evaluating Epoch 9  26.2% | batch:        45 of       172\t|\tloss: 0.982871\n",
      "Evaluating Epoch 9  26.7% | batch:        46 of       172\t|\tloss: 0.598577\n",
      "Evaluating Epoch 9  27.3% | batch:        47 of       172\t|\tloss: 2.41639\n",
      "Evaluating Epoch 9  27.9% | batch:        48 of       172\t|\tloss: 0.492091\n",
      "Evaluating Epoch 9  28.5% | batch:        49 of       172\t|\tloss: 1.89188\n",
      "Evaluating Epoch 9  29.1% | batch:        50 of       172\t|\tloss: 0.465501\n",
      "Evaluating Epoch 9  29.7% | batch:        51 of       172\t|\tloss: 0.644631\n",
      "Evaluating Epoch 9  30.2% | batch:        52 of       172\t|\tloss: 1.93358\n",
      "Evaluating Epoch 9  30.8% | batch:        53 of       172\t|\tloss: 1.14297\n",
      "Evaluating Epoch 9  31.4% | batch:        54 of       172\t|\tloss: 1.1828\n",
      "Evaluating Epoch 9  32.0% | batch:        55 of       172\t|\tloss: 2.47346\n",
      "Evaluating Epoch 9  32.6% | batch:        56 of       172\t|\tloss: 1.66467\n",
      "Evaluating Epoch 9  33.1% | batch:        57 of       172\t|\tloss: 3.00843\n",
      "Evaluating Epoch 9  33.7% | batch:        58 of       172\t|\tloss: 0.855796\n",
      "Evaluating Epoch 9  34.3% | batch:        59 of       172\t|\tloss: 2.83799\n",
      "Evaluating Epoch 9  34.9% | batch:        60 of       172\t|\tloss: 0.555649\n",
      "Evaluating Epoch 9  35.5% | batch:        61 of       172\t|\tloss: 3.63372\n",
      "Evaluating Epoch 9  36.0% | batch:        62 of       172\t|\tloss: 0.845175\n",
      "Evaluating Epoch 9  36.6% | batch:        63 of       172\t|\tloss: 1.41316\n",
      "Evaluating Epoch 9  37.2% | batch:        64 of       172\t|\tloss: 2.05904\n",
      "Evaluating Epoch 9  37.8% | batch:        65 of       172\t|\tloss: 0.969769\n",
      "Evaluating Epoch 9  38.4% | batch:        66 of       172\t|\tloss: 2.80059\n",
      "Evaluating Epoch 9  39.0% | batch:        67 of       172\t|\tloss: 1.29981\n",
      "Evaluating Epoch 9  39.5% | batch:        68 of       172\t|\tloss: 2.10685\n",
      "Evaluating Epoch 9  40.1% | batch:        69 of       172\t|\tloss: 4.00658\n",
      "Evaluating Epoch 9  40.7% | batch:        70 of       172\t|\tloss: 0.456095\n",
      "Evaluating Epoch 9  41.3% | batch:        71 of       172\t|\tloss: 1.24573\n",
      "Evaluating Epoch 9  41.9% | batch:        72 of       172\t|\tloss: 1.97315\n",
      "Evaluating Epoch 9  42.4% | batch:        73 of       172\t|\tloss: 1.05072\n",
      "Evaluating Epoch 9  43.0% | batch:        74 of       172\t|\tloss: 0.262766\n",
      "Evaluating Epoch 9  43.6% | batch:        75 of       172\t|\tloss: 0.299526\n",
      "Evaluating Epoch 9  44.2% | batch:        76 of       172\t|\tloss: 0.176054\n",
      "Evaluating Epoch 9  44.8% | batch:        77 of       172\t|\tloss: 0.260165\n",
      "Evaluating Epoch 9  45.3% | batch:        78 of       172\t|\tloss: 0.372048\n",
      "Evaluating Epoch 9  45.9% | batch:        79 of       172\t|\tloss: 0.250886\n",
      "Evaluating Epoch 9  46.5% | batch:        80 of       172\t|\tloss: 0.331046\n",
      "Evaluating Epoch 9  47.1% | batch:        81 of       172\t|\tloss: 0.286191\n",
      "Evaluating Epoch 9  47.7% | batch:        82 of       172\t|\tloss: 0.334025\n",
      "Evaluating Epoch 9  48.3% | batch:        83 of       172\t|\tloss: 0.578235\n",
      "Evaluating Epoch 9  48.8% | batch:        84 of       172\t|\tloss: 0.751503\n",
      "Evaluating Epoch 9  49.4% | batch:        85 of       172\t|\tloss: 1.28354\n",
      "Evaluating Epoch 9  50.0% | batch:        86 of       172\t|\tloss: 0.915615\n",
      "Evaluating Epoch 9  50.6% | batch:        87 of       172\t|\tloss: 0.599632\n",
      "Evaluating Epoch 9  51.2% | batch:        88 of       172\t|\tloss: 1.0952\n",
      "Evaluating Epoch 9  51.7% | batch:        89 of       172\t|\tloss: 1.55493\n",
      "Evaluating Epoch 9  52.3% | batch:        90 of       172\t|\tloss: 0.95365\n",
      "Evaluating Epoch 9  52.9% | batch:        91 of       172\t|\tloss: 1.44089\n",
      "Evaluating Epoch 9  53.5% | batch:        92 of       172\t|\tloss: 1.65384\n",
      "Evaluating Epoch 9  54.1% | batch:        93 of       172\t|\tloss: 0.990619\n",
      "Evaluating Epoch 9  54.7% | batch:        94 of       172\t|\tloss: 1.22801\n",
      "Evaluating Epoch 9  55.2% | batch:        95 of       172\t|\tloss: 1.23535\n",
      "Evaluating Epoch 9  55.8% | batch:        96 of       172\t|\tloss: 1.39111\n",
      "Evaluating Epoch 9  56.4% | batch:        97 of       172\t|\tloss: 0.860466\n",
      "Evaluating Epoch 9  57.0% | batch:        98 of       172\t|\tloss: 1.23877\n",
      "Evaluating Epoch 9  57.6% | batch:        99 of       172\t|\tloss: 1.95226\n",
      "Evaluating Epoch 9  58.1% | batch:       100 of       172\t|\tloss: 0.689641\n",
      "Evaluating Epoch 9  58.7% | batch:       101 of       172\t|\tloss: 0.637532\n",
      "Evaluating Epoch 9  59.3% | batch:       102 of       172\t|\tloss: 1.47384\n",
      "Evaluating Epoch 9  59.9% | batch:       103 of       172\t|\tloss: 1.27387\n",
      "Evaluating Epoch 9  60.5% | batch:       104 of       172\t|\tloss: 0.819742\n",
      "Evaluating Epoch 9  61.0% | batch:       105 of       172\t|\tloss: 1.09565\n",
      "Evaluating Epoch 9  61.6% | batch:       106 of       172\t|\tloss: 1.91125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 9  62.2% | batch:       107 of       172\t|\tloss: 1.05719\n",
      "Evaluating Epoch 9  62.8% | batch:       108 of       172\t|\tloss: 0.992369\n",
      "Evaluating Epoch 9  63.4% | batch:       109 of       172\t|\tloss: 1.61642\n",
      "Evaluating Epoch 9  64.0% | batch:       110 of       172\t|\tloss: 1.65529\n",
      "Evaluating Epoch 9  64.5% | batch:       111 of       172\t|\tloss: 0.883839\n",
      "Evaluating Epoch 9  65.1% | batch:       112 of       172\t|\tloss: 0.643518\n",
      "Evaluating Epoch 9  65.7% | batch:       113 of       172\t|\tloss: 1.14588\n",
      "Evaluating Epoch 9  66.3% | batch:       114 of       172\t|\tloss: 1.37307\n",
      "Evaluating Epoch 9  66.9% | batch:       115 of       172\t|\tloss: 0.681667\n",
      "Evaluating Epoch 9  67.4% | batch:       116 of       172\t|\tloss: 0.853205\n",
      "Evaluating Epoch 9  68.0% | batch:       117 of       172\t|\tloss: 0.780369\n",
      "Evaluating Epoch 9  68.6% | batch:       118 of       172\t|\tloss: 0.758263\n",
      "Evaluating Epoch 9  69.2% | batch:       119 of       172\t|\tloss: 0.855331\n",
      "Evaluating Epoch 9  69.8% | batch:       120 of       172\t|\tloss: 0.750157\n",
      "Evaluating Epoch 9  70.3% | batch:       121 of       172\t|\tloss: 2.06021\n",
      "Evaluating Epoch 9  70.9% | batch:       122 of       172\t|\tloss: 1.58199\n",
      "Evaluating Epoch 9  71.5% | batch:       123 of       172\t|\tloss: 3.19813\n",
      "Evaluating Epoch 9  72.1% | batch:       124 of       172\t|\tloss: 12.1319\n",
      "Evaluating Epoch 9  72.7% | batch:       125 of       172\t|\tloss: 1.19701\n",
      "Evaluating Epoch 9  73.3% | batch:       126 of       172\t|\tloss: 0.884188\n",
      "Evaluating Epoch 9  73.8% | batch:       127 of       172\t|\tloss: 0.821893\n",
      "Evaluating Epoch 9  74.4% | batch:       128 of       172\t|\tloss: 0.990568\n",
      "Evaluating Epoch 9  75.0% | batch:       129 of       172\t|\tloss: 0.787259\n",
      "Evaluating Epoch 9  75.6% | batch:       130 of       172\t|\tloss: 0.806638\n",
      "Evaluating Epoch 9  76.2% | batch:       131 of       172\t|\tloss: 1.37925\n",
      "Evaluating Epoch 9  76.7% | batch:       132 of       172\t|\tloss: 0.821598\n",
      "Evaluating Epoch 9  77.3% | batch:       133 of       172\t|\tloss: 0.358301\n",
      "Evaluating Epoch 9  77.9% | batch:       134 of       172\t|\tloss: 0.639293\n",
      "Evaluating Epoch 9  78.5% | batch:       135 of       172\t|\tloss: 0.245802\n",
      "Evaluating Epoch 9  79.1% | batch:       136 of       172\t|\tloss: 0.407202\n",
      "Evaluating Epoch 9  79.7% | batch:       137 of       172\t|\tloss: 0.212371\n",
      "Evaluating Epoch 9  80.2% | batch:       138 of       172\t|\tloss: 0.791851\n",
      "Evaluating Epoch 9  80.8% | batch:       139 of       172\t|\tloss: 0.656064\n",
      "Evaluating Epoch 9  81.4% | batch:       140 of       172\t|\tloss: 0.489286\n",
      "Evaluating Epoch 9  82.0% | batch:       141 of       172\t|\tloss: 0.336377\n",
      "Evaluating Epoch 9  82.6% | batch:       142 of       172\t|\tloss: 0.440291\n",
      "Evaluating Epoch 9  83.1% | batch:       143 of       172\t|\tloss: 0.323822\n",
      "Evaluating Epoch 9  83.7% | batch:       144 of       172\t|\tloss: 0.5564\n",
      "Evaluating Epoch 9  84.3% | batch:       145 of       172\t|\tloss: 0.239765\n",
      "Evaluating Epoch 9  84.9% | batch:       146 of       172\t|\tloss: 0.516455\n",
      "Evaluating Epoch 9  85.5% | batch:       147 of       172\t|\tloss: 0.264664\n",
      "Evaluating Epoch 9  86.0% | batch:       148 of       172\t|\tloss: 0.365281\n",
      "Evaluating Epoch 9  86.6% | batch:       149 of       172\t|\tloss: 0.313654\n",
      "Evaluating Epoch 9  87.2% | batch:       150 of       172\t|\tloss: 0.625603\n",
      "Evaluating Epoch 9  87.8% | batch:       151 of       172\t|\tloss: 0.599604\n",
      "Evaluating Epoch 9  88.4% | batch:       152 of       172\t|\tloss: 0.579445\n",
      "Evaluating Epoch 9  89.0% | batch:       153 of       172\t|\tloss: 0.610522\n",
      "Evaluating Epoch 9  89.5% | batch:       154 of       172\t|\tloss: 0.645783\n",
      "Evaluating Epoch 9  90.1% | batch:       155 of       172\t|\tloss: 0.902612\n",
      "Evaluating Epoch 9  90.7% | batch:       156 of       172\t|\tloss: 0.761751\n",
      "Evaluating Epoch 9  91.3% | batch:       157 of       172\t|\tloss: 0.833657\n",
      "Evaluating Epoch 9  91.9% | batch:       158 of       172\t|\tloss: 0.573312\n",
      "Evaluating Epoch 9  92.4% | batch:       159 of       172\t|\tloss: 1.01696\n",
      "Evaluating Epoch 9  93.0% | batch:       160 of       172\t|\tloss: 3.3694\n",
      "Evaluating Epoch 9  93.6% | batch:       161 of       172\t|\tloss: 2.46501\n",
      "Evaluating Epoch 9  94.2% | batch:       162 of       172\t|\tloss: 0.758374\n",
      "Evaluating Epoch 9  94.8% | batch:       163 of       172\t|\tloss: 0.680823\n",
      "Evaluating Epoch 9  95.3% | batch:       164 of       172\t|\tloss: 0.872934\n",
      "Evaluating Epoch 9  95.9% | batch:       165 of       172\t|\tloss: 0.598348\n",
      "Evaluating Epoch 9  96.5% | batch:       166 of       172\t|\tloss: 0.524371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:04:01,772 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 4.759536266326904 seconds\n",
      "\n",
      "2023-05-25 22:04:01,772 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.092628622055054 seconds\n",
      "2023-05-25 22:04:01,773 | INFO : Avg batch val. time: 0.023794352453808454 seconds\n",
      "2023-05-25 22:04:01,773 | INFO : Avg sample val. time: 0.00018639288710001613 seconds\n",
      "2023-05-25 22:04:01,774 | INFO : Epoch 9 Validation Summary: epoch: 9.000000 | loss: 1.564719 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 9  97.1% | batch:       167 of       172\t|\tloss: 0.891525\n",
      "Evaluating Epoch 9  97.7% | batch:       168 of       172\t|\tloss: 0.524622\n",
      "Evaluating Epoch 9  98.3% | batch:       169 of       172\t|\tloss: 0.694632\n",
      "Evaluating Epoch 9  98.8% | batch:       170 of       172\t|\tloss: 0.759165\n",
      "Evaluating Epoch 9  99.4% | batch:       171 of       172\t|\tloss: 0.695178\n",
      "\n",
      "Training Epoch 10   0.0% | batch:         0 of       686\t|\tloss: 7.93648\n",
      "Training Epoch 10   0.1% | batch:         1 of       686\t|\tloss: 8.91723\n",
      "Training Epoch 10   0.3% | batch:         2 of       686\t|\tloss: 8.60251\n",
      "Training Epoch 10   0.4% | batch:         3 of       686\t|\tloss: 6.9577\n",
      "Training Epoch 10   0.6% | batch:         4 of       686\t|\tloss: 7.35359\n",
      "Training Epoch 10   0.7% | batch:         5 of       686\t|\tloss: 8.78204\n",
      "Training Epoch 10   0.9% | batch:         6 of       686\t|\tloss: 8.45895\n",
      "Training Epoch 10   1.0% | batch:         7 of       686\t|\tloss: 8.04341\n",
      "Training Epoch 10   1.2% | batch:         8 of       686\t|\tloss: 8.79894\n",
      "Training Epoch 10   1.3% | batch:         9 of       686\t|\tloss: 9.18565\n",
      "Training Epoch 10   1.5% | batch:        10 of       686\t|\tloss: 8.81415\n",
      "Training Epoch 10   1.6% | batch:        11 of       686\t|\tloss: 7.85696\n",
      "Training Epoch 10   1.7% | batch:        12 of       686\t|\tloss: 11.3863\n",
      "Training Epoch 10   1.9% | batch:        13 of       686\t|\tloss: 7.25642\n",
      "Training Epoch 10   2.0% | batch:        14 of       686\t|\tloss: 8.42503\n",
      "Training Epoch 10   2.2% | batch:        15 of       686\t|\tloss: 8.8799\n",
      "Training Epoch 10   2.3% | batch:        16 of       686\t|\tloss: 12.935\n",
      "Training Epoch 10   2.5% | batch:        17 of       686\t|\tloss: 7.46574\n",
      "Training Epoch 10   2.6% | batch:        18 of       686\t|\tloss: 9.22596\n",
      "Training Epoch 10   2.8% | batch:        19 of       686\t|\tloss: 6.62867\n",
      "Training Epoch 10   2.9% | batch:        20 of       686\t|\tloss: 7.63433\n",
      "Training Epoch 10   3.1% | batch:        21 of       686\t|\tloss: 10.8881\n",
      "Training Epoch 10   3.2% | batch:        22 of       686\t|\tloss: 7.47787\n",
      "Training Epoch 10   3.4% | batch:        23 of       686\t|\tloss: 9.3491\n",
      "Training Epoch 10   3.5% | batch:        24 of       686\t|\tloss: 8.12181\n",
      "Training Epoch 10   3.6% | batch:        25 of       686\t|\tloss: 7.2243\n",
      "Training Epoch 10   3.8% | batch:        26 of       686\t|\tloss: 12.9773\n",
      "Training Epoch 10   3.9% | batch:        27 of       686\t|\tloss: 8.83712\n",
      "Training Epoch 10   4.1% | batch:        28 of       686\t|\tloss: 10.2684\n",
      "Training Epoch 10   4.2% | batch:        29 of       686\t|\tloss: 8.78282\n",
      "Training Epoch 10   4.4% | batch:        30 of       686\t|\tloss: 9.11611\n",
      "Training Epoch 10   4.5% | batch:        31 of       686\t|\tloss: 12.9594\n",
      "Training Epoch 10   4.7% | batch:        32 of       686\t|\tloss: 10.4617\n",
      "Training Epoch 10   4.8% | batch:        33 of       686\t|\tloss: 9.20882\n",
      "Training Epoch 10   5.0% | batch:        34 of       686\t|\tloss: 8.33771\n",
      "Training Epoch 10   5.1% | batch:        35 of       686\t|\tloss: 8.3127\n",
      "Training Epoch 10   5.2% | batch:        36 of       686\t|\tloss: 8.03716\n",
      "Training Epoch 10   5.4% | batch:        37 of       686\t|\tloss: 8.18239\n",
      "Training Epoch 10   5.5% | batch:        38 of       686\t|\tloss: 9.24765\n",
      "Training Epoch 10   5.7% | batch:        39 of       686\t|\tloss: 8.36797\n",
      "Training Epoch 10   5.8% | batch:        40 of       686\t|\tloss: 8.25689\n",
      "Training Epoch 10   6.0% | batch:        41 of       686\t|\tloss: 8.3941\n",
      "Training Epoch 10   6.1% | batch:        42 of       686\t|\tloss: 7.68534\n",
      "Training Epoch 10   6.3% | batch:        43 of       686\t|\tloss: 8.04476\n",
      "Training Epoch 10   6.4% | batch:        44 of       686\t|\tloss: 8.28343\n",
      "Training Epoch 10   6.6% | batch:        45 of       686\t|\tloss: 9.15339\n",
      "Training Epoch 10   6.7% | batch:        46 of       686\t|\tloss: 7.05265\n",
      "Training Epoch 10   6.9% | batch:        47 of       686\t|\tloss: 7.73132\n",
      "Training Epoch 10   7.0% | batch:        48 of       686\t|\tloss: 11.1029\n",
      "Training Epoch 10   7.1% | batch:        49 of       686\t|\tloss: 8.30874\n",
      "Training Epoch 10   7.3% | batch:        50 of       686\t|\tloss: 8.6898\n",
      "Training Epoch 10   7.4% | batch:        51 of       686\t|\tloss: 9.54794\n",
      "Training Epoch 10   7.6% | batch:        52 of       686\t|\tloss: 8.44765\n",
      "Training Epoch 10   7.7% | batch:        53 of       686\t|\tloss: 8.14386\n",
      "Training Epoch 10   7.9% | batch:        54 of       686\t|\tloss: 10.031\n",
      "Training Epoch 10   8.0% | batch:        55 of       686\t|\tloss: 8.01604\n",
      "Training Epoch 10   8.2% | batch:        56 of       686\t|\tloss: 9.35604\n",
      "Training Epoch 10   8.3% | batch:        57 of       686\t|\tloss: 8.50227\n",
      "Training Epoch 10   8.5% | batch:        58 of       686\t|\tloss: 8.38019\n",
      "Training Epoch 10   8.6% | batch:        59 of       686\t|\tloss: 8.19055\n",
      "Training Epoch 10   8.7% | batch:        60 of       686\t|\tloss: 8.99243\n",
      "Training Epoch 10   8.9% | batch:        61 of       686\t|\tloss: 6.4634\n",
      "Training Epoch 10   9.0% | batch:        62 of       686\t|\tloss: 8.71576\n",
      "Training Epoch 10   9.2% | batch:        63 of       686\t|\tloss: 11.4245\n",
      "Training Epoch 10   9.3% | batch:        64 of       686\t|\tloss: 8.821\n",
      "Training Epoch 10   9.5% | batch:        65 of       686\t|\tloss: 6.36393\n",
      "Training Epoch 10   9.6% | batch:        66 of       686\t|\tloss: 7.88533\n",
      "Training Epoch 10   9.8% | batch:        67 of       686\t|\tloss: 9.50086\n",
      "Training Epoch 10   9.9% | batch:        68 of       686\t|\tloss: 11.5904\n",
      "Training Epoch 10  10.1% | batch:        69 of       686\t|\tloss: 9.2961\n",
      "Training Epoch 10  10.2% | batch:        70 of       686\t|\tloss: 8.81276\n",
      "Training Epoch 10  10.3% | batch:        71 of       686\t|\tloss: 12.2028\n",
      "Training Epoch 10  10.5% | batch:        72 of       686\t|\tloss: 8.36946\n",
      "Training Epoch 10  10.6% | batch:        73 of       686\t|\tloss: 7.4642\n",
      "Training Epoch 10  10.8% | batch:        74 of       686\t|\tloss: 6.78432\n",
      "Training Epoch 10  10.9% | batch:        75 of       686\t|\tloss: 8.92269\n",
      "Training Epoch 10  11.1% | batch:        76 of       686\t|\tloss: 12.5009\n",
      "Training Epoch 10  11.2% | batch:        77 of       686\t|\tloss: 9.24966\n",
      "Training Epoch 10  11.4% | batch:        78 of       686\t|\tloss: 6.76412\n",
      "Training Epoch 10  11.5% | batch:        79 of       686\t|\tloss: 8.27589\n",
      "Training Epoch 10  11.7% | batch:        80 of       686\t|\tloss: 7.72337\n",
      "Training Epoch 10  11.8% | batch:        81 of       686\t|\tloss: 8.02688\n",
      "Training Epoch 10  12.0% | batch:        82 of       686\t|\tloss: 6.61284\n",
      "Training Epoch 10  12.1% | batch:        83 of       686\t|\tloss: 8.86024\n",
      "Training Epoch 10  12.2% | batch:        84 of       686\t|\tloss: 10.3492\n",
      "Training Epoch 10  12.4% | batch:        85 of       686\t|\tloss: 8.73722\n",
      "Training Epoch 10  12.5% | batch:        86 of       686\t|\tloss: 7.35154\n",
      "Training Epoch 10  12.7% | batch:        87 of       686\t|\tloss: 6.75293\n",
      "Training Epoch 10  12.8% | batch:        88 of       686\t|\tloss: 9.51356\n",
      "Training Epoch 10  13.0% | batch:        89 of       686\t|\tloss: 7.17887\n",
      "Training Epoch 10  13.1% | batch:        90 of       686\t|\tloss: 11.0655\n",
      "Training Epoch 10  13.3% | batch:        91 of       686\t|\tloss: 7.27952\n",
      "Training Epoch 10  13.4% | batch:        92 of       686\t|\tloss: 10.2438\n",
      "Training Epoch 10  13.6% | batch:        93 of       686\t|\tloss: 10.2742\n",
      "Training Epoch 10  13.7% | batch:        94 of       686\t|\tloss: 8.96048\n",
      "Training Epoch 10  13.8% | batch:        95 of       686\t|\tloss: 10.5769\n",
      "Training Epoch 10  14.0% | batch:        96 of       686\t|\tloss: 8.14536\n",
      "Training Epoch 10  14.1% | batch:        97 of       686\t|\tloss: 10.1237\n",
      "Training Epoch 10  14.3% | batch:        98 of       686\t|\tloss: 8.06435\n",
      "Training Epoch 10  14.4% | batch:        99 of       686\t|\tloss: 7.82271\n",
      "Training Epoch 10  14.6% | batch:       100 of       686\t|\tloss: 15.6943\n",
      "Training Epoch 10  14.7% | batch:       101 of       686\t|\tloss: 8.22388\n",
      "Training Epoch 10  14.9% | batch:       102 of       686\t|\tloss: 8.7786\n",
      "Training Epoch 10  15.0% | batch:       103 of       686\t|\tloss: 7.22857\n",
      "Training Epoch 10  15.2% | batch:       104 of       686\t|\tloss: 6.89683\n",
      "Training Epoch 10  15.3% | batch:       105 of       686\t|\tloss: 7.99811\n",
      "Training Epoch 10  15.5% | batch:       106 of       686\t|\tloss: 6.39639\n",
      "Training Epoch 10  15.6% | batch:       107 of       686\t|\tloss: 7.05517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  15.7% | batch:       108 of       686\t|\tloss: 8.34231\n",
      "Training Epoch 10  15.9% | batch:       109 of       686\t|\tloss: 6.36848\n",
      "Training Epoch 10  16.0% | batch:       110 of       686\t|\tloss: 11.2084\n",
      "Training Epoch 10  16.2% | batch:       111 of       686\t|\tloss: 9.39218\n",
      "Training Epoch 10  16.3% | batch:       112 of       686\t|\tloss: 10.5424\n",
      "Training Epoch 10  16.5% | batch:       113 of       686\t|\tloss: 7.30938\n",
      "Training Epoch 10  16.6% | batch:       114 of       686\t|\tloss: 7.61302\n",
      "Training Epoch 10  16.8% | batch:       115 of       686\t|\tloss: 10.2734\n",
      "Training Epoch 10  16.9% | batch:       116 of       686\t|\tloss: 9.85566\n",
      "Training Epoch 10  17.1% | batch:       117 of       686\t|\tloss: 8.02999\n",
      "Training Epoch 10  17.2% | batch:       118 of       686\t|\tloss: 7.38258\n",
      "Training Epoch 10  17.3% | batch:       119 of       686\t|\tloss: 7.9966\n",
      "Training Epoch 10  17.5% | batch:       120 of       686\t|\tloss: 8.00786\n",
      "Training Epoch 10  17.6% | batch:       121 of       686\t|\tloss: 8.10827\n",
      "Training Epoch 10  17.8% | batch:       122 of       686\t|\tloss: 9.83747\n",
      "Training Epoch 10  17.9% | batch:       123 of       686\t|\tloss: 8.0875\n",
      "Training Epoch 10  18.1% | batch:       124 of       686\t|\tloss: 7.16556\n",
      "Training Epoch 10  18.2% | batch:       125 of       686\t|\tloss: 10.4551\n",
      "Training Epoch 10  18.4% | batch:       126 of       686\t|\tloss: 8.83626\n",
      "Training Epoch 10  18.5% | batch:       127 of       686\t|\tloss: 8.87394\n",
      "Training Epoch 10  18.7% | batch:       128 of       686\t|\tloss: 6.67531\n",
      "Training Epoch 10  18.8% | batch:       129 of       686\t|\tloss: 9.4132\n",
      "Training Epoch 10  19.0% | batch:       130 of       686\t|\tloss: 8.84872\n",
      "Training Epoch 10  19.1% | batch:       131 of       686\t|\tloss: 8.94811\n",
      "Training Epoch 10  19.2% | batch:       132 of       686\t|\tloss: 8.71251\n",
      "Training Epoch 10  19.4% | batch:       133 of       686\t|\tloss: 9.06796\n",
      "Training Epoch 10  19.5% | batch:       134 of       686\t|\tloss: 8.73934\n",
      "Training Epoch 10  19.7% | batch:       135 of       686\t|\tloss: 8.6894\n",
      "Training Epoch 10  19.8% | batch:       136 of       686\t|\tloss: 8.18846\n",
      "Training Epoch 10  20.0% | batch:       137 of       686\t|\tloss: 10.4693\n",
      "Training Epoch 10  20.1% | batch:       138 of       686\t|\tloss: 8.37228\n",
      "Training Epoch 10  20.3% | batch:       139 of       686\t|\tloss: 8.43679\n",
      "Training Epoch 10  20.4% | batch:       140 of       686\t|\tloss: 10.5241\n",
      "Training Epoch 10  20.6% | batch:       141 of       686\t|\tloss: 8.96983\n",
      "Training Epoch 10  20.7% | batch:       142 of       686\t|\tloss: 8.72512\n",
      "Training Epoch 10  20.8% | batch:       143 of       686\t|\tloss: 8.37016\n",
      "Training Epoch 10  21.0% | batch:       144 of       686\t|\tloss: 10.6388\n",
      "Training Epoch 10  21.1% | batch:       145 of       686\t|\tloss: 6.69069\n",
      "Training Epoch 10  21.3% | batch:       146 of       686\t|\tloss: 10.5787\n",
      "Training Epoch 10  21.4% | batch:       147 of       686\t|\tloss: 5.88781\n",
      "Training Epoch 10  21.6% | batch:       148 of       686\t|\tloss: 8.82987\n",
      "Training Epoch 10  21.7% | batch:       149 of       686\t|\tloss: 5.33828\n",
      "Training Epoch 10  21.9% | batch:       150 of       686\t|\tloss: 7.94476\n",
      "Training Epoch 10  22.0% | batch:       151 of       686\t|\tloss: 6.80292\n",
      "Training Epoch 10  22.2% | batch:       152 of       686\t|\tloss: 7.28807\n",
      "Training Epoch 10  22.3% | batch:       153 of       686\t|\tloss: 10.4942\n",
      "Training Epoch 10  22.4% | batch:       154 of       686\t|\tloss: 9.15718\n",
      "Training Epoch 10  22.6% | batch:       155 of       686\t|\tloss: 7.49449\n",
      "Training Epoch 10  22.7% | batch:       156 of       686\t|\tloss: 7.00355\n",
      "Training Epoch 10  22.9% | batch:       157 of       686\t|\tloss: 7.49903\n",
      "Training Epoch 10  23.0% | batch:       158 of       686\t|\tloss: 8.6519\n",
      "Training Epoch 10  23.2% | batch:       159 of       686\t|\tloss: 9.39662\n",
      "Training Epoch 10  23.3% | batch:       160 of       686\t|\tloss: 6.98366\n",
      "Training Epoch 10  23.5% | batch:       161 of       686\t|\tloss: 11.5542\n",
      "Training Epoch 10  23.6% | batch:       162 of       686\t|\tloss: 7.38418\n",
      "Training Epoch 10  23.8% | batch:       163 of       686\t|\tloss: 9.25629\n",
      "Training Epoch 10  23.9% | batch:       164 of       686\t|\tloss: 7.92574\n",
      "Training Epoch 10  24.1% | batch:       165 of       686\t|\tloss: 8.52634\n",
      "Training Epoch 10  24.2% | batch:       166 of       686\t|\tloss: 7.51596\n",
      "Training Epoch 10  24.3% | batch:       167 of       686\t|\tloss: 10.2953\n",
      "Training Epoch 10  24.5% | batch:       168 of       686\t|\tloss: 6.70994\n",
      "Training Epoch 10  24.6% | batch:       169 of       686\t|\tloss: 8.52436\n",
      "Training Epoch 10  24.8% | batch:       170 of       686\t|\tloss: 7.59623\n",
      "Training Epoch 10  24.9% | batch:       171 of       686\t|\tloss: 10.0071\n",
      "Training Epoch 10  25.1% | batch:       172 of       686\t|\tloss: 8.16528\n",
      "Training Epoch 10  25.2% | batch:       173 of       686\t|\tloss: 9.55781\n",
      "Training Epoch 10  25.4% | batch:       174 of       686\t|\tloss: 8.3893\n",
      "Training Epoch 10  25.5% | batch:       175 of       686\t|\tloss: 9.12988\n",
      "Training Epoch 10  25.7% | batch:       176 of       686\t|\tloss: 7.16104\n",
      "Training Epoch 10  25.8% | batch:       177 of       686\t|\tloss: 8.57182\n",
      "Training Epoch 10  25.9% | batch:       178 of       686\t|\tloss: 5.73055\n",
      "Training Epoch 10  26.1% | batch:       179 of       686\t|\tloss: 6.355\n",
      "Training Epoch 10  26.2% | batch:       180 of       686\t|\tloss: 7.18703\n",
      "Training Epoch 10  26.4% | batch:       181 of       686\t|\tloss: 6.40507\n",
      "Training Epoch 10  26.5% | batch:       182 of       686\t|\tloss: 7.63071\n",
      "Training Epoch 10  26.7% | batch:       183 of       686\t|\tloss: 7.58002\n",
      "Training Epoch 10  26.8% | batch:       184 of       686\t|\tloss: 8.3778\n",
      "Training Epoch 10  27.0% | batch:       185 of       686\t|\tloss: 9.92323\n",
      "Training Epoch 10  27.1% | batch:       186 of       686\t|\tloss: 10.5381\n",
      "Training Epoch 10  27.3% | batch:       187 of       686\t|\tloss: 7.28338\n",
      "Training Epoch 10  27.4% | batch:       188 of       686\t|\tloss: 9.58469\n",
      "Training Epoch 10  27.6% | batch:       189 of       686\t|\tloss: 6.35146\n",
      "Training Epoch 10  27.7% | batch:       190 of       686\t|\tloss: 8.08559\n",
      "Training Epoch 10  27.8% | batch:       191 of       686\t|\tloss: 10.6265\n",
      "Training Epoch 10  28.0% | batch:       192 of       686\t|\tloss: 7.48842\n",
      "Training Epoch 10  28.1% | batch:       193 of       686\t|\tloss: 7.39473\n",
      "Training Epoch 10  28.3% | batch:       194 of       686\t|\tloss: 10.1044\n",
      "Training Epoch 10  28.4% | batch:       195 of       686\t|\tloss: 8.4837\n",
      "Training Epoch 10  28.6% | batch:       196 of       686\t|\tloss: 8.0616\n",
      "Training Epoch 10  28.7% | batch:       197 of       686\t|\tloss: 8.48584\n",
      "Training Epoch 10  28.9% | batch:       198 of       686\t|\tloss: 8.10163\n",
      "Training Epoch 10  29.0% | batch:       199 of       686\t|\tloss: 9.39158\n",
      "Training Epoch 10  29.2% | batch:       200 of       686\t|\tloss: 8.15854\n",
      "Training Epoch 10  29.3% | batch:       201 of       686\t|\tloss: 6.70607\n",
      "Training Epoch 10  29.4% | batch:       202 of       686\t|\tloss: 5.86441\n",
      "Training Epoch 10  29.6% | batch:       203 of       686\t|\tloss: 8.66225\n",
      "Training Epoch 10  29.7% | batch:       204 of       686\t|\tloss: 11.0841\n",
      "Training Epoch 10  29.9% | batch:       205 of       686\t|\tloss: 7.24915\n",
      "Training Epoch 10  30.0% | batch:       206 of       686\t|\tloss: 10.0541\n",
      "Training Epoch 10  30.2% | batch:       207 of       686\t|\tloss: 8.65853\n",
      "Training Epoch 10  30.3% | batch:       208 of       686\t|\tloss: 7.45181\n",
      "Training Epoch 10  30.5% | batch:       209 of       686\t|\tloss: 7.78163\n",
      "Training Epoch 10  30.6% | batch:       210 of       686\t|\tloss: 8.30044\n",
      "Training Epoch 10  30.8% | batch:       211 of       686\t|\tloss: 7.32995\n",
      "Training Epoch 10  30.9% | batch:       212 of       686\t|\tloss: 8.91402\n",
      "Training Epoch 10  31.0% | batch:       213 of       686\t|\tloss: 8.04266\n",
      "Training Epoch 10  31.2% | batch:       214 of       686\t|\tloss: 7.01721\n",
      "Training Epoch 10  31.3% | batch:       215 of       686\t|\tloss: 5.49082\n",
      "Training Epoch 10  31.5% | batch:       216 of       686\t|\tloss: 9.35143\n",
      "Training Epoch 10  31.6% | batch:       217 of       686\t|\tloss: 7.39979\n",
      "Training Epoch 10  31.8% | batch:       218 of       686\t|\tloss: 6.79926\n",
      "Training Epoch 10  31.9% | batch:       219 of       686\t|\tloss: 9.07649\n",
      "Training Epoch 10  32.1% | batch:       220 of       686\t|\tloss: 6.98863\n",
      "Training Epoch 10  32.2% | batch:       221 of       686\t|\tloss: 7.62209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  32.4% | batch:       222 of       686\t|\tloss: 10.326\n",
      "Training Epoch 10  32.5% | batch:       223 of       686\t|\tloss: 9.8725\n",
      "Training Epoch 10  32.7% | batch:       224 of       686\t|\tloss: 8.20376\n",
      "Training Epoch 10  32.8% | batch:       225 of       686\t|\tloss: 8.83568\n",
      "Training Epoch 10  32.9% | batch:       226 of       686\t|\tloss: 7.57614\n",
      "Training Epoch 10  33.1% | batch:       227 of       686\t|\tloss: 8.06566\n",
      "Training Epoch 10  33.2% | batch:       228 of       686\t|\tloss: 7.63626\n",
      "Training Epoch 10  33.4% | batch:       229 of       686\t|\tloss: 7.94098\n",
      "Training Epoch 10  33.5% | batch:       230 of       686\t|\tloss: 8.03407\n",
      "Training Epoch 10  33.7% | batch:       231 of       686\t|\tloss: 10.2943\n",
      "Training Epoch 10  33.8% | batch:       232 of       686\t|\tloss: 8.21478\n",
      "Training Epoch 10  34.0% | batch:       233 of       686\t|\tloss: 8.77781\n",
      "Training Epoch 10  34.1% | batch:       234 of       686\t|\tloss: 7.40593\n",
      "Training Epoch 10  34.3% | batch:       235 of       686\t|\tloss: 9.34777\n",
      "Training Epoch 10  34.4% | batch:       236 of       686\t|\tloss: 9.44822\n",
      "Training Epoch 10  34.5% | batch:       237 of       686\t|\tloss: 8.18943\n",
      "Training Epoch 10  34.7% | batch:       238 of       686\t|\tloss: 8.65937\n",
      "Training Epoch 10  34.8% | batch:       239 of       686\t|\tloss: 8.62644\n",
      "Training Epoch 10  35.0% | batch:       240 of       686\t|\tloss: 7.71438\n",
      "Training Epoch 10  35.1% | batch:       241 of       686\t|\tloss: 10.5169\n",
      "Training Epoch 10  35.3% | batch:       242 of       686\t|\tloss: 7.88125\n",
      "Training Epoch 10  35.4% | batch:       243 of       686\t|\tloss: 9.16228\n",
      "Training Epoch 10  35.6% | batch:       244 of       686\t|\tloss: 7.08723\n",
      "Training Epoch 10  35.7% | batch:       245 of       686\t|\tloss: 5.36097\n",
      "Training Epoch 10  35.9% | batch:       246 of       686\t|\tloss: 7.00684\n",
      "Training Epoch 10  36.0% | batch:       247 of       686\t|\tloss: 8.91462\n",
      "Training Epoch 10  36.2% | batch:       248 of       686\t|\tloss: 8.6186\n",
      "Training Epoch 10  36.3% | batch:       249 of       686\t|\tloss: 7.34518\n",
      "Training Epoch 10  36.4% | batch:       250 of       686\t|\tloss: 7.95783\n",
      "Training Epoch 10  36.6% | batch:       251 of       686\t|\tloss: 7.50439\n",
      "Training Epoch 10  36.7% | batch:       252 of       686\t|\tloss: 8.62109\n",
      "Training Epoch 10  36.9% | batch:       253 of       686\t|\tloss: 8.13343\n",
      "Training Epoch 10  37.0% | batch:       254 of       686\t|\tloss: 9.6389\n",
      "Training Epoch 10  37.2% | batch:       255 of       686\t|\tloss: 10.298\n",
      "Training Epoch 10  37.3% | batch:       256 of       686\t|\tloss: 9.21502\n",
      "Training Epoch 10  37.5% | batch:       257 of       686\t|\tloss: 7.54174\n",
      "Training Epoch 10  37.6% | batch:       258 of       686\t|\tloss: 11.9782\n",
      "Training Epoch 10  37.8% | batch:       259 of       686\t|\tloss: 7.95437\n",
      "Training Epoch 10  37.9% | batch:       260 of       686\t|\tloss: 9.58433\n",
      "Training Epoch 10  38.0% | batch:       261 of       686\t|\tloss: 8.06276\n",
      "Training Epoch 10  38.2% | batch:       262 of       686\t|\tloss: 10.7888\n",
      "Training Epoch 10  38.3% | batch:       263 of       686\t|\tloss: 6.27977\n",
      "Training Epoch 10  38.5% | batch:       264 of       686\t|\tloss: 9.74373\n",
      "Training Epoch 10  38.6% | batch:       265 of       686\t|\tloss: 8.32262\n",
      "Training Epoch 10  38.8% | batch:       266 of       686\t|\tloss: 8.29217\n",
      "Training Epoch 10  38.9% | batch:       267 of       686\t|\tloss: 11.2175\n",
      "Training Epoch 10  39.1% | batch:       268 of       686\t|\tloss: 8.01321\n",
      "Training Epoch 10  39.2% | batch:       269 of       686\t|\tloss: 7.9302\n",
      "Training Epoch 10  39.4% | batch:       270 of       686\t|\tloss: 7.49105\n",
      "Training Epoch 10  39.5% | batch:       271 of       686\t|\tloss: 7.55727\n",
      "Training Epoch 10  39.7% | batch:       272 of       686\t|\tloss: 9.15253\n",
      "Training Epoch 10  39.8% | batch:       273 of       686\t|\tloss: 9.66676\n",
      "Training Epoch 10  39.9% | batch:       274 of       686\t|\tloss: 8.86056\n",
      "Training Epoch 10  40.1% | batch:       275 of       686\t|\tloss: 10.9391\n",
      "Training Epoch 10  40.2% | batch:       276 of       686\t|\tloss: 7.80597\n",
      "Training Epoch 10  40.4% | batch:       277 of       686\t|\tloss: 7.66541\n",
      "Training Epoch 10  40.5% | batch:       278 of       686\t|\tloss: 11.8176\n",
      "Training Epoch 10  40.7% | batch:       279 of       686\t|\tloss: 8.33335\n",
      "Training Epoch 10  40.8% | batch:       280 of       686\t|\tloss: 10.2848\n",
      "Training Epoch 10  41.0% | batch:       281 of       686\t|\tloss: 8.7722\n",
      "Training Epoch 10  41.1% | batch:       282 of       686\t|\tloss: 8.43262\n",
      "Training Epoch 10  41.3% | batch:       283 of       686\t|\tloss: 7.37988\n",
      "Training Epoch 10  41.4% | batch:       284 of       686\t|\tloss: 8.81238\n",
      "Training Epoch 10  41.5% | batch:       285 of       686\t|\tloss: 6.76341\n",
      "Training Epoch 10  41.7% | batch:       286 of       686\t|\tloss: 9.70836\n",
      "Training Epoch 10  41.8% | batch:       287 of       686\t|\tloss: 8.24075\n",
      "Training Epoch 10  42.0% | batch:       288 of       686\t|\tloss: 8.9367\n",
      "Training Epoch 10  42.1% | batch:       289 of       686\t|\tloss: 9.31867\n",
      "Training Epoch 10  42.3% | batch:       290 of       686\t|\tloss: 6.75845\n",
      "Training Epoch 10  42.4% | batch:       291 of       686\t|\tloss: 8.57702\n",
      "Training Epoch 10  42.6% | batch:       292 of       686\t|\tloss: 8.84\n",
      "Training Epoch 10  42.7% | batch:       293 of       686\t|\tloss: 9.3404\n",
      "Training Epoch 10  42.9% | batch:       294 of       686\t|\tloss: 7.95642\n",
      "Training Epoch 10  43.0% | batch:       295 of       686\t|\tloss: 6.90714\n",
      "Training Epoch 10  43.1% | batch:       296 of       686\t|\tloss: 6.72326\n",
      "Training Epoch 10  43.3% | batch:       297 of       686\t|\tloss: 9.84588\n",
      "Training Epoch 10  43.4% | batch:       298 of       686\t|\tloss: 7.11117\n",
      "Training Epoch 10  43.6% | batch:       299 of       686\t|\tloss: 7.06984\n",
      "Training Epoch 10  43.7% | batch:       300 of       686\t|\tloss: 9.54968\n",
      "Training Epoch 10  43.9% | batch:       301 of       686\t|\tloss: 7.73527\n",
      "Training Epoch 10  44.0% | batch:       302 of       686\t|\tloss: 6.97701\n",
      "Training Epoch 10  44.2% | batch:       303 of       686\t|\tloss: 7.05092\n",
      "Training Epoch 10  44.3% | batch:       304 of       686\t|\tloss: 7.58649\n",
      "Training Epoch 10  44.5% | batch:       305 of       686\t|\tloss: 8.22867\n",
      "Training Epoch 10  44.6% | batch:       306 of       686\t|\tloss: 8.32242\n",
      "Training Epoch 10  44.8% | batch:       307 of       686\t|\tloss: 7.34616\n",
      "Training Epoch 10  44.9% | batch:       308 of       686\t|\tloss: 9.93059\n",
      "Training Epoch 10  45.0% | batch:       309 of       686\t|\tloss: 8.55865\n",
      "Training Epoch 10  45.2% | batch:       310 of       686\t|\tloss: 11.216\n",
      "Training Epoch 10  45.3% | batch:       311 of       686\t|\tloss: 8.01652\n",
      "Training Epoch 10  45.5% | batch:       312 of       686\t|\tloss: 9.0177\n",
      "Training Epoch 10  45.6% | batch:       313 of       686\t|\tloss: 8.13199\n",
      "Training Epoch 10  45.8% | batch:       314 of       686\t|\tloss: 7.77242\n",
      "Training Epoch 10  45.9% | batch:       315 of       686\t|\tloss: 7.34276\n",
      "Training Epoch 10  46.1% | batch:       316 of       686\t|\tloss: 11.8367\n",
      "Training Epoch 10  46.2% | batch:       317 of       686\t|\tloss: 6.78605\n",
      "Training Epoch 10  46.4% | batch:       318 of       686\t|\tloss: 8.61604\n",
      "Training Epoch 10  46.5% | batch:       319 of       686\t|\tloss: 9.21612\n",
      "Training Epoch 10  46.6% | batch:       320 of       686\t|\tloss: 10.373\n",
      "Training Epoch 10  46.8% | batch:       321 of       686\t|\tloss: 7.52279\n",
      "Training Epoch 10  46.9% | batch:       322 of       686\t|\tloss: 7.59159\n",
      "Training Epoch 10  47.1% | batch:       323 of       686\t|\tloss: 7.76426\n",
      "Training Epoch 10  47.2% | batch:       324 of       686\t|\tloss: 7.0637\n",
      "Training Epoch 10  47.4% | batch:       325 of       686\t|\tloss: 6.60384\n",
      "Training Epoch 10  47.5% | batch:       326 of       686\t|\tloss: 7.92896\n",
      "Training Epoch 10  47.7% | batch:       327 of       686\t|\tloss: 6.48236\n",
      "Training Epoch 10  47.8% | batch:       328 of       686\t|\tloss: 8.9544\n",
      "Training Epoch 10  48.0% | batch:       329 of       686\t|\tloss: 6.98143\n",
      "Training Epoch 10  48.1% | batch:       330 of       686\t|\tloss: 7.17857\n",
      "Training Epoch 10  48.3% | batch:       331 of       686\t|\tloss: 9.50358\n",
      "Training Epoch 10  48.4% | batch:       332 of       686\t|\tloss: 9.56169\n",
      "Training Epoch 10  48.5% | batch:       333 of       686\t|\tloss: 10.3662\n",
      "Training Epoch 10  48.7% | batch:       334 of       686\t|\tloss: 7.31142\n",
      "Training Epoch 10  48.8% | batch:       335 of       686\t|\tloss: 6.49552\n",
      "Training Epoch 10  49.0% | batch:       336 of       686\t|\tloss: 7.59801\n",
      "Training Epoch 10  49.1% | batch:       337 of       686\t|\tloss: 6.40174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  49.3% | batch:       338 of       686\t|\tloss: 10.0416\n",
      "Training Epoch 10  49.4% | batch:       339 of       686\t|\tloss: 6.20901\n",
      "Training Epoch 10  49.6% | batch:       340 of       686\t|\tloss: 8.71913\n",
      "Training Epoch 10  49.7% | batch:       341 of       686\t|\tloss: 9.5828\n",
      "Training Epoch 10  49.9% | batch:       342 of       686\t|\tloss: 10.8647\n",
      "Training Epoch 10  50.0% | batch:       343 of       686\t|\tloss: 9.53129\n",
      "Training Epoch 10  50.1% | batch:       344 of       686\t|\tloss: 7.92576\n",
      "Training Epoch 10  50.3% | batch:       345 of       686\t|\tloss: 7.67514\n",
      "Training Epoch 10  50.4% | batch:       346 of       686\t|\tloss: 8.20018\n",
      "Training Epoch 10  50.6% | batch:       347 of       686\t|\tloss: 6.12419\n",
      "Training Epoch 10  50.7% | batch:       348 of       686\t|\tloss: 9.07311\n",
      "Training Epoch 10  50.9% | batch:       349 of       686\t|\tloss: 6.13855\n",
      "Training Epoch 10  51.0% | batch:       350 of       686\t|\tloss: 8.83224\n",
      "Training Epoch 10  51.2% | batch:       351 of       686\t|\tloss: 9.51826\n",
      "Training Epoch 10  51.3% | batch:       352 of       686\t|\tloss: 6.35306\n",
      "Training Epoch 10  51.5% | batch:       353 of       686\t|\tloss: 8.32218\n",
      "Training Epoch 10  51.6% | batch:       354 of       686\t|\tloss: 9.11859\n",
      "Training Epoch 10  51.7% | batch:       355 of       686\t|\tloss: 9.05738\n",
      "Training Epoch 10  51.9% | batch:       356 of       686\t|\tloss: 8.14805\n",
      "Training Epoch 10  52.0% | batch:       357 of       686\t|\tloss: 7.54096\n",
      "Training Epoch 10  52.2% | batch:       358 of       686\t|\tloss: 8.85452\n",
      "Training Epoch 10  52.3% | batch:       359 of       686\t|\tloss: 7.54314\n",
      "Training Epoch 10  52.5% | batch:       360 of       686\t|\tloss: 7.8264\n",
      "Training Epoch 10  52.6% | batch:       361 of       686\t|\tloss: 8.89575\n",
      "Training Epoch 10  52.8% | batch:       362 of       686\t|\tloss: 11.3169\n",
      "Training Epoch 10  52.9% | batch:       363 of       686\t|\tloss: 5.41551\n",
      "Training Epoch 10  53.1% | batch:       364 of       686\t|\tloss: 8.16409\n",
      "Training Epoch 10  53.2% | batch:       365 of       686\t|\tloss: 12.0263\n",
      "Training Epoch 10  53.4% | batch:       366 of       686\t|\tloss: 8.13632\n",
      "Training Epoch 10  53.5% | batch:       367 of       686\t|\tloss: 7.95529\n",
      "Training Epoch 10  53.6% | batch:       368 of       686\t|\tloss: 8.21669\n",
      "Training Epoch 10  53.8% | batch:       369 of       686\t|\tloss: 7.82312\n",
      "Training Epoch 10  53.9% | batch:       370 of       686\t|\tloss: 9.61281\n",
      "Training Epoch 10  54.1% | batch:       371 of       686\t|\tloss: 7.72125\n",
      "Training Epoch 10  54.2% | batch:       372 of       686\t|\tloss: 5.74109\n",
      "Training Epoch 10  54.4% | batch:       373 of       686\t|\tloss: 9.01103\n",
      "Training Epoch 10  54.5% | batch:       374 of       686\t|\tloss: 8.75375\n",
      "Training Epoch 10  54.7% | batch:       375 of       686\t|\tloss: 5.95493\n",
      "Training Epoch 10  54.8% | batch:       376 of       686\t|\tloss: 7.27732\n",
      "Training Epoch 10  55.0% | batch:       377 of       686\t|\tloss: 7.30466\n",
      "Training Epoch 10  55.1% | batch:       378 of       686\t|\tloss: 8.76758\n",
      "Training Epoch 10  55.2% | batch:       379 of       686\t|\tloss: 5.79864\n",
      "Training Epoch 10  55.4% | batch:       380 of       686\t|\tloss: 8.60111\n",
      "Training Epoch 10  55.5% | batch:       381 of       686\t|\tloss: 12.3567\n",
      "Training Epoch 10  55.7% | batch:       382 of       686\t|\tloss: 8.13869\n",
      "Training Epoch 10  55.8% | batch:       383 of       686\t|\tloss: 10.2124\n",
      "Training Epoch 10  56.0% | batch:       384 of       686\t|\tloss: 5.67676\n",
      "Training Epoch 10  56.1% | batch:       385 of       686\t|\tloss: 8.5119\n",
      "Training Epoch 10  56.3% | batch:       386 of       686\t|\tloss: 9.11801\n",
      "Training Epoch 10  56.4% | batch:       387 of       686\t|\tloss: 8.13058\n",
      "Training Epoch 10  56.6% | batch:       388 of       686\t|\tloss: 8.3053\n",
      "Training Epoch 10  56.7% | batch:       389 of       686\t|\tloss: 7.37103\n",
      "Training Epoch 10  56.9% | batch:       390 of       686\t|\tloss: 7.29614\n",
      "Training Epoch 10  57.0% | batch:       391 of       686\t|\tloss: 10.2968\n",
      "Training Epoch 10  57.1% | batch:       392 of       686\t|\tloss: 7.58089\n",
      "Training Epoch 10  57.3% | batch:       393 of       686\t|\tloss: 8.43804\n",
      "Training Epoch 10  57.4% | batch:       394 of       686\t|\tloss: 7.43069\n",
      "Training Epoch 10  57.6% | batch:       395 of       686\t|\tloss: 8.96103\n",
      "Training Epoch 10  57.7% | batch:       396 of       686\t|\tloss: 5.60879\n",
      "Training Epoch 10  57.9% | batch:       397 of       686\t|\tloss: 8.80672\n",
      "Training Epoch 10  58.0% | batch:       398 of       686\t|\tloss: 9.94418\n",
      "Training Epoch 10  58.2% | batch:       399 of       686\t|\tloss: 7.3268\n",
      "Training Epoch 10  58.3% | batch:       400 of       686\t|\tloss: 7.75553\n",
      "Training Epoch 10  58.5% | batch:       401 of       686\t|\tloss: 7.36404\n",
      "Training Epoch 10  58.6% | batch:       402 of       686\t|\tloss: 7.60727\n",
      "Training Epoch 10  58.7% | batch:       403 of       686\t|\tloss: 6.94295\n",
      "Training Epoch 10  58.9% | batch:       404 of       686\t|\tloss: 7.38177\n",
      "Training Epoch 10  59.0% | batch:       405 of       686\t|\tloss: 7.5161\n",
      "Training Epoch 10  59.2% | batch:       406 of       686\t|\tloss: 8.18297\n",
      "Training Epoch 10  59.3% | batch:       407 of       686\t|\tloss: 6.93455\n",
      "Training Epoch 10  59.5% | batch:       408 of       686\t|\tloss: 8.45623\n",
      "Training Epoch 10  59.6% | batch:       409 of       686\t|\tloss: 6.69257\n",
      "Training Epoch 10  59.8% | batch:       410 of       686\t|\tloss: 6.73106\n",
      "Training Epoch 10  59.9% | batch:       411 of       686\t|\tloss: 7.96211\n",
      "Training Epoch 10  60.1% | batch:       412 of       686\t|\tloss: 6.51725\n",
      "Training Epoch 10  60.2% | batch:       413 of       686\t|\tloss: 7.71969\n",
      "Training Epoch 10  60.3% | batch:       414 of       686\t|\tloss: 7.4272\n",
      "Training Epoch 10  60.5% | batch:       415 of       686\t|\tloss: 10.029\n",
      "Training Epoch 10  60.6% | batch:       416 of       686\t|\tloss: 6.29769\n",
      "Training Epoch 10  60.8% | batch:       417 of       686\t|\tloss: 9.21239\n",
      "Training Epoch 10  60.9% | batch:       418 of       686\t|\tloss: 9.56153\n",
      "Training Epoch 10  61.1% | batch:       419 of       686\t|\tloss: 8.15225\n",
      "Training Epoch 10  61.2% | batch:       420 of       686\t|\tloss: 9.79254\n",
      "Training Epoch 10  61.4% | batch:       421 of       686\t|\tloss: 6.56254\n",
      "Training Epoch 10  61.5% | batch:       422 of       686\t|\tloss: 7.41807\n",
      "Training Epoch 10  61.7% | batch:       423 of       686\t|\tloss: 8.11852\n",
      "Training Epoch 10  61.8% | batch:       424 of       686\t|\tloss: 7.47034\n",
      "Training Epoch 10  62.0% | batch:       425 of       686\t|\tloss: 8.90467\n",
      "Training Epoch 10  62.1% | batch:       426 of       686\t|\tloss: 8.67257\n",
      "Training Epoch 10  62.2% | batch:       427 of       686\t|\tloss: 7.23136\n",
      "Training Epoch 10  62.4% | batch:       428 of       686\t|\tloss: 9.77738\n",
      "Training Epoch 10  62.5% | batch:       429 of       686\t|\tloss: 5.80603\n",
      "Training Epoch 10  62.7% | batch:       430 of       686\t|\tloss: 9.89403\n",
      "Training Epoch 10  62.8% | batch:       431 of       686\t|\tloss: 6.07202\n",
      "Training Epoch 10  63.0% | batch:       432 of       686\t|\tloss: 9.132\n",
      "Training Epoch 10  63.1% | batch:       433 of       686\t|\tloss: 7.18109\n",
      "Training Epoch 10  63.3% | batch:       434 of       686\t|\tloss: 6.12318\n",
      "Training Epoch 10  63.4% | batch:       435 of       686\t|\tloss: 6.94986\n",
      "Training Epoch 10  63.6% | batch:       436 of       686\t|\tloss: 7.34507\n",
      "Training Epoch 10  63.7% | batch:       437 of       686\t|\tloss: 8.76541\n",
      "Training Epoch 10  63.8% | batch:       438 of       686\t|\tloss: 8.20327\n",
      "Training Epoch 10  64.0% | batch:       439 of       686\t|\tloss: 7.18811\n",
      "Training Epoch 10  64.1% | batch:       440 of       686\t|\tloss: 8.41839\n",
      "Training Epoch 10  64.3% | batch:       441 of       686\t|\tloss: 6.42391\n",
      "Training Epoch 10  64.4% | batch:       442 of       686\t|\tloss: 8.53013\n",
      "Training Epoch 10  64.6% | batch:       443 of       686\t|\tloss: 7.55685\n",
      "Training Epoch 10  64.7% | batch:       444 of       686\t|\tloss: 5.98259\n",
      "Training Epoch 10  64.9% | batch:       445 of       686\t|\tloss: 5.93077\n",
      "Training Epoch 10  65.0% | batch:       446 of       686\t|\tloss: 6.92945\n",
      "Training Epoch 10  65.2% | batch:       447 of       686\t|\tloss: 6.37565\n",
      "Training Epoch 10  65.3% | batch:       448 of       686\t|\tloss: 7.4481\n",
      "Training Epoch 10  65.5% | batch:       449 of       686\t|\tloss: 7.03998\n",
      "Training Epoch 10  65.6% | batch:       450 of       686\t|\tloss: 6.77121\n",
      "Training Epoch 10  65.7% | batch:       451 of       686\t|\tloss: 6.25253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  65.9% | batch:       452 of       686\t|\tloss: 7.28573\n",
      "Training Epoch 10  66.0% | batch:       453 of       686\t|\tloss: 7.69982\n",
      "Training Epoch 10  66.2% | batch:       454 of       686\t|\tloss: 8.0553\n",
      "Training Epoch 10  66.3% | batch:       455 of       686\t|\tloss: 6.58467\n",
      "Training Epoch 10  66.5% | batch:       456 of       686\t|\tloss: 7.34974\n",
      "Training Epoch 10  66.6% | batch:       457 of       686\t|\tloss: 8.35522\n",
      "Training Epoch 10  66.8% | batch:       458 of       686\t|\tloss: 7.4839\n",
      "Training Epoch 10  66.9% | batch:       459 of       686\t|\tloss: 7.19673\n",
      "Training Epoch 10  67.1% | batch:       460 of       686\t|\tloss: 6.2665\n",
      "Training Epoch 10  67.2% | batch:       461 of       686\t|\tloss: 6.4681\n",
      "Training Epoch 10  67.3% | batch:       462 of       686\t|\tloss: 8.30322\n",
      "Training Epoch 10  67.5% | batch:       463 of       686\t|\tloss: 8.92722\n",
      "Training Epoch 10  67.6% | batch:       464 of       686\t|\tloss: 7.67095\n",
      "Training Epoch 10  67.8% | batch:       465 of       686\t|\tloss: 7.42876\n",
      "Training Epoch 10  67.9% | batch:       466 of       686\t|\tloss: 7.04407\n",
      "Training Epoch 10  68.1% | batch:       467 of       686\t|\tloss: 12.2867\n",
      "Training Epoch 10  68.2% | batch:       468 of       686\t|\tloss: 8.41115\n",
      "Training Epoch 10  68.4% | batch:       469 of       686\t|\tloss: 7.88807\n",
      "Training Epoch 10  68.5% | batch:       470 of       686\t|\tloss: 6.5983\n",
      "Training Epoch 10  68.7% | batch:       471 of       686\t|\tloss: 9.32368\n",
      "Training Epoch 10  68.8% | batch:       472 of       686\t|\tloss: 6.64786\n",
      "Training Epoch 10  69.0% | batch:       473 of       686\t|\tloss: 7.52311\n",
      "Training Epoch 10  69.1% | batch:       474 of       686\t|\tloss: 8.67019\n",
      "Training Epoch 10  69.2% | batch:       475 of       686\t|\tloss: 6.94263\n",
      "Training Epoch 10  69.4% | batch:       476 of       686\t|\tloss: 7.97363\n",
      "Training Epoch 10  69.5% | batch:       477 of       686\t|\tloss: 7.26209\n",
      "Training Epoch 10  69.7% | batch:       478 of       686\t|\tloss: 7.92658\n",
      "Training Epoch 10  69.8% | batch:       479 of       686\t|\tloss: 7.50139\n",
      "Training Epoch 10  70.0% | batch:       480 of       686\t|\tloss: 6.87114\n",
      "Training Epoch 10  70.1% | batch:       481 of       686\t|\tloss: 8.83378\n",
      "Training Epoch 10  70.3% | batch:       482 of       686\t|\tloss: 7.80297\n",
      "Training Epoch 10  70.4% | batch:       483 of       686\t|\tloss: 6.75308\n",
      "Training Epoch 10  70.6% | batch:       484 of       686\t|\tloss: 7.92924\n",
      "Training Epoch 10  70.7% | batch:       485 of       686\t|\tloss: 6.99432\n",
      "Training Epoch 10  70.8% | batch:       486 of       686\t|\tloss: 9.64106\n",
      "Training Epoch 10  71.0% | batch:       487 of       686\t|\tloss: 8.27854\n",
      "Training Epoch 10  71.1% | batch:       488 of       686\t|\tloss: 5.36709\n",
      "Training Epoch 10  71.3% | batch:       489 of       686\t|\tloss: 7.1502\n",
      "Training Epoch 10  71.4% | batch:       490 of       686\t|\tloss: 7.86853\n",
      "Training Epoch 10  71.6% | batch:       491 of       686\t|\tloss: 7.12133\n",
      "Training Epoch 10  71.7% | batch:       492 of       686\t|\tloss: 6.26106\n",
      "Training Epoch 10  71.9% | batch:       493 of       686\t|\tloss: 6.7049\n",
      "Training Epoch 10  72.0% | batch:       494 of       686\t|\tloss: 10.1786\n",
      "Training Epoch 10  72.2% | batch:       495 of       686\t|\tloss: 6.13876\n",
      "Training Epoch 10  72.3% | batch:       496 of       686\t|\tloss: 6.86095\n",
      "Training Epoch 10  72.4% | batch:       497 of       686\t|\tloss: 6.63812\n",
      "Training Epoch 10  72.6% | batch:       498 of       686\t|\tloss: 7.95634\n",
      "Training Epoch 10  72.7% | batch:       499 of       686\t|\tloss: 6.63937\n",
      "Training Epoch 10  72.9% | batch:       500 of       686\t|\tloss: 5.64656\n",
      "Training Epoch 10  73.0% | batch:       501 of       686\t|\tloss: 10.9185\n",
      "Training Epoch 10  73.2% | batch:       502 of       686\t|\tloss: 6.71244\n",
      "Training Epoch 10  73.3% | batch:       503 of       686\t|\tloss: 5.927\n",
      "Training Epoch 10  73.5% | batch:       504 of       686\t|\tloss: 7.51682\n",
      "Training Epoch 10  73.6% | batch:       505 of       686\t|\tloss: 9.64991\n",
      "Training Epoch 10  73.8% | batch:       506 of       686\t|\tloss: 11.8001\n",
      "Training Epoch 10  73.9% | batch:       507 of       686\t|\tloss: 7.51141\n",
      "Training Epoch 10  74.1% | batch:       508 of       686\t|\tloss: 7.89437\n",
      "Training Epoch 10  74.2% | batch:       509 of       686\t|\tloss: 7.48005\n",
      "Training Epoch 10  74.3% | batch:       510 of       686\t|\tloss: 6.50564\n",
      "Training Epoch 10  74.5% | batch:       511 of       686\t|\tloss: 7.50035\n",
      "Training Epoch 10  74.6% | batch:       512 of       686\t|\tloss: 6.90492\n",
      "Training Epoch 10  74.8% | batch:       513 of       686\t|\tloss: 6.13278\n",
      "Training Epoch 10  74.9% | batch:       514 of       686\t|\tloss: 7.46212\n",
      "Training Epoch 10  75.1% | batch:       515 of       686\t|\tloss: 6.56485\n",
      "Training Epoch 10  75.2% | batch:       516 of       686\t|\tloss: 13.3652\n",
      "Training Epoch 10  75.4% | batch:       517 of       686\t|\tloss: 5.66492\n",
      "Training Epoch 10  75.5% | batch:       518 of       686\t|\tloss: 8.91846\n",
      "Training Epoch 10  75.7% | batch:       519 of       686\t|\tloss: 7.50656\n",
      "Training Epoch 10  75.8% | batch:       520 of       686\t|\tloss: 7.41125\n",
      "Training Epoch 10  75.9% | batch:       521 of       686\t|\tloss: 7.16171\n",
      "Training Epoch 10  76.1% | batch:       522 of       686\t|\tloss: 8.23699\n",
      "Training Epoch 10  76.2% | batch:       523 of       686\t|\tloss: 9.22716\n",
      "Training Epoch 10  76.4% | batch:       524 of       686\t|\tloss: 8.16887\n",
      "Training Epoch 10  76.5% | batch:       525 of       686\t|\tloss: 6.37919\n",
      "Training Epoch 10  76.7% | batch:       526 of       686\t|\tloss: 8.18523\n",
      "Training Epoch 10  76.8% | batch:       527 of       686\t|\tloss: 5.77495\n",
      "Training Epoch 10  77.0% | batch:       528 of       686\t|\tloss: 6.7254\n",
      "Training Epoch 10  77.1% | batch:       529 of       686\t|\tloss: 7.83803\n",
      "Training Epoch 10  77.3% | batch:       530 of       686\t|\tloss: 7.40906\n",
      "Training Epoch 10  77.4% | batch:       531 of       686\t|\tloss: 8.97978\n",
      "Training Epoch 10  77.6% | batch:       532 of       686\t|\tloss: 6.74096\n",
      "Training Epoch 10  77.7% | batch:       533 of       686\t|\tloss: 8.27598\n",
      "Training Epoch 10  77.8% | batch:       534 of       686\t|\tloss: 4.94647\n",
      "Training Epoch 10  78.0% | batch:       535 of       686\t|\tloss: 8.57047\n",
      "Training Epoch 10  78.1% | batch:       536 of       686\t|\tloss: 10.351\n",
      "Training Epoch 10  78.3% | batch:       537 of       686\t|\tloss: 6.74962\n",
      "Training Epoch 10  78.4% | batch:       538 of       686\t|\tloss: 4.4481\n",
      "Training Epoch 10  78.6% | batch:       539 of       686\t|\tloss: 7.25055\n",
      "Training Epoch 10  78.7% | batch:       540 of       686\t|\tloss: 8.2123\n",
      "Training Epoch 10  78.9% | batch:       541 of       686\t|\tloss: 8.52958\n",
      "Training Epoch 10  79.0% | batch:       542 of       686\t|\tloss: 8.34103\n",
      "Training Epoch 10  79.2% | batch:       543 of       686\t|\tloss: 7.14158\n",
      "Training Epoch 10  79.3% | batch:       544 of       686\t|\tloss: 6.54756\n",
      "Training Epoch 10  79.4% | batch:       545 of       686\t|\tloss: 11.6879\n",
      "Training Epoch 10  79.6% | batch:       546 of       686\t|\tloss: 6.85036\n",
      "Training Epoch 10  79.7% | batch:       547 of       686\t|\tloss: 6.31348\n",
      "Training Epoch 10  79.9% | batch:       548 of       686\t|\tloss: 6.66118\n",
      "Training Epoch 10  80.0% | batch:       549 of       686\t|\tloss: 9.79978\n",
      "Training Epoch 10  80.2% | batch:       550 of       686\t|\tloss: 7.20343\n",
      "Training Epoch 10  80.3% | batch:       551 of       686\t|\tloss: 8.21689\n",
      "Training Epoch 10  80.5% | batch:       552 of       686\t|\tloss: 8.91383\n",
      "Training Epoch 10  80.6% | batch:       553 of       686\t|\tloss: 7.60666\n",
      "Training Epoch 10  80.8% | batch:       554 of       686\t|\tloss: 7.58472\n",
      "Training Epoch 10  80.9% | batch:       555 of       686\t|\tloss: 6.22773\n",
      "Training Epoch 10  81.0% | batch:       556 of       686\t|\tloss: 7.88569\n",
      "Training Epoch 10  81.2% | batch:       557 of       686\t|\tloss: 8.07453\n",
      "Training Epoch 10  81.3% | batch:       558 of       686\t|\tloss: 7.75116\n",
      "Training Epoch 10  81.5% | batch:       559 of       686\t|\tloss: 7.51227\n",
      "Training Epoch 10  81.6% | batch:       560 of       686\t|\tloss: 7.63725\n",
      "Training Epoch 10  81.8% | batch:       561 of       686\t|\tloss: 6.66551\n",
      "Training Epoch 10  81.9% | batch:       562 of       686\t|\tloss: 7.87637\n",
      "Training Epoch 10  82.1% | batch:       563 of       686\t|\tloss: 6.5813\n",
      "Training Epoch 10  82.2% | batch:       564 of       686\t|\tloss: 6.93163\n",
      "Training Epoch 10  82.4% | batch:       565 of       686\t|\tloss: 8.58253\n",
      "Training Epoch 10  82.5% | batch:       566 of       686\t|\tloss: 7.22608\n",
      "Training Epoch 10  82.7% | batch:       567 of       686\t|\tloss: 7.45017\n",
      "Training Epoch 10  82.8% | batch:       568 of       686\t|\tloss: 8.08276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  82.9% | batch:       569 of       686\t|\tloss: 7.85482\n",
      "Training Epoch 10  83.1% | batch:       570 of       686\t|\tloss: 7.93007\n",
      "Training Epoch 10  83.2% | batch:       571 of       686\t|\tloss: 7.715\n",
      "Training Epoch 10  83.4% | batch:       572 of       686\t|\tloss: 7.58126\n",
      "Training Epoch 10  83.5% | batch:       573 of       686\t|\tloss: 6.63516\n",
      "Training Epoch 10  83.7% | batch:       574 of       686\t|\tloss: 6.19869\n",
      "Training Epoch 10  83.8% | batch:       575 of       686\t|\tloss: 10.2085\n",
      "Training Epoch 10  84.0% | batch:       576 of       686\t|\tloss: 8.81811\n",
      "Training Epoch 10  84.1% | batch:       577 of       686\t|\tloss: 6.27543\n",
      "Training Epoch 10  84.3% | batch:       578 of       686\t|\tloss: 8.99332\n",
      "Training Epoch 10  84.4% | batch:       579 of       686\t|\tloss: 10.7274\n",
      "Training Epoch 10  84.5% | batch:       580 of       686\t|\tloss: 8.98738\n",
      "Training Epoch 10  84.7% | batch:       581 of       686\t|\tloss: 6.8552\n",
      "Training Epoch 10  84.8% | batch:       582 of       686\t|\tloss: 5.94727\n",
      "Training Epoch 10  85.0% | batch:       583 of       686\t|\tloss: 8.76584\n",
      "Training Epoch 10  85.1% | batch:       584 of       686\t|\tloss: 8.5475\n",
      "Training Epoch 10  85.3% | batch:       585 of       686\t|\tloss: 9.06801\n",
      "Training Epoch 10  85.4% | batch:       586 of       686\t|\tloss: 7.73862\n",
      "Training Epoch 10  85.6% | batch:       587 of       686\t|\tloss: 7.91736\n",
      "Training Epoch 10  85.7% | batch:       588 of       686\t|\tloss: 8.03699\n",
      "Training Epoch 10  85.9% | batch:       589 of       686\t|\tloss: 6.67881\n",
      "Training Epoch 10  86.0% | batch:       590 of       686\t|\tloss: 5.314\n",
      "Training Epoch 10  86.2% | batch:       591 of       686\t|\tloss: 8.9886\n",
      "Training Epoch 10  86.3% | batch:       592 of       686\t|\tloss: 7.67735\n",
      "Training Epoch 10  86.4% | batch:       593 of       686\t|\tloss: 9.35937\n",
      "Training Epoch 10  86.6% | batch:       594 of       686\t|\tloss: 5.87592\n",
      "Training Epoch 10  86.7% | batch:       595 of       686\t|\tloss: 6.4475\n",
      "Training Epoch 10  86.9% | batch:       596 of       686\t|\tloss: 6.56846\n",
      "Training Epoch 10  87.0% | batch:       597 of       686\t|\tloss: 6.33668\n",
      "Training Epoch 10  87.2% | batch:       598 of       686\t|\tloss: 8.67683\n",
      "Training Epoch 10  87.3% | batch:       599 of       686\t|\tloss: 7.11815\n",
      "Training Epoch 10  87.5% | batch:       600 of       686\t|\tloss: 5.33901\n",
      "Training Epoch 10  87.6% | batch:       601 of       686\t|\tloss: 12.0151\n",
      "Training Epoch 10  87.8% | batch:       602 of       686\t|\tloss: 6.54655\n",
      "Training Epoch 10  87.9% | batch:       603 of       686\t|\tloss: 5.86616\n",
      "Training Epoch 10  88.0% | batch:       604 of       686\t|\tloss: 9.47097\n",
      "Training Epoch 10  88.2% | batch:       605 of       686\t|\tloss: 7.02679\n",
      "Training Epoch 10  88.3% | batch:       606 of       686\t|\tloss: 6.80651\n",
      "Training Epoch 10  88.5% | batch:       607 of       686\t|\tloss: 6.62606\n",
      "Training Epoch 10  88.6% | batch:       608 of       686\t|\tloss: 6.85724\n",
      "Training Epoch 10  88.8% | batch:       609 of       686\t|\tloss: 7.90198\n",
      "Training Epoch 10  88.9% | batch:       610 of       686\t|\tloss: 8.88369\n",
      "Training Epoch 10  89.1% | batch:       611 of       686\t|\tloss: 6.10341\n",
      "Training Epoch 10  89.2% | batch:       612 of       686\t|\tloss: 7.01001\n",
      "Training Epoch 10  89.4% | batch:       613 of       686\t|\tloss: 7.51364\n",
      "Training Epoch 10  89.5% | batch:       614 of       686\t|\tloss: 9.35432\n",
      "Training Epoch 10  89.7% | batch:       615 of       686\t|\tloss: 8.22379\n",
      "Training Epoch 10  89.8% | batch:       616 of       686\t|\tloss: 8.54479\n",
      "Training Epoch 10  89.9% | batch:       617 of       686\t|\tloss: 6.37019\n",
      "Training Epoch 10  90.1% | batch:       618 of       686\t|\tloss: 7.23945\n",
      "Training Epoch 10  90.2% | batch:       619 of       686\t|\tloss: 6.778\n",
      "Training Epoch 10  90.4% | batch:       620 of       686\t|\tloss: 7.85383\n",
      "Training Epoch 10  90.5% | batch:       621 of       686\t|\tloss: 6.92941\n",
      "Training Epoch 10  90.7% | batch:       622 of       686\t|\tloss: 6.79905\n",
      "Training Epoch 10  90.8% | batch:       623 of       686\t|\tloss: 6.83745\n",
      "Training Epoch 10  91.0% | batch:       624 of       686\t|\tloss: 7.33035\n",
      "Training Epoch 10  91.1% | batch:       625 of       686\t|\tloss: 6.78493\n",
      "Training Epoch 10  91.3% | batch:       626 of       686\t|\tloss: 6.50901\n",
      "Training Epoch 10  91.4% | batch:       627 of       686\t|\tloss: 7.17636\n",
      "Training Epoch 10  91.5% | batch:       628 of       686\t|\tloss: 6.50745\n",
      "Training Epoch 10  91.7% | batch:       629 of       686\t|\tloss: 8.33682\n",
      "Training Epoch 10  91.8% | batch:       630 of       686\t|\tloss: 8.05164\n",
      "Training Epoch 10  92.0% | batch:       631 of       686\t|\tloss: 8.45753\n",
      "Training Epoch 10  92.1% | batch:       632 of       686\t|\tloss: 7.53293\n",
      "Training Epoch 10  92.3% | batch:       633 of       686\t|\tloss: 4.93793\n",
      "Training Epoch 10  92.4% | batch:       634 of       686\t|\tloss: 6.82137\n",
      "Training Epoch 10  92.6% | batch:       635 of       686\t|\tloss: 6.6242\n",
      "Training Epoch 10  92.7% | batch:       636 of       686\t|\tloss: 6.11103\n",
      "Training Epoch 10  92.9% | batch:       637 of       686\t|\tloss: 8.95863\n",
      "Training Epoch 10  93.0% | batch:       638 of       686\t|\tloss: 5.48695\n",
      "Training Epoch 10  93.1% | batch:       639 of       686\t|\tloss: 6.50295\n",
      "Training Epoch 10  93.3% | batch:       640 of       686\t|\tloss: 7.36914\n",
      "Training Epoch 10  93.4% | batch:       641 of       686\t|\tloss: 7.15831\n",
      "Training Epoch 10  93.6% | batch:       642 of       686\t|\tloss: 11.657\n",
      "Training Epoch 10  93.7% | batch:       643 of       686\t|\tloss: 9.98193\n",
      "Training Epoch 10  93.9% | batch:       644 of       686\t|\tloss: 8.22147\n",
      "Training Epoch 10  94.0% | batch:       645 of       686\t|\tloss: 6.41593\n",
      "Training Epoch 10  94.2% | batch:       646 of       686\t|\tloss: 5.89419\n",
      "Training Epoch 10  94.3% | batch:       647 of       686\t|\tloss: 6.53249\n",
      "Training Epoch 10  94.5% | batch:       648 of       686\t|\tloss: 9.29081\n",
      "Training Epoch 10  94.6% | batch:       649 of       686\t|\tloss: 5.5125\n",
      "Training Epoch 10  94.8% | batch:       650 of       686\t|\tloss: 6.00695\n",
      "Training Epoch 10  94.9% | batch:       651 of       686\t|\tloss: 7.8103\n",
      "Training Epoch 10  95.0% | batch:       652 of       686\t|\tloss: 6.1764\n",
      "Training Epoch 10  95.2% | batch:       653 of       686\t|\tloss: 7.70651\n",
      "Training Epoch 10  95.3% | batch:       654 of       686\t|\tloss: 7.86328\n",
      "Training Epoch 10  95.5% | batch:       655 of       686\t|\tloss: 5.67862\n",
      "Training Epoch 10  95.6% | batch:       656 of       686\t|\tloss: 6.97483\n",
      "Training Epoch 10  95.8% | batch:       657 of       686\t|\tloss: 6.98918\n",
      "Training Epoch 10  95.9% | batch:       658 of       686\t|\tloss: 6.97035\n",
      "Training Epoch 10  96.1% | batch:       659 of       686\t|\tloss: 6.94583\n",
      "Training Epoch 10  96.2% | batch:       660 of       686\t|\tloss: 8.06582\n",
      "Training Epoch 10  96.4% | batch:       661 of       686\t|\tloss: 8.24531\n",
      "Training Epoch 10  96.5% | batch:       662 of       686\t|\tloss: 7.00862\n",
      "Training Epoch 10  96.6% | batch:       663 of       686\t|\tloss: 6.67582\n",
      "Training Epoch 10  96.8% | batch:       664 of       686\t|\tloss: 8.21699\n",
      "Training Epoch 10  96.9% | batch:       665 of       686\t|\tloss: 5.56901\n",
      "Training Epoch 10  97.1% | batch:       666 of       686\t|\tloss: 6.24774\n",
      "Training Epoch 10  97.2% | batch:       667 of       686\t|\tloss: 5.55081\n",
      "Training Epoch 10  97.4% | batch:       668 of       686\t|\tloss: 7.7175\n",
      "Training Epoch 10  97.5% | batch:       669 of       686\t|\tloss: 6.29716\n",
      "Training Epoch 10  97.7% | batch:       670 of       686\t|\tloss: 11.2249\n",
      "Training Epoch 10  97.8% | batch:       671 of       686\t|\tloss: 9.09663\n",
      "Training Epoch 10  98.0% | batch:       672 of       686\t|\tloss: 11.8254\n",
      "Training Epoch 10  98.1% | batch:       673 of       686\t|\tloss: 6.242\n",
      "Training Epoch 10  98.3% | batch:       674 of       686\t|\tloss: 6.74351\n",
      "Training Epoch 10  98.4% | batch:       675 of       686\t|\tloss: 6.25382\n",
      "Training Epoch 10  98.5% | batch:       676 of       686\t|\tloss: 8.0306\n",
      "Training Epoch 10  98.7% | batch:       677 of       686\t|\tloss: 6.4197\n",
      "Training Epoch 10  98.8% | batch:       678 of       686\t|\tloss: 6.43499\n",
      "Training Epoch 10  99.0% | batch:       679 of       686\t|\tloss: 6.41155\n",
      "Training Epoch 10  99.1% | batch:       680 of       686\t|\tloss: 7.59694\n",
      "Training Epoch 10  99.3% | batch:       681 of       686\t|\tloss: 7.79972\n",
      "Training Epoch 10  99.4% | batch:       682 of       686\t|\tloss: 7.4318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:04:23,610 | INFO : Epoch 10 Training Summary: epoch: 10.000000 | loss: 8.094857 | \n",
      "2023-05-25 22:04:23,611 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 21.774489164352417 seconds\n",
      "\n",
      "2023-05-25 22:04:23,611 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.474595379829406 seconds\n",
      "2023-05-25 22:04:23,612 | INFO : Avg batch train. time: 0.03421952679275424 seconds\n",
      "2023-05-25 22:04:23,612 | INFO : Avg sample train. time: 0.0002676845359465124 seconds\n",
      "2023-05-25 22:04:23,613 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  99.6% | batch:       683 of       686\t|\tloss: 8.71929\n",
      "Training Epoch 10  99.7% | batch:       684 of       686\t|\tloss: 7.71077\n",
      "Training Epoch 10  99.9% | batch:       685 of       686\t|\tloss: 3.5718\n",
      "\n",
      "Evaluating Epoch 10   0.0% | batch:         0 of       172\t|\tloss: 2.22596\n",
      "Evaluating Epoch 10   0.6% | batch:         1 of       172\t|\tloss: 2.64242\n",
      "Evaluating Epoch 10   1.2% | batch:         2 of       172\t|\tloss: 2.12384\n",
      "Evaluating Epoch 10   1.7% | batch:         3 of       172\t|\tloss: 4.23987\n",
      "Evaluating Epoch 10   2.3% | batch:         4 of       172\t|\tloss: 2.20429\n",
      "Evaluating Epoch 10   2.9% | batch:         5 of       172\t|\tloss: 1.80724\n",
      "Evaluating Epoch 10   3.5% | batch:         6 of       172\t|\tloss: 2.66991\n",
      "Evaluating Epoch 10   4.1% | batch:         7 of       172\t|\tloss: 4.25781\n",
      "Evaluating Epoch 10   4.7% | batch:         8 of       172\t|\tloss: 2.01548\n",
      "Evaluating Epoch 10   5.2% | batch:         9 of       172\t|\tloss: 2.78743\n",
      "Evaluating Epoch 10   5.8% | batch:        10 of       172\t|\tloss: 3.09183\n",
      "Evaluating Epoch 10   6.4% | batch:        11 of       172\t|\tloss: 2.60769\n",
      "Evaluating Epoch 10   7.0% | batch:        12 of       172\t|\tloss: 1.88837\n",
      "Evaluating Epoch 10   7.6% | batch:        13 of       172\t|\tloss: 2.96886\n",
      "Evaluating Epoch 10   8.1% | batch:        14 of       172\t|\tloss: 2.9906\n",
      "Evaluating Epoch 10   8.7% | batch:        15 of       172\t|\tloss: 2.10431\n",
      "Evaluating Epoch 10   9.3% | batch:        16 of       172\t|\tloss: 3.05149\n",
      "Evaluating Epoch 10   9.9% | batch:        17 of       172\t|\tloss: 2.06382\n",
      "Evaluating Epoch 10  10.5% | batch:        18 of       172\t|\tloss: 11.8133\n",
      "Evaluating Epoch 10  11.0% | batch:        19 of       172\t|\tloss: 1.37504\n",
      "Evaluating Epoch 10  11.6% | batch:        20 of       172\t|\tloss: 2.31164\n",
      "Evaluating Epoch 10  12.2% | batch:        21 of       172\t|\tloss: 0.848371\n",
      "Evaluating Epoch 10  12.8% | batch:        22 of       172\t|\tloss: 1.89308\n",
      "Evaluating Epoch 10  13.4% | batch:        23 of       172\t|\tloss: 1.64482\n",
      "Evaluating Epoch 10  14.0% | batch:        24 of       172\t|\tloss: 1.24805\n",
      "Evaluating Epoch 10  14.5% | batch:        25 of       172\t|\tloss: 2.47281\n",
      "Evaluating Epoch 10  15.1% | batch:        26 of       172\t|\tloss: 5.93615\n",
      "Evaluating Epoch 10  15.7% | batch:        27 of       172\t|\tloss: 11.9652\n",
      "Evaluating Epoch 10  16.3% | batch:        28 of       172\t|\tloss: 0.291714\n",
      "Evaluating Epoch 10  16.9% | batch:        29 of       172\t|\tloss: 1.81514\n",
      "Evaluating Epoch 10  17.4% | batch:        30 of       172\t|\tloss: 1.31558\n",
      "Evaluating Epoch 10  18.0% | batch:        31 of       172\t|\tloss: 1.98213\n",
      "Evaluating Epoch 10  18.6% | batch:        32 of       172\t|\tloss: 0.37676\n",
      "Evaluating Epoch 10  19.2% | batch:        33 of       172\t|\tloss: 0.911378\n",
      "Evaluating Epoch 10  19.8% | batch:        34 of       172\t|\tloss: 0.342808\n",
      "Evaluating Epoch 10  20.3% | batch:        35 of       172\t|\tloss: 0.532798\n",
      "Evaluating Epoch 10  20.9% | batch:        36 of       172\t|\tloss: 3.94315\n",
      "Evaluating Epoch 10  21.5% | batch:        37 of       172\t|\tloss: 3.27378\n",
      "Evaluating Epoch 10  22.1% | batch:        38 of       172\t|\tloss: 2.26464\n",
      "Evaluating Epoch 10  22.7% | batch:        39 of       172\t|\tloss: 4.09615\n",
      "Evaluating Epoch 10  23.3% | batch:        40 of       172\t|\tloss: 0.260827\n",
      "Evaluating Epoch 10  23.8% | batch:        41 of       172\t|\tloss: 1.32283\n",
      "Evaluating Epoch 10  24.4% | batch:        42 of       172\t|\tloss: 0.672414\n",
      "Evaluating Epoch 10  25.0% | batch:        43 of       172\t|\tloss: 13.0479\n",
      "Evaluating Epoch 10  25.6% | batch:        44 of       172\t|\tloss: 1.28553\n",
      "Evaluating Epoch 10  26.2% | batch:        45 of       172\t|\tloss: 0.990948\n",
      "Evaluating Epoch 10  26.7% | batch:        46 of       172\t|\tloss: 0.633627\n",
      "Evaluating Epoch 10  27.3% | batch:        47 of       172\t|\tloss: 2.00787\n",
      "Evaluating Epoch 10  27.9% | batch:        48 of       172\t|\tloss: 0.456326\n",
      "Evaluating Epoch 10  28.5% | batch:        49 of       172\t|\tloss: 1.50387\n",
      "Evaluating Epoch 10  29.1% | batch:        50 of       172\t|\tloss: 0.360275\n",
      "Evaluating Epoch 10  29.7% | batch:        51 of       172\t|\tloss: 0.708057\n",
      "Evaluating Epoch 10  30.2% | batch:        52 of       172\t|\tloss: 1.72433\n",
      "Evaluating Epoch 10  30.8% | batch:        53 of       172\t|\tloss: 1.17852\n",
      "Evaluating Epoch 10  31.4% | batch:        54 of       172\t|\tloss: 1.06653\n",
      "Evaluating Epoch 10  32.0% | batch:        55 of       172\t|\tloss: 2.32987\n",
      "Evaluating Epoch 10  32.6% | batch:        56 of       172\t|\tloss: 1.58048\n",
      "Evaluating Epoch 10  33.1% | batch:        57 of       172\t|\tloss: 2.94213\n",
      "Evaluating Epoch 10  33.7% | batch:        58 of       172\t|\tloss: 0.795763\n",
      "Evaluating Epoch 10  34.3% | batch:        59 of       172\t|\tloss: 2.50031\n",
      "Evaluating Epoch 10  34.9% | batch:        60 of       172\t|\tloss: 0.584939\n",
      "Evaluating Epoch 10  35.5% | batch:        61 of       172\t|\tloss: 3.42443\n",
      "Evaluating Epoch 10  36.0% | batch:        62 of       172\t|\tloss: 0.818019\n",
      "Evaluating Epoch 10  36.6% | batch:        63 of       172\t|\tloss: 1.33336\n",
      "Evaluating Epoch 10  37.2% | batch:        64 of       172\t|\tloss: 1.81445\n",
      "Evaluating Epoch 10  37.8% | batch:        65 of       172\t|\tloss: 0.979569\n",
      "Evaluating Epoch 10  38.4% | batch:        66 of       172\t|\tloss: 2.70076\n",
      "Evaluating Epoch 10  39.0% | batch:        67 of       172\t|\tloss: 1.22315\n",
      "Evaluating Epoch 10  39.5% | batch:        68 of       172\t|\tloss: 2.2195\n",
      "Evaluating Epoch 10  40.1% | batch:        69 of       172\t|\tloss: 3.66431\n",
      "Evaluating Epoch 10  40.7% | batch:        70 of       172\t|\tloss: 0.414849\n",
      "Evaluating Epoch 10  41.3% | batch:        71 of       172\t|\tloss: 1.21192\n",
      "Evaluating Epoch 10  41.9% | batch:        72 of       172\t|\tloss: 1.82126\n",
      "Evaluating Epoch 10  42.4% | batch:        73 of       172\t|\tloss: 1.0357\n",
      "Evaluating Epoch 10  43.0% | batch:        74 of       172\t|\tloss: 0.27785\n",
      "Evaluating Epoch 10  43.6% | batch:        75 of       172\t|\tloss: 0.324332\n",
      "Evaluating Epoch 10  44.2% | batch:        76 of       172\t|\tloss: 0.184517\n",
      "Evaluating Epoch 10  44.8% | batch:        77 of       172\t|\tloss: 0.270403\n",
      "Evaluating Epoch 10  45.3% | batch:        78 of       172\t|\tloss: 0.391876\n",
      "Evaluating Epoch 10  45.9% | batch:        79 of       172\t|\tloss: 0.292087\n",
      "Evaluating Epoch 10  46.5% | batch:        80 of       172\t|\tloss: 0.286889\n",
      "Evaluating Epoch 10  47.1% | batch:        81 of       172\t|\tloss: 0.297547\n",
      "Evaluating Epoch 10  47.7% | batch:        82 of       172\t|\tloss: 0.327945\n",
      "Evaluating Epoch 10  48.3% | batch:        83 of       172\t|\tloss: 0.608043\n",
      "Evaluating Epoch 10  48.8% | batch:        84 of       172\t|\tloss: 0.923567\n",
      "Evaluating Epoch 10  49.4% | batch:        85 of       172\t|\tloss: 1.32978\n",
      "Evaluating Epoch 10  50.0% | batch:        86 of       172\t|\tloss: 0.838944\n",
      "Evaluating Epoch 10  50.6% | batch:        87 of       172\t|\tloss: 0.62787\n",
      "Evaluating Epoch 10  51.2% | batch:        88 of       172\t|\tloss: 1.28899\n",
      "Evaluating Epoch 10  51.7% | batch:        89 of       172\t|\tloss: 1.76426\n",
      "Evaluating Epoch 10  52.3% | batch:        90 of       172\t|\tloss: 0.842734\n",
      "Evaluating Epoch 10  52.9% | batch:        91 of       172\t|\tloss: 1.61038\n",
      "Evaluating Epoch 10  53.5% | batch:        92 of       172\t|\tloss: 1.91292\n",
      "Evaluating Epoch 10  54.1% | batch:        93 of       172\t|\tloss: 0.935879\n",
      "Evaluating Epoch 10  54.7% | batch:        94 of       172\t|\tloss: 1.12406\n",
      "Evaluating Epoch 10  55.2% | batch:        95 of       172\t|\tloss: 1.45405\n",
      "Evaluating Epoch 10  55.8% | batch:        96 of       172\t|\tloss: 1.43203\n",
      "Evaluating Epoch 10  56.4% | batch:        97 of       172\t|\tloss: 0.893516\n",
      "Evaluating Epoch 10  57.0% | batch:        98 of       172\t|\tloss: 1.37635\n",
      "Evaluating Epoch 10  57.6% | batch:        99 of       172\t|\tloss: 2.04317\n",
      "Evaluating Epoch 10  58.1% | batch:       100 of       172\t|\tloss: 0.555111\n",
      "Evaluating Epoch 10  58.7% | batch:       101 of       172\t|\tloss: 0.709589\n",
      "Evaluating Epoch 10  59.3% | batch:       102 of       172\t|\tloss: 1.76946\n",
      "Evaluating Epoch 10  59.9% | batch:       103 of       172\t|\tloss: 1.29692\n",
      "Evaluating Epoch 10  60.5% | batch:       104 of       172\t|\tloss: 0.793871\n",
      "Evaluating Epoch 10  61.0% | batch:       105 of       172\t|\tloss: 1.28729\n",
      "Evaluating Epoch 10  61.6% | batch:       106 of       172\t|\tloss: 2.14984\n",
      "Evaluating Epoch 10  62.2% | batch:       107 of       172\t|\tloss: 0.959103\n",
      "Evaluating Epoch 10  62.8% | batch:       108 of       172\t|\tloss: 1.04638\n",
      "Evaluating Epoch 10  63.4% | batch:       109 of       172\t|\tloss: 1.88084\n",
      "Evaluating Epoch 10  64.0% | batch:       110 of       172\t|\tloss: 1.68483\n",
      "Evaluating Epoch 10  64.5% | batch:       111 of       172\t|\tloss: 0.761102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 10  65.1% | batch:       112 of       172\t|\tloss: 0.741078\n",
      "Evaluating Epoch 10  65.7% | batch:       113 of       172\t|\tloss: 1.16818\n",
      "Evaluating Epoch 10  66.3% | batch:       114 of       172\t|\tloss: 1.1288\n",
      "Evaluating Epoch 10  66.9% | batch:       115 of       172\t|\tloss: 0.584141\n",
      "Evaluating Epoch 10  67.4% | batch:       116 of       172\t|\tloss: 0.803715\n",
      "Evaluating Epoch 10  68.0% | batch:       117 of       172\t|\tloss: 0.678078\n",
      "Evaluating Epoch 10  68.6% | batch:       118 of       172\t|\tloss: 0.776682\n",
      "Evaluating Epoch 10  69.2% | batch:       119 of       172\t|\tloss: 0.834823\n",
      "Evaluating Epoch 10  69.8% | batch:       120 of       172\t|\tloss: 0.580538\n",
      "Evaluating Epoch 10  70.3% | batch:       121 of       172\t|\tloss: 2.2237\n",
      "Evaluating Epoch 10  70.9% | batch:       122 of       172\t|\tloss: 1.50608\n",
      "Evaluating Epoch 10  71.5% | batch:       123 of       172\t|\tloss: 2.84003\n",
      "Evaluating Epoch 10  72.1% | batch:       124 of       172\t|\tloss: 7.31332\n",
      "Evaluating Epoch 10  72.7% | batch:       125 of       172\t|\tloss: 0.944522\n",
      "Evaluating Epoch 10  73.3% | batch:       126 of       172\t|\tloss: 0.645647\n",
      "Evaluating Epoch 10  73.8% | batch:       127 of       172\t|\tloss: 0.813429\n",
      "Evaluating Epoch 10  74.4% | batch:       128 of       172\t|\tloss: 1.15748\n",
      "Evaluating Epoch 10  75.0% | batch:       129 of       172\t|\tloss: 0.504617\n",
      "Evaluating Epoch 10  75.6% | batch:       130 of       172\t|\tloss: 0.861896\n",
      "Evaluating Epoch 10  76.2% | batch:       131 of       172\t|\tloss: 1.51024\n",
      "Evaluating Epoch 10  76.7% | batch:       132 of       172\t|\tloss: 1.03845\n",
      "Evaluating Epoch 10  77.3% | batch:       133 of       172\t|\tloss: 0.651573\n",
      "Evaluating Epoch 10  77.9% | batch:       134 of       172\t|\tloss: 1.14288\n",
      "Evaluating Epoch 10  78.5% | batch:       135 of       172\t|\tloss: 0.432949\n",
      "Evaluating Epoch 10  79.1% | batch:       136 of       172\t|\tloss: 0.810902\n",
      "Evaluating Epoch 10  79.7% | batch:       137 of       172\t|\tloss: 0.345315\n",
      "Evaluating Epoch 10  80.2% | batch:       138 of       172\t|\tloss: 0.941769\n",
      "Evaluating Epoch 10  80.8% | batch:       139 of       172\t|\tloss: 0.685541\n",
      "Evaluating Epoch 10  81.4% | batch:       140 of       172\t|\tloss: 0.724866\n",
      "Evaluating Epoch 10  82.0% | batch:       141 of       172\t|\tloss: 0.39354\n",
      "Evaluating Epoch 10  82.6% | batch:       142 of       172\t|\tloss: 0.544693\n",
      "Evaluating Epoch 10  83.1% | batch:       143 of       172\t|\tloss: 0.49928\n",
      "Evaluating Epoch 10  83.7% | batch:       144 of       172\t|\tloss: 0.890716\n",
      "Evaluating Epoch 10  84.3% | batch:       145 of       172\t|\tloss: 0.510642\n",
      "Evaluating Epoch 10  84.9% | batch:       146 of       172\t|\tloss: 0.709269\n",
      "Evaluating Epoch 10  85.5% | batch:       147 of       172\t|\tloss: 0.539903\n",
      "Evaluating Epoch 10  86.0% | batch:       148 of       172\t|\tloss: 0.566542\n",
      "Evaluating Epoch 10  86.6% | batch:       149 of       172\t|\tloss: 0.554251\n",
      "Evaluating Epoch 10  87.2% | batch:       150 of       172\t|\tloss: 0.642987\n",
      "Evaluating Epoch 10  87.8% | batch:       151 of       172\t|\tloss: 0.624853\n",
      "Evaluating Epoch 10  88.4% | batch:       152 of       172\t|\tloss: 0.626357\n",
      "Evaluating Epoch 10  89.0% | batch:       153 of       172\t|\tloss: 0.638535\n",
      "Evaluating Epoch 10  89.5% | batch:       154 of       172\t|\tloss: 0.69383\n",
      "Evaluating Epoch 10  90.1% | batch:       155 of       172\t|\tloss: 0.80266\n",
      "Evaluating Epoch 10  90.7% | batch:       156 of       172\t|\tloss: 0.762353\n",
      "Evaluating Epoch 10  91.3% | batch:       157 of       172\t|\tloss: 0.775399\n",
      "Evaluating Epoch 10  91.9% | batch:       158 of       172\t|\tloss: 0.611658\n",
      "Evaluating Epoch 10  92.4% | batch:       159 of       172\t|\tloss: 1.04758\n",
      "Evaluating Epoch 10  93.0% | batch:       160 of       172\t|\tloss: 2.50859\n",
      "Evaluating Epoch 10  93.6% | batch:       161 of       172\t|\tloss: 1.77625\n",
      "Evaluating Epoch 10  94.2% | batch:       162 of       172\t|\tloss: 0.782562\n",
      "Evaluating Epoch 10  94.8% | batch:       163 of       172\t|\tloss: 0.680139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:04:26,855 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.241835832595825 seconds\n",
      "\n",
      "2023-05-25 22:04:26,857 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.015283823013306 seconds\n",
      "2023-05-25 22:04:26,858 | INFO : Avg batch val. time: 0.02334467338961224 seconds\n",
      "2023-05-25 22:04:26,859 | INFO : Avg sample val. time: 0.00018287032941719295 seconds\n",
      "2023-05-25 22:04:26,860 | INFO : Epoch 10 Validation Summary: epoch: 10.000000 | loss: 1.558454 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 10  95.3% | batch:       164 of       172\t|\tloss: 0.873095\n",
      "Evaluating Epoch 10  95.9% | batch:       165 of       172\t|\tloss: 0.561522\n",
      "Evaluating Epoch 10  96.5% | batch:       166 of       172\t|\tloss: 0.577047\n",
      "Evaluating Epoch 10  97.1% | batch:       167 of       172\t|\tloss: 0.877181\n",
      "Evaluating Epoch 10  97.7% | batch:       168 of       172\t|\tloss: 0.558226\n",
      "Evaluating Epoch 10  98.3% | batch:       169 of       172\t|\tloss: 0.643588\n",
      "Evaluating Epoch 10  98.8% | batch:       170 of       172\t|\tloss: 0.816562\n",
      "Evaluating Epoch 10  99.4% | batch:       171 of       172\t|\tloss: 0.537803\n",
      "\n",
      "Training Epoch 11   0.0% | batch:         0 of       686\t|\tloss: 7.06706\n",
      "Training Epoch 11   0.1% | batch:         1 of       686\t|\tloss: 5.97288\n",
      "Training Epoch 11   0.3% | batch:         2 of       686\t|\tloss: 8.11216\n",
      "Training Epoch 11   0.4% | batch:         3 of       686\t|\tloss: 10.0692\n",
      "Training Epoch 11   0.6% | batch:         4 of       686\t|\tloss: 9.96754\n",
      "Training Epoch 11   0.7% | batch:         5 of       686\t|\tloss: 7.36729\n",
      "Training Epoch 11   0.9% | batch:         6 of       686\t|\tloss: 7.10086\n",
      "Training Epoch 11   1.0% | batch:         7 of       686\t|\tloss: 7.63552\n",
      "Training Epoch 11   1.2% | batch:         8 of       686\t|\tloss: 6.7536\n",
      "Training Epoch 11   1.3% | batch:         9 of       686\t|\tloss: 6.62028\n",
      "Training Epoch 11   1.5% | batch:        10 of       686\t|\tloss: 6.7277\n",
      "Training Epoch 11   1.6% | batch:        11 of       686\t|\tloss: 7.17785\n",
      "Training Epoch 11   1.7% | batch:        12 of       686\t|\tloss: 6.86758\n",
      "Training Epoch 11   1.9% | batch:        13 of       686\t|\tloss: 7.04138\n",
      "Training Epoch 11   2.0% | batch:        14 of       686\t|\tloss: 7.34801\n",
      "Training Epoch 11   2.2% | batch:        15 of       686\t|\tloss: 11.1276\n",
      "Training Epoch 11   2.3% | batch:        16 of       686\t|\tloss: 7.42019\n",
      "Training Epoch 11   2.5% | batch:        17 of       686\t|\tloss: 7.02672\n",
      "Training Epoch 11   2.6% | batch:        18 of       686\t|\tloss: 6.68918\n",
      "Training Epoch 11   2.8% | batch:        19 of       686\t|\tloss: 8.69977\n",
      "Training Epoch 11   2.9% | batch:        20 of       686\t|\tloss: 7.59084\n",
      "Training Epoch 11   3.1% | batch:        21 of       686\t|\tloss: 7.50182\n",
      "Training Epoch 11   3.2% | batch:        22 of       686\t|\tloss: 7.06725\n",
      "Training Epoch 11   3.4% | batch:        23 of       686\t|\tloss: 8.86543\n",
      "Training Epoch 11   3.5% | batch:        24 of       686\t|\tloss: 5.43058\n",
      "Training Epoch 11   3.6% | batch:        25 of       686\t|\tloss: 5.9583\n",
      "Training Epoch 11   3.8% | batch:        26 of       686\t|\tloss: 6.9333\n",
      "Training Epoch 11   3.9% | batch:        27 of       686\t|\tloss: 7.47116\n",
      "Training Epoch 11   4.1% | batch:        28 of       686\t|\tloss: 6.90173\n",
      "Training Epoch 11   4.2% | batch:        29 of       686\t|\tloss: 6.22162\n",
      "Training Epoch 11   4.4% | batch:        30 of       686\t|\tloss: 6.77983\n",
      "Training Epoch 11   4.5% | batch:        31 of       686\t|\tloss: 6.32668\n",
      "Training Epoch 11   4.7% | batch:        32 of       686\t|\tloss: 7.26833\n",
      "Training Epoch 11   4.8% | batch:        33 of       686\t|\tloss: 7.26285\n",
      "Training Epoch 11   5.0% | batch:        34 of       686\t|\tloss: 7.76495\n",
      "Training Epoch 11   5.1% | batch:        35 of       686\t|\tloss: 5.92642\n",
      "Training Epoch 11   5.2% | batch:        36 of       686\t|\tloss: 5.3667\n",
      "Training Epoch 11   5.4% | batch:        37 of       686\t|\tloss: 5.96086\n",
      "Training Epoch 11   5.5% | batch:        38 of       686\t|\tloss: 5.18375\n",
      "Training Epoch 11   5.7% | batch:        39 of       686\t|\tloss: 7.46822\n",
      "Training Epoch 11   5.8% | batch:        40 of       686\t|\tloss: 5.8401\n",
      "Training Epoch 11   6.0% | batch:        41 of       686\t|\tloss: 7.63361\n",
      "Training Epoch 11   6.1% | batch:        42 of       686\t|\tloss: 7.50075\n",
      "Training Epoch 11   6.3% | batch:        43 of       686\t|\tloss: 6.98851\n",
      "Training Epoch 11   6.4% | batch:        44 of       686\t|\tloss: 6.39605\n",
      "Training Epoch 11   6.6% | batch:        45 of       686\t|\tloss: 7.12444\n",
      "Training Epoch 11   6.7% | batch:        46 of       686\t|\tloss: 9.06569\n",
      "Training Epoch 11   6.9% | batch:        47 of       686\t|\tloss: 6.56904\n",
      "Training Epoch 11   7.0% | batch:        48 of       686\t|\tloss: 5.79818\n",
      "Training Epoch 11   7.1% | batch:        49 of       686\t|\tloss: 9.49639\n",
      "Training Epoch 11   7.3% | batch:        50 of       686\t|\tloss: 6.93443\n",
      "Training Epoch 11   7.4% | batch:        51 of       686\t|\tloss: 7.52732\n",
      "Training Epoch 11   7.6% | batch:        52 of       686\t|\tloss: 9.78493\n",
      "Training Epoch 11   7.7% | batch:        53 of       686\t|\tloss: 5.997\n",
      "Training Epoch 11   7.9% | batch:        54 of       686\t|\tloss: 5.95096\n",
      "Training Epoch 11   8.0% | batch:        55 of       686\t|\tloss: 10.0744\n",
      "Training Epoch 11   8.2% | batch:        56 of       686\t|\tloss: 6.11727\n",
      "Training Epoch 11   8.3% | batch:        57 of       686\t|\tloss: 10.2899\n",
      "Training Epoch 11   8.5% | batch:        58 of       686\t|\tloss: 7.38211\n",
      "Training Epoch 11   8.6% | batch:        59 of       686\t|\tloss: 5.00653\n",
      "Training Epoch 11   8.7% | batch:        60 of       686\t|\tloss: 7.89893\n",
      "Training Epoch 11   8.9% | batch:        61 of       686\t|\tloss: 5.98739\n",
      "Training Epoch 11   9.0% | batch:        62 of       686\t|\tloss: 6.33431\n",
      "Training Epoch 11   9.2% | batch:        63 of       686\t|\tloss: 8.32144\n",
      "Training Epoch 11   9.3% | batch:        64 of       686\t|\tloss: 10.2191\n",
      "Training Epoch 11   9.5% | batch:        65 of       686\t|\tloss: 8.00206\n",
      "Training Epoch 11   9.6% | batch:        66 of       686\t|\tloss: 7.03891\n",
      "Training Epoch 11   9.8% | batch:        67 of       686\t|\tloss: 7.3362\n",
      "Training Epoch 11   9.9% | batch:        68 of       686\t|\tloss: 5.36554\n",
      "Training Epoch 11  10.1% | batch:        69 of       686\t|\tloss: 6.26365\n",
      "Training Epoch 11  10.2% | batch:        70 of       686\t|\tloss: 9.12322\n",
      "Training Epoch 11  10.3% | batch:        71 of       686\t|\tloss: 7.64469\n",
      "Training Epoch 11  10.5% | batch:        72 of       686\t|\tloss: 5.83006\n",
      "Training Epoch 11  10.6% | batch:        73 of       686\t|\tloss: 6.81028\n",
      "Training Epoch 11  10.8% | batch:        74 of       686\t|\tloss: 6.62372\n",
      "Training Epoch 11  10.9% | batch:        75 of       686\t|\tloss: 6.32728\n",
      "Training Epoch 11  11.1% | batch:        76 of       686\t|\tloss: 6.25437\n",
      "Training Epoch 11  11.2% | batch:        77 of       686\t|\tloss: 5.24184\n",
      "Training Epoch 11  11.4% | batch:        78 of       686\t|\tloss: 9.94934\n",
      "Training Epoch 11  11.5% | batch:        79 of       686\t|\tloss: 6.04634\n",
      "Training Epoch 11  11.7% | batch:        80 of       686\t|\tloss: 5.71278\n",
      "Training Epoch 11  11.8% | batch:        81 of       686\t|\tloss: 5.0908\n",
      "Training Epoch 11  12.0% | batch:        82 of       686\t|\tloss: 7.79526\n",
      "Training Epoch 11  12.1% | batch:        83 of       686\t|\tloss: 9.6996\n",
      "Training Epoch 11  12.2% | batch:        84 of       686\t|\tloss: 5.87422\n",
      "Training Epoch 11  12.4% | batch:        85 of       686\t|\tloss: 7.57734\n",
      "Training Epoch 11  12.5% | batch:        86 of       686\t|\tloss: 5.81319\n",
      "Training Epoch 11  12.7% | batch:        87 of       686\t|\tloss: 7.96355\n",
      "Training Epoch 11  12.8% | batch:        88 of       686\t|\tloss: 9.34516\n",
      "Training Epoch 11  13.0% | batch:        89 of       686\t|\tloss: 7.26634\n",
      "Training Epoch 11  13.1% | batch:        90 of       686\t|\tloss: 7.30462\n",
      "Training Epoch 11  13.3% | batch:        91 of       686\t|\tloss: 8.46414\n",
      "Training Epoch 11  13.4% | batch:        92 of       686\t|\tloss: 6.9227\n",
      "Training Epoch 11  13.6% | batch:        93 of       686\t|\tloss: 6.809\n",
      "Training Epoch 11  13.7% | batch:        94 of       686\t|\tloss: 6.39802\n",
      "Training Epoch 11  13.8% | batch:        95 of       686\t|\tloss: 6.87273\n",
      "Training Epoch 11  14.0% | batch:        96 of       686\t|\tloss: 5.4205\n",
      "Training Epoch 11  14.1% | batch:        97 of       686\t|\tloss: 7.55368\n",
      "Training Epoch 11  14.3% | batch:        98 of       686\t|\tloss: 7.55748\n",
      "Training Epoch 11  14.4% | batch:        99 of       686\t|\tloss: 8.05334\n",
      "Training Epoch 11  14.6% | batch:       100 of       686\t|\tloss: 9.15592\n",
      "Training Epoch 11  14.7% | batch:       101 of       686\t|\tloss: 6.24614\n",
      "Training Epoch 11  14.9% | batch:       102 of       686\t|\tloss: 8.77905\n",
      "Training Epoch 11  15.0% | batch:       103 of       686\t|\tloss: 7.15193\n",
      "Training Epoch 11  15.2% | batch:       104 of       686\t|\tloss: 5.05593\n",
      "Training Epoch 11  15.3% | batch:       105 of       686\t|\tloss: 6.09912\n",
      "Training Epoch 11  15.5% | batch:       106 of       686\t|\tloss: 6.57689\n",
      "Training Epoch 11  15.6% | batch:       107 of       686\t|\tloss: 5.72664\n",
      "Training Epoch 11  15.7% | batch:       108 of       686\t|\tloss: 6.5967\n",
      "Training Epoch 11  15.9% | batch:       109 of       686\t|\tloss: 5.29263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  16.0% | batch:       110 of       686\t|\tloss: 7.25518\n",
      "Training Epoch 11  16.2% | batch:       111 of       686\t|\tloss: 9.60154\n",
      "Training Epoch 11  16.3% | batch:       112 of       686\t|\tloss: 6.22923\n",
      "Training Epoch 11  16.5% | batch:       113 of       686\t|\tloss: 8.47528\n",
      "Training Epoch 11  16.6% | batch:       114 of       686\t|\tloss: 5.35214\n",
      "Training Epoch 11  16.8% | batch:       115 of       686\t|\tloss: 6.92818\n",
      "Training Epoch 11  16.9% | batch:       116 of       686\t|\tloss: 7.18849\n",
      "Training Epoch 11  17.1% | batch:       117 of       686\t|\tloss: 6.32846\n",
      "Training Epoch 11  17.2% | batch:       118 of       686\t|\tloss: 7.67395\n",
      "Training Epoch 11  17.3% | batch:       119 of       686\t|\tloss: 6.52865\n",
      "Training Epoch 11  17.5% | batch:       120 of       686\t|\tloss: 5.09743\n",
      "Training Epoch 11  17.6% | batch:       121 of       686\t|\tloss: 7.04339\n",
      "Training Epoch 11  17.8% | batch:       122 of       686\t|\tloss: 8.87144\n",
      "Training Epoch 11  17.9% | batch:       123 of       686\t|\tloss: 6.37799\n",
      "Training Epoch 11  18.1% | batch:       124 of       686\t|\tloss: 5.75154\n",
      "Training Epoch 11  18.2% | batch:       125 of       686\t|\tloss: 5.14962\n",
      "Training Epoch 11  18.4% | batch:       126 of       686\t|\tloss: 8.1148\n",
      "Training Epoch 11  18.5% | batch:       127 of       686\t|\tloss: 8.12314\n",
      "Training Epoch 11  18.7% | batch:       128 of       686\t|\tloss: 8.82593\n",
      "Training Epoch 11  18.8% | batch:       129 of       686\t|\tloss: 5.74582\n",
      "Training Epoch 11  19.0% | batch:       130 of       686\t|\tloss: 6.32655\n",
      "Training Epoch 11  19.1% | batch:       131 of       686\t|\tloss: 7.21929\n",
      "Training Epoch 11  19.2% | batch:       132 of       686\t|\tloss: 8.29912\n",
      "Training Epoch 11  19.4% | batch:       133 of       686\t|\tloss: 8.34374\n",
      "Training Epoch 11  19.5% | batch:       134 of       686\t|\tloss: 6.62765\n",
      "Training Epoch 11  19.7% | batch:       135 of       686\t|\tloss: 5.62926\n",
      "Training Epoch 11  19.8% | batch:       136 of       686\t|\tloss: 5.87633\n",
      "Training Epoch 11  20.0% | batch:       137 of       686\t|\tloss: 7.57631\n",
      "Training Epoch 11  20.1% | batch:       138 of       686\t|\tloss: 8.64603\n",
      "Training Epoch 11  20.3% | batch:       139 of       686\t|\tloss: 6.17793\n",
      "Training Epoch 11  20.4% | batch:       140 of       686\t|\tloss: 5.60178\n",
      "Training Epoch 11  20.6% | batch:       141 of       686\t|\tloss: 6.03914\n",
      "Training Epoch 11  20.7% | batch:       142 of       686\t|\tloss: 7.69777\n",
      "Training Epoch 11  20.8% | batch:       143 of       686\t|\tloss: 10.6807\n",
      "Training Epoch 11  21.0% | batch:       144 of       686\t|\tloss: 6.08241\n",
      "Training Epoch 11  21.1% | batch:       145 of       686\t|\tloss: 10.775\n",
      "Training Epoch 11  21.3% | batch:       146 of       686\t|\tloss: 10.1395\n",
      "Training Epoch 11  21.4% | batch:       147 of       686\t|\tloss: 7.01605\n",
      "Training Epoch 11  21.6% | batch:       148 of       686\t|\tloss: 7.48877\n",
      "Training Epoch 11  21.7% | batch:       149 of       686\t|\tloss: 5.98164\n",
      "Training Epoch 11  21.9% | batch:       150 of       686\t|\tloss: 7.50439\n",
      "Training Epoch 11  22.0% | batch:       151 of       686\t|\tloss: 6.57895\n",
      "Training Epoch 11  22.2% | batch:       152 of       686\t|\tloss: 9.35708\n",
      "Training Epoch 11  22.3% | batch:       153 of       686\t|\tloss: 5.851\n",
      "Training Epoch 11  22.4% | batch:       154 of       686\t|\tloss: 5.60552\n",
      "Training Epoch 11  22.6% | batch:       155 of       686\t|\tloss: 6.28298\n",
      "Training Epoch 11  22.7% | batch:       156 of       686\t|\tloss: 6.82634\n",
      "Training Epoch 11  22.9% | batch:       157 of       686\t|\tloss: 7.39885\n",
      "Training Epoch 11  23.0% | batch:       158 of       686\t|\tloss: 6.39807\n",
      "Training Epoch 11  23.2% | batch:       159 of       686\t|\tloss: 7.51321\n",
      "Training Epoch 11  23.3% | batch:       160 of       686\t|\tloss: 7.31472\n",
      "Training Epoch 11  23.5% | batch:       161 of       686\t|\tloss: 8.52898\n",
      "Training Epoch 11  23.6% | batch:       162 of       686\t|\tloss: 7.00329\n",
      "Training Epoch 11  23.8% | batch:       163 of       686\t|\tloss: 7.96631\n",
      "Training Epoch 11  23.9% | batch:       164 of       686\t|\tloss: 7.86713\n",
      "Training Epoch 11  24.1% | batch:       165 of       686\t|\tloss: 5.1544\n",
      "Training Epoch 11  24.2% | batch:       166 of       686\t|\tloss: 9.22163\n",
      "Training Epoch 11  24.3% | batch:       167 of       686\t|\tloss: 6.79453\n",
      "Training Epoch 11  24.5% | batch:       168 of       686\t|\tloss: 9.62262\n",
      "Training Epoch 11  24.6% | batch:       169 of       686\t|\tloss: 7.81492\n",
      "Training Epoch 11  24.8% | batch:       170 of       686\t|\tloss: 8.59211\n",
      "Training Epoch 11  24.9% | batch:       171 of       686\t|\tloss: 7.25107\n",
      "Training Epoch 11  25.1% | batch:       172 of       686\t|\tloss: 8.13243\n",
      "Training Epoch 11  25.2% | batch:       173 of       686\t|\tloss: 13.1903\n",
      "Training Epoch 11  25.4% | batch:       174 of       686\t|\tloss: 8.08562\n",
      "Training Epoch 11  25.5% | batch:       175 of       686\t|\tloss: 5.05395\n",
      "Training Epoch 11  25.7% | batch:       176 of       686\t|\tloss: 8.09846\n",
      "Training Epoch 11  25.8% | batch:       177 of       686\t|\tloss: 6.93419\n",
      "Training Epoch 11  25.9% | batch:       178 of       686\t|\tloss: 8.0449\n",
      "Training Epoch 11  26.1% | batch:       179 of       686\t|\tloss: 11.0353\n",
      "Training Epoch 11  26.2% | batch:       180 of       686\t|\tloss: 5.28749\n",
      "Training Epoch 11  26.4% | batch:       181 of       686\t|\tloss: 7.46057\n",
      "Training Epoch 11  26.5% | batch:       182 of       686\t|\tloss: 9.1094\n",
      "Training Epoch 11  26.7% | batch:       183 of       686\t|\tloss: 6.66955\n",
      "Training Epoch 11  26.8% | batch:       184 of       686\t|\tloss: 8.86242\n",
      "Training Epoch 11  27.0% | batch:       185 of       686\t|\tloss: 7.04607\n",
      "Training Epoch 11  27.1% | batch:       186 of       686\t|\tloss: 8.77995\n",
      "Training Epoch 11  27.3% | batch:       187 of       686\t|\tloss: 6.92651\n",
      "Training Epoch 11  27.4% | batch:       188 of       686\t|\tloss: 7.5558\n",
      "Training Epoch 11  27.6% | batch:       189 of       686\t|\tloss: 7.86728\n",
      "Training Epoch 11  27.7% | batch:       190 of       686\t|\tloss: 6.82622\n",
      "Training Epoch 11  27.8% | batch:       191 of       686\t|\tloss: 7.53607\n",
      "Training Epoch 11  28.0% | batch:       192 of       686\t|\tloss: 7.30081\n",
      "Training Epoch 11  28.1% | batch:       193 of       686\t|\tloss: 6.71264\n",
      "Training Epoch 11  28.3% | batch:       194 of       686\t|\tloss: 7.52892\n",
      "Training Epoch 11  28.4% | batch:       195 of       686\t|\tloss: 8.67695\n",
      "Training Epoch 11  28.6% | batch:       196 of       686\t|\tloss: 5.7322\n",
      "Training Epoch 11  28.7% | batch:       197 of       686\t|\tloss: 6.46951\n",
      "Training Epoch 11  28.9% | batch:       198 of       686\t|\tloss: 7.24875\n",
      "Training Epoch 11  29.0% | batch:       199 of       686\t|\tloss: 5.58248\n",
      "Training Epoch 11  29.2% | batch:       200 of       686\t|\tloss: 5.50484\n",
      "Training Epoch 11  29.3% | batch:       201 of       686\t|\tloss: 7.11421\n",
      "Training Epoch 11  29.4% | batch:       202 of       686\t|\tloss: 5.48367\n",
      "Training Epoch 11  29.6% | batch:       203 of       686\t|\tloss: 6.2623\n",
      "Training Epoch 11  29.7% | batch:       204 of       686\t|\tloss: 6.94315\n",
      "Training Epoch 11  29.9% | batch:       205 of       686\t|\tloss: 6.68295\n",
      "Training Epoch 11  30.0% | batch:       206 of       686\t|\tloss: 7.15963\n",
      "Training Epoch 11  30.2% | batch:       207 of       686\t|\tloss: 8.2406\n",
      "Training Epoch 11  30.3% | batch:       208 of       686\t|\tloss: 6.24168\n",
      "Training Epoch 11  30.5% | batch:       209 of       686\t|\tloss: 6.70859\n",
      "Training Epoch 11  30.6% | batch:       210 of       686\t|\tloss: 7.00498\n",
      "Training Epoch 11  30.8% | batch:       211 of       686\t|\tloss: 9.08198\n",
      "Training Epoch 11  30.9% | batch:       212 of       686\t|\tloss: 5.25466\n",
      "Training Epoch 11  31.0% | batch:       213 of       686\t|\tloss: 7.26787\n",
      "Training Epoch 11  31.2% | batch:       214 of       686\t|\tloss: 6.92742\n",
      "Training Epoch 11  31.3% | batch:       215 of       686\t|\tloss: 8.78651\n",
      "Training Epoch 11  31.5% | batch:       216 of       686\t|\tloss: 6.1811\n",
      "Training Epoch 11  31.6% | batch:       217 of       686\t|\tloss: 4.49741\n",
      "Training Epoch 11  31.8% | batch:       218 of       686\t|\tloss: 7.25652\n",
      "Training Epoch 11  31.9% | batch:       219 of       686\t|\tloss: 6.29808\n",
      "Training Epoch 11  32.1% | batch:       220 of       686\t|\tloss: 6.7553\n",
      "Training Epoch 11  32.2% | batch:       221 of       686\t|\tloss: 6.27182\n",
      "Training Epoch 11  32.4% | batch:       222 of       686\t|\tloss: 9.54882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  32.5% | batch:       223 of       686\t|\tloss: 7.92812\n",
      "Training Epoch 11  32.7% | batch:       224 of       686\t|\tloss: 7.34402\n",
      "Training Epoch 11  32.8% | batch:       225 of       686\t|\tloss: 6.85799\n",
      "Training Epoch 11  32.9% | batch:       226 of       686\t|\tloss: 5.7554\n",
      "Training Epoch 11  33.1% | batch:       227 of       686\t|\tloss: 6.23801\n",
      "Training Epoch 11  33.2% | batch:       228 of       686\t|\tloss: 6.41653\n",
      "Training Epoch 11  33.4% | batch:       229 of       686\t|\tloss: 6.95055\n",
      "Training Epoch 11  33.5% | batch:       230 of       686\t|\tloss: 6.55689\n",
      "Training Epoch 11  33.7% | batch:       231 of       686\t|\tloss: 6.87525\n",
      "Training Epoch 11  33.8% | batch:       232 of       686\t|\tloss: 7.85262\n",
      "Training Epoch 11  34.0% | batch:       233 of       686\t|\tloss: 5.41817\n",
      "Training Epoch 11  34.1% | batch:       234 of       686\t|\tloss: 6.7482\n",
      "Training Epoch 11  34.3% | batch:       235 of       686\t|\tloss: 7.06237\n",
      "Training Epoch 11  34.4% | batch:       236 of       686\t|\tloss: 9.13301\n",
      "Training Epoch 11  34.5% | batch:       237 of       686\t|\tloss: 7.34718\n",
      "Training Epoch 11  34.7% | batch:       238 of       686\t|\tloss: 5.87995\n",
      "Training Epoch 11  34.8% | batch:       239 of       686\t|\tloss: 6.27215\n",
      "Training Epoch 11  35.0% | batch:       240 of       686\t|\tloss: 8.69702\n",
      "Training Epoch 11  35.1% | batch:       241 of       686\t|\tloss: 6.22632\n",
      "Training Epoch 11  35.3% | batch:       242 of       686\t|\tloss: 6.54605\n",
      "Training Epoch 11  35.4% | batch:       243 of       686\t|\tloss: 5.34655\n",
      "Training Epoch 11  35.6% | batch:       244 of       686\t|\tloss: 6.25102\n",
      "Training Epoch 11  35.7% | batch:       245 of       686\t|\tloss: 6.05509\n",
      "Training Epoch 11  35.9% | batch:       246 of       686\t|\tloss: 6.60427\n",
      "Training Epoch 11  36.0% | batch:       247 of       686\t|\tloss: 8.86104\n",
      "Training Epoch 11  36.2% | batch:       248 of       686\t|\tloss: 7.70599\n",
      "Training Epoch 11  36.3% | batch:       249 of       686\t|\tloss: 6.07891\n",
      "Training Epoch 11  36.4% | batch:       250 of       686\t|\tloss: 6.4489\n",
      "Training Epoch 11  36.6% | batch:       251 of       686\t|\tloss: 7.50051\n",
      "Training Epoch 11  36.7% | batch:       252 of       686\t|\tloss: 9.0759\n",
      "Training Epoch 11  36.9% | batch:       253 of       686\t|\tloss: 9.2133\n",
      "Training Epoch 11  37.0% | batch:       254 of       686\t|\tloss: 6.0759\n",
      "Training Epoch 11  37.2% | batch:       255 of       686\t|\tloss: 7.63221\n",
      "Training Epoch 11  37.3% | batch:       256 of       686\t|\tloss: 7.46532\n",
      "Training Epoch 11  37.5% | batch:       257 of       686\t|\tloss: 6.06814\n",
      "Training Epoch 11  37.6% | batch:       258 of       686\t|\tloss: 5.95005\n",
      "Training Epoch 11  37.8% | batch:       259 of       686\t|\tloss: 6.44926\n",
      "Training Epoch 11  37.9% | batch:       260 of       686\t|\tloss: 6.01562\n",
      "Training Epoch 11  38.0% | batch:       261 of       686\t|\tloss: 6.85927\n",
      "Training Epoch 11  38.2% | batch:       262 of       686\t|\tloss: 7.91312\n",
      "Training Epoch 11  38.3% | batch:       263 of       686\t|\tloss: 6.54196\n",
      "Training Epoch 11  38.5% | batch:       264 of       686\t|\tloss: 4.70573\n",
      "Training Epoch 11  38.6% | batch:       265 of       686\t|\tloss: 7.28233\n",
      "Training Epoch 11  38.8% | batch:       266 of       686\t|\tloss: 5.21144\n",
      "Training Epoch 11  38.9% | batch:       267 of       686\t|\tloss: 6.38284\n",
      "Training Epoch 11  39.1% | batch:       268 of       686\t|\tloss: 6.05359\n",
      "Training Epoch 11  39.2% | batch:       269 of       686\t|\tloss: 7.07423\n",
      "Training Epoch 11  39.4% | batch:       270 of       686\t|\tloss: 6.71472\n",
      "Training Epoch 11  39.5% | batch:       271 of       686\t|\tloss: 6.56523\n",
      "Training Epoch 11  39.7% | batch:       272 of       686\t|\tloss: 5.01041\n",
      "Training Epoch 11  39.8% | batch:       273 of       686\t|\tloss: 7.85434\n",
      "Training Epoch 11  39.9% | batch:       274 of       686\t|\tloss: 7.28417\n",
      "Training Epoch 11  40.1% | batch:       275 of       686\t|\tloss: 10.6117\n",
      "Training Epoch 11  40.2% | batch:       276 of       686\t|\tloss: 7.44556\n",
      "Training Epoch 11  40.4% | batch:       277 of       686\t|\tloss: 6.81744\n",
      "Training Epoch 11  40.5% | batch:       278 of       686\t|\tloss: 8.01637\n",
      "Training Epoch 11  40.7% | batch:       279 of       686\t|\tloss: 8.66915\n",
      "Training Epoch 11  40.8% | batch:       280 of       686\t|\tloss: 6.29072\n",
      "Training Epoch 11  41.0% | batch:       281 of       686\t|\tloss: 7.08539\n",
      "Training Epoch 11  41.1% | batch:       282 of       686\t|\tloss: 6.87617\n",
      "Training Epoch 11  41.3% | batch:       283 of       686\t|\tloss: 5.14553\n",
      "Training Epoch 11  41.4% | batch:       284 of       686\t|\tloss: 6.08014\n",
      "Training Epoch 11  41.5% | batch:       285 of       686\t|\tloss: 9.51716\n",
      "Training Epoch 11  41.7% | batch:       286 of       686\t|\tloss: 5.46395\n",
      "Training Epoch 11  41.8% | batch:       287 of       686\t|\tloss: 5.17636\n",
      "Training Epoch 11  42.0% | batch:       288 of       686\t|\tloss: 8.24889\n",
      "Training Epoch 11  42.1% | batch:       289 of       686\t|\tloss: 6.77136\n",
      "Training Epoch 11  42.3% | batch:       290 of       686\t|\tloss: 6.69806\n",
      "Training Epoch 11  42.4% | batch:       291 of       686\t|\tloss: 5.41363\n",
      "Training Epoch 11  42.6% | batch:       292 of       686\t|\tloss: 7.17557\n",
      "Training Epoch 11  42.7% | batch:       293 of       686\t|\tloss: 5.42365\n",
      "Training Epoch 11  42.9% | batch:       294 of       686\t|\tloss: 6.3916\n",
      "Training Epoch 11  43.0% | batch:       295 of       686\t|\tloss: 5.13574\n",
      "Training Epoch 11  43.1% | batch:       296 of       686\t|\tloss: 6.86406\n",
      "Training Epoch 11  43.3% | batch:       297 of       686\t|\tloss: 5.99494\n",
      "Training Epoch 11  43.4% | batch:       298 of       686\t|\tloss: 7.65746\n",
      "Training Epoch 11  43.6% | batch:       299 of       686\t|\tloss: 9.99496\n",
      "Training Epoch 11  43.7% | batch:       300 of       686\t|\tloss: 7.46384\n",
      "Training Epoch 11  43.9% | batch:       301 of       686\t|\tloss: 6.89926\n",
      "Training Epoch 11  44.0% | batch:       302 of       686\t|\tloss: 6.47234\n",
      "Training Epoch 11  44.2% | batch:       303 of       686\t|\tloss: 6.27235\n",
      "Training Epoch 11  44.3% | batch:       304 of       686\t|\tloss: 6.25894\n",
      "Training Epoch 11  44.5% | batch:       305 of       686\t|\tloss: 6.85635\n",
      "Training Epoch 11  44.6% | batch:       306 of       686\t|\tloss: 6.94427\n",
      "Training Epoch 11  44.8% | batch:       307 of       686\t|\tloss: 6.32034\n",
      "Training Epoch 11  44.9% | batch:       308 of       686\t|\tloss: 6.12701\n",
      "Training Epoch 11  45.0% | batch:       309 of       686\t|\tloss: 6.4902\n",
      "Training Epoch 11  45.2% | batch:       310 of       686\t|\tloss: 4.68538\n",
      "Training Epoch 11  45.3% | batch:       311 of       686\t|\tloss: 6.82496\n",
      "Training Epoch 11  45.5% | batch:       312 of       686\t|\tloss: 5.85391\n",
      "Training Epoch 11  45.6% | batch:       313 of       686\t|\tloss: 6.71075\n",
      "Training Epoch 11  45.8% | batch:       314 of       686\t|\tloss: 7.18658\n",
      "Training Epoch 11  45.9% | batch:       315 of       686\t|\tloss: 8.88412\n",
      "Training Epoch 11  46.1% | batch:       316 of       686\t|\tloss: 7.69238\n",
      "Training Epoch 11  46.2% | batch:       317 of       686\t|\tloss: 6.28522\n",
      "Training Epoch 11  46.4% | batch:       318 of       686\t|\tloss: 9.22769\n",
      "Training Epoch 11  46.5% | batch:       319 of       686\t|\tloss: 7.57042\n",
      "Training Epoch 11  46.6% | batch:       320 of       686\t|\tloss: 6.3811\n",
      "Training Epoch 11  46.8% | batch:       321 of       686\t|\tloss: 5.52546\n",
      "Training Epoch 11  46.9% | batch:       322 of       686\t|\tloss: 4.68156\n",
      "Training Epoch 11  47.1% | batch:       323 of       686\t|\tloss: 8.23932\n",
      "Training Epoch 11  47.2% | batch:       324 of       686\t|\tloss: 5.85504\n",
      "Training Epoch 11  47.4% | batch:       325 of       686\t|\tloss: 9.43701\n",
      "Training Epoch 11  47.5% | batch:       326 of       686\t|\tloss: 6.22815\n",
      "Training Epoch 11  47.7% | batch:       327 of       686\t|\tloss: 8.43871\n",
      "Training Epoch 11  47.8% | batch:       328 of       686\t|\tloss: 7.91334\n",
      "Training Epoch 11  48.0% | batch:       329 of       686\t|\tloss: 6.58649\n",
      "Training Epoch 11  48.1% | batch:       330 of       686\t|\tloss: 6.28415\n",
      "Training Epoch 11  48.3% | batch:       331 of       686\t|\tloss: 5.97482\n",
      "Training Epoch 11  48.4% | batch:       332 of       686\t|\tloss: 5.64251\n",
      "Training Epoch 11  48.5% | batch:       333 of       686\t|\tloss: 4.85086\n",
      "Training Epoch 11  48.7% | batch:       334 of       686\t|\tloss: 7.06367\n",
      "Training Epoch 11  48.8% | batch:       335 of       686\t|\tloss: 4.97253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  49.0% | batch:       336 of       686\t|\tloss: 5.55566\n",
      "Training Epoch 11  49.1% | batch:       337 of       686\t|\tloss: 8.41299\n",
      "Training Epoch 11  49.3% | batch:       338 of       686\t|\tloss: 5.84288\n",
      "Training Epoch 11  49.4% | batch:       339 of       686\t|\tloss: 5.17732\n",
      "Training Epoch 11  49.6% | batch:       340 of       686\t|\tloss: 5.92433\n",
      "Training Epoch 11  49.7% | batch:       341 of       686\t|\tloss: 5.50796\n",
      "Training Epoch 11  49.9% | batch:       342 of       686\t|\tloss: 6.38616\n",
      "Training Epoch 11  50.0% | batch:       343 of       686\t|\tloss: 7.52922\n",
      "Training Epoch 11  50.1% | batch:       344 of       686\t|\tloss: 5.78943\n",
      "Training Epoch 11  50.3% | batch:       345 of       686\t|\tloss: 6.79629\n",
      "Training Epoch 11  50.4% | batch:       346 of       686\t|\tloss: 5.63017\n",
      "Training Epoch 11  50.6% | batch:       347 of       686\t|\tloss: 6.00028\n",
      "Training Epoch 11  50.7% | batch:       348 of       686\t|\tloss: 6.984\n",
      "Training Epoch 11  50.9% | batch:       349 of       686\t|\tloss: 7.48978\n",
      "Training Epoch 11  51.0% | batch:       350 of       686\t|\tloss: 6.12416\n",
      "Training Epoch 11  51.2% | batch:       351 of       686\t|\tloss: 6.05096\n",
      "Training Epoch 11  51.3% | batch:       352 of       686\t|\tloss: 5.27412\n",
      "Training Epoch 11  51.5% | batch:       353 of       686\t|\tloss: 6.26256\n",
      "Training Epoch 11  51.6% | batch:       354 of       686\t|\tloss: 6.04071\n",
      "Training Epoch 11  51.7% | batch:       355 of       686\t|\tloss: 8.07785\n",
      "Training Epoch 11  51.9% | batch:       356 of       686\t|\tloss: 8.19015\n",
      "Training Epoch 11  52.0% | batch:       357 of       686\t|\tloss: 7.53889\n",
      "Training Epoch 11  52.2% | batch:       358 of       686\t|\tloss: 7.32269\n",
      "Training Epoch 11  52.3% | batch:       359 of       686\t|\tloss: 7.40117\n",
      "Training Epoch 11  52.5% | batch:       360 of       686\t|\tloss: 7.77719\n",
      "Training Epoch 11  52.6% | batch:       361 of       686\t|\tloss: 7.3183\n",
      "Training Epoch 11  52.8% | batch:       362 of       686\t|\tloss: 5.5041\n",
      "Training Epoch 11  52.9% | batch:       363 of       686\t|\tloss: 7.6327\n",
      "Training Epoch 11  53.1% | batch:       364 of       686\t|\tloss: 5.75027\n",
      "Training Epoch 11  53.2% | batch:       365 of       686\t|\tloss: 7.47311\n",
      "Training Epoch 11  53.4% | batch:       366 of       686\t|\tloss: 5.17882\n",
      "Training Epoch 11  53.5% | batch:       367 of       686\t|\tloss: 5.46316\n",
      "Training Epoch 11  53.6% | batch:       368 of       686\t|\tloss: 7.03535\n",
      "Training Epoch 11  53.8% | batch:       369 of       686\t|\tloss: 5.45561\n",
      "Training Epoch 11  53.9% | batch:       370 of       686\t|\tloss: 6.1072\n",
      "Training Epoch 11  54.1% | batch:       371 of       686\t|\tloss: 6.68289\n",
      "Training Epoch 11  54.2% | batch:       372 of       686\t|\tloss: 9.11447\n",
      "Training Epoch 11  54.4% | batch:       373 of       686\t|\tloss: 5.89279\n",
      "Training Epoch 11  54.5% | batch:       374 of       686\t|\tloss: 9.14918\n",
      "Training Epoch 11  54.7% | batch:       375 of       686\t|\tloss: 7.41719\n",
      "Training Epoch 11  54.8% | batch:       376 of       686\t|\tloss: 10.7776\n",
      "Training Epoch 11  55.0% | batch:       377 of       686\t|\tloss: 7.02612\n",
      "Training Epoch 11  55.1% | batch:       378 of       686\t|\tloss: 7.42685\n",
      "Training Epoch 11  55.2% | batch:       379 of       686\t|\tloss: 8.01991\n",
      "Training Epoch 11  55.4% | batch:       380 of       686\t|\tloss: 5.37984\n",
      "Training Epoch 11  55.5% | batch:       381 of       686\t|\tloss: 5.74831\n",
      "Training Epoch 11  55.7% | batch:       382 of       686\t|\tloss: 6.86324\n",
      "Training Epoch 11  55.8% | batch:       383 of       686\t|\tloss: 7.14053\n",
      "Training Epoch 11  56.0% | batch:       384 of       686\t|\tloss: 9.71036\n",
      "Training Epoch 11  56.1% | batch:       385 of       686\t|\tloss: 6.11457\n",
      "Training Epoch 11  56.3% | batch:       386 of       686\t|\tloss: 8.56909\n",
      "Training Epoch 11  56.4% | batch:       387 of       686\t|\tloss: 5.3522\n",
      "Training Epoch 11  56.6% | batch:       388 of       686\t|\tloss: 5.47383\n",
      "Training Epoch 11  56.7% | batch:       389 of       686\t|\tloss: 5.60137\n",
      "Training Epoch 11  56.9% | batch:       390 of       686\t|\tloss: 5.86401\n",
      "Training Epoch 11  57.0% | batch:       391 of       686\t|\tloss: 5.40523\n",
      "Training Epoch 11  57.1% | batch:       392 of       686\t|\tloss: 6.72856\n",
      "Training Epoch 11  57.3% | batch:       393 of       686\t|\tloss: 6.74019\n",
      "Training Epoch 11  57.4% | batch:       394 of       686\t|\tloss: 6.74465\n",
      "Training Epoch 11  57.6% | batch:       395 of       686\t|\tloss: 6.86025\n",
      "Training Epoch 11  57.7% | batch:       396 of       686\t|\tloss: 6.98759\n",
      "Training Epoch 11  57.9% | batch:       397 of       686\t|\tloss: 6.37093\n",
      "Training Epoch 11  58.0% | batch:       398 of       686\t|\tloss: 8.41293\n",
      "Training Epoch 11  58.2% | batch:       399 of       686\t|\tloss: 5.71883\n",
      "Training Epoch 11  58.3% | batch:       400 of       686\t|\tloss: 5.10957\n",
      "Training Epoch 11  58.5% | batch:       401 of       686\t|\tloss: 6.63282\n",
      "Training Epoch 11  58.6% | batch:       402 of       686\t|\tloss: 8.29301\n",
      "Training Epoch 11  58.7% | batch:       403 of       686\t|\tloss: 12.8435\n",
      "Training Epoch 11  58.9% | batch:       404 of       686\t|\tloss: 5.79106\n",
      "Training Epoch 11  59.0% | batch:       405 of       686\t|\tloss: 5.55936\n",
      "Training Epoch 11  59.2% | batch:       406 of       686\t|\tloss: 4.94758\n",
      "Training Epoch 11  59.3% | batch:       407 of       686\t|\tloss: 6.87665\n",
      "Training Epoch 11  59.5% | batch:       408 of       686\t|\tloss: 10.029\n",
      "Training Epoch 11  59.6% | batch:       409 of       686\t|\tloss: 6.37317\n",
      "Training Epoch 11  59.8% | batch:       410 of       686\t|\tloss: 6.31349\n",
      "Training Epoch 11  59.9% | batch:       411 of       686\t|\tloss: 6.3828\n",
      "Training Epoch 11  60.1% | batch:       412 of       686\t|\tloss: 8.14827\n",
      "Training Epoch 11  60.2% | batch:       413 of       686\t|\tloss: 6.86432\n",
      "Training Epoch 11  60.3% | batch:       414 of       686\t|\tloss: 6.72706\n",
      "Training Epoch 11  60.5% | batch:       415 of       686\t|\tloss: 6.16472\n",
      "Training Epoch 11  60.6% | batch:       416 of       686\t|\tloss: 6.25977\n",
      "Training Epoch 11  60.8% | batch:       417 of       686\t|\tloss: 6.91273\n",
      "Training Epoch 11  60.9% | batch:       418 of       686\t|\tloss: 6.36628\n",
      "Training Epoch 11  61.1% | batch:       419 of       686\t|\tloss: 6.18311\n",
      "Training Epoch 11  61.2% | batch:       420 of       686\t|\tloss: 5.5008\n",
      "Training Epoch 11  61.4% | batch:       421 of       686\t|\tloss: 5.06317\n",
      "Training Epoch 11  61.5% | batch:       422 of       686\t|\tloss: 6.76529\n",
      "Training Epoch 11  61.7% | batch:       423 of       686\t|\tloss: 6.95336\n",
      "Training Epoch 11  61.8% | batch:       424 of       686\t|\tloss: 5.33997\n",
      "Training Epoch 11  62.0% | batch:       425 of       686\t|\tloss: 7.57259\n",
      "Training Epoch 11  62.1% | batch:       426 of       686\t|\tloss: 8.47128\n",
      "Training Epoch 11  62.2% | batch:       427 of       686\t|\tloss: 5.95818\n",
      "Training Epoch 11  62.4% | batch:       428 of       686\t|\tloss: 7.08987\n",
      "Training Epoch 11  62.5% | batch:       429 of       686\t|\tloss: 6.15044\n",
      "Training Epoch 11  62.7% | batch:       430 of       686\t|\tloss: 6.8432\n",
      "Training Epoch 11  62.8% | batch:       431 of       686\t|\tloss: 7.48734\n",
      "Training Epoch 11  63.0% | batch:       432 of       686\t|\tloss: 6.82579\n",
      "Training Epoch 11  63.1% | batch:       433 of       686\t|\tloss: 5.16318\n",
      "Training Epoch 11  63.3% | batch:       434 of       686\t|\tloss: 6.63564\n",
      "Training Epoch 11  63.4% | batch:       435 of       686\t|\tloss: 6.58205\n",
      "Training Epoch 11  63.6% | batch:       436 of       686\t|\tloss: 5.97327\n",
      "Training Epoch 11  63.7% | batch:       437 of       686\t|\tloss: 9.00769\n",
      "Training Epoch 11  63.8% | batch:       438 of       686\t|\tloss: 6.562\n",
      "Training Epoch 11  64.0% | batch:       439 of       686\t|\tloss: 7.32668\n",
      "Training Epoch 11  64.1% | batch:       440 of       686\t|\tloss: 8.20622\n",
      "Training Epoch 11  64.3% | batch:       441 of       686\t|\tloss: 7.55664\n",
      "Training Epoch 11  64.4% | batch:       442 of       686\t|\tloss: 5.79953\n",
      "Training Epoch 11  64.6% | batch:       443 of       686\t|\tloss: 5.66791\n",
      "Training Epoch 11  64.7% | batch:       444 of       686\t|\tloss: 6.37431\n",
      "Training Epoch 11  64.9% | batch:       445 of       686\t|\tloss: 6.78063\n",
      "Training Epoch 11  65.0% | batch:       446 of       686\t|\tloss: 5.90396\n",
      "Training Epoch 11  65.2% | batch:       447 of       686\t|\tloss: 6.22582\n",
      "Training Epoch 11  65.3% | batch:       448 of       686\t|\tloss: 5.81061\n",
      "Training Epoch 11  65.5% | batch:       449 of       686\t|\tloss: 5.81563\n",
      "Training Epoch 11  65.6% | batch:       450 of       686\t|\tloss: 6.78205\n",
      "Training Epoch 11  65.7% | batch:       451 of       686\t|\tloss: 4.86543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  65.9% | batch:       452 of       686\t|\tloss: 7.74891\n",
      "Training Epoch 11  66.0% | batch:       453 of       686\t|\tloss: 8.4041\n",
      "Training Epoch 11  66.2% | batch:       454 of       686\t|\tloss: 5.59341\n",
      "Training Epoch 11  66.3% | batch:       455 of       686\t|\tloss: 5.27483\n",
      "Training Epoch 11  66.5% | batch:       456 of       686\t|\tloss: 5.64036\n",
      "Training Epoch 11  66.6% | batch:       457 of       686\t|\tloss: 7.36692\n",
      "Training Epoch 11  66.8% | batch:       458 of       686\t|\tloss: 7.4089\n",
      "Training Epoch 11  66.9% | batch:       459 of       686\t|\tloss: 8.68801\n",
      "Training Epoch 11  67.1% | batch:       460 of       686\t|\tloss: 6.88212\n",
      "Training Epoch 11  67.2% | batch:       461 of       686\t|\tloss: 5.59959\n",
      "Training Epoch 11  67.3% | batch:       462 of       686\t|\tloss: 6.38024\n",
      "Training Epoch 11  67.5% | batch:       463 of       686\t|\tloss: 7.7332\n",
      "Training Epoch 11  67.6% | batch:       464 of       686\t|\tloss: 6.39709\n",
      "Training Epoch 11  67.8% | batch:       465 of       686\t|\tloss: 5.65321\n",
      "Training Epoch 11  67.9% | batch:       466 of       686\t|\tloss: 4.57396\n",
      "Training Epoch 11  68.1% | batch:       467 of       686\t|\tloss: 7.43919\n",
      "Training Epoch 11  68.2% | batch:       468 of       686\t|\tloss: 4.89737\n",
      "Training Epoch 11  68.4% | batch:       469 of       686\t|\tloss: 5.27024\n",
      "Training Epoch 11  68.5% | batch:       470 of       686\t|\tloss: 6.1526\n",
      "Training Epoch 11  68.7% | batch:       471 of       686\t|\tloss: 6.70408\n",
      "Training Epoch 11  68.8% | batch:       472 of       686\t|\tloss: 6.97183\n",
      "Training Epoch 11  69.0% | batch:       473 of       686\t|\tloss: 7.16428\n",
      "Training Epoch 11  69.1% | batch:       474 of       686\t|\tloss: 6.39125\n",
      "Training Epoch 11  69.2% | batch:       475 of       686\t|\tloss: 6.96786\n",
      "Training Epoch 11  69.4% | batch:       476 of       686\t|\tloss: 4.97036\n",
      "Training Epoch 11  69.5% | batch:       477 of       686\t|\tloss: 6.50249\n",
      "Training Epoch 11  69.7% | batch:       478 of       686\t|\tloss: 7.20674\n",
      "Training Epoch 11  69.8% | batch:       479 of       686\t|\tloss: 6.18609\n",
      "Training Epoch 11  70.0% | batch:       480 of       686\t|\tloss: 5.65751\n",
      "Training Epoch 11  70.1% | batch:       481 of       686\t|\tloss: 4.79692\n",
      "Training Epoch 11  70.3% | batch:       482 of       686\t|\tloss: 6.02367\n",
      "Training Epoch 11  70.4% | batch:       483 of       686\t|\tloss: 7.34912\n",
      "Training Epoch 11  70.6% | batch:       484 of       686\t|\tloss: 8.06825\n",
      "Training Epoch 11  70.7% | batch:       485 of       686\t|\tloss: 5.4421\n",
      "Training Epoch 11  70.8% | batch:       486 of       686\t|\tloss: 8.64069\n",
      "Training Epoch 11  71.0% | batch:       487 of       686\t|\tloss: 7.72022\n",
      "Training Epoch 11  71.1% | batch:       488 of       686\t|\tloss: 6.43777\n",
      "Training Epoch 11  71.3% | batch:       489 of       686\t|\tloss: 5.64345\n",
      "Training Epoch 11  71.4% | batch:       490 of       686\t|\tloss: 6.74315\n",
      "Training Epoch 11  71.6% | batch:       491 of       686\t|\tloss: 7.08739\n",
      "Training Epoch 11  71.7% | batch:       492 of       686\t|\tloss: 5.97013\n",
      "Training Epoch 11  71.9% | batch:       493 of       686\t|\tloss: 5.75025\n",
      "Training Epoch 11  72.0% | batch:       494 of       686\t|\tloss: 4.71921\n",
      "Training Epoch 11  72.2% | batch:       495 of       686\t|\tloss: 6.56361\n",
      "Training Epoch 11  72.3% | batch:       496 of       686\t|\tloss: 7.04502\n",
      "Training Epoch 11  72.4% | batch:       497 of       686\t|\tloss: 6.67639\n",
      "Training Epoch 11  72.6% | batch:       498 of       686\t|\tloss: 5.42156\n",
      "Training Epoch 11  72.7% | batch:       499 of       686\t|\tloss: 5.45616\n",
      "Training Epoch 11  72.9% | batch:       500 of       686\t|\tloss: 6.85494\n",
      "Training Epoch 11  73.0% | batch:       501 of       686\t|\tloss: 5.47669\n",
      "Training Epoch 11  73.2% | batch:       502 of       686\t|\tloss: 8.28655\n",
      "Training Epoch 11  73.3% | batch:       503 of       686\t|\tloss: 6.26188\n",
      "Training Epoch 11  73.5% | batch:       504 of       686\t|\tloss: 7.70915\n",
      "Training Epoch 11  73.6% | batch:       505 of       686\t|\tloss: 8.02039\n",
      "Training Epoch 11  73.8% | batch:       506 of       686\t|\tloss: 6.14034\n",
      "Training Epoch 11  73.9% | batch:       507 of       686\t|\tloss: 7.7593\n",
      "Training Epoch 11  74.1% | batch:       508 of       686\t|\tloss: 6.13942\n",
      "Training Epoch 11  74.2% | batch:       509 of       686\t|\tloss: 6.38882\n",
      "Training Epoch 11  74.3% | batch:       510 of       686\t|\tloss: 6.14261\n",
      "Training Epoch 11  74.5% | batch:       511 of       686\t|\tloss: 5.85709\n",
      "Training Epoch 11  74.6% | batch:       512 of       686\t|\tloss: 5.85074\n",
      "Training Epoch 11  74.8% | batch:       513 of       686\t|\tloss: 7.16093\n",
      "Training Epoch 11  74.9% | batch:       514 of       686\t|\tloss: 9.10331\n",
      "Training Epoch 11  75.1% | batch:       515 of       686\t|\tloss: 6.74392\n",
      "Training Epoch 11  75.2% | batch:       516 of       686\t|\tloss: 6.22672\n",
      "Training Epoch 11  75.4% | batch:       517 of       686\t|\tloss: 7.34541\n",
      "Training Epoch 11  75.5% | batch:       518 of       686\t|\tloss: 7.04941\n",
      "Training Epoch 11  75.7% | batch:       519 of       686\t|\tloss: 5.83765\n",
      "Training Epoch 11  75.8% | batch:       520 of       686\t|\tloss: 6.96293\n",
      "Training Epoch 11  75.9% | batch:       521 of       686\t|\tloss: 6.11738\n",
      "Training Epoch 11  76.1% | batch:       522 of       686\t|\tloss: 6.8249\n",
      "Training Epoch 11  76.2% | batch:       523 of       686\t|\tloss: 4.98251\n",
      "Training Epoch 11  76.4% | batch:       524 of       686\t|\tloss: 7.1204\n",
      "Training Epoch 11  76.5% | batch:       525 of       686\t|\tloss: 5.01661\n",
      "Training Epoch 11  76.7% | batch:       526 of       686\t|\tloss: 7.72054\n",
      "Training Epoch 11  76.8% | batch:       527 of       686\t|\tloss: 6.70895\n",
      "Training Epoch 11  77.0% | batch:       528 of       686\t|\tloss: 4.2928\n",
      "Training Epoch 11  77.1% | batch:       529 of       686\t|\tloss: 5.18153\n",
      "Training Epoch 11  77.3% | batch:       530 of       686\t|\tloss: 4.8667\n",
      "Training Epoch 11  77.4% | batch:       531 of       686\t|\tloss: 7.08136\n",
      "Training Epoch 11  77.6% | batch:       532 of       686\t|\tloss: 5.51994\n",
      "Training Epoch 11  77.7% | batch:       533 of       686\t|\tloss: 9.66293\n",
      "Training Epoch 11  77.8% | batch:       534 of       686\t|\tloss: 7.74363\n",
      "Training Epoch 11  78.0% | batch:       535 of       686\t|\tloss: 7.13061\n",
      "Training Epoch 11  78.1% | batch:       536 of       686\t|\tloss: 6.55877\n",
      "Training Epoch 11  78.3% | batch:       537 of       686\t|\tloss: 6.08583\n",
      "Training Epoch 11  78.4% | batch:       538 of       686\t|\tloss: 4.5398\n",
      "Training Epoch 11  78.6% | batch:       539 of       686\t|\tloss: 6.62674\n",
      "Training Epoch 11  78.7% | batch:       540 of       686\t|\tloss: 4.98233\n",
      "Training Epoch 11  78.9% | batch:       541 of       686\t|\tloss: 8.53284\n",
      "Training Epoch 11  79.0% | batch:       542 of       686\t|\tloss: 6.16226\n",
      "Training Epoch 11  79.2% | batch:       543 of       686\t|\tloss: 7.41823\n",
      "Training Epoch 11  79.3% | batch:       544 of       686\t|\tloss: 5.35282\n",
      "Training Epoch 11  79.4% | batch:       545 of       686\t|\tloss: 7.75798\n",
      "Training Epoch 11  79.6% | batch:       546 of       686\t|\tloss: 5.56342\n",
      "Training Epoch 11  79.7% | batch:       547 of       686\t|\tloss: 4.67585\n",
      "Training Epoch 11  79.9% | batch:       548 of       686\t|\tloss: 6.31974\n",
      "Training Epoch 11  80.0% | batch:       549 of       686\t|\tloss: 7.61096\n",
      "Training Epoch 11  80.2% | batch:       550 of       686\t|\tloss: 6.46507\n",
      "Training Epoch 11  80.3% | batch:       551 of       686\t|\tloss: 5.3282\n",
      "Training Epoch 11  80.5% | batch:       552 of       686\t|\tloss: 5.93766\n",
      "Training Epoch 11  80.6% | batch:       553 of       686\t|\tloss: 5.55278\n",
      "Training Epoch 11  80.8% | batch:       554 of       686\t|\tloss: 5.92459\n",
      "Training Epoch 11  80.9% | batch:       555 of       686\t|\tloss: 5.63702\n",
      "Training Epoch 11  81.0% | batch:       556 of       686\t|\tloss: 5.52177\n",
      "Training Epoch 11  81.2% | batch:       557 of       686\t|\tloss: 6.37229\n",
      "Training Epoch 11  81.3% | batch:       558 of       686\t|\tloss: 7.29138\n",
      "Training Epoch 11  81.5% | batch:       559 of       686\t|\tloss: 6.83122\n",
      "Training Epoch 11  81.6% | batch:       560 of       686\t|\tloss: 6.52224\n",
      "Training Epoch 11  81.8% | batch:       561 of       686\t|\tloss: 9.46712\n",
      "Training Epoch 11  81.9% | batch:       562 of       686\t|\tloss: 5.92196\n",
      "Training Epoch 11  82.1% | batch:       563 of       686\t|\tloss: 5.6102\n",
      "Training Epoch 11  82.2% | batch:       564 of       686\t|\tloss: 5.08584\n",
      "Training Epoch 11  82.4% | batch:       565 of       686\t|\tloss: 6.11907\n",
      "Training Epoch 11  82.5% | batch:       566 of       686\t|\tloss: 6.48243\n",
      "Training Epoch 11  82.7% | batch:       567 of       686\t|\tloss: 7.40408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  82.8% | batch:       568 of       686\t|\tloss: 5.79216\n",
      "Training Epoch 11  82.9% | batch:       569 of       686\t|\tloss: 5.16075\n",
      "Training Epoch 11  83.1% | batch:       570 of       686\t|\tloss: 6.93188\n",
      "Training Epoch 11  83.2% | batch:       571 of       686\t|\tloss: 5.0789\n",
      "Training Epoch 11  83.4% | batch:       572 of       686\t|\tloss: 7.17151\n",
      "Training Epoch 11  83.5% | batch:       573 of       686\t|\tloss: 4.59237\n",
      "Training Epoch 11  83.7% | batch:       574 of       686\t|\tloss: 6.56997\n",
      "Training Epoch 11  83.8% | batch:       575 of       686\t|\tloss: 5.9528\n",
      "Training Epoch 11  84.0% | batch:       576 of       686\t|\tloss: 7.15806\n",
      "Training Epoch 11  84.1% | batch:       577 of       686\t|\tloss: 5.26236\n",
      "Training Epoch 11  84.3% | batch:       578 of       686\t|\tloss: 7.13614\n",
      "Training Epoch 11  84.4% | batch:       579 of       686\t|\tloss: 6.96516\n",
      "Training Epoch 11  84.5% | batch:       580 of       686\t|\tloss: 5.4946\n",
      "Training Epoch 11  84.7% | batch:       581 of       686\t|\tloss: 5.71839\n",
      "Training Epoch 11  84.8% | batch:       582 of       686\t|\tloss: 5.99724\n",
      "Training Epoch 11  85.0% | batch:       583 of       686\t|\tloss: 5.82029\n",
      "Training Epoch 11  85.1% | batch:       584 of       686\t|\tloss: 6.30215\n",
      "Training Epoch 11  85.3% | batch:       585 of       686\t|\tloss: 6.10602\n",
      "Training Epoch 11  85.4% | batch:       586 of       686\t|\tloss: 5.03618\n",
      "Training Epoch 11  85.6% | batch:       587 of       686\t|\tloss: 5.09329\n",
      "Training Epoch 11  85.7% | batch:       588 of       686\t|\tloss: 6.04809\n",
      "Training Epoch 11  85.9% | batch:       589 of       686\t|\tloss: 6.18689\n",
      "Training Epoch 11  86.0% | batch:       590 of       686\t|\tloss: 5.42976\n",
      "Training Epoch 11  86.2% | batch:       591 of       686\t|\tloss: 5.43525\n",
      "Training Epoch 11  86.3% | batch:       592 of       686\t|\tloss: 5.09595\n",
      "Training Epoch 11  86.4% | batch:       593 of       686\t|\tloss: 8.35814\n",
      "Training Epoch 11  86.6% | batch:       594 of       686\t|\tloss: 8.36673\n",
      "Training Epoch 11  86.7% | batch:       595 of       686\t|\tloss: 6.73834\n",
      "Training Epoch 11  86.9% | batch:       596 of       686\t|\tloss: 7.31689\n",
      "Training Epoch 11  87.0% | batch:       597 of       686\t|\tloss: 5.85597\n",
      "Training Epoch 11  87.2% | batch:       598 of       686\t|\tloss: 6.97234\n",
      "Training Epoch 11  87.3% | batch:       599 of       686\t|\tloss: 8.41906\n",
      "Training Epoch 11  87.5% | batch:       600 of       686\t|\tloss: 5.49707\n",
      "Training Epoch 11  87.6% | batch:       601 of       686\t|\tloss: 6.39013\n",
      "Training Epoch 11  87.8% | batch:       602 of       686\t|\tloss: 7.29541\n",
      "Training Epoch 11  87.9% | batch:       603 of       686\t|\tloss: 6.92918\n",
      "Training Epoch 11  88.0% | batch:       604 of       686\t|\tloss: 6.53714\n",
      "Training Epoch 11  88.2% | batch:       605 of       686\t|\tloss: 5.51667\n",
      "Training Epoch 11  88.3% | batch:       606 of       686\t|\tloss: 7.49585\n",
      "Training Epoch 11  88.5% | batch:       607 of       686\t|\tloss: 5.42698\n",
      "Training Epoch 11  88.6% | batch:       608 of       686\t|\tloss: 6.26425\n",
      "Training Epoch 11  88.8% | batch:       609 of       686\t|\tloss: 7.01568\n",
      "Training Epoch 11  88.9% | batch:       610 of       686\t|\tloss: 5.68101\n",
      "Training Epoch 11  89.1% | batch:       611 of       686\t|\tloss: 6.47362\n",
      "Training Epoch 11  89.2% | batch:       612 of       686\t|\tloss: 7.51675\n",
      "Training Epoch 11  89.4% | batch:       613 of       686\t|\tloss: 7.77714\n",
      "Training Epoch 11  89.5% | batch:       614 of       686\t|\tloss: 4.18148\n",
      "Training Epoch 11  89.7% | batch:       615 of       686\t|\tloss: 4.85318\n",
      "Training Epoch 11  89.8% | batch:       616 of       686\t|\tloss: 5.52339\n",
      "Training Epoch 11  89.9% | batch:       617 of       686\t|\tloss: 5.30308\n",
      "Training Epoch 11  90.1% | batch:       618 of       686\t|\tloss: 5.72282\n",
      "Training Epoch 11  90.2% | batch:       619 of       686\t|\tloss: 7.84593\n",
      "Training Epoch 11  90.4% | batch:       620 of       686\t|\tloss: 5.00213\n",
      "Training Epoch 11  90.5% | batch:       621 of       686\t|\tloss: 5.55638\n",
      "Training Epoch 11  90.7% | batch:       622 of       686\t|\tloss: 8.42885\n",
      "Training Epoch 11  90.8% | batch:       623 of       686\t|\tloss: 5.90662\n",
      "Training Epoch 11  91.0% | batch:       624 of       686\t|\tloss: 6.4168\n",
      "Training Epoch 11  91.1% | batch:       625 of       686\t|\tloss: 6.28074\n",
      "Training Epoch 11  91.3% | batch:       626 of       686\t|\tloss: 5.49845\n",
      "Training Epoch 11  91.4% | batch:       627 of       686\t|\tloss: 6.80447\n",
      "Training Epoch 11  91.5% | batch:       628 of       686\t|\tloss: 6.58735\n",
      "Training Epoch 11  91.7% | batch:       629 of       686\t|\tloss: 5.67532\n",
      "Training Epoch 11  91.8% | batch:       630 of       686\t|\tloss: 6.10328\n",
      "Training Epoch 11  92.0% | batch:       631 of       686\t|\tloss: 6.75563\n",
      "Training Epoch 11  92.1% | batch:       632 of       686\t|\tloss: 6.4567\n",
      "Training Epoch 11  92.3% | batch:       633 of       686\t|\tloss: 6.18756\n",
      "Training Epoch 11  92.4% | batch:       634 of       686\t|\tloss: 5.12765\n",
      "Training Epoch 11  92.6% | batch:       635 of       686\t|\tloss: 5.52753\n",
      "Training Epoch 11  92.7% | batch:       636 of       686\t|\tloss: 6.19757\n",
      "Training Epoch 11  92.9% | batch:       637 of       686\t|\tloss: 4.43643\n",
      "Training Epoch 11  93.0% | batch:       638 of       686\t|\tloss: 5.21925\n",
      "Training Epoch 11  93.1% | batch:       639 of       686\t|\tloss: 5.54658\n",
      "Training Epoch 11  93.3% | batch:       640 of       686\t|\tloss: 5.32022\n",
      "Training Epoch 11  93.4% | batch:       641 of       686\t|\tloss: 5.89286\n",
      "Training Epoch 11  93.6% | batch:       642 of       686\t|\tloss: 7.06219\n",
      "Training Epoch 11  93.7% | batch:       643 of       686\t|\tloss: 7.93412\n",
      "Training Epoch 11  93.9% | batch:       644 of       686\t|\tloss: 4.86006\n",
      "Training Epoch 11  94.0% | batch:       645 of       686\t|\tloss: 4.90521\n",
      "Training Epoch 11  94.2% | batch:       646 of       686\t|\tloss: 7.92329\n",
      "Training Epoch 11  94.3% | batch:       647 of       686\t|\tloss: 8.46784\n",
      "Training Epoch 11  94.5% | batch:       648 of       686\t|\tloss: 6.53104\n",
      "Training Epoch 11  94.6% | batch:       649 of       686\t|\tloss: 8.64926\n",
      "Training Epoch 11  94.8% | batch:       650 of       686\t|\tloss: 6.57444\n",
      "Training Epoch 11  94.9% | batch:       651 of       686\t|\tloss: 7.24405\n",
      "Training Epoch 11  95.0% | batch:       652 of       686\t|\tloss: 5.343\n",
      "Training Epoch 11  95.2% | batch:       653 of       686\t|\tloss: 4.8255\n",
      "Training Epoch 11  95.3% | batch:       654 of       686\t|\tloss: 6.28954\n",
      "Training Epoch 11  95.5% | batch:       655 of       686\t|\tloss: 6.42289\n",
      "Training Epoch 11  95.6% | batch:       656 of       686\t|\tloss: 6.68692\n",
      "Training Epoch 11  95.8% | batch:       657 of       686\t|\tloss: 6.34031\n",
      "Training Epoch 11  95.9% | batch:       658 of       686\t|\tloss: 8.88254\n",
      "Training Epoch 11  96.1% | batch:       659 of       686\t|\tloss: 6.50085\n",
      "Training Epoch 11  96.2% | batch:       660 of       686\t|\tloss: 6.34447\n",
      "Training Epoch 11  96.4% | batch:       661 of       686\t|\tloss: 6.07515\n",
      "Training Epoch 11  96.5% | batch:       662 of       686\t|\tloss: 6.513\n",
      "Training Epoch 11  96.6% | batch:       663 of       686\t|\tloss: 5.48024\n",
      "Training Epoch 11  96.8% | batch:       664 of       686\t|\tloss: 6.58838\n",
      "Training Epoch 11  96.9% | batch:       665 of       686\t|\tloss: 6.46194\n",
      "Training Epoch 11  97.1% | batch:       666 of       686\t|\tloss: 6.71778\n",
      "Training Epoch 11  97.2% | batch:       667 of       686\t|\tloss: 5.8457\n",
      "Training Epoch 11  97.4% | batch:       668 of       686\t|\tloss: 6.64627\n",
      "Training Epoch 11  97.5% | batch:       669 of       686\t|\tloss: 5.48119\n",
      "Training Epoch 11  97.7% | batch:       670 of       686\t|\tloss: 5.72003\n",
      "Training Epoch 11  97.8% | batch:       671 of       686\t|\tloss: 4.7196\n",
      "Training Epoch 11  98.0% | batch:       672 of       686\t|\tloss: 6.951\n",
      "Training Epoch 11  98.1% | batch:       673 of       686\t|\tloss: 5.85328\n",
      "Training Epoch 11  98.3% | batch:       674 of       686\t|\tloss: 5.1879\n",
      "Training Epoch 11  98.4% | batch:       675 of       686\t|\tloss: 5.96582\n",
      "Training Epoch 11  98.5% | batch:       676 of       686\t|\tloss: 6.9491\n",
      "Training Epoch 11  98.7% | batch:       677 of       686\t|\tloss: 6.50226\n",
      "Training Epoch 11  98.8% | batch:       678 of       686\t|\tloss: 4.59705\n",
      "Training Epoch 11  99.0% | batch:       679 of       686\t|\tloss: 6.32904\n",
      "Training Epoch 11  99.1% | batch:       680 of       686\t|\tloss: 6.06712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:04:50,687 | INFO : Epoch 11 Training Summary: epoch: 11.000000 | loss: 6.775367 | \n",
      "2023-05-25 22:04:50,688 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 23.762619018554688 seconds\n",
      "\n",
      "2023-05-25 22:04:50,688 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.50077934698625 seconds\n",
      "2023-05-25 22:04:50,689 | INFO : Avg batch train. time: 0.03425769584108783 seconds\n",
      "2023-05-25 22:04:50,689 | INFO : Avg sample train. time: 0.0002679831158787417 seconds\n",
      "2023-05-25 22:04:50,689 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  99.3% | batch:       681 of       686\t|\tloss: 9.14383\n",
      "Training Epoch 11  99.4% | batch:       682 of       686\t|\tloss: 5.80713\n",
      "Training Epoch 11  99.6% | batch:       683 of       686\t|\tloss: 4.64411\n",
      "Training Epoch 11  99.7% | batch:       684 of       686\t|\tloss: 6.80187\n",
      "Training Epoch 11  99.9% | batch:       685 of       686\t|\tloss: 4.8813\n",
      "\n",
      "Evaluating Epoch 11   0.0% | batch:         0 of       172\t|\tloss: 2.68944\n",
      "Evaluating Epoch 11   0.6% | batch:         1 of       172\t|\tloss: 3.11711\n",
      "Evaluating Epoch 11   1.2% | batch:         2 of       172\t|\tloss: 2.4715\n",
      "Evaluating Epoch 11   1.7% | batch:         3 of       172\t|\tloss: 4.6973\n",
      "Evaluating Epoch 11   2.3% | batch:         4 of       172\t|\tloss: 2.56953\n",
      "Evaluating Epoch 11   2.9% | batch:         5 of       172\t|\tloss: 2.05921\n",
      "Evaluating Epoch 11   3.5% | batch:         6 of       172\t|\tloss: 3.08062\n",
      "Evaluating Epoch 11   4.1% | batch:         7 of       172\t|\tloss: 4.71278\n",
      "Evaluating Epoch 11   4.7% | batch:         8 of       172\t|\tloss: 2.41776\n",
      "Evaluating Epoch 11   5.2% | batch:         9 of       172\t|\tloss: 3.23243\n",
      "Evaluating Epoch 11   5.8% | batch:        10 of       172\t|\tloss: 3.5153\n",
      "Evaluating Epoch 11   6.4% | batch:        11 of       172\t|\tloss: 3.10598\n",
      "Evaluating Epoch 11   7.0% | batch:        12 of       172\t|\tloss: 2.05063\n",
      "Evaluating Epoch 11   7.6% | batch:        13 of       172\t|\tloss: 3.32842\n",
      "Evaluating Epoch 11   8.1% | batch:        14 of       172\t|\tloss: 3.31044\n",
      "Evaluating Epoch 11   8.7% | batch:        15 of       172\t|\tloss: 2.52789\n",
      "Evaluating Epoch 11   9.3% | batch:        16 of       172\t|\tloss: 3.51827\n",
      "Evaluating Epoch 11   9.9% | batch:        17 of       172\t|\tloss: 2.43698\n",
      "Evaluating Epoch 11  10.5% | batch:        18 of       172\t|\tloss: 17.0107\n",
      "Evaluating Epoch 11  11.0% | batch:        19 of       172\t|\tloss: 1.79612\n",
      "Evaluating Epoch 11  11.6% | batch:        20 of       172\t|\tloss: 1.22165\n",
      "Evaluating Epoch 11  12.2% | batch:        21 of       172\t|\tloss: 0.854131\n",
      "Evaluating Epoch 11  12.8% | batch:        22 of       172\t|\tloss: 3.71519\n",
      "Evaluating Epoch 11  13.4% | batch:        23 of       172\t|\tloss: 2.57566\n",
      "Evaluating Epoch 11  14.0% | batch:        24 of       172\t|\tloss: 0.753149\n",
      "Evaluating Epoch 11  14.5% | batch:        25 of       172\t|\tloss: 1.28507\n",
      "Evaluating Epoch 11  15.1% | batch:        26 of       172\t|\tloss: 7.89812\n",
      "Evaluating Epoch 11  15.7% | batch:        27 of       172\t|\tloss: 16.184\n",
      "Evaluating Epoch 11  16.3% | batch:        28 of       172\t|\tloss: 0.313102\n",
      "Evaluating Epoch 11  16.9% | batch:        29 of       172\t|\tloss: 0.855867\n",
      "Evaluating Epoch 11  17.4% | batch:        30 of       172\t|\tloss: 1.39719\n",
      "Evaluating Epoch 11  18.0% | batch:        31 of       172\t|\tloss: 0.512306\n",
      "Evaluating Epoch 11  18.6% | batch:        32 of       172\t|\tloss: 0.365586\n",
      "Evaluating Epoch 11  19.2% | batch:        33 of       172\t|\tloss: 1.19258\n",
      "Evaluating Epoch 11  19.8% | batch:        34 of       172\t|\tloss: 0.290833\n",
      "Evaluating Epoch 11  20.3% | batch:        35 of       172\t|\tloss: 0.591231\n",
      "Evaluating Epoch 11  20.9% | batch:        36 of       172\t|\tloss: 4.45678\n",
      "Evaluating Epoch 11  21.5% | batch:        37 of       172\t|\tloss: 5.16907\n",
      "Evaluating Epoch 11  22.1% | batch:        38 of       172\t|\tloss: 3.48511\n",
      "Evaluating Epoch 11  22.7% | batch:        39 of       172\t|\tloss: 6.71767\n",
      "Evaluating Epoch 11  23.3% | batch:        40 of       172\t|\tloss: 0.200533\n",
      "Evaluating Epoch 11  23.8% | batch:        41 of       172\t|\tloss: 0.529013\n",
      "Evaluating Epoch 11  24.4% | batch:        42 of       172\t|\tloss: 0.890704\n",
      "Evaluating Epoch 11  25.0% | batch:        43 of       172\t|\tloss: 18.4002\n",
      "Evaluating Epoch 11  25.6% | batch:        44 of       172\t|\tloss: 1.47673\n",
      "Evaluating Epoch 11  26.2% | batch:        45 of       172\t|\tloss: 0.377305\n",
      "Evaluating Epoch 11  26.7% | batch:        46 of       172\t|\tloss: 0.775665\n",
      "Evaluating Epoch 11  27.3% | batch:        47 of       172\t|\tloss: 0.789968\n",
      "Evaluating Epoch 11  27.9% | batch:        48 of       172\t|\tloss: 0.354951\n",
      "Evaluating Epoch 11  28.5% | batch:        49 of       172\t|\tloss: 1.90451\n",
      "Evaluating Epoch 11  29.1% | batch:        50 of       172\t|\tloss: 0.758145\n",
      "Evaluating Epoch 11  29.7% | batch:        51 of       172\t|\tloss: 0.551928\n",
      "Evaluating Epoch 11  30.2% | batch:        52 of       172\t|\tloss: 0.758144\n",
      "Evaluating Epoch 11  30.8% | batch:        53 of       172\t|\tloss: 0.998945\n",
      "Evaluating Epoch 11  31.4% | batch:        54 of       172\t|\tloss: 0.783091\n",
      "Evaluating Epoch 11  32.0% | batch:        55 of       172\t|\tloss: 1.37618\n",
      "Evaluating Epoch 11  32.6% | batch:        56 of       172\t|\tloss: 1.18655\n",
      "Evaluating Epoch 11  33.1% | batch:        57 of       172\t|\tloss: 1.95879\n",
      "Evaluating Epoch 11  33.7% | batch:        58 of       172\t|\tloss: 0.627594\n",
      "Evaluating Epoch 11  34.3% | batch:        59 of       172\t|\tloss: 1.16263\n",
      "Evaluating Epoch 11  34.9% | batch:        60 of       172\t|\tloss: 0.504528\n",
      "Evaluating Epoch 11  35.5% | batch:        61 of       172\t|\tloss: 2.00006\n",
      "Evaluating Epoch 11  36.0% | batch:        62 of       172\t|\tloss: 0.740926\n",
      "Evaluating Epoch 11  36.6% | batch:        63 of       172\t|\tloss: 0.998305\n",
      "Evaluating Epoch 11  37.2% | batch:        64 of       172\t|\tloss: 0.817428\n",
      "Evaluating Epoch 11  37.8% | batch:        65 of       172\t|\tloss: 0.765156\n",
      "Evaluating Epoch 11  38.4% | batch:        66 of       172\t|\tloss: 1.61536\n",
      "Evaluating Epoch 11  39.0% | batch:        67 of       172\t|\tloss: 0.67643\n",
      "Evaluating Epoch 11  39.5% | batch:        68 of       172\t|\tloss: 1.9751\n",
      "Evaluating Epoch 11  40.1% | batch:        69 of       172\t|\tloss: 2.20331\n",
      "Evaluating Epoch 11  40.7% | batch:        70 of       172\t|\tloss: 0.320291\n",
      "Evaluating Epoch 11  41.3% | batch:        71 of       172\t|\tloss: 0.756321\n",
      "Evaluating Epoch 11  41.9% | batch:        72 of       172\t|\tloss: 1.02023\n",
      "Evaluating Epoch 11  42.4% | batch:        73 of       172\t|\tloss: 0.833828\n",
      "Evaluating Epoch 11  43.0% | batch:        74 of       172\t|\tloss: 0.201159\n",
      "Evaluating Epoch 11  43.6% | batch:        75 of       172\t|\tloss: 0.331563\n",
      "Evaluating Epoch 11  44.2% | batch:        76 of       172\t|\tloss: 0.241312\n",
      "Evaluating Epoch 11  44.8% | batch:        77 of       172\t|\tloss: 0.301071\n",
      "Evaluating Epoch 11  45.3% | batch:        78 of       172\t|\tloss: 0.272418\n",
      "Evaluating Epoch 11  45.9% | batch:        79 of       172\t|\tloss: 0.27476\n",
      "Evaluating Epoch 11  46.5% | batch:        80 of       172\t|\tloss: 0.522339\n",
      "Evaluating Epoch 11  47.1% | batch:        81 of       172\t|\tloss: 0.366677\n",
      "Evaluating Epoch 11  47.7% | batch:        82 of       172\t|\tloss: 0.305824\n",
      "Evaluating Epoch 11  48.3% | batch:        83 of       172\t|\tloss: 0.615738\n",
      "Evaluating Epoch 11  48.8% | batch:        84 of       172\t|\tloss: 0.8792\n",
      "Evaluating Epoch 11  49.4% | batch:        85 of       172\t|\tloss: 0.902713\n",
      "Evaluating Epoch 11  50.0% | batch:        86 of       172\t|\tloss: 1.00463\n",
      "Evaluating Epoch 11  50.6% | batch:        87 of       172\t|\tloss: 0.632359\n",
      "Evaluating Epoch 11  51.2% | batch:        88 of       172\t|\tloss: 1.08532\n",
      "Evaluating Epoch 11  51.7% | batch:        89 of       172\t|\tloss: 1.67749\n",
      "Evaluating Epoch 11  52.3% | batch:        90 of       172\t|\tloss: 0.962423\n",
      "Evaluating Epoch 11  52.9% | batch:        91 of       172\t|\tloss: 1.54508\n",
      "Evaluating Epoch 11  53.5% | batch:        92 of       172\t|\tloss: 1.50156\n",
      "Evaluating Epoch 11  54.1% | batch:        93 of       172\t|\tloss: 1.19688\n",
      "Evaluating Epoch 11  54.7% | batch:        94 of       172\t|\tloss: 1.06412\n",
      "Evaluating Epoch 11  55.2% | batch:        95 of       172\t|\tloss: 1.16449\n",
      "Evaluating Epoch 11  55.8% | batch:        96 of       172\t|\tloss: 1.73428\n",
      "Evaluating Epoch 11  56.4% | batch:        97 of       172\t|\tloss: 0.930059\n",
      "Evaluating Epoch 11  57.0% | batch:        98 of       172\t|\tloss: 1.27449\n",
      "Evaluating Epoch 11  57.6% | batch:        99 of       172\t|\tloss: 1.59692\n",
      "Evaluating Epoch 11  58.1% | batch:       100 of       172\t|\tloss: 0.759014\n",
      "Evaluating Epoch 11  58.7% | batch:       101 of       172\t|\tloss: 0.709384\n",
      "Evaluating Epoch 11  59.3% | batch:       102 of       172\t|\tloss: 1.41016\n",
      "Evaluating Epoch 11  59.9% | batch:       103 of       172\t|\tloss: 1.64002\n",
      "Evaluating Epoch 11  60.5% | batch:       104 of       172\t|\tloss: 0.874467\n",
      "Evaluating Epoch 11  61.0% | batch:       105 of       172\t|\tloss: 1.28029\n",
      "Evaluating Epoch 11  61.6% | batch:       106 of       172\t|\tloss: 1.75968\n",
      "Evaluating Epoch 11  62.2% | batch:       107 of       172\t|\tloss: 1.17505\n",
      "Evaluating Epoch 11  62.8% | batch:       108 of       172\t|\tloss: 1.10892\n",
      "Evaluating Epoch 11  63.4% | batch:       109 of       172\t|\tloss: 1.52136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 11  64.0% | batch:       110 of       172\t|\tloss: 1.97876\n",
      "Evaluating Epoch 11  64.5% | batch:       111 of       172\t|\tloss: 0.774713\n",
      "Evaluating Epoch 11  65.1% | batch:       112 of       172\t|\tloss: 0.817228\n",
      "Evaluating Epoch 11  65.7% | batch:       113 of       172\t|\tloss: 1.22512\n",
      "Evaluating Epoch 11  66.3% | batch:       114 of       172\t|\tloss: 1.44883\n",
      "Evaluating Epoch 11  66.9% | batch:       115 of       172\t|\tloss: 0.667657\n",
      "Evaluating Epoch 11  67.4% | batch:       116 of       172\t|\tloss: 1.16592\n",
      "Evaluating Epoch 11  68.0% | batch:       117 of       172\t|\tloss: 0.990085\n",
      "Evaluating Epoch 11  68.6% | batch:       118 of       172\t|\tloss: 1.38149\n",
      "Evaluating Epoch 11  69.2% | batch:       119 of       172\t|\tloss: 0.977579\n",
      "Evaluating Epoch 11  69.8% | batch:       120 of       172\t|\tloss: 0.736224\n",
      "Evaluating Epoch 11  70.3% | batch:       121 of       172\t|\tloss: 2.90195\n",
      "Evaluating Epoch 11  70.9% | batch:       122 of       172\t|\tloss: 1.86858\n",
      "Evaluating Epoch 11  71.5% | batch:       123 of       172\t|\tloss: 2.65051\n",
      "Evaluating Epoch 11  72.1% | batch:       124 of       172\t|\tloss: 5.71581\n",
      "Evaluating Epoch 11  72.7% | batch:       125 of       172\t|\tloss: 0.949305\n",
      "Evaluating Epoch 11  73.3% | batch:       126 of       172\t|\tloss: 0.73462\n",
      "Evaluating Epoch 11  73.8% | batch:       127 of       172\t|\tloss: 1.13976\n",
      "Evaluating Epoch 11  74.4% | batch:       128 of       172\t|\tloss: 1.9341\n",
      "Evaluating Epoch 11  75.0% | batch:       129 of       172\t|\tloss: 0.782443\n",
      "Evaluating Epoch 11  75.6% | batch:       130 of       172\t|\tloss: 1.50125\n",
      "Evaluating Epoch 11  76.2% | batch:       131 of       172\t|\tloss: 1.86208\n",
      "Evaluating Epoch 11  76.7% | batch:       132 of       172\t|\tloss: 1.07238\n",
      "Evaluating Epoch 11  77.3% | batch:       133 of       172\t|\tloss: 0.802686\n",
      "Evaluating Epoch 11  77.9% | batch:       134 of       172\t|\tloss: 1.20964\n",
      "Evaluating Epoch 11  78.5% | batch:       135 of       172\t|\tloss: 0.443076\n",
      "Evaluating Epoch 11  79.1% | batch:       136 of       172\t|\tloss: 0.979882\n",
      "Evaluating Epoch 11  79.7% | batch:       137 of       172\t|\tloss: 0.386866\n",
      "Evaluating Epoch 11  80.2% | batch:       138 of       172\t|\tloss: 1.03923\n",
      "Evaluating Epoch 11  80.8% | batch:       139 of       172\t|\tloss: 0.691633\n",
      "Evaluating Epoch 11  81.4% | batch:       140 of       172\t|\tloss: 0.741891\n",
      "Evaluating Epoch 11  82.0% | batch:       141 of       172\t|\tloss: 0.489529\n",
      "Evaluating Epoch 11  82.6% | batch:       142 of       172\t|\tloss: 0.558112\n",
      "Evaluating Epoch 11  83.1% | batch:       143 of       172\t|\tloss: 0.67634\n",
      "Evaluating Epoch 11  83.7% | batch:       144 of       172\t|\tloss: 0.869714\n",
      "Evaluating Epoch 11  84.3% | batch:       145 of       172\t|\tloss: 0.601943\n",
      "Evaluating Epoch 11  84.9% | batch:       146 of       172\t|\tloss: 0.758527\n",
      "Evaluating Epoch 11  85.5% | batch:       147 of       172\t|\tloss: 0.668258\n",
      "Evaluating Epoch 11  86.0% | batch:       148 of       172\t|\tloss: 0.587863\n",
      "Evaluating Epoch 11  86.6% | batch:       149 of       172\t|\tloss: 0.735372\n",
      "Evaluating Epoch 11  87.2% | batch:       150 of       172\t|\tloss: 0.810245\n",
      "Evaluating Epoch 11  87.8% | batch:       151 of       172\t|\tloss: 0.318655\n",
      "Evaluating Epoch 11  88.4% | batch:       152 of       172\t|\tloss: 0.706668\n",
      "Evaluating Epoch 11  89.0% | batch:       153 of       172\t|\tloss: 0.62046\n",
      "Evaluating Epoch 11  89.5% | batch:       154 of       172\t|\tloss: 0.301401\n",
      "Evaluating Epoch 11  90.1% | batch:       155 of       172\t|\tloss: 0.813\n",
      "Evaluating Epoch 11  90.7% | batch:       156 of       172\t|\tloss: 0.46498\n",
      "Evaluating Epoch 11  91.3% | batch:       157 of       172\t|\tloss: 0.38505\n",
      "Evaluating Epoch 11  91.9% | batch:       158 of       172\t|\tloss: 0.831771\n",
      "Evaluating Epoch 11  92.4% | batch:       159 of       172\t|\tloss: 0.584764\n",
      "Evaluating Epoch 11  93.0% | batch:       160 of       172\t|\tloss: 2.6214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:04:54,621 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.931269884109497 seconds\n",
      "\n",
      "2023-05-25 22:04:54,622 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.008282661437988 seconds\n",
      "2023-05-25 22:04:54,622 | INFO : Avg batch val. time: 0.02330396896184877 seconds\n",
      "2023-05-25 22:04:54,623 | INFO : Avg sample val. time: 0.0001825514715779928 seconds\n",
      "2023-05-25 22:04:54,623 | INFO : Epoch 11 Validation Summary: epoch: 11.000000 | loss: 1.644226 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 11  93.6% | batch:       161 of       172\t|\tloss: 1.65899\n",
      "Evaluating Epoch 11  94.2% | batch:       162 of       172\t|\tloss: 0.3121\n",
      "Evaluating Epoch 11  94.8% | batch:       163 of       172\t|\tloss: 0.928959\n",
      "Evaluating Epoch 11  95.3% | batch:       164 of       172\t|\tloss: 0.653744\n",
      "Evaluating Epoch 11  95.9% | batch:       165 of       172\t|\tloss: 0.228981\n",
      "Evaluating Epoch 11  96.5% | batch:       166 of       172\t|\tloss: 0.778997\n",
      "Evaluating Epoch 11  97.1% | batch:       167 of       172\t|\tloss: 0.427214\n",
      "Evaluating Epoch 11  97.7% | batch:       168 of       172\t|\tloss: 0.383369\n",
      "Evaluating Epoch 11  98.3% | batch:       169 of       172\t|\tloss: 0.7104\n",
      "Evaluating Epoch 11  98.8% | batch:       170 of       172\t|\tloss: 0.326876\n",
      "Evaluating Epoch 11  99.4% | batch:       171 of       172\t|\tloss: 0.499698\n",
      "\n",
      "Training Epoch 12   0.0% | batch:         0 of       686\t|\tloss: 6.12595\n",
      "Training Epoch 12   0.1% | batch:         1 of       686\t|\tloss: 5.18047\n",
      "Training Epoch 12   0.3% | batch:         2 of       686\t|\tloss: 7.65156\n",
      "Training Epoch 12   0.4% | batch:         3 of       686\t|\tloss: 5.93073\n",
      "Training Epoch 12   0.6% | batch:         4 of       686\t|\tloss: 5.33529\n",
      "Training Epoch 12   0.7% | batch:         5 of       686\t|\tloss: 6.38165\n",
      "Training Epoch 12   0.9% | batch:         6 of       686\t|\tloss: 7.56649\n",
      "Training Epoch 12   1.0% | batch:         7 of       686\t|\tloss: 5.98686\n",
      "Training Epoch 12   1.2% | batch:         8 of       686\t|\tloss: 5.03109\n",
      "Training Epoch 12   1.3% | batch:         9 of       686\t|\tloss: 5.19153\n",
      "Training Epoch 12   1.5% | batch:        10 of       686\t|\tloss: 5.88902\n",
      "Training Epoch 12   1.6% | batch:        11 of       686\t|\tloss: 6.74526\n",
      "Training Epoch 12   1.7% | batch:        12 of       686\t|\tloss: 4.96013\n",
      "Training Epoch 12   1.9% | batch:        13 of       686\t|\tloss: 6.65874\n",
      "Training Epoch 12   2.0% | batch:        14 of       686\t|\tloss: 6.96985\n",
      "Training Epoch 12   2.2% | batch:        15 of       686\t|\tloss: 8.78959\n",
      "Training Epoch 12   2.3% | batch:        16 of       686\t|\tloss: 5.8059\n",
      "Training Epoch 12   2.5% | batch:        17 of       686\t|\tloss: 7.79686\n",
      "Training Epoch 12   2.6% | batch:        18 of       686\t|\tloss: 5.26886\n",
      "Training Epoch 12   2.8% | batch:        19 of       686\t|\tloss: 6.20678\n",
      "Training Epoch 12   2.9% | batch:        20 of       686\t|\tloss: 5.38449\n",
      "Training Epoch 12   3.1% | batch:        21 of       686\t|\tloss: 5.40457\n",
      "Training Epoch 12   3.2% | batch:        22 of       686\t|\tloss: 7.08755\n",
      "Training Epoch 12   3.4% | batch:        23 of       686\t|\tloss: 6.32641\n",
      "Training Epoch 12   3.5% | batch:        24 of       686\t|\tloss: 6.36915\n",
      "Training Epoch 12   3.6% | batch:        25 of       686\t|\tloss: 5.04623\n",
      "Training Epoch 12   3.8% | batch:        26 of       686\t|\tloss: 7.76022\n",
      "Training Epoch 12   3.9% | batch:        27 of       686\t|\tloss: 8.70216\n",
      "Training Epoch 12   4.1% | batch:        28 of       686\t|\tloss: 5.90843\n",
      "Training Epoch 12   4.2% | batch:        29 of       686\t|\tloss: 5.06407\n",
      "Training Epoch 12   4.4% | batch:        30 of       686\t|\tloss: 5.0258\n",
      "Training Epoch 12   4.5% | batch:        31 of       686\t|\tloss: 4.92461\n",
      "Training Epoch 12   4.7% | batch:        32 of       686\t|\tloss: 6.25697\n",
      "Training Epoch 12   4.8% | batch:        33 of       686\t|\tloss: 5.58152\n",
      "Training Epoch 12   5.0% | batch:        34 of       686\t|\tloss: 5.82754\n",
      "Training Epoch 12   5.1% | batch:        35 of       686\t|\tloss: 7.75899\n",
      "Training Epoch 12   5.2% | batch:        36 of       686\t|\tloss: 7.03834\n",
      "Training Epoch 12   5.4% | batch:        37 of       686\t|\tloss: 6.51498\n",
      "Training Epoch 12   5.5% | batch:        38 of       686\t|\tloss: 7.63435\n",
      "Training Epoch 12   5.7% | batch:        39 of       686\t|\tloss: 6.48959\n",
      "Training Epoch 12   5.8% | batch:        40 of       686\t|\tloss: 6.03673\n",
      "Training Epoch 12   6.0% | batch:        41 of       686\t|\tloss: 8.24476\n",
      "Training Epoch 12   6.1% | batch:        42 of       686\t|\tloss: 6.39751\n",
      "Training Epoch 12   6.3% | batch:        43 of       686\t|\tloss: 5.87875\n",
      "Training Epoch 12   6.4% | batch:        44 of       686\t|\tloss: 10.0582\n",
      "Training Epoch 12   6.6% | batch:        45 of       686\t|\tloss: 6.36255\n",
      "Training Epoch 12   6.7% | batch:        46 of       686\t|\tloss: 5.63517\n",
      "Training Epoch 12   6.9% | batch:        47 of       686\t|\tloss: 5.77926\n",
      "Training Epoch 12   7.0% | batch:        48 of       686\t|\tloss: 4.76517\n",
      "Training Epoch 12   7.1% | batch:        49 of       686\t|\tloss: 6.0702\n",
      "Training Epoch 12   7.3% | batch:        50 of       686\t|\tloss: 7.24679\n",
      "Training Epoch 12   7.4% | batch:        51 of       686\t|\tloss: 6.50728\n",
      "Training Epoch 12   7.6% | batch:        52 of       686\t|\tloss: 7.11103\n",
      "Training Epoch 12   7.7% | batch:        53 of       686\t|\tloss: 7.4291\n",
      "Training Epoch 12   7.9% | batch:        54 of       686\t|\tloss: 6.07625\n",
      "Training Epoch 12   8.0% | batch:        55 of       686\t|\tloss: 5.19903\n",
      "Training Epoch 12   8.2% | batch:        56 of       686\t|\tloss: 5.49478\n",
      "Training Epoch 12   8.3% | batch:        57 of       686\t|\tloss: 6.81894\n",
      "Training Epoch 12   8.5% | batch:        58 of       686\t|\tloss: 5.66392\n",
      "Training Epoch 12   8.6% | batch:        59 of       686\t|\tloss: 5.04597\n",
      "Training Epoch 12   8.7% | batch:        60 of       686\t|\tloss: 5.94466\n",
      "Training Epoch 12   8.9% | batch:        61 of       686\t|\tloss: 7.11538\n",
      "Training Epoch 12   9.0% | batch:        62 of       686\t|\tloss: 7.15001\n",
      "Training Epoch 12   9.2% | batch:        63 of       686\t|\tloss: 6.27416\n",
      "Training Epoch 12   9.3% | batch:        64 of       686\t|\tloss: 6.524\n",
      "Training Epoch 12   9.5% | batch:        65 of       686\t|\tloss: 5.76667\n",
      "Training Epoch 12   9.6% | batch:        66 of       686\t|\tloss: 5.39698\n",
      "Training Epoch 12   9.8% | batch:        67 of       686\t|\tloss: 5.598\n",
      "Training Epoch 12   9.9% | batch:        68 of       686\t|\tloss: 4.84392\n",
      "Training Epoch 12  10.1% | batch:        69 of       686\t|\tloss: 7.80683\n",
      "Training Epoch 12  10.2% | batch:        70 of       686\t|\tloss: 4.77658\n",
      "Training Epoch 12  10.3% | batch:        71 of       686\t|\tloss: 7.44828\n",
      "Training Epoch 12  10.5% | batch:        72 of       686\t|\tloss: 5.77516\n",
      "Training Epoch 12  10.6% | batch:        73 of       686\t|\tloss: 3.95762\n",
      "Training Epoch 12  10.8% | batch:        74 of       686\t|\tloss: 5.00159\n",
      "Training Epoch 12  10.9% | batch:        75 of       686\t|\tloss: 5.6156\n",
      "Training Epoch 12  11.1% | batch:        76 of       686\t|\tloss: 4.75137\n",
      "Training Epoch 12  11.2% | batch:        77 of       686\t|\tloss: 8.94707\n",
      "Training Epoch 12  11.4% | batch:        78 of       686\t|\tloss: 4.55261\n",
      "Training Epoch 12  11.5% | batch:        79 of       686\t|\tloss: 5.6528\n",
      "Training Epoch 12  11.7% | batch:        80 of       686\t|\tloss: 6.82434\n",
      "Training Epoch 12  11.8% | batch:        81 of       686\t|\tloss: 6.01473\n",
      "Training Epoch 12  12.0% | batch:        82 of       686\t|\tloss: 6.82602\n",
      "Training Epoch 12  12.1% | batch:        83 of       686\t|\tloss: 5.10211\n",
      "Training Epoch 12  12.2% | batch:        84 of       686\t|\tloss: 5.97613\n",
      "Training Epoch 12  12.4% | batch:        85 of       686\t|\tloss: 6.98034\n",
      "Training Epoch 12  12.5% | batch:        86 of       686\t|\tloss: 4.45042\n",
      "Training Epoch 12  12.7% | batch:        87 of       686\t|\tloss: 9.64948\n",
      "Training Epoch 12  12.8% | batch:        88 of       686\t|\tloss: 6.10198\n",
      "Training Epoch 12  13.0% | batch:        89 of       686\t|\tloss: 6.91505\n",
      "Training Epoch 12  13.1% | batch:        90 of       686\t|\tloss: 7.33227\n",
      "Training Epoch 12  13.3% | batch:        91 of       686\t|\tloss: 5.95708\n",
      "Training Epoch 12  13.4% | batch:        92 of       686\t|\tloss: 6.91851\n",
      "Training Epoch 12  13.6% | batch:        93 of       686\t|\tloss: 6.51054\n",
      "Training Epoch 12  13.7% | batch:        94 of       686\t|\tloss: 6.15492\n",
      "Training Epoch 12  13.8% | batch:        95 of       686\t|\tloss: 6.44893\n",
      "Training Epoch 12  14.0% | batch:        96 of       686\t|\tloss: 7.31093\n",
      "Training Epoch 12  14.1% | batch:        97 of       686\t|\tloss: 11.0234\n",
      "Training Epoch 12  14.3% | batch:        98 of       686\t|\tloss: 5.21036\n",
      "Training Epoch 12  14.4% | batch:        99 of       686\t|\tloss: 6.5725\n",
      "Training Epoch 12  14.6% | batch:       100 of       686\t|\tloss: 3.82455\n",
      "Training Epoch 12  14.7% | batch:       101 of       686\t|\tloss: 8.03825\n",
      "Training Epoch 12  14.9% | batch:       102 of       686\t|\tloss: 4.77632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  15.0% | batch:       103 of       686\t|\tloss: 4.4711\n",
      "Training Epoch 12  15.2% | batch:       104 of       686\t|\tloss: 5.02098\n",
      "Training Epoch 12  15.3% | batch:       105 of       686\t|\tloss: 5.10079\n",
      "Training Epoch 12  15.5% | batch:       106 of       686\t|\tloss: 5.16437\n",
      "Training Epoch 12  15.6% | batch:       107 of       686\t|\tloss: 6.70964\n",
      "Training Epoch 12  15.7% | batch:       108 of       686\t|\tloss: 6.59322\n",
      "Training Epoch 12  15.9% | batch:       109 of       686\t|\tloss: 6.3952\n",
      "Training Epoch 12  16.0% | batch:       110 of       686\t|\tloss: 5.71441\n",
      "Training Epoch 12  16.2% | batch:       111 of       686\t|\tloss: 6.49735\n",
      "Training Epoch 12  16.3% | batch:       112 of       686\t|\tloss: 5.18164\n",
      "Training Epoch 12  16.5% | batch:       113 of       686\t|\tloss: 5.93448\n",
      "Training Epoch 12  16.6% | batch:       114 of       686\t|\tloss: 5.16641\n",
      "Training Epoch 12  16.8% | batch:       115 of       686\t|\tloss: 6.464\n",
      "Training Epoch 12  16.9% | batch:       116 of       686\t|\tloss: 6.29026\n",
      "Training Epoch 12  17.1% | batch:       117 of       686\t|\tloss: 4.60012\n",
      "Training Epoch 12  17.2% | batch:       118 of       686\t|\tloss: 7.06951\n",
      "Training Epoch 12  17.3% | batch:       119 of       686\t|\tloss: 7.62786\n",
      "Training Epoch 12  17.5% | batch:       120 of       686\t|\tloss: 4.87976\n",
      "Training Epoch 12  17.6% | batch:       121 of       686\t|\tloss: 5.84108\n",
      "Training Epoch 12  17.8% | batch:       122 of       686\t|\tloss: 6.03568\n",
      "Training Epoch 12  17.9% | batch:       123 of       686\t|\tloss: 5.37543\n",
      "Training Epoch 12  18.1% | batch:       124 of       686\t|\tloss: 8.21226\n",
      "Training Epoch 12  18.2% | batch:       125 of       686\t|\tloss: 5.45065\n",
      "Training Epoch 12  18.4% | batch:       126 of       686\t|\tloss: 6.63038\n",
      "Training Epoch 12  18.5% | batch:       127 of       686\t|\tloss: 5.01023\n",
      "Training Epoch 12  18.7% | batch:       128 of       686\t|\tloss: 5.6254\n",
      "Training Epoch 12  18.8% | batch:       129 of       686\t|\tloss: 5.14491\n",
      "Training Epoch 12  19.0% | batch:       130 of       686\t|\tloss: 4.84932\n",
      "Training Epoch 12  19.1% | batch:       131 of       686\t|\tloss: 5.77519\n",
      "Training Epoch 12  19.2% | batch:       132 of       686\t|\tloss: 5.61919\n",
      "Training Epoch 12  19.4% | batch:       133 of       686\t|\tloss: 6.3854\n",
      "Training Epoch 12  19.5% | batch:       134 of       686\t|\tloss: 5.86049\n",
      "Training Epoch 12  19.7% | batch:       135 of       686\t|\tloss: 5.24162\n",
      "Training Epoch 12  19.8% | batch:       136 of       686\t|\tloss: 6.47365\n",
      "Training Epoch 12  20.0% | batch:       137 of       686\t|\tloss: 4.05982\n",
      "Training Epoch 12  20.1% | batch:       138 of       686\t|\tloss: 5.15126\n",
      "Training Epoch 12  20.3% | batch:       139 of       686\t|\tloss: 5.3533\n",
      "Training Epoch 12  20.4% | batch:       140 of       686\t|\tloss: 5.99761\n",
      "Training Epoch 12  20.6% | batch:       141 of       686\t|\tloss: 5.10444\n",
      "Training Epoch 12  20.7% | batch:       142 of       686\t|\tloss: 4.86865\n",
      "Training Epoch 12  20.8% | batch:       143 of       686\t|\tloss: 6.02915\n",
      "Training Epoch 12  21.0% | batch:       144 of       686\t|\tloss: 4.55253\n",
      "Training Epoch 12  21.1% | batch:       145 of       686\t|\tloss: 7.66556\n",
      "Training Epoch 12  21.3% | batch:       146 of       686\t|\tloss: 6.83335\n",
      "Training Epoch 12  21.4% | batch:       147 of       686\t|\tloss: 4.88359\n",
      "Training Epoch 12  21.6% | batch:       148 of       686\t|\tloss: 6.16189\n",
      "Training Epoch 12  21.7% | batch:       149 of       686\t|\tloss: 7.11737\n",
      "Training Epoch 12  21.9% | batch:       150 of       686\t|\tloss: 5.24107\n",
      "Training Epoch 12  22.0% | batch:       151 of       686\t|\tloss: 11.0472\n",
      "Training Epoch 12  22.2% | batch:       152 of       686\t|\tloss: 6.9034\n",
      "Training Epoch 12  22.3% | batch:       153 of       686\t|\tloss: 6.97186\n",
      "Training Epoch 12  22.4% | batch:       154 of       686\t|\tloss: 7.23826\n",
      "Training Epoch 12  22.6% | batch:       155 of       686\t|\tloss: 7.78077\n",
      "Training Epoch 12  22.7% | batch:       156 of       686\t|\tloss: 4.53435\n",
      "Training Epoch 12  22.9% | batch:       157 of       686\t|\tloss: 6.83604\n",
      "Training Epoch 12  23.0% | batch:       158 of       686\t|\tloss: 6.6346\n",
      "Training Epoch 12  23.2% | batch:       159 of       686\t|\tloss: 4.47665\n",
      "Training Epoch 12  23.3% | batch:       160 of       686\t|\tloss: 6.0271\n",
      "Training Epoch 12  23.5% | batch:       161 of       686\t|\tloss: 4.37478\n",
      "Training Epoch 12  23.6% | batch:       162 of       686\t|\tloss: 4.57344\n",
      "Training Epoch 12  23.8% | batch:       163 of       686\t|\tloss: 5.69947\n",
      "Training Epoch 12  23.9% | batch:       164 of       686\t|\tloss: 7.36799\n",
      "Training Epoch 12  24.1% | batch:       165 of       686\t|\tloss: 6.70001\n",
      "Training Epoch 12  24.2% | batch:       166 of       686\t|\tloss: 5.66232\n",
      "Training Epoch 12  24.3% | batch:       167 of       686\t|\tloss: 6.45961\n",
      "Training Epoch 12  24.5% | batch:       168 of       686\t|\tloss: 6.60765\n",
      "Training Epoch 12  24.6% | batch:       169 of       686\t|\tloss: 7.56059\n",
      "Training Epoch 12  24.8% | batch:       170 of       686\t|\tloss: 5.25688\n",
      "Training Epoch 12  24.9% | batch:       171 of       686\t|\tloss: 5.17131\n",
      "Training Epoch 12  25.1% | batch:       172 of       686\t|\tloss: 5.79838\n",
      "Training Epoch 12  25.2% | batch:       173 of       686\t|\tloss: 5.3414\n",
      "Training Epoch 12  25.4% | batch:       174 of       686\t|\tloss: 6.82585\n",
      "Training Epoch 12  25.5% | batch:       175 of       686\t|\tloss: 6.76383\n",
      "Training Epoch 12  25.7% | batch:       176 of       686\t|\tloss: 5.02809\n",
      "Training Epoch 12  25.8% | batch:       177 of       686\t|\tloss: 5.21386\n",
      "Training Epoch 12  25.9% | batch:       178 of       686\t|\tloss: 5.82572\n",
      "Training Epoch 12  26.1% | batch:       179 of       686\t|\tloss: 5.56532\n",
      "Training Epoch 12  26.2% | batch:       180 of       686\t|\tloss: 5.77449\n",
      "Training Epoch 12  26.4% | batch:       181 of       686\t|\tloss: 6.74134\n",
      "Training Epoch 12  26.5% | batch:       182 of       686\t|\tloss: 6.19243\n",
      "Training Epoch 12  26.7% | batch:       183 of       686\t|\tloss: 8.01938\n",
      "Training Epoch 12  26.8% | batch:       184 of       686\t|\tloss: 7.95365\n",
      "Training Epoch 12  27.0% | batch:       185 of       686\t|\tloss: 5.63659\n",
      "Training Epoch 12  27.1% | batch:       186 of       686\t|\tloss: 5.62249\n",
      "Training Epoch 12  27.3% | batch:       187 of       686\t|\tloss: 5.99844\n",
      "Training Epoch 12  27.4% | batch:       188 of       686\t|\tloss: 3.80527\n",
      "Training Epoch 12  27.6% | batch:       189 of       686\t|\tloss: 5.73461\n",
      "Training Epoch 12  27.7% | batch:       190 of       686\t|\tloss: 5.62305\n",
      "Training Epoch 12  27.8% | batch:       191 of       686\t|\tloss: 5.86124\n",
      "Training Epoch 12  28.0% | batch:       192 of       686\t|\tloss: 5.71166\n",
      "Training Epoch 12  28.1% | batch:       193 of       686\t|\tloss: 4.7742\n",
      "Training Epoch 12  28.3% | batch:       194 of       686\t|\tloss: 4.8639\n",
      "Training Epoch 12  28.4% | batch:       195 of       686\t|\tloss: 4.66757\n",
      "Training Epoch 12  28.6% | batch:       196 of       686\t|\tloss: 4.29232\n",
      "Training Epoch 12  28.7% | batch:       197 of       686\t|\tloss: 6.38074\n",
      "Training Epoch 12  28.9% | batch:       198 of       686\t|\tloss: 5.15585\n",
      "Training Epoch 12  29.0% | batch:       199 of       686\t|\tloss: 8.25507\n",
      "Training Epoch 12  29.2% | batch:       200 of       686\t|\tloss: 6.17111\n",
      "Training Epoch 12  29.3% | batch:       201 of       686\t|\tloss: 6.08312\n",
      "Training Epoch 12  29.4% | batch:       202 of       686\t|\tloss: 6.09594\n",
      "Training Epoch 12  29.6% | batch:       203 of       686\t|\tloss: 5.57196\n",
      "Training Epoch 12  29.7% | batch:       204 of       686\t|\tloss: 5.8427\n",
      "Training Epoch 12  29.9% | batch:       205 of       686\t|\tloss: 6.2056\n",
      "Training Epoch 12  30.0% | batch:       206 of       686\t|\tloss: 4.13125\n",
      "Training Epoch 12  30.2% | batch:       207 of       686\t|\tloss: 4.78959\n",
      "Training Epoch 12  30.3% | batch:       208 of       686\t|\tloss: 7.5356\n",
      "Training Epoch 12  30.5% | batch:       209 of       686\t|\tloss: 5.30777\n",
      "Training Epoch 12  30.6% | batch:       210 of       686\t|\tloss: 5.19044\n",
      "Training Epoch 12  30.8% | batch:       211 of       686\t|\tloss: 4.99161\n",
      "Training Epoch 12  30.9% | batch:       212 of       686\t|\tloss: 5.48236\n",
      "Training Epoch 12  31.0% | batch:       213 of       686\t|\tloss: 7.33478\n",
      "Training Epoch 12  31.2% | batch:       214 of       686\t|\tloss: 5.55038\n",
      "Training Epoch 12  31.3% | batch:       215 of       686\t|\tloss: 5.61146\n",
      "Training Epoch 12  31.5% | batch:       216 of       686\t|\tloss: 5.39402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  31.6% | batch:       217 of       686\t|\tloss: 6.45308\n",
      "Training Epoch 12  31.8% | batch:       218 of       686\t|\tloss: 4.94811\n",
      "Training Epoch 12  31.9% | batch:       219 of       686\t|\tloss: 5.44351\n",
      "Training Epoch 12  32.1% | batch:       220 of       686\t|\tloss: 4.80639\n",
      "Training Epoch 12  32.2% | batch:       221 of       686\t|\tloss: 5.20267\n",
      "Training Epoch 12  32.4% | batch:       222 of       686\t|\tloss: 6.04737\n",
      "Training Epoch 12  32.5% | batch:       223 of       686\t|\tloss: 7.13039\n",
      "Training Epoch 12  32.7% | batch:       224 of       686\t|\tloss: 4.80015\n",
      "Training Epoch 12  32.8% | batch:       225 of       686\t|\tloss: 6.61335\n",
      "Training Epoch 12  32.9% | batch:       226 of       686\t|\tloss: 6.598\n",
      "Training Epoch 12  33.1% | batch:       227 of       686\t|\tloss: 5.13446\n",
      "Training Epoch 12  33.2% | batch:       228 of       686\t|\tloss: 5.771\n",
      "Training Epoch 12  33.4% | batch:       229 of       686\t|\tloss: 5.79439\n",
      "Training Epoch 12  33.5% | batch:       230 of       686\t|\tloss: 5.54155\n",
      "Training Epoch 12  33.7% | batch:       231 of       686\t|\tloss: 4.99765\n",
      "Training Epoch 12  33.8% | batch:       232 of       686\t|\tloss: 3.87437\n",
      "Training Epoch 12  34.0% | batch:       233 of       686\t|\tloss: 8.27791\n",
      "Training Epoch 12  34.1% | batch:       234 of       686\t|\tloss: 5.96575\n",
      "Training Epoch 12  34.3% | batch:       235 of       686\t|\tloss: 5.70974\n",
      "Training Epoch 12  34.4% | batch:       236 of       686\t|\tloss: 4.39533\n",
      "Training Epoch 12  34.5% | batch:       237 of       686\t|\tloss: 5.85778\n",
      "Training Epoch 12  34.7% | batch:       238 of       686\t|\tloss: 7.01245\n",
      "Training Epoch 12  34.8% | batch:       239 of       686\t|\tloss: 5.38868\n",
      "Training Epoch 12  35.0% | batch:       240 of       686\t|\tloss: 4.67234\n",
      "Training Epoch 12  35.1% | batch:       241 of       686\t|\tloss: 6.6944\n",
      "Training Epoch 12  35.3% | batch:       242 of       686\t|\tloss: 7.32112\n",
      "Training Epoch 12  35.4% | batch:       243 of       686\t|\tloss: 5.35617\n",
      "Training Epoch 12  35.6% | batch:       244 of       686\t|\tloss: 4.52314\n",
      "Training Epoch 12  35.7% | batch:       245 of       686\t|\tloss: 6.28405\n",
      "Training Epoch 12  35.9% | batch:       246 of       686\t|\tloss: 4.34904\n",
      "Training Epoch 12  36.0% | batch:       247 of       686\t|\tloss: 5.29286\n",
      "Training Epoch 12  36.2% | batch:       248 of       686\t|\tloss: 5.75826\n",
      "Training Epoch 12  36.3% | batch:       249 of       686\t|\tloss: 4.15088\n",
      "Training Epoch 12  36.4% | batch:       250 of       686\t|\tloss: 5.57635\n",
      "Training Epoch 12  36.6% | batch:       251 of       686\t|\tloss: 5.63519\n",
      "Training Epoch 12  36.7% | batch:       252 of       686\t|\tloss: 5.89515\n",
      "Training Epoch 12  36.9% | batch:       253 of       686\t|\tloss: 5.47109\n",
      "Training Epoch 12  37.0% | batch:       254 of       686\t|\tloss: 8.32975\n",
      "Training Epoch 12  37.2% | batch:       255 of       686\t|\tloss: 5.3044\n",
      "Training Epoch 12  37.3% | batch:       256 of       686\t|\tloss: 4.63258\n",
      "Training Epoch 12  37.5% | batch:       257 of       686\t|\tloss: 5.07058\n",
      "Training Epoch 12  37.6% | batch:       258 of       686\t|\tloss: 7.0331\n",
      "Training Epoch 12  37.8% | batch:       259 of       686\t|\tloss: 6.41478\n",
      "Training Epoch 12  37.9% | batch:       260 of       686\t|\tloss: 4.01758\n",
      "Training Epoch 12  38.0% | batch:       261 of       686\t|\tloss: 5.07815\n",
      "Training Epoch 12  38.2% | batch:       262 of       686\t|\tloss: 6.10938\n",
      "Training Epoch 12  38.3% | batch:       263 of       686\t|\tloss: 5.90604\n",
      "Training Epoch 12  38.5% | batch:       264 of       686\t|\tloss: 4.04467\n",
      "Training Epoch 12  38.6% | batch:       265 of       686\t|\tloss: 5.13767\n",
      "Training Epoch 12  38.8% | batch:       266 of       686\t|\tloss: 4.48677\n",
      "Training Epoch 12  38.9% | batch:       267 of       686\t|\tloss: 5.70223\n",
      "Training Epoch 12  39.1% | batch:       268 of       686\t|\tloss: 9.61904\n",
      "Training Epoch 12  39.2% | batch:       269 of       686\t|\tloss: 6.48958\n",
      "Training Epoch 12  39.4% | batch:       270 of       686\t|\tloss: 5.65978\n",
      "Training Epoch 12  39.5% | batch:       271 of       686\t|\tloss: 5.70888\n",
      "Training Epoch 12  39.7% | batch:       272 of       686\t|\tloss: 6.00124\n",
      "Training Epoch 12  39.8% | batch:       273 of       686\t|\tloss: 4.59078\n",
      "Training Epoch 12  39.9% | batch:       274 of       686\t|\tloss: 7.96197\n",
      "Training Epoch 12  40.1% | batch:       275 of       686\t|\tloss: 6.86691\n",
      "Training Epoch 12  40.2% | batch:       276 of       686\t|\tloss: 6.5556\n",
      "Training Epoch 12  40.4% | batch:       277 of       686\t|\tloss: 5.40788\n",
      "Training Epoch 12  40.5% | batch:       278 of       686\t|\tloss: 6.68384\n",
      "Training Epoch 12  40.7% | batch:       279 of       686\t|\tloss: 5.79669\n",
      "Training Epoch 12  40.8% | batch:       280 of       686\t|\tloss: 7.22665\n",
      "Training Epoch 12  41.0% | batch:       281 of       686\t|\tloss: 4.37791\n",
      "Training Epoch 12  41.1% | batch:       282 of       686\t|\tloss: 5.89493\n",
      "Training Epoch 12  41.3% | batch:       283 of       686\t|\tloss: 5.34951\n",
      "Training Epoch 12  41.4% | batch:       284 of       686\t|\tloss: 6.55759\n",
      "Training Epoch 12  41.5% | batch:       285 of       686\t|\tloss: 4.42556\n",
      "Training Epoch 12  41.7% | batch:       286 of       686\t|\tloss: 6.65665\n",
      "Training Epoch 12  41.8% | batch:       287 of       686\t|\tloss: 5.33674\n",
      "Training Epoch 12  42.0% | batch:       288 of       686\t|\tloss: 5.91999\n",
      "Training Epoch 12  42.1% | batch:       289 of       686\t|\tloss: 4.9605\n",
      "Training Epoch 12  42.3% | batch:       290 of       686\t|\tloss: 6.61623\n",
      "Training Epoch 12  42.4% | batch:       291 of       686\t|\tloss: 5.43402\n",
      "Training Epoch 12  42.6% | batch:       292 of       686\t|\tloss: 6.12789\n",
      "Training Epoch 12  42.7% | batch:       293 of       686\t|\tloss: 6.27476\n",
      "Training Epoch 12  42.9% | batch:       294 of       686\t|\tloss: 5.20571\n",
      "Training Epoch 12  43.0% | batch:       295 of       686\t|\tloss: 7.17224\n",
      "Training Epoch 12  43.1% | batch:       296 of       686\t|\tloss: 3.74657\n",
      "Training Epoch 12  43.3% | batch:       297 of       686\t|\tloss: 6.42933\n",
      "Training Epoch 12  43.4% | batch:       298 of       686\t|\tloss: 4.92825\n",
      "Training Epoch 12  43.6% | batch:       299 of       686\t|\tloss: 5.54313\n",
      "Training Epoch 12  43.7% | batch:       300 of       686\t|\tloss: 3.69865\n",
      "Training Epoch 12  43.9% | batch:       301 of       686\t|\tloss: 6.57167\n",
      "Training Epoch 12  44.0% | batch:       302 of       686\t|\tloss: 5.87273\n",
      "Training Epoch 12  44.2% | batch:       303 of       686\t|\tloss: 5.18433\n",
      "Training Epoch 12  44.3% | batch:       304 of       686\t|\tloss: 5.50067\n",
      "Training Epoch 12  44.5% | batch:       305 of       686\t|\tloss: 8.6512\n",
      "Training Epoch 12  44.6% | batch:       306 of       686\t|\tloss: 4.9084\n",
      "Training Epoch 12  44.8% | batch:       307 of       686\t|\tloss: 5.35075\n",
      "Training Epoch 12  44.9% | batch:       308 of       686\t|\tloss: 7.48312\n",
      "Training Epoch 12  45.0% | batch:       309 of       686\t|\tloss: 6.1457\n",
      "Training Epoch 12  45.2% | batch:       310 of       686\t|\tloss: 6.29001\n",
      "Training Epoch 12  45.3% | batch:       311 of       686\t|\tloss: 5.8799\n",
      "Training Epoch 12  45.5% | batch:       312 of       686\t|\tloss: 4.70209\n",
      "Training Epoch 12  45.6% | batch:       313 of       686\t|\tloss: 5.45676\n",
      "Training Epoch 12  45.8% | batch:       314 of       686\t|\tloss: 5.01248\n",
      "Training Epoch 12  45.9% | batch:       315 of       686\t|\tloss: 5.89152\n",
      "Training Epoch 12  46.1% | batch:       316 of       686\t|\tloss: 6.22008\n",
      "Training Epoch 12  46.2% | batch:       317 of       686\t|\tloss: 5.33229\n",
      "Training Epoch 12  46.4% | batch:       318 of       686\t|\tloss: 5.30027\n",
      "Training Epoch 12  46.5% | batch:       319 of       686\t|\tloss: 4.44291\n",
      "Training Epoch 12  46.6% | batch:       320 of       686\t|\tloss: 11.0927\n",
      "Training Epoch 12  46.8% | batch:       321 of       686\t|\tloss: 4.53998\n",
      "Training Epoch 12  46.9% | batch:       322 of       686\t|\tloss: 4.86066\n",
      "Training Epoch 12  47.1% | batch:       323 of       686\t|\tloss: 5.44697\n",
      "Training Epoch 12  47.2% | batch:       324 of       686\t|\tloss: 3.87087\n",
      "Training Epoch 12  47.4% | batch:       325 of       686\t|\tloss: 4.77659\n",
      "Training Epoch 12  47.5% | batch:       326 of       686\t|\tloss: 5.01481\n",
      "Training Epoch 12  47.7% | batch:       327 of       686\t|\tloss: 3.74547\n",
      "Training Epoch 12  47.8% | batch:       328 of       686\t|\tloss: 5.9455\n",
      "Training Epoch 12  48.0% | batch:       329 of       686\t|\tloss: 4.8292\n",
      "Training Epoch 12  48.1% | batch:       330 of       686\t|\tloss: 6.20169\n",
      "Training Epoch 12  48.3% | batch:       331 of       686\t|\tloss: 4.68026\n",
      "Training Epoch 12  48.4% | batch:       332 of       686\t|\tloss: 5.64991\n",
      "Training Epoch 12  48.5% | batch:       333 of       686\t|\tloss: 5.18138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  48.7% | batch:       334 of       686\t|\tloss: 4.72542\n",
      "Training Epoch 12  48.8% | batch:       335 of       686\t|\tloss: 5.84916\n",
      "Training Epoch 12  49.0% | batch:       336 of       686\t|\tloss: 4.30206\n",
      "Training Epoch 12  49.1% | batch:       337 of       686\t|\tloss: 6.10198\n",
      "Training Epoch 12  49.3% | batch:       338 of       686\t|\tloss: 6.10049\n",
      "Training Epoch 12  49.4% | batch:       339 of       686\t|\tloss: 6.54739\n",
      "Training Epoch 12  49.6% | batch:       340 of       686\t|\tloss: 5.32393\n",
      "Training Epoch 12  49.7% | batch:       341 of       686\t|\tloss: 4.9294\n",
      "Training Epoch 12  49.9% | batch:       342 of       686\t|\tloss: 6.78143\n",
      "Training Epoch 12  50.0% | batch:       343 of       686\t|\tloss: 6.09711\n",
      "Training Epoch 12  50.1% | batch:       344 of       686\t|\tloss: 5.17384\n",
      "Training Epoch 12  50.3% | batch:       345 of       686\t|\tloss: 5.2358\n",
      "Training Epoch 12  50.4% | batch:       346 of       686\t|\tloss: 6.70256\n",
      "Training Epoch 12  50.6% | batch:       347 of       686\t|\tloss: 6.5391\n",
      "Training Epoch 12  50.7% | batch:       348 of       686\t|\tloss: 7.40183\n",
      "Training Epoch 12  50.9% | batch:       349 of       686\t|\tloss: 5.80562\n",
      "Training Epoch 12  51.0% | batch:       350 of       686\t|\tloss: 8.21103\n",
      "Training Epoch 12  51.2% | batch:       351 of       686\t|\tloss: 4.7078\n",
      "Training Epoch 12  51.3% | batch:       352 of       686\t|\tloss: 7.06401\n",
      "Training Epoch 12  51.5% | batch:       353 of       686\t|\tloss: 8.93562\n",
      "Training Epoch 12  51.6% | batch:       354 of       686\t|\tloss: 8.51782\n",
      "Training Epoch 12  51.7% | batch:       355 of       686\t|\tloss: 6.41502\n",
      "Training Epoch 12  51.9% | batch:       356 of       686\t|\tloss: 4.94668\n",
      "Training Epoch 12  52.0% | batch:       357 of       686\t|\tloss: 5.15367\n",
      "Training Epoch 12  52.2% | batch:       358 of       686\t|\tloss: 6.31373\n",
      "Training Epoch 12  52.3% | batch:       359 of       686\t|\tloss: 5.44434\n",
      "Training Epoch 12  52.5% | batch:       360 of       686\t|\tloss: 5.14914\n",
      "Training Epoch 12  52.6% | batch:       361 of       686\t|\tloss: 9.48612\n",
      "Training Epoch 12  52.8% | batch:       362 of       686\t|\tloss: 5.70826\n",
      "Training Epoch 12  52.9% | batch:       363 of       686\t|\tloss: 5.00906\n",
      "Training Epoch 12  53.1% | batch:       364 of       686\t|\tloss: 5.09538\n",
      "Training Epoch 12  53.2% | batch:       365 of       686\t|\tloss: 5.43677\n",
      "Training Epoch 12  53.4% | batch:       366 of       686\t|\tloss: 6.50711\n",
      "Training Epoch 12  53.5% | batch:       367 of       686\t|\tloss: 6.44172\n",
      "Training Epoch 12  53.6% | batch:       368 of       686\t|\tloss: 6.45777\n",
      "Training Epoch 12  53.8% | batch:       369 of       686\t|\tloss: 4.83337\n",
      "Training Epoch 12  53.9% | batch:       370 of       686\t|\tloss: 4.68502\n",
      "Training Epoch 12  54.1% | batch:       371 of       686\t|\tloss: 8.74193\n",
      "Training Epoch 12  54.2% | batch:       372 of       686\t|\tloss: 5.07805\n",
      "Training Epoch 12  54.4% | batch:       373 of       686\t|\tloss: 5.00068\n",
      "Training Epoch 12  54.5% | batch:       374 of       686\t|\tloss: 5.54665\n",
      "Training Epoch 12  54.7% | batch:       375 of       686\t|\tloss: 5.43837\n",
      "Training Epoch 12  54.8% | batch:       376 of       686\t|\tloss: 6.75441\n",
      "Training Epoch 12  55.0% | batch:       377 of       686\t|\tloss: 6.02522\n",
      "Training Epoch 12  55.1% | batch:       378 of       686\t|\tloss: 4.79767\n",
      "Training Epoch 12  55.2% | batch:       379 of       686\t|\tloss: 5.40968\n",
      "Training Epoch 12  55.4% | batch:       380 of       686\t|\tloss: 5.26306\n",
      "Training Epoch 12  55.5% | batch:       381 of       686\t|\tloss: 3.91014\n",
      "Training Epoch 12  55.7% | batch:       382 of       686\t|\tloss: 4.99528\n",
      "Training Epoch 12  55.8% | batch:       383 of       686\t|\tloss: 5.73146\n",
      "Training Epoch 12  56.0% | batch:       384 of       686\t|\tloss: 5.09433\n",
      "Training Epoch 12  56.1% | batch:       385 of       686\t|\tloss: 5.89777\n",
      "Training Epoch 12  56.3% | batch:       386 of       686\t|\tloss: 5.14245\n",
      "Training Epoch 12  56.4% | batch:       387 of       686\t|\tloss: 5.17189\n",
      "Training Epoch 12  56.6% | batch:       388 of       686\t|\tloss: 5.26748\n",
      "Training Epoch 12  56.7% | batch:       389 of       686\t|\tloss: 5.65637\n",
      "Training Epoch 12  56.9% | batch:       390 of       686\t|\tloss: 5.64428\n",
      "Training Epoch 12  57.0% | batch:       391 of       686\t|\tloss: 5.59278\n",
      "Training Epoch 12  57.1% | batch:       392 of       686\t|\tloss: 6.64\n",
      "Training Epoch 12  57.3% | batch:       393 of       686\t|\tloss: 7.01743\n",
      "Training Epoch 12  57.4% | batch:       394 of       686\t|\tloss: 5.83218\n",
      "Training Epoch 12  57.6% | batch:       395 of       686\t|\tloss: 4.67521\n",
      "Training Epoch 12  57.7% | batch:       396 of       686\t|\tloss: 5.65671\n",
      "Training Epoch 12  57.9% | batch:       397 of       686\t|\tloss: 6.92626\n",
      "Training Epoch 12  58.0% | batch:       398 of       686\t|\tloss: 5.66437\n",
      "Training Epoch 12  58.2% | batch:       399 of       686\t|\tloss: 5.82007\n",
      "Training Epoch 12  58.3% | batch:       400 of       686\t|\tloss: 5.07101\n",
      "Training Epoch 12  58.5% | batch:       401 of       686\t|\tloss: 6.25621\n",
      "Training Epoch 12  58.6% | batch:       402 of       686\t|\tloss: 5.80959\n",
      "Training Epoch 12  58.7% | batch:       403 of       686\t|\tloss: 5.19437\n",
      "Training Epoch 12  58.9% | batch:       404 of       686\t|\tloss: 6.58335\n",
      "Training Epoch 12  59.0% | batch:       405 of       686\t|\tloss: 4.7634\n",
      "Training Epoch 12  59.2% | batch:       406 of       686\t|\tloss: 5.83531\n",
      "Training Epoch 12  59.3% | batch:       407 of       686\t|\tloss: 8.31452\n",
      "Training Epoch 12  59.5% | batch:       408 of       686\t|\tloss: 9.27995\n",
      "Training Epoch 12  59.6% | batch:       409 of       686\t|\tloss: 5.68721\n",
      "Training Epoch 12  59.8% | batch:       410 of       686\t|\tloss: 5.49105\n",
      "Training Epoch 12  59.9% | batch:       411 of       686\t|\tloss: 4.85575\n",
      "Training Epoch 12  60.1% | batch:       412 of       686\t|\tloss: 4.77437\n",
      "Training Epoch 12  60.2% | batch:       413 of       686\t|\tloss: 5.58578\n",
      "Training Epoch 12  60.3% | batch:       414 of       686\t|\tloss: 6.16558\n",
      "Training Epoch 12  60.5% | batch:       415 of       686\t|\tloss: 4.99804\n",
      "Training Epoch 12  60.6% | batch:       416 of       686\t|\tloss: 5.75421\n",
      "Training Epoch 12  60.8% | batch:       417 of       686\t|\tloss: 4.8922\n",
      "Training Epoch 12  60.9% | batch:       418 of       686\t|\tloss: 5.07071\n",
      "Training Epoch 12  61.1% | batch:       419 of       686\t|\tloss: 7.196\n",
      "Training Epoch 12  61.2% | batch:       420 of       686\t|\tloss: 5.61358\n",
      "Training Epoch 12  61.4% | batch:       421 of       686\t|\tloss: 4.30936\n",
      "Training Epoch 12  61.5% | batch:       422 of       686\t|\tloss: 5.52224\n",
      "Training Epoch 12  61.7% | batch:       423 of       686\t|\tloss: 5.43486\n",
      "Training Epoch 12  61.8% | batch:       424 of       686\t|\tloss: 4.65689\n",
      "Training Epoch 12  62.0% | batch:       425 of       686\t|\tloss: 7.35553\n",
      "Training Epoch 12  62.1% | batch:       426 of       686\t|\tloss: 4.37877\n",
      "Training Epoch 12  62.2% | batch:       427 of       686\t|\tloss: 3.70101\n",
      "Training Epoch 12  62.4% | batch:       428 of       686\t|\tloss: 3.56778\n",
      "Training Epoch 12  62.5% | batch:       429 of       686\t|\tloss: 6.31414\n",
      "Training Epoch 12  62.7% | batch:       430 of       686\t|\tloss: 5.00983\n",
      "Training Epoch 12  62.8% | batch:       431 of       686\t|\tloss: 4.99295\n",
      "Training Epoch 12  63.0% | batch:       432 of       686\t|\tloss: 4.77727\n",
      "Training Epoch 12  63.1% | batch:       433 of       686\t|\tloss: 5.17292\n",
      "Training Epoch 12  63.3% | batch:       434 of       686\t|\tloss: 5.1\n",
      "Training Epoch 12  63.4% | batch:       435 of       686\t|\tloss: 5.94153\n",
      "Training Epoch 12  63.6% | batch:       436 of       686\t|\tloss: 5.03842\n",
      "Training Epoch 12  63.7% | batch:       437 of       686\t|\tloss: 5.28961\n",
      "Training Epoch 12  63.8% | batch:       438 of       686\t|\tloss: 5.89296\n",
      "Training Epoch 12  64.0% | batch:       439 of       686\t|\tloss: 5.67481\n",
      "Training Epoch 12  64.1% | batch:       440 of       686\t|\tloss: 6.67676\n",
      "Training Epoch 12  64.3% | batch:       441 of       686\t|\tloss: 6.43259\n",
      "Training Epoch 12  64.4% | batch:       442 of       686\t|\tloss: 5.24812\n",
      "Training Epoch 12  64.6% | batch:       443 of       686\t|\tloss: 5.96122\n",
      "Training Epoch 12  64.7% | batch:       444 of       686\t|\tloss: 5.26163\n",
      "Training Epoch 12  64.9% | batch:       445 of       686\t|\tloss: 5.61915\n",
      "Training Epoch 12  65.0% | batch:       446 of       686\t|\tloss: 5.15387\n",
      "Training Epoch 12  65.2% | batch:       447 of       686\t|\tloss: 4.88572\n",
      "Training Epoch 12  65.3% | batch:       448 of       686\t|\tloss: 6.92811\n",
      "Training Epoch 12  65.5% | batch:       449 of       686\t|\tloss: 6.0107\n",
      "Training Epoch 12  65.6% | batch:       450 of       686\t|\tloss: 5.0458\n",
      "Training Epoch 12  65.7% | batch:       451 of       686\t|\tloss: 4.28509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  65.9% | batch:       452 of       686\t|\tloss: 5.70473\n",
      "Training Epoch 12  66.0% | batch:       453 of       686\t|\tloss: 4.41325\n",
      "Training Epoch 12  66.2% | batch:       454 of       686\t|\tloss: 4.73013\n",
      "Training Epoch 12  66.3% | batch:       455 of       686\t|\tloss: 8.02598\n",
      "Training Epoch 12  66.5% | batch:       456 of       686\t|\tloss: 4.89143\n",
      "Training Epoch 12  66.6% | batch:       457 of       686\t|\tloss: 4.07636\n",
      "Training Epoch 12  66.8% | batch:       458 of       686\t|\tloss: 6.51293\n",
      "Training Epoch 12  66.9% | batch:       459 of       686\t|\tloss: 6.39276\n",
      "Training Epoch 12  67.1% | batch:       460 of       686\t|\tloss: 5.75713\n",
      "Training Epoch 12  67.2% | batch:       461 of       686\t|\tloss: 5.67821\n",
      "Training Epoch 12  67.3% | batch:       462 of       686\t|\tloss: 5.13019\n",
      "Training Epoch 12  67.5% | batch:       463 of       686\t|\tloss: 9.79072\n",
      "Training Epoch 12  67.6% | batch:       464 of       686\t|\tloss: 5.9981\n",
      "Training Epoch 12  67.8% | batch:       465 of       686\t|\tloss: 3.69331\n",
      "Training Epoch 12  67.9% | batch:       466 of       686\t|\tloss: 4.77109\n",
      "Training Epoch 12  68.1% | batch:       467 of       686\t|\tloss: 6.56656\n",
      "Training Epoch 12  68.2% | batch:       468 of       686\t|\tloss: 6.23908\n",
      "Training Epoch 12  68.4% | batch:       469 of       686\t|\tloss: 5.46479\n",
      "Training Epoch 12  68.5% | batch:       470 of       686\t|\tloss: 5.25595\n",
      "Training Epoch 12  68.7% | batch:       471 of       686\t|\tloss: 5.74202\n",
      "Training Epoch 12  68.8% | batch:       472 of       686\t|\tloss: 3.41906\n",
      "Training Epoch 12  69.0% | batch:       473 of       686\t|\tloss: 5.27307\n",
      "Training Epoch 12  69.1% | batch:       474 of       686\t|\tloss: 7.00457\n",
      "Training Epoch 12  69.2% | batch:       475 of       686\t|\tloss: 5.1016\n",
      "Training Epoch 12  69.4% | batch:       476 of       686\t|\tloss: 5.70884\n",
      "Training Epoch 12  69.5% | batch:       477 of       686\t|\tloss: 4.93748\n",
      "Training Epoch 12  69.7% | batch:       478 of       686\t|\tloss: 7.70941\n",
      "Training Epoch 12  69.8% | batch:       479 of       686\t|\tloss: 4.15841\n",
      "Training Epoch 12  70.0% | batch:       480 of       686\t|\tloss: 5.29722\n",
      "Training Epoch 12  70.1% | batch:       481 of       686\t|\tloss: 4.83792\n",
      "Training Epoch 12  70.3% | batch:       482 of       686\t|\tloss: 3.71266\n",
      "Training Epoch 12  70.4% | batch:       483 of       686\t|\tloss: 4.17159\n",
      "Training Epoch 12  70.6% | batch:       484 of       686\t|\tloss: 6.50975\n",
      "Training Epoch 12  70.7% | batch:       485 of       686\t|\tloss: 5.25527\n",
      "Training Epoch 12  70.8% | batch:       486 of       686\t|\tloss: 6.90877\n",
      "Training Epoch 12  71.0% | batch:       487 of       686\t|\tloss: 8.13379\n",
      "Training Epoch 12  71.1% | batch:       488 of       686\t|\tloss: 6.07149\n",
      "Training Epoch 12  71.3% | batch:       489 of       686\t|\tloss: 4.70762\n",
      "Training Epoch 12  71.4% | batch:       490 of       686\t|\tloss: 7.63479\n",
      "Training Epoch 12  71.6% | batch:       491 of       686\t|\tloss: 6.67811\n",
      "Training Epoch 12  71.7% | batch:       492 of       686\t|\tloss: 3.76018\n",
      "Training Epoch 12  71.9% | batch:       493 of       686\t|\tloss: 5.13365\n",
      "Training Epoch 12  72.0% | batch:       494 of       686\t|\tloss: 5.48324\n",
      "Training Epoch 12  72.2% | batch:       495 of       686\t|\tloss: 4.49875\n",
      "Training Epoch 12  72.3% | batch:       496 of       686\t|\tloss: 5.75558\n",
      "Training Epoch 12  72.4% | batch:       497 of       686\t|\tloss: 5.90354\n",
      "Training Epoch 12  72.6% | batch:       498 of       686\t|\tloss: 4.33401\n",
      "Training Epoch 12  72.7% | batch:       499 of       686\t|\tloss: 6.07476\n",
      "Training Epoch 12  72.9% | batch:       500 of       686\t|\tloss: 5.64637\n",
      "Training Epoch 12  73.0% | batch:       501 of       686\t|\tloss: 5.0624\n",
      "Training Epoch 12  73.2% | batch:       502 of       686\t|\tloss: 5.60109\n",
      "Training Epoch 12  73.3% | batch:       503 of       686\t|\tloss: 5.65837\n",
      "Training Epoch 12  73.5% | batch:       504 of       686\t|\tloss: 3.07601\n",
      "Training Epoch 12  73.6% | batch:       505 of       686\t|\tloss: 3.93816\n",
      "Training Epoch 12  73.8% | batch:       506 of       686\t|\tloss: 6.95024\n",
      "Training Epoch 12  73.9% | batch:       507 of       686\t|\tloss: 5.55717\n",
      "Training Epoch 12  74.1% | batch:       508 of       686\t|\tloss: 4.52552\n",
      "Training Epoch 12  74.2% | batch:       509 of       686\t|\tloss: 6.07172\n",
      "Training Epoch 12  74.3% | batch:       510 of       686\t|\tloss: 6.45678\n",
      "Training Epoch 12  74.5% | batch:       511 of       686\t|\tloss: 4.4401\n",
      "Training Epoch 12  74.6% | batch:       512 of       686\t|\tloss: 5.08816\n",
      "Training Epoch 12  74.8% | batch:       513 of       686\t|\tloss: 10.8036\n",
      "Training Epoch 12  74.9% | batch:       514 of       686\t|\tloss: 5.97643\n",
      "Training Epoch 12  75.1% | batch:       515 of       686\t|\tloss: 5.02169\n",
      "Training Epoch 12  75.2% | batch:       516 of       686\t|\tloss: 5.31367\n",
      "Training Epoch 12  75.4% | batch:       517 of       686\t|\tloss: 3.77709\n",
      "Training Epoch 12  75.5% | batch:       518 of       686\t|\tloss: 3.75878\n",
      "Training Epoch 12  75.7% | batch:       519 of       686\t|\tloss: 5.68098\n",
      "Training Epoch 12  75.8% | batch:       520 of       686\t|\tloss: 4.6838\n",
      "Training Epoch 12  75.9% | batch:       521 of       686\t|\tloss: 4.88771\n",
      "Training Epoch 12  76.1% | batch:       522 of       686\t|\tloss: 6.338\n",
      "Training Epoch 12  76.2% | batch:       523 of       686\t|\tloss: 6.02273\n",
      "Training Epoch 12  76.4% | batch:       524 of       686\t|\tloss: 4.24957\n",
      "Training Epoch 12  76.5% | batch:       525 of       686\t|\tloss: 6.83201\n",
      "Training Epoch 12  76.7% | batch:       526 of       686\t|\tloss: 4.81383\n",
      "Training Epoch 12  76.8% | batch:       527 of       686\t|\tloss: 5.13108\n",
      "Training Epoch 12  77.0% | batch:       528 of       686\t|\tloss: 6.99722\n",
      "Training Epoch 12  77.1% | batch:       529 of       686\t|\tloss: 4.99629\n",
      "Training Epoch 12  77.3% | batch:       530 of       686\t|\tloss: 5.22627\n",
      "Training Epoch 12  77.4% | batch:       531 of       686\t|\tloss: 5.08763\n",
      "Training Epoch 12  77.6% | batch:       532 of       686\t|\tloss: 5.02528\n",
      "Training Epoch 12  77.7% | batch:       533 of       686\t|\tloss: 5.22425\n",
      "Training Epoch 12  77.8% | batch:       534 of       686\t|\tloss: 6.10146\n",
      "Training Epoch 12  78.0% | batch:       535 of       686\t|\tloss: 7.44742\n",
      "Training Epoch 12  78.1% | batch:       536 of       686\t|\tloss: 6.28066\n",
      "Training Epoch 12  78.3% | batch:       537 of       686\t|\tloss: 7.17393\n",
      "Training Epoch 12  78.4% | batch:       538 of       686\t|\tloss: 5.81249\n",
      "Training Epoch 12  78.6% | batch:       539 of       686\t|\tloss: 5.19387\n",
      "Training Epoch 12  78.7% | batch:       540 of       686\t|\tloss: 4.40988\n",
      "Training Epoch 12  78.9% | batch:       541 of       686\t|\tloss: 5.08263\n",
      "Training Epoch 12  79.0% | batch:       542 of       686\t|\tloss: 7.68843\n",
      "Training Epoch 12  79.2% | batch:       543 of       686\t|\tloss: 5.81611\n",
      "Training Epoch 12  79.3% | batch:       544 of       686\t|\tloss: 4.00634\n",
      "Training Epoch 12  79.4% | batch:       545 of       686\t|\tloss: 5.73366\n",
      "Training Epoch 12  79.6% | batch:       546 of       686\t|\tloss: 4.61106\n",
      "Training Epoch 12  79.7% | batch:       547 of       686\t|\tloss: 6.19639\n",
      "Training Epoch 12  79.9% | batch:       548 of       686\t|\tloss: 4.71552\n",
      "Training Epoch 12  80.0% | batch:       549 of       686\t|\tloss: 4.58277\n",
      "Training Epoch 12  80.2% | batch:       550 of       686\t|\tloss: 4.98145\n",
      "Training Epoch 12  80.3% | batch:       551 of       686\t|\tloss: 6.76138\n",
      "Training Epoch 12  80.5% | batch:       552 of       686\t|\tloss: 5.11127\n",
      "Training Epoch 12  80.6% | batch:       553 of       686\t|\tloss: 6.41234\n",
      "Training Epoch 12  80.8% | batch:       554 of       686\t|\tloss: 6.56811\n",
      "Training Epoch 12  80.9% | batch:       555 of       686\t|\tloss: 6.04431\n",
      "Training Epoch 12  81.0% | batch:       556 of       686\t|\tloss: 5.52354\n",
      "Training Epoch 12  81.2% | batch:       557 of       686\t|\tloss: 5.15082\n",
      "Training Epoch 12  81.3% | batch:       558 of       686\t|\tloss: 5.37507\n",
      "Training Epoch 12  81.5% | batch:       559 of       686\t|\tloss: 4.71846\n",
      "Training Epoch 12  81.6% | batch:       560 of       686\t|\tloss: 4.58579\n",
      "Training Epoch 12  81.8% | batch:       561 of       686\t|\tloss: 6.99405\n",
      "Training Epoch 12  81.9% | batch:       562 of       686\t|\tloss: 5.36425\n",
      "Training Epoch 12  82.1% | batch:       563 of       686\t|\tloss: 9.18145\n",
      "Training Epoch 12  82.2% | batch:       564 of       686\t|\tloss: 4.38643\n",
      "Training Epoch 12  82.4% | batch:       565 of       686\t|\tloss: 4.24621\n",
      "Training Epoch 12  82.5% | batch:       566 of       686\t|\tloss: 6.6564\n",
      "Training Epoch 12  82.7% | batch:       567 of       686\t|\tloss: 4.97368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  82.8% | batch:       568 of       686\t|\tloss: 6.09636\n",
      "Training Epoch 12  82.9% | batch:       569 of       686\t|\tloss: 8.33377\n",
      "Training Epoch 12  83.1% | batch:       570 of       686\t|\tloss: 5.81974\n",
      "Training Epoch 12  83.2% | batch:       571 of       686\t|\tloss: 5.13122\n",
      "Training Epoch 12  83.4% | batch:       572 of       686\t|\tloss: 6.69755\n",
      "Training Epoch 12  83.5% | batch:       573 of       686\t|\tloss: 4.56894\n",
      "Training Epoch 12  83.7% | batch:       574 of       686\t|\tloss: 4.28583\n",
      "Training Epoch 12  83.8% | batch:       575 of       686\t|\tloss: 5.40005\n",
      "Training Epoch 12  84.0% | batch:       576 of       686\t|\tloss: 4.73223\n",
      "Training Epoch 12  84.1% | batch:       577 of       686\t|\tloss: 5.08539\n",
      "Training Epoch 12  84.3% | batch:       578 of       686\t|\tloss: 4.46328\n",
      "Training Epoch 12  84.4% | batch:       579 of       686\t|\tloss: 4.85281\n",
      "Training Epoch 12  84.5% | batch:       580 of       686\t|\tloss: 6.19012\n",
      "Training Epoch 12  84.7% | batch:       581 of       686\t|\tloss: 4.5695\n",
      "Training Epoch 12  84.8% | batch:       582 of       686\t|\tloss: 5.3169\n",
      "Training Epoch 12  85.0% | batch:       583 of       686\t|\tloss: 7.20258\n",
      "Training Epoch 12  85.1% | batch:       584 of       686\t|\tloss: 5.79078\n",
      "Training Epoch 12  85.3% | batch:       585 of       686\t|\tloss: 3.88227\n",
      "Training Epoch 12  85.4% | batch:       586 of       686\t|\tloss: 5.57337\n",
      "Training Epoch 12  85.6% | batch:       587 of       686\t|\tloss: 5.02937\n",
      "Training Epoch 12  85.7% | batch:       588 of       686\t|\tloss: 5.67235\n",
      "Training Epoch 12  85.9% | batch:       589 of       686\t|\tloss: 6.7311\n",
      "Training Epoch 12  86.0% | batch:       590 of       686\t|\tloss: 5.99562\n",
      "Training Epoch 12  86.2% | batch:       591 of       686\t|\tloss: 4.47653\n",
      "Training Epoch 12  86.3% | batch:       592 of       686\t|\tloss: 5.449\n",
      "Training Epoch 12  86.4% | batch:       593 of       686\t|\tloss: 4.72855\n",
      "Training Epoch 12  86.6% | batch:       594 of       686\t|\tloss: 3.80732\n",
      "Training Epoch 12  86.7% | batch:       595 of       686\t|\tloss: 5.08442\n",
      "Training Epoch 12  86.9% | batch:       596 of       686\t|\tloss: 6.25163\n",
      "Training Epoch 12  87.0% | batch:       597 of       686\t|\tloss: 6.5225\n",
      "Training Epoch 12  87.2% | batch:       598 of       686\t|\tloss: 4.46317\n",
      "Training Epoch 12  87.3% | batch:       599 of       686\t|\tloss: 5.88629\n",
      "Training Epoch 12  87.5% | batch:       600 of       686\t|\tloss: 5.60246\n",
      "Training Epoch 12  87.6% | batch:       601 of       686\t|\tloss: 4.71776\n",
      "Training Epoch 12  87.8% | batch:       602 of       686\t|\tloss: 4.23312\n",
      "Training Epoch 12  87.9% | batch:       603 of       686\t|\tloss: 6.16674\n",
      "Training Epoch 12  88.0% | batch:       604 of       686\t|\tloss: 6.13206\n",
      "Training Epoch 12  88.2% | batch:       605 of       686\t|\tloss: 5.79011\n",
      "Training Epoch 12  88.3% | batch:       606 of       686\t|\tloss: 5.77947\n",
      "Training Epoch 12  88.5% | batch:       607 of       686\t|\tloss: 5.35728\n",
      "Training Epoch 12  88.6% | batch:       608 of       686\t|\tloss: 5.8027\n",
      "Training Epoch 12  88.8% | batch:       609 of       686\t|\tloss: 6.37173\n",
      "Training Epoch 12  88.9% | batch:       610 of       686\t|\tloss: 5.80649\n",
      "Training Epoch 12  89.1% | batch:       611 of       686\t|\tloss: 6.65823\n",
      "Training Epoch 12  89.2% | batch:       612 of       686\t|\tloss: 4.64518\n",
      "Training Epoch 12  89.4% | batch:       613 of       686\t|\tloss: 5.99595\n",
      "Training Epoch 12  89.5% | batch:       614 of       686\t|\tloss: 5.6602\n",
      "Training Epoch 12  89.7% | batch:       615 of       686\t|\tloss: 5.92514\n",
      "Training Epoch 12  89.8% | batch:       616 of       686\t|\tloss: 4.74146\n",
      "Training Epoch 12  89.9% | batch:       617 of       686\t|\tloss: 4.11924\n",
      "Training Epoch 12  90.1% | batch:       618 of       686\t|\tloss: 4.15517\n",
      "Training Epoch 12  90.2% | batch:       619 of       686\t|\tloss: 4.53672\n",
      "Training Epoch 12  90.4% | batch:       620 of       686\t|\tloss: 4.99457\n",
      "Training Epoch 12  90.5% | batch:       621 of       686\t|\tloss: 5.6586\n",
      "Training Epoch 12  90.7% | batch:       622 of       686\t|\tloss: 5.48094\n",
      "Training Epoch 12  90.8% | batch:       623 of       686\t|\tloss: 3.83441\n",
      "Training Epoch 12  91.0% | batch:       624 of       686\t|\tloss: 5.08764\n",
      "Training Epoch 12  91.1% | batch:       625 of       686\t|\tloss: 4.10624\n",
      "Training Epoch 12  91.3% | batch:       626 of       686\t|\tloss: 6.77413\n",
      "Training Epoch 12  91.4% | batch:       627 of       686\t|\tloss: 7.62476\n",
      "Training Epoch 12  91.5% | batch:       628 of       686\t|\tloss: 6.54567\n",
      "Training Epoch 12  91.7% | batch:       629 of       686\t|\tloss: 5.37147\n",
      "Training Epoch 12  91.8% | batch:       630 of       686\t|\tloss: 4.76383\n",
      "Training Epoch 12  92.0% | batch:       631 of       686\t|\tloss: 4.88967\n",
      "Training Epoch 12  92.1% | batch:       632 of       686\t|\tloss: 4.46014\n",
      "Training Epoch 12  92.3% | batch:       633 of       686\t|\tloss: 5.65612\n",
      "Training Epoch 12  92.4% | batch:       634 of       686\t|\tloss: 6.67344\n",
      "Training Epoch 12  92.6% | batch:       635 of       686\t|\tloss: 5.59357\n",
      "Training Epoch 12  92.7% | batch:       636 of       686\t|\tloss: 4.47086\n",
      "Training Epoch 12  92.9% | batch:       637 of       686\t|\tloss: 3.80382\n",
      "Training Epoch 12  93.0% | batch:       638 of       686\t|\tloss: 4.97052\n",
      "Training Epoch 12  93.1% | batch:       639 of       686\t|\tloss: 5.93906\n",
      "Training Epoch 12  93.3% | batch:       640 of       686\t|\tloss: 5.85952\n",
      "Training Epoch 12  93.4% | batch:       641 of       686\t|\tloss: 6.23376\n",
      "Training Epoch 12  93.6% | batch:       642 of       686\t|\tloss: 3.67386\n",
      "Training Epoch 12  93.7% | batch:       643 of       686\t|\tloss: 8.68409\n",
      "Training Epoch 12  93.9% | batch:       644 of       686\t|\tloss: 5.49473\n",
      "Training Epoch 12  94.0% | batch:       645 of       686\t|\tloss: 5.32194\n",
      "Training Epoch 12  94.2% | batch:       646 of       686\t|\tloss: 4.16684\n",
      "Training Epoch 12  94.3% | batch:       647 of       686\t|\tloss: 6.26517\n",
      "Training Epoch 12  94.5% | batch:       648 of       686\t|\tloss: 5.37862\n",
      "Training Epoch 12  94.6% | batch:       649 of       686\t|\tloss: 4.80444\n",
      "Training Epoch 12  94.8% | batch:       650 of       686\t|\tloss: 5.89923\n",
      "Training Epoch 12  94.9% | batch:       651 of       686\t|\tloss: 5.76101\n",
      "Training Epoch 12  95.0% | batch:       652 of       686\t|\tloss: 5.51401\n",
      "Training Epoch 12  95.2% | batch:       653 of       686\t|\tloss: 6.62924\n",
      "Training Epoch 12  95.3% | batch:       654 of       686\t|\tloss: 5.14138\n",
      "Training Epoch 12  95.5% | batch:       655 of       686\t|\tloss: 6.9523\n",
      "Training Epoch 12  95.6% | batch:       656 of       686\t|\tloss: 5.02714\n",
      "Training Epoch 12  95.8% | batch:       657 of       686\t|\tloss: 6.03822\n",
      "Training Epoch 12  95.9% | batch:       658 of       686\t|\tloss: 3.88089\n",
      "Training Epoch 12  96.1% | batch:       659 of       686\t|\tloss: 7.14285\n",
      "Training Epoch 12  96.2% | batch:       660 of       686\t|\tloss: 5.68858\n",
      "Training Epoch 12  96.4% | batch:       661 of       686\t|\tloss: 4.31591\n",
      "Training Epoch 12  96.5% | batch:       662 of       686\t|\tloss: 4.85394\n",
      "Training Epoch 12  96.6% | batch:       663 of       686\t|\tloss: 6.51624\n",
      "Training Epoch 12  96.8% | batch:       664 of       686\t|\tloss: 6.88004\n",
      "Training Epoch 12  96.9% | batch:       665 of       686\t|\tloss: 4.35442\n",
      "Training Epoch 12  97.1% | batch:       666 of       686\t|\tloss: 5.66867\n",
      "Training Epoch 12  97.2% | batch:       667 of       686\t|\tloss: 3.83918\n",
      "Training Epoch 12  97.4% | batch:       668 of       686\t|\tloss: 5.96227\n",
      "Training Epoch 12  97.5% | batch:       669 of       686\t|\tloss: 3.9778\n",
      "Training Epoch 12  97.7% | batch:       670 of       686\t|\tloss: 5.53574\n",
      "Training Epoch 12  97.8% | batch:       671 of       686\t|\tloss: 8.07059\n",
      "Training Epoch 12  98.0% | batch:       672 of       686\t|\tloss: 4.67476\n",
      "Training Epoch 12  98.1% | batch:       673 of       686\t|\tloss: 3.93879\n",
      "Training Epoch 12  98.3% | batch:       674 of       686\t|\tloss: 4.39129\n",
      "Training Epoch 12  98.4% | batch:       675 of       686\t|\tloss: 3.55375\n",
      "Training Epoch 12  98.5% | batch:       676 of       686\t|\tloss: 5.11072\n",
      "Training Epoch 12  98.7% | batch:       677 of       686\t|\tloss: 4.98634\n",
      "Training Epoch 12  98.8% | batch:       678 of       686\t|\tloss: 4.53396\n",
      "Training Epoch 12  99.0% | batch:       679 of       686\t|\tloss: 4.21993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:05:19,150 | INFO : Epoch 12 Training Summary: epoch: 12.000000 | loss: 5.752065 | \n",
      "2023-05-25 22:05:19,151 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 24.506510972976685 seconds\n",
      "\n",
      "2023-05-25 22:05:19,152 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.584590315818787 seconds\n",
      "2023-05-25 22:05:19,152 | INFO : Avg batch train. time: 0.03437986926504196 seconds\n",
      "2023-05-25 22:05:19,152 | INFO : Avg sample train. time: 0.0002689388256550406 seconds\n",
      "2023-05-25 22:05:19,153 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  99.1% | batch:       680 of       686\t|\tloss: 4.89161\n",
      "Training Epoch 12  99.3% | batch:       681 of       686\t|\tloss: 4.72973\n",
      "Training Epoch 12  99.4% | batch:       682 of       686\t|\tloss: 5.14502\n",
      "Training Epoch 12  99.6% | batch:       683 of       686\t|\tloss: 3.87484\n",
      "Training Epoch 12  99.7% | batch:       684 of       686\t|\tloss: 4.411\n",
      "Training Epoch 12  99.9% | batch:       685 of       686\t|\tloss: 9.88457\n",
      "\n",
      "Evaluating Epoch 12   0.0% | batch:         0 of       172\t|\tloss: 1.5532\n",
      "Evaluating Epoch 12   0.6% | batch:         1 of       172\t|\tloss: 2.11571\n",
      "Evaluating Epoch 12   1.2% | batch:         2 of       172\t|\tloss: 1.61827\n",
      "Evaluating Epoch 12   1.7% | batch:         3 of       172\t|\tloss: 3.6144\n",
      "Evaluating Epoch 12   2.3% | batch:         4 of       172\t|\tloss: 1.72282\n",
      "Evaluating Epoch 12   2.9% | batch:         5 of       172\t|\tloss: 1.68111\n",
      "Evaluating Epoch 12   3.5% | batch:         6 of       172\t|\tloss: 2.04599\n",
      "Evaluating Epoch 12   4.1% | batch:         7 of       172\t|\tloss: 3.71056\n",
      "Evaluating Epoch 12   4.7% | batch:         8 of       172\t|\tloss: 1.51455\n",
      "Evaluating Epoch 12   5.2% | batch:         9 of       172\t|\tloss: 2.18157\n",
      "Evaluating Epoch 12   5.8% | batch:        10 of       172\t|\tloss: 2.4848\n",
      "Evaluating Epoch 12   6.4% | batch:        11 of       172\t|\tloss: 1.91256\n",
      "Evaluating Epoch 12   7.0% | batch:        12 of       172\t|\tloss: 1.76933\n",
      "Evaluating Epoch 12   7.6% | batch:        13 of       172\t|\tloss: 2.1421\n",
      "Evaluating Epoch 12   8.1% | batch:        14 of       172\t|\tloss: 2.76709\n",
      "Evaluating Epoch 12   8.7% | batch:        15 of       172\t|\tloss: 1.57329\n",
      "Evaluating Epoch 12   9.3% | batch:        16 of       172\t|\tloss: 2.52474\n",
      "Evaluating Epoch 12   9.9% | batch:        17 of       172\t|\tloss: 1.52857\n",
      "Evaluating Epoch 12  10.5% | batch:        18 of       172\t|\tloss: 16.1192\n",
      "Evaluating Epoch 12  11.0% | batch:        19 of       172\t|\tloss: 1.67603\n",
      "Evaluating Epoch 12  11.6% | batch:        20 of       172\t|\tloss: 1.4857\n",
      "Evaluating Epoch 12  12.2% | batch:        21 of       172\t|\tloss: 0.36233\n",
      "Evaluating Epoch 12  12.8% | batch:        22 of       172\t|\tloss: 2.9203\n",
      "Evaluating Epoch 12  13.4% | batch:        23 of       172\t|\tloss: 2.59959\n",
      "Evaluating Epoch 12  14.0% | batch:        24 of       172\t|\tloss: 1.13789\n",
      "Evaluating Epoch 12  14.5% | batch:        25 of       172\t|\tloss: 1.59353\n",
      "Evaluating Epoch 12  15.1% | batch:        26 of       172\t|\tloss: 7.65405\n",
      "Evaluating Epoch 12  15.7% | batch:        27 of       172\t|\tloss: 15.6882\n",
      "Evaluating Epoch 12  16.3% | batch:        28 of       172\t|\tloss: 0.267581\n",
      "Evaluating Epoch 12  16.9% | batch:        29 of       172\t|\tloss: 0.899263\n",
      "Evaluating Epoch 12  17.4% | batch:        30 of       172\t|\tloss: 0.714426\n",
      "Evaluating Epoch 12  18.0% | batch:        31 of       172\t|\tloss: 0.667538\n",
      "Evaluating Epoch 12  18.6% | batch:        32 of       172\t|\tloss: 0.762808\n",
      "Evaluating Epoch 12  19.2% | batch:        33 of       172\t|\tloss: 0.913811\n",
      "Evaluating Epoch 12  19.8% | batch:        34 of       172\t|\tloss: 0.336415\n",
      "Evaluating Epoch 12  20.3% | batch:        35 of       172\t|\tloss: 0.41177\n",
      "Evaluating Epoch 12  20.9% | batch:        36 of       172\t|\tloss: 4.25827\n",
      "Evaluating Epoch 12  21.5% | batch:        37 of       172\t|\tloss: 5.02847\n",
      "Evaluating Epoch 12  22.1% | batch:        38 of       172\t|\tloss: 3.20203\n",
      "Evaluating Epoch 12  22.7% | batch:        39 of       172\t|\tloss: 6.37953\n",
      "Evaluating Epoch 12  23.3% | batch:        40 of       172\t|\tloss: 0.635857\n",
      "Evaluating Epoch 12  23.8% | batch:        41 of       172\t|\tloss: 0.561339\n",
      "Evaluating Epoch 12  24.4% | batch:        42 of       172\t|\tloss: 0.800439\n",
      "Evaluating Epoch 12  25.0% | batch:        43 of       172\t|\tloss: 17.6498\n",
      "Evaluating Epoch 12  25.6% | batch:        44 of       172\t|\tloss: 1.46264\n",
      "Evaluating Epoch 12  26.2% | batch:        45 of       172\t|\tloss: 0.468986\n",
      "Evaluating Epoch 12  26.7% | batch:        46 of       172\t|\tloss: 0.252475\n",
      "Evaluating Epoch 12  27.3% | batch:        47 of       172\t|\tloss: 0.787335\n",
      "Evaluating Epoch 12  27.9% | batch:        48 of       172\t|\tloss: 0.25235\n",
      "Evaluating Epoch 12  28.5% | batch:        49 of       172\t|\tloss: 1.78718\n",
      "Evaluating Epoch 12  29.1% | batch:        50 of       172\t|\tloss: 0.725586\n",
      "Evaluating Epoch 12  29.7% | batch:        51 of       172\t|\tloss: 0.669335\n",
      "Evaluating Epoch 12  30.2% | batch:        52 of       172\t|\tloss: 0.757094\n",
      "Evaluating Epoch 12  30.8% | batch:        53 of       172\t|\tloss: 1.40816\n",
      "Evaluating Epoch 12  31.4% | batch:        54 of       172\t|\tloss: 0.852715\n",
      "Evaluating Epoch 12  32.0% | batch:        55 of       172\t|\tloss: 1.15842\n",
      "Evaluating Epoch 12  32.6% | batch:        56 of       172\t|\tloss: 1.72564\n",
      "Evaluating Epoch 12  33.1% | batch:        57 of       172\t|\tloss: 1.61596\n",
      "Evaluating Epoch 12  33.7% | batch:        58 of       172\t|\tloss: 1.09109\n",
      "Evaluating Epoch 12  34.3% | batch:        59 of       172\t|\tloss: 1.36037\n",
      "Evaluating Epoch 12  34.9% | batch:        60 of       172\t|\tloss: 0.639252\n",
      "Evaluating Epoch 12  35.5% | batch:        61 of       172\t|\tloss: 2.17459\n",
      "Evaluating Epoch 12  36.0% | batch:        62 of       172\t|\tloss: 0.588528\n",
      "Evaluating Epoch 12  36.6% | batch:        63 of       172\t|\tloss: 1.50759\n",
      "Evaluating Epoch 12  37.2% | batch:        64 of       172\t|\tloss: 0.858402\n",
      "Evaluating Epoch 12  37.8% | batch:        65 of       172\t|\tloss: 1.19073\n",
      "Evaluating Epoch 12  38.4% | batch:        66 of       172\t|\tloss: 1.89295\n",
      "Evaluating Epoch 12  39.0% | batch:        67 of       172\t|\tloss: 0.594732\n",
      "Evaluating Epoch 12  39.5% | batch:        68 of       172\t|\tloss: 2.06965\n",
      "Evaluating Epoch 12  40.1% | batch:        69 of       172\t|\tloss: 2.08667\n",
      "Evaluating Epoch 12  40.7% | batch:        70 of       172\t|\tloss: 0.666166\n",
      "Evaluating Epoch 12  41.3% | batch:        71 of       172\t|\tloss: 1.09571\n",
      "Evaluating Epoch 12  41.9% | batch:        72 of       172\t|\tloss: 0.947955\n",
      "Evaluating Epoch 12  42.4% | batch:        73 of       172\t|\tloss: 1.29136\n",
      "Evaluating Epoch 12  43.0% | batch:        74 of       172\t|\tloss: 0.153028\n",
      "Evaluating Epoch 12  43.6% | batch:        75 of       172\t|\tloss: 0.27765\n",
      "Evaluating Epoch 12  44.2% | batch:        76 of       172\t|\tloss: 0.194551\n",
      "Evaluating Epoch 12  44.8% | batch:        77 of       172\t|\tloss: 0.277301\n",
      "Evaluating Epoch 12  45.3% | batch:        78 of       172\t|\tloss: 0.280091\n",
      "Evaluating Epoch 12  45.9% | batch:        79 of       172\t|\tloss: 0.235729\n",
      "Evaluating Epoch 12  46.5% | batch:        80 of       172\t|\tloss: 0.369873\n",
      "Evaluating Epoch 12  47.1% | batch:        81 of       172\t|\tloss: 0.29737\n",
      "Evaluating Epoch 12  47.7% | batch:        82 of       172\t|\tloss: 0.266527\n",
      "Evaluating Epoch 12  48.3% | batch:        83 of       172\t|\tloss: 0.433005\n",
      "Evaluating Epoch 12  48.8% | batch:        84 of       172\t|\tloss: 0.647121\n",
      "Evaluating Epoch 12  49.4% | batch:        85 of       172\t|\tloss: 0.925318\n",
      "Evaluating Epoch 12  50.0% | batch:        86 of       172\t|\tloss: 0.662282\n",
      "Evaluating Epoch 12  50.6% | batch:        87 of       172\t|\tloss: 0.395313\n",
      "Evaluating Epoch 12  51.2% | batch:        88 of       172\t|\tloss: 0.873363\n",
      "Evaluating Epoch 12  51.7% | batch:        89 of       172\t|\tloss: 1.22457\n",
      "Evaluating Epoch 12  52.3% | batch:        90 of       172\t|\tloss: 0.693335\n",
      "Evaluating Epoch 12  52.9% | batch:        91 of       172\t|\tloss: 1.10175\n",
      "Evaluating Epoch 12  53.5% | batch:        92 of       172\t|\tloss: 1.25798\n",
      "Evaluating Epoch 12  54.1% | batch:        93 of       172\t|\tloss: 0.857534\n",
      "Evaluating Epoch 12  54.7% | batch:        94 of       172\t|\tloss: 0.983567\n",
      "Evaluating Epoch 12  55.2% | batch:        95 of       172\t|\tloss: 1.00855\n",
      "Evaluating Epoch 12  55.8% | batch:        96 of       172\t|\tloss: 1.05422\n",
      "Evaluating Epoch 12  56.4% | batch:        97 of       172\t|\tloss: 0.606565\n",
      "Evaluating Epoch 12  57.0% | batch:        98 of       172\t|\tloss: 1.11303\n",
      "Evaluating Epoch 12  57.6% | batch:        99 of       172\t|\tloss: 1.28881\n",
      "Evaluating Epoch 12  58.1% | batch:       100 of       172\t|\tloss: 0.536058\n",
      "Evaluating Epoch 12  58.7% | batch:       101 of       172\t|\tloss: 0.482437\n",
      "Evaluating Epoch 12  59.3% | batch:       102 of       172\t|\tloss: 1.20336\n",
      "Evaluating Epoch 12  59.9% | batch:       103 of       172\t|\tloss: 1.02084\n",
      "Evaluating Epoch 12  60.5% | batch:       104 of       172\t|\tloss: 0.600719\n",
      "Evaluating Epoch 12  61.0% | batch:       105 of       172\t|\tloss: 1.03821\n",
      "Evaluating Epoch 12  61.6% | batch:       106 of       172\t|\tloss: 1.42873\n",
      "Evaluating Epoch 12  62.2% | batch:       107 of       172\t|\tloss: 0.901821\n",
      "Evaluating Epoch 12  62.8% | batch:       108 of       172\t|\tloss: 0.65822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 12  63.4% | batch:       109 of       172\t|\tloss: 1.30755\n",
      "Evaluating Epoch 12  64.0% | batch:       110 of       172\t|\tloss: 1.2735\n",
      "Evaluating Epoch 12  64.5% | batch:       111 of       172\t|\tloss: 0.581005\n",
      "Evaluating Epoch 12  65.1% | batch:       112 of       172\t|\tloss: 0.5897\n",
      "Evaluating Epoch 12  65.7% | batch:       113 of       172\t|\tloss: 0.801959\n",
      "Evaluating Epoch 12  66.3% | batch:       114 of       172\t|\tloss: 1.11762\n",
      "Evaluating Epoch 12  66.9% | batch:       115 of       172\t|\tloss: 0.610693\n",
      "Evaluating Epoch 12  67.4% | batch:       116 of       172\t|\tloss: 0.519701\n",
      "Evaluating Epoch 12  68.0% | batch:       117 of       172\t|\tloss: 0.546168\n",
      "Evaluating Epoch 12  68.6% | batch:       118 of       172\t|\tloss: 0.337756\n",
      "Evaluating Epoch 12  69.2% | batch:       119 of       172\t|\tloss: 0.53538\n",
      "Evaluating Epoch 12  69.8% | batch:       120 of       172\t|\tloss: 0.413607\n",
      "Evaluating Epoch 12  70.3% | batch:       121 of       172\t|\tloss: 1.25224\n",
      "Evaluating Epoch 12  70.9% | batch:       122 of       172\t|\tloss: 0.888852\n",
      "Evaluating Epoch 12  71.5% | batch:       123 of       172\t|\tloss: 1.7208\n",
      "Evaluating Epoch 12  72.1% | batch:       124 of       172\t|\tloss: 8.97787\n",
      "Evaluating Epoch 12  72.7% | batch:       125 of       172\t|\tloss: 0.944944\n",
      "Evaluating Epoch 12  73.3% | batch:       126 of       172\t|\tloss: 0.708754\n",
      "Evaluating Epoch 12  73.8% | batch:       127 of       172\t|\tloss: 0.596155\n",
      "Evaluating Epoch 12  74.4% | batch:       128 of       172\t|\tloss: 1.01424\n",
      "Evaluating Epoch 12  75.0% | batch:       129 of       172\t|\tloss: 0.414183\n",
      "Evaluating Epoch 12  75.6% | batch:       130 of       172\t|\tloss: 0.457467\n",
      "Evaluating Epoch 12  76.2% | batch:       131 of       172\t|\tloss: 0.979343\n",
      "Evaluating Epoch 12  76.7% | batch:       132 of       172\t|\tloss: 0.527177\n",
      "Evaluating Epoch 12  77.3% | batch:       133 of       172\t|\tloss: 0.239158\n",
      "Evaluating Epoch 12  77.9% | batch:       134 of       172\t|\tloss: 0.510665\n",
      "Evaluating Epoch 12  78.5% | batch:       135 of       172\t|\tloss: 0.242528\n",
      "Evaluating Epoch 12  79.1% | batch:       136 of       172\t|\tloss: 0.332338\n",
      "Evaluating Epoch 12  79.7% | batch:       137 of       172\t|\tloss: 0.197983\n",
      "Evaluating Epoch 12  80.2% | batch:       138 of       172\t|\tloss: 0.578791\n",
      "Evaluating Epoch 12  80.8% | batch:       139 of       172\t|\tloss: 0.393773\n",
      "Evaluating Epoch 12  81.4% | batch:       140 of       172\t|\tloss: 0.428175\n",
      "Evaluating Epoch 12  82.0% | batch:       141 of       172\t|\tloss: 0.242871\n",
      "Evaluating Epoch 12  82.6% | batch:       142 of       172\t|\tloss: 0.445567\n",
      "Evaluating Epoch 12  83.1% | batch:       143 of       172\t|\tloss: 0.228219\n",
      "Evaluating Epoch 12  83.7% | batch:       144 of       172\t|\tloss: 0.479955\n",
      "Evaluating Epoch 12  84.3% | batch:       145 of       172\t|\tloss: 0.207295\n",
      "Evaluating Epoch 12  84.9% | batch:       146 of       172\t|\tloss: 0.42926\n",
      "Evaluating Epoch 12  85.5% | batch:       147 of       172\t|\tloss: 0.174907\n",
      "Evaluating Epoch 12  86.0% | batch:       148 of       172\t|\tloss: 0.37829\n",
      "Evaluating Epoch 12  86.6% | batch:       149 of       172\t|\tloss: 0.242141\n",
      "Evaluating Epoch 12  87.2% | batch:       150 of       172\t|\tloss: 0.601373\n",
      "Evaluating Epoch 12  87.8% | batch:       151 of       172\t|\tloss: 1.37588\n",
      "Evaluating Epoch 12  88.4% | batch:       152 of       172\t|\tloss: 0.725646\n",
      "Evaluating Epoch 12  89.0% | batch:       153 of       172\t|\tloss: 0.851408\n",
      "Evaluating Epoch 12  89.5% | batch:       154 of       172\t|\tloss: 1.33533\n",
      "Evaluating Epoch 12  90.1% | batch:       155 of       172\t|\tloss: 0.643444\n",
      "Evaluating Epoch 12  90.7% | batch:       156 of       172\t|\tloss: 1.35694\n",
      "Evaluating Epoch 12  91.3% | batch:       157 of       172\t|\tloss: 1.42419\n",
      "Evaluating Epoch 12  91.9% | batch:       158 of       172\t|\tloss: 0.548181\n",
      "Evaluating Epoch 12  92.4% | batch:       159 of       172\t|\tloss: 1.91236\n",
      "Evaluating Epoch 12  93.0% | batch:       160 of       172\t|\tloss: 1.61064\n",
      "Evaluating Epoch 12  93.6% | batch:       161 of       172\t|\tloss: 2.23775\n",
      "Evaluating Epoch 12  94.2% | batch:       162 of       172\t|\tloss: 1.48569\n",
      "Evaluating Epoch 12  94.8% | batch:       163 of       172\t|\tloss: 0.725152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:05:22,479 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.326054811477661 seconds\n",
      "\n",
      "2023-05-25 22:05:22,480 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 3.9558035960564246 seconds\n",
      "2023-05-25 22:05:22,480 | INFO : Avg batch val. time: 0.02299885811660712 seconds\n",
      "2023-05-25 22:05:22,481 | INFO : Avg sample val. time: 0.00018016138798817802 seconds\n",
      "2023-05-25 22:05:22,481 | INFO : Epoch 12 Validation Summary: epoch: 12.000000 | loss: 1.461891 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 12  95.3% | batch:       164 of       172\t|\tloss: 1.29061\n",
      "Evaluating Epoch 12  95.9% | batch:       165 of       172\t|\tloss: 1.16154\n",
      "Evaluating Epoch 12  96.5% | batch:       166 of       172\t|\tloss: 0.417324\n",
      "Evaluating Epoch 12  97.1% | batch:       167 of       172\t|\tloss: 1.65499\n",
      "Evaluating Epoch 12  97.7% | batch:       168 of       172\t|\tloss: 1.01889\n",
      "Evaluating Epoch 12  98.3% | batch:       169 of       172\t|\tloss: 0.805382\n",
      "Evaluating Epoch 12  98.8% | batch:       170 of       172\t|\tloss: 1.59979\n",
      "Evaluating Epoch 12  99.4% | batch:       171 of       172\t|\tloss: 1.12248\n",
      "\n",
      "Training Epoch 13   0.0% | batch:         0 of       686\t|\tloss: 6.48244\n",
      "Training Epoch 13   0.1% | batch:         1 of       686\t|\tloss: 6.96034\n",
      "Training Epoch 13   0.3% | batch:         2 of       686\t|\tloss: 5.33333\n",
      "Training Epoch 13   0.4% | batch:         3 of       686\t|\tloss: 5.19923\n",
      "Training Epoch 13   0.6% | batch:         4 of       686\t|\tloss: 4.13284\n",
      "Training Epoch 13   0.7% | batch:         5 of       686\t|\tloss: 5.38899\n",
      "Training Epoch 13   0.9% | batch:         6 of       686\t|\tloss: 5.60661\n",
      "Training Epoch 13   1.0% | batch:         7 of       686\t|\tloss: 3.90162\n",
      "Training Epoch 13   1.2% | batch:         8 of       686\t|\tloss: 5.12791\n",
      "Training Epoch 13   1.3% | batch:         9 of       686\t|\tloss: 7.09853\n",
      "Training Epoch 13   1.5% | batch:        10 of       686\t|\tloss: 5.0247\n",
      "Training Epoch 13   1.6% | batch:        11 of       686\t|\tloss: 6.91572\n",
      "Training Epoch 13   1.7% | batch:        12 of       686\t|\tloss: 6.30687\n",
      "Training Epoch 13   1.9% | batch:        13 of       686\t|\tloss: 4.30582\n",
      "Training Epoch 13   2.0% | batch:        14 of       686\t|\tloss: 7.88699\n",
      "Training Epoch 13   2.2% | batch:        15 of       686\t|\tloss: 4.97904\n",
      "Training Epoch 13   2.3% | batch:        16 of       686\t|\tloss: 4.01087\n",
      "Training Epoch 13   2.5% | batch:        17 of       686\t|\tloss: 5.2235\n",
      "Training Epoch 13   2.6% | batch:        18 of       686\t|\tloss: 5.73058\n",
      "Training Epoch 13   2.8% | batch:        19 of       686\t|\tloss: 5.21276\n",
      "Training Epoch 13   2.9% | batch:        20 of       686\t|\tloss: 4.38466\n",
      "Training Epoch 13   3.1% | batch:        21 of       686\t|\tloss: 4.53098\n",
      "Training Epoch 13   3.2% | batch:        22 of       686\t|\tloss: 4.92844\n",
      "Training Epoch 13   3.4% | batch:        23 of       686\t|\tloss: 4.42899\n",
      "Training Epoch 13   3.5% | batch:        24 of       686\t|\tloss: 5.87636\n",
      "Training Epoch 13   3.6% | batch:        25 of       686\t|\tloss: 5.20441\n",
      "Training Epoch 13   3.8% | batch:        26 of       686\t|\tloss: 5.26852\n",
      "Training Epoch 13   3.9% | batch:        27 of       686\t|\tloss: 3.74139\n",
      "Training Epoch 13   4.1% | batch:        28 of       686\t|\tloss: 5.7239\n",
      "Training Epoch 13   4.2% | batch:        29 of       686\t|\tloss: 3.96544\n",
      "Training Epoch 13   4.4% | batch:        30 of       686\t|\tloss: 5.7725\n",
      "Training Epoch 13   4.5% | batch:        31 of       686\t|\tloss: 5.58973\n",
      "Training Epoch 13   4.7% | batch:        32 of       686\t|\tloss: 4.43625\n",
      "Training Epoch 13   4.8% | batch:        33 of       686\t|\tloss: 6.43147\n",
      "Training Epoch 13   5.0% | batch:        34 of       686\t|\tloss: 4.71348\n",
      "Training Epoch 13   5.1% | batch:        35 of       686\t|\tloss: 5.49302\n",
      "Training Epoch 13   5.2% | batch:        36 of       686\t|\tloss: 5.54278\n",
      "Training Epoch 13   5.4% | batch:        37 of       686\t|\tloss: 4.64225\n",
      "Training Epoch 13   5.5% | batch:        38 of       686\t|\tloss: 4.86264\n",
      "Training Epoch 13   5.7% | batch:        39 of       686\t|\tloss: 3.80793\n",
      "Training Epoch 13   5.8% | batch:        40 of       686\t|\tloss: 5.68851\n",
      "Training Epoch 13   6.0% | batch:        41 of       686\t|\tloss: 4.52845\n",
      "Training Epoch 13   6.1% | batch:        42 of       686\t|\tloss: 4.0045\n",
      "Training Epoch 13   6.3% | batch:        43 of       686\t|\tloss: 4.78568\n",
      "Training Epoch 13   6.4% | batch:        44 of       686\t|\tloss: 4.58342\n",
      "Training Epoch 13   6.6% | batch:        45 of       686\t|\tloss: 4.23965\n",
      "Training Epoch 13   6.7% | batch:        46 of       686\t|\tloss: 4.77904\n",
      "Training Epoch 13   6.9% | batch:        47 of       686\t|\tloss: 4.62027\n",
      "Training Epoch 13   7.0% | batch:        48 of       686\t|\tloss: 6.32795\n",
      "Training Epoch 13   7.1% | batch:        49 of       686\t|\tloss: 6.15462\n",
      "Training Epoch 13   7.3% | batch:        50 of       686\t|\tloss: 4.54608\n",
      "Training Epoch 13   7.4% | batch:        51 of       686\t|\tloss: 4.00836\n",
      "Training Epoch 13   7.6% | batch:        52 of       686\t|\tloss: 5.06934\n",
      "Training Epoch 13   7.7% | batch:        53 of       686\t|\tloss: 4.12292\n",
      "Training Epoch 13   7.9% | batch:        54 of       686\t|\tloss: 6.00817\n",
      "Training Epoch 13   8.0% | batch:        55 of       686\t|\tloss: 4.20379\n",
      "Training Epoch 13   8.2% | batch:        56 of       686\t|\tloss: 4.41628\n",
      "Training Epoch 13   8.3% | batch:        57 of       686\t|\tloss: 4.96751\n",
      "Training Epoch 13   8.5% | batch:        58 of       686\t|\tloss: 6.51338\n",
      "Training Epoch 13   8.6% | batch:        59 of       686\t|\tloss: 5.34817\n",
      "Training Epoch 13   8.7% | batch:        60 of       686\t|\tloss: 5.08996\n",
      "Training Epoch 13   8.9% | batch:        61 of       686\t|\tloss: 6.39554\n",
      "Training Epoch 13   9.0% | batch:        62 of       686\t|\tloss: 4.03598\n",
      "Training Epoch 13   9.2% | batch:        63 of       686\t|\tloss: 5.19423\n",
      "Training Epoch 13   9.3% | batch:        64 of       686\t|\tloss: 5.47044\n",
      "Training Epoch 13   9.5% | batch:        65 of       686\t|\tloss: 3.63858\n",
      "Training Epoch 13   9.6% | batch:        66 of       686\t|\tloss: 4.18066\n",
      "Training Epoch 13   9.8% | batch:        67 of       686\t|\tloss: 5.61371\n",
      "Training Epoch 13   9.9% | batch:        68 of       686\t|\tloss: 5.58474\n",
      "Training Epoch 13  10.1% | batch:        69 of       686\t|\tloss: 5.92906\n",
      "Training Epoch 13  10.2% | batch:        70 of       686\t|\tloss: 6.85981\n",
      "Training Epoch 13  10.3% | batch:        71 of       686\t|\tloss: 5.47421\n",
      "Training Epoch 13  10.5% | batch:        72 of       686\t|\tloss: 4.98907\n",
      "Training Epoch 13  10.6% | batch:        73 of       686\t|\tloss: 4.96082\n",
      "Training Epoch 13  10.8% | batch:        74 of       686\t|\tloss: 6.04002\n",
      "Training Epoch 13  10.9% | batch:        75 of       686\t|\tloss: 4.73984\n",
      "Training Epoch 13  11.1% | batch:        76 of       686\t|\tloss: 5.06242\n",
      "Training Epoch 13  11.2% | batch:        77 of       686\t|\tloss: 3.49021\n",
      "Training Epoch 13  11.4% | batch:        78 of       686\t|\tloss: 4.8649\n",
      "Training Epoch 13  11.5% | batch:        79 of       686\t|\tloss: 6.81505\n",
      "Training Epoch 13  11.7% | batch:        80 of       686\t|\tloss: 4.3862\n",
      "Training Epoch 13  11.8% | batch:        81 of       686\t|\tloss: 4.4205\n",
      "Training Epoch 13  12.0% | batch:        82 of       686\t|\tloss: 5.37424\n",
      "Training Epoch 13  12.1% | batch:        83 of       686\t|\tloss: 5.49199\n",
      "Training Epoch 13  12.2% | batch:        84 of       686\t|\tloss: 5.28549\n",
      "Training Epoch 13  12.4% | batch:        85 of       686\t|\tloss: 5.61945\n",
      "Training Epoch 13  12.5% | batch:        86 of       686\t|\tloss: 5.81556\n",
      "Training Epoch 13  12.7% | batch:        87 of       686\t|\tloss: 4.06243\n",
      "Training Epoch 13  12.8% | batch:        88 of       686\t|\tloss: 5.93024\n",
      "Training Epoch 13  13.0% | batch:        89 of       686\t|\tloss: 5.82626\n",
      "Training Epoch 13  13.1% | batch:        90 of       686\t|\tloss: 3.73722\n",
      "Training Epoch 13  13.3% | batch:        91 of       686\t|\tloss: 6.24448\n",
      "Training Epoch 13  13.4% | batch:        92 of       686\t|\tloss: 5.06677\n",
      "Training Epoch 13  13.6% | batch:        93 of       686\t|\tloss: 4.05078\n",
      "Training Epoch 13  13.7% | batch:        94 of       686\t|\tloss: 4.99989\n",
      "Training Epoch 13  13.8% | batch:        95 of       686\t|\tloss: 5.55474\n",
      "Training Epoch 13  14.0% | batch:        96 of       686\t|\tloss: 6.16645\n",
      "Training Epoch 13  14.1% | batch:        97 of       686\t|\tloss: 3.90491\n",
      "Training Epoch 13  14.3% | batch:        98 of       686\t|\tloss: 4.61616\n",
      "Training Epoch 13  14.4% | batch:        99 of       686\t|\tloss: 4.50852\n",
      "Training Epoch 13  14.6% | batch:       100 of       686\t|\tloss: 8.57669\n",
      "Training Epoch 13  14.7% | batch:       101 of       686\t|\tloss: 4.28605\n",
      "Training Epoch 13  14.9% | batch:       102 of       686\t|\tloss: 5.40856\n",
      "Training Epoch 13  15.0% | batch:       103 of       686\t|\tloss: 4.95334\n",
      "Training Epoch 13  15.2% | batch:       104 of       686\t|\tloss: 5.24865\n",
      "Training Epoch 13  15.3% | batch:       105 of       686\t|\tloss: 5.08453\n",
      "Training Epoch 13  15.5% | batch:       106 of       686\t|\tloss: 4.8173\n",
      "Training Epoch 13  15.6% | batch:       107 of       686\t|\tloss: 4.55777\n",
      "Training Epoch 13  15.7% | batch:       108 of       686\t|\tloss: 3.88969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 13  15.9% | batch:       109 of       686\t|\tloss: 4.03969\n",
      "Training Epoch 13  16.0% | batch:       110 of       686\t|\tloss: 4.29392\n",
      "Training Epoch 13  16.2% | batch:       111 of       686\t|\tloss: 4.82716\n",
      "Training Epoch 13  16.3% | batch:       112 of       686\t|\tloss: 4.80095\n",
      "Training Epoch 13  16.5% | batch:       113 of       686\t|\tloss: 5.07833\n",
      "Training Epoch 13  16.6% | batch:       114 of       686\t|\tloss: 4.41946\n",
      "Training Epoch 13  16.8% | batch:       115 of       686\t|\tloss: 6.00727\n",
      "Training Epoch 13  16.9% | batch:       116 of       686\t|\tloss: 5.92722\n",
      "Training Epoch 13  17.1% | batch:       117 of       686\t|\tloss: 5.90785\n",
      "Training Epoch 13  17.2% | batch:       118 of       686\t|\tloss: 6.99561\n",
      "Training Epoch 13  17.3% | batch:       119 of       686\t|\tloss: 6.9648\n",
      "Training Epoch 13  17.5% | batch:       120 of       686\t|\tloss: 4.87895\n",
      "Training Epoch 13  17.6% | batch:       121 of       686\t|\tloss: 3.85243\n",
      "Training Epoch 13  17.8% | batch:       122 of       686\t|\tloss: 5.82431\n",
      "Training Epoch 13  17.9% | batch:       123 of       686\t|\tloss: 7.1891\n",
      "Training Epoch 13  18.1% | batch:       124 of       686\t|\tloss: 5.24005\n",
      "Training Epoch 13  18.2% | batch:       125 of       686\t|\tloss: 4.11014\n",
      "Training Epoch 13  18.4% | batch:       126 of       686\t|\tloss: 4.73065\n",
      "Training Epoch 13  18.5% | batch:       127 of       686\t|\tloss: 5.18407\n",
      "Training Epoch 13  18.7% | batch:       128 of       686\t|\tloss: 6.50331\n",
      "Training Epoch 13  18.8% | batch:       129 of       686\t|\tloss: 6.67406\n",
      "Training Epoch 13  19.0% | batch:       130 of       686\t|\tloss: 3.49322\n",
      "Training Epoch 13  19.1% | batch:       131 of       686\t|\tloss: 4.72419\n",
      "Training Epoch 13  19.2% | batch:       132 of       686\t|\tloss: 4.30689\n",
      "Training Epoch 13  19.4% | batch:       133 of       686\t|\tloss: 3.83993\n",
      "Training Epoch 13  19.5% | batch:       134 of       686\t|\tloss: 6.03805\n",
      "Training Epoch 13  19.7% | batch:       135 of       686\t|\tloss: 4.60676\n",
      "Training Epoch 13  19.8% | batch:       136 of       686\t|\tloss: 4.46402\n",
      "Training Epoch 13  20.0% | batch:       137 of       686\t|\tloss: 4.88173\n",
      "Training Epoch 13  20.1% | batch:       138 of       686\t|\tloss: 3.63918\n",
      "Training Epoch 13  20.3% | batch:       139 of       686\t|\tloss: 4.91429\n",
      "Training Epoch 13  20.4% | batch:       140 of       686\t|\tloss: 3.64254\n",
      "Training Epoch 13  20.6% | batch:       141 of       686\t|\tloss: 6.08761\n",
      "Training Epoch 13  20.7% | batch:       142 of       686\t|\tloss: 6.34578\n",
      "Training Epoch 13  20.8% | batch:       143 of       686\t|\tloss: 4.99405\n",
      "Training Epoch 13  21.0% | batch:       144 of       686\t|\tloss: 4.74918\n",
      "Training Epoch 13  21.1% | batch:       145 of       686\t|\tloss: 3.49905\n",
      "Training Epoch 13  21.3% | batch:       146 of       686\t|\tloss: 3.41444\n",
      "Training Epoch 13  21.4% | batch:       147 of       686\t|\tloss: 3.78027\n",
      "Training Epoch 13  21.6% | batch:       148 of       686\t|\tloss: 8.22643\n",
      "Training Epoch 13  21.7% | batch:       149 of       686\t|\tloss: 5.24668\n",
      "Training Epoch 13  21.9% | batch:       150 of       686\t|\tloss: 4.34455\n",
      "Training Epoch 13  22.0% | batch:       151 of       686\t|\tloss: 5.39983\n",
      "Training Epoch 13  22.2% | batch:       152 of       686\t|\tloss: 4.43194\n",
      "Training Epoch 13  22.3% | batch:       153 of       686\t|\tloss: 5.42698\n",
      "Training Epoch 13  22.4% | batch:       154 of       686\t|\tloss: 4.66464\n",
      "Training Epoch 13  22.6% | batch:       155 of       686\t|\tloss: 5.30592\n",
      "Training Epoch 13  22.7% | batch:       156 of       686\t|\tloss: 3.77057\n",
      "Training Epoch 13  22.9% | batch:       157 of       686\t|\tloss: 4.03228\n",
      "Training Epoch 13  23.0% | batch:       158 of       686\t|\tloss: 7.86192\n",
      "Training Epoch 13  23.2% | batch:       159 of       686\t|\tloss: 6.21038\n",
      "Training Epoch 13  23.3% | batch:       160 of       686\t|\tloss: 6.44825\n",
      "Training Epoch 13  23.5% | batch:       161 of       686\t|\tloss: 3.86816\n",
      "Training Epoch 13  23.6% | batch:       162 of       686\t|\tloss: 5.64505\n",
      "Training Epoch 13  23.8% | batch:       163 of       686\t|\tloss: 6.51843\n",
      "Training Epoch 13  23.9% | batch:       164 of       686\t|\tloss: 5.32542\n",
      "Training Epoch 13  24.1% | batch:       165 of       686\t|\tloss: 6.53327\n",
      "Training Epoch 13  24.2% | batch:       166 of       686\t|\tloss: 3.94427\n",
      "Training Epoch 13  24.3% | batch:       167 of       686\t|\tloss: 5.51241\n",
      "Training Epoch 13  24.5% | batch:       168 of       686\t|\tloss: 5.95286\n",
      "Training Epoch 13  24.6% | batch:       169 of       686\t|\tloss: 5.36248\n",
      "Training Epoch 13  24.8% | batch:       170 of       686\t|\tloss: 4.2169\n",
      "Training Epoch 13  24.9% | batch:       171 of       686\t|\tloss: 4.98429\n",
      "Training Epoch 13  25.1% | batch:       172 of       686\t|\tloss: 9.20855\n",
      "Training Epoch 13  25.2% | batch:       173 of       686\t|\tloss: 4.41472\n",
      "Training Epoch 13  25.4% | batch:       174 of       686\t|\tloss: 7.27527\n",
      "Training Epoch 13  25.5% | batch:       175 of       686\t|\tloss: 4.83741\n",
      "Training Epoch 13  25.7% | batch:       176 of       686\t|\tloss: 6.24959\n",
      "Training Epoch 13  25.8% | batch:       177 of       686\t|\tloss: 5.68899\n",
      "Training Epoch 13  25.9% | batch:       178 of       686\t|\tloss: 4.14991\n",
      "Training Epoch 13  26.1% | batch:       179 of       686\t|\tloss: 4.81321\n",
      "Training Epoch 13  26.2% | batch:       180 of       686\t|\tloss: 5.07334\n",
      "Training Epoch 13  26.4% | batch:       181 of       686\t|\tloss: 4.55919\n",
      "Training Epoch 13  26.5% | batch:       182 of       686\t|\tloss: 5.80236\n",
      "Training Epoch 13  26.7% | batch:       183 of       686\t|\tloss: 4.85794\n",
      "Training Epoch 13  26.8% | batch:       184 of       686\t|\tloss: 5.48437\n",
      "Training Epoch 13  27.0% | batch:       185 of       686\t|\tloss: 6.18919\n",
      "Training Epoch 13  27.1% | batch:       186 of       686\t|\tloss: 5.19779\n",
      "Training Epoch 13  27.3% | batch:       187 of       686\t|\tloss: 3.91158\n",
      "Training Epoch 13  27.4% | batch:       188 of       686\t|\tloss: 6.80857\n",
      "Training Epoch 13  27.6% | batch:       189 of       686\t|\tloss: 4.99308\n",
      "Training Epoch 13  27.7% | batch:       190 of       686\t|\tloss: 8.89828\n",
      "Training Epoch 13  27.8% | batch:       191 of       686\t|\tloss: 6.43675\n",
      "Training Epoch 13  28.0% | batch:       192 of       686\t|\tloss: 6.30283\n",
      "Training Epoch 13  28.1% | batch:       193 of       686\t|\tloss: 5.12188\n",
      "Training Epoch 13  28.3% | batch:       194 of       686\t|\tloss: 4.88669\n",
      "Training Epoch 13  28.4% | batch:       195 of       686\t|\tloss: 3.87243\n",
      "Training Epoch 13  28.6% | batch:       196 of       686\t|\tloss: 4.58255\n",
      "Training Epoch 13  28.7% | batch:       197 of       686\t|\tloss: 3.50494\n",
      "Training Epoch 13  28.9% | batch:       198 of       686\t|\tloss: 3.97451\n",
      "Training Epoch 13  29.0% | batch:       199 of       686\t|\tloss: 4.39567\n",
      "Training Epoch 13  29.2% | batch:       200 of       686\t|\tloss: 4.57597\n",
      "Training Epoch 13  29.3% | batch:       201 of       686\t|\tloss: 4.09877\n",
      "Training Epoch 13  29.4% | batch:       202 of       686\t|\tloss: 4.65287\n",
      "Training Epoch 13  29.6% | batch:       203 of       686\t|\tloss: 5.70744\n",
      "Training Epoch 13  29.7% | batch:       204 of       686\t|\tloss: 5.69491\n",
      "Training Epoch 13  29.9% | batch:       205 of       686\t|\tloss: 5.35539\n",
      "Training Epoch 13  30.0% | batch:       206 of       686\t|\tloss: 6.01529\n",
      "Training Epoch 13  30.2% | batch:       207 of       686\t|\tloss: 5.53625\n",
      "Training Epoch 13  30.3% | batch:       208 of       686\t|\tloss: 6.17591\n",
      "Training Epoch 13  30.5% | batch:       209 of       686\t|\tloss: 4.49155\n",
      "Training Epoch 13  30.6% | batch:       210 of       686\t|\tloss: 3.64841\n",
      "Training Epoch 13  30.8% | batch:       211 of       686\t|\tloss: 5.01115\n",
      "Training Epoch 13  30.9% | batch:       212 of       686\t|\tloss: 5.13303\n",
      "Training Epoch 13  31.0% | batch:       213 of       686\t|\tloss: 4.59754\n",
      "Training Epoch 13  31.2% | batch:       214 of       686\t|\tloss: 5.01191\n",
      "Training Epoch 13  31.3% | batch:       215 of       686\t|\tloss: 4.14195\n",
      "Training Epoch 13  31.5% | batch:       216 of       686\t|\tloss: 4.09213\n",
      "Training Epoch 13  31.6% | batch:       217 of       686\t|\tloss: 4.40785\n",
      "Training Epoch 13  31.8% | batch:       218 of       686\t|\tloss: 4.51182\n",
      "Training Epoch 13  31.9% | batch:       219 of       686\t|\tloss: 4.33882\n",
      "Training Epoch 13  32.1% | batch:       220 of       686\t|\tloss: 6.62961\n",
      "Training Epoch 13  32.2% | batch:       221 of       686\t|\tloss: 5.10713\n",
      "Training Epoch 13  32.4% | batch:       222 of       686\t|\tloss: 4.54434\n",
      "Training Epoch 13  32.5% | batch:       223 of       686\t|\tloss: 4.58309\n",
      "Training Epoch 13  32.7% | batch:       224 of       686\t|\tloss: 4.27474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 13  32.8% | batch:       225 of       686\t|\tloss: 7.83187\n",
      "Training Epoch 13  32.9% | batch:       226 of       686\t|\tloss: 4.243\n",
      "Training Epoch 13  33.1% | batch:       227 of       686\t|\tloss: 5.47606\n",
      "Training Epoch 13  33.2% | batch:       228 of       686\t|\tloss: 8.50164\n",
      "Training Epoch 13  33.4% | batch:       229 of       686\t|\tloss: 5.30823\n",
      "Training Epoch 13  33.5% | batch:       230 of       686\t|\tloss: 4.7531\n",
      "Training Epoch 13  33.7% | batch:       231 of       686\t|\tloss: 5.74878\n",
      "Training Epoch 13  33.8% | batch:       232 of       686\t|\tloss: 4.50016\n",
      "Training Epoch 13  34.0% | batch:       233 of       686\t|\tloss: 4.30662\n",
      "Training Epoch 13  34.1% | batch:       234 of       686\t|\tloss: 4.84438\n",
      "Training Epoch 13  34.3% | batch:       235 of       686\t|\tloss: 5.58542\n",
      "Training Epoch 13  34.4% | batch:       236 of       686\t|\tloss: 4.98312\n",
      "Training Epoch 13  34.5% | batch:       237 of       686\t|\tloss: 5.92906\n",
      "Training Epoch 13  34.7% | batch:       238 of       686\t|\tloss: 5.06856\n",
      "Training Epoch 13  34.8% | batch:       239 of       686\t|\tloss: 6.03775\n",
      "Training Epoch 13  35.0% | batch:       240 of       686\t|\tloss: 5.41344\n",
      "Training Epoch 13  35.1% | batch:       241 of       686\t|\tloss: 3.8343\n",
      "Training Epoch 13  35.3% | batch:       242 of       686\t|\tloss: 5.83329\n",
      "Training Epoch 13  35.4% | batch:       243 of       686\t|\tloss: 4.82211\n",
      "Training Epoch 13  35.6% | batch:       244 of       686\t|\tloss: 5.95669\n",
      "Training Epoch 13  35.7% | batch:       245 of       686\t|\tloss: 5.25459\n",
      "Training Epoch 13  35.9% | batch:       246 of       686\t|\tloss: 4.40069\n",
      "Training Epoch 13  36.0% | batch:       247 of       686\t|\tloss: 6.21311\n",
      "Training Epoch 13  36.2% | batch:       248 of       686\t|\tloss: 3.98194\n",
      "Training Epoch 13  36.3% | batch:       249 of       686\t|\tloss: 4.67643\n",
      "Training Epoch 13  36.4% | batch:       250 of       686\t|\tloss: 4.70084\n",
      "Training Epoch 13  36.6% | batch:       251 of       686\t|\tloss: 5.51828\n",
      "Training Epoch 13  36.7% | batch:       252 of       686\t|\tloss: 3.99673\n",
      "Training Epoch 13  36.9% | batch:       253 of       686\t|\tloss: 5.40375\n",
      "Training Epoch 13  37.0% | batch:       254 of       686\t|\tloss: 4.44626\n",
      "Training Epoch 13  37.2% | batch:       255 of       686\t|\tloss: 4.3089\n",
      "Training Epoch 13  37.3% | batch:       256 of       686\t|\tloss: 5.23667\n",
      "Training Epoch 13  37.5% | batch:       257 of       686\t|\tloss: 4.30407\n",
      "Training Epoch 13  37.6% | batch:       258 of       686\t|\tloss: 4.62402\n",
      "Training Epoch 13  37.8% | batch:       259 of       686\t|\tloss: 4.67516\n",
      "Training Epoch 13  37.9% | batch:       260 of       686\t|\tloss: 4.03091\n",
      "Training Epoch 13  38.0% | batch:       261 of       686\t|\tloss: 3.94343\n",
      "Training Epoch 13  38.2% | batch:       262 of       686\t|\tloss: 3.55413\n",
      "Training Epoch 13  38.3% | batch:       263 of       686\t|\tloss: 5.3381\n",
      "Training Epoch 13  38.5% | batch:       264 of       686\t|\tloss: 5.59231\n",
      "Training Epoch 13  38.6% | batch:       265 of       686\t|\tloss: 5.44189\n",
      "Training Epoch 13  38.8% | batch:       266 of       686\t|\tloss: 5.59149\n",
      "Training Epoch 13  38.9% | batch:       267 of       686\t|\tloss: 3.81775\n",
      "Training Epoch 13  39.1% | batch:       268 of       686\t|\tloss: 4.74224\n",
      "Training Epoch 13  39.2% | batch:       269 of       686\t|\tloss: 4.96098\n",
      "Training Epoch 13  39.4% | batch:       270 of       686\t|\tloss: 5.68936\n",
      "Training Epoch 13  39.5% | batch:       271 of       686\t|\tloss: 5.31466\n",
      "Training Epoch 13  39.7% | batch:       272 of       686\t|\tloss: 7.72548\n",
      "Training Epoch 13  39.8% | batch:       273 of       686\t|\tloss: 6.42086\n",
      "Training Epoch 13  39.9% | batch:       274 of       686\t|\tloss: 5.64603\n",
      "Training Epoch 13  40.1% | batch:       275 of       686\t|\tloss: 6.53533\n",
      "Training Epoch 13  40.2% | batch:       276 of       686\t|\tloss: 5.48404\n",
      "Training Epoch 13  40.4% | batch:       277 of       686\t|\tloss: 4.30007\n",
      "Training Epoch 13  40.5% | batch:       278 of       686\t|\tloss: 6.93733\n",
      "Training Epoch 13  40.7% | batch:       279 of       686\t|\tloss: 4.24009\n",
      "Training Epoch 13  40.8% | batch:       280 of       686\t|\tloss: 5.34051\n",
      "Training Epoch 13  41.0% | batch:       281 of       686\t|\tloss: 3.85389\n",
      "Training Epoch 13  41.1% | batch:       282 of       686\t|\tloss: 5.42055\n",
      "Training Epoch 13  41.3% | batch:       283 of       686\t|\tloss: 3.38875\n",
      "Training Epoch 13  41.4% | batch:       284 of       686\t|\tloss: 4.24205\n",
      "Training Epoch 13  41.5% | batch:       285 of       686\t|\tloss: 5.264\n",
      "Training Epoch 13  41.7% | batch:       286 of       686\t|\tloss: 4.43257\n",
      "Training Epoch 13  41.8% | batch:       287 of       686\t|\tloss: 5.34332\n",
      "Training Epoch 13  42.0% | batch:       288 of       686\t|\tloss: 5.39064\n",
      "Training Epoch 13  42.1% | batch:       289 of       686\t|\tloss: 3.52188\n",
      "Training Epoch 13  42.3% | batch:       290 of       686\t|\tloss: 4.23743\n",
      "Training Epoch 13  42.4% | batch:       291 of       686\t|\tloss: 7.23259\n",
      "Training Epoch 13  42.6% | batch:       292 of       686\t|\tloss: 5.16452\n",
      "Training Epoch 13  42.7% | batch:       293 of       686\t|\tloss: 4.37275\n",
      "Training Epoch 13  42.9% | batch:       294 of       686\t|\tloss: 3.70141\n",
      "Training Epoch 13  43.0% | batch:       295 of       686\t|\tloss: 5.2627\n",
      "Training Epoch 13  43.1% | batch:       296 of       686\t|\tloss: 4.21087\n",
      "Training Epoch 13  43.3% | batch:       297 of       686\t|\tloss: 5.70379\n",
      "Training Epoch 13  43.4% | batch:       298 of       686\t|\tloss: 6.03583\n",
      "Training Epoch 13  43.6% | batch:       299 of       686\t|\tloss: 5.95832\n",
      "Training Epoch 13  43.7% | batch:       300 of       686\t|\tloss: 5.2034\n",
      "Training Epoch 13  43.9% | batch:       301 of       686\t|\tloss: 4.68562\n",
      "Training Epoch 13  44.0% | batch:       302 of       686\t|\tloss: 4.637\n",
      "Training Epoch 13  44.2% | batch:       303 of       686\t|\tloss: 5.55803\n",
      "Training Epoch 13  44.3% | batch:       304 of       686\t|\tloss: 4.40492\n",
      "Training Epoch 13  44.5% | batch:       305 of       686\t|\tloss: 5.44263\n",
      "Training Epoch 13  44.6% | batch:       306 of       686\t|\tloss: 5.34219\n",
      "Training Epoch 13  44.8% | batch:       307 of       686\t|\tloss: 6.54488\n",
      "Training Epoch 13  44.9% | batch:       308 of       686\t|\tloss: 4.09241\n",
      "Training Epoch 13  45.0% | batch:       309 of       686\t|\tloss: 6.02057\n",
      "Training Epoch 13  45.2% | batch:       310 of       686\t|\tloss: 6.34254\n",
      "Training Epoch 13  45.3% | batch:       311 of       686\t|\tloss: 4.37868\n",
      "Training Epoch 13  45.5% | batch:       312 of       686\t|\tloss: 5.79602\n",
      "Training Epoch 13  45.6% | batch:       313 of       686\t|\tloss: 4.68763\n",
      "Training Epoch 13  45.8% | batch:       314 of       686\t|\tloss: 4.72804\n",
      "Training Epoch 13  45.9% | batch:       315 of       686\t|\tloss: 4.65425\n",
      "Training Epoch 13  46.1% | batch:       316 of       686\t|\tloss: 4.97932\n",
      "Training Epoch 13  46.2% | batch:       317 of       686\t|\tloss: 4.74483\n",
      "Training Epoch 13  46.4% | batch:       318 of       686\t|\tloss: 4.59757\n",
      "Training Epoch 13  46.5% | batch:       319 of       686\t|\tloss: 5.33293\n",
      "Training Epoch 13  46.6% | batch:       320 of       686\t|\tloss: 3.26966\n",
      "Training Epoch 13  46.8% | batch:       321 of       686\t|\tloss: 5.00227\n",
      "Training Epoch 13  46.9% | batch:       322 of       686\t|\tloss: 5.39878\n",
      "Training Epoch 13  47.1% | batch:       323 of       686\t|\tloss: 4.84368\n",
      "Training Epoch 13  47.2% | batch:       324 of       686\t|\tloss: 4.71015\n",
      "Training Epoch 13  47.4% | batch:       325 of       686\t|\tloss: 4.27761\n",
      "Training Epoch 13  47.5% | batch:       326 of       686\t|\tloss: 4.29057\n",
      "Training Epoch 13  47.7% | batch:       327 of       686\t|\tloss: 6.44857\n",
      "Training Epoch 13  47.8% | batch:       328 of       686\t|\tloss: 8.0001\n",
      "Training Epoch 13  48.0% | batch:       329 of       686\t|\tloss: 5.07666\n",
      "Training Epoch 13  48.1% | batch:       330 of       686\t|\tloss: 6.43582\n",
      "Training Epoch 13  48.3% | batch:       331 of       686\t|\tloss: 4.91569\n",
      "Training Epoch 13  48.4% | batch:       332 of       686\t|\tloss: 4.39558\n",
      "Training Epoch 13  48.5% | batch:       333 of       686\t|\tloss: 8.22959\n",
      "Training Epoch 13  48.7% | batch:       334 of       686\t|\tloss: 6.34571\n",
      "Training Epoch 13  48.8% | batch:       335 of       686\t|\tloss: 5.30186\n",
      "Training Epoch 13  49.0% | batch:       336 of       686\t|\tloss: 6.10585\n",
      "Training Epoch 13  49.1% | batch:       337 of       686\t|\tloss: 3.58335\n",
      "Training Epoch 13  49.3% | batch:       338 of       686\t|\tloss: 4.93413\n",
      "Training Epoch 13  49.4% | batch:       339 of       686\t|\tloss: 4.34113\n",
      "Training Epoch 13  49.6% | batch:       340 of       686\t|\tloss: 4.83365\n",
      "Training Epoch 13  49.7% | batch:       341 of       686\t|\tloss: 3.89643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 13  49.9% | batch:       342 of       686\t|\tloss: 4.15854\n",
      "Training Epoch 13  50.0% | batch:       343 of       686\t|\tloss: 3.8735\n",
      "Training Epoch 13  50.1% | batch:       344 of       686\t|\tloss: 4.94834\n",
      "Training Epoch 13  50.3% | batch:       345 of       686\t|\tloss: 6.72104\n",
      "Training Epoch 13  50.4% | batch:       346 of       686\t|\tloss: 5.3429\n",
      "Training Epoch 13  50.6% | batch:       347 of       686\t|\tloss: 5.34334\n",
      "Training Epoch 13  50.7% | batch:       348 of       686\t|\tloss: 5.20801\n",
      "Training Epoch 13  50.9% | batch:       349 of       686\t|\tloss: 4.74289\n",
      "Training Epoch 13  51.0% | batch:       350 of       686\t|\tloss: 8.6045\n",
      "Training Epoch 13  51.2% | batch:       351 of       686\t|\tloss: 4.97602\n",
      "Training Epoch 13  51.3% | batch:       352 of       686\t|\tloss: 4.035\n",
      "Training Epoch 13  51.5% | batch:       353 of       686\t|\tloss: 5.2605\n",
      "Training Epoch 13  51.6% | batch:       354 of       686\t|\tloss: 5.83968\n",
      "Training Epoch 13  51.7% | batch:       355 of       686\t|\tloss: 6.83118\n",
      "Training Epoch 13  51.9% | batch:       356 of       686\t|\tloss: 4.36384\n",
      "Training Epoch 13  52.0% | batch:       357 of       686\t|\tloss: 5.11379\n",
      "Training Epoch 13  52.2% | batch:       358 of       686\t|\tloss: 6.07004\n",
      "Training Epoch 13  52.3% | batch:       359 of       686\t|\tloss: 4.50172\n",
      "Training Epoch 13  52.5% | batch:       360 of       686\t|\tloss: 4.98239\n",
      "Training Epoch 13  52.6% | batch:       361 of       686\t|\tloss: 4.60775\n",
      "Training Epoch 13  52.8% | batch:       362 of       686\t|\tloss: 7.78037\n",
      "Training Epoch 13  52.9% | batch:       363 of       686\t|\tloss: 4.73168\n",
      "Training Epoch 13  53.1% | batch:       364 of       686\t|\tloss: 4.83655\n",
      "Training Epoch 13  53.2% | batch:       365 of       686\t|\tloss: 4.35005\n",
      "Training Epoch 13  53.4% | batch:       366 of       686\t|\tloss: 5.16368\n",
      "Training Epoch 13  53.5% | batch:       367 of       686\t|\tloss: 6.29869\n",
      "Training Epoch 13  53.6% | batch:       368 of       686\t|\tloss: 4.33223\n",
      "Training Epoch 13  53.8% | batch:       369 of       686\t|\tloss: 4.28128\n",
      "Training Epoch 13  53.9% | batch:       370 of       686\t|\tloss: 4.80041\n",
      "Training Epoch 13  54.1% | batch:       371 of       686\t|\tloss: 5.80379\n",
      "Training Epoch 13  54.2% | batch:       372 of       686\t|\tloss: 4.37504\n",
      "Training Epoch 13  54.4% | batch:       373 of       686\t|\tloss: 4.13968\n",
      "Training Epoch 13  54.5% | batch:       374 of       686\t|\tloss: 5.82319\n",
      "Training Epoch 13  54.7% | batch:       375 of       686\t|\tloss: 7.9637\n",
      "Training Epoch 13  54.8% | batch:       376 of       686\t|\tloss: 4.75587\n",
      "Training Epoch 13  55.0% | batch:       377 of       686\t|\tloss: 5.12468\n",
      "Training Epoch 13  55.1% | batch:       378 of       686\t|\tloss: 4.29925\n",
      "Training Epoch 13  55.2% | batch:       379 of       686\t|\tloss: 4.54368\n",
      "Training Epoch 13  55.4% | batch:       380 of       686\t|\tloss: 4.60798\n",
      "Training Epoch 13  55.5% | batch:       381 of       686\t|\tloss: 6.56241\n",
      "Training Epoch 13  55.7% | batch:       382 of       686\t|\tloss: 4.46394\n",
      "Training Epoch 13  55.8% | batch:       383 of       686\t|\tloss: 4.14088\n",
      "Training Epoch 13  56.0% | batch:       384 of       686\t|\tloss: 4.76391\n",
      "Training Epoch 13  56.1% | batch:       385 of       686\t|\tloss: 4.2936\n",
      "Training Epoch 13  56.3% | batch:       386 of       686\t|\tloss: 3.78346\n",
      "Training Epoch 13  56.4% | batch:       387 of       686\t|\tloss: 3.73526\n",
      "Training Epoch 13  56.6% | batch:       388 of       686\t|\tloss: 4.33394\n",
      "Training Epoch 13  56.7% | batch:       389 of       686\t|\tloss: 5.22355\n",
      "Training Epoch 13  56.9% | batch:       390 of       686\t|\tloss: 5.3581\n",
      "Training Epoch 13  57.0% | batch:       391 of       686\t|\tloss: 5.88295\n",
      "Training Epoch 13  57.1% | batch:       392 of       686\t|\tloss: 4.41111\n",
      "Training Epoch 13  57.3% | batch:       393 of       686\t|\tloss: 4.03581\n",
      "Training Epoch 13  57.4% | batch:       394 of       686\t|\tloss: 3.77685\n",
      "Training Epoch 13  57.6% | batch:       395 of       686\t|\tloss: 4.39738\n",
      "Training Epoch 13  57.7% | batch:       396 of       686\t|\tloss: 4.44029\n",
      "Training Epoch 13  57.9% | batch:       397 of       686\t|\tloss: 4.44796\n",
      "Training Epoch 13  58.0% | batch:       398 of       686\t|\tloss: 3.7708\n",
      "Training Epoch 13  58.2% | batch:       399 of       686\t|\tloss: 4.78487\n",
      "Training Epoch 13  58.3% | batch:       400 of       686\t|\tloss: 5.03012\n",
      "Training Epoch 13  58.5% | batch:       401 of       686\t|\tloss: 4.66983\n",
      "Training Epoch 13  58.6% | batch:       402 of       686\t|\tloss: 3.83904\n",
      "Training Epoch 13  58.7% | batch:       403 of       686\t|\tloss: 4.37225\n",
      "Training Epoch 13  58.9% | batch:       404 of       686\t|\tloss: 5.43568\n",
      "Training Epoch 13  59.0% | batch:       405 of       686\t|\tloss: 7.87874\n",
      "Training Epoch 13  59.2% | batch:       406 of       686\t|\tloss: 5.14445\n",
      "Training Epoch 13  59.3% | batch:       407 of       686\t|\tloss: 5.19334\n",
      "Training Epoch 13  59.5% | batch:       408 of       686\t|\tloss: 4.51503\n",
      "Training Epoch 13  59.6% | batch:       409 of       686\t|\tloss: 4.80825\n",
      "Training Epoch 13  59.8% | batch:       410 of       686\t|\tloss: 4.29009\n",
      "Training Epoch 13  59.9% | batch:       411 of       686\t|\tloss: 4.18456\n",
      "Training Epoch 13  60.1% | batch:       412 of       686\t|\tloss: 5.42245\n",
      "Training Epoch 13  60.2% | batch:       413 of       686\t|\tloss: 6.27726\n",
      "Training Epoch 13  60.3% | batch:       414 of       686\t|\tloss: 4.17687\n",
      "Training Epoch 13  60.5% | batch:       415 of       686\t|\tloss: 6.09902\n",
      "Training Epoch 13  60.6% | batch:       416 of       686\t|\tloss: 4.79561\n",
      "Training Epoch 13  60.8% | batch:       417 of       686\t|\tloss: 5.7094\n",
      "Training Epoch 13  60.9% | batch:       418 of       686\t|\tloss: 4.93036\n",
      "Training Epoch 13  61.1% | batch:       419 of       686\t|\tloss: 4.37915\n",
      "Training Epoch 13  61.2% | batch:       420 of       686\t|\tloss: 4.76243\n",
      "Training Epoch 13  61.4% | batch:       421 of       686\t|\tloss: 4.92458\n",
      "Training Epoch 13  61.5% | batch:       422 of       686\t|\tloss: 3.86857\n",
      "Training Epoch 13  61.7% | batch:       423 of       686\t|\tloss: 3.74171\n",
      "Training Epoch 13  61.8% | batch:       424 of       686\t|\tloss: 3.06634\n",
      "Training Epoch 13  62.0% | batch:       425 of       686\t|\tloss: 3.69537\n",
      "Training Epoch 13  62.1% | batch:       426 of       686\t|\tloss: 5.47855\n",
      "Training Epoch 13  62.2% | batch:       427 of       686\t|\tloss: 3.66641\n",
      "Training Epoch 13  62.4% | batch:       428 of       686\t|\tloss: 5.39093\n",
      "Training Epoch 13  62.5% | batch:       429 of       686\t|\tloss: 5.11769\n",
      "Training Epoch 13  62.7% | batch:       430 of       686\t|\tloss: 5.19636\n",
      "Training Epoch 13  62.8% | batch:       431 of       686\t|\tloss: 3.23564\n",
      "Training Epoch 13  63.0% | batch:       432 of       686\t|\tloss: 4.90567\n",
      "Training Epoch 13  63.1% | batch:       433 of       686\t|\tloss: 5.04127\n",
      "Training Epoch 13  63.3% | batch:       434 of       686\t|\tloss: 4.54149\n",
      "Training Epoch 13  63.4% | batch:       435 of       686\t|\tloss: 4.2724\n",
      "Training Epoch 13  63.6% | batch:       436 of       686\t|\tloss: 5.31205\n",
      "Training Epoch 13  63.7% | batch:       437 of       686\t|\tloss: 5.77526\n",
      "Training Epoch 13  63.8% | batch:       438 of       686\t|\tloss: 5.28893\n",
      "Training Epoch 13  64.0% | batch:       439 of       686\t|\tloss: 3.76373\n",
      "Training Epoch 13  64.1% | batch:       440 of       686\t|\tloss: 4.20555\n",
      "Training Epoch 13  64.3% | batch:       441 of       686\t|\tloss: 5.59022\n",
      "Training Epoch 13  64.4% | batch:       442 of       686\t|\tloss: 4.46771\n",
      "Training Epoch 13  64.6% | batch:       443 of       686\t|\tloss: 4.9072\n",
      "Training Epoch 13  64.7% | batch:       444 of       686\t|\tloss: 5.01257\n",
      "Training Epoch 13  64.9% | batch:       445 of       686\t|\tloss: 3.91467\n",
      "Training Epoch 13  65.0% | batch:       446 of       686\t|\tloss: 2.86776\n",
      "Training Epoch 13  65.2% | batch:       447 of       686\t|\tloss: 4.80994\n",
      "Training Epoch 13  65.3% | batch:       448 of       686\t|\tloss: 4.38582\n",
      "Training Epoch 13  65.5% | batch:       449 of       686\t|\tloss: 5.04756\n",
      "Training Epoch 13  65.6% | batch:       450 of       686\t|\tloss: 5.97123\n",
      "Training Epoch 13  65.7% | batch:       451 of       686\t|\tloss: 4.59423\n",
      "Training Epoch 13  65.9% | batch:       452 of       686\t|\tloss: 3.77306\n",
      "Training Epoch 13  66.0% | batch:       453 of       686\t|\tloss: 4.63143\n",
      "Training Epoch 13  66.2% | batch:       454 of       686\t|\tloss: 5.37693\n",
      "Training Epoch 13  66.3% | batch:       455 of       686\t|\tloss: 4.10838\n",
      "Training Epoch 13  66.5% | batch:       456 of       686\t|\tloss: 3.33052\n",
      "Training Epoch 13  66.6% | batch:       457 of       686\t|\tloss: 4.57515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 13  66.8% | batch:       458 of       686\t|\tloss: 4.87841\n",
      "Training Epoch 13  66.9% | batch:       459 of       686\t|\tloss: 5.33476\n",
      "Training Epoch 13  67.1% | batch:       460 of       686\t|\tloss: 4.84852\n",
      "Training Epoch 13  67.2% | batch:       461 of       686\t|\tloss: 4.64716\n",
      "Training Epoch 13  67.3% | batch:       462 of       686\t|\tloss: 4.88894\n",
      "Training Epoch 13  67.5% | batch:       463 of       686\t|\tloss: 5.26786\n",
      "Training Epoch 13  67.6% | batch:       464 of       686\t|\tloss: 4.42979\n",
      "Training Epoch 13  67.8% | batch:       465 of       686\t|\tloss: 4.7582\n",
      "Training Epoch 13  67.9% | batch:       466 of       686\t|\tloss: 4.42488\n",
      "Training Epoch 13  68.1% | batch:       467 of       686\t|\tloss: 4.57768\n",
      "Training Epoch 13  68.2% | batch:       468 of       686\t|\tloss: 5.6272\n",
      "Training Epoch 13  68.4% | batch:       469 of       686\t|\tloss: 3.98425\n",
      "Training Epoch 13  68.5% | batch:       470 of       686\t|\tloss: 4.66435\n",
      "Training Epoch 13  68.7% | batch:       471 of       686\t|\tloss: 4.28055\n",
      "Training Epoch 13  68.8% | batch:       472 of       686\t|\tloss: 4.63731\n",
      "Training Epoch 13  69.0% | batch:       473 of       686\t|\tloss: 3.90794\n",
      "Training Epoch 13  69.1% | batch:       474 of       686\t|\tloss: 4.36841\n",
      "Training Epoch 13  69.2% | batch:       475 of       686\t|\tloss: 3.82883\n",
      "Training Epoch 13  69.4% | batch:       476 of       686\t|\tloss: 4.23298\n",
      "Training Epoch 13  69.5% | batch:       477 of       686\t|\tloss: 3.5489\n",
      "Training Epoch 13  69.7% | batch:       478 of       686\t|\tloss: 4.9322\n",
      "Training Epoch 13  69.8% | batch:       479 of       686\t|\tloss: 5.48559\n",
      "Training Epoch 13  70.0% | batch:       480 of       686\t|\tloss: 3.93522\n",
      "Training Epoch 13  70.1% | batch:       481 of       686\t|\tloss: 4.62168\n",
      "Training Epoch 13  70.3% | batch:       482 of       686\t|\tloss: 3.88954\n",
      "Training Epoch 13  70.4% | batch:       483 of       686\t|\tloss: 4.33435\n",
      "Training Epoch 13  70.6% | batch:       484 of       686\t|\tloss: 4.18594\n",
      "Training Epoch 13  70.7% | batch:       485 of       686\t|\tloss: 6.83449\n",
      "Training Epoch 13  70.8% | batch:       486 of       686\t|\tloss: 3.72254\n",
      "Training Epoch 13  71.0% | batch:       487 of       686\t|\tloss: 3.55269\n",
      "Training Epoch 13  71.1% | batch:       488 of       686\t|\tloss: 4.82164\n",
      "Training Epoch 13  71.3% | batch:       489 of       686\t|\tloss: 3.57048\n",
      "Training Epoch 13  71.4% | batch:       490 of       686\t|\tloss: 5.06708\n",
      "Training Epoch 13  71.6% | batch:       491 of       686\t|\tloss: 4.49796\n",
      "Training Epoch 13  71.7% | batch:       492 of       686\t|\tloss: 5.35425\n",
      "Training Epoch 13  71.9% | batch:       493 of       686\t|\tloss: 5.23221\n",
      "Training Epoch 13  72.0% | batch:       494 of       686\t|\tloss: 4.89014\n",
      "Training Epoch 13  72.2% | batch:       495 of       686\t|\tloss: 4.7594\n",
      "Training Epoch 13  72.3% | batch:       496 of       686\t|\tloss: 4.77342\n",
      "Training Epoch 13  72.4% | batch:       497 of       686\t|\tloss: 7.43319\n",
      "Training Epoch 13  72.6% | batch:       498 of       686\t|\tloss: 5.09143\n",
      "Training Epoch 13  72.7% | batch:       499 of       686\t|\tloss: 4.66926\n",
      "Training Epoch 13  72.9% | batch:       500 of       686\t|\tloss: 4.67383\n",
      "Training Epoch 13  73.0% | batch:       501 of       686\t|\tloss: 4.01785\n",
      "Training Epoch 13  73.2% | batch:       502 of       686\t|\tloss: 4.60344\n",
      "Training Epoch 13  73.3% | batch:       503 of       686\t|\tloss: 4.22874\n",
      "Training Epoch 13  73.5% | batch:       504 of       686\t|\tloss: 3.7402\n",
      "Training Epoch 13  73.6% | batch:       505 of       686\t|\tloss: 4.23401\n",
      "Training Epoch 13  73.8% | batch:       506 of       686\t|\tloss: 3.90809\n",
      "Training Epoch 13  73.9% | batch:       507 of       686\t|\tloss: 4.02708\n",
      "Training Epoch 13  74.1% | batch:       508 of       686\t|\tloss: 4.58771\n",
      "Training Epoch 13  74.2% | batch:       509 of       686\t|\tloss: 4.7791\n",
      "Training Epoch 13  74.3% | batch:       510 of       686\t|\tloss: 5.33322\n",
      "Training Epoch 13  74.5% | batch:       511 of       686\t|\tloss: 6.64577\n",
      "Training Epoch 13  74.6% | batch:       512 of       686\t|\tloss: 6.3864\n",
      "Training Epoch 13  74.8% | batch:       513 of       686\t|\tloss: 4.2764\n",
      "Training Epoch 13  74.9% | batch:       514 of       686\t|\tloss: 6.87404\n",
      "Training Epoch 13  75.1% | batch:       515 of       686\t|\tloss: 5.43396\n",
      "Training Epoch 13  75.2% | batch:       516 of       686\t|\tloss: 4.13605\n",
      "Training Epoch 13  75.4% | batch:       517 of       686\t|\tloss: 4.73337\n",
      "Training Epoch 13  75.5% | batch:       518 of       686\t|\tloss: 5.0037\n",
      "Training Epoch 13  75.7% | batch:       519 of       686\t|\tloss: 4.42468\n",
      "Training Epoch 13  75.8% | batch:       520 of       686\t|\tloss: 4.4006\n",
      "Training Epoch 13  75.9% | batch:       521 of       686\t|\tloss: 4.48616\n",
      "Training Epoch 13  76.1% | batch:       522 of       686\t|\tloss: 4.40389\n",
      "Training Epoch 13  76.2% | batch:       523 of       686\t|\tloss: 7.17034\n",
      "Training Epoch 13  76.4% | batch:       524 of       686\t|\tloss: 4.261\n",
      "Training Epoch 13  76.5% | batch:       525 of       686\t|\tloss: 4.53655\n",
      "Training Epoch 13  76.7% | batch:       526 of       686\t|\tloss: 6.33725\n",
      "Training Epoch 13  76.8% | batch:       527 of       686\t|\tloss: 5.55305\n",
      "Training Epoch 13  77.0% | batch:       528 of       686\t|\tloss: 3.68643\n",
      "Training Epoch 13  77.1% | batch:       529 of       686\t|\tloss: 5.41012\n",
      "Training Epoch 13  77.3% | batch:       530 of       686\t|\tloss: 5.46794\n",
      "Training Epoch 13  77.4% | batch:       531 of       686\t|\tloss: 5.64156\n",
      "Training Epoch 13  77.6% | batch:       532 of       686\t|\tloss: 4.06674\n",
      "Training Epoch 13  77.7% | batch:       533 of       686\t|\tloss: 4.33051\n",
      "Training Epoch 13  77.8% | batch:       534 of       686\t|\tloss: 3.99086\n",
      "Training Epoch 13  78.0% | batch:       535 of       686\t|\tloss: 6.16623\n",
      "Training Epoch 13  78.1% | batch:       536 of       686\t|\tloss: 4.09411\n",
      "Training Epoch 13  78.3% | batch:       537 of       686\t|\tloss: 5.484\n",
      "Training Epoch 13  78.4% | batch:       538 of       686\t|\tloss: 3.69603\n",
      "Training Epoch 13  78.6% | batch:       539 of       686\t|\tloss: 5.33465\n",
      "Training Epoch 13  78.7% | batch:       540 of       686\t|\tloss: 5.54195\n",
      "Training Epoch 13  78.9% | batch:       541 of       686\t|\tloss: 4.16725\n",
      "Training Epoch 13  79.0% | batch:       542 of       686\t|\tloss: 3.18285\n",
      "Training Epoch 13  79.2% | batch:       543 of       686\t|\tloss: 4.31686\n",
      "Training Epoch 13  79.3% | batch:       544 of       686\t|\tloss: 4.80038\n",
      "Training Epoch 13  79.4% | batch:       545 of       686\t|\tloss: 4.32809\n",
      "Training Epoch 13  79.6% | batch:       546 of       686\t|\tloss: 4.56868\n",
      "Training Epoch 13  79.7% | batch:       547 of       686\t|\tloss: 5.76188\n",
      "Training Epoch 13  79.9% | batch:       548 of       686\t|\tloss: 5.61582\n",
      "Training Epoch 13  80.0% | batch:       549 of       686\t|\tloss: 5.98931\n",
      "Training Epoch 13  80.2% | batch:       550 of       686\t|\tloss: 4.75839\n",
      "Training Epoch 13  80.3% | batch:       551 of       686\t|\tloss: 3.48562\n",
      "Training Epoch 13  80.5% | batch:       552 of       686\t|\tloss: 4.26881\n",
      "Training Epoch 13  80.6% | batch:       553 of       686\t|\tloss: 6.79411\n",
      "Training Epoch 13  80.8% | batch:       554 of       686\t|\tloss: 4.5729\n",
      "Training Epoch 13  80.9% | batch:       555 of       686\t|\tloss: 4.11223\n",
      "Training Epoch 13  81.0% | batch:       556 of       686\t|\tloss: 5.35057\n",
      "Training Epoch 13  81.2% | batch:       557 of       686\t|\tloss: 5.18446\n",
      "Training Epoch 13  81.3% | batch:       558 of       686\t|\tloss: 4.39817\n",
      "Training Epoch 13  81.5% | batch:       559 of       686\t|\tloss: 3.71358\n",
      "Training Epoch 13  81.6% | batch:       560 of       686\t|\tloss: 4.05283\n",
      "Training Epoch 13  81.8% | batch:       561 of       686\t|\tloss: 5.26174\n",
      "Training Epoch 13  81.9% | batch:       562 of       686\t|\tloss: 4.92766\n",
      "Training Epoch 13  82.1% | batch:       563 of       686\t|\tloss: 4.65908\n",
      "Training Epoch 13  82.2% | batch:       564 of       686\t|\tloss: 5.25001\n",
      "Training Epoch 13  82.4% | batch:       565 of       686\t|\tloss: 4.96348\n",
      "Training Epoch 13  82.5% | batch:       566 of       686\t|\tloss: 4.24614\n",
      "Training Epoch 13  82.7% | batch:       567 of       686\t|\tloss: 4.76502\n",
      "Training Epoch 13  82.8% | batch:       568 of       686\t|\tloss: 4.94114\n",
      "Training Epoch 13  82.9% | batch:       569 of       686\t|\tloss: 4.11388\n",
      "Training Epoch 13  83.1% | batch:       570 of       686\t|\tloss: 4.7701\n",
      "Training Epoch 13  83.2% | batch:       571 of       686\t|\tloss: 5.0432\n",
      "Training Epoch 13  83.4% | batch:       572 of       686\t|\tloss: 4.89456\n",
      "Training Epoch 13  83.5% | batch:       573 of       686\t|\tloss: 5.39247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 13  83.7% | batch:       574 of       686\t|\tloss: 4.51135\n",
      "Training Epoch 13  83.8% | batch:       575 of       686\t|\tloss: 4.7971\n",
      "Training Epoch 13  84.0% | batch:       576 of       686\t|\tloss: 5.75546\n",
      "Training Epoch 13  84.1% | batch:       577 of       686\t|\tloss: 3.96008\n",
      "Training Epoch 13  84.3% | batch:       578 of       686\t|\tloss: 4.35639\n",
      "Training Epoch 13  84.4% | batch:       579 of       686\t|\tloss: 5.73201\n",
      "Training Epoch 13  84.5% | batch:       580 of       686\t|\tloss: 3.68273\n",
      "Training Epoch 13  84.7% | batch:       581 of       686\t|\tloss: 5.07159\n",
      "Training Epoch 13  84.8% | batch:       582 of       686\t|\tloss: 5.59403\n",
      "Training Epoch 13  85.0% | batch:       583 of       686\t|\tloss: 4.90237\n",
      "Training Epoch 13  85.1% | batch:       584 of       686\t|\tloss: 4.51212\n",
      "Training Epoch 13  85.3% | batch:       585 of       686\t|\tloss: 5.36138\n",
      "Training Epoch 13  85.4% | batch:       586 of       686\t|\tloss: 6.91014\n",
      "Training Epoch 13  85.6% | batch:       587 of       686\t|\tloss: 4.51576\n",
      "Training Epoch 13  85.7% | batch:       588 of       686\t|\tloss: 4.95837\n",
      "Training Epoch 13  85.9% | batch:       589 of       686\t|\tloss: 6.17328\n",
      "Training Epoch 13  86.0% | batch:       590 of       686\t|\tloss: 4.38659\n",
      "Training Epoch 13  86.2% | batch:       591 of       686\t|\tloss: 5.39482\n",
      "Training Epoch 13  86.3% | batch:       592 of       686\t|\tloss: 4.80784\n",
      "Training Epoch 13  86.4% | batch:       593 of       686\t|\tloss: 4.52559\n",
      "Training Epoch 13  86.6% | batch:       594 of       686\t|\tloss: 4.95891\n",
      "Training Epoch 13  86.7% | batch:       595 of       686\t|\tloss: 5.15774\n",
      "Training Epoch 13  86.9% | batch:       596 of       686\t|\tloss: 3.57099\n",
      "Training Epoch 13  87.0% | batch:       597 of       686\t|\tloss: 6.45323\n",
      "Training Epoch 13  87.2% | batch:       598 of       686\t|\tloss: 5.20464\n",
      "Training Epoch 13  87.3% | batch:       599 of       686\t|\tloss: 5.98031\n",
      "Training Epoch 13  87.5% | batch:       600 of       686\t|\tloss: 4.00228\n",
      "Training Epoch 13  87.6% | batch:       601 of       686\t|\tloss: 4.78408\n",
      "Training Epoch 13  87.8% | batch:       602 of       686\t|\tloss: 4.69742\n",
      "Training Epoch 13  87.9% | batch:       603 of       686\t|\tloss: 4.00643\n",
      "Training Epoch 13  88.0% | batch:       604 of       686\t|\tloss: 5.4169\n",
      "Training Epoch 13  88.2% | batch:       605 of       686\t|\tloss: 5.26429\n",
      "Training Epoch 13  88.3% | batch:       606 of       686\t|\tloss: 4.62059\n",
      "Training Epoch 13  88.5% | batch:       607 of       686\t|\tloss: 5.58664\n",
      "Training Epoch 13  88.6% | batch:       608 of       686\t|\tloss: 4.58472\n",
      "Training Epoch 13  88.8% | batch:       609 of       686\t|\tloss: 4.8267\n",
      "Training Epoch 13  88.9% | batch:       610 of       686\t|\tloss: 6.185\n",
      "Training Epoch 13  89.1% | batch:       611 of       686\t|\tloss: 3.01147\n",
      "Training Epoch 13  89.2% | batch:       612 of       686\t|\tloss: 4.97101\n",
      "Training Epoch 13  89.4% | batch:       613 of       686\t|\tloss: 4.18775\n",
      "Training Epoch 13  89.5% | batch:       614 of       686\t|\tloss: 4.5253\n",
      "Training Epoch 13  89.7% | batch:       615 of       686\t|\tloss: 3.68206\n",
      "Training Epoch 13  89.8% | batch:       616 of       686\t|\tloss: 4.35391\n",
      "Training Epoch 13  89.9% | batch:       617 of       686\t|\tloss: 6.30017\n",
      "Training Epoch 13  90.1% | batch:       618 of       686\t|\tloss: 4.59903\n",
      "Training Epoch 13  90.2% | batch:       619 of       686\t|\tloss: 4.23756\n",
      "Training Epoch 13  90.4% | batch:       620 of       686\t|\tloss: 5.14699\n",
      "Training Epoch 13  90.5% | batch:       621 of       686\t|\tloss: 4.1018\n",
      "Training Epoch 13  90.7% | batch:       622 of       686\t|\tloss: 4.6076\n",
      "Training Epoch 13  90.8% | batch:       623 of       686\t|\tloss: 4.73612\n",
      "Training Epoch 13  91.0% | batch:       624 of       686\t|\tloss: 6.35038\n",
      "Training Epoch 13  91.1% | batch:       625 of       686\t|\tloss: 3.35354\n",
      "Training Epoch 13  91.3% | batch:       626 of       686\t|\tloss: 4.62322\n",
      "Training Epoch 13  91.4% | batch:       627 of       686\t|\tloss: 4.79006\n",
      "Training Epoch 13  91.5% | batch:       628 of       686\t|\tloss: 5.15898\n",
      "Training Epoch 13  91.7% | batch:       629 of       686\t|\tloss: 4.92582\n",
      "Training Epoch 13  91.8% | batch:       630 of       686\t|\tloss: 4.73024\n",
      "Training Epoch 13  92.0% | batch:       631 of       686\t|\tloss: 5.18049\n",
      "Training Epoch 13  92.1% | batch:       632 of       686\t|\tloss: 3.86778\n",
      "Training Epoch 13  92.3% | batch:       633 of       686\t|\tloss: 5.59336\n",
      "Training Epoch 13  92.4% | batch:       634 of       686\t|\tloss: 3.73114\n",
      "Training Epoch 13  92.6% | batch:       635 of       686\t|\tloss: 6.06485\n",
      "Training Epoch 13  92.7% | batch:       636 of       686\t|\tloss: 3.30259\n",
      "Training Epoch 13  92.9% | batch:       637 of       686\t|\tloss: 4.50612\n",
      "Training Epoch 13  93.0% | batch:       638 of       686\t|\tloss: 4.1641\n",
      "Training Epoch 13  93.1% | batch:       639 of       686\t|\tloss: 5.48737\n",
      "Training Epoch 13  93.3% | batch:       640 of       686\t|\tloss: 4.49696\n",
      "Training Epoch 13  93.4% | batch:       641 of       686\t|\tloss: 4.36824\n",
      "Training Epoch 13  93.6% | batch:       642 of       686\t|\tloss: 5.44026\n",
      "Training Epoch 13  93.7% | batch:       643 of       686\t|\tloss: 6.52726\n",
      "Training Epoch 13  93.9% | batch:       644 of       686\t|\tloss: 5.46452\n",
      "Training Epoch 13  94.0% | batch:       645 of       686\t|\tloss: 4.62609\n",
      "Training Epoch 13  94.2% | batch:       646 of       686\t|\tloss: 5.89937\n",
      "Training Epoch 13  94.3% | batch:       647 of       686\t|\tloss: 4.66084\n",
      "Training Epoch 13  94.5% | batch:       648 of       686\t|\tloss: 4.6776\n",
      "Training Epoch 13  94.6% | batch:       649 of       686\t|\tloss: 4.3139\n",
      "Training Epoch 13  94.8% | batch:       650 of       686\t|\tloss: 4.2054\n",
      "Training Epoch 13  94.9% | batch:       651 of       686\t|\tloss: 5.38958\n",
      "Training Epoch 13  95.0% | batch:       652 of       686\t|\tloss: 4.07805\n",
      "Training Epoch 13  95.2% | batch:       653 of       686\t|\tloss: 3.94773\n",
      "Training Epoch 13  95.3% | batch:       654 of       686\t|\tloss: 4.68073\n",
      "Training Epoch 13  95.5% | batch:       655 of       686\t|\tloss: 4.48705\n",
      "Training Epoch 13  95.6% | batch:       656 of       686\t|\tloss: 4.56632\n",
      "Training Epoch 13  95.8% | batch:       657 of       686\t|\tloss: 3.82831\n",
      "Training Epoch 13  95.9% | batch:       658 of       686\t|\tloss: 3.77998\n",
      "Training Epoch 13  96.1% | batch:       659 of       686\t|\tloss: 3.51373\n",
      "Training Epoch 13  96.2% | batch:       660 of       686\t|\tloss: 3.42264\n",
      "Training Epoch 13  96.4% | batch:       661 of       686\t|\tloss: 5.48488\n",
      "Training Epoch 13  96.5% | batch:       662 of       686\t|\tloss: 5.01226\n",
      "Training Epoch 13  96.6% | batch:       663 of       686\t|\tloss: 6.561\n",
      "Training Epoch 13  96.8% | batch:       664 of       686\t|\tloss: 5.00624\n",
      "Training Epoch 13  96.9% | batch:       665 of       686\t|\tloss: 4.17254\n",
      "Training Epoch 13  97.1% | batch:       666 of       686\t|\tloss: 4.3843\n",
      "Training Epoch 13  97.2% | batch:       667 of       686\t|\tloss: 4.5811\n",
      "Training Epoch 13  97.4% | batch:       668 of       686\t|\tloss: 6.05755\n",
      "Training Epoch 13  97.5% | batch:       669 of       686\t|\tloss: 4.25284\n",
      "Training Epoch 13  97.7% | batch:       670 of       686\t|\tloss: 5.00406\n",
      "Training Epoch 13  97.8% | batch:       671 of       686\t|\tloss: 4.85159\n",
      "Training Epoch 13  98.0% | batch:       672 of       686\t|\tloss: 4.19911\n",
      "Training Epoch 13  98.1% | batch:       673 of       686\t|\tloss: 4.3085\n",
      "Training Epoch 13  98.3% | batch:       674 of       686\t|\tloss: 3.35475\n",
      "Training Epoch 13  98.4% | batch:       675 of       686\t|\tloss: 4.86055\n",
      "Training Epoch 13  98.5% | batch:       676 of       686\t|\tloss: 4.9838\n",
      "Training Epoch 13  98.7% | batch:       677 of       686\t|\tloss: 3.64046\n",
      "Training Epoch 13  98.8% | batch:       678 of       686\t|\tloss: 4.56647\n",
      "Training Epoch 13  99.0% | batch:       679 of       686\t|\tloss: 4.66392\n",
      "Training Epoch 13  99.1% | batch:       680 of       686\t|\tloss: 5.19664\n",
      "Training Epoch 13  99.3% | batch:       681 of       686\t|\tloss: 6.03109\n",
      "Training Epoch 13  99.4% | batch:       682 of       686\t|\tloss: 4.55288\n",
      "Training Epoch 13  99.6% | batch:       683 of       686\t|\tloss: 4.6338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:05:47,607 | INFO : Epoch 13 Training Summary: epoch: 13.000000 | loss: 4.978216 | \n",
      "2023-05-25 22:05:47,609 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 25.04056692123413 seconds\n",
      "\n",
      "2023-05-25 22:05:47,610 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.69658851623535 seconds\n",
      "2023-05-25 22:05:47,611 | INFO : Avg batch train. time: 0.034543131947864944 seconds\n",
      "2023-05-25 22:05:47,611 | INFO : Avg sample train. time: 0.0002702159589056999 seconds\n",
      "2023-05-25 22:05:47,612 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 13  99.7% | batch:       684 of       686\t|\tloss: 6.30391\n",
      "Training Epoch 13  99.9% | batch:       685 of       686\t|\tloss: 3.48985\n",
      "\n",
      "Evaluating Epoch 13   0.0% | batch:         0 of       172\t|\tloss: 2.4152\n",
      "Evaluating Epoch 13   0.6% | batch:         1 of       172\t|\tloss: 3.03616\n",
      "Evaluating Epoch 13   1.2% | batch:         2 of       172\t|\tloss: 2.50936\n",
      "Evaluating Epoch 13   1.7% | batch:         3 of       172\t|\tloss: 4.27473\n",
      "Evaluating Epoch 13   2.3% | batch:         4 of       172\t|\tloss: 2.57491\n",
      "Evaluating Epoch 13   2.9% | batch:         5 of       172\t|\tloss: 2.00527\n",
      "Evaluating Epoch 13   3.5% | batch:         6 of       172\t|\tloss: 3.1261\n",
      "Evaluating Epoch 13   4.1% | batch:         7 of       172\t|\tloss: 4.29483\n",
      "Evaluating Epoch 13   4.7% | batch:         8 of       172\t|\tloss: 2.37332\n",
      "Evaluating Epoch 13   5.2% | batch:         9 of       172\t|\tloss: 2.79844\n",
      "Evaluating Epoch 13   5.8% | batch:        10 of       172\t|\tloss: 3.33306\n",
      "Evaluating Epoch 13   6.4% | batch:        11 of       172\t|\tloss: 3.0355\n",
      "Evaluating Epoch 13   7.0% | batch:        12 of       172\t|\tloss: 1.97531\n",
      "Evaluating Epoch 13   7.6% | batch:        13 of       172\t|\tloss: 3.14754\n",
      "Evaluating Epoch 13   8.1% | batch:        14 of       172\t|\tloss: 3.26216\n",
      "Evaluating Epoch 13   8.7% | batch:        15 of       172\t|\tloss: 2.55022\n",
      "Evaluating Epoch 13   9.3% | batch:        16 of       172\t|\tloss: 3.29735\n",
      "Evaluating Epoch 13   9.9% | batch:        17 of       172\t|\tloss: 2.44632\n",
      "Evaluating Epoch 13  10.5% | batch:        18 of       172\t|\tloss: 10.4907\n",
      "Evaluating Epoch 13  11.0% | batch:        19 of       172\t|\tloss: 1.33327\n",
      "Evaluating Epoch 13  11.6% | batch:        20 of       172\t|\tloss: 2.64391\n",
      "Evaluating Epoch 13  12.2% | batch:        21 of       172\t|\tloss: 0.56464\n",
      "Evaluating Epoch 13  12.8% | batch:        22 of       172\t|\tloss: 1.47254\n",
      "Evaluating Epoch 13  13.4% | batch:        23 of       172\t|\tloss: 1.70316\n",
      "Evaluating Epoch 13  14.0% | batch:        24 of       172\t|\tloss: 1.41567\n",
      "Evaluating Epoch 13  14.5% | batch:        25 of       172\t|\tloss: 2.56733\n",
      "Evaluating Epoch 13  15.1% | batch:        26 of       172\t|\tloss: 5.60622\n",
      "Evaluating Epoch 13  15.7% | batch:        27 of       172\t|\tloss: 10.7871\n",
      "Evaluating Epoch 13  16.3% | batch:        28 of       172\t|\tloss: 0.245423\n",
      "Evaluating Epoch 13  16.9% | batch:        29 of       172\t|\tloss: 1.94677\n",
      "Evaluating Epoch 13  17.4% | batch:        30 of       172\t|\tloss: 0.945857\n",
      "Evaluating Epoch 13  18.0% | batch:        31 of       172\t|\tloss: 2.40257\n",
      "Evaluating Epoch 13  18.6% | batch:        32 of       172\t|\tloss: 0.60412\n",
      "Evaluating Epoch 13  19.2% | batch:        33 of       172\t|\tloss: 0.528222\n",
      "Evaluating Epoch 13  19.8% | batch:        34 of       172\t|\tloss: 0.254161\n",
      "Evaluating Epoch 13  20.3% | batch:        35 of       172\t|\tloss: 0.329945\n",
      "Evaluating Epoch 13  20.9% | batch:        36 of       172\t|\tloss: 3.32987\n",
      "Evaluating Epoch 13  21.5% | batch:        37 of       172\t|\tloss: 3.51138\n",
      "Evaluating Epoch 13  22.1% | batch:        38 of       172\t|\tloss: 2.03315\n",
      "Evaluating Epoch 13  22.7% | batch:        39 of       172\t|\tloss: 3.61438\n",
      "Evaluating Epoch 13  23.3% | batch:        40 of       172\t|\tloss: 0.542898\n",
      "Evaluating Epoch 13  23.8% | batch:        41 of       172\t|\tloss: 1.4068\n",
      "Evaluating Epoch 13  24.4% | batch:        42 of       172\t|\tloss: 0.666206\n",
      "Evaluating Epoch 13  25.0% | batch:        43 of       172\t|\tloss: 11.6529\n",
      "Evaluating Epoch 13  25.6% | batch:        44 of       172\t|\tloss: 1.27504\n",
      "Evaluating Epoch 13  26.2% | batch:        45 of       172\t|\tloss: 1.05212\n",
      "Evaluating Epoch 13  26.7% | batch:        46 of       172\t|\tloss: 0.309509\n",
      "Evaluating Epoch 13  27.3% | batch:        47 of       172\t|\tloss: 2.35914\n",
      "Evaluating Epoch 13  27.9% | batch:        48 of       172\t|\tloss: 0.383614\n",
      "Evaluating Epoch 13  28.5% | batch:        49 of       172\t|\tloss: 1.11479\n",
      "Evaluating Epoch 13  29.1% | batch:        50 of       172\t|\tloss: 0.377853\n",
      "Evaluating Epoch 13  29.7% | batch:        51 of       172\t|\tloss: 0.312816\n",
      "Evaluating Epoch 13  30.2% | batch:        52 of       172\t|\tloss: 1.93257\n",
      "Evaluating Epoch 13  30.8% | batch:        53 of       172\t|\tloss: 0.887589\n",
      "Evaluating Epoch 13  31.4% | batch:        54 of       172\t|\tloss: 1.06172\n",
      "Evaluating Epoch 13  32.0% | batch:        55 of       172\t|\tloss: 2.41228\n",
      "Evaluating Epoch 13  32.6% | batch:        56 of       172\t|\tloss: 1.47182\n",
      "Evaluating Epoch 13  33.1% | batch:        57 of       172\t|\tloss: 3.07978\n",
      "Evaluating Epoch 13  33.7% | batch:        58 of       172\t|\tloss: 0.611402\n",
      "Evaluating Epoch 13  34.3% | batch:        59 of       172\t|\tloss: 2.49263\n",
      "Evaluating Epoch 13  34.9% | batch:        60 of       172\t|\tloss: 0.426152\n",
      "Evaluating Epoch 13  35.5% | batch:        61 of       172\t|\tloss: 3.47394\n",
      "Evaluating Epoch 13  36.0% | batch:        62 of       172\t|\tloss: 0.564584\n",
      "Evaluating Epoch 13  36.6% | batch:        63 of       172\t|\tloss: 0.948917\n",
      "Evaluating Epoch 13  37.2% | batch:        64 of       172\t|\tloss: 2.0466\n",
      "Evaluating Epoch 13  37.8% | batch:        65 of       172\t|\tloss: 0.649437\n",
      "Evaluating Epoch 13  38.4% | batch:        66 of       172\t|\tloss: 2.35536\n",
      "Evaluating Epoch 13  39.0% | batch:        67 of       172\t|\tloss: 1.34684\n",
      "Evaluating Epoch 13  39.5% | batch:        68 of       172\t|\tloss: 1.64322\n",
      "Evaluating Epoch 13  40.1% | batch:        69 of       172\t|\tloss: 3.96816\n",
      "Evaluating Epoch 13  40.7% | batch:        70 of       172\t|\tloss: 0.31521\n",
      "Evaluating Epoch 13  41.3% | batch:        71 of       172\t|\tloss: 0.923815\n",
      "Evaluating Epoch 13  41.9% | batch:        72 of       172\t|\tloss: 1.96884\n",
      "Evaluating Epoch 13  42.4% | batch:        73 of       172\t|\tloss: 0.763822\n",
      "Evaluating Epoch 13  43.0% | batch:        74 of       172\t|\tloss: 0.406654\n",
      "Evaluating Epoch 13  43.6% | batch:        75 of       172\t|\tloss: 0.458529\n",
      "Evaluating Epoch 13  44.2% | batch:        76 of       172\t|\tloss: 0.428553\n",
      "Evaluating Epoch 13  44.8% | batch:        77 of       172\t|\tloss: 0.435143\n",
      "Evaluating Epoch 13  45.3% | batch:        78 of       172\t|\tloss: 0.481401\n",
      "Evaluating Epoch 13  45.9% | batch:        79 of       172\t|\tloss: 0.314303\n",
      "Evaluating Epoch 13  46.5% | batch:        80 of       172\t|\tloss: 0.58254\n",
      "Evaluating Epoch 13  47.1% | batch:        81 of       172\t|\tloss: 0.453824\n",
      "Evaluating Epoch 13  47.7% | batch:        82 of       172\t|\tloss: 0.431516\n",
      "Evaluating Epoch 13  48.3% | batch:        83 of       172\t|\tloss: 1.04082\n",
      "Evaluating Epoch 13  48.8% | batch:        84 of       172\t|\tloss: 1.11323\n",
      "Evaluating Epoch 13  49.4% | batch:        85 of       172\t|\tloss: 2.12138\n",
      "Evaluating Epoch 13  50.0% | batch:        86 of       172\t|\tloss: 1.07231\n",
      "Evaluating Epoch 13  50.6% | batch:        87 of       172\t|\tloss: 1.10767\n",
      "Evaluating Epoch 13  51.2% | batch:        88 of       172\t|\tloss: 1.72876\n",
      "Evaluating Epoch 13  51.7% | batch:        89 of       172\t|\tloss: 2.5107\n",
      "Evaluating Epoch 13  52.3% | batch:        90 of       172\t|\tloss: 1.25405\n",
      "Evaluating Epoch 13  52.9% | batch:        91 of       172\t|\tloss: 1.79187\n",
      "Evaluating Epoch 13  53.5% | batch:        92 of       172\t|\tloss: 2.84755\n",
      "Evaluating Epoch 13  54.1% | batch:        93 of       172\t|\tloss: 1.39931\n",
      "Evaluating Epoch 13  54.7% | batch:        94 of       172\t|\tloss: 1.14383\n",
      "Evaluating Epoch 13  55.2% | batch:        95 of       172\t|\tloss: 2.27612\n",
      "Evaluating Epoch 13  55.8% | batch:        96 of       172\t|\tloss: 2.00304\n",
      "Evaluating Epoch 13  56.4% | batch:        97 of       172\t|\tloss: 1.44266\n",
      "Evaluating Epoch 13  57.0% | batch:        98 of       172\t|\tloss: 1.71881\n",
      "Evaluating Epoch 13  57.6% | batch:        99 of       172\t|\tloss: 2.92558\n",
      "Evaluating Epoch 13  58.1% | batch:       100 of       172\t|\tloss: 0.677487\n",
      "Evaluating Epoch 13  58.7% | batch:       101 of       172\t|\tloss: 0.95983\n",
      "Evaluating Epoch 13  59.3% | batch:       102 of       172\t|\tloss: 2.52137\n",
      "Evaluating Epoch 13  59.9% | batch:       103 of       172\t|\tloss: 1.81003\n",
      "Evaluating Epoch 13  60.5% | batch:       104 of       172\t|\tloss: 1.26199\n",
      "Evaluating Epoch 13  61.0% | batch:       105 of       172\t|\tloss: 1.58272\n",
      "Evaluating Epoch 13  61.6% | batch:       106 of       172\t|\tloss: 3.10252\n",
      "Evaluating Epoch 13  62.2% | batch:       107 of       172\t|\tloss: 1.47689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 13  62.8% | batch:       108 of       172\t|\tloss: 1.46141\n",
      "Evaluating Epoch 13  63.4% | batch:       109 of       172\t|\tloss: 2.64213\n",
      "Evaluating Epoch 13  64.0% | batch:       110 of       172\t|\tloss: 2.21433\n",
      "Evaluating Epoch 13  64.5% | batch:       111 of       172\t|\tloss: 1.21854\n",
      "Evaluating Epoch 13  65.1% | batch:       112 of       172\t|\tloss: 0.980838\n",
      "Evaluating Epoch 13  65.7% | batch:       113 of       172\t|\tloss: 1.9162\n",
      "Evaluating Epoch 13  66.3% | batch:       114 of       172\t|\tloss: 1.29372\n",
      "Evaluating Epoch 13  66.9% | batch:       115 of       172\t|\tloss: 0.711998\n",
      "Evaluating Epoch 13  67.4% | batch:       116 of       172\t|\tloss: 0.509552\n",
      "Evaluating Epoch 13  68.0% | batch:       117 of       172\t|\tloss: 0.605998\n",
      "Evaluating Epoch 13  68.6% | batch:       118 of       172\t|\tloss: 0.290556\n",
      "Evaluating Epoch 13  69.2% | batch:       119 of       172\t|\tloss: 0.480645\n",
      "Evaluating Epoch 13  69.8% | batch:       120 of       172\t|\tloss: 0.445987\n",
      "Evaluating Epoch 13  70.3% | batch:       121 of       172\t|\tloss: 0.963109\n",
      "Evaluating Epoch 13  70.9% | batch:       122 of       172\t|\tloss: 0.462616\n",
      "Evaluating Epoch 13  71.5% | batch:       123 of       172\t|\tloss: 1.4025\n",
      "Evaluating Epoch 13  72.1% | batch:       124 of       172\t|\tloss: 5.11638\n",
      "Evaluating Epoch 13  72.7% | batch:       125 of       172\t|\tloss: 0.777619\n",
      "Evaluating Epoch 13  73.3% | batch:       126 of       172\t|\tloss: 0.606399\n",
      "Evaluating Epoch 13  73.8% | batch:       127 of       172\t|\tloss: 0.462807\n",
      "Evaluating Epoch 13  74.4% | batch:       128 of       172\t|\tloss: 0.557001\n",
      "Evaluating Epoch 13  75.0% | batch:       129 of       172\t|\tloss: 0.518872\n",
      "Evaluating Epoch 13  75.6% | batch:       130 of       172\t|\tloss: 0.435121\n",
      "Evaluating Epoch 13  76.2% | batch:       131 of       172\t|\tloss: 0.698189\n",
      "Evaluating Epoch 13  76.7% | batch:       132 of       172\t|\tloss: 0.574263\n",
      "Evaluating Epoch 13  77.3% | batch:       133 of       172\t|\tloss: 0.485836\n",
      "Evaluating Epoch 13  77.9% | batch:       134 of       172\t|\tloss: 0.715849\n",
      "Evaluating Epoch 13  78.5% | batch:       135 of       172\t|\tloss: 0.330253\n",
      "Evaluating Epoch 13  79.1% | batch:       136 of       172\t|\tloss: 0.617791\n",
      "Evaluating Epoch 13  79.7% | batch:       137 of       172\t|\tloss: 0.260764\n",
      "Evaluating Epoch 13  80.2% | batch:       138 of       172\t|\tloss: 0.703919\n",
      "Evaluating Epoch 13  80.8% | batch:       139 of       172\t|\tloss: 0.414085\n",
      "Evaluating Epoch 13  81.4% | batch:       140 of       172\t|\tloss: 0.572824\n",
      "Evaluating Epoch 13  82.0% | batch:       141 of       172\t|\tloss: 0.206346\n",
      "Evaluating Epoch 13  82.6% | batch:       142 of       172\t|\tloss: 0.426009\n",
      "Evaluating Epoch 13  83.1% | batch:       143 of       172\t|\tloss: 0.313046\n",
      "Evaluating Epoch 13  83.7% | batch:       144 of       172\t|\tloss: 0.607818\n",
      "Evaluating Epoch 13  84.3% | batch:       145 of       172\t|\tloss: 0.394182\n",
      "Evaluating Epoch 13  84.9% | batch:       146 of       172\t|\tloss: 0.529938\n",
      "Evaluating Epoch 13  85.5% | batch:       147 of       172\t|\tloss: 0.380921\n",
      "Evaluating Epoch 13  86.0% | batch:       148 of       172\t|\tloss: 0.480585\n",
      "Evaluating Epoch 13  86.6% | batch:       149 of       172\t|\tloss: 0.442217\n",
      "Evaluating Epoch 13  87.2% | batch:       150 of       172\t|\tloss: 0.608914\n",
      "Evaluating Epoch 13  87.8% | batch:       151 of       172\t|\tloss: 0.611612\n",
      "Evaluating Epoch 13  88.4% | batch:       152 of       172\t|\tloss: 0.451823\n",
      "Evaluating Epoch 13  89.0% | batch:       153 of       172\t|\tloss: 0.607424\n",
      "Evaluating Epoch 13  89.5% | batch:       154 of       172\t|\tloss: 0.580253\n",
      "Evaluating Epoch 13  90.1% | batch:       155 of       172\t|\tloss: 0.499551\n",
      "Evaluating Epoch 13  90.7% | batch:       156 of       172\t|\tloss: 0.711387\n",
      "Evaluating Epoch 13  91.3% | batch:       157 of       172\t|\tloss: 0.672661\n",
      "Evaluating Epoch 13  91.9% | batch:       158 of       172\t|\tloss: 0.616061\n",
      "Evaluating Epoch 13  92.4% | batch:       159 of       172\t|\tloss: 1.07048\n",
      "Evaluating Epoch 13  93.0% | batch:       160 of       172\t|\tloss: 1.48065\n",
      "Evaluating Epoch 13  93.6% | batch:       161 of       172\t|\tloss: 1.72803\n",
      "Evaluating Epoch 13  94.2% | batch:       162 of       172\t|\tloss: 0.639292\n",
      "Evaluating Epoch 13  94.8% | batch:       163 of       172\t|\tloss: 0.575035\n",
      "Evaluating Epoch 13  95.3% | batch:       164 of       172\t|\tloss: 0.814286\n",
      "Evaluating Epoch 13  95.9% | batch:       165 of       172\t|\tloss: 0.392016\n",
      "Evaluating Epoch 13  96.5% | batch:       166 of       172\t|\tloss: 0.473351\n",
      "Evaluating Epoch 13  97.1% | batch:       167 of       172\t|\tloss: 0.848597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:05:51,794 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 4.181293964385986 seconds\n",
      "\n",
      "2023-05-25 22:05:51,798 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 3.9719100509371077 seconds\n",
      "2023-05-25 22:05:51,800 | INFO : Avg batch val. time: 0.023092500296145974 seconds\n",
      "2023-05-25 22:05:51,802 | INFO : Avg sample val. time: 0.0001808949333213603 seconds\n",
      "2023-05-25 22:05:51,805 | INFO : Epoch 13 Validation Summary: epoch: 13.000000 | loss: 1.545589 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 13  97.7% | batch:       168 of       172\t|\tloss: 0.476369\n",
      "Evaluating Epoch 13  98.3% | batch:       169 of       172\t|\tloss: 0.631759\n",
      "Evaluating Epoch 13  98.8% | batch:       170 of       172\t|\tloss: 0.737975\n",
      "Evaluating Epoch 13  99.4% | batch:       171 of       172\t|\tloss: 0.379666\n",
      "\n",
      "Training Epoch 14   0.0% | batch:         0 of       686\t|\tloss: 4.14125\n",
      "Training Epoch 14   0.1% | batch:         1 of       686\t|\tloss: 5.55368\n",
      "Training Epoch 14   0.3% | batch:         2 of       686\t|\tloss: 4.44213\n",
      "Training Epoch 14   0.4% | batch:         3 of       686\t|\tloss: 3.53192\n",
      "Training Epoch 14   0.6% | batch:         4 of       686\t|\tloss: 4.20049\n",
      "Training Epoch 14   0.7% | batch:         5 of       686\t|\tloss: 3.8261\n",
      "Training Epoch 14   0.9% | batch:         6 of       686\t|\tloss: 4.44555\n",
      "Training Epoch 14   1.0% | batch:         7 of       686\t|\tloss: 3.99894\n",
      "Training Epoch 14   1.2% | batch:         8 of       686\t|\tloss: 4.4496\n",
      "Training Epoch 14   1.3% | batch:         9 of       686\t|\tloss: 5.06244\n",
      "Training Epoch 14   1.5% | batch:        10 of       686\t|\tloss: 3.92859\n",
      "Training Epoch 14   1.6% | batch:        11 of       686\t|\tloss: 3.77607\n",
      "Training Epoch 14   1.7% | batch:        12 of       686\t|\tloss: 3.28215\n",
      "Training Epoch 14   1.9% | batch:        13 of       686\t|\tloss: 5.50122\n",
      "Training Epoch 14   2.0% | batch:        14 of       686\t|\tloss: 3.55822\n",
      "Training Epoch 14   2.2% | batch:        15 of       686\t|\tloss: 8.23968\n",
      "Training Epoch 14   2.3% | batch:        16 of       686\t|\tloss: 4.25739\n",
      "Training Epoch 14   2.5% | batch:        17 of       686\t|\tloss: 4.4043\n",
      "Training Epoch 14   2.6% | batch:        18 of       686\t|\tloss: 4.71149\n",
      "Training Epoch 14   2.8% | batch:        19 of       686\t|\tloss: 2.89748\n",
      "Training Epoch 14   2.9% | batch:        20 of       686\t|\tloss: 5.45067\n",
      "Training Epoch 14   3.1% | batch:        21 of       686\t|\tloss: 4.18228\n",
      "Training Epoch 14   3.2% | batch:        22 of       686\t|\tloss: 4.71195\n",
      "Training Epoch 14   3.4% | batch:        23 of       686\t|\tloss: 4.5281\n",
      "Training Epoch 14   3.5% | batch:        24 of       686\t|\tloss: 3.97681\n",
      "Training Epoch 14   3.6% | batch:        25 of       686\t|\tloss: 4.08561\n",
      "Training Epoch 14   3.8% | batch:        26 of       686\t|\tloss: 4.13609\n",
      "Training Epoch 14   3.9% | batch:        27 of       686\t|\tloss: 3.62209\n",
      "Training Epoch 14   4.1% | batch:        28 of       686\t|\tloss: 3.59035\n",
      "Training Epoch 14   4.2% | batch:        29 of       686\t|\tloss: 4.00552\n",
      "Training Epoch 14   4.4% | batch:        30 of       686\t|\tloss: 6.14698\n",
      "Training Epoch 14   4.5% | batch:        31 of       686\t|\tloss: 3.81445\n",
      "Training Epoch 14   4.7% | batch:        32 of       686\t|\tloss: 3.89962\n",
      "Training Epoch 14   4.8% | batch:        33 of       686\t|\tloss: 4.59974\n",
      "Training Epoch 14   5.0% | batch:        34 of       686\t|\tloss: 4.95047\n",
      "Training Epoch 14   5.1% | batch:        35 of       686\t|\tloss: 3.83945\n",
      "Training Epoch 14   5.2% | batch:        36 of       686\t|\tloss: 5.72848\n",
      "Training Epoch 14   5.4% | batch:        37 of       686\t|\tloss: 3.7773\n",
      "Training Epoch 14   5.5% | batch:        38 of       686\t|\tloss: 4.30236\n",
      "Training Epoch 14   5.7% | batch:        39 of       686\t|\tloss: 3.85001\n",
      "Training Epoch 14   5.8% | batch:        40 of       686\t|\tloss: 3.41358\n",
      "Training Epoch 14   6.0% | batch:        41 of       686\t|\tloss: 3.83983\n",
      "Training Epoch 14   6.1% | batch:        42 of       686\t|\tloss: 4.03783\n",
      "Training Epoch 14   6.3% | batch:        43 of       686\t|\tloss: 4.97814\n",
      "Training Epoch 14   6.4% | batch:        44 of       686\t|\tloss: 4.1468\n",
      "Training Epoch 14   6.6% | batch:        45 of       686\t|\tloss: 5.1884\n",
      "Training Epoch 14   6.7% | batch:        46 of       686\t|\tloss: 4.24418\n",
      "Training Epoch 14   6.9% | batch:        47 of       686\t|\tloss: 4.93441\n",
      "Training Epoch 14   7.0% | batch:        48 of       686\t|\tloss: 4.53299\n",
      "Training Epoch 14   7.1% | batch:        49 of       686\t|\tloss: 4.8154\n",
      "Training Epoch 14   7.3% | batch:        50 of       686\t|\tloss: 4.02122\n",
      "Training Epoch 14   7.4% | batch:        51 of       686\t|\tloss: 3.99751\n",
      "Training Epoch 14   7.6% | batch:        52 of       686\t|\tloss: 4.86509\n",
      "Training Epoch 14   7.7% | batch:        53 of       686\t|\tloss: 4.23646\n",
      "Training Epoch 14   7.9% | batch:        54 of       686\t|\tloss: 4.51347\n",
      "Training Epoch 14   8.0% | batch:        55 of       686\t|\tloss: 3.42282\n",
      "Training Epoch 14   8.2% | batch:        56 of       686\t|\tloss: 4.64765\n",
      "Training Epoch 14   8.3% | batch:        57 of       686\t|\tloss: 4.59062\n",
      "Training Epoch 14   8.5% | batch:        58 of       686\t|\tloss: 5.11368\n",
      "Training Epoch 14   8.6% | batch:        59 of       686\t|\tloss: 3.1275\n",
      "Training Epoch 14   8.7% | batch:        60 of       686\t|\tloss: 4.90889\n",
      "Training Epoch 14   8.9% | batch:        61 of       686\t|\tloss: 4.59866\n",
      "Training Epoch 14   9.0% | batch:        62 of       686\t|\tloss: 3.66835\n",
      "Training Epoch 14   9.2% | batch:        63 of       686\t|\tloss: 6.01698\n",
      "Training Epoch 14   9.3% | batch:        64 of       686\t|\tloss: 5.02899\n",
      "Training Epoch 14   9.5% | batch:        65 of       686\t|\tloss: 4.4599\n",
      "Training Epoch 14   9.6% | batch:        66 of       686\t|\tloss: 4.35801\n",
      "Training Epoch 14   9.8% | batch:        67 of       686\t|\tloss: 4.04069\n",
      "Training Epoch 14   9.9% | batch:        68 of       686\t|\tloss: 6.14411\n",
      "Training Epoch 14  10.1% | batch:        69 of       686\t|\tloss: 2.96153\n",
      "Training Epoch 14  10.2% | batch:        70 of       686\t|\tloss: 3.72352\n",
      "Training Epoch 14  10.3% | batch:        71 of       686\t|\tloss: 3.15041\n",
      "Training Epoch 14  10.5% | batch:        72 of       686\t|\tloss: 5.36169\n",
      "Training Epoch 14  10.6% | batch:        73 of       686\t|\tloss: 4.54501\n",
      "Training Epoch 14  10.8% | batch:        74 of       686\t|\tloss: 3.78336\n",
      "Training Epoch 14  10.9% | batch:        75 of       686\t|\tloss: 5.07103\n",
      "Training Epoch 14  11.1% | batch:        76 of       686\t|\tloss: 3.94961\n",
      "Training Epoch 14  11.2% | batch:        77 of       686\t|\tloss: 4.35259\n",
      "Training Epoch 14  11.4% | batch:        78 of       686\t|\tloss: 3.47549\n",
      "Training Epoch 14  11.5% | batch:        79 of       686\t|\tloss: 4.80681\n",
      "Training Epoch 14  11.7% | batch:        80 of       686\t|\tloss: 3.40983\n",
      "Training Epoch 14  11.8% | batch:        81 of       686\t|\tloss: 4.39084\n",
      "Training Epoch 14  12.0% | batch:        82 of       686\t|\tloss: 7.05638\n",
      "Training Epoch 14  12.1% | batch:        83 of       686\t|\tloss: 4.38484\n",
      "Training Epoch 14  12.2% | batch:        84 of       686\t|\tloss: 3.3513\n",
      "Training Epoch 14  12.4% | batch:        85 of       686\t|\tloss: 4.75017\n",
      "Training Epoch 14  12.5% | batch:        86 of       686\t|\tloss: 4.92436\n",
      "Training Epoch 14  12.7% | batch:        87 of       686\t|\tloss: 4.80009\n",
      "Training Epoch 14  12.8% | batch:        88 of       686\t|\tloss: 3.98517\n",
      "Training Epoch 14  13.0% | batch:        89 of       686\t|\tloss: 5.17703\n",
      "Training Epoch 14  13.1% | batch:        90 of       686\t|\tloss: 4.69281\n",
      "Training Epoch 14  13.3% | batch:        91 of       686\t|\tloss: 3.85147\n",
      "Training Epoch 14  13.4% | batch:        92 of       686\t|\tloss: 4.08482\n",
      "Training Epoch 14  13.6% | batch:        93 of       686\t|\tloss: 4.2122\n",
      "Training Epoch 14  13.7% | batch:        94 of       686\t|\tloss: 3.83309\n",
      "Training Epoch 14  13.8% | batch:        95 of       686\t|\tloss: 5.7539\n",
      "Training Epoch 14  14.0% | batch:        96 of       686\t|\tloss: 4.16322\n",
      "Training Epoch 14  14.1% | batch:        97 of       686\t|\tloss: 4.18915\n",
      "Training Epoch 14  14.3% | batch:        98 of       686\t|\tloss: 6.20857\n",
      "Training Epoch 14  14.4% | batch:        99 of       686\t|\tloss: 3.19921\n",
      "Training Epoch 14  14.6% | batch:       100 of       686\t|\tloss: 5.43432\n",
      "Training Epoch 14  14.7% | batch:       101 of       686\t|\tloss: 4.77442\n",
      "Training Epoch 14  14.9% | batch:       102 of       686\t|\tloss: 5.50912\n",
      "Training Epoch 14  15.0% | batch:       103 of       686\t|\tloss: 3.77712\n",
      "Training Epoch 14  15.2% | batch:       104 of       686\t|\tloss: 5.11207\n",
      "Training Epoch 14  15.3% | batch:       105 of       686\t|\tloss: 4.58013\n",
      "Training Epoch 14  15.5% | batch:       106 of       686\t|\tloss: 4.71361\n",
      "Training Epoch 14  15.6% | batch:       107 of       686\t|\tloss: 3.9159\n",
      "Training Epoch 14  15.7% | batch:       108 of       686\t|\tloss: 4.91427\n",
      "Training Epoch 14  15.9% | batch:       109 of       686\t|\tloss: 4.91139\n",
      "Training Epoch 14  16.0% | batch:       110 of       686\t|\tloss: 4.29084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 14  16.2% | batch:       111 of       686\t|\tloss: 4.58042\n",
      "Training Epoch 14  16.3% | batch:       112 of       686\t|\tloss: 4.19027\n",
      "Training Epoch 14  16.5% | batch:       113 of       686\t|\tloss: 3.82323\n",
      "Training Epoch 14  16.6% | batch:       114 of       686\t|\tloss: 4.82526\n",
      "Training Epoch 14  16.8% | batch:       115 of       686\t|\tloss: 4.05881\n",
      "Training Epoch 14  16.9% | batch:       116 of       686\t|\tloss: 3.73901\n",
      "Training Epoch 14  17.1% | batch:       117 of       686\t|\tloss: 4.5448\n",
      "Training Epoch 14  17.2% | batch:       118 of       686\t|\tloss: 4.38\n",
      "Training Epoch 14  17.3% | batch:       119 of       686\t|\tloss: 3.56392\n",
      "Training Epoch 14  17.5% | batch:       120 of       686\t|\tloss: 4.13399\n",
      "Training Epoch 14  17.6% | batch:       121 of       686\t|\tloss: 3.59488\n",
      "Training Epoch 14  17.8% | batch:       122 of       686\t|\tloss: 3.34906\n",
      "Training Epoch 14  17.9% | batch:       123 of       686\t|\tloss: 5.16987\n",
      "Training Epoch 14  18.1% | batch:       124 of       686\t|\tloss: 4.18087\n",
      "Training Epoch 14  18.2% | batch:       125 of       686\t|\tloss: 4.16149\n",
      "Training Epoch 14  18.4% | batch:       126 of       686\t|\tloss: 5.10394\n",
      "Training Epoch 14  18.5% | batch:       127 of       686\t|\tloss: 4.05829\n",
      "Training Epoch 14  18.7% | batch:       128 of       686\t|\tloss: 3.93378\n",
      "Training Epoch 14  18.8% | batch:       129 of       686\t|\tloss: 3.50904\n",
      "Training Epoch 14  19.0% | batch:       130 of       686\t|\tloss: 5.40084\n",
      "Training Epoch 14  19.1% | batch:       131 of       686\t|\tloss: 4.86437\n",
      "Training Epoch 14  19.2% | batch:       132 of       686\t|\tloss: 5.47671\n",
      "Training Epoch 14  19.4% | batch:       133 of       686\t|\tloss: 4.99404\n",
      "Training Epoch 14  19.5% | batch:       134 of       686\t|\tloss: 5.82074\n",
      "Training Epoch 14  19.7% | batch:       135 of       686\t|\tloss: 3.94499\n",
      "Training Epoch 14  19.8% | batch:       136 of       686\t|\tloss: 4.11968\n",
      "Training Epoch 14  20.0% | batch:       137 of       686\t|\tloss: 4.69663\n",
      "Training Epoch 14  20.1% | batch:       138 of       686\t|\tloss: 5.0999\n",
      "Training Epoch 14  20.3% | batch:       139 of       686\t|\tloss: 5.90438\n",
      "Training Epoch 14  20.4% | batch:       140 of       686\t|\tloss: 3.36956\n",
      "Training Epoch 14  20.6% | batch:       141 of       686\t|\tloss: 3.77199\n",
      "Training Epoch 14  20.7% | batch:       142 of       686\t|\tloss: 5.10146\n",
      "Training Epoch 14  20.8% | batch:       143 of       686\t|\tloss: 3.95877\n",
      "Training Epoch 14  21.0% | batch:       144 of       686\t|\tloss: 3.7374\n",
      "Training Epoch 14  21.1% | batch:       145 of       686\t|\tloss: 3.79997\n",
      "Training Epoch 14  21.3% | batch:       146 of       686\t|\tloss: 4.11342\n",
      "Training Epoch 14  21.4% | batch:       147 of       686\t|\tloss: 3.91551\n",
      "Training Epoch 14  21.6% | batch:       148 of       686\t|\tloss: 5.40259\n",
      "Training Epoch 14  21.7% | batch:       149 of       686\t|\tloss: 5.68941\n",
      "Training Epoch 14  21.9% | batch:       150 of       686\t|\tloss: 3.48633\n",
      "Training Epoch 14  22.0% | batch:       151 of       686\t|\tloss: 4.35457\n",
      "Training Epoch 14  22.2% | batch:       152 of       686\t|\tloss: 3.63452\n",
      "Training Epoch 14  22.3% | batch:       153 of       686\t|\tloss: 4.1135\n",
      "Training Epoch 14  22.4% | batch:       154 of       686\t|\tloss: 3.94635\n",
      "Training Epoch 14  22.6% | batch:       155 of       686\t|\tloss: 4.0388\n",
      "Training Epoch 14  22.7% | batch:       156 of       686\t|\tloss: 3.45988\n",
      "Training Epoch 14  22.9% | batch:       157 of       686\t|\tloss: 4.28822\n",
      "Training Epoch 14  23.0% | batch:       158 of       686\t|\tloss: 4.31799\n",
      "Training Epoch 14  23.2% | batch:       159 of       686\t|\tloss: 3.20927\n",
      "Training Epoch 14  23.3% | batch:       160 of       686\t|\tloss: 3.87849\n",
      "Training Epoch 14  23.5% | batch:       161 of       686\t|\tloss: 5.01023\n",
      "Training Epoch 14  23.6% | batch:       162 of       686\t|\tloss: 3.75819\n",
      "Training Epoch 14  23.8% | batch:       163 of       686\t|\tloss: 3.83867\n",
      "Training Epoch 14  23.9% | batch:       164 of       686\t|\tloss: 4.56028\n",
      "Training Epoch 14  24.1% | batch:       165 of       686\t|\tloss: 5.1789\n",
      "Training Epoch 14  24.2% | batch:       166 of       686\t|\tloss: 5.30503\n",
      "Training Epoch 14  24.3% | batch:       167 of       686\t|\tloss: 4.60507\n",
      "Training Epoch 14  24.5% | batch:       168 of       686\t|\tloss: 3.44932\n",
      "Training Epoch 14  24.6% | batch:       169 of       686\t|\tloss: 3.6517\n",
      "Training Epoch 14  24.8% | batch:       170 of       686\t|\tloss: 4.38889\n",
      "Training Epoch 14  24.9% | batch:       171 of       686\t|\tloss: 2.93222\n",
      "Training Epoch 14  25.1% | batch:       172 of       686\t|\tloss: 4.37876\n",
      "Training Epoch 14  25.2% | batch:       173 of       686\t|\tloss: 4.64871\n",
      "Training Epoch 14  25.4% | batch:       174 of       686\t|\tloss: 2.95947\n",
      "Training Epoch 14  25.5% | batch:       175 of       686\t|\tloss: 3.66227\n",
      "Training Epoch 14  25.7% | batch:       176 of       686\t|\tloss: 3.66026\n",
      "Training Epoch 14  25.8% | batch:       177 of       686\t|\tloss: 3.96113\n",
      "Training Epoch 14  25.9% | batch:       178 of       686\t|\tloss: 4.383\n",
      "Training Epoch 14  26.1% | batch:       179 of       686\t|\tloss: 3.85527\n",
      "Training Epoch 14  26.2% | batch:       180 of       686\t|\tloss: 3.38545\n",
      "Training Epoch 14  26.4% | batch:       181 of       686\t|\tloss: 4.07605\n",
      "Training Epoch 14  26.5% | batch:       182 of       686\t|\tloss: 4.01337\n",
      "Training Epoch 14  26.7% | batch:       183 of       686\t|\tloss: 3.99351\n",
      "Training Epoch 14  26.8% | batch:       184 of       686\t|\tloss: 4.2381\n",
      "Training Epoch 14  27.0% | batch:       185 of       686\t|\tloss: 3.49649\n",
      "Training Epoch 14  27.1% | batch:       186 of       686\t|\tloss: 3.82936\n",
      "Training Epoch 14  27.3% | batch:       187 of       686\t|\tloss: 3.92785\n",
      "Training Epoch 14  27.4% | batch:       188 of       686\t|\tloss: 3.14\n",
      "Training Epoch 14  27.6% | batch:       189 of       686\t|\tloss: 6.1267\n",
      "Training Epoch 14  27.7% | batch:       190 of       686\t|\tloss: 4.91822\n",
      "Training Epoch 14  27.8% | batch:       191 of       686\t|\tloss: 4.10681\n",
      "Training Epoch 14  28.0% | batch:       192 of       686\t|\tloss: 4.70077\n",
      "Training Epoch 14  28.1% | batch:       193 of       686\t|\tloss: 4.47336\n",
      "Training Epoch 14  28.3% | batch:       194 of       686\t|\tloss: 4.18466\n",
      "Training Epoch 14  28.4% | batch:       195 of       686\t|\tloss: 3.53323\n",
      "Training Epoch 14  28.6% | batch:       196 of       686\t|\tloss: 3.95442\n",
      "Training Epoch 14  28.7% | batch:       197 of       686\t|\tloss: 5.2601\n",
      "Training Epoch 14  28.9% | batch:       198 of       686\t|\tloss: 6.886\n",
      "Training Epoch 14  29.0% | batch:       199 of       686\t|\tloss: 5.28311\n",
      "Training Epoch 14  29.2% | batch:       200 of       686\t|\tloss: 4.04163\n",
      "Training Epoch 14  29.3% | batch:       201 of       686\t|\tloss: 4.3792\n",
      "Training Epoch 14  29.4% | batch:       202 of       686\t|\tloss: 4.05744\n",
      "Training Epoch 14  29.6% | batch:       203 of       686\t|\tloss: 4.19614\n",
      "Training Epoch 14  29.7% | batch:       204 of       686\t|\tloss: 4.95638\n",
      "Training Epoch 14  29.9% | batch:       205 of       686\t|\tloss: 4.0732\n",
      "Training Epoch 14  30.0% | batch:       206 of       686\t|\tloss: 4.80586\n",
      "Training Epoch 14  30.2% | batch:       207 of       686\t|\tloss: 6.1333\n",
      "Training Epoch 14  30.3% | batch:       208 of       686\t|\tloss: 4.51141\n",
      "Training Epoch 14  30.5% | batch:       209 of       686\t|\tloss: 3.98189\n",
      "Training Epoch 14  30.6% | batch:       210 of       686\t|\tloss: 5.30648\n",
      "Training Epoch 14  30.8% | batch:       211 of       686\t|\tloss: 4.64327\n",
      "Training Epoch 14  30.9% | batch:       212 of       686\t|\tloss: 4.19998\n",
      "Training Epoch 14  31.0% | batch:       213 of       686\t|\tloss: 3.81596\n",
      "Training Epoch 14  31.2% | batch:       214 of       686\t|\tloss: 3.82898\n",
      "Training Epoch 14  31.3% | batch:       215 of       686\t|\tloss: 4.64021\n",
      "Training Epoch 14  31.5% | batch:       216 of       686\t|\tloss: 5.57181\n",
      "Training Epoch 14  31.6% | batch:       217 of       686\t|\tloss: 3.49419\n",
      "Training Epoch 14  31.8% | batch:       218 of       686\t|\tloss: 4.17243\n",
      "Training Epoch 14  31.9% | batch:       219 of       686\t|\tloss: 4.13653\n",
      "Training Epoch 14  32.1% | batch:       220 of       686\t|\tloss: 4.41423\n",
      "Training Epoch 14  32.2% | batch:       221 of       686\t|\tloss: 4.79432\n",
      "Training Epoch 14  32.4% | batch:       222 of       686\t|\tloss: 3.18026\n",
      "Training Epoch 14  32.5% | batch:       223 of       686\t|\tloss: 5.80826\n",
      "Training Epoch 14  32.7% | batch:       224 of       686\t|\tloss: 3.42124\n",
      "Training Epoch 14  32.8% | batch:       225 of       686\t|\tloss: 4.40634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 14  32.9% | batch:       226 of       686\t|\tloss: 5.73184\n",
      "Training Epoch 14  33.1% | batch:       227 of       686\t|\tloss: 3.87256\n",
      "Training Epoch 14  33.2% | batch:       228 of       686\t|\tloss: 4.05768\n",
      "Training Epoch 14  33.4% | batch:       229 of       686\t|\tloss: 4.44535\n",
      "Training Epoch 14  33.5% | batch:       230 of       686\t|\tloss: 4.08174\n",
      "Training Epoch 14  33.7% | batch:       231 of       686\t|\tloss: 4.12617\n",
      "Training Epoch 14  33.8% | batch:       232 of       686\t|\tloss: 3.92012\n",
      "Training Epoch 14  34.0% | batch:       233 of       686\t|\tloss: 4.29544\n",
      "Training Epoch 14  34.1% | batch:       234 of       686\t|\tloss: 4.96737\n",
      "Training Epoch 14  34.3% | batch:       235 of       686\t|\tloss: 4.45899\n",
      "Training Epoch 14  34.4% | batch:       236 of       686\t|\tloss: 4.48106\n",
      "Training Epoch 14  34.5% | batch:       237 of       686\t|\tloss: 4.95533\n",
      "Training Epoch 14  34.7% | batch:       238 of       686\t|\tloss: 4.08442\n",
      "Training Epoch 14  34.8% | batch:       239 of       686\t|\tloss: 4.18675\n",
      "Training Epoch 14  35.0% | batch:       240 of       686\t|\tloss: 3.94055\n",
      "Training Epoch 14  35.1% | batch:       241 of       686\t|\tloss: 4.43538\n",
      "Training Epoch 14  35.3% | batch:       242 of       686\t|\tloss: 5.87011\n",
      "Training Epoch 14  35.4% | batch:       243 of       686\t|\tloss: 5.1454\n",
      "Training Epoch 14  35.6% | batch:       244 of       686\t|\tloss: 3.42547\n",
      "Training Epoch 14  35.7% | batch:       245 of       686\t|\tloss: 5.32533\n",
      "Training Epoch 14  35.9% | batch:       246 of       686\t|\tloss: 4.06272\n",
      "Training Epoch 14  36.0% | batch:       247 of       686\t|\tloss: 3.42274\n",
      "Training Epoch 14  36.2% | batch:       248 of       686\t|\tloss: 4.39614\n",
      "Training Epoch 14  36.3% | batch:       249 of       686\t|\tloss: 4.0105\n",
      "Training Epoch 14  36.4% | batch:       250 of       686\t|\tloss: 5.0382\n",
      "Training Epoch 14  36.6% | batch:       251 of       686\t|\tloss: 3.99895\n",
      "Training Epoch 14  36.7% | batch:       252 of       686\t|\tloss: 5.53242\n",
      "Training Epoch 14  36.9% | batch:       253 of       686\t|\tloss: 4.30702\n",
      "Training Epoch 14  37.0% | batch:       254 of       686\t|\tloss: 3.05721\n",
      "Training Epoch 14  37.2% | batch:       255 of       686\t|\tloss: 4.1653\n",
      "Training Epoch 14  37.3% | batch:       256 of       686\t|\tloss: 3.26489\n",
      "Training Epoch 14  37.5% | batch:       257 of       686\t|\tloss: 4.11701\n",
      "Training Epoch 14  37.6% | batch:       258 of       686\t|\tloss: 4.20253\n",
      "Training Epoch 14  37.8% | batch:       259 of       686\t|\tloss: 4.30361\n",
      "Training Epoch 14  37.9% | batch:       260 of       686\t|\tloss: 4.46733\n",
      "Training Epoch 14  38.0% | batch:       261 of       686\t|\tloss: 4.06827\n",
      "Training Epoch 14  38.2% | batch:       262 of       686\t|\tloss: 6.00509\n",
      "Training Epoch 14  38.3% | batch:       263 of       686\t|\tloss: 4.69321\n",
      "Training Epoch 14  38.5% | batch:       264 of       686\t|\tloss: 3.85298\n",
      "Training Epoch 14  38.6% | batch:       265 of       686\t|\tloss: 3.90299\n",
      "Training Epoch 14  38.8% | batch:       266 of       686\t|\tloss: 3.69202\n",
      "Training Epoch 14  38.9% | batch:       267 of       686\t|\tloss: 5.32326\n",
      "Training Epoch 14  39.1% | batch:       268 of       686\t|\tloss: 3.78677\n",
      "Training Epoch 14  39.2% | batch:       269 of       686\t|\tloss: 5.32707\n",
      "Training Epoch 14  39.4% | batch:       270 of       686\t|\tloss: 3.72125\n",
      "Training Epoch 14  39.5% | batch:       271 of       686\t|\tloss: 6.40154\n",
      "Training Epoch 14  39.7% | batch:       272 of       686\t|\tloss: 3.29865\n",
      "Training Epoch 14  39.8% | batch:       273 of       686\t|\tloss: 4.08109\n",
      "Training Epoch 14  39.9% | batch:       274 of       686\t|\tloss: 3.55363\n",
      "Training Epoch 14  40.1% | batch:       275 of       686\t|\tloss: 4.7471\n",
      "Training Epoch 14  40.2% | batch:       276 of       686\t|\tloss: 3.75977\n",
      "Training Epoch 14  40.4% | batch:       277 of       686\t|\tloss: 4.07634\n",
      "Training Epoch 14  40.5% | batch:       278 of       686\t|\tloss: 5.20591\n",
      "Training Epoch 14  40.7% | batch:       279 of       686\t|\tloss: 3.50044\n",
      "Training Epoch 14  40.8% | batch:       280 of       686\t|\tloss: 4.5408\n",
      "Training Epoch 14  41.0% | batch:       281 of       686\t|\tloss: 3.48473\n",
      "Training Epoch 14  41.1% | batch:       282 of       686\t|\tloss: 4.51046\n",
      "Training Epoch 14  41.3% | batch:       283 of       686\t|\tloss: 5.07349\n",
      "Training Epoch 14  41.4% | batch:       284 of       686\t|\tloss: 3.04426\n",
      "Training Epoch 14  41.5% | batch:       285 of       686\t|\tloss: 4.4054\n",
      "Training Epoch 14  41.7% | batch:       286 of       686\t|\tloss: 2.92808\n",
      "Training Epoch 14  41.8% | batch:       287 of       686\t|\tloss: 3.38938\n",
      "Training Epoch 14  42.0% | batch:       288 of       686\t|\tloss: 5.03186\n",
      "Training Epoch 14  42.1% | batch:       289 of       686\t|\tloss: 4.56648\n",
      "Training Epoch 14  42.3% | batch:       290 of       686\t|\tloss: 3.61697\n",
      "Training Epoch 14  42.4% | batch:       291 of       686\t|\tloss: 4.22672\n",
      "Training Epoch 14  42.6% | batch:       292 of       686\t|\tloss: 4.94753\n",
      "Training Epoch 14  42.7% | batch:       293 of       686\t|\tloss: 4.58208\n",
      "Training Epoch 14  42.9% | batch:       294 of       686\t|\tloss: 4.73205\n",
      "Training Epoch 14  43.0% | batch:       295 of       686\t|\tloss: 3.79631\n",
      "Training Epoch 14  43.1% | batch:       296 of       686\t|\tloss: 4.5569\n",
      "Training Epoch 14  43.3% | batch:       297 of       686\t|\tloss: 5.39729\n",
      "Training Epoch 14  43.4% | batch:       298 of       686\t|\tloss: 3.40602\n",
      "Training Epoch 14  43.6% | batch:       299 of       686\t|\tloss: 3.66801\n",
      "Training Epoch 14  43.7% | batch:       300 of       686\t|\tloss: 5.72159\n",
      "Training Epoch 14  43.9% | batch:       301 of       686\t|\tloss: 3.83578\n",
      "Training Epoch 14  44.0% | batch:       302 of       686\t|\tloss: 4.57312\n",
      "Training Epoch 14  44.2% | batch:       303 of       686\t|\tloss: 3.79887\n",
      "Training Epoch 14  44.3% | batch:       304 of       686\t|\tloss: 3.83062\n",
      "Training Epoch 14  44.5% | batch:       305 of       686\t|\tloss: 3.91713\n",
      "Training Epoch 14  44.6% | batch:       306 of       686\t|\tloss: 7.99419\n",
      "Training Epoch 14  44.8% | batch:       307 of       686\t|\tloss: 3.968\n",
      "Training Epoch 14  44.9% | batch:       308 of       686\t|\tloss: 3.63815\n",
      "Training Epoch 14  45.0% | batch:       309 of       686\t|\tloss: 6.15701\n",
      "Training Epoch 14  45.2% | batch:       310 of       686\t|\tloss: 3.8789\n",
      "Training Epoch 14  45.3% | batch:       311 of       686\t|\tloss: 4.18446\n",
      "Training Epoch 14  45.5% | batch:       312 of       686\t|\tloss: 4.47854\n",
      "Training Epoch 14  45.6% | batch:       313 of       686\t|\tloss: 4.31189\n",
      "Training Epoch 14  45.8% | batch:       314 of       686\t|\tloss: 4.17866\n",
      "Training Epoch 14  45.9% | batch:       315 of       686\t|\tloss: 5.53986\n",
      "Training Epoch 14  46.1% | batch:       316 of       686\t|\tloss: 4.17558\n",
      "Training Epoch 14  46.2% | batch:       317 of       686\t|\tloss: 4.64507\n",
      "Training Epoch 14  46.4% | batch:       318 of       686\t|\tloss: 3.80795\n",
      "Training Epoch 14  46.5% | batch:       319 of       686\t|\tloss: 4.74419\n",
      "Training Epoch 14  46.6% | batch:       320 of       686\t|\tloss: 5.40895\n",
      "Training Epoch 14  46.8% | batch:       321 of       686\t|\tloss: 4.71298\n",
      "Training Epoch 14  46.9% | batch:       322 of       686\t|\tloss: 4.39252\n",
      "Training Epoch 14  47.1% | batch:       323 of       686\t|\tloss: 3.84064\n",
      "Training Epoch 14  47.2% | batch:       324 of       686\t|\tloss: 4.19182\n",
      "Training Epoch 14  47.4% | batch:       325 of       686\t|\tloss: 3.74407\n",
      "Training Epoch 14  47.5% | batch:       326 of       686\t|\tloss: 3.19593\n",
      "Training Epoch 14  47.7% | batch:       327 of       686\t|\tloss: 4.26315\n",
      "Training Epoch 14  47.8% | batch:       328 of       686\t|\tloss: 3.86694\n",
      "Training Epoch 14  48.0% | batch:       329 of       686\t|\tloss: 4.21859\n",
      "Training Epoch 14  48.1% | batch:       330 of       686\t|\tloss: 4.64224\n",
      "Training Epoch 14  48.3% | batch:       331 of       686\t|\tloss: 3.59807\n",
      "Training Epoch 14  48.4% | batch:       332 of       686\t|\tloss: 5.12918\n",
      "Training Epoch 14  48.5% | batch:       333 of       686\t|\tloss: 3.82211\n",
      "Training Epoch 14  48.7% | batch:       334 of       686\t|\tloss: 3.80346\n",
      "Training Epoch 14  48.8% | batch:       335 of       686\t|\tloss: 3.63557\n",
      "Training Epoch 14  49.0% | batch:       336 of       686\t|\tloss: 3.59773\n",
      "Training Epoch 14  49.1% | batch:       337 of       686\t|\tloss: 3.8498\n",
      "Training Epoch 14  49.3% | batch:       338 of       686\t|\tloss: 4.05376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 14  49.4% | batch:       339 of       686\t|\tloss: 6.24267\n",
      "Training Epoch 14  49.6% | batch:       340 of       686\t|\tloss: 4.09267\n",
      "Training Epoch 14  49.7% | batch:       341 of       686\t|\tloss: 5.0394\n",
      "Training Epoch 14  49.9% | batch:       342 of       686\t|\tloss: 3.99779\n",
      "Training Epoch 14  50.0% | batch:       343 of       686\t|\tloss: 3.51755\n",
      "Training Epoch 14  50.1% | batch:       344 of       686\t|\tloss: 3.94476\n",
      "Training Epoch 14  50.3% | batch:       345 of       686\t|\tloss: 3.01053\n",
      "Training Epoch 14  50.4% | batch:       346 of       686\t|\tloss: 4.60063\n",
      "Training Epoch 14  50.6% | batch:       347 of       686\t|\tloss: 3.33094\n",
      "Training Epoch 14  50.7% | batch:       348 of       686\t|\tloss: 3.92047\n",
      "Training Epoch 14  50.9% | batch:       349 of       686\t|\tloss: 4.97529\n",
      "Training Epoch 14  51.0% | batch:       350 of       686\t|\tloss: 5.52419\n",
      "Training Epoch 14  51.2% | batch:       351 of       686\t|\tloss: 4.27503\n",
      "Training Epoch 14  51.3% | batch:       352 of       686\t|\tloss: 3.99927\n",
      "Training Epoch 14  51.5% | batch:       353 of       686\t|\tloss: 3.2976\n",
      "Training Epoch 14  51.6% | batch:       354 of       686\t|\tloss: 5.51613\n",
      "Training Epoch 14  51.7% | batch:       355 of       686\t|\tloss: 4.24112\n",
      "Training Epoch 14  51.9% | batch:       356 of       686\t|\tloss: 4.34728\n",
      "Training Epoch 14  52.0% | batch:       357 of       686\t|\tloss: 4.62199\n",
      "Training Epoch 14  52.2% | batch:       358 of       686\t|\tloss: 5.22868\n",
      "Training Epoch 14  52.3% | batch:       359 of       686\t|\tloss: 4.01801\n",
      "Training Epoch 14  52.5% | batch:       360 of       686\t|\tloss: 3.9735\n",
      "Training Epoch 14  52.6% | batch:       361 of       686\t|\tloss: 3.64779\n",
      "Training Epoch 14  52.8% | batch:       362 of       686\t|\tloss: 5.05163\n",
      "Training Epoch 14  52.9% | batch:       363 of       686\t|\tloss: 4.2258\n",
      "Training Epoch 14  53.1% | batch:       364 of       686\t|\tloss: 5.15369\n",
      "Training Epoch 14  53.2% | batch:       365 of       686\t|\tloss: 4.00281\n",
      "Training Epoch 14  53.4% | batch:       366 of       686\t|\tloss: 3.75966\n",
      "Training Epoch 14  53.5% | batch:       367 of       686\t|\tloss: 3.41862\n",
      "Training Epoch 14  53.6% | batch:       368 of       686\t|\tloss: 5.87799\n",
      "Training Epoch 14  53.8% | batch:       369 of       686\t|\tloss: 4.5946\n",
      "Training Epoch 14  53.9% | batch:       370 of       686\t|\tloss: 4.33923\n",
      "Training Epoch 14  54.1% | batch:       371 of       686\t|\tloss: 3.67125\n",
      "Training Epoch 14  54.2% | batch:       372 of       686\t|\tloss: 4.33881\n",
      "Training Epoch 14  54.4% | batch:       373 of       686\t|\tloss: 3.61846\n",
      "Training Epoch 14  54.5% | batch:       374 of       686\t|\tloss: 4.17344\n",
      "Training Epoch 14  54.7% | batch:       375 of       686\t|\tloss: 5.09529\n",
      "Training Epoch 14  54.8% | batch:       376 of       686\t|\tloss: 4.19821\n",
      "Training Epoch 14  55.0% | batch:       377 of       686\t|\tloss: 3.85888\n",
      "Training Epoch 14  55.1% | batch:       378 of       686\t|\tloss: 3.61919\n",
      "Training Epoch 14  55.2% | batch:       379 of       686\t|\tloss: 3.44837\n",
      "Training Epoch 14  55.4% | batch:       380 of       686\t|\tloss: 4.65581\n",
      "Training Epoch 14  55.5% | batch:       381 of       686\t|\tloss: 4.8879\n",
      "Training Epoch 14  55.7% | batch:       382 of       686\t|\tloss: 4.47535\n",
      "Training Epoch 14  55.8% | batch:       383 of       686\t|\tloss: 5.07103\n",
      "Training Epoch 14  56.0% | batch:       384 of       686\t|\tloss: 4.82314\n",
      "Training Epoch 14  56.1% | batch:       385 of       686\t|\tloss: 4.15753\n",
      "Training Epoch 14  56.3% | batch:       386 of       686\t|\tloss: 3.42212\n",
      "Training Epoch 14  56.4% | batch:       387 of       686\t|\tloss: 3.24365\n",
      "Training Epoch 14  56.6% | batch:       388 of       686\t|\tloss: 4.02909\n",
      "Training Epoch 14  56.7% | batch:       389 of       686\t|\tloss: 3.95801\n",
      "Training Epoch 14  56.9% | batch:       390 of       686\t|\tloss: 3.98689\n",
      "Training Epoch 14  57.0% | batch:       391 of       686\t|\tloss: 5.04599\n",
      "Training Epoch 14  57.1% | batch:       392 of       686\t|\tloss: 4.1612\n",
      "Training Epoch 14  57.3% | batch:       393 of       686\t|\tloss: 4.77827\n",
      "Training Epoch 14  57.4% | batch:       394 of       686\t|\tloss: 3.87447\n",
      "Training Epoch 14  57.6% | batch:       395 of       686\t|\tloss: 3.59942\n",
      "Training Epoch 14  57.7% | batch:       396 of       686\t|\tloss: 4.33587\n",
      "Training Epoch 14  57.9% | batch:       397 of       686\t|\tloss: 5.3125\n",
      "Training Epoch 14  58.0% | batch:       398 of       686\t|\tloss: 4.08731\n",
      "Training Epoch 14  58.2% | batch:       399 of       686\t|\tloss: 4.51713\n",
      "Training Epoch 14  58.3% | batch:       400 of       686\t|\tloss: 5.0514\n",
      "Training Epoch 14  58.5% | batch:       401 of       686\t|\tloss: 3.8756\n",
      "Training Epoch 14  58.6% | batch:       402 of       686\t|\tloss: 3.30934\n",
      "Training Epoch 14  58.7% | batch:       403 of       686\t|\tloss: 4.27981\n",
      "Training Epoch 14  58.9% | batch:       404 of       686\t|\tloss: 4.3412\n",
      "Training Epoch 14  59.0% | batch:       405 of       686\t|\tloss: 5.63328\n",
      "Training Epoch 14  59.2% | batch:       406 of       686\t|\tloss: 3.99094\n",
      "Training Epoch 14  59.3% | batch:       407 of       686\t|\tloss: 4.10816\n",
      "Training Epoch 14  59.5% | batch:       408 of       686\t|\tloss: 4.06902\n",
      "Training Epoch 14  59.6% | batch:       409 of       686\t|\tloss: 3.77797\n",
      "Training Epoch 14  59.8% | batch:       410 of       686\t|\tloss: 3.81068\n",
      "Training Epoch 14  59.9% | batch:       411 of       686\t|\tloss: 4.59045\n",
      "Training Epoch 14  60.1% | batch:       412 of       686\t|\tloss: 3.76423\n",
      "Training Epoch 14  60.2% | batch:       413 of       686\t|\tloss: 3.74534\n",
      "Training Epoch 14  60.3% | batch:       414 of       686\t|\tloss: 3.89074\n",
      "Training Epoch 14  60.5% | batch:       415 of       686\t|\tloss: 3.56107\n",
      "Training Epoch 14  60.6% | batch:       416 of       686\t|\tloss: 3.17003\n",
      "Training Epoch 14  60.8% | batch:       417 of       686\t|\tloss: 4.41564\n",
      "Training Epoch 14  60.9% | batch:       418 of       686\t|\tloss: 4.10348\n",
      "Training Epoch 14  61.1% | batch:       419 of       686\t|\tloss: 4.1755\n",
      "Training Epoch 14  61.2% | batch:       420 of       686\t|\tloss: 3.86648\n",
      "Training Epoch 14  61.4% | batch:       421 of       686\t|\tloss: 3.68914\n",
      "Training Epoch 14  61.5% | batch:       422 of       686\t|\tloss: 3.60905\n",
      "Training Epoch 14  61.7% | batch:       423 of       686\t|\tloss: 3.92945\n",
      "Training Epoch 14  61.8% | batch:       424 of       686\t|\tloss: 3.88981\n",
      "Training Epoch 14  62.0% | batch:       425 of       686\t|\tloss: 3.50796\n",
      "Training Epoch 14  62.1% | batch:       426 of       686\t|\tloss: 3.27818\n",
      "Training Epoch 14  62.2% | batch:       427 of       686\t|\tloss: 5.18643\n",
      "Training Epoch 14  62.4% | batch:       428 of       686\t|\tloss: 5.39456\n",
      "Training Epoch 14  62.5% | batch:       429 of       686\t|\tloss: 3.49226\n",
      "Training Epoch 14  62.7% | batch:       430 of       686\t|\tloss: 2.88223\n",
      "Training Epoch 14  62.8% | batch:       431 of       686\t|\tloss: 4.24794\n",
      "Training Epoch 14  63.0% | batch:       432 of       686\t|\tloss: 4.53218\n",
      "Training Epoch 14  63.1% | batch:       433 of       686\t|\tloss: 3.64098\n",
      "Training Epoch 14  63.3% | batch:       434 of       686\t|\tloss: 4.39534\n",
      "Training Epoch 14  63.4% | batch:       435 of       686\t|\tloss: 3.35873\n",
      "Training Epoch 14  63.6% | batch:       436 of       686\t|\tloss: 4.01827\n",
      "Training Epoch 14  63.7% | batch:       437 of       686\t|\tloss: 3.51015\n",
      "Training Epoch 14  63.8% | batch:       438 of       686\t|\tloss: 5.14849\n",
      "Training Epoch 14  64.0% | batch:       439 of       686\t|\tloss: 3.56699\n",
      "Training Epoch 14  64.1% | batch:       440 of       686\t|\tloss: 4.79905\n",
      "Training Epoch 14  64.3% | batch:       441 of       686\t|\tloss: 3.8499\n",
      "Training Epoch 14  64.4% | batch:       442 of       686\t|\tloss: 4.25563\n",
      "Training Epoch 14  64.6% | batch:       443 of       686\t|\tloss: 5.40504\n",
      "Training Epoch 14  64.7% | batch:       444 of       686\t|\tloss: 4.34678\n",
      "Training Epoch 14  64.9% | batch:       445 of       686\t|\tloss: 6.20839\n",
      "Training Epoch 14  65.0% | batch:       446 of       686\t|\tloss: 6.09846\n",
      "Training Epoch 14  65.2% | batch:       447 of       686\t|\tloss: 3.37109\n",
      "Training Epoch 14  65.3% | batch:       448 of       686\t|\tloss: 4.26492\n",
      "Training Epoch 14  65.5% | batch:       449 of       686\t|\tloss: 5.38442\n",
      "Training Epoch 14  65.6% | batch:       450 of       686\t|\tloss: 3.62084\n",
      "Training Epoch 14  65.7% | batch:       451 of       686\t|\tloss: 3.64984\n",
      "Training Epoch 14  65.9% | batch:       452 of       686\t|\tloss: 5.26764\n",
      "Training Epoch 14  66.0% | batch:       453 of       686\t|\tloss: 3.89907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 14  66.2% | batch:       454 of       686\t|\tloss: 5.54918\n",
      "Training Epoch 14  66.3% | batch:       455 of       686\t|\tloss: 3.61571\n",
      "Training Epoch 14  66.5% | batch:       456 of       686\t|\tloss: 4.40678\n",
      "Training Epoch 14  66.6% | batch:       457 of       686\t|\tloss: 4.6521\n",
      "Training Epoch 14  66.8% | batch:       458 of       686\t|\tloss: 4.42136\n",
      "Training Epoch 14  66.9% | batch:       459 of       686\t|\tloss: 4.7087\n",
      "Training Epoch 14  67.1% | batch:       460 of       686\t|\tloss: 3.46647\n",
      "Training Epoch 14  67.2% | batch:       461 of       686\t|\tloss: 2.78773\n",
      "Training Epoch 14  67.3% | batch:       462 of       686\t|\tloss: 3.43007\n",
      "Training Epoch 14  67.5% | batch:       463 of       686\t|\tloss: 3.11434\n",
      "Training Epoch 14  67.6% | batch:       464 of       686\t|\tloss: 5.02339\n",
      "Training Epoch 14  67.8% | batch:       465 of       686\t|\tloss: 2.92165\n",
      "Training Epoch 14  67.9% | batch:       466 of       686\t|\tloss: 3.98256\n",
      "Training Epoch 14  68.1% | batch:       467 of       686\t|\tloss: 4.02215\n",
      "Training Epoch 14  68.2% | batch:       468 of       686\t|\tloss: 4.51922\n",
      "Training Epoch 14  68.4% | batch:       469 of       686\t|\tloss: 4.50822\n",
      "Training Epoch 14  68.5% | batch:       470 of       686\t|\tloss: 3.77745\n",
      "Training Epoch 14  68.7% | batch:       471 of       686\t|\tloss: 3.64598\n",
      "Training Epoch 14  68.8% | batch:       472 of       686\t|\tloss: 3.2182\n",
      "Training Epoch 14  69.0% | batch:       473 of       686\t|\tloss: 3.47651\n",
      "Training Epoch 14  69.1% | batch:       474 of       686\t|\tloss: 3.2532\n",
      "Training Epoch 14  69.2% | batch:       475 of       686\t|\tloss: 5.27186\n",
      "Training Epoch 14  69.4% | batch:       476 of       686\t|\tloss: 4.01255\n",
      "Training Epoch 14  69.5% | batch:       477 of       686\t|\tloss: 3.65325\n",
      "Training Epoch 14  69.7% | batch:       478 of       686\t|\tloss: 5.22801\n",
      "Training Epoch 14  69.8% | batch:       479 of       686\t|\tloss: 3.49213\n",
      "Training Epoch 14  70.0% | batch:       480 of       686\t|\tloss: 3.57469\n",
      "Training Epoch 14  70.1% | batch:       481 of       686\t|\tloss: 3.39344\n",
      "Training Epoch 14  70.3% | batch:       482 of       686\t|\tloss: 5.05035\n",
      "Training Epoch 14  70.4% | batch:       483 of       686\t|\tloss: 4.3126\n",
      "Training Epoch 14  70.6% | batch:       484 of       686\t|\tloss: 5.60344\n",
      "Training Epoch 14  70.7% | batch:       485 of       686\t|\tloss: 3.08529\n",
      "Training Epoch 14  70.8% | batch:       486 of       686\t|\tloss: 3.84332\n",
      "Training Epoch 14  71.0% | batch:       487 of       686\t|\tloss: 3.11467\n",
      "Training Epoch 14  71.1% | batch:       488 of       686\t|\tloss: 5.29461\n",
      "Training Epoch 14  71.3% | batch:       489 of       686\t|\tloss: 4.08605\n",
      "Training Epoch 14  71.4% | batch:       490 of       686\t|\tloss: 4.15568\n",
      "Training Epoch 14  71.6% | batch:       491 of       686\t|\tloss: 2.82753\n",
      "Training Epoch 14  71.7% | batch:       492 of       686\t|\tloss: 5.32543\n",
      "Training Epoch 14  71.9% | batch:       493 of       686\t|\tloss: 4.90341\n",
      "Training Epoch 14  72.0% | batch:       494 of       686\t|\tloss: 4.25515\n",
      "Training Epoch 14  72.2% | batch:       495 of       686\t|\tloss: 7.56642\n",
      "Training Epoch 14  72.3% | batch:       496 of       686\t|\tloss: 4.32192\n",
      "Training Epoch 14  72.4% | batch:       497 of       686\t|\tloss: 4.33708\n",
      "Training Epoch 14  72.6% | batch:       498 of       686\t|\tloss: 3.6248\n",
      "Training Epoch 14  72.7% | batch:       499 of       686\t|\tloss: 3.61638\n",
      "Training Epoch 14  72.9% | batch:       500 of       686\t|\tloss: 3.9344\n",
      "Training Epoch 14  73.0% | batch:       501 of       686\t|\tloss: 4.34941\n",
      "Training Epoch 14  73.2% | batch:       502 of       686\t|\tloss: 3.75557\n",
      "Training Epoch 14  73.3% | batch:       503 of       686\t|\tloss: 4.27959\n",
      "Training Epoch 14  73.5% | batch:       504 of       686\t|\tloss: 3.85215\n",
      "Training Epoch 14  73.6% | batch:       505 of       686\t|\tloss: 3.49395\n",
      "Training Epoch 14  73.8% | batch:       506 of       686\t|\tloss: 4.39084\n",
      "Training Epoch 14  73.9% | batch:       507 of       686\t|\tloss: 3.0719\n",
      "Training Epoch 14  74.1% | batch:       508 of       686\t|\tloss: 3.0242\n",
      "Training Epoch 14  74.2% | batch:       509 of       686\t|\tloss: 4.32734\n",
      "Training Epoch 14  74.3% | batch:       510 of       686\t|\tloss: 4.07181\n",
      "Training Epoch 14  74.5% | batch:       511 of       686\t|\tloss: 3.93793\n",
      "Training Epoch 14  74.6% | batch:       512 of       686\t|\tloss: 4.19822\n",
      "Training Epoch 14  74.8% | batch:       513 of       686\t|\tloss: 3.99853\n",
      "Training Epoch 14  74.9% | batch:       514 of       686\t|\tloss: 2.8629\n",
      "Training Epoch 14  75.1% | batch:       515 of       686\t|\tloss: 3.41176\n",
      "Training Epoch 14  75.2% | batch:       516 of       686\t|\tloss: 4.80592\n",
      "Training Epoch 14  75.4% | batch:       517 of       686\t|\tloss: 4.28419\n",
      "Training Epoch 14  75.5% | batch:       518 of       686\t|\tloss: 3.44302\n",
      "Training Epoch 14  75.7% | batch:       519 of       686\t|\tloss: 4.16758\n",
      "Training Epoch 14  75.8% | batch:       520 of       686\t|\tloss: 3.66275\n",
      "Training Epoch 14  75.9% | batch:       521 of       686\t|\tloss: 2.8563\n",
      "Training Epoch 14  76.1% | batch:       522 of       686\t|\tloss: 3.40741\n",
      "Training Epoch 14  76.2% | batch:       523 of       686\t|\tloss: 4.06268\n",
      "Training Epoch 14  76.4% | batch:       524 of       686\t|\tloss: 4.38475\n",
      "Training Epoch 14  76.5% | batch:       525 of       686\t|\tloss: 3.6257\n",
      "Training Epoch 14  76.7% | batch:       526 of       686\t|\tloss: 4.66417\n",
      "Training Epoch 14  76.8% | batch:       527 of       686\t|\tloss: 4.00944\n",
      "Training Epoch 14  77.0% | batch:       528 of       686\t|\tloss: 5.08207\n",
      "Training Epoch 14  77.1% | batch:       529 of       686\t|\tloss: 5.31851\n",
      "Training Epoch 14  77.3% | batch:       530 of       686\t|\tloss: 4.19515\n",
      "Training Epoch 14  77.4% | batch:       531 of       686\t|\tloss: 3.23136\n",
      "Training Epoch 14  77.6% | batch:       532 of       686\t|\tloss: 4.62814\n",
      "Training Epoch 14  77.7% | batch:       533 of       686\t|\tloss: 3.95944\n",
      "Training Epoch 14  77.8% | batch:       534 of       686\t|\tloss: 3.99589\n",
      "Training Epoch 14  78.0% | batch:       535 of       686\t|\tloss: 4.0373\n",
      "Training Epoch 14  78.1% | batch:       536 of       686\t|\tloss: 4.26372\n",
      "Training Epoch 14  78.3% | batch:       537 of       686\t|\tloss: 3.70328\n",
      "Training Epoch 14  78.4% | batch:       538 of       686\t|\tloss: 5.0817\n",
      "Training Epoch 14  78.6% | batch:       539 of       686\t|\tloss: 3.18112\n",
      "Training Epoch 14  78.7% | batch:       540 of       686\t|\tloss: 3.27771\n",
      "Training Epoch 14  78.9% | batch:       541 of       686\t|\tloss: 5.40084\n",
      "Training Epoch 14  79.0% | batch:       542 of       686\t|\tloss: 4.31292\n",
      "Training Epoch 14  79.2% | batch:       543 of       686\t|\tloss: 4.26016\n",
      "Training Epoch 14  79.3% | batch:       544 of       686\t|\tloss: 3.65248\n",
      "Training Epoch 14  79.4% | batch:       545 of       686\t|\tloss: 3.75023\n",
      "Training Epoch 14  79.6% | batch:       546 of       686\t|\tloss: 4.31667\n",
      "Training Epoch 14  79.7% | batch:       547 of       686\t|\tloss: 2.63995\n",
      "Training Epoch 14  79.9% | batch:       548 of       686\t|\tloss: 3.84722\n",
      "Training Epoch 14  80.0% | batch:       549 of       686\t|\tloss: 4.23788\n",
      "Training Epoch 14  80.2% | batch:       550 of       686\t|\tloss: 3.56534\n",
      "Training Epoch 14  80.3% | batch:       551 of       686\t|\tloss: 3.83631\n",
      "Training Epoch 14  80.5% | batch:       552 of       686\t|\tloss: 4.25438\n",
      "Training Epoch 14  80.6% | batch:       553 of       686\t|\tloss: 2.68853\n",
      "Training Epoch 14  80.8% | batch:       554 of       686\t|\tloss: 4.80509\n",
      "Training Epoch 14  80.9% | batch:       555 of       686\t|\tloss: 4.22211\n",
      "Training Epoch 14  81.0% | batch:       556 of       686\t|\tloss: 3.30482\n",
      "Training Epoch 14  81.2% | batch:       557 of       686\t|\tloss: 3.47611\n",
      "Training Epoch 14  81.3% | batch:       558 of       686\t|\tloss: 3.35879\n",
      "Training Epoch 14  81.5% | batch:       559 of       686\t|\tloss: 3.80029\n",
      "Training Epoch 14  81.6% | batch:       560 of       686\t|\tloss: 4.61592\n",
      "Training Epoch 14  81.8% | batch:       561 of       686\t|\tloss: 3.38226\n",
      "Training Epoch 14  81.9% | batch:       562 of       686\t|\tloss: 4.07751\n",
      "Training Epoch 14  82.1% | batch:       563 of       686\t|\tloss: 5.11971\n",
      "Training Epoch 14  82.2% | batch:       564 of       686\t|\tloss: 3.10913\n",
      "Training Epoch 14  82.4% | batch:       565 of       686\t|\tloss: 4.79314\n",
      "Training Epoch 14  82.5% | batch:       566 of       686\t|\tloss: 4.1648\n",
      "Training Epoch 14  82.7% | batch:       567 of       686\t|\tloss: 2.97716\n",
      "Training Epoch 14  82.8% | batch:       568 of       686\t|\tloss: 4.54819\n",
      "Training Epoch 14  82.9% | batch:       569 of       686\t|\tloss: 4.42384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 14  83.1% | batch:       570 of       686\t|\tloss: 3.68933\n",
      "Training Epoch 14  83.2% | batch:       571 of       686\t|\tloss: 3.11699\n",
      "Training Epoch 14  83.4% | batch:       572 of       686\t|\tloss: 4.42634\n",
      "Training Epoch 14  83.5% | batch:       573 of       686\t|\tloss: 3.20036\n",
      "Training Epoch 14  83.7% | batch:       574 of       686\t|\tloss: 3.603\n",
      "Training Epoch 14  83.8% | batch:       575 of       686\t|\tloss: 5.02649\n",
      "Training Epoch 14  84.0% | batch:       576 of       686\t|\tloss: 3.39161\n",
      "Training Epoch 14  84.1% | batch:       577 of       686\t|\tloss: 3.5925\n",
      "Training Epoch 14  84.3% | batch:       578 of       686\t|\tloss: 6.56226\n",
      "Training Epoch 14  84.4% | batch:       579 of       686\t|\tloss: 3.42905\n",
      "Training Epoch 14  84.5% | batch:       580 of       686\t|\tloss: 4.40883\n",
      "Training Epoch 14  84.7% | batch:       581 of       686\t|\tloss: 2.96721\n",
      "Training Epoch 14  84.8% | batch:       582 of       686\t|\tloss: 3.71012\n",
      "Training Epoch 14  85.0% | batch:       583 of       686\t|\tloss: 3.83528\n",
      "Training Epoch 14  85.1% | batch:       584 of       686\t|\tloss: 4.4208\n",
      "Training Epoch 14  85.3% | batch:       585 of       686\t|\tloss: 3.44768\n",
      "Training Epoch 14  85.4% | batch:       586 of       686\t|\tloss: 4.32471\n",
      "Training Epoch 14  85.6% | batch:       587 of       686\t|\tloss: 3.05724\n",
      "Training Epoch 14  85.7% | batch:       588 of       686\t|\tloss: 4.02973\n",
      "Training Epoch 14  85.9% | batch:       589 of       686\t|\tloss: 3.59595\n",
      "Training Epoch 14  86.0% | batch:       590 of       686\t|\tloss: 4.74202\n",
      "Training Epoch 14  86.2% | batch:       591 of       686\t|\tloss: 4.1088\n",
      "Training Epoch 14  86.3% | batch:       592 of       686\t|\tloss: 5.35666\n",
      "Training Epoch 14  86.4% | batch:       593 of       686\t|\tloss: 5.30999\n",
      "Training Epoch 14  86.6% | batch:       594 of       686\t|\tloss: 2.94316\n",
      "Training Epoch 14  86.7% | batch:       595 of       686\t|\tloss: 3.91091\n",
      "Training Epoch 14  86.9% | batch:       596 of       686\t|\tloss: 4.0147\n",
      "Training Epoch 14  87.0% | batch:       597 of       686\t|\tloss: 4.90907\n",
      "Training Epoch 14  87.2% | batch:       598 of       686\t|\tloss: 4.0824\n",
      "Training Epoch 14  87.3% | batch:       599 of       686\t|\tloss: 4.32988\n",
      "Training Epoch 14  87.5% | batch:       600 of       686\t|\tloss: 3.15917\n",
      "Training Epoch 14  87.6% | batch:       601 of       686\t|\tloss: 4.06593\n",
      "Training Epoch 14  87.8% | batch:       602 of       686\t|\tloss: 3.72524\n",
      "Training Epoch 14  87.9% | batch:       603 of       686\t|\tloss: 3.66798\n",
      "Training Epoch 14  88.0% | batch:       604 of       686\t|\tloss: 4.50866\n",
      "Training Epoch 14  88.2% | batch:       605 of       686\t|\tloss: 4.86185\n",
      "Training Epoch 14  88.3% | batch:       606 of       686\t|\tloss: 5.58359\n",
      "Training Epoch 14  88.5% | batch:       607 of       686\t|\tloss: 3.12844\n",
      "Training Epoch 14  88.6% | batch:       608 of       686\t|\tloss: 3.66766\n",
      "Training Epoch 14  88.8% | batch:       609 of       686\t|\tloss: 3.51231\n",
      "Training Epoch 14  88.9% | batch:       610 of       686\t|\tloss: 3.93069\n",
      "Training Epoch 14  89.1% | batch:       611 of       686\t|\tloss: 4.01619\n",
      "Training Epoch 14  89.2% | batch:       612 of       686\t|\tloss: 4.14665\n",
      "Training Epoch 14  89.4% | batch:       613 of       686\t|\tloss: 3.12691\n",
      "Training Epoch 14  89.5% | batch:       614 of       686\t|\tloss: 2.25317\n",
      "Training Epoch 14  89.7% | batch:       615 of       686\t|\tloss: 4.31672\n",
      "Training Epoch 14  89.8% | batch:       616 of       686\t|\tloss: 5.49049\n",
      "Training Epoch 14  89.9% | batch:       617 of       686\t|\tloss: 4.05704\n",
      "Training Epoch 14  90.1% | batch:       618 of       686\t|\tloss: 3.86564\n",
      "Training Epoch 14  90.2% | batch:       619 of       686\t|\tloss: 4.18917\n",
      "Training Epoch 14  90.4% | batch:       620 of       686\t|\tloss: 3.78267\n",
      "Training Epoch 14  90.5% | batch:       621 of       686\t|\tloss: 3.82411\n",
      "Training Epoch 14  90.7% | batch:       622 of       686\t|\tloss: 3.71491\n",
      "Training Epoch 14  90.8% | batch:       623 of       686\t|\tloss: 3.46013\n",
      "Training Epoch 14  91.0% | batch:       624 of       686\t|\tloss: 3.82749\n",
      "Training Epoch 14  91.1% | batch:       625 of       686\t|\tloss: 4.73466\n",
      "Training Epoch 14  91.3% | batch:       626 of       686\t|\tloss: 3.51562\n",
      "Training Epoch 14  91.4% | batch:       627 of       686\t|\tloss: 3.71723\n",
      "Training Epoch 14  91.5% | batch:       628 of       686\t|\tloss: 3.02694\n",
      "Training Epoch 14  91.7% | batch:       629 of       686\t|\tloss: 3.42906\n",
      "Training Epoch 14  91.8% | batch:       630 of       686\t|\tloss: 2.74113\n",
      "Training Epoch 14  92.0% | batch:       631 of       686\t|\tloss: 4.1265\n",
      "Training Epoch 14  92.1% | batch:       632 of       686\t|\tloss: 3.58188\n",
      "Training Epoch 14  92.3% | batch:       633 of       686\t|\tloss: 4.91303\n",
      "Training Epoch 14  92.4% | batch:       634 of       686\t|\tloss: 4.42085\n",
      "Training Epoch 14  92.6% | batch:       635 of       686\t|\tloss: 3.7879\n",
      "Training Epoch 14  92.7% | batch:       636 of       686\t|\tloss: 4.38255\n",
      "Training Epoch 14  92.9% | batch:       637 of       686\t|\tloss: 3.87466\n",
      "Training Epoch 14  93.0% | batch:       638 of       686\t|\tloss: 3.29371\n",
      "Training Epoch 14  93.1% | batch:       639 of       686\t|\tloss: 3.4093\n",
      "Training Epoch 14  93.3% | batch:       640 of       686\t|\tloss: 4.13261\n",
      "Training Epoch 14  93.4% | batch:       641 of       686\t|\tloss: 4.01077\n",
      "Training Epoch 14  93.6% | batch:       642 of       686\t|\tloss: 4.06302\n",
      "Training Epoch 14  93.7% | batch:       643 of       686\t|\tloss: 3.17339\n",
      "Training Epoch 14  93.9% | batch:       644 of       686\t|\tloss: 4.21508\n",
      "Training Epoch 14  94.0% | batch:       645 of       686\t|\tloss: 3.58168\n",
      "Training Epoch 14  94.2% | batch:       646 of       686\t|\tloss: 4.62101\n",
      "Training Epoch 14  94.3% | batch:       647 of       686\t|\tloss: 4.05091\n",
      "Training Epoch 14  94.5% | batch:       648 of       686\t|\tloss: 4.11433\n",
      "Training Epoch 14  94.6% | batch:       649 of       686\t|\tloss: 3.24367\n",
      "Training Epoch 14  94.8% | batch:       650 of       686\t|\tloss: 3.20743\n",
      "Training Epoch 14  94.9% | batch:       651 of       686\t|\tloss: 3.12746\n",
      "Training Epoch 14  95.0% | batch:       652 of       686\t|\tloss: 4.38498\n",
      "Training Epoch 14  95.2% | batch:       653 of       686\t|\tloss: 5.47134\n",
      "Training Epoch 14  95.3% | batch:       654 of       686\t|\tloss: 5.66436\n",
      "Training Epoch 14  95.5% | batch:       655 of       686\t|\tloss: 4.93128\n",
      "Training Epoch 14  95.6% | batch:       656 of       686\t|\tloss: 4.00946\n",
      "Training Epoch 14  95.8% | batch:       657 of       686\t|\tloss: 3.3763\n",
      "Training Epoch 14  95.9% | batch:       658 of       686\t|\tloss: 4.13743\n",
      "Training Epoch 14  96.1% | batch:       659 of       686\t|\tloss: 5.16229\n",
      "Training Epoch 14  96.2% | batch:       660 of       686\t|\tloss: 3.76712\n",
      "Training Epoch 14  96.4% | batch:       661 of       686\t|\tloss: 3.9429\n",
      "Training Epoch 14  96.5% | batch:       662 of       686\t|\tloss: 2.93525\n",
      "Training Epoch 14  96.6% | batch:       663 of       686\t|\tloss: 3.04304\n",
      "Training Epoch 14  96.8% | batch:       664 of       686\t|\tloss: 4.24917\n",
      "Training Epoch 14  96.9% | batch:       665 of       686\t|\tloss: 3.80298\n",
      "Training Epoch 14  97.1% | batch:       666 of       686\t|\tloss: 4.35565\n",
      "Training Epoch 14  97.2% | batch:       667 of       686\t|\tloss: 3.6516\n",
      "Training Epoch 14  97.4% | batch:       668 of       686\t|\tloss: 4.0509\n",
      "Training Epoch 14  97.5% | batch:       669 of       686\t|\tloss: 5.54043\n",
      "Training Epoch 14  97.7% | batch:       670 of       686\t|\tloss: 4.2211\n",
      "Training Epoch 14  97.8% | batch:       671 of       686\t|\tloss: 4.35343\n",
      "Training Epoch 14  98.0% | batch:       672 of       686\t|\tloss: 2.86822\n",
      "Training Epoch 14  98.1% | batch:       673 of       686\t|\tloss: 6.52748\n",
      "Training Epoch 14  98.3% | batch:       674 of       686\t|\tloss: 2.87539\n",
      "Training Epoch 14  98.4% | batch:       675 of       686\t|\tloss: 3.39094\n",
      "Training Epoch 14  98.5% | batch:       676 of       686\t|\tloss: 3.79231\n",
      "Training Epoch 14  98.7% | batch:       677 of       686\t|\tloss: 3.11648\n",
      "Training Epoch 14  98.8% | batch:       678 of       686\t|\tloss: 3.52791\n",
      "Training Epoch 14  99.0% | batch:       679 of       686\t|\tloss: 3.79162\n",
      "Training Epoch 14  99.1% | batch:       680 of       686\t|\tloss: 4.63304\n",
      "Training Epoch 14  99.3% | batch:       681 of       686\t|\tloss: 4.16777\n",
      "Training Epoch 14  99.4% | batch:       682 of       686\t|\tloss: 3.37253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:06:17,363 | INFO : Epoch 14 Training Summary: epoch: 14.000000 | loss: 4.218057 | \n",
      "2023-05-25 22:06:17,366 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 25.53176736831665 seconds\n",
      "\n",
      "2023-05-25 22:06:17,367 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.827672719955444 seconds\n",
      "2023-05-25 22:06:17,368 | INFO : Avg batch train. time: 0.034734216792937964 seconds\n",
      "2023-05-25 22:06:17,369 | INFO : Avg sample train. time: 0.0002717107328805 seconds\n",
      "2023-05-25 22:06:17,371 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 14  99.6% | batch:       683 of       686\t|\tloss: 4.61586\n",
      "Training Epoch 14  99.7% | batch:       684 of       686\t|\tloss: 3.71203\n",
      "Training Epoch 14  99.9% | batch:       685 of       686\t|\tloss: 5.31014\n",
      "\n",
      "Evaluating Epoch 14   0.0% | batch:         0 of       172\t|\tloss: 1.97283\n",
      "Evaluating Epoch 14   0.6% | batch:         1 of       172\t|\tloss: 2.71168\n",
      "Evaluating Epoch 14   1.2% | batch:         2 of       172\t|\tloss: 2.15515\n",
      "Evaluating Epoch 14   1.7% | batch:         3 of       172\t|\tloss: 3.87409\n",
      "Evaluating Epoch 14   2.3% | batch:         4 of       172\t|\tloss: 2.25753\n",
      "Evaluating Epoch 14   2.9% | batch:         5 of       172\t|\tloss: 1.80169\n",
      "Evaluating Epoch 14   3.5% | batch:         6 of       172\t|\tloss: 2.73601\n",
      "Evaluating Epoch 14   4.1% | batch:         7 of       172\t|\tloss: 3.91681\n",
      "Evaluating Epoch 14   4.7% | batch:         8 of       172\t|\tloss: 2.02388\n",
      "Evaluating Epoch 14   5.2% | batch:         9 of       172\t|\tloss: 2.42722\n",
      "Evaluating Epoch 14   5.8% | batch:        10 of       172\t|\tloss: 2.99975\n",
      "Evaluating Epoch 14   6.4% | batch:        11 of       172\t|\tloss: 2.58223\n",
      "Evaluating Epoch 14   7.0% | batch:        12 of       172\t|\tloss: 1.81639\n",
      "Evaluating Epoch 14   7.6% | batch:        13 of       172\t|\tloss: 2.68997\n",
      "Evaluating Epoch 14   8.1% | batch:        14 of       172\t|\tloss: 2.99666\n",
      "Evaluating Epoch 14   8.7% | batch:        15 of       172\t|\tloss: 2.16304\n",
      "Evaluating Epoch 14   9.3% | batch:        16 of       172\t|\tloss: 3.01013\n",
      "Evaluating Epoch 14   9.9% | batch:        17 of       172\t|\tloss: 2.08042\n",
      "Evaluating Epoch 14  10.5% | batch:        18 of       172\t|\tloss: 21.3535\n",
      "Evaluating Epoch 14  11.0% | batch:        19 of       172\t|\tloss: 1.85913\n",
      "Evaluating Epoch 14  11.6% | batch:        20 of       172\t|\tloss: 0.749405\n",
      "Evaluating Epoch 14  12.2% | batch:        21 of       172\t|\tloss: 0.254006\n",
      "Evaluating Epoch 14  12.8% | batch:        22 of       172\t|\tloss: 5.0626\n",
      "Evaluating Epoch 14  13.4% | batch:        23 of       172\t|\tloss: 3.45877\n",
      "Evaluating Epoch 14  14.0% | batch:        24 of       172\t|\tloss: 0.923864\n",
      "Evaluating Epoch 14  14.5% | batch:        25 of       172\t|\tloss: 1.01647\n",
      "Evaluating Epoch 14  15.1% | batch:        26 of       172\t|\tloss: 9.293\n",
      "Evaluating Epoch 14  15.7% | batch:        27 of       172\t|\tloss: 19.5948\n",
      "Evaluating Epoch 14  16.3% | batch:        28 of       172\t|\tloss: 0.316907\n",
      "Evaluating Epoch 14  16.9% | batch:        29 of       172\t|\tloss: 0.380937\n",
      "Evaluating Epoch 14  17.4% | batch:        30 of       172\t|\tloss: 0.601345\n",
      "Evaluating Epoch 14  18.0% | batch:        31 of       172\t|\tloss: 0.43655\n",
      "Evaluating Epoch 14  18.6% | batch:        32 of       172\t|\tloss: 0.93641\n",
      "Evaluating Epoch 14  19.2% | batch:        33 of       172\t|\tloss: 1.34536\n",
      "Evaluating Epoch 14  19.8% | batch:        34 of       172\t|\tloss: 0.422249\n",
      "Evaluating Epoch 14  20.3% | batch:        35 of       172\t|\tloss: 0.595395\n",
      "Evaluating Epoch 14  20.9% | batch:        36 of       172\t|\tloss: 4.63981\n",
      "Evaluating Epoch 14  21.5% | batch:        37 of       172\t|\tloss: 6.9854\n",
      "Evaluating Epoch 14  22.1% | batch:        38 of       172\t|\tloss: 4.25458\n",
      "Evaluating Epoch 14  22.7% | batch:        39 of       172\t|\tloss: 9.23939\n",
      "Evaluating Epoch 14  23.3% | batch:        40 of       172\t|\tloss: 0.8045\n",
      "Evaluating Epoch 14  23.8% | batch:        41 of       172\t|\tloss: 0.255885\n",
      "Evaluating Epoch 14  24.4% | batch:        42 of       172\t|\tloss: 1.01945\n",
      "Evaluating Epoch 14  25.0% | batch:        43 of       172\t|\tloss: 22.857\n",
      "Evaluating Epoch 14  25.6% | batch:        44 of       172\t|\tloss: 1.53602\n",
      "Evaluating Epoch 14  26.2% | batch:        45 of       172\t|\tloss: 0.27716\n",
      "Evaluating Epoch 14  26.7% | batch:        46 of       172\t|\tloss: 0.178931\n",
      "Evaluating Epoch 14  27.3% | batch:        47 of       172\t|\tloss: 0.586268\n",
      "Evaluating Epoch 14  27.9% | batch:        48 of       172\t|\tloss: 0.382814\n",
      "Evaluating Epoch 14  28.5% | batch:        49 of       172\t|\tloss: 2.18762\n",
      "Evaluating Epoch 14  29.1% | batch:        50 of       172\t|\tloss: 1.18968\n",
      "Evaluating Epoch 14  29.7% | batch:        51 of       172\t|\tloss: 0.353441\n",
      "Evaluating Epoch 14  30.2% | batch:        52 of       172\t|\tloss: 0.320566\n",
      "Evaluating Epoch 14  30.8% | batch:        53 of       172\t|\tloss: 0.974767\n",
      "Evaluating Epoch 14  31.4% | batch:        54 of       172\t|\tloss: 0.609366\n",
      "Evaluating Epoch 14  32.0% | batch:        55 of       172\t|\tloss: 0.489821\n",
      "Evaluating Epoch 14  32.6% | batch:        56 of       172\t|\tloss: 1.09481\n",
      "Evaluating Epoch 14  33.1% | batch:        57 of       172\t|\tloss: 0.858085\n",
      "Evaluating Epoch 14  33.7% | batch:        58 of       172\t|\tloss: 0.615354\n",
      "Evaluating Epoch 14  34.3% | batch:        59 of       172\t|\tloss: 0.545648\n",
      "Evaluating Epoch 14  34.9% | batch:        60 of       172\t|\tloss: 0.407382\n",
      "Evaluating Epoch 14  35.5% | batch:        61 of       172\t|\tloss: 0.926565\n",
      "Evaluating Epoch 14  36.0% | batch:        62 of       172\t|\tloss: 0.389621\n",
      "Evaluating Epoch 14  36.6% | batch:        63 of       172\t|\tloss: 0.917453\n",
      "Evaluating Epoch 14  37.2% | batch:        64 of       172\t|\tloss: 0.409655\n",
      "Evaluating Epoch 14  37.8% | batch:        65 of       172\t|\tloss: 0.68963\n",
      "Evaluating Epoch 14  38.4% | batch:        66 of       172\t|\tloss: 1.02785\n",
      "Evaluating Epoch 14  39.0% | batch:        67 of       172\t|\tloss: 0.31906\n",
      "Evaluating Epoch 14  39.5% | batch:        68 of       172\t|\tloss: 1.62163\n",
      "Evaluating Epoch 14  40.1% | batch:        69 of       172\t|\tloss: 0.860814\n",
      "Evaluating Epoch 14  40.7% | batch:        70 of       172\t|\tloss: 0.323228\n",
      "Evaluating Epoch 14  41.3% | batch:        71 of       172\t|\tloss: 0.550042\n",
      "Evaluating Epoch 14  41.9% | batch:        72 of       172\t|\tloss: 0.6004\n",
      "Evaluating Epoch 14  42.4% | batch:        73 of       172\t|\tloss: 0.762706\n",
      "Evaluating Epoch 14  43.0% | batch:        74 of       172\t|\tloss: 0.149069\n",
      "Evaluating Epoch 14  43.6% | batch:        75 of       172\t|\tloss: 0.213843\n",
      "Evaluating Epoch 14  44.2% | batch:        76 of       172\t|\tloss: 0.189846\n",
      "Evaluating Epoch 14  44.8% | batch:        77 of       172\t|\tloss: 0.170553\n",
      "Evaluating Epoch 14  45.3% | batch:        78 of       172\t|\tloss: 0.250774\n",
      "Evaluating Epoch 14  45.9% | batch:        79 of       172\t|\tloss: 0.207372\n",
      "Evaluating Epoch 14  46.5% | batch:        80 of       172\t|\tloss: 0.296621\n",
      "Evaluating Epoch 14  47.1% | batch:        81 of       172\t|\tloss: 0.272354\n",
      "Evaluating Epoch 14  47.7% | batch:        82 of       172\t|\tloss: 0.223022\n",
      "Evaluating Epoch 14  48.3% | batch:        83 of       172\t|\tloss: 0.501048\n",
      "Evaluating Epoch 14  48.8% | batch:        84 of       172\t|\tloss: 0.464359\n",
      "Evaluating Epoch 14  49.4% | batch:        85 of       172\t|\tloss: 0.76525\n",
      "Evaluating Epoch 14  50.0% | batch:        86 of       172\t|\tloss: 0.641118\n",
      "Evaluating Epoch 14  50.6% | batch:        87 of       172\t|\tloss: 0.717899\n",
      "Evaluating Epoch 14  51.2% | batch:        88 of       172\t|\tloss: 0.580776\n",
      "Evaluating Epoch 14  51.7% | batch:        89 of       172\t|\tloss: 0.592019\n",
      "Evaluating Epoch 14  52.3% | batch:        90 of       172\t|\tloss: 1.07809\n",
      "Evaluating Epoch 14  52.9% | batch:        91 of       172\t|\tloss: 0.780208\n",
      "Evaluating Epoch 14  53.5% | batch:        92 of       172\t|\tloss: 0.545418\n",
      "Evaluating Epoch 14  54.1% | batch:        93 of       172\t|\tloss: 0.856817\n",
      "Evaluating Epoch 14  54.7% | batch:        94 of       172\t|\tloss: 1.26304\n",
      "Evaluating Epoch 14  55.2% | batch:        95 of       172\t|\tloss: 0.516083\n",
      "Evaluating Epoch 14  55.8% | batch:        96 of       172\t|\tloss: 0.74736\n",
      "Evaluating Epoch 14  56.4% | batch:        97 of       172\t|\tloss: 0.843729\n",
      "Evaluating Epoch 14  57.0% | batch:        98 of       172\t|\tloss: 0.863572\n",
      "Evaluating Epoch 14  57.6% | batch:        99 of       172\t|\tloss: 0.527577\n",
      "Evaluating Epoch 14  58.1% | batch:       100 of       172\t|\tloss: 0.764826\n",
      "Evaluating Epoch 14  58.7% | batch:       101 of       172\t|\tloss: 0.476075\n",
      "Evaluating Epoch 14  59.3% | batch:       102 of       172\t|\tloss: 0.771591\n",
      "Evaluating Epoch 14  59.9% | batch:       103 of       172\t|\tloss: 0.843713\n",
      "Evaluating Epoch 14  60.5% | batch:       104 of       172\t|\tloss: 0.875324\n",
      "Evaluating Epoch 14  61.0% | batch:       105 of       172\t|\tloss: 0.876747\n",
      "Evaluating Epoch 14  61.6% | batch:       106 of       172\t|\tloss: 0.680294\n",
      "Evaluating Epoch 14  62.2% | batch:       107 of       172\t|\tloss: 0.949541\n",
      "Evaluating Epoch 14  62.8% | batch:       108 of       172\t|\tloss: 0.641351\n",
      "Evaluating Epoch 14  63.4% | batch:       109 of       172\t|\tloss: 0.821192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 14  64.0% | batch:       110 of       172\t|\tloss: 0.93957\n",
      "Evaluating Epoch 14  64.5% | batch:       111 of       172\t|\tloss: 0.849136\n",
      "Evaluating Epoch 14  65.1% | batch:       112 of       172\t|\tloss: 0.489184\n",
      "Evaluating Epoch 14  65.7% | batch:       113 of       172\t|\tloss: 0.82993\n",
      "Evaluating Epoch 14  66.3% | batch:       114 of       172\t|\tloss: 0.755903\n",
      "Evaluating Epoch 14  66.9% | batch:       115 of       172\t|\tloss: 0.685554\n",
      "Evaluating Epoch 14  67.4% | batch:       116 of       172\t|\tloss: 0.499136\n",
      "Evaluating Epoch 14  68.0% | batch:       117 of       172\t|\tloss: 0.578922\n",
      "Evaluating Epoch 14  68.6% | batch:       118 of       172\t|\tloss: 0.742535\n",
      "Evaluating Epoch 14  69.2% | batch:       119 of       172\t|\tloss: 0.46514\n",
      "Evaluating Epoch 14  69.8% | batch:       120 of       172\t|\tloss: 0.478513\n",
      "Evaluating Epoch 14  70.3% | batch:       121 of       172\t|\tloss: 1.30935\n",
      "Evaluating Epoch 14  70.9% | batch:       122 of       172\t|\tloss: 0.986676\n",
      "Evaluating Epoch 14  71.5% | batch:       123 of       172\t|\tloss: 1.43645\n",
      "Evaluating Epoch 14  72.1% | batch:       124 of       172\t|\tloss: 2.72015\n",
      "Evaluating Epoch 14  72.7% | batch:       125 of       172\t|\tloss: 0.582567\n",
      "Evaluating Epoch 14  73.3% | batch:       126 of       172\t|\tloss: 0.373088\n",
      "Evaluating Epoch 14  73.8% | batch:       127 of       172\t|\tloss: 0.646126\n",
      "Evaluating Epoch 14  74.4% | batch:       128 of       172\t|\tloss: 0.96531\n",
      "Evaluating Epoch 14  75.0% | batch:       129 of       172\t|\tloss: 0.413584\n",
      "Evaluating Epoch 14  75.6% | batch:       130 of       172\t|\tloss: 0.902667\n",
      "Evaluating Epoch 14  76.2% | batch:       131 of       172\t|\tloss: 0.862724\n",
      "Evaluating Epoch 14  76.7% | batch:       132 of       172\t|\tloss: 0.467608\n",
      "Evaluating Epoch 14  77.3% | batch:       133 of       172\t|\tloss: 0.363981\n",
      "Evaluating Epoch 14  77.9% | batch:       134 of       172\t|\tloss: 0.680305\n",
      "Evaluating Epoch 14  78.5% | batch:       135 of       172\t|\tloss: 0.184649\n",
      "Evaluating Epoch 14  79.1% | batch:       136 of       172\t|\tloss: 0.49764\n",
      "Evaluating Epoch 14  79.7% | batch:       137 of       172\t|\tloss: 0.158435\n",
      "Evaluating Epoch 14  80.2% | batch:       138 of       172\t|\tloss: 0.558716\n",
      "Evaluating Epoch 14  80.8% | batch:       139 of       172\t|\tloss: 0.343892\n",
      "Evaluating Epoch 14  81.4% | batch:       140 of       172\t|\tloss: 0.444765\n",
      "Evaluating Epoch 14  82.0% | batch:       141 of       172\t|\tloss: 0.193772\n",
      "Evaluating Epoch 14  82.6% | batch:       142 of       172\t|\tloss: 0.372062\n",
      "Evaluating Epoch 14  83.1% | batch:       143 of       172\t|\tloss: 0.333933\n",
      "Evaluating Epoch 14  83.7% | batch:       144 of       172\t|\tloss: 0.477857\n",
      "Evaluating Epoch 14  84.3% | batch:       145 of       172\t|\tloss: 0.26445\n",
      "Evaluating Epoch 14  84.9% | batch:       146 of       172\t|\tloss: 0.438237\n",
      "Evaluating Epoch 14  85.5% | batch:       147 of       172\t|\tloss: 0.245305\n",
      "Evaluating Epoch 14  86.0% | batch:       148 of       172\t|\tloss: 0.355418\n",
      "Evaluating Epoch 14  86.6% | batch:       149 of       172\t|\tloss: 0.369817\n",
      "Evaluating Epoch 14  87.2% | batch:       150 of       172\t|\tloss: 0.589676\n",
      "Evaluating Epoch 14  87.8% | batch:       151 of       172\t|\tloss: 0.922984\n",
      "Evaluating Epoch 14  88.4% | batch:       152 of       172\t|\tloss: 0.464449\n",
      "Evaluating Epoch 14  89.0% | batch:       153 of       172\t|\tloss: 0.6726\n",
      "Evaluating Epoch 14  89.5% | batch:       154 of       172\t|\tloss: 0.820567\n",
      "Evaluating Epoch 14  90.1% | batch:       155 of       172\t|\tloss: 0.506938\n",
      "Evaluating Epoch 14  90.7% | batch:       156 of       172\t|\tloss: 0.910786\n",
      "Evaluating Epoch 14  91.3% | batch:       157 of       172\t|\tloss: 0.822727\n",
      "Evaluating Epoch 14  91.9% | batch:       158 of       172\t|\tloss: 0.563714\n",
      "Evaluating Epoch 14  92.4% | batch:       159 of       172\t|\tloss: 1.38535\n",
      "Evaluating Epoch 14  93.0% | batch:       160 of       172\t|\tloss: 1.21666\n",
      "Evaluating Epoch 14  93.6% | batch:       161 of       172\t|\tloss: 1.66516\n",
      "Evaluating Epoch 14  94.2% | batch:       162 of       172\t|\tloss: 0.904644\n",
      "Evaluating Epoch 14  94.8% | batch:       163 of       172\t|\tloss: 0.580673\n",
      "Evaluating Epoch 14  95.3% | batch:       164 of       172\t|\tloss: 0.847902\n",
      "Evaluating Epoch 14  95.9% | batch:       165 of       172\t|\tloss: 0.639373\n",
      "Evaluating Epoch 14  96.5% | batch:       166 of       172\t|\tloss: 0.446435\n",
      "Evaluating Epoch 14  97.1% | batch:       167 of       172\t|\tloss: 1.129\n",
      "Evaluating Epoch 14  97.7% | batch:       168 of       172\t|\tloss: 0.544681\n",
      "Evaluating Epoch 14  98.3% | batch:       169 of       172\t|\tloss: 0.692153\n",
      "Evaluating Epoch 14  98.8% | batch:       170 of       172\t|\tloss: 1.02148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:06:21,147 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.7746832370758057 seconds\n",
      "\n",
      "2023-05-25 22:06:21,149 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 3.9587615966796874 seconds\n",
      "2023-05-25 22:06:21,150 | INFO : Avg batch val. time: 0.023016055794649347 seconds\n",
      "2023-05-25 22:06:21,151 | INFO : Avg sample val. time: 0.00018029610587419443 seconds\n",
      "2023-05-25 22:06:21,152 | INFO : Epoch 14 Validation Summary: epoch: 14.000000 | loss: 1.467559 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 14  99.4% | batch:       171 of       172\t|\tloss: 0.548228\n",
      "\n",
      "Training Epoch 15   0.0% | batch:         0 of       686\t|\tloss: 3.62178\n",
      "Training Epoch 15   0.1% | batch:         1 of       686\t|\tloss: 4.33924\n",
      "Training Epoch 15   0.3% | batch:         2 of       686\t|\tloss: 3.70712\n",
      "Training Epoch 15   0.4% | batch:         3 of       686\t|\tloss: 3.51491\n",
      "Training Epoch 15   0.6% | batch:         4 of       686\t|\tloss: 3.58437\n",
      "Training Epoch 15   0.7% | batch:         5 of       686\t|\tloss: 4.26525\n",
      "Training Epoch 15   0.9% | batch:         6 of       686\t|\tloss: 2.55478\n",
      "Training Epoch 15   1.0% | batch:         7 of       686\t|\tloss: 3.45551\n",
      "Training Epoch 15   1.2% | batch:         8 of       686\t|\tloss: 4.40674\n",
      "Training Epoch 15   1.3% | batch:         9 of       686\t|\tloss: 3.57733\n",
      "Training Epoch 15   1.5% | batch:        10 of       686\t|\tloss: 4.07488\n",
      "Training Epoch 15   1.6% | batch:        11 of       686\t|\tloss: 4.29447\n",
      "Training Epoch 15   1.7% | batch:        12 of       686\t|\tloss: 4.06301\n",
      "Training Epoch 15   1.9% | batch:        13 of       686\t|\tloss: 3.89039\n",
      "Training Epoch 15   2.0% | batch:        14 of       686\t|\tloss: 4.0075\n",
      "Training Epoch 15   2.2% | batch:        15 of       686\t|\tloss: 3.92532\n",
      "Training Epoch 15   2.3% | batch:        16 of       686\t|\tloss: 2.81137\n",
      "Training Epoch 15   2.5% | batch:        17 of       686\t|\tloss: 3.85166\n",
      "Training Epoch 15   2.6% | batch:        18 of       686\t|\tloss: 3.11808\n",
      "Training Epoch 15   2.8% | batch:        19 of       686\t|\tloss: 2.88101\n",
      "Training Epoch 15   2.9% | batch:        20 of       686\t|\tloss: 3.14882\n",
      "Training Epoch 15   3.1% | batch:        21 of       686\t|\tloss: 3.07712\n",
      "Training Epoch 15   3.2% | batch:        22 of       686\t|\tloss: 3.74038\n",
      "Training Epoch 15   3.4% | batch:        23 of       686\t|\tloss: 5.4133\n",
      "Training Epoch 15   3.5% | batch:        24 of       686\t|\tloss: 3.41992\n",
      "Training Epoch 15   3.6% | batch:        25 of       686\t|\tloss: 3.83918\n",
      "Training Epoch 15   3.8% | batch:        26 of       686\t|\tloss: 3.38566\n",
      "Training Epoch 15   3.9% | batch:        27 of       686\t|\tloss: 3.17117\n",
      "Training Epoch 15   4.1% | batch:        28 of       686\t|\tloss: 3.26146\n",
      "Training Epoch 15   4.2% | batch:        29 of       686\t|\tloss: 4.40675\n",
      "Training Epoch 15   4.4% | batch:        30 of       686\t|\tloss: 3.8518\n",
      "Training Epoch 15   4.5% | batch:        31 of       686\t|\tloss: 3.65406\n",
      "Training Epoch 15   4.7% | batch:        32 of       686\t|\tloss: 3.79803\n",
      "Training Epoch 15   4.8% | batch:        33 of       686\t|\tloss: 4.23539\n",
      "Training Epoch 15   5.0% | batch:        34 of       686\t|\tloss: 5.37071\n",
      "Training Epoch 15   5.1% | batch:        35 of       686\t|\tloss: 4.36655\n",
      "Training Epoch 15   5.2% | batch:        36 of       686\t|\tloss: 5.1247\n",
      "Training Epoch 15   5.4% | batch:        37 of       686\t|\tloss: 3.86232\n",
      "Training Epoch 15   5.5% | batch:        38 of       686\t|\tloss: 4.70635\n",
      "Training Epoch 15   5.7% | batch:        39 of       686\t|\tloss: 3.34808\n",
      "Training Epoch 15   5.8% | batch:        40 of       686\t|\tloss: 3.40483\n",
      "Training Epoch 15   6.0% | batch:        41 of       686\t|\tloss: 3.77126\n",
      "Training Epoch 15   6.1% | batch:        42 of       686\t|\tloss: 3.0876\n",
      "Training Epoch 15   6.3% | batch:        43 of       686\t|\tloss: 3.17437\n",
      "Training Epoch 15   6.4% | batch:        44 of       686\t|\tloss: 3.93226\n",
      "Training Epoch 15   6.6% | batch:        45 of       686\t|\tloss: 3.83922\n",
      "Training Epoch 15   6.7% | batch:        46 of       686\t|\tloss: 3.44057\n",
      "Training Epoch 15   6.9% | batch:        47 of       686\t|\tloss: 3.03295\n",
      "Training Epoch 15   7.0% | batch:        48 of       686\t|\tloss: 2.70187\n",
      "Training Epoch 15   7.1% | batch:        49 of       686\t|\tloss: 4.50828\n",
      "Training Epoch 15   7.3% | batch:        50 of       686\t|\tloss: 3.35666\n",
      "Training Epoch 15   7.4% | batch:        51 of       686\t|\tloss: 5.21096\n",
      "Training Epoch 15   7.6% | batch:        52 of       686\t|\tloss: 3.43873\n",
      "Training Epoch 15   7.7% | batch:        53 of       686\t|\tloss: 3.59664\n",
      "Training Epoch 15   7.9% | batch:        54 of       686\t|\tloss: 4.82547\n",
      "Training Epoch 15   8.0% | batch:        55 of       686\t|\tloss: 3.96905\n",
      "Training Epoch 15   8.2% | batch:        56 of       686\t|\tloss: 5.30544\n",
      "Training Epoch 15   8.3% | batch:        57 of       686\t|\tloss: 5.27039\n",
      "Training Epoch 15   8.5% | batch:        58 of       686\t|\tloss: 3.55029\n",
      "Training Epoch 15   8.6% | batch:        59 of       686\t|\tloss: 3.55878\n",
      "Training Epoch 15   8.7% | batch:        60 of       686\t|\tloss: 5.28316\n",
      "Training Epoch 15   8.9% | batch:        61 of       686\t|\tloss: 2.59689\n",
      "Training Epoch 15   9.0% | batch:        62 of       686\t|\tloss: 4.2741\n",
      "Training Epoch 15   9.2% | batch:        63 of       686\t|\tloss: 3.54105\n",
      "Training Epoch 15   9.3% | batch:        64 of       686\t|\tloss: 2.83893\n",
      "Training Epoch 15   9.5% | batch:        65 of       686\t|\tloss: 4.34111\n",
      "Training Epoch 15   9.6% | batch:        66 of       686\t|\tloss: 4.38857\n",
      "Training Epoch 15   9.8% | batch:        67 of       686\t|\tloss: 4.22608\n",
      "Training Epoch 15   9.9% | batch:        68 of       686\t|\tloss: 3.70941\n",
      "Training Epoch 15  10.1% | batch:        69 of       686\t|\tloss: 3.19588\n",
      "Training Epoch 15  10.2% | batch:        70 of       686\t|\tloss: 4.47323\n",
      "Training Epoch 15  10.3% | batch:        71 of       686\t|\tloss: 4.3003\n",
      "Training Epoch 15  10.5% | batch:        72 of       686\t|\tloss: 4.50062\n",
      "Training Epoch 15  10.6% | batch:        73 of       686\t|\tloss: 3.23759\n",
      "Training Epoch 15  10.8% | batch:        74 of       686\t|\tloss: 3.8922\n",
      "Training Epoch 15  10.9% | batch:        75 of       686\t|\tloss: 4.68773\n",
      "Training Epoch 15  11.1% | batch:        76 of       686\t|\tloss: 3.79384\n",
      "Training Epoch 15  11.2% | batch:        77 of       686\t|\tloss: 3.87417\n",
      "Training Epoch 15  11.4% | batch:        78 of       686\t|\tloss: 3.36118\n",
      "Training Epoch 15  11.5% | batch:        79 of       686\t|\tloss: 3.8434\n",
      "Training Epoch 15  11.7% | batch:        80 of       686\t|\tloss: 3.97853\n",
      "Training Epoch 15  11.8% | batch:        81 of       686\t|\tloss: 4.0684\n",
      "Training Epoch 15  12.0% | batch:        82 of       686\t|\tloss: 3.17786\n",
      "Training Epoch 15  12.1% | batch:        83 of       686\t|\tloss: 2.65986\n",
      "Training Epoch 15  12.2% | batch:        84 of       686\t|\tloss: 4.59048\n",
      "Training Epoch 15  12.4% | batch:        85 of       686\t|\tloss: 3.34334\n",
      "Training Epoch 15  12.5% | batch:        86 of       686\t|\tloss: 3.23398\n",
      "Training Epoch 15  12.7% | batch:        87 of       686\t|\tloss: 2.59267\n",
      "Training Epoch 15  12.8% | batch:        88 of       686\t|\tloss: 4.13395\n",
      "Training Epoch 15  13.0% | batch:        89 of       686\t|\tloss: 4.13432\n",
      "Training Epoch 15  13.1% | batch:        90 of       686\t|\tloss: 3.0985\n",
      "Training Epoch 15  13.3% | batch:        91 of       686\t|\tloss: 3.72913\n",
      "Training Epoch 15  13.4% | batch:        92 of       686\t|\tloss: 5.22922\n",
      "Training Epoch 15  13.6% | batch:        93 of       686\t|\tloss: 4.59767\n",
      "Training Epoch 15  13.7% | batch:        94 of       686\t|\tloss: 2.73544\n",
      "Training Epoch 15  13.8% | batch:        95 of       686\t|\tloss: 3.68819\n",
      "Training Epoch 15  14.0% | batch:        96 of       686\t|\tloss: 4.14986\n",
      "Training Epoch 15  14.1% | batch:        97 of       686\t|\tloss: 4.26466\n",
      "Training Epoch 15  14.3% | batch:        98 of       686\t|\tloss: 3.82212\n",
      "Training Epoch 15  14.4% | batch:        99 of       686\t|\tloss: 2.96101\n",
      "Training Epoch 15  14.6% | batch:       100 of       686\t|\tloss: 4.22511\n",
      "Training Epoch 15  14.7% | batch:       101 of       686\t|\tloss: 3.99714\n",
      "Training Epoch 15  14.9% | batch:       102 of       686\t|\tloss: 5.21716\n",
      "Training Epoch 15  15.0% | batch:       103 of       686\t|\tloss: 4.0152\n",
      "Training Epoch 15  15.2% | batch:       104 of       686\t|\tloss: 3.74703\n",
      "Training Epoch 15  15.3% | batch:       105 of       686\t|\tloss: 5.21337\n",
      "Training Epoch 15  15.5% | batch:       106 of       686\t|\tloss: 3.90567\n",
      "Training Epoch 15  15.6% | batch:       107 of       686\t|\tloss: 3.27319\n",
      "Training Epoch 15  15.7% | batch:       108 of       686\t|\tloss: 3.24612\n",
      "Training Epoch 15  15.9% | batch:       109 of       686\t|\tloss: 2.7577\n",
      "Training Epoch 15  16.0% | batch:       110 of       686\t|\tloss: 4.09579\n",
      "Training Epoch 15  16.2% | batch:       111 of       686\t|\tloss: 3.4199\n",
      "Training Epoch 15  16.3% | batch:       112 of       686\t|\tloss: 5.53643\n",
      "Training Epoch 15  16.5% | batch:       113 of       686\t|\tloss: 3.70427\n",
      "Training Epoch 15  16.6% | batch:       114 of       686\t|\tloss: 2.8406\n",
      "Training Epoch 15  16.8% | batch:       115 of       686\t|\tloss: 4.69611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 15  16.9% | batch:       116 of       686\t|\tloss: 4.14186\n",
      "Training Epoch 15  17.1% | batch:       117 of       686\t|\tloss: 3.79058\n",
      "Training Epoch 15  17.2% | batch:       118 of       686\t|\tloss: 3.82516\n",
      "Training Epoch 15  17.3% | batch:       119 of       686\t|\tloss: 3.33415\n",
      "Training Epoch 15  17.5% | batch:       120 of       686\t|\tloss: 3.59662\n",
      "Training Epoch 15  17.6% | batch:       121 of       686\t|\tloss: 3.84751\n",
      "Training Epoch 15  17.8% | batch:       122 of       686\t|\tloss: 2.47346\n",
      "Training Epoch 15  17.9% | batch:       123 of       686\t|\tloss: 4.19841\n",
      "Training Epoch 15  18.1% | batch:       124 of       686\t|\tloss: 3.55007\n",
      "Training Epoch 15  18.2% | batch:       125 of       686\t|\tloss: 3.04459\n",
      "Training Epoch 15  18.4% | batch:       126 of       686\t|\tloss: 3.01525\n",
      "Training Epoch 15  18.5% | batch:       127 of       686\t|\tloss: 2.7263\n",
      "Training Epoch 15  18.7% | batch:       128 of       686\t|\tloss: 3.23166\n",
      "Training Epoch 15  18.8% | batch:       129 of       686\t|\tloss: 5.56701\n",
      "Training Epoch 15  19.0% | batch:       130 of       686\t|\tloss: 2.89738\n",
      "Training Epoch 15  19.1% | batch:       131 of       686\t|\tloss: 3.91615\n",
      "Training Epoch 15  19.2% | batch:       132 of       686\t|\tloss: 4.24349\n",
      "Training Epoch 15  19.4% | batch:       133 of       686\t|\tloss: 3.48824\n",
      "Training Epoch 15  19.5% | batch:       134 of       686\t|\tloss: 3.05297\n",
      "Training Epoch 15  19.7% | batch:       135 of       686\t|\tloss: 4.04734\n",
      "Training Epoch 15  19.8% | batch:       136 of       686\t|\tloss: 3.9737\n",
      "Training Epoch 15  20.0% | batch:       137 of       686\t|\tloss: 2.69564\n",
      "Training Epoch 15  20.1% | batch:       138 of       686\t|\tloss: 2.95063\n",
      "Training Epoch 15  20.3% | batch:       139 of       686\t|\tloss: 4.76975\n",
      "Training Epoch 15  20.4% | batch:       140 of       686\t|\tloss: 3.38287\n",
      "Training Epoch 15  20.6% | batch:       141 of       686\t|\tloss: 3.75439\n",
      "Training Epoch 15  20.7% | batch:       142 of       686\t|\tloss: 3.62192\n",
      "Training Epoch 15  20.8% | batch:       143 of       686\t|\tloss: 3.27913\n",
      "Training Epoch 15  21.0% | batch:       144 of       686\t|\tloss: 3.70261\n",
      "Training Epoch 15  21.1% | batch:       145 of       686\t|\tloss: 3.31818\n",
      "Training Epoch 15  21.3% | batch:       146 of       686\t|\tloss: 3.90403\n",
      "Training Epoch 15  21.4% | batch:       147 of       686\t|\tloss: 3.55447\n",
      "Training Epoch 15  21.6% | batch:       148 of       686\t|\tloss: 3.28514\n",
      "Training Epoch 15  21.7% | batch:       149 of       686\t|\tloss: 5.2964\n",
      "Training Epoch 15  21.9% | batch:       150 of       686\t|\tloss: 3.28777\n",
      "Training Epoch 15  22.0% | batch:       151 of       686\t|\tloss: 4.00223\n",
      "Training Epoch 15  22.2% | batch:       152 of       686\t|\tloss: 4.29601\n",
      "Training Epoch 15  22.3% | batch:       153 of       686\t|\tloss: 4.81921\n",
      "Training Epoch 15  22.4% | batch:       154 of       686\t|\tloss: 4.64719\n",
      "Training Epoch 15  22.6% | batch:       155 of       686\t|\tloss: 3.20095\n",
      "Training Epoch 15  22.7% | batch:       156 of       686\t|\tloss: 3.15367\n",
      "Training Epoch 15  22.9% | batch:       157 of       686\t|\tloss: 4.53692\n",
      "Training Epoch 15  23.0% | batch:       158 of       686\t|\tloss: 4.63354\n",
      "Training Epoch 15  23.2% | batch:       159 of       686\t|\tloss: 4.19638\n",
      "Training Epoch 15  23.3% | batch:       160 of       686\t|\tloss: 3.77204\n",
      "Training Epoch 15  23.5% | batch:       161 of       686\t|\tloss: 3.55015\n",
      "Training Epoch 15  23.6% | batch:       162 of       686\t|\tloss: 3.6887\n",
      "Training Epoch 15  23.8% | batch:       163 of       686\t|\tloss: 3.15934\n",
      "Training Epoch 15  23.9% | batch:       164 of       686\t|\tloss: 3.34105\n",
      "Training Epoch 15  24.1% | batch:       165 of       686\t|\tloss: 3.60738\n",
      "Training Epoch 15  24.2% | batch:       166 of       686\t|\tloss: 3.28212\n",
      "Training Epoch 15  24.3% | batch:       167 of       686\t|\tloss: 3.96473\n",
      "Training Epoch 15  24.5% | batch:       168 of       686\t|\tloss: 3.27535\n",
      "Training Epoch 15  24.6% | batch:       169 of       686\t|\tloss: 3.24599\n",
      "Training Epoch 15  24.8% | batch:       170 of       686\t|\tloss: 3.7477\n",
      "Training Epoch 15  24.9% | batch:       171 of       686\t|\tloss: 4.15158\n",
      "Training Epoch 15  25.1% | batch:       172 of       686\t|\tloss: 3.63823\n",
      "Training Epoch 15  25.2% | batch:       173 of       686\t|\tloss: 3.93231\n",
      "Training Epoch 15  25.4% | batch:       174 of       686\t|\tloss: 4.33958\n",
      "Training Epoch 15  25.5% | batch:       175 of       686\t|\tloss: 3.80483\n",
      "Training Epoch 15  25.7% | batch:       176 of       686\t|\tloss: 3.53507\n",
      "Training Epoch 15  25.8% | batch:       177 of       686\t|\tloss: 3.36981\n",
      "Training Epoch 15  25.9% | batch:       178 of       686\t|\tloss: 3.6868\n",
      "Training Epoch 15  26.1% | batch:       179 of       686\t|\tloss: 3.91304\n",
      "Training Epoch 15  26.2% | batch:       180 of       686\t|\tloss: 2.95083\n",
      "Training Epoch 15  26.4% | batch:       181 of       686\t|\tloss: 2.61871\n",
      "Training Epoch 15  26.5% | batch:       182 of       686\t|\tloss: 4.92253\n",
      "Training Epoch 15  26.7% | batch:       183 of       686\t|\tloss: 3.62201\n",
      "Training Epoch 15  26.8% | batch:       184 of       686\t|\tloss: 3.46629\n",
      "Training Epoch 15  27.0% | batch:       185 of       686\t|\tloss: 4.69133\n",
      "Training Epoch 15  27.1% | batch:       186 of       686\t|\tloss: 3.67787\n",
      "Training Epoch 15  27.3% | batch:       187 of       686\t|\tloss: 3.86425\n",
      "Training Epoch 15  27.4% | batch:       188 of       686\t|\tloss: 4.66162\n",
      "Training Epoch 15  27.6% | batch:       189 of       686\t|\tloss: 3.59718\n",
      "Training Epoch 15  27.7% | batch:       190 of       686\t|\tloss: 3.25428\n",
      "Training Epoch 15  27.8% | batch:       191 of       686\t|\tloss: 2.99733\n",
      "Training Epoch 15  28.0% | batch:       192 of       686\t|\tloss: 3.87816\n",
      "Training Epoch 15  28.1% | batch:       193 of       686\t|\tloss: 3.77252\n",
      "Training Epoch 15  28.3% | batch:       194 of       686\t|\tloss: 3.02529\n",
      "Training Epoch 15  28.4% | batch:       195 of       686\t|\tloss: 3.28098\n",
      "Training Epoch 15  28.6% | batch:       196 of       686\t|\tloss: 3.51553\n",
      "Training Epoch 15  28.7% | batch:       197 of       686\t|\tloss: 5.27511\n",
      "Training Epoch 15  28.9% | batch:       198 of       686\t|\tloss: 3.43044\n",
      "Training Epoch 15  29.0% | batch:       199 of       686\t|\tloss: 3.4473\n",
      "Training Epoch 15  29.2% | batch:       200 of       686\t|\tloss: 3.6186\n",
      "Training Epoch 15  29.3% | batch:       201 of       686\t|\tloss: 4.03865\n",
      "Training Epoch 15  29.4% | batch:       202 of       686\t|\tloss: 2.96591\n",
      "Training Epoch 15  29.6% | batch:       203 of       686\t|\tloss: 4.2745\n",
      "Training Epoch 15  29.7% | batch:       204 of       686\t|\tloss: 4.0034\n",
      "Training Epoch 15  29.9% | batch:       205 of       686\t|\tloss: 3.45332\n",
      "Training Epoch 15  30.0% | batch:       206 of       686\t|\tloss: 3.48491\n",
      "Training Epoch 15  30.2% | batch:       207 of       686\t|\tloss: 3.22888\n",
      "Training Epoch 15  30.3% | batch:       208 of       686\t|\tloss: 3.44706\n",
      "Training Epoch 15  30.5% | batch:       209 of       686\t|\tloss: 3.95506\n",
      "Training Epoch 15  30.6% | batch:       210 of       686\t|\tloss: 3.9175\n",
      "Training Epoch 15  30.8% | batch:       211 of       686\t|\tloss: 2.54641\n",
      "Training Epoch 15  30.9% | batch:       212 of       686\t|\tloss: 3.87801\n",
      "Training Epoch 15  31.0% | batch:       213 of       686\t|\tloss: 3.69974\n",
      "Training Epoch 15  31.2% | batch:       214 of       686\t|\tloss: 4.32667\n",
      "Training Epoch 15  31.3% | batch:       215 of       686\t|\tloss: 3.10394\n",
      "Training Epoch 15  31.5% | batch:       216 of       686\t|\tloss: 4.2483\n",
      "Training Epoch 15  31.6% | batch:       217 of       686\t|\tloss: 4.41807\n",
      "Training Epoch 15  31.8% | batch:       218 of       686\t|\tloss: 3.25165\n",
      "Training Epoch 15  31.9% | batch:       219 of       686\t|\tloss: 3.63316\n",
      "Training Epoch 15  32.1% | batch:       220 of       686\t|\tloss: 4.2713\n",
      "Training Epoch 15  32.2% | batch:       221 of       686\t|\tloss: 4.64538\n",
      "Training Epoch 15  32.4% | batch:       222 of       686\t|\tloss: 3.33326\n",
      "Training Epoch 15  32.5% | batch:       223 of       686\t|\tloss: 3.5831\n",
      "Training Epoch 15  32.7% | batch:       224 of       686\t|\tloss: 3.65202\n",
      "Training Epoch 15  32.8% | batch:       225 of       686\t|\tloss: 5.55617\n",
      "Training Epoch 15  32.9% | batch:       226 of       686\t|\tloss: 2.60384\n",
      "Training Epoch 15  33.1% | batch:       227 of       686\t|\tloss: 6.68272\n",
      "Training Epoch 15  33.2% | batch:       228 of       686\t|\tloss: 5.29486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 15  33.4% | batch:       229 of       686\t|\tloss: 5.31337\n",
      "Training Epoch 15  33.5% | batch:       230 of       686\t|\tloss: 4.61839\n",
      "Training Epoch 15  33.7% | batch:       231 of       686\t|\tloss: 3.67975\n",
      "Training Epoch 15  33.8% | batch:       232 of       686\t|\tloss: 3.6966\n",
      "Training Epoch 15  34.0% | batch:       233 of       686\t|\tloss: 3.32067\n",
      "Training Epoch 15  34.1% | batch:       234 of       686\t|\tloss: 3.53423\n",
      "Training Epoch 15  34.3% | batch:       235 of       686\t|\tloss: 3.07711\n",
      "Training Epoch 15  34.4% | batch:       236 of       686\t|\tloss: 3.43993\n",
      "Training Epoch 15  34.5% | batch:       237 of       686\t|\tloss: 3.6352\n",
      "Training Epoch 15  34.7% | batch:       238 of       686\t|\tloss: 3.41217\n",
      "Training Epoch 15  34.8% | batch:       239 of       686\t|\tloss: 3.29081\n",
      "Training Epoch 15  35.0% | batch:       240 of       686\t|\tloss: 3.43824\n",
      "Training Epoch 15  35.1% | batch:       241 of       686\t|\tloss: 2.96223\n",
      "Training Epoch 15  35.3% | batch:       242 of       686\t|\tloss: 2.77105\n",
      "Training Epoch 15  35.4% | batch:       243 of       686\t|\tloss: 3.04128\n",
      "Training Epoch 15  35.6% | batch:       244 of       686\t|\tloss: 4.39512\n",
      "Training Epoch 15  35.7% | batch:       245 of       686\t|\tloss: 3.4144\n",
      "Training Epoch 15  35.9% | batch:       246 of       686\t|\tloss: 4.30414\n",
      "Training Epoch 15  36.0% | batch:       247 of       686\t|\tloss: 3.76234\n",
      "Training Epoch 15  36.2% | batch:       248 of       686\t|\tloss: 4.25671\n",
      "Training Epoch 15  36.3% | batch:       249 of       686\t|\tloss: 3.05684\n",
      "Training Epoch 15  36.4% | batch:       250 of       686\t|\tloss: 4.76351\n",
      "Training Epoch 15  36.6% | batch:       251 of       686\t|\tloss: 3.14432\n",
      "Training Epoch 15  36.7% | batch:       252 of       686\t|\tloss: 3.88436\n",
      "Training Epoch 15  36.9% | batch:       253 of       686\t|\tloss: 3.66177\n",
      "Training Epoch 15  37.0% | batch:       254 of       686\t|\tloss: 3.63058\n",
      "Training Epoch 15  37.2% | batch:       255 of       686\t|\tloss: 4.17962\n",
      "Training Epoch 15  37.3% | batch:       256 of       686\t|\tloss: 2.99947\n",
      "Training Epoch 15  37.5% | batch:       257 of       686\t|\tloss: 3.77238\n",
      "Training Epoch 15  37.6% | batch:       258 of       686\t|\tloss: 4.37149\n",
      "Training Epoch 15  37.8% | batch:       259 of       686\t|\tloss: 3.54807\n",
      "Training Epoch 15  37.9% | batch:       260 of       686\t|\tloss: 3.57746\n",
      "Training Epoch 15  38.0% | batch:       261 of       686\t|\tloss: 4.11638\n",
      "Training Epoch 15  38.2% | batch:       262 of       686\t|\tloss: 3.38169\n",
      "Training Epoch 15  38.3% | batch:       263 of       686\t|\tloss: 3.21663\n",
      "Training Epoch 15  38.5% | batch:       264 of       686\t|\tloss: 3.97873\n",
      "Training Epoch 15  38.6% | batch:       265 of       686\t|\tloss: 4.73733\n",
      "Training Epoch 15  38.8% | batch:       266 of       686\t|\tloss: 3.90925\n",
      "Training Epoch 15  38.9% | batch:       267 of       686\t|\tloss: 3.9497\n",
      "Training Epoch 15  39.1% | batch:       268 of       686\t|\tloss: 2.84208\n",
      "Training Epoch 15  39.2% | batch:       269 of       686\t|\tloss: 4.26309\n",
      "Training Epoch 15  39.4% | batch:       270 of       686\t|\tloss: 2.51956\n",
      "Training Epoch 15  39.5% | batch:       271 of       686\t|\tloss: 3.03294\n",
      "Training Epoch 15  39.7% | batch:       272 of       686\t|\tloss: 2.40428\n",
      "Training Epoch 15  39.8% | batch:       273 of       686\t|\tloss: 3.21564\n",
      "Training Epoch 15  39.9% | batch:       274 of       686\t|\tloss: 3.61913\n",
      "Training Epoch 15  40.1% | batch:       275 of       686\t|\tloss: 2.76592\n",
      "Training Epoch 15  40.2% | batch:       276 of       686\t|\tloss: 3.8976\n",
      "Training Epoch 15  40.4% | batch:       277 of       686\t|\tloss: 3.77172\n",
      "Training Epoch 15  40.5% | batch:       278 of       686\t|\tloss: 6.0371\n",
      "Training Epoch 15  40.7% | batch:       279 of       686\t|\tloss: 3.20497\n",
      "Training Epoch 15  40.8% | batch:       280 of       686\t|\tloss: 2.92418\n",
      "Training Epoch 15  41.0% | batch:       281 of       686\t|\tloss: 4.0363\n",
      "Training Epoch 15  41.1% | batch:       282 of       686\t|\tloss: 2.66973\n",
      "Training Epoch 15  41.3% | batch:       283 of       686\t|\tloss: 3.73826\n",
      "Training Epoch 15  41.4% | batch:       284 of       686\t|\tloss: 3.75512\n",
      "Training Epoch 15  41.5% | batch:       285 of       686\t|\tloss: 4.36728\n",
      "Training Epoch 15  41.7% | batch:       286 of       686\t|\tloss: 3.70438\n",
      "Training Epoch 15  41.8% | batch:       287 of       686\t|\tloss: 3.96698\n",
      "Training Epoch 15  42.0% | batch:       288 of       686\t|\tloss: 3.80461\n",
      "Training Epoch 15  42.1% | batch:       289 of       686\t|\tloss: 3.68061\n",
      "Training Epoch 15  42.3% | batch:       290 of       686\t|\tloss: 3.66537\n",
      "Training Epoch 15  42.4% | batch:       291 of       686\t|\tloss: 3.72092\n",
      "Training Epoch 15  42.6% | batch:       292 of       686\t|\tloss: 3.33457\n",
      "Training Epoch 15  42.7% | batch:       293 of       686\t|\tloss: 4.06877\n",
      "Training Epoch 15  42.9% | batch:       294 of       686\t|\tloss: 3.46541\n",
      "Training Epoch 15  43.0% | batch:       295 of       686\t|\tloss: 3.21019\n",
      "Training Epoch 15  43.1% | batch:       296 of       686\t|\tloss: 2.55164\n",
      "Training Epoch 15  43.3% | batch:       297 of       686\t|\tloss: 3.7103\n",
      "Training Epoch 15  43.4% | batch:       298 of       686\t|\tloss: 3.52566\n",
      "Training Epoch 15  43.6% | batch:       299 of       686\t|\tloss: 4.15904\n",
      "Training Epoch 15  43.7% | batch:       300 of       686\t|\tloss: 4.70776\n",
      "Training Epoch 15  43.9% | batch:       301 of       686\t|\tloss: 3.00174\n",
      "Training Epoch 15  44.0% | batch:       302 of       686\t|\tloss: 3.76231\n",
      "Training Epoch 15  44.2% | batch:       303 of       686\t|\tloss: 4.43576\n",
      "Training Epoch 15  44.3% | batch:       304 of       686\t|\tloss: 3.63621\n",
      "Training Epoch 15  44.5% | batch:       305 of       686\t|\tloss: 3.98895\n",
      "Training Epoch 15  44.6% | batch:       306 of       686\t|\tloss: 3.69688\n",
      "Training Epoch 15  44.8% | batch:       307 of       686\t|\tloss: 4.53549\n",
      "Training Epoch 15  44.9% | batch:       308 of       686\t|\tloss: 3.63097\n",
      "Training Epoch 15  45.0% | batch:       309 of       686\t|\tloss: 3.69255\n",
      "Training Epoch 15  45.2% | batch:       310 of       686\t|\tloss: 2.75403\n",
      "Training Epoch 15  45.3% | batch:       311 of       686\t|\tloss: 2.50918\n",
      "Training Epoch 15  45.5% | batch:       312 of       686\t|\tloss: 3.32538\n",
      "Training Epoch 15  45.6% | batch:       313 of       686\t|\tloss: 3.20632\n",
      "Training Epoch 15  45.8% | batch:       314 of       686\t|\tloss: 3.696\n",
      "Training Epoch 15  45.9% | batch:       315 of       686\t|\tloss: 2.90539\n",
      "Training Epoch 15  46.1% | batch:       316 of       686\t|\tloss: 3.30807\n",
      "Training Epoch 15  46.2% | batch:       317 of       686\t|\tloss: 4.05476\n",
      "Training Epoch 15  46.4% | batch:       318 of       686\t|\tloss: 3.72971\n",
      "Training Epoch 15  46.5% | batch:       319 of       686\t|\tloss: 3.92948\n",
      "Training Epoch 15  46.6% | batch:       320 of       686\t|\tloss: 2.49236\n",
      "Training Epoch 15  46.8% | batch:       321 of       686\t|\tloss: 3.46579\n",
      "Training Epoch 15  46.9% | batch:       322 of       686\t|\tloss: 3.05862\n",
      "Training Epoch 15  47.1% | batch:       323 of       686\t|\tloss: 3.35507\n",
      "Training Epoch 15  47.2% | batch:       324 of       686\t|\tloss: 3.23182\n",
      "Training Epoch 15  47.4% | batch:       325 of       686\t|\tloss: 3.4663\n",
      "Training Epoch 15  47.5% | batch:       326 of       686\t|\tloss: 3.60875\n",
      "Training Epoch 15  47.7% | batch:       327 of       686\t|\tloss: 3.08154\n",
      "Training Epoch 15  47.8% | batch:       328 of       686\t|\tloss: 3.58444\n",
      "Training Epoch 15  48.0% | batch:       329 of       686\t|\tloss: 2.76936\n",
      "Training Epoch 15  48.1% | batch:       330 of       686\t|\tloss: 4.70834\n",
      "Training Epoch 15  48.3% | batch:       331 of       686\t|\tloss: 3.80217\n",
      "Training Epoch 15  48.4% | batch:       332 of       686\t|\tloss: 3.04119\n",
      "Training Epoch 15  48.5% | batch:       333 of       686\t|\tloss: 5.07394\n",
      "Training Epoch 15  48.7% | batch:       334 of       686\t|\tloss: 4.15984\n",
      "Training Epoch 15  48.8% | batch:       335 of       686\t|\tloss: 4.1335\n",
      "Training Epoch 15  49.0% | batch:       336 of       686\t|\tloss: 4.74001\n",
      "Training Epoch 15  49.1% | batch:       337 of       686\t|\tloss: 3.28997\n",
      "Training Epoch 15  49.3% | batch:       338 of       686\t|\tloss: 2.92223\n",
      "Training Epoch 15  49.4% | batch:       339 of       686\t|\tloss: 4.1162\n",
      "Training Epoch 15  49.6% | batch:       340 of       686\t|\tloss: 2.39856\n",
      "Training Epoch 15  49.7% | batch:       341 of       686\t|\tloss: 3.27237\n",
      "Training Epoch 15  49.9% | batch:       342 of       686\t|\tloss: 3.97499\n",
      "Training Epoch 15  50.0% | batch:       343 of       686\t|\tloss: 3.51312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 15  50.1% | batch:       344 of       686\t|\tloss: 4.27971\n",
      "Training Epoch 15  50.3% | batch:       345 of       686\t|\tloss: 4.27039\n",
      "Training Epoch 15  50.4% | batch:       346 of       686\t|\tloss: 3.67657\n",
      "Training Epoch 15  50.6% | batch:       347 of       686\t|\tloss: 4.23454\n",
      "Training Epoch 15  50.7% | batch:       348 of       686\t|\tloss: 3.76497\n",
      "Training Epoch 15  50.9% | batch:       349 of       686\t|\tloss: 3.7242\n",
      "Training Epoch 15  51.0% | batch:       350 of       686\t|\tloss: 4.12652\n",
      "Training Epoch 15  51.2% | batch:       351 of       686\t|\tloss: 2.58285\n",
      "Training Epoch 15  51.3% | batch:       352 of       686\t|\tloss: 3.01489\n",
      "Training Epoch 15  51.5% | batch:       353 of       686\t|\tloss: 3.74762\n",
      "Training Epoch 15  51.6% | batch:       354 of       686\t|\tloss: 3.62352\n",
      "Training Epoch 15  51.7% | batch:       355 of       686\t|\tloss: 4.45493\n",
      "Training Epoch 15  51.9% | batch:       356 of       686\t|\tloss: 5.00089\n",
      "Training Epoch 15  52.0% | batch:       357 of       686\t|\tloss: 4.98926\n",
      "Training Epoch 15  52.2% | batch:       358 of       686\t|\tloss: 4.1835\n",
      "Training Epoch 15  52.3% | batch:       359 of       686\t|\tloss: 3.06269\n",
      "Training Epoch 15  52.5% | batch:       360 of       686\t|\tloss: 3.05854\n",
      "Training Epoch 15  52.6% | batch:       361 of       686\t|\tloss: 3.16953\n",
      "Training Epoch 15  52.8% | batch:       362 of       686\t|\tloss: 5.11191\n",
      "Training Epoch 15  52.9% | batch:       363 of       686\t|\tloss: 5.00004\n",
      "Training Epoch 15  53.1% | batch:       364 of       686\t|\tloss: 3.00378\n",
      "Training Epoch 15  53.2% | batch:       365 of       686\t|\tloss: 3.33735\n",
      "Training Epoch 15  53.4% | batch:       366 of       686\t|\tloss: 2.93412\n",
      "Training Epoch 15  53.5% | batch:       367 of       686\t|\tloss: 2.77523\n",
      "Training Epoch 15  53.6% | batch:       368 of       686\t|\tloss: 3.35497\n",
      "Training Epoch 15  53.8% | batch:       369 of       686\t|\tloss: 3.5792\n",
      "Training Epoch 15  53.9% | batch:       370 of       686\t|\tloss: 3.57755\n",
      "Training Epoch 15  54.1% | batch:       371 of       686\t|\tloss: 3.22508\n",
      "Training Epoch 15  54.2% | batch:       372 of       686\t|\tloss: 3.57511\n",
      "Training Epoch 15  54.4% | batch:       373 of       686\t|\tloss: 4.10068\n",
      "Training Epoch 15  54.5% | batch:       374 of       686\t|\tloss: 3.49743\n",
      "Training Epoch 15  54.7% | batch:       375 of       686\t|\tloss: 2.79477\n",
      "Training Epoch 15  54.8% | batch:       376 of       686\t|\tloss: 3.23209\n",
      "Training Epoch 15  55.0% | batch:       377 of       686\t|\tloss: 3.20324\n",
      "Training Epoch 15  55.1% | batch:       378 of       686\t|\tloss: 2.86107\n",
      "Training Epoch 15  55.2% | batch:       379 of       686\t|\tloss: 2.95312\n",
      "Training Epoch 15  55.4% | batch:       380 of       686\t|\tloss: 3.28325\n",
      "Training Epoch 15  55.5% | batch:       381 of       686\t|\tloss: 3.68428\n",
      "Training Epoch 15  55.7% | batch:       382 of       686\t|\tloss: 3.46372\n",
      "Training Epoch 15  55.8% | batch:       383 of       686\t|\tloss: 3.9274\n",
      "Training Epoch 15  56.0% | batch:       384 of       686\t|\tloss: 3.36401\n",
      "Training Epoch 15  56.1% | batch:       385 of       686\t|\tloss: 3.59173\n",
      "Training Epoch 15  56.3% | batch:       386 of       686\t|\tloss: 5.04\n",
      "Training Epoch 15  56.4% | batch:       387 of       686\t|\tloss: 2.46014\n",
      "Training Epoch 15  56.6% | batch:       388 of       686\t|\tloss: 3.46263\n",
      "Training Epoch 15  56.7% | batch:       389 of       686\t|\tloss: 3.55907\n",
      "Training Epoch 15  56.9% | batch:       390 of       686\t|\tloss: 3.57997\n",
      "Training Epoch 15  57.0% | batch:       391 of       686\t|\tloss: 3.33365\n",
      "Training Epoch 15  57.1% | batch:       392 of       686\t|\tloss: 2.91126\n",
      "Training Epoch 15  57.3% | batch:       393 of       686\t|\tloss: 3.74461\n",
      "Training Epoch 15  57.4% | batch:       394 of       686\t|\tloss: 2.63355\n",
      "Training Epoch 15  57.6% | batch:       395 of       686\t|\tloss: 3.77379\n",
      "Training Epoch 15  57.7% | batch:       396 of       686\t|\tloss: 2.61837\n",
      "Training Epoch 15  57.9% | batch:       397 of       686\t|\tloss: 2.85508\n",
      "Training Epoch 15  58.0% | batch:       398 of       686\t|\tloss: 3.17122\n",
      "Training Epoch 15  58.2% | batch:       399 of       686\t|\tloss: 3.32892\n",
      "Training Epoch 15  58.3% | batch:       400 of       686\t|\tloss: 4.69934\n",
      "Training Epoch 15  58.5% | batch:       401 of       686\t|\tloss: 4.01904\n",
      "Training Epoch 15  58.6% | batch:       402 of       686\t|\tloss: 2.98818\n",
      "Training Epoch 15  58.7% | batch:       403 of       686\t|\tloss: 3.11772\n",
      "Training Epoch 15  58.9% | batch:       404 of       686\t|\tloss: 4.7423\n",
      "Training Epoch 15  59.0% | batch:       405 of       686\t|\tloss: 2.9579\n",
      "Training Epoch 15  59.2% | batch:       406 of       686\t|\tloss: 4.4415\n",
      "Training Epoch 15  59.3% | batch:       407 of       686\t|\tloss: 3.29511\n",
      "Training Epoch 15  59.5% | batch:       408 of       686\t|\tloss: 3.45012\n",
      "Training Epoch 15  59.6% | batch:       409 of       686\t|\tloss: 3.17688\n",
      "Training Epoch 15  59.8% | batch:       410 of       686\t|\tloss: 4.4144\n",
      "Training Epoch 15  59.9% | batch:       411 of       686\t|\tloss: 3.12444\n",
      "Training Epoch 15  60.1% | batch:       412 of       686\t|\tloss: 3.25308\n",
      "Training Epoch 15  60.2% | batch:       413 of       686\t|\tloss: 4.51506\n",
      "Training Epoch 15  60.3% | batch:       414 of       686\t|\tloss: 4.18793\n",
      "Training Epoch 15  60.5% | batch:       415 of       686\t|\tloss: 4.34727\n",
      "Training Epoch 15  60.6% | batch:       416 of       686\t|\tloss: 3.88447\n",
      "Training Epoch 15  60.8% | batch:       417 of       686\t|\tloss: 3.94632\n",
      "Training Epoch 15  60.9% | batch:       418 of       686\t|\tloss: 3.11223\n",
      "Training Epoch 15  61.1% | batch:       419 of       686\t|\tloss: 3.34224\n",
      "Training Epoch 15  61.2% | batch:       420 of       686\t|\tloss: 4.1313\n",
      "Training Epoch 15  61.4% | batch:       421 of       686\t|\tloss: 3.52457\n",
      "Training Epoch 15  61.5% | batch:       422 of       686\t|\tloss: 4.05395\n",
      "Training Epoch 15  61.7% | batch:       423 of       686\t|\tloss: 4.01457\n",
      "Training Epoch 15  61.8% | batch:       424 of       686\t|\tloss: 4.33273\n",
      "Training Epoch 15  62.0% | batch:       425 of       686\t|\tloss: 4.42118\n",
      "Training Epoch 15  62.1% | batch:       426 of       686\t|\tloss: 4.0101\n",
      "Training Epoch 15  62.2% | batch:       427 of       686\t|\tloss: 3.74991\n",
      "Training Epoch 15  62.4% | batch:       428 of       686\t|\tloss: 3.33223\n",
      "Training Epoch 15  62.5% | batch:       429 of       686\t|\tloss: 3.4602\n",
      "Training Epoch 15  62.7% | batch:       430 of       686\t|\tloss: 3.26426\n",
      "Training Epoch 15  62.8% | batch:       431 of       686\t|\tloss: 3.11166\n",
      "Training Epoch 15  63.0% | batch:       432 of       686\t|\tloss: 3.51274\n",
      "Training Epoch 15  63.1% | batch:       433 of       686\t|\tloss: 3.4688\n",
      "Training Epoch 15  63.3% | batch:       434 of       686\t|\tloss: 3.62098\n",
      "Training Epoch 15  63.4% | batch:       435 of       686\t|\tloss: 2.9807\n",
      "Training Epoch 15  63.6% | batch:       436 of       686\t|\tloss: 2.26689\n",
      "Training Epoch 15  63.7% | batch:       437 of       686\t|\tloss: 4.31809\n",
      "Training Epoch 15  63.8% | batch:       438 of       686\t|\tloss: 3.46599\n",
      "Training Epoch 15  64.0% | batch:       439 of       686\t|\tloss: 4.73298\n",
      "Training Epoch 15  64.1% | batch:       440 of       686\t|\tloss: 4.39838\n",
      "Training Epoch 15  64.3% | batch:       441 of       686\t|\tloss: 3.32674\n",
      "Training Epoch 15  64.4% | batch:       442 of       686\t|\tloss: 3.61726\n",
      "Training Epoch 15  64.6% | batch:       443 of       686\t|\tloss: 3.66926\n",
      "Training Epoch 15  64.7% | batch:       444 of       686\t|\tloss: 3.32149\n",
      "Training Epoch 15  64.9% | batch:       445 of       686\t|\tloss: 3.48966\n",
      "Training Epoch 15  65.0% | batch:       446 of       686\t|\tloss: 4.83873\n",
      "Training Epoch 15  65.2% | batch:       447 of       686\t|\tloss: 2.94047\n",
      "Training Epoch 15  65.3% | batch:       448 of       686\t|\tloss: 3.34197\n",
      "Training Epoch 15  65.5% | batch:       449 of       686\t|\tloss: 3.08835\n",
      "Training Epoch 15  65.6% | batch:       450 of       686\t|\tloss: 3.55465\n",
      "Training Epoch 15  65.7% | batch:       451 of       686\t|\tloss: 4.22353\n",
      "Training Epoch 15  65.9% | batch:       452 of       686\t|\tloss: 2.74559\n",
      "Training Epoch 15  66.0% | batch:       453 of       686\t|\tloss: 2.71175\n",
      "Training Epoch 15  66.2% | batch:       454 of       686\t|\tloss: 3.53448\n",
      "Training Epoch 15  66.3% | batch:       455 of       686\t|\tloss: 3.35355\n",
      "Training Epoch 15  66.5% | batch:       456 of       686\t|\tloss: 4.0705\n",
      "Training Epoch 15  66.6% | batch:       457 of       686\t|\tloss: 3.08965\n",
      "Training Epoch 15  66.8% | batch:       458 of       686\t|\tloss: 4.18759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 15  66.9% | batch:       459 of       686\t|\tloss: 2.58064\n",
      "Training Epoch 15  67.1% | batch:       460 of       686\t|\tloss: 2.7178\n",
      "Training Epoch 15  67.2% | batch:       461 of       686\t|\tloss: 3.42356\n",
      "Training Epoch 15  67.3% | batch:       462 of       686\t|\tloss: 3.11868\n",
      "Training Epoch 15  67.5% | batch:       463 of       686\t|\tloss: 3.39494\n",
      "Training Epoch 15  67.6% | batch:       464 of       686\t|\tloss: 3.46257\n",
      "Training Epoch 15  67.8% | batch:       465 of       686\t|\tloss: 4.60371\n",
      "Training Epoch 15  67.9% | batch:       466 of       686\t|\tloss: 3.61334\n",
      "Training Epoch 15  68.1% | batch:       467 of       686\t|\tloss: 3.52153\n",
      "Training Epoch 15  68.2% | batch:       468 of       686\t|\tloss: 3.93038\n",
      "Training Epoch 15  68.4% | batch:       469 of       686\t|\tloss: 2.7801\n",
      "Training Epoch 15  68.5% | batch:       470 of       686\t|\tloss: 3.48471\n",
      "Training Epoch 15  68.7% | batch:       471 of       686\t|\tloss: 2.95292\n",
      "Training Epoch 15  68.8% | batch:       472 of       686\t|\tloss: 3.92696\n",
      "Training Epoch 15  69.0% | batch:       473 of       686\t|\tloss: 2.74019\n",
      "Training Epoch 15  69.1% | batch:       474 of       686\t|\tloss: 3.5303\n",
      "Training Epoch 15  69.2% | batch:       475 of       686\t|\tloss: 3.86139\n",
      "Training Epoch 15  69.4% | batch:       476 of       686\t|\tloss: 2.88876\n",
      "Training Epoch 15  69.5% | batch:       477 of       686\t|\tloss: 3.50726\n",
      "Training Epoch 15  69.7% | batch:       478 of       686\t|\tloss: 3.9692\n",
      "Training Epoch 15  69.8% | batch:       479 of       686\t|\tloss: 3.53514\n",
      "Training Epoch 15  70.0% | batch:       480 of       686\t|\tloss: 3.2866\n",
      "Training Epoch 15  70.1% | batch:       481 of       686\t|\tloss: 3.10174\n",
      "Training Epoch 15  70.3% | batch:       482 of       686\t|\tloss: 3.94889\n",
      "Training Epoch 15  70.4% | batch:       483 of       686\t|\tloss: 3.49193\n",
      "Training Epoch 15  70.6% | batch:       484 of       686\t|\tloss: 2.945\n",
      "Training Epoch 15  70.7% | batch:       485 of       686\t|\tloss: 3.57683\n",
      "Training Epoch 15  70.8% | batch:       486 of       686\t|\tloss: 3.68547\n",
      "Training Epoch 15  71.0% | batch:       487 of       686\t|\tloss: 2.70076\n",
      "Training Epoch 15  71.1% | batch:       488 of       686\t|\tloss: 4.65958\n",
      "Training Epoch 15  71.3% | batch:       489 of       686\t|\tloss: 3.73222\n",
      "Training Epoch 15  71.4% | batch:       490 of       686\t|\tloss: 4.73646\n",
      "Training Epoch 15  71.6% | batch:       491 of       686\t|\tloss: 3.04956\n",
      "Training Epoch 15  71.7% | batch:       492 of       686\t|\tloss: 2.72209\n",
      "Training Epoch 15  71.9% | batch:       493 of       686\t|\tloss: 3.47436\n",
      "Training Epoch 15  72.0% | batch:       494 of       686\t|\tloss: 3.91047\n",
      "Training Epoch 15  72.2% | batch:       495 of       686\t|\tloss: 2.94241\n",
      "Training Epoch 15  72.3% | batch:       496 of       686\t|\tloss: 3.3827\n",
      "Training Epoch 15  72.4% | batch:       497 of       686\t|\tloss: 3.60407\n",
      "Training Epoch 15  72.6% | batch:       498 of       686\t|\tloss: 3.05483\n",
      "Training Epoch 15  72.7% | batch:       499 of       686\t|\tloss: 3.83867\n",
      "Training Epoch 15  72.9% | batch:       500 of       686\t|\tloss: 3.51839\n",
      "Training Epoch 15  73.0% | batch:       501 of       686\t|\tloss: 4.08085\n",
      "Training Epoch 15  73.2% | batch:       502 of       686\t|\tloss: 2.81786\n",
      "Training Epoch 15  73.3% | batch:       503 of       686\t|\tloss: 4.1563\n",
      "Training Epoch 15  73.5% | batch:       504 of       686\t|\tloss: 3.8944\n",
      "Training Epoch 15  73.6% | batch:       505 of       686\t|\tloss: 3.37176\n",
      "Training Epoch 15  73.8% | batch:       506 of       686\t|\tloss: 3.96224\n",
      "Training Epoch 15  73.9% | batch:       507 of       686\t|\tloss: 3.70419\n",
      "Training Epoch 15  74.1% | batch:       508 of       686\t|\tloss: 3.76316\n",
      "Training Epoch 15  74.2% | batch:       509 of       686\t|\tloss: 2.9338\n",
      "Training Epoch 15  74.3% | batch:       510 of       686\t|\tloss: 4.43147\n",
      "Training Epoch 15  74.5% | batch:       511 of       686\t|\tloss: 3.38978\n",
      "Training Epoch 15  74.6% | batch:       512 of       686\t|\tloss: 3.37445\n",
      "Training Epoch 15  74.8% | batch:       513 of       686\t|\tloss: 3.52051\n",
      "Training Epoch 15  74.9% | batch:       514 of       686\t|\tloss: 2.83639\n",
      "Training Epoch 15  75.1% | batch:       515 of       686\t|\tloss: 2.40838\n",
      "Training Epoch 15  75.2% | batch:       516 of       686\t|\tloss: 3.24077\n",
      "Training Epoch 15  75.4% | batch:       517 of       686\t|\tloss: 2.72435\n",
      "Training Epoch 15  75.5% | batch:       518 of       686\t|\tloss: 3.13799\n",
      "Training Epoch 15  75.7% | batch:       519 of       686\t|\tloss: 3.59176\n",
      "Training Epoch 15  75.8% | batch:       520 of       686\t|\tloss: 4.04371\n",
      "Training Epoch 15  75.9% | batch:       521 of       686\t|\tloss: 2.61773\n",
      "Training Epoch 15  76.1% | batch:       522 of       686\t|\tloss: 4.45124\n",
      "Training Epoch 15  76.2% | batch:       523 of       686\t|\tloss: 4.32232\n",
      "Training Epoch 15  76.4% | batch:       524 of       686\t|\tloss: 3.10505\n",
      "Training Epoch 15  76.5% | batch:       525 of       686\t|\tloss: 4.36551\n",
      "Training Epoch 15  76.7% | batch:       526 of       686\t|\tloss: 2.81294\n",
      "Training Epoch 15  76.8% | batch:       527 of       686\t|\tloss: 4.72911\n",
      "Training Epoch 15  77.0% | batch:       528 of       686\t|\tloss: 4.22634\n",
      "Training Epoch 15  77.1% | batch:       529 of       686\t|\tloss: 3.77014\n",
      "Training Epoch 15  77.3% | batch:       530 of       686\t|\tloss: 4.7529\n",
      "Training Epoch 15  77.4% | batch:       531 of       686\t|\tloss: 3.22949\n",
      "Training Epoch 15  77.6% | batch:       532 of       686\t|\tloss: 5.41707\n",
      "Training Epoch 15  77.7% | batch:       533 of       686\t|\tloss: 3.95449\n",
      "Training Epoch 15  77.8% | batch:       534 of       686\t|\tloss: 3.88352\n",
      "Training Epoch 15  78.0% | batch:       535 of       686\t|\tloss: 2.82809\n",
      "Training Epoch 15  78.1% | batch:       536 of       686\t|\tloss: 4.0186\n",
      "Training Epoch 15  78.3% | batch:       537 of       686\t|\tloss: 2.92105\n",
      "Training Epoch 15  78.4% | batch:       538 of       686\t|\tloss: 3.81384\n",
      "Training Epoch 15  78.6% | batch:       539 of       686\t|\tloss: 4.01879\n",
      "Training Epoch 15  78.7% | batch:       540 of       686\t|\tloss: 3.53945\n",
      "Training Epoch 15  78.9% | batch:       541 of       686\t|\tloss: 3.9545\n",
      "Training Epoch 15  79.0% | batch:       542 of       686\t|\tloss: 3.28119\n",
      "Training Epoch 15  79.2% | batch:       543 of       686\t|\tloss: 3.14389\n",
      "Training Epoch 15  79.3% | batch:       544 of       686\t|\tloss: 3.11779\n",
      "Training Epoch 15  79.4% | batch:       545 of       686\t|\tloss: 4.00846\n",
      "Training Epoch 15  79.6% | batch:       546 of       686\t|\tloss: 3.43185\n",
      "Training Epoch 15  79.7% | batch:       547 of       686\t|\tloss: 3.08028\n",
      "Training Epoch 15  79.9% | batch:       548 of       686\t|\tloss: 2.86136\n",
      "Training Epoch 15  80.0% | batch:       549 of       686\t|\tloss: 2.45199\n",
      "Training Epoch 15  80.2% | batch:       550 of       686\t|\tloss: 2.81512\n",
      "Training Epoch 15  80.3% | batch:       551 of       686\t|\tloss: 4.77034\n",
      "Training Epoch 15  80.5% | batch:       552 of       686\t|\tloss: 3.25196\n",
      "Training Epoch 15  80.6% | batch:       553 of       686\t|\tloss: 3.73257\n",
      "Training Epoch 15  80.8% | batch:       554 of       686\t|\tloss: 3.41515\n",
      "Training Epoch 15  80.9% | batch:       555 of       686\t|\tloss: 2.98953\n",
      "Training Epoch 15  81.0% | batch:       556 of       686\t|\tloss: 4.38292\n",
      "Training Epoch 15  81.2% | batch:       557 of       686\t|\tloss: 3.13818\n",
      "Training Epoch 15  81.3% | batch:       558 of       686\t|\tloss: 2.85259\n",
      "Training Epoch 15  81.5% | batch:       559 of       686\t|\tloss: 3.93656\n",
      "Training Epoch 15  81.6% | batch:       560 of       686\t|\tloss: 3.16418\n",
      "Training Epoch 15  81.8% | batch:       561 of       686\t|\tloss: 2.95856\n",
      "Training Epoch 15  81.9% | batch:       562 of       686\t|\tloss: 2.90116\n",
      "Training Epoch 15  82.1% | batch:       563 of       686\t|\tloss: 3.65864\n",
      "Training Epoch 15  82.2% | batch:       564 of       686\t|\tloss: 2.78062\n",
      "Training Epoch 15  82.4% | batch:       565 of       686\t|\tloss: 4.38726\n",
      "Training Epoch 15  82.5% | batch:       566 of       686\t|\tloss: 2.58534\n",
      "Training Epoch 15  82.7% | batch:       567 of       686\t|\tloss: 3.29006\n",
      "Training Epoch 15  82.8% | batch:       568 of       686\t|\tloss: 4.0139\n",
      "Training Epoch 15  82.9% | batch:       569 of       686\t|\tloss: 4.40347\n",
      "Training Epoch 15  83.1% | batch:       570 of       686\t|\tloss: 3.74368\n",
      "Training Epoch 15  83.2% | batch:       571 of       686\t|\tloss: 2.65817\n",
      "Training Epoch 15  83.4% | batch:       572 of       686\t|\tloss: 2.99735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 15  83.5% | batch:       573 of       686\t|\tloss: 2.91455\n",
      "Training Epoch 15  83.7% | batch:       574 of       686\t|\tloss: 3.99751\n",
      "Training Epoch 15  83.8% | batch:       575 of       686\t|\tloss: 3.17412\n",
      "Training Epoch 15  84.0% | batch:       576 of       686\t|\tloss: 3.5007\n",
      "Training Epoch 15  84.1% | batch:       577 of       686\t|\tloss: 4.10111\n",
      "Training Epoch 15  84.3% | batch:       578 of       686\t|\tloss: 2.56381\n",
      "Training Epoch 15  84.4% | batch:       579 of       686\t|\tloss: 3.11872\n",
      "Training Epoch 15  84.5% | batch:       580 of       686\t|\tloss: 3.78536\n",
      "Training Epoch 15  84.7% | batch:       581 of       686\t|\tloss: 3.28677\n",
      "Training Epoch 15  84.8% | batch:       582 of       686\t|\tloss: 2.98442\n",
      "Training Epoch 15  85.0% | batch:       583 of       686\t|\tloss: 4.35141\n",
      "Training Epoch 15  85.1% | batch:       584 of       686\t|\tloss: 2.7419\n",
      "Training Epoch 15  85.3% | batch:       585 of       686\t|\tloss: 3.86558\n",
      "Training Epoch 15  85.4% | batch:       586 of       686\t|\tloss: 2.93568\n",
      "Training Epoch 15  85.6% | batch:       587 of       686\t|\tloss: 3.18941\n",
      "Training Epoch 15  85.7% | batch:       588 of       686\t|\tloss: 2.93292\n",
      "Training Epoch 15  85.9% | batch:       589 of       686\t|\tloss: 3.08566\n",
      "Training Epoch 15  86.0% | batch:       590 of       686\t|\tloss: 4.92041\n",
      "Training Epoch 15  86.2% | batch:       591 of       686\t|\tloss: 2.7413\n",
      "Training Epoch 15  86.3% | batch:       592 of       686\t|\tloss: 4.44387\n",
      "Training Epoch 15  86.4% | batch:       593 of       686\t|\tloss: 4.06242\n",
      "Training Epoch 15  86.6% | batch:       594 of       686\t|\tloss: 3.05384\n",
      "Training Epoch 15  86.7% | batch:       595 of       686\t|\tloss: 2.99839\n",
      "Training Epoch 15  86.9% | batch:       596 of       686\t|\tloss: 2.75193\n",
      "Training Epoch 15  87.0% | batch:       597 of       686\t|\tloss: 3.11022\n",
      "Training Epoch 15  87.2% | batch:       598 of       686\t|\tloss: 3.14864\n",
      "Training Epoch 15  87.3% | batch:       599 of       686\t|\tloss: 3.9644\n",
      "Training Epoch 15  87.5% | batch:       600 of       686\t|\tloss: 3.528\n",
      "Training Epoch 15  87.6% | batch:       601 of       686\t|\tloss: 2.82023\n",
      "Training Epoch 15  87.8% | batch:       602 of       686\t|\tloss: 4.11204\n",
      "Training Epoch 15  87.9% | batch:       603 of       686\t|\tloss: 3.10912\n",
      "Training Epoch 15  88.0% | batch:       604 of       686\t|\tloss: 2.94724\n",
      "Training Epoch 15  88.2% | batch:       605 of       686\t|\tloss: 2.65516\n",
      "Training Epoch 15  88.3% | batch:       606 of       686\t|\tloss: 4.27192\n",
      "Training Epoch 15  88.5% | batch:       607 of       686\t|\tloss: 4.06436\n",
      "Training Epoch 15  88.6% | batch:       608 of       686\t|\tloss: 3.22511\n",
      "Training Epoch 15  88.8% | batch:       609 of       686\t|\tloss: 3.16704\n",
      "Training Epoch 15  88.9% | batch:       610 of       686\t|\tloss: 3.63844\n",
      "Training Epoch 15  89.1% | batch:       611 of       686\t|\tloss: 2.48491\n",
      "Training Epoch 15  89.2% | batch:       612 of       686\t|\tloss: 3.43935\n",
      "Training Epoch 15  89.4% | batch:       613 of       686\t|\tloss: 3.02017\n",
      "Training Epoch 15  89.5% | batch:       614 of       686\t|\tloss: 2.82604\n",
      "Training Epoch 15  89.7% | batch:       615 of       686\t|\tloss: 3.87403\n",
      "Training Epoch 15  89.8% | batch:       616 of       686\t|\tloss: 2.74655\n",
      "Training Epoch 15  89.9% | batch:       617 of       686\t|\tloss: 3.26204\n",
      "Training Epoch 15  90.1% | batch:       618 of       686\t|\tloss: 2.99917\n",
      "Training Epoch 15  90.2% | batch:       619 of       686\t|\tloss: 3.21952\n",
      "Training Epoch 15  90.4% | batch:       620 of       686\t|\tloss: 3.93044\n",
      "Training Epoch 15  90.5% | batch:       621 of       686\t|\tloss: 2.88475\n",
      "Training Epoch 15  90.7% | batch:       622 of       686\t|\tloss: 3.22162\n",
      "Training Epoch 15  90.8% | batch:       623 of       686\t|\tloss: 3.03657\n",
      "Training Epoch 15  91.0% | batch:       624 of       686\t|\tloss: 3.15055\n",
      "Training Epoch 15  91.1% | batch:       625 of       686\t|\tloss: 4.08315\n",
      "Training Epoch 15  91.3% | batch:       626 of       686\t|\tloss: 3.38318\n",
      "Training Epoch 15  91.4% | batch:       627 of       686\t|\tloss: 3.23063\n",
      "Training Epoch 15  91.5% | batch:       628 of       686\t|\tloss: 2.44381\n",
      "Training Epoch 15  91.7% | batch:       629 of       686\t|\tloss: 3.89633\n",
      "Training Epoch 15  91.8% | batch:       630 of       686\t|\tloss: 3.14328\n",
      "Training Epoch 15  92.0% | batch:       631 of       686\t|\tloss: 3.05703\n",
      "Training Epoch 15  92.1% | batch:       632 of       686\t|\tloss: 3.46292\n",
      "Training Epoch 15  92.3% | batch:       633 of       686\t|\tloss: 2.47597\n",
      "Training Epoch 15  92.4% | batch:       634 of       686\t|\tloss: 3.43981\n",
      "Training Epoch 15  92.6% | batch:       635 of       686\t|\tloss: 3.41806\n",
      "Training Epoch 15  92.7% | batch:       636 of       686\t|\tloss: 2.80636\n",
      "Training Epoch 15  92.9% | batch:       637 of       686\t|\tloss: 4.22925\n",
      "Training Epoch 15  93.0% | batch:       638 of       686\t|\tloss: 4.15121\n",
      "Training Epoch 15  93.1% | batch:       639 of       686\t|\tloss: 4.11716\n",
      "Training Epoch 15  93.3% | batch:       640 of       686\t|\tloss: 3.41187\n",
      "Training Epoch 15  93.4% | batch:       641 of       686\t|\tloss: 2.91021\n",
      "Training Epoch 15  93.6% | batch:       642 of       686\t|\tloss: 3.00731\n",
      "Training Epoch 15  93.7% | batch:       643 of       686\t|\tloss: 3.44566\n",
      "Training Epoch 15  93.9% | batch:       644 of       686\t|\tloss: 3.20953\n",
      "Training Epoch 15  94.0% | batch:       645 of       686\t|\tloss: 2.94234\n",
      "Training Epoch 15  94.2% | batch:       646 of       686\t|\tloss: 3.06757\n",
      "Training Epoch 15  94.3% | batch:       647 of       686\t|\tloss: 4.71618\n",
      "Training Epoch 15  94.5% | batch:       648 of       686\t|\tloss: 3.5665\n",
      "Training Epoch 15  94.6% | batch:       649 of       686\t|\tloss: 2.96036\n",
      "Training Epoch 15  94.8% | batch:       650 of       686\t|\tloss: 3.58293\n",
      "Training Epoch 15  94.9% | batch:       651 of       686\t|\tloss: 3.27536\n",
      "Training Epoch 15  95.0% | batch:       652 of       686\t|\tloss: 3.30657\n",
      "Training Epoch 15  95.2% | batch:       653 of       686\t|\tloss: 2.75097\n",
      "Training Epoch 15  95.3% | batch:       654 of       686\t|\tloss: 3.52805\n",
      "Training Epoch 15  95.5% | batch:       655 of       686\t|\tloss: 3.24244\n",
      "Training Epoch 15  95.6% | batch:       656 of       686\t|\tloss: 3.53915\n",
      "Training Epoch 15  95.8% | batch:       657 of       686\t|\tloss: 3.65541\n",
      "Training Epoch 15  95.9% | batch:       658 of       686\t|\tloss: 3.01892\n",
      "Training Epoch 15  96.1% | batch:       659 of       686\t|\tloss: 2.49589\n",
      "Training Epoch 15  96.2% | batch:       660 of       686\t|\tloss: 3.76931\n",
      "Training Epoch 15  96.4% | batch:       661 of       686\t|\tloss: 2.49633\n",
      "Training Epoch 15  96.5% | batch:       662 of       686\t|\tloss: 2.62446\n",
      "Training Epoch 15  96.6% | batch:       663 of       686\t|\tloss: 2.60732\n",
      "Training Epoch 15  96.8% | batch:       664 of       686\t|\tloss: 3.90049\n",
      "Training Epoch 15  96.9% | batch:       665 of       686\t|\tloss: 3.3084\n",
      "Training Epoch 15  97.1% | batch:       666 of       686\t|\tloss: 2.68052\n",
      "Training Epoch 15  97.2% | batch:       667 of       686\t|\tloss: 3.38501\n",
      "Training Epoch 15  97.4% | batch:       668 of       686\t|\tloss: 3.47658\n",
      "Training Epoch 15  97.5% | batch:       669 of       686\t|\tloss: 3.00977\n",
      "Training Epoch 15  97.7% | batch:       670 of       686\t|\tloss: 3.18261\n",
      "Training Epoch 15  97.8% | batch:       671 of       686\t|\tloss: 2.34939\n",
      "Training Epoch 15  98.0% | batch:       672 of       686\t|\tloss: 3.40312\n",
      "Training Epoch 15  98.1% | batch:       673 of       686\t|\tloss: 3.69156\n",
      "Training Epoch 15  98.3% | batch:       674 of       686\t|\tloss: 2.49735\n",
      "Training Epoch 15  98.4% | batch:       675 of       686\t|\tloss: 5.70753\n",
      "Training Epoch 15  98.5% | batch:       676 of       686\t|\tloss: 2.57825\n",
      "Training Epoch 15  98.7% | batch:       677 of       686\t|\tloss: 3.72075\n",
      "Training Epoch 15  98.8% | batch:       678 of       686\t|\tloss: 3.09703\n",
      "Training Epoch 15  99.0% | batch:       679 of       686\t|\tloss: 3.29242\n",
      "Training Epoch 15  99.1% | batch:       680 of       686\t|\tloss: 3.97127\n",
      "Training Epoch 15  99.3% | batch:       681 of       686\t|\tloss: 2.20922\n",
      "Training Epoch 15  99.4% | batch:       682 of       686\t|\tloss: 3.13199\n",
      "Training Epoch 15  99.6% | batch:       683 of       686\t|\tloss: 3.22932\n",
      "Training Epoch 15  99.7% | batch:       684 of       686\t|\tloss: 3.17645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:06:45,505 | INFO : Epoch 15 Training Summary: epoch: 15.000000 | loss: 3.609958 | \n",
      "2023-05-25 22:06:45,509 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 24.3312509059906 seconds\n",
      "\n",
      "2023-05-25 22:06:45,510 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.861244599024456 seconds\n",
      "2023-05-25 22:06:45,510 | INFO : Avg batch train. time: 0.03478315539216393 seconds\n",
      "2023-05-25 22:06:45,511 | INFO : Avg sample train. time: 0.00027209355834454025 seconds\n",
      "2023-05-25 22:06:45,511 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 15  99.9% | batch:       685 of       686\t|\tloss: 4.4341\n",
      "\n",
      "Evaluating Epoch 15   0.0% | batch:         0 of       172\t|\tloss: 1.41571\n",
      "Evaluating Epoch 15   0.6% | batch:         1 of       172\t|\tloss: 2.15921\n",
      "Evaluating Epoch 15   1.2% | batch:         2 of       172\t|\tloss: 1.67791\n",
      "Evaluating Epoch 15   1.7% | batch:         3 of       172\t|\tloss: 3.14859\n",
      "Evaluating Epoch 15   2.3% | batch:         4 of       172\t|\tloss: 1.85143\n",
      "Evaluating Epoch 15   2.9% | batch:         5 of       172\t|\tloss: 1.48314\n",
      "Evaluating Epoch 15   3.5% | batch:         6 of       172\t|\tloss: 2.18518\n",
      "Evaluating Epoch 15   4.1% | batch:         7 of       172\t|\tloss: 3.2932\n",
      "Evaluating Epoch 15   4.7% | batch:         8 of       172\t|\tloss: 1.50726\n",
      "Evaluating Epoch 15   5.2% | batch:         9 of       172\t|\tloss: 1.86188\n",
      "Evaluating Epoch 15   5.8% | batch:        10 of       172\t|\tloss: 2.32785\n",
      "Evaluating Epoch 15   6.4% | batch:        11 of       172\t|\tloss: 2.01458\n",
      "Evaluating Epoch 15   7.0% | batch:        12 of       172\t|\tloss: 1.58316\n",
      "Evaluating Epoch 15   7.6% | batch:        13 of       172\t|\tloss: 2.16599\n",
      "Evaluating Epoch 15   8.1% | batch:        14 of       172\t|\tloss: 2.48798\n",
      "Evaluating Epoch 15   8.7% | batch:        15 of       172\t|\tloss: 1.65386\n",
      "Evaluating Epoch 15   9.3% | batch:        16 of       172\t|\tloss: 2.42397\n",
      "Evaluating Epoch 15   9.9% | batch:        17 of       172\t|\tloss: 1.5787\n",
      "Evaluating Epoch 15  10.5% | batch:        18 of       172\t|\tloss: 8.19442\n",
      "Evaluating Epoch 15  11.0% | batch:        19 of       172\t|\tloss: 1.40088\n",
      "Evaluating Epoch 15  11.6% | batch:        20 of       172\t|\tloss: 3.18618\n",
      "Evaluating Epoch 15  12.2% | batch:        21 of       172\t|\tloss: 0.460777\n",
      "Evaluating Epoch 15  12.8% | batch:        22 of       172\t|\tloss: 0.763859\n",
      "Evaluating Epoch 15  13.4% | batch:        23 of       172\t|\tloss: 1.66667\n",
      "Evaluating Epoch 15  14.0% | batch:        24 of       172\t|\tloss: 2.01306\n",
      "Evaluating Epoch 15  14.5% | batch:        25 of       172\t|\tloss: 3.47878\n",
      "Evaluating Epoch 15  15.1% | batch:        26 of       172\t|\tloss: 4.80305\n",
      "Evaluating Epoch 15  15.7% | batch:        27 of       172\t|\tloss: 8.88254\n",
      "Evaluating Epoch 15  16.3% | batch:        28 of       172\t|\tloss: 0.510091\n",
      "Evaluating Epoch 15  16.9% | batch:        29 of       172\t|\tloss: 2.53758\n",
      "Evaluating Epoch 15  17.4% | batch:        30 of       172\t|\tloss: 0.541519\n",
      "Evaluating Epoch 15  18.0% | batch:        31 of       172\t|\tloss: 3.82445\n",
      "Evaluating Epoch 15  18.6% | batch:        32 of       172\t|\tloss: 1.27088\n",
      "Evaluating Epoch 15  19.2% | batch:        33 of       172\t|\tloss: 0.557867\n",
      "Evaluating Epoch 15  19.8% | batch:        34 of       172\t|\tloss: 0.841025\n",
      "Evaluating Epoch 15  20.3% | batch:        35 of       172\t|\tloss: 0.374228\n",
      "Evaluating Epoch 15  20.9% | batch:        36 of       172\t|\tloss: 3.32655\n",
      "Evaluating Epoch 15  21.5% | batch:        37 of       172\t|\tloss: 3.43177\n",
      "Evaluating Epoch 15  22.1% | batch:        38 of       172\t|\tloss: 1.65609\n",
      "Evaluating Epoch 15  22.7% | batch:        39 of       172\t|\tloss: 2.78088\n",
      "Evaluating Epoch 15  23.3% | batch:        40 of       172\t|\tloss: 1.08474\n",
      "Evaluating Epoch 15  23.8% | batch:        41 of       172\t|\tloss: 1.97988\n",
      "Evaluating Epoch 15  24.4% | batch:        42 of       172\t|\tloss: 1.03326\n",
      "Evaluating Epoch 15  25.0% | batch:        43 of       172\t|\tloss: 9.11548\n",
      "Evaluating Epoch 15  25.6% | batch:        44 of       172\t|\tloss: 1.46094\n",
      "Evaluating Epoch 15  26.2% | batch:        45 of       172\t|\tloss: 1.47559\n",
      "Evaluating Epoch 15  26.7% | batch:        46 of       172\t|\tloss: 0.138878\n",
      "Evaluating Epoch 15  27.3% | batch:        47 of       172\t|\tloss: 3.67035\n",
      "Evaluating Epoch 15  27.9% | batch:        48 of       172\t|\tloss: 0.622822\n",
      "Evaluating Epoch 15  28.5% | batch:        49 of       172\t|\tloss: 1.19669\n",
      "Evaluating Epoch 15  29.1% | batch:        50 of       172\t|\tloss: 0.679601\n",
      "Evaluating Epoch 15  29.7% | batch:        51 of       172\t|\tloss: 0.755265\n",
      "Evaluating Epoch 15  30.2% | batch:        52 of       172\t|\tloss: 3.4537\n",
      "Evaluating Epoch 15  30.8% | batch:        53 of       172\t|\tloss: 1.32541\n",
      "Evaluating Epoch 15  31.4% | batch:        54 of       172\t|\tloss: 1.47272\n",
      "Evaluating Epoch 15  32.0% | batch:        55 of       172\t|\tloss: 3.65365\n",
      "Evaluating Epoch 15  32.6% | batch:        56 of       172\t|\tloss: 2.40709\n",
      "Evaluating Epoch 15  33.1% | batch:        57 of       172\t|\tloss: 4.33997\n",
      "Evaluating Epoch 15  33.7% | batch:        58 of       172\t|\tloss: 1.04965\n",
      "Evaluating Epoch 15  34.3% | batch:        59 of       172\t|\tloss: 4.42812\n",
      "Evaluating Epoch 15  34.9% | batch:        60 of       172\t|\tloss: 0.560616\n",
      "Evaluating Epoch 15  35.5% | batch:        61 of       172\t|\tloss: 5.48396\n",
      "Evaluating Epoch 15  36.0% | batch:        62 of       172\t|\tloss: 0.615635\n",
      "Evaluating Epoch 15  36.6% | batch:        63 of       172\t|\tloss: 1.55496\n",
      "Evaluating Epoch 15  37.2% | batch:        64 of       172\t|\tloss: 3.59141\n",
      "Evaluating Epoch 15  37.8% | batch:        65 of       172\t|\tloss: 1.04545\n",
      "Evaluating Epoch 15  38.4% | batch:        66 of       172\t|\tloss: 3.40998\n",
      "Evaluating Epoch 15  39.0% | batch:        67 of       172\t|\tloss: 2.25028\n",
      "Evaluating Epoch 15  39.5% | batch:        68 of       172\t|\tloss: 1.7248\n",
      "Evaluating Epoch 15  40.1% | batch:        69 of       172\t|\tloss: 6.17257\n",
      "Evaluating Epoch 15  40.7% | batch:        70 of       172\t|\tloss: 0.615827\n",
      "Evaluating Epoch 15  41.3% | batch:        71 of       172\t|\tloss: 1.56536\n",
      "Evaluating Epoch 15  41.9% | batch:        72 of       172\t|\tloss: 3.11202\n",
      "Evaluating Epoch 15  42.4% | batch:        73 of       172\t|\tloss: 1.18619\n",
      "Evaluating Epoch 15  43.0% | batch:        74 of       172\t|\tloss: 0.232236\n",
      "Evaluating Epoch 15  43.6% | batch:        75 of       172\t|\tloss: 0.280301\n",
      "Evaluating Epoch 15  44.2% | batch:        76 of       172\t|\tloss: 0.243349\n",
      "Evaluating Epoch 15  44.8% | batch:        77 of       172\t|\tloss: 0.180052\n",
      "Evaluating Epoch 15  45.3% | batch:        78 of       172\t|\tloss: 0.324991\n",
      "Evaluating Epoch 15  45.9% | batch:        79 of       172\t|\tloss: 0.207527\n",
      "Evaluating Epoch 15  46.5% | batch:        80 of       172\t|\tloss: 0.391073\n",
      "Evaluating Epoch 15  47.1% | batch:        81 of       172\t|\tloss: 0.257049\n",
      "Evaluating Epoch 15  47.7% | batch:        82 of       172\t|\tloss: 0.263322\n",
      "Evaluating Epoch 15  48.3% | batch:        83 of       172\t|\tloss: 1.08071\n",
      "Evaluating Epoch 15  48.8% | batch:        84 of       172\t|\tloss: 1.19259\n",
      "Evaluating Epoch 15  49.4% | batch:        85 of       172\t|\tloss: 2.70067\n",
      "Evaluating Epoch 15  50.0% | batch:        86 of       172\t|\tloss: 1.51388\n",
      "Evaluating Epoch 15  50.6% | batch:        87 of       172\t|\tloss: 1.46665\n",
      "Evaluating Epoch 15  51.2% | batch:        88 of       172\t|\tloss: 2.14148\n",
      "Evaluating Epoch 15  51.7% | batch:        89 of       172\t|\tloss: 3.06713\n",
      "Evaluating Epoch 15  52.3% | batch:        90 of       172\t|\tloss: 1.85126\n",
      "Evaluating Epoch 15  52.9% | batch:        91 of       172\t|\tloss: 1.96349\n",
      "Evaluating Epoch 15  53.5% | batch:        92 of       172\t|\tloss: 3.47191\n",
      "Evaluating Epoch 15  54.1% | batch:        93 of       172\t|\tloss: 2.00536\n",
      "Evaluating Epoch 15  54.7% | batch:        94 of       172\t|\tloss: 1.36017\n",
      "Evaluating Epoch 15  55.2% | batch:        95 of       172\t|\tloss: 2.96468\n",
      "Evaluating Epoch 15  55.8% | batch:        96 of       172\t|\tloss: 2.48058\n",
      "Evaluating Epoch 15  56.4% | batch:        97 of       172\t|\tloss: 2.03971\n",
      "Evaluating Epoch 15  57.0% | batch:        98 of       172\t|\tloss: 2.01065\n",
      "Evaluating Epoch 15  57.6% | batch:        99 of       172\t|\tloss: 3.73549\n",
      "Evaluating Epoch 15  58.1% | batch:       100 of       172\t|\tloss: 1.17967\n",
      "Evaluating Epoch 15  58.7% | batch:       101 of       172\t|\tloss: 1.00534\n",
      "Evaluating Epoch 15  59.3% | batch:       102 of       172\t|\tloss: 3.08036\n",
      "Evaluating Epoch 15  59.9% | batch:       103 of       172\t|\tloss: 2.22998\n",
      "Evaluating Epoch 15  60.5% | batch:       104 of       172\t|\tloss: 1.77206\n",
      "Evaluating Epoch 15  61.0% | batch:       105 of       172\t|\tloss: 1.82954\n",
      "Evaluating Epoch 15  61.6% | batch:       106 of       172\t|\tloss: 3.81636\n",
      "Evaluating Epoch 15  62.2% | batch:       107 of       172\t|\tloss: 2.1453\n",
      "Evaluating Epoch 15  62.8% | batch:       108 of       172\t|\tloss: 1.63638\n",
      "Evaluating Epoch 15  63.4% | batch:       109 of       172\t|\tloss: 3.29895\n",
      "Evaluating Epoch 15  64.0% | batch:       110 of       172\t|\tloss: 2.70934\n",
      "Evaluating Epoch 15  64.5% | batch:       111 of       172\t|\tloss: 1.73344\n",
      "Evaluating Epoch 15  65.1% | batch:       112 of       172\t|\tloss: 1.1685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 15  65.7% | batch:       113 of       172\t|\tloss: 2.51611\n",
      "Evaluating Epoch 15  66.3% | batch:       114 of       172\t|\tloss: 1.941\n",
      "Evaluating Epoch 15  66.9% | batch:       115 of       172\t|\tloss: 1.46197\n",
      "Evaluating Epoch 15  67.4% | batch:       116 of       172\t|\tloss: 1.53282\n",
      "Evaluating Epoch 15  68.0% | batch:       117 of       172\t|\tloss: 1.60791\n",
      "Evaluating Epoch 15  68.6% | batch:       118 of       172\t|\tloss: 0.767806\n",
      "Evaluating Epoch 15  69.2% | batch:       119 of       172\t|\tloss: 1.14506\n",
      "Evaluating Epoch 15  69.8% | batch:       120 of       172\t|\tloss: 0.942745\n",
      "Evaluating Epoch 15  70.3% | batch:       121 of       172\t|\tloss: 0.786826\n",
      "Evaluating Epoch 15  70.9% | batch:       122 of       172\t|\tloss: 0.623363\n",
      "Evaluating Epoch 15  71.5% | batch:       123 of       172\t|\tloss: 0.975385\n",
      "Evaluating Epoch 15  72.1% | batch:       124 of       172\t|\tloss: 2.39785\n",
      "Evaluating Epoch 15  72.7% | batch:       125 of       172\t|\tloss: 1.41688\n",
      "Evaluating Epoch 15  73.3% | batch:       126 of       172\t|\tloss: 1.36577\n",
      "Evaluating Epoch 15  73.8% | batch:       127 of       172\t|\tloss: 0.679382\n",
      "Evaluating Epoch 15  74.4% | batch:       128 of       172\t|\tloss: 0.723507\n",
      "Evaluating Epoch 15  75.0% | batch:       129 of       172\t|\tloss: 1.14477\n",
      "Evaluating Epoch 15  75.6% | batch:       130 of       172\t|\tloss: 0.51679\n",
      "Evaluating Epoch 15  76.2% | batch:       131 of       172\t|\tloss: 0.86516\n",
      "Evaluating Epoch 15  76.7% | batch:       132 of       172\t|\tloss: 0.58812\n",
      "Evaluating Epoch 15  77.3% | batch:       133 of       172\t|\tloss: 0.173238\n",
      "Evaluating Epoch 15  77.9% | batch:       134 of       172\t|\tloss: 0.285478\n",
      "Evaluating Epoch 15  78.5% | batch:       135 of       172\t|\tloss: 0.234601\n",
      "Evaluating Epoch 15  79.1% | batch:       136 of       172\t|\tloss: 0.220305\n",
      "Evaluating Epoch 15  79.7% | batch:       137 of       172\t|\tloss: 0.301525\n",
      "Evaluating Epoch 15  80.2% | batch:       138 of       172\t|\tloss: 0.379548\n",
      "Evaluating Epoch 15  80.8% | batch:       139 of       172\t|\tloss: 0.316304\n",
      "Evaluating Epoch 15  81.4% | batch:       140 of       172\t|\tloss: 0.287613\n",
      "Evaluating Epoch 15  82.0% | batch:       141 of       172\t|\tloss: 0.395717\n",
      "Evaluating Epoch 15  82.6% | batch:       142 of       172\t|\tloss: 0.398464\n",
      "Evaluating Epoch 15  83.1% | batch:       143 of       172\t|\tloss: 0.326462\n",
      "Evaluating Epoch 15  83.7% | batch:       144 of       172\t|\tloss: 0.283984\n",
      "Evaluating Epoch 15  84.3% | batch:       145 of       172\t|\tloss: 0.134445\n",
      "Evaluating Epoch 15  84.9% | batch:       146 of       172\t|\tloss: 0.347215\n",
      "Evaluating Epoch 15  85.5% | batch:       147 of       172\t|\tloss: 0.185886\n",
      "Evaluating Epoch 15  86.0% | batch:       148 of       172\t|\tloss: 0.328733\n",
      "Evaluating Epoch 15  86.6% | batch:       149 of       172\t|\tloss: 0.179129\n",
      "Evaluating Epoch 15  87.2% | batch:       150 of       172\t|\tloss: 0.537664\n",
      "Evaluating Epoch 15  87.8% | batch:       151 of       172\t|\tloss: 1.28036\n",
      "Evaluating Epoch 15  88.4% | batch:       152 of       172\t|\tloss: 0.715073\n",
      "Evaluating Epoch 15  89.0% | batch:       153 of       172\t|\tloss: 0.821062\n",
      "Evaluating Epoch 15  89.5% | batch:       154 of       172\t|\tloss: 1.35188\n",
      "Evaluating Epoch 15  90.1% | batch:       155 of       172\t|\tloss: 0.601493\n",
      "Evaluating Epoch 15  90.7% | batch:       156 of       172\t|\tloss: 1.33889\n",
      "Evaluating Epoch 15  91.3% | batch:       157 of       172\t|\tloss: 1.53759\n",
      "Evaluating Epoch 15  91.9% | batch:       158 of       172\t|\tloss: 0.600793\n",
      "Evaluating Epoch 15  92.4% | batch:       159 of       172\t|\tloss: 1.89143\n",
      "Evaluating Epoch 15  93.0% | batch:       160 of       172\t|\tloss: 1.08341\n",
      "Evaluating Epoch 15  93.6% | batch:       161 of       172\t|\tloss: 2.3109\n",
      "Evaluating Epoch 15  94.2% | batch:       162 of       172\t|\tloss: 1.40895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:06:49,354 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.8418235778808594 seconds\n",
      "\n",
      "2023-05-25 22:06:49,354 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 3.9514529705047607 seconds\n",
      "2023-05-25 22:06:49,355 | INFO : Avg batch val. time: 0.022973563782004422 seconds\n",
      "2023-05-25 22:06:49,355 | INFO : Avg sample val. time: 0.00017996324500181086 seconds\n",
      "2023-05-25 22:06:49,356 | INFO : Epoch 15 Validation Summary: epoch: 15.000000 | loss: 1.703111 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 15  94.8% | batch:       163 of       172\t|\tloss: 0.768325\n",
      "Evaluating Epoch 15  95.3% | batch:       164 of       172\t|\tloss: 1.22667\n",
      "Evaluating Epoch 15  95.9% | batch:       165 of       172\t|\tloss: 1.09155\n",
      "Evaluating Epoch 15  96.5% | batch:       166 of       172\t|\tloss: 0.43985\n",
      "Evaluating Epoch 15  97.1% | batch:       167 of       172\t|\tloss: 1.61765\n",
      "Evaluating Epoch 15  97.7% | batch:       168 of       172\t|\tloss: 1.06314\n",
      "Evaluating Epoch 15  98.3% | batch:       169 of       172\t|\tloss: 0.77205\n",
      "Evaluating Epoch 15  98.8% | batch:       170 of       172\t|\tloss: 1.58417\n",
      "Evaluating Epoch 15  99.4% | batch:       171 of       172\t|\tloss: 1.37693\n",
      "\n",
      "Training Epoch 16   0.0% | batch:         0 of       686\t|\tloss: 2.79528\n",
      "Training Epoch 16   0.1% | batch:         1 of       686\t|\tloss: 4.36443\n",
      "Training Epoch 16   0.3% | batch:         2 of       686\t|\tloss: 2.93166\n",
      "Training Epoch 16   0.4% | batch:         3 of       686\t|\tloss: 2.84144\n",
      "Training Epoch 16   0.6% | batch:         4 of       686\t|\tloss: 2.92051\n",
      "Training Epoch 16   0.7% | batch:         5 of       686\t|\tloss: 4.15682\n",
      "Training Epoch 16   0.9% | batch:         6 of       686\t|\tloss: 4.10073\n",
      "Training Epoch 16   1.0% | batch:         7 of       686\t|\tloss: 3.53018\n",
      "Training Epoch 16   1.2% | batch:         8 of       686\t|\tloss: 3.76247\n",
      "Training Epoch 16   1.3% | batch:         9 of       686\t|\tloss: 3.44225\n",
      "Training Epoch 16   1.5% | batch:        10 of       686\t|\tloss: 3.01896\n",
      "Training Epoch 16   1.6% | batch:        11 of       686\t|\tloss: 3.36253\n",
      "Training Epoch 16   1.7% | batch:        12 of       686\t|\tloss: 2.70523\n",
      "Training Epoch 16   1.9% | batch:        13 of       686\t|\tloss: 3.15261\n",
      "Training Epoch 16   2.0% | batch:        14 of       686\t|\tloss: 3.31112\n",
      "Training Epoch 16   2.2% | batch:        15 of       686\t|\tloss: 4.94566\n",
      "Training Epoch 16   2.3% | batch:        16 of       686\t|\tloss: 2.82294\n",
      "Training Epoch 16   2.5% | batch:        17 of       686\t|\tloss: 3.19118\n",
      "Training Epoch 16   2.6% | batch:        18 of       686\t|\tloss: 2.59536\n",
      "Training Epoch 16   2.8% | batch:        19 of       686\t|\tloss: 3.29136\n",
      "Training Epoch 16   2.9% | batch:        20 of       686\t|\tloss: 2.86621\n",
      "Training Epoch 16   3.1% | batch:        21 of       686\t|\tloss: 3.06565\n",
      "Training Epoch 16   3.2% | batch:        22 of       686\t|\tloss: 2.57801\n",
      "Training Epoch 16   3.4% | batch:        23 of       686\t|\tloss: 2.53582\n",
      "Training Epoch 16   3.5% | batch:        24 of       686\t|\tloss: 3.27761\n",
      "Training Epoch 16   3.6% | batch:        25 of       686\t|\tloss: 3.34602\n",
      "Training Epoch 16   3.8% | batch:        26 of       686\t|\tloss: 3.665\n",
      "Training Epoch 16   3.9% | batch:        27 of       686\t|\tloss: 2.68208\n",
      "Training Epoch 16   4.1% | batch:        28 of       686\t|\tloss: 3.99858\n",
      "Training Epoch 16   4.2% | batch:        29 of       686\t|\tloss: 3.14676\n",
      "Training Epoch 16   4.4% | batch:        30 of       686\t|\tloss: 2.40233\n",
      "Training Epoch 16   4.5% | batch:        31 of       686\t|\tloss: 3.85813\n",
      "Training Epoch 16   4.7% | batch:        32 of       686\t|\tloss: 2.46329\n",
      "Training Epoch 16   4.8% | batch:        33 of       686\t|\tloss: 4.31406\n",
      "Training Epoch 16   5.0% | batch:        34 of       686\t|\tloss: 3.85\n",
      "Training Epoch 16   5.1% | batch:        35 of       686\t|\tloss: 3.83491\n",
      "Training Epoch 16   5.2% | batch:        36 of       686\t|\tloss: 2.77405\n",
      "Training Epoch 16   5.4% | batch:        37 of       686\t|\tloss: 3.23003\n",
      "Training Epoch 16   5.5% | batch:        38 of       686\t|\tloss: 3.21496\n",
      "Training Epoch 16   5.7% | batch:        39 of       686\t|\tloss: 3.38958\n",
      "Training Epoch 16   5.8% | batch:        40 of       686\t|\tloss: 3.17914\n",
      "Training Epoch 16   6.0% | batch:        41 of       686\t|\tloss: 3.51797\n",
      "Training Epoch 16   6.1% | batch:        42 of       686\t|\tloss: 3.36063\n",
      "Training Epoch 16   6.3% | batch:        43 of       686\t|\tloss: 3.84618\n",
      "Training Epoch 16   6.4% | batch:        44 of       686\t|\tloss: 3.23866\n",
      "Training Epoch 16   6.6% | batch:        45 of       686\t|\tloss: 3.64385\n",
      "Training Epoch 16   6.7% | batch:        46 of       686\t|\tloss: 4.03099\n",
      "Training Epoch 16   6.9% | batch:        47 of       686\t|\tloss: 3.68281\n",
      "Training Epoch 16   7.0% | batch:        48 of       686\t|\tloss: 3.44099\n",
      "Training Epoch 16   7.1% | batch:        49 of       686\t|\tloss: 3.77014\n",
      "Training Epoch 16   7.3% | batch:        50 of       686\t|\tloss: 4.00244\n",
      "Training Epoch 16   7.4% | batch:        51 of       686\t|\tloss: 3.79268\n",
      "Training Epoch 16   7.6% | batch:        52 of       686\t|\tloss: 2.7868\n",
      "Training Epoch 16   7.7% | batch:        53 of       686\t|\tloss: 2.95534\n",
      "Training Epoch 16   7.9% | batch:        54 of       686\t|\tloss: 2.94202\n",
      "Training Epoch 16   8.0% | batch:        55 of       686\t|\tloss: 3.78087\n",
      "Training Epoch 16   8.2% | batch:        56 of       686\t|\tloss: 4.13464\n",
      "Training Epoch 16   8.3% | batch:        57 of       686\t|\tloss: 3.59834\n",
      "Training Epoch 16   8.5% | batch:        58 of       686\t|\tloss: 2.53123\n",
      "Training Epoch 16   8.6% | batch:        59 of       686\t|\tloss: 2.96081\n",
      "Training Epoch 16   8.7% | batch:        60 of       686\t|\tloss: 4.64998\n",
      "Training Epoch 16   8.9% | batch:        61 of       686\t|\tloss: 2.97552\n",
      "Training Epoch 16   9.0% | batch:        62 of       686\t|\tloss: 3.14688\n",
      "Training Epoch 16   9.2% | batch:        63 of       686\t|\tloss: 2.72656\n",
      "Training Epoch 16   9.3% | batch:        64 of       686\t|\tloss: 3.12593\n",
      "Training Epoch 16   9.5% | batch:        65 of       686\t|\tloss: 2.90649\n",
      "Training Epoch 16   9.6% | batch:        66 of       686\t|\tloss: 3.82842\n",
      "Training Epoch 16   9.8% | batch:        67 of       686\t|\tloss: 4.19054\n",
      "Training Epoch 16   9.9% | batch:        68 of       686\t|\tloss: 2.99515\n",
      "Training Epoch 16  10.1% | batch:        69 of       686\t|\tloss: 2.40996\n",
      "Training Epoch 16  10.2% | batch:        70 of       686\t|\tloss: 3.17893\n",
      "Training Epoch 16  10.3% | batch:        71 of       686\t|\tloss: 2.90369\n",
      "Training Epoch 16  10.5% | batch:        72 of       686\t|\tloss: 3.08562\n",
      "Training Epoch 16  10.6% | batch:        73 of       686\t|\tloss: 2.92678\n",
      "Training Epoch 16  10.8% | batch:        74 of       686\t|\tloss: 2.63869\n",
      "Training Epoch 16  10.9% | batch:        75 of       686\t|\tloss: 4.07832\n",
      "Training Epoch 16  11.1% | batch:        76 of       686\t|\tloss: 3.36391\n",
      "Training Epoch 16  11.2% | batch:        77 of       686\t|\tloss: 3.03121\n",
      "Training Epoch 16  11.4% | batch:        78 of       686\t|\tloss: 3.01817\n",
      "Training Epoch 16  11.5% | batch:        79 of       686\t|\tloss: 4.66985\n",
      "Training Epoch 16  11.7% | batch:        80 of       686\t|\tloss: 2.77524\n",
      "Training Epoch 16  11.8% | batch:        81 of       686\t|\tloss: 3.89305\n",
      "Training Epoch 16  12.0% | batch:        82 of       686\t|\tloss: 3.79508\n",
      "Training Epoch 16  12.1% | batch:        83 of       686\t|\tloss: 3.74884\n",
      "Training Epoch 16  12.2% | batch:        84 of       686\t|\tloss: 2.88384\n",
      "Training Epoch 16  12.4% | batch:        85 of       686\t|\tloss: 3.4049\n",
      "Training Epoch 16  12.5% | batch:        86 of       686\t|\tloss: 2.83947\n",
      "Training Epoch 16  12.7% | batch:        87 of       686\t|\tloss: 3.29476\n",
      "Training Epoch 16  12.8% | batch:        88 of       686\t|\tloss: 3.43362\n",
      "Training Epoch 16  13.0% | batch:        89 of       686\t|\tloss: 2.7355\n",
      "Training Epoch 16  13.1% | batch:        90 of       686\t|\tloss: 3.86472\n",
      "Training Epoch 16  13.3% | batch:        91 of       686\t|\tloss: 2.8005\n",
      "Training Epoch 16  13.4% | batch:        92 of       686\t|\tloss: 3.46694\n",
      "Training Epoch 16  13.6% | batch:        93 of       686\t|\tloss: 3.43863\n",
      "Training Epoch 16  13.7% | batch:        94 of       686\t|\tloss: 3.5816\n",
      "Training Epoch 16  13.8% | batch:        95 of       686\t|\tloss: 3.32234\n",
      "Training Epoch 16  14.0% | batch:        96 of       686\t|\tloss: 3.55533\n",
      "Training Epoch 16  14.1% | batch:        97 of       686\t|\tloss: 2.90509\n",
      "Training Epoch 16  14.3% | batch:        98 of       686\t|\tloss: 3.94144\n",
      "Training Epoch 16  14.4% | batch:        99 of       686\t|\tloss: 2.8239\n",
      "Training Epoch 16  14.6% | batch:       100 of       686\t|\tloss: 4.36127\n",
      "Training Epoch 16  14.7% | batch:       101 of       686\t|\tloss: 2.86642\n",
      "Training Epoch 16  14.9% | batch:       102 of       686\t|\tloss: 3.4549\n",
      "Training Epoch 16  15.0% | batch:       103 of       686\t|\tloss: 2.82647\n",
      "Training Epoch 16  15.2% | batch:       104 of       686\t|\tloss: 2.66034\n",
      "Training Epoch 16  15.3% | batch:       105 of       686\t|\tloss: 3.97301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 16  15.5% | batch:       106 of       686\t|\tloss: 2.87391\n",
      "Training Epoch 16  15.6% | batch:       107 of       686\t|\tloss: 3.54647\n",
      "Training Epoch 16  15.7% | batch:       108 of       686\t|\tloss: 3.88197\n",
      "Training Epoch 16  15.9% | batch:       109 of       686\t|\tloss: 3.50987\n",
      "Training Epoch 16  16.0% | batch:       110 of       686\t|\tloss: 2.83428\n",
      "Training Epoch 16  16.2% | batch:       111 of       686\t|\tloss: 4.76798\n",
      "Training Epoch 16  16.3% | batch:       112 of       686\t|\tloss: 3.23295\n",
      "Training Epoch 16  16.5% | batch:       113 of       686\t|\tloss: 3.64624\n",
      "Training Epoch 16  16.6% | batch:       114 of       686\t|\tloss: 3.20124\n",
      "Training Epoch 16  16.8% | batch:       115 of       686\t|\tloss: 2.92016\n",
      "Training Epoch 16  16.9% | batch:       116 of       686\t|\tloss: 3.49369\n",
      "Training Epoch 16  17.1% | batch:       117 of       686\t|\tloss: 2.82343\n",
      "Training Epoch 16  17.2% | batch:       118 of       686\t|\tloss: 2.52975\n",
      "Training Epoch 16  17.3% | batch:       119 of       686\t|\tloss: 2.39046\n",
      "Training Epoch 16  17.5% | batch:       120 of       686\t|\tloss: 3.16278\n",
      "Training Epoch 16  17.6% | batch:       121 of       686\t|\tloss: 2.46407\n",
      "Training Epoch 16  17.8% | batch:       122 of       686\t|\tloss: 3.83586\n",
      "Training Epoch 16  17.9% | batch:       123 of       686\t|\tloss: 3.05252\n",
      "Training Epoch 16  18.1% | batch:       124 of       686\t|\tloss: 3.39679\n",
      "Training Epoch 16  18.2% | batch:       125 of       686\t|\tloss: 2.79938\n",
      "Training Epoch 16  18.4% | batch:       126 of       686\t|\tloss: 3.83571\n",
      "Training Epoch 16  18.5% | batch:       127 of       686\t|\tloss: 2.69883\n",
      "Training Epoch 16  18.7% | batch:       128 of       686\t|\tloss: 2.14044\n",
      "Training Epoch 16  18.8% | batch:       129 of       686\t|\tloss: 2.78512\n",
      "Training Epoch 16  19.0% | batch:       130 of       686\t|\tloss: 3.32425\n",
      "Training Epoch 16  19.1% | batch:       131 of       686\t|\tloss: 3.34165\n",
      "Training Epoch 16  19.2% | batch:       132 of       686\t|\tloss: 3.6781\n",
      "Training Epoch 16  19.4% | batch:       133 of       686\t|\tloss: 3.21993\n",
      "Training Epoch 16  19.5% | batch:       134 of       686\t|\tloss: 3.58993\n",
      "Training Epoch 16  19.7% | batch:       135 of       686\t|\tloss: 3.35641\n",
      "Training Epoch 16  19.8% | batch:       136 of       686\t|\tloss: 2.16458\n",
      "Training Epoch 16  20.0% | batch:       137 of       686\t|\tloss: 2.69958\n",
      "Training Epoch 16  20.1% | batch:       138 of       686\t|\tloss: 4.02946\n",
      "Training Epoch 16  20.3% | batch:       139 of       686\t|\tloss: 3.03507\n",
      "Training Epoch 16  20.4% | batch:       140 of       686\t|\tloss: 2.81709\n",
      "Training Epoch 16  20.6% | batch:       141 of       686\t|\tloss: 3.43177\n",
      "Training Epoch 16  20.7% | batch:       142 of       686\t|\tloss: 2.70642\n",
      "Training Epoch 16  20.8% | batch:       143 of       686\t|\tloss: 2.89681\n",
      "Training Epoch 16  21.0% | batch:       144 of       686\t|\tloss: 2.8802\n",
      "Training Epoch 16  21.1% | batch:       145 of       686\t|\tloss: 2.31414\n",
      "Training Epoch 16  21.3% | batch:       146 of       686\t|\tloss: 2.9157\n",
      "Training Epoch 16  21.4% | batch:       147 of       686\t|\tloss: 2.90732\n",
      "Training Epoch 16  21.6% | batch:       148 of       686\t|\tloss: 2.92361\n",
      "Training Epoch 16  21.7% | batch:       149 of       686\t|\tloss: 2.94041\n",
      "Training Epoch 16  21.9% | batch:       150 of       686\t|\tloss: 2.51649\n",
      "Training Epoch 16  22.0% | batch:       151 of       686\t|\tloss: 3.16906\n",
      "Training Epoch 16  22.2% | batch:       152 of       686\t|\tloss: 3.18436\n",
      "Training Epoch 16  22.3% | batch:       153 of       686\t|\tloss: 2.55499\n",
      "Training Epoch 16  22.4% | batch:       154 of       686\t|\tloss: 2.79926\n",
      "Training Epoch 16  22.6% | batch:       155 of       686\t|\tloss: 4.63212\n",
      "Training Epoch 16  22.7% | batch:       156 of       686\t|\tloss: 3.21473\n",
      "Training Epoch 16  22.9% | batch:       157 of       686\t|\tloss: 3.40075\n",
      "Training Epoch 16  23.0% | batch:       158 of       686\t|\tloss: 2.78424\n",
      "Training Epoch 16  23.2% | batch:       159 of       686\t|\tloss: 2.95203\n",
      "Training Epoch 16  23.3% | batch:       160 of       686\t|\tloss: 3.41096\n",
      "Training Epoch 16  23.5% | batch:       161 of       686\t|\tloss: 3.19814\n",
      "Training Epoch 16  23.6% | batch:       162 of       686\t|\tloss: 2.71781\n",
      "Training Epoch 16  23.8% | batch:       163 of       686\t|\tloss: 3.43446\n",
      "Training Epoch 16  23.9% | batch:       164 of       686\t|\tloss: 3.79681\n",
      "Training Epoch 16  24.1% | batch:       165 of       686\t|\tloss: 3.84875\n",
      "Training Epoch 16  24.2% | batch:       166 of       686\t|\tloss: 3.50926\n",
      "Training Epoch 16  24.3% | batch:       167 of       686\t|\tloss: 3.04949\n",
      "Training Epoch 16  24.5% | batch:       168 of       686\t|\tloss: 3.25282\n",
      "Training Epoch 16  24.6% | batch:       169 of       686\t|\tloss: 2.52466\n",
      "Training Epoch 16  24.8% | batch:       170 of       686\t|\tloss: 3.11597\n",
      "Training Epoch 16  24.9% | batch:       171 of       686\t|\tloss: 3.32202\n",
      "Training Epoch 16  25.1% | batch:       172 of       686\t|\tloss: 2.82527\n",
      "Training Epoch 16  25.2% | batch:       173 of       686\t|\tloss: 3.06992\n",
      "Training Epoch 16  25.4% | batch:       174 of       686\t|\tloss: 3.07322\n",
      "Training Epoch 16  25.5% | batch:       175 of       686\t|\tloss: 2.66588\n",
      "Training Epoch 16  25.7% | batch:       176 of       686\t|\tloss: 3.48764\n",
      "Training Epoch 16  25.8% | batch:       177 of       686\t|\tloss: 3.80039\n",
      "Training Epoch 16  25.9% | batch:       178 of       686\t|\tloss: 3.21058\n",
      "Training Epoch 16  26.1% | batch:       179 of       686\t|\tloss: 3.36058\n",
      "Training Epoch 16  26.2% | batch:       180 of       686\t|\tloss: 4.3207\n",
      "Training Epoch 16  26.4% | batch:       181 of       686\t|\tloss: 3.27165\n",
      "Training Epoch 16  26.5% | batch:       182 of       686\t|\tloss: 4.71677\n",
      "Training Epoch 16  26.7% | batch:       183 of       686\t|\tloss: 3.44854\n",
      "Training Epoch 16  26.8% | batch:       184 of       686\t|\tloss: 4.19859\n",
      "Training Epoch 16  27.0% | batch:       185 of       686\t|\tloss: 3.33074\n",
      "Training Epoch 16  27.1% | batch:       186 of       686\t|\tloss: 4.01574\n",
      "Training Epoch 16  27.3% | batch:       187 of       686\t|\tloss: 4.19691\n",
      "Training Epoch 16  27.4% | batch:       188 of       686\t|\tloss: 2.54181\n",
      "Training Epoch 16  27.6% | batch:       189 of       686\t|\tloss: 3.11876\n",
      "Training Epoch 16  27.7% | batch:       190 of       686\t|\tloss: 3.26909\n",
      "Training Epoch 16  27.8% | batch:       191 of       686\t|\tloss: 2.91675\n",
      "Training Epoch 16  28.0% | batch:       192 of       686\t|\tloss: 2.9818\n",
      "Training Epoch 16  28.1% | batch:       193 of       686\t|\tloss: 3.10815\n",
      "Training Epoch 16  28.3% | batch:       194 of       686\t|\tloss: 3.80469\n",
      "Training Epoch 16  28.4% | batch:       195 of       686\t|\tloss: 2.82692\n",
      "Training Epoch 16  28.6% | batch:       196 of       686\t|\tloss: 3.63393\n",
      "Training Epoch 16  28.7% | batch:       197 of       686\t|\tloss: 3.03625\n",
      "Training Epoch 16  28.9% | batch:       198 of       686\t|\tloss: 3.68131\n",
      "Training Epoch 16  29.0% | batch:       199 of       686\t|\tloss: 3.15019\n",
      "Training Epoch 16  29.2% | batch:       200 of       686\t|\tloss: 3.52072\n",
      "Training Epoch 16  29.3% | batch:       201 of       686\t|\tloss: 2.7223\n",
      "Training Epoch 16  29.4% | batch:       202 of       686\t|\tloss: 3.0636\n",
      "Training Epoch 16  29.6% | batch:       203 of       686\t|\tloss: 3.71699\n",
      "Training Epoch 16  29.7% | batch:       204 of       686\t|\tloss: 3.21468\n",
      "Training Epoch 16  29.9% | batch:       205 of       686\t|\tloss: 3.42435\n",
      "Training Epoch 16  30.0% | batch:       206 of       686\t|\tloss: 2.1696\n",
      "Training Epoch 16  30.2% | batch:       207 of       686\t|\tloss: 2.77492\n",
      "Training Epoch 16  30.3% | batch:       208 of       686\t|\tloss: 3.60159\n",
      "Training Epoch 16  30.5% | batch:       209 of       686\t|\tloss: 2.67748\n",
      "Training Epoch 16  30.6% | batch:       210 of       686\t|\tloss: 5.22559\n",
      "Training Epoch 16  30.8% | batch:       211 of       686\t|\tloss: 2.8894\n",
      "Training Epoch 16  30.9% | batch:       212 of       686\t|\tloss: 3.1668\n",
      "Training Epoch 16  31.0% | batch:       213 of       686\t|\tloss: 2.54515\n",
      "Training Epoch 16  31.2% | batch:       214 of       686\t|\tloss: 2.93695\n",
      "Training Epoch 16  31.3% | batch:       215 of       686\t|\tloss: 3.13452\n",
      "Training Epoch 16  31.5% | batch:       216 of       686\t|\tloss: 2.23315\n",
      "Training Epoch 16  31.6% | batch:       217 of       686\t|\tloss: 3.07618\n",
      "Training Epoch 16  31.8% | batch:       218 of       686\t|\tloss: 3.30202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 16  31.9% | batch:       219 of       686\t|\tloss: 3.18489\n",
      "Training Epoch 16  32.1% | batch:       220 of       686\t|\tloss: 2.88436\n",
      "Training Epoch 16  32.2% | batch:       221 of       686\t|\tloss: 2.48742\n",
      "Training Epoch 16  32.4% | batch:       222 of       686\t|\tloss: 3.65446\n",
      "Training Epoch 16  32.5% | batch:       223 of       686\t|\tloss: 3.88364\n",
      "Training Epoch 16  32.7% | batch:       224 of       686\t|\tloss: 2.47955\n",
      "Training Epoch 16  32.8% | batch:       225 of       686\t|\tloss: 2.80196\n",
      "Training Epoch 16  32.9% | batch:       226 of       686\t|\tloss: 4.36207\n",
      "Training Epoch 16  33.1% | batch:       227 of       686\t|\tloss: 3.99345\n",
      "Training Epoch 16  33.2% | batch:       228 of       686\t|\tloss: 3.78479\n",
      "Training Epoch 16  33.4% | batch:       229 of       686\t|\tloss: 3.31431\n",
      "Training Epoch 16  33.5% | batch:       230 of       686\t|\tloss: 3.02365\n",
      "Training Epoch 16  33.7% | batch:       231 of       686\t|\tloss: 3.42073\n",
      "Training Epoch 16  33.8% | batch:       232 of       686\t|\tloss: 3.3865\n",
      "Training Epoch 16  34.0% | batch:       233 of       686\t|\tloss: 2.65873\n",
      "Training Epoch 16  34.1% | batch:       234 of       686\t|\tloss: 2.09299\n",
      "Training Epoch 16  34.3% | batch:       235 of       686\t|\tloss: 3.81905\n",
      "Training Epoch 16  34.4% | batch:       236 of       686\t|\tloss: 2.72031\n",
      "Training Epoch 16  34.5% | batch:       237 of       686\t|\tloss: 2.37269\n",
      "Training Epoch 16  34.7% | batch:       238 of       686\t|\tloss: 2.89421\n",
      "Training Epoch 16  34.8% | batch:       239 of       686\t|\tloss: 3.49237\n",
      "Training Epoch 16  35.0% | batch:       240 of       686\t|\tloss: 3.30351\n",
      "Training Epoch 16  35.1% | batch:       241 of       686\t|\tloss: 2.55891\n",
      "Training Epoch 16  35.3% | batch:       242 of       686\t|\tloss: 2.59078\n",
      "Training Epoch 16  35.4% | batch:       243 of       686\t|\tloss: 4.04185\n",
      "Training Epoch 16  35.6% | batch:       244 of       686\t|\tloss: 2.482\n",
      "Training Epoch 16  35.7% | batch:       245 of       686\t|\tloss: 2.88386\n",
      "Training Epoch 16  35.9% | batch:       246 of       686\t|\tloss: 3.71828\n",
      "Training Epoch 16  36.0% | batch:       247 of       686\t|\tloss: 3.84781\n",
      "Training Epoch 16  36.2% | batch:       248 of       686\t|\tloss: 3.6627\n",
      "Training Epoch 16  36.3% | batch:       249 of       686\t|\tloss: 2.68942\n",
      "Training Epoch 16  36.4% | batch:       250 of       686\t|\tloss: 3.57553\n",
      "Training Epoch 16  36.6% | batch:       251 of       686\t|\tloss: 3.15101\n",
      "Training Epoch 16  36.7% | batch:       252 of       686\t|\tloss: 3.03091\n",
      "Training Epoch 16  36.9% | batch:       253 of       686\t|\tloss: 3.00336\n",
      "Training Epoch 16  37.0% | batch:       254 of       686\t|\tloss: 3.29062\n",
      "Training Epoch 16  37.2% | batch:       255 of       686\t|\tloss: 3.12401\n",
      "Training Epoch 16  37.3% | batch:       256 of       686\t|\tloss: 3.02078\n",
      "Training Epoch 16  37.5% | batch:       257 of       686\t|\tloss: 3.52476\n",
      "Training Epoch 16  37.6% | batch:       258 of       686\t|\tloss: 3.31529\n",
      "Training Epoch 16  37.8% | batch:       259 of       686\t|\tloss: 3.5608\n",
      "Training Epoch 16  37.9% | batch:       260 of       686\t|\tloss: 3.87864\n",
      "Training Epoch 16  38.0% | batch:       261 of       686\t|\tloss: 3.29303\n",
      "Training Epoch 16  38.2% | batch:       262 of       686\t|\tloss: 3.69664\n",
      "Training Epoch 16  38.3% | batch:       263 of       686\t|\tloss: 2.85079\n",
      "Training Epoch 16  38.5% | batch:       264 of       686\t|\tloss: 3.10369\n",
      "Training Epoch 16  38.6% | batch:       265 of       686\t|\tloss: 3.01252\n",
      "Training Epoch 16  38.8% | batch:       266 of       686\t|\tloss: 3.55582\n",
      "Training Epoch 16  38.9% | batch:       267 of       686\t|\tloss: 2.88391\n",
      "Training Epoch 16  39.1% | batch:       268 of       686\t|\tloss: 3.3136\n",
      "Training Epoch 16  39.2% | batch:       269 of       686\t|\tloss: 3.26008\n",
      "Training Epoch 16  39.4% | batch:       270 of       686\t|\tloss: 3.68778\n",
      "Training Epoch 16  39.5% | batch:       271 of       686\t|\tloss: 3.24073\n",
      "Training Epoch 16  39.7% | batch:       272 of       686\t|\tloss: 4.22247\n",
      "Training Epoch 16  39.8% | batch:       273 of       686\t|\tloss: 4.26206\n",
      "Training Epoch 16  39.9% | batch:       274 of       686\t|\tloss: 3.05157\n",
      "Training Epoch 16  40.1% | batch:       275 of       686\t|\tloss: 3.64794\n",
      "Training Epoch 16  40.2% | batch:       276 of       686\t|\tloss: 2.64646\n",
      "Training Epoch 16  40.4% | batch:       277 of       686\t|\tloss: 2.52628\n",
      "Training Epoch 16  40.5% | batch:       278 of       686\t|\tloss: 3.06977\n",
      "Training Epoch 16  40.7% | batch:       279 of       686\t|\tloss: 3.1817\n",
      "Training Epoch 16  40.8% | batch:       280 of       686\t|\tloss: 3.45954\n",
      "Training Epoch 16  41.0% | batch:       281 of       686\t|\tloss: 2.78983\n",
      "Training Epoch 16  41.1% | batch:       282 of       686\t|\tloss: 2.91562\n",
      "Training Epoch 16  41.3% | batch:       283 of       686\t|\tloss: 2.75156\n",
      "Training Epoch 16  41.4% | batch:       284 of       686\t|\tloss: 3.41624\n",
      "Training Epoch 16  41.5% | batch:       285 of       686\t|\tloss: 2.53261\n",
      "Training Epoch 16  41.7% | batch:       286 of       686\t|\tloss: 3.33982\n",
      "Training Epoch 16  41.8% | batch:       287 of       686\t|\tloss: 2.92056\n",
      "Training Epoch 16  42.0% | batch:       288 of       686\t|\tloss: 2.67726\n",
      "Training Epoch 16  42.1% | batch:       289 of       686\t|\tloss: 2.65482\n",
      "Training Epoch 16  42.3% | batch:       290 of       686\t|\tloss: 3.91651\n",
      "Training Epoch 16  42.4% | batch:       291 of       686\t|\tloss: 3.26242\n",
      "Training Epoch 16  42.6% | batch:       292 of       686\t|\tloss: 2.78445\n",
      "Training Epoch 16  42.7% | batch:       293 of       686\t|\tloss: 2.56633\n",
      "Training Epoch 16  42.9% | batch:       294 of       686\t|\tloss: 3.59304\n",
      "Training Epoch 16  43.0% | batch:       295 of       686\t|\tloss: 2.63478\n",
      "Training Epoch 16  43.1% | batch:       296 of       686\t|\tloss: 2.54939\n",
      "Training Epoch 16  43.3% | batch:       297 of       686\t|\tloss: 3.05012\n",
      "Training Epoch 16  43.4% | batch:       298 of       686\t|\tloss: 2.823\n",
      "Training Epoch 16  43.6% | batch:       299 of       686\t|\tloss: 2.84676\n",
      "Training Epoch 16  43.7% | batch:       300 of       686\t|\tloss: 2.60787\n",
      "Training Epoch 16  43.9% | batch:       301 of       686\t|\tloss: 2.55099\n",
      "Training Epoch 16  44.0% | batch:       302 of       686\t|\tloss: 2.58165\n",
      "Training Epoch 16  44.2% | batch:       303 of       686\t|\tloss: 3.07479\n",
      "Training Epoch 16  44.3% | batch:       304 of       686\t|\tloss: 2.50482\n",
      "Training Epoch 16  44.5% | batch:       305 of       686\t|\tloss: 2.66796\n",
      "Training Epoch 16  44.6% | batch:       306 of       686\t|\tloss: 3.8912\n",
      "Training Epoch 16  44.8% | batch:       307 of       686\t|\tloss: 3.21674\n",
      "Training Epoch 16  44.9% | batch:       308 of       686\t|\tloss: 3.14161\n",
      "Training Epoch 16  45.0% | batch:       309 of       686\t|\tloss: 3.10511\n",
      "Training Epoch 16  45.2% | batch:       310 of       686\t|\tloss: 2.421\n",
      "Training Epoch 16  45.3% | batch:       311 of       686\t|\tloss: 2.32426\n",
      "Training Epoch 16  45.5% | batch:       312 of       686\t|\tloss: 3.31087\n",
      "Training Epoch 16  45.6% | batch:       313 of       686\t|\tloss: 2.65294\n",
      "Training Epoch 16  45.8% | batch:       314 of       686\t|\tloss: 2.68118\n",
      "Training Epoch 16  45.9% | batch:       315 of       686\t|\tloss: 2.73511\n",
      "Training Epoch 16  46.1% | batch:       316 of       686\t|\tloss: 2.82399\n",
      "Training Epoch 16  46.2% | batch:       317 of       686\t|\tloss: 2.73471\n",
      "Training Epoch 16  46.4% | batch:       318 of       686\t|\tloss: 3.24972\n",
      "Training Epoch 16  46.5% | batch:       319 of       686\t|\tloss: 3.21092\n",
      "Training Epoch 16  46.6% | batch:       320 of       686\t|\tloss: 3.37745\n",
      "Training Epoch 16  46.8% | batch:       321 of       686\t|\tloss: 3.48187\n",
      "Training Epoch 16  46.9% | batch:       322 of       686\t|\tloss: 2.76405\n",
      "Training Epoch 16  47.1% | batch:       323 of       686\t|\tloss: 2.22516\n",
      "Training Epoch 16  47.2% | batch:       324 of       686\t|\tloss: 3.02054\n",
      "Training Epoch 16  47.4% | batch:       325 of       686\t|\tloss: 3.59271\n",
      "Training Epoch 16  47.5% | batch:       326 of       686\t|\tloss: 2.93231\n",
      "Training Epoch 16  47.7% | batch:       327 of       686\t|\tloss: 2.25488\n",
      "Training Epoch 16  47.8% | batch:       328 of       686\t|\tloss: 3.36605\n",
      "Training Epoch 16  48.0% | batch:       329 of       686\t|\tloss: 2.89889\n",
      "Training Epoch 16  48.1% | batch:       330 of       686\t|\tloss: 3.26685\n",
      "Training Epoch 16  48.3% | batch:       331 of       686\t|\tloss: 4.34749\n",
      "Training Epoch 16  48.4% | batch:       332 of       686\t|\tloss: 2.85063\n",
      "Training Epoch 16  48.5% | batch:       333 of       686\t|\tloss: 2.76991\n",
      "Training Epoch 16  48.7% | batch:       334 of       686\t|\tloss: 2.62467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 16  48.8% | batch:       335 of       686\t|\tloss: 3.79241\n",
      "Training Epoch 16  49.0% | batch:       336 of       686\t|\tloss: 2.23724\n",
      "Training Epoch 16  49.1% | batch:       337 of       686\t|\tloss: 3.4848\n",
      "Training Epoch 16  49.3% | batch:       338 of       686\t|\tloss: 2.82976\n",
      "Training Epoch 16  49.4% | batch:       339 of       686\t|\tloss: 3.09359\n",
      "Training Epoch 16  49.6% | batch:       340 of       686\t|\tloss: 3.07078\n",
      "Training Epoch 16  49.7% | batch:       341 of       686\t|\tloss: 3.00307\n",
      "Training Epoch 16  49.9% | batch:       342 of       686\t|\tloss: 3.25408\n",
      "Training Epoch 16  50.0% | batch:       343 of       686\t|\tloss: 3.0233\n",
      "Training Epoch 16  50.1% | batch:       344 of       686\t|\tloss: 3.69327\n",
      "Training Epoch 16  50.3% | batch:       345 of       686\t|\tloss: 2.35927\n",
      "Training Epoch 16  50.4% | batch:       346 of       686\t|\tloss: 3.76102\n",
      "Training Epoch 16  50.6% | batch:       347 of       686\t|\tloss: 3.10422\n",
      "Training Epoch 16  50.7% | batch:       348 of       686\t|\tloss: 3.63877\n",
      "Training Epoch 16  50.9% | batch:       349 of       686\t|\tloss: 4.35091\n",
      "Training Epoch 16  51.0% | batch:       350 of       686\t|\tloss: 3.33422\n",
      "Training Epoch 16  51.2% | batch:       351 of       686\t|\tloss: 3.04334\n",
      "Training Epoch 16  51.3% | batch:       352 of       686\t|\tloss: 3.35396\n",
      "Training Epoch 16  51.5% | batch:       353 of       686\t|\tloss: 3.2022\n",
      "Training Epoch 16  51.6% | batch:       354 of       686\t|\tloss: 2.93583\n",
      "Training Epoch 16  51.7% | batch:       355 of       686\t|\tloss: 3.18117\n",
      "Training Epoch 16  51.9% | batch:       356 of       686\t|\tloss: 2.31789\n",
      "Training Epoch 16  52.0% | batch:       357 of       686\t|\tloss: 2.23197\n",
      "Training Epoch 16  52.2% | batch:       358 of       686\t|\tloss: 2.54864\n",
      "Training Epoch 16  52.3% | batch:       359 of       686\t|\tloss: 2.7508\n",
      "Training Epoch 16  52.5% | batch:       360 of       686\t|\tloss: 2.57457\n",
      "Training Epoch 16  52.6% | batch:       361 of       686\t|\tloss: 2.04713\n",
      "Training Epoch 16  52.8% | batch:       362 of       686\t|\tloss: 3.20291\n",
      "Training Epoch 16  52.9% | batch:       363 of       686\t|\tloss: 2.84479\n",
      "Training Epoch 16  53.1% | batch:       364 of       686\t|\tloss: 2.57195\n",
      "Training Epoch 16  53.2% | batch:       365 of       686\t|\tloss: 2.74323\n",
      "Training Epoch 16  53.4% | batch:       366 of       686\t|\tloss: 2.88213\n",
      "Training Epoch 16  53.5% | batch:       367 of       686\t|\tloss: 3.02153\n",
      "Training Epoch 16  53.6% | batch:       368 of       686\t|\tloss: 2.60761\n",
      "Training Epoch 16  53.8% | batch:       369 of       686\t|\tloss: 2.87513\n",
      "Training Epoch 16  53.9% | batch:       370 of       686\t|\tloss: 3.34875\n",
      "Training Epoch 16  54.1% | batch:       371 of       686\t|\tloss: 2.89952\n",
      "Training Epoch 16  54.2% | batch:       372 of       686\t|\tloss: 2.80389\n",
      "Training Epoch 16  54.4% | batch:       373 of       686\t|\tloss: 3.15135\n",
      "Training Epoch 16  54.5% | batch:       374 of       686\t|\tloss: 2.89901\n",
      "Training Epoch 16  54.7% | batch:       375 of       686\t|\tloss: 3.52531\n",
      "Training Epoch 16  54.8% | batch:       376 of       686\t|\tloss: 3.22677\n",
      "Training Epoch 16  55.0% | batch:       377 of       686\t|\tloss: 2.50462\n",
      "Training Epoch 16  55.1% | batch:       378 of       686\t|\tloss: 3.30302\n",
      "Training Epoch 16  55.2% | batch:       379 of       686\t|\tloss: 2.85616\n",
      "Training Epoch 16  55.4% | batch:       380 of       686\t|\tloss: 2.32208\n",
      "Training Epoch 16  55.5% | batch:       381 of       686\t|\tloss: 3.33139\n",
      "Training Epoch 16  55.7% | batch:       382 of       686\t|\tloss: 2.77485\n",
      "Training Epoch 16  55.8% | batch:       383 of       686\t|\tloss: 3.34342\n",
      "Training Epoch 16  56.0% | batch:       384 of       686\t|\tloss: 2.5735\n",
      "Training Epoch 16  56.1% | batch:       385 of       686\t|\tloss: 2.82773\n",
      "Training Epoch 16  56.3% | batch:       386 of       686\t|\tloss: 2.8754\n",
      "Training Epoch 16  56.4% | batch:       387 of       686\t|\tloss: 3.39897\n",
      "Training Epoch 16  56.6% | batch:       388 of       686\t|\tloss: 3.12039\n",
      "Training Epoch 16  56.7% | batch:       389 of       686\t|\tloss: 2.82712\n",
      "Training Epoch 16  56.9% | batch:       390 of       686\t|\tloss: 3.42208\n",
      "Training Epoch 16  57.0% | batch:       391 of       686\t|\tloss: 3.2822\n",
      "Training Epoch 16  57.1% | batch:       392 of       686\t|\tloss: 2.66668\n",
      "Training Epoch 16  57.3% | batch:       393 of       686\t|\tloss: 3.91126\n",
      "Training Epoch 16  57.4% | batch:       394 of       686\t|\tloss: 4.247\n",
      "Training Epoch 16  57.6% | batch:       395 of       686\t|\tloss: 2.76631\n",
      "Training Epoch 16  57.7% | batch:       396 of       686\t|\tloss: 2.92219\n",
      "Training Epoch 16  57.9% | batch:       397 of       686\t|\tloss: 2.89288\n",
      "Training Epoch 16  58.0% | batch:       398 of       686\t|\tloss: 3.01858\n",
      "Training Epoch 16  58.2% | batch:       399 of       686\t|\tloss: 3.2913\n",
      "Training Epoch 16  58.3% | batch:       400 of       686\t|\tloss: 2.26084\n",
      "Training Epoch 16  58.5% | batch:       401 of       686\t|\tloss: 2.78482\n",
      "Training Epoch 16  58.6% | batch:       402 of       686\t|\tloss: 3.82839\n",
      "Training Epoch 16  58.7% | batch:       403 of       686\t|\tloss: 2.79371\n",
      "Training Epoch 16  58.9% | batch:       404 of       686\t|\tloss: 3.20071\n",
      "Training Epoch 16  59.0% | batch:       405 of       686\t|\tloss: 3.03022\n",
      "Training Epoch 16  59.2% | batch:       406 of       686\t|\tloss: 2.52688\n",
      "Training Epoch 16  59.3% | batch:       407 of       686\t|\tloss: 2.90486\n",
      "Training Epoch 16  59.5% | batch:       408 of       686\t|\tloss: 3.4904\n",
      "Training Epoch 16  59.6% | batch:       409 of       686\t|\tloss: 3.13026\n",
      "Training Epoch 16  59.8% | batch:       410 of       686\t|\tloss: 2.89884\n",
      "Training Epoch 16  59.9% | batch:       411 of       686\t|\tloss: 3.22156\n",
      "Training Epoch 16  60.1% | batch:       412 of       686\t|\tloss: 2.76009\n",
      "Training Epoch 16  60.2% | batch:       413 of       686\t|\tloss: 2.67156\n",
      "Training Epoch 16  60.3% | batch:       414 of       686\t|\tloss: 2.3437\n",
      "Training Epoch 16  60.5% | batch:       415 of       686\t|\tloss: 2.40828\n",
      "Training Epoch 16  60.6% | batch:       416 of       686\t|\tloss: 3.23107\n",
      "Training Epoch 16  60.8% | batch:       417 of       686\t|\tloss: 2.31595\n",
      "Training Epoch 16  60.9% | batch:       418 of       686\t|\tloss: 2.81308\n",
      "Training Epoch 16  61.1% | batch:       419 of       686\t|\tloss: 2.81082\n",
      "Training Epoch 16  61.2% | batch:       420 of       686\t|\tloss: 3.34193\n",
      "Training Epoch 16  61.4% | batch:       421 of       686\t|\tloss: 2.40723\n",
      "Training Epoch 16  61.5% | batch:       422 of       686\t|\tloss: 2.89204\n",
      "Training Epoch 16  61.7% | batch:       423 of       686\t|\tloss: 2.94447\n",
      "Training Epoch 16  61.8% | batch:       424 of       686\t|\tloss: 3.03959\n",
      "Training Epoch 16  62.0% | batch:       425 of       686\t|\tloss: 3.45996\n",
      "Training Epoch 16  62.1% | batch:       426 of       686\t|\tloss: 3.16093\n",
      "Training Epoch 16  62.2% | batch:       427 of       686\t|\tloss: 3.3749\n",
      "Training Epoch 16  62.4% | batch:       428 of       686\t|\tloss: 2.68891\n",
      "Training Epoch 16  62.5% | batch:       429 of       686\t|\tloss: 3.01403\n",
      "Training Epoch 16  62.7% | batch:       430 of       686\t|\tloss: 3.40165\n",
      "Training Epoch 16  62.8% | batch:       431 of       686\t|\tloss: 3.7806\n",
      "Training Epoch 16  63.0% | batch:       432 of       686\t|\tloss: 3.01609\n",
      "Training Epoch 16  63.1% | batch:       433 of       686\t|\tloss: 2.54215\n",
      "Training Epoch 16  63.3% | batch:       434 of       686\t|\tloss: 2.9666\n",
      "Training Epoch 16  63.4% | batch:       435 of       686\t|\tloss: 3.18606\n",
      "Training Epoch 16  63.6% | batch:       436 of       686\t|\tloss: 3.07395\n",
      "Training Epoch 16  63.7% | batch:       437 of       686\t|\tloss: 4.18617\n",
      "Training Epoch 16  63.8% | batch:       438 of       686\t|\tloss: 2.95933\n",
      "Training Epoch 16  64.0% | batch:       439 of       686\t|\tloss: 2.5466\n",
      "Training Epoch 16  64.1% | batch:       440 of       686\t|\tloss: 2.56396\n",
      "Training Epoch 16  64.3% | batch:       441 of       686\t|\tloss: 2.37066\n",
      "Training Epoch 16  64.4% | batch:       442 of       686\t|\tloss: 3.81618\n",
      "Training Epoch 16  64.6% | batch:       443 of       686\t|\tloss: 2.89293\n",
      "Training Epoch 16  64.7% | batch:       444 of       686\t|\tloss: 2.75508\n",
      "Training Epoch 16  64.9% | batch:       445 of       686\t|\tloss: 2.69123\n",
      "Training Epoch 16  65.0% | batch:       446 of       686\t|\tloss: 2.73981\n",
      "Training Epoch 16  65.2% | batch:       447 of       686\t|\tloss: 2.66373\n",
      "Training Epoch 16  65.3% | batch:       448 of       686\t|\tloss: 3.50181\n",
      "Training Epoch 16  65.5% | batch:       449 of       686\t|\tloss: 2.81653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 16  65.6% | batch:       450 of       686\t|\tloss: 3.10466\n",
      "Training Epoch 16  65.7% | batch:       451 of       686\t|\tloss: 2.26032\n",
      "Training Epoch 16  65.9% | batch:       452 of       686\t|\tloss: 3.63787\n",
      "Training Epoch 16  66.0% | batch:       453 of       686\t|\tloss: 3.24335\n",
      "Training Epoch 16  66.2% | batch:       454 of       686\t|\tloss: 2.87673\n",
      "Training Epoch 16  66.3% | batch:       455 of       686\t|\tloss: 2.78321\n",
      "Training Epoch 16  66.5% | batch:       456 of       686\t|\tloss: 3.63383\n",
      "Training Epoch 16  66.6% | batch:       457 of       686\t|\tloss: 4.76579\n",
      "Training Epoch 16  66.8% | batch:       458 of       686\t|\tloss: 3.74971\n",
      "Training Epoch 16  66.9% | batch:       459 of       686\t|\tloss: 2.72803\n",
      "Training Epoch 16  67.1% | batch:       460 of       686\t|\tloss: 3.18437\n",
      "Training Epoch 16  67.2% | batch:       461 of       686\t|\tloss: 2.92978\n",
      "Training Epoch 16  67.3% | batch:       462 of       686\t|\tloss: 3.55478\n",
      "Training Epoch 16  67.5% | batch:       463 of       686\t|\tloss: 1.93858\n",
      "Training Epoch 16  67.6% | batch:       464 of       686\t|\tloss: 3.28043\n",
      "Training Epoch 16  67.8% | batch:       465 of       686\t|\tloss: 3.02361\n",
      "Training Epoch 16  67.9% | batch:       466 of       686\t|\tloss: 2.9636\n",
      "Training Epoch 16  68.1% | batch:       467 of       686\t|\tloss: 2.557\n",
      "Training Epoch 16  68.2% | batch:       468 of       686\t|\tloss: 3.75139\n",
      "Training Epoch 16  68.4% | batch:       469 of       686\t|\tloss: 3.42841\n",
      "Training Epoch 16  68.5% | batch:       470 of       686\t|\tloss: 2.83933\n",
      "Training Epoch 16  68.7% | batch:       471 of       686\t|\tloss: 2.84501\n",
      "Training Epoch 16  68.8% | batch:       472 of       686\t|\tloss: 2.56779\n",
      "Training Epoch 16  69.0% | batch:       473 of       686\t|\tloss: 2.4152\n",
      "Training Epoch 16  69.1% | batch:       474 of       686\t|\tloss: 2.8233\n",
      "Training Epoch 16  69.2% | batch:       475 of       686\t|\tloss: 3.82624\n",
      "Training Epoch 16  69.4% | batch:       476 of       686\t|\tloss: 2.64794\n",
      "Training Epoch 16  69.5% | batch:       477 of       686\t|\tloss: 3.32682\n",
      "Training Epoch 16  69.7% | batch:       478 of       686\t|\tloss: 2.58731\n",
      "Training Epoch 16  69.8% | batch:       479 of       686\t|\tloss: 3.75366\n",
      "Training Epoch 16  70.0% | batch:       480 of       686\t|\tloss: 3.4565\n",
      "Training Epoch 16  70.1% | batch:       481 of       686\t|\tloss: 2.87963\n",
      "Training Epoch 16  70.3% | batch:       482 of       686\t|\tloss: 2.75619\n",
      "Training Epoch 16  70.4% | batch:       483 of       686\t|\tloss: 3.04155\n",
      "Training Epoch 16  70.6% | batch:       484 of       686\t|\tloss: 2.90071\n",
      "Training Epoch 16  70.7% | batch:       485 of       686\t|\tloss: 2.42371\n",
      "Training Epoch 16  70.8% | batch:       486 of       686\t|\tloss: 3.53822\n",
      "Training Epoch 16  71.0% | batch:       487 of       686\t|\tloss: 2.54134\n",
      "Training Epoch 16  71.1% | batch:       488 of       686\t|\tloss: 3.00791\n",
      "Training Epoch 16  71.3% | batch:       489 of       686\t|\tloss: 2.68852\n",
      "Training Epoch 16  71.4% | batch:       490 of       686\t|\tloss: 3.38032\n",
      "Training Epoch 16  71.6% | batch:       491 of       686\t|\tloss: 3.16735\n",
      "Training Epoch 16  71.7% | batch:       492 of       686\t|\tloss: 2.68958\n",
      "Training Epoch 16  71.9% | batch:       493 of       686\t|\tloss: 2.67724\n",
      "Training Epoch 16  72.0% | batch:       494 of       686\t|\tloss: 2.78849\n",
      "Training Epoch 16  72.2% | batch:       495 of       686\t|\tloss: 3.01508\n",
      "Training Epoch 16  72.3% | batch:       496 of       686\t|\tloss: 2.09938\n",
      "Training Epoch 16  72.4% | batch:       497 of       686\t|\tloss: 2.97665\n",
      "Training Epoch 16  72.6% | batch:       498 of       686\t|\tloss: 2.65027\n",
      "Training Epoch 16  72.7% | batch:       499 of       686\t|\tloss: 3.19786\n",
      "Training Epoch 16  72.9% | batch:       500 of       686\t|\tloss: 2.16159\n",
      "Training Epoch 16  73.0% | batch:       501 of       686\t|\tloss: 3.08204\n",
      "Training Epoch 16  73.2% | batch:       502 of       686\t|\tloss: 2.4802\n",
      "Training Epoch 16  73.3% | batch:       503 of       686\t|\tloss: 3.20067\n",
      "Training Epoch 16  73.5% | batch:       504 of       686\t|\tloss: 2.53484\n",
      "Training Epoch 16  73.6% | batch:       505 of       686\t|\tloss: 2.87388\n",
      "Training Epoch 16  73.8% | batch:       506 of       686\t|\tloss: 2.80594\n",
      "Training Epoch 16  73.9% | batch:       507 of       686\t|\tloss: 4.294\n",
      "Training Epoch 16  74.1% | batch:       508 of       686\t|\tloss: 4.91236\n",
      "Training Epoch 16  74.2% | batch:       509 of       686\t|\tloss: 2.2793\n",
      "Training Epoch 16  74.3% | batch:       510 of       686\t|\tloss: 2.90625\n",
      "Training Epoch 16  74.5% | batch:       511 of       686\t|\tloss: 2.83089\n",
      "Training Epoch 16  74.6% | batch:       512 of       686\t|\tloss: 3.07453\n",
      "Training Epoch 16  74.8% | batch:       513 of       686\t|\tloss: 2.91897\n",
      "Training Epoch 16  74.9% | batch:       514 of       686\t|\tloss: 2.94254\n",
      "Training Epoch 16  75.1% | batch:       515 of       686\t|\tloss: 3.26805\n",
      "Training Epoch 16  75.2% | batch:       516 of       686\t|\tloss: 3.27419\n",
      "Training Epoch 16  75.4% | batch:       517 of       686\t|\tloss: 2.63947\n",
      "Training Epoch 16  75.5% | batch:       518 of       686\t|\tloss: 2.67193\n",
      "Training Epoch 16  75.7% | batch:       519 of       686\t|\tloss: 3.56032\n",
      "Training Epoch 16  75.8% | batch:       520 of       686\t|\tloss: 3.11898\n",
      "Training Epoch 16  75.9% | batch:       521 of       686\t|\tloss: 2.89779\n",
      "Training Epoch 16  76.1% | batch:       522 of       686\t|\tloss: 2.83908\n",
      "Training Epoch 16  76.2% | batch:       523 of       686\t|\tloss: 2.98687\n",
      "Training Epoch 16  76.4% | batch:       524 of       686\t|\tloss: 2.9848\n",
      "Training Epoch 16  76.5% | batch:       525 of       686\t|\tloss: 2.95474\n",
      "Training Epoch 16  76.7% | batch:       526 of       686\t|\tloss: 2.23025\n",
      "Training Epoch 16  76.8% | batch:       527 of       686\t|\tloss: 3.86604\n",
      "Training Epoch 16  77.0% | batch:       528 of       686\t|\tloss: 2.37963\n",
      "Training Epoch 16  77.1% | batch:       529 of       686\t|\tloss: 3.98014\n",
      "Training Epoch 16  77.3% | batch:       530 of       686\t|\tloss: 3.50992\n",
      "Training Epoch 16  77.4% | batch:       531 of       686\t|\tloss: 3.0185\n",
      "Training Epoch 16  77.6% | batch:       532 of       686\t|\tloss: 2.81381\n",
      "Training Epoch 16  77.7% | batch:       533 of       686\t|\tloss: 2.57353\n",
      "Training Epoch 16  77.8% | batch:       534 of       686\t|\tloss: 3.92302\n",
      "Training Epoch 16  78.0% | batch:       535 of       686\t|\tloss: 3.25316\n",
      "Training Epoch 16  78.1% | batch:       536 of       686\t|\tloss: 2.58429\n",
      "Training Epoch 16  78.3% | batch:       537 of       686\t|\tloss: 3.02071\n",
      "Training Epoch 16  78.4% | batch:       538 of       686\t|\tloss: 2.92295\n",
      "Training Epoch 16  78.6% | batch:       539 of       686\t|\tloss: 2.43461\n",
      "Training Epoch 16  78.7% | batch:       540 of       686\t|\tloss: 3.21974\n",
      "Training Epoch 16  78.9% | batch:       541 of       686\t|\tloss: 2.80535\n",
      "Training Epoch 16  79.0% | batch:       542 of       686\t|\tloss: 2.71423\n",
      "Training Epoch 16  79.2% | batch:       543 of       686\t|\tloss: 2.94217\n",
      "Training Epoch 16  79.3% | batch:       544 of       686\t|\tloss: 3.36051\n",
      "Training Epoch 16  79.4% | batch:       545 of       686\t|\tloss: 2.88255\n",
      "Training Epoch 16  79.6% | batch:       546 of       686\t|\tloss: 2.55856\n",
      "Training Epoch 16  79.7% | batch:       547 of       686\t|\tloss: 3.53829\n",
      "Training Epoch 16  79.9% | batch:       548 of       686\t|\tloss: 2.54738\n",
      "Training Epoch 16  80.0% | batch:       549 of       686\t|\tloss: 2.989\n",
      "Training Epoch 16  80.2% | batch:       550 of       686\t|\tloss: 3.19342\n",
      "Training Epoch 16  80.3% | batch:       551 of       686\t|\tloss: 2.45704\n",
      "Training Epoch 16  80.5% | batch:       552 of       686\t|\tloss: 2.87754\n",
      "Training Epoch 16  80.6% | batch:       553 of       686\t|\tloss: 2.72972\n",
      "Training Epoch 16  80.8% | batch:       554 of       686\t|\tloss: 2.86907\n",
      "Training Epoch 16  80.9% | batch:       555 of       686\t|\tloss: 3.22106\n",
      "Training Epoch 16  81.0% | batch:       556 of       686\t|\tloss: 2.90166\n",
      "Training Epoch 16  81.2% | batch:       557 of       686\t|\tloss: 2.88584\n",
      "Training Epoch 16  81.3% | batch:       558 of       686\t|\tloss: 2.87801\n",
      "Training Epoch 16  81.5% | batch:       559 of       686\t|\tloss: 3.31746\n",
      "Training Epoch 16  81.6% | batch:       560 of       686\t|\tloss: 3.51619\n",
      "Training Epoch 16  81.8% | batch:       561 of       686\t|\tloss: 2.71978\n",
      "Training Epoch 16  81.9% | batch:       562 of       686\t|\tloss: 2.48622\n",
      "Training Epoch 16  82.1% | batch:       563 of       686\t|\tloss: 3.0785\n",
      "Training Epoch 16  82.2% | batch:       564 of       686\t|\tloss: 2.29128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 16  82.4% | batch:       565 of       686\t|\tloss: 2.95856\n",
      "Training Epoch 16  82.5% | batch:       566 of       686\t|\tloss: 3.28291\n",
      "Training Epoch 16  82.7% | batch:       567 of       686\t|\tloss: 3.03104\n",
      "Training Epoch 16  82.8% | batch:       568 of       686\t|\tloss: 2.60109\n",
      "Training Epoch 16  82.9% | batch:       569 of       686\t|\tloss: 3.01684\n",
      "Training Epoch 16  83.1% | batch:       570 of       686\t|\tloss: 3.58579\n",
      "Training Epoch 16  83.2% | batch:       571 of       686\t|\tloss: 2.74028\n",
      "Training Epoch 16  83.4% | batch:       572 of       686\t|\tloss: 2.72377\n",
      "Training Epoch 16  83.5% | batch:       573 of       686\t|\tloss: 2.90738\n",
      "Training Epoch 16  83.7% | batch:       574 of       686\t|\tloss: 3.12008\n",
      "Training Epoch 16  83.8% | batch:       575 of       686\t|\tloss: 2.79096\n",
      "Training Epoch 16  84.0% | batch:       576 of       686\t|\tloss: 2.09023\n",
      "Training Epoch 16  84.1% | batch:       577 of       686\t|\tloss: 2.66673\n",
      "Training Epoch 16  84.3% | batch:       578 of       686\t|\tloss: 2.53902\n",
      "Training Epoch 16  84.4% | batch:       579 of       686\t|\tloss: 3.19011\n",
      "Training Epoch 16  84.5% | batch:       580 of       686\t|\tloss: 3.32973\n",
      "Training Epoch 16  84.7% | batch:       581 of       686\t|\tloss: 2.98183\n",
      "Training Epoch 16  84.8% | batch:       582 of       686\t|\tloss: 3.34953\n",
      "Training Epoch 16  85.0% | batch:       583 of       686\t|\tloss: 2.02412\n",
      "Training Epoch 16  85.1% | batch:       584 of       686\t|\tloss: 2.59195\n",
      "Training Epoch 16  85.3% | batch:       585 of       686\t|\tloss: 2.54867\n",
      "Training Epoch 16  85.4% | batch:       586 of       686\t|\tloss: 2.70573\n",
      "Training Epoch 16  85.6% | batch:       587 of       686\t|\tloss: 2.92721\n",
      "Training Epoch 16  85.7% | batch:       588 of       686\t|\tloss: 2.63112\n",
      "Training Epoch 16  85.9% | batch:       589 of       686\t|\tloss: 2.99829\n",
      "Training Epoch 16  86.0% | batch:       590 of       686\t|\tloss: 2.2002\n",
      "Training Epoch 16  86.2% | batch:       591 of       686\t|\tloss: 2.577\n",
      "Training Epoch 16  86.3% | batch:       592 of       686\t|\tloss: 3.27694\n",
      "Training Epoch 16  86.4% | batch:       593 of       686\t|\tloss: 2.82859\n",
      "Training Epoch 16  86.6% | batch:       594 of       686\t|\tloss: 2.59045\n",
      "Training Epoch 16  86.7% | batch:       595 of       686\t|\tloss: 2.1207\n",
      "Training Epoch 16  86.9% | batch:       596 of       686\t|\tloss: 2.52499\n",
      "Training Epoch 16  87.0% | batch:       597 of       686\t|\tloss: 2.62494\n",
      "Training Epoch 16  87.2% | batch:       598 of       686\t|\tloss: 2.50289\n",
      "Training Epoch 16  87.3% | batch:       599 of       686\t|\tloss: 3.02058\n",
      "Training Epoch 16  87.5% | batch:       600 of       686\t|\tloss: 3.91304\n",
      "Training Epoch 16  87.6% | batch:       601 of       686\t|\tloss: 3.68666\n",
      "Training Epoch 16  87.8% | batch:       602 of       686\t|\tloss: 2.1125\n",
      "Training Epoch 16  87.9% | batch:       603 of       686\t|\tloss: 2.54312\n",
      "Training Epoch 16  88.0% | batch:       604 of       686\t|\tloss: 2.80685\n",
      "Training Epoch 16  88.2% | batch:       605 of       686\t|\tloss: 2.24543\n",
      "Training Epoch 16  88.3% | batch:       606 of       686\t|\tloss: 2.39308\n",
      "Training Epoch 16  88.5% | batch:       607 of       686\t|\tloss: 3.14283\n",
      "Training Epoch 16  88.6% | batch:       608 of       686\t|\tloss: 3.40646\n",
      "Training Epoch 16  88.8% | batch:       609 of       686\t|\tloss: 2.77043\n",
      "Training Epoch 16  88.9% | batch:       610 of       686\t|\tloss: 2.95119\n",
      "Training Epoch 16  89.1% | batch:       611 of       686\t|\tloss: 2.92697\n",
      "Training Epoch 16  89.2% | batch:       612 of       686\t|\tloss: 2.5593\n",
      "Training Epoch 16  89.4% | batch:       613 of       686\t|\tloss: 2.40876\n",
      "Training Epoch 16  89.5% | batch:       614 of       686\t|\tloss: 2.58441\n",
      "Training Epoch 16  89.7% | batch:       615 of       686\t|\tloss: 2.32192\n",
      "Training Epoch 16  89.8% | batch:       616 of       686\t|\tloss: 2.98009\n",
      "Training Epoch 16  89.9% | batch:       617 of       686\t|\tloss: 3.9245\n",
      "Training Epoch 16  90.1% | batch:       618 of       686\t|\tloss: 2.90306\n",
      "Training Epoch 16  90.2% | batch:       619 of       686\t|\tloss: 3.17662\n",
      "Training Epoch 16  90.4% | batch:       620 of       686\t|\tloss: 3.48208\n",
      "Training Epoch 16  90.5% | batch:       621 of       686\t|\tloss: 2.47625\n",
      "Training Epoch 16  90.7% | batch:       622 of       686\t|\tloss: 2.40083\n",
      "Training Epoch 16  90.8% | batch:       623 of       686\t|\tloss: 2.91083\n",
      "Training Epoch 16  91.0% | batch:       624 of       686\t|\tloss: 3.15558\n",
      "Training Epoch 16  91.1% | batch:       625 of       686\t|\tloss: 3.14242\n",
      "Training Epoch 16  91.3% | batch:       626 of       686\t|\tloss: 1.98478\n",
      "Training Epoch 16  91.4% | batch:       627 of       686\t|\tloss: 2.65587\n",
      "Training Epoch 16  91.5% | batch:       628 of       686\t|\tloss: 3.20679\n",
      "Training Epoch 16  91.7% | batch:       629 of       686\t|\tloss: 3.35778\n",
      "Training Epoch 16  91.8% | batch:       630 of       686\t|\tloss: 2.39737\n",
      "Training Epoch 16  92.0% | batch:       631 of       686\t|\tloss: 2.44272\n",
      "Training Epoch 16  92.1% | batch:       632 of       686\t|\tloss: 2.85978\n",
      "Training Epoch 16  92.3% | batch:       633 of       686\t|\tloss: 2.20961\n",
      "Training Epoch 16  92.4% | batch:       634 of       686\t|\tloss: 3.38899\n",
      "Training Epoch 16  92.6% | batch:       635 of       686\t|\tloss: 2.44931\n",
      "Training Epoch 16  92.7% | batch:       636 of       686\t|\tloss: 3.38535\n",
      "Training Epoch 16  92.9% | batch:       637 of       686\t|\tloss: 2.92632\n",
      "Training Epoch 16  93.0% | batch:       638 of       686\t|\tloss: 2.46178\n",
      "Training Epoch 16  93.1% | batch:       639 of       686\t|\tloss: 2.15549\n",
      "Training Epoch 16  93.3% | batch:       640 of       686\t|\tloss: 3.4599\n",
      "Training Epoch 16  93.4% | batch:       641 of       686\t|\tloss: 2.37081\n",
      "Training Epoch 16  93.6% | batch:       642 of       686\t|\tloss: 2.69618\n",
      "Training Epoch 16  93.7% | batch:       643 of       686\t|\tloss: 2.94894\n",
      "Training Epoch 16  93.9% | batch:       644 of       686\t|\tloss: 3.31172\n",
      "Training Epoch 16  94.0% | batch:       645 of       686\t|\tloss: 2.34855\n",
      "Training Epoch 16  94.2% | batch:       646 of       686\t|\tloss: 3.71765\n",
      "Training Epoch 16  94.3% | batch:       647 of       686\t|\tloss: 3.27521\n",
      "Training Epoch 16  94.5% | batch:       648 of       686\t|\tloss: 3.00892\n",
      "Training Epoch 16  94.6% | batch:       649 of       686\t|\tloss: 2.7724\n",
      "Training Epoch 16  94.8% | batch:       650 of       686\t|\tloss: 3.69983\n",
      "Training Epoch 16  94.9% | batch:       651 of       686\t|\tloss: 2.98711\n",
      "Training Epoch 16  95.0% | batch:       652 of       686\t|\tloss: 3.66766\n",
      "Training Epoch 16  95.2% | batch:       653 of       686\t|\tloss: 2.85962\n",
      "Training Epoch 16  95.3% | batch:       654 of       686\t|\tloss: 2.73747\n",
      "Training Epoch 16  95.5% | batch:       655 of       686\t|\tloss: 2.4128\n",
      "Training Epoch 16  95.6% | batch:       656 of       686\t|\tloss: 2.54687\n",
      "Training Epoch 16  95.8% | batch:       657 of       686\t|\tloss: 2.89662\n",
      "Training Epoch 16  95.9% | batch:       658 of       686\t|\tloss: 3.41509\n",
      "Training Epoch 16  96.1% | batch:       659 of       686\t|\tloss: 2.65825\n",
      "Training Epoch 16  96.2% | batch:       660 of       686\t|\tloss: 2.50377\n",
      "Training Epoch 16  96.4% | batch:       661 of       686\t|\tloss: 3.43025\n",
      "Training Epoch 16  96.5% | batch:       662 of       686\t|\tloss: 2.68884\n",
      "Training Epoch 16  96.6% | batch:       663 of       686\t|\tloss: 2.34621\n",
      "Training Epoch 16  96.8% | batch:       664 of       686\t|\tloss: 2.31407\n",
      "Training Epoch 16  96.9% | batch:       665 of       686\t|\tloss: 2.98974\n",
      "Training Epoch 16  97.1% | batch:       666 of       686\t|\tloss: 2.93291\n",
      "Training Epoch 16  97.2% | batch:       667 of       686\t|\tloss: 3.19568\n",
      "Training Epoch 16  97.4% | batch:       668 of       686\t|\tloss: 2.9035\n",
      "Training Epoch 16  97.5% | batch:       669 of       686\t|\tloss: 3.01993\n",
      "Training Epoch 16  97.7% | batch:       670 of       686\t|\tloss: 3.22945\n",
      "Training Epoch 16  97.8% | batch:       671 of       686\t|\tloss: 3.02292\n",
      "Training Epoch 16  98.0% | batch:       672 of       686\t|\tloss: 3.20703\n",
      "Training Epoch 16  98.1% | batch:       673 of       686\t|\tloss: 2.8643\n",
      "Training Epoch 16  98.3% | batch:       674 of       686\t|\tloss: 2.72757\n",
      "Training Epoch 16  98.4% | batch:       675 of       686\t|\tloss: 2.32989\n",
      "Training Epoch 16  98.5% | batch:       676 of       686\t|\tloss: 2.98924\n",
      "Training Epoch 16  98.7% | batch:       677 of       686\t|\tloss: 2.05868\n",
      "Training Epoch 16  98.8% | batch:       678 of       686\t|\tloss: 3.39486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:07:14,148 | INFO : Epoch 16 Training Summary: epoch: 16.000000 | loss: 3.075257 | \n",
      "2023-05-25 22:07:14,149 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 24.777631759643555 seconds\n",
      "\n",
      "2023-05-25 22:07:14,149 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.91851879656315 seconds\n",
      "2023-05-25 22:07:14,149 | INFO : Avg batch train. time: 0.03486664547603958 seconds\n",
      "2023-05-25 22:07:14,150 | INFO : Avg sample train. time: 0.00027274666510705454 seconds\n",
      "2023-05-25 22:07:14,150 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 16  99.0% | batch:       679 of       686\t|\tloss: 2.54349\n",
      "Training Epoch 16  99.1% | batch:       680 of       686\t|\tloss: 2.65721\n",
      "Training Epoch 16  99.3% | batch:       681 of       686\t|\tloss: 2.47375\n",
      "Training Epoch 16  99.4% | batch:       682 of       686\t|\tloss: 2.38688\n",
      "Training Epoch 16  99.6% | batch:       683 of       686\t|\tloss: 2.78676\n",
      "Training Epoch 16  99.7% | batch:       684 of       686\t|\tloss: 2.42169\n",
      "Training Epoch 16  99.9% | batch:       685 of       686\t|\tloss: 8.43062\n",
      "\n",
      "Evaluating Epoch 16   0.0% | batch:         0 of       172\t|\tloss: 1.35577\n",
      "Evaluating Epoch 16   0.6% | batch:         1 of       172\t|\tloss: 2.06603\n",
      "Evaluating Epoch 16   1.2% | batch:         2 of       172\t|\tloss: 1.6107\n",
      "Evaluating Epoch 16   1.7% | batch:         3 of       172\t|\tloss: 3.20069\n",
      "Evaluating Epoch 16   2.3% | batch:         4 of       172\t|\tloss: 1.79247\n",
      "Evaluating Epoch 16   2.9% | batch:         5 of       172\t|\tloss: 1.5151\n",
      "Evaluating Epoch 16   3.5% | batch:         6 of       172\t|\tloss: 2.14387\n",
      "Evaluating Epoch 16   4.1% | batch:         7 of       172\t|\tloss: 3.31983\n",
      "Evaluating Epoch 16   4.7% | batch:         8 of       172\t|\tloss: 1.41136\n",
      "Evaluating Epoch 16   5.2% | batch:         9 of       172\t|\tloss: 1.84304\n",
      "Evaluating Epoch 16   5.8% | batch:        10 of       172\t|\tloss: 2.20517\n",
      "Evaluating Epoch 16   6.4% | batch:        11 of       172\t|\tloss: 1.92207\n",
      "Evaluating Epoch 16   7.0% | batch:        12 of       172\t|\tloss: 1.58373\n",
      "Evaluating Epoch 16   7.6% | batch:        13 of       172\t|\tloss: 2.07589\n",
      "Evaluating Epoch 16   8.1% | batch:        14 of       172\t|\tloss: 2.64085\n",
      "Evaluating Epoch 16   8.7% | batch:        15 of       172\t|\tloss: 1.68212\n",
      "Evaluating Epoch 16   9.3% | batch:        16 of       172\t|\tloss: 2.35048\n",
      "Evaluating Epoch 16   9.9% | batch:        17 of       172\t|\tloss: 1.52695\n",
      "Evaluating Epoch 16  10.5% | batch:        18 of       172\t|\tloss: 11.4021\n",
      "Evaluating Epoch 16  11.0% | batch:        19 of       172\t|\tloss: 1.48744\n",
      "Evaluating Epoch 16  11.6% | batch:        20 of       172\t|\tloss: 2.96653\n",
      "Evaluating Epoch 16  12.2% | batch:        21 of       172\t|\tloss: 0.415759\n",
      "Evaluating Epoch 16  12.8% | batch:        22 of       172\t|\tloss: 1.49346\n",
      "Evaluating Epoch 16  13.4% | batch:        23 of       172\t|\tloss: 1.90676\n",
      "Evaluating Epoch 16  14.0% | batch:        24 of       172\t|\tloss: 2.10256\n",
      "Evaluating Epoch 16  14.5% | batch:        25 of       172\t|\tloss: 3.2814\n",
      "Evaluating Epoch 16  15.1% | batch:        26 of       172\t|\tloss: 6.44538\n",
      "Evaluating Epoch 16  15.7% | batch:        27 of       172\t|\tloss: 10.9934\n",
      "Evaluating Epoch 16  16.3% | batch:        28 of       172\t|\tloss: 0.742609\n",
      "Evaluating Epoch 16  16.9% | batch:        29 of       172\t|\tloss: 2.25365\n",
      "Evaluating Epoch 16  17.4% | batch:        30 of       172\t|\tloss: 0.460877\n",
      "Evaluating Epoch 16  18.0% | batch:        31 of       172\t|\tloss: 1.78626\n",
      "Evaluating Epoch 16  18.6% | batch:        32 of       172\t|\tloss: 1.4772\n",
      "Evaluating Epoch 16  19.2% | batch:        33 of       172\t|\tloss: 0.434716\n",
      "Evaluating Epoch 16  19.8% | batch:        34 of       172\t|\tloss: 0.923286\n",
      "Evaluating Epoch 16  20.3% | batch:        35 of       172\t|\tloss: 0.354901\n",
      "Evaluating Epoch 16  20.9% | batch:        36 of       172\t|\tloss: 3.39222\n",
      "Evaluating Epoch 16  21.5% | batch:        37 of       172\t|\tloss: 3.79004\n",
      "Evaluating Epoch 16  22.1% | batch:        38 of       172\t|\tloss: 2.60193\n",
      "Evaluating Epoch 16  22.7% | batch:        39 of       172\t|\tloss: 3.83102\n",
      "Evaluating Epoch 16  23.3% | batch:        40 of       172\t|\tloss: 1.3648\n",
      "Evaluating Epoch 16  23.8% | batch:        41 of       172\t|\tloss: 1.69077\n",
      "Evaluating Epoch 16  24.4% | batch:        42 of       172\t|\tloss: 1.25133\n",
      "Evaluating Epoch 16  25.0% | batch:        43 of       172\t|\tloss: 12.4949\n",
      "Evaluating Epoch 16  25.6% | batch:        44 of       172\t|\tloss: 1.59261\n",
      "Evaluating Epoch 16  26.2% | batch:        45 of       172\t|\tloss: 1.41064\n",
      "Evaluating Epoch 16  26.7% | batch:        46 of       172\t|\tloss: 0.161428\n",
      "Evaluating Epoch 16  27.3% | batch:        47 of       172\t|\tloss: 1.65801\n",
      "Evaluating Epoch 16  27.9% | batch:        48 of       172\t|\tloss: 0.616978\n",
      "Evaluating Epoch 16  28.5% | batch:        49 of       172\t|\tloss: 1.27314\n",
      "Evaluating Epoch 16  29.1% | batch:        50 of       172\t|\tloss: 0.790492\n",
      "Evaluating Epoch 16  29.7% | batch:        51 of       172\t|\tloss: 0.65593\n",
      "Evaluating Epoch 16  30.2% | batch:        52 of       172\t|\tloss: 0.93947\n",
      "Evaluating Epoch 16  30.8% | batch:        53 of       172\t|\tloss: 1.27001\n",
      "Evaluating Epoch 16  31.4% | batch:        54 of       172\t|\tloss: 0.85161\n",
      "Evaluating Epoch 16  32.0% | batch:        55 of       172\t|\tloss: 1.42726\n",
      "Evaluating Epoch 16  32.6% | batch:        56 of       172\t|\tloss: 1.66942\n",
      "Evaluating Epoch 16  33.1% | batch:        57 of       172\t|\tloss: 2.02519\n",
      "Evaluating Epoch 16  33.7% | batch:        58 of       172\t|\tloss: 0.882618\n",
      "Evaluating Epoch 16  34.3% | batch:        59 of       172\t|\tloss: 1.46767\n",
      "Evaluating Epoch 16  34.9% | batch:        60 of       172\t|\tloss: 0.524087\n",
      "Evaluating Epoch 16  35.5% | batch:        61 of       172\t|\tloss: 2.48177\n",
      "Evaluating Epoch 16  36.0% | batch:        62 of       172\t|\tloss: 0.444566\n",
      "Evaluating Epoch 16  36.6% | batch:        63 of       172\t|\tloss: 1.28706\n",
      "Evaluating Epoch 16  37.2% | batch:        64 of       172\t|\tloss: 1.10974\n",
      "Evaluating Epoch 16  37.8% | batch:        65 of       172\t|\tloss: 0.934453\n",
      "Evaluating Epoch 16  38.4% | batch:        66 of       172\t|\tloss: 1.67561\n",
      "Evaluating Epoch 16  39.0% | batch:        67 of       172\t|\tloss: 0.767477\n",
      "Evaluating Epoch 16  39.5% | batch:        68 of       172\t|\tloss: 1.5092\n",
      "Evaluating Epoch 16  40.1% | batch:        69 of       172\t|\tloss: 2.47421\n",
      "Evaluating Epoch 16  40.7% | batch:        70 of       172\t|\tloss: 0.473965\n",
      "Evaluating Epoch 16  41.3% | batch:        71 of       172\t|\tloss: 0.864305\n",
      "Evaluating Epoch 16  41.9% | batch:        72 of       172\t|\tloss: 1.23059\n",
      "Evaluating Epoch 16  42.4% | batch:        73 of       172\t|\tloss: 1.07733\n",
      "Evaluating Epoch 16  43.0% | batch:        74 of       172\t|\tloss: 0.410404\n",
      "Evaluating Epoch 16  43.6% | batch:        75 of       172\t|\tloss: 0.437669\n",
      "Evaluating Epoch 16  44.2% | batch:        76 of       172\t|\tloss: 0.424383\n",
      "Evaluating Epoch 16  44.8% | batch:        77 of       172\t|\tloss: 0.413723\n",
      "Evaluating Epoch 16  45.3% | batch:        78 of       172\t|\tloss: 0.481962\n",
      "Evaluating Epoch 16  45.9% | batch:        79 of       172\t|\tloss: 0.299381\n",
      "Evaluating Epoch 16  46.5% | batch:        80 of       172\t|\tloss: 0.508805\n",
      "Evaluating Epoch 16  47.1% | batch:        81 of       172\t|\tloss: 0.452493\n",
      "Evaluating Epoch 16  47.7% | batch:        82 of       172\t|\tloss: 0.421951\n",
      "Evaluating Epoch 16  48.3% | batch:        83 of       172\t|\tloss: 0.733548\n",
      "Evaluating Epoch 16  48.8% | batch:        84 of       172\t|\tloss: 0.764893\n",
      "Evaluating Epoch 16  49.4% | batch:        85 of       172\t|\tloss: 1.30116\n",
      "Evaluating Epoch 16  50.0% | batch:        86 of       172\t|\tloss: 0.764808\n",
      "Evaluating Epoch 16  50.6% | batch:        87 of       172\t|\tloss: 0.651246\n",
      "Evaluating Epoch 16  51.2% | batch:        88 of       172\t|\tloss: 1.04657\n",
      "Evaluating Epoch 16  51.7% | batch:        89 of       172\t|\tloss: 1.6778\n",
      "Evaluating Epoch 16  52.3% | batch:        90 of       172\t|\tloss: 0.770627\n",
      "Evaluating Epoch 16  52.9% | batch:        91 of       172\t|\tloss: 1.13653\n",
      "Evaluating Epoch 16  53.5% | batch:        92 of       172\t|\tloss: 1.86557\n",
      "Evaluating Epoch 16  54.1% | batch:        93 of       172\t|\tloss: 0.873147\n",
      "Evaluating Epoch 16  54.7% | batch:        94 of       172\t|\tloss: 0.921278\n",
      "Evaluating Epoch 16  55.2% | batch:        95 of       172\t|\tloss: 1.31109\n",
      "Evaluating Epoch 16  55.8% | batch:        96 of       172\t|\tloss: 1.32565\n",
      "Evaluating Epoch 16  56.4% | batch:        97 of       172\t|\tloss: 0.748691\n",
      "Evaluating Epoch 16  57.0% | batch:        98 of       172\t|\tloss: 1.18827\n",
      "Evaluating Epoch 16  57.6% | batch:        99 of       172\t|\tloss: 1.79653\n",
      "Evaluating Epoch 16  58.1% | batch:       100 of       172\t|\tloss: 0.303442\n",
      "Evaluating Epoch 16  58.7% | batch:       101 of       172\t|\tloss: 0.712517\n",
      "Evaluating Epoch 16  59.3% | batch:       102 of       172\t|\tloss: 1.53507\n",
      "Evaluating Epoch 16  59.9% | batch:       103 of       172\t|\tloss: 1.32159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 16  60.5% | batch:       104 of       172\t|\tloss: 0.699209\n",
      "Evaluating Epoch 16  61.0% | batch:       105 of       172\t|\tloss: 1.0913\n",
      "Evaluating Epoch 16  61.6% | batch:       106 of       172\t|\tloss: 2.05408\n",
      "Evaluating Epoch 16  62.2% | batch:       107 of       172\t|\tloss: 0.859227\n",
      "Evaluating Epoch 16  62.8% | batch:       108 of       172\t|\tloss: 0.873794\n",
      "Evaluating Epoch 16  63.4% | batch:       109 of       172\t|\tloss: 1.54797\n",
      "Evaluating Epoch 16  64.0% | batch:       110 of       172\t|\tloss: 1.52839\n",
      "Evaluating Epoch 16  64.5% | batch:       111 of       172\t|\tloss: 0.563362\n",
      "Evaluating Epoch 16  65.1% | batch:       112 of       172\t|\tloss: 0.538896\n",
      "Evaluating Epoch 16  65.7% | batch:       113 of       172\t|\tloss: 1.09579\n",
      "Evaluating Epoch 16  66.3% | batch:       114 of       172\t|\tloss: 1.22771\n",
      "Evaluating Epoch 16  66.9% | batch:       115 of       172\t|\tloss: 1.39802\n",
      "Evaluating Epoch 16  67.4% | batch:       116 of       172\t|\tloss: 1.61674\n",
      "Evaluating Epoch 16  68.0% | batch:       117 of       172\t|\tloss: 1.57635\n",
      "Evaluating Epoch 16  68.6% | batch:       118 of       172\t|\tloss: 0.819184\n",
      "Evaluating Epoch 16  69.2% | batch:       119 of       172\t|\tloss: 1.08725\n",
      "Evaluating Epoch 16  69.8% | batch:       120 of       172\t|\tloss: 0.767478\n",
      "Evaluating Epoch 16  70.3% | batch:       121 of       172\t|\tloss: 0.962669\n",
      "Evaluating Epoch 16  70.9% | batch:       122 of       172\t|\tloss: 0.702858\n",
      "Evaluating Epoch 16  71.5% | batch:       123 of       172\t|\tloss: 1.10916\n",
      "Evaluating Epoch 16  72.1% | batch:       124 of       172\t|\tloss: 3.52563\n",
      "Evaluating Epoch 16  72.7% | batch:       125 of       172\t|\tloss: 1.33617\n",
      "Evaluating Epoch 16  73.3% | batch:       126 of       172\t|\tloss: 1.31577\n",
      "Evaluating Epoch 16  73.8% | batch:       127 of       172\t|\tloss: 0.662462\n",
      "Evaluating Epoch 16  74.4% | batch:       128 of       172\t|\tloss: 0.866508\n",
      "Evaluating Epoch 16  75.0% | batch:       129 of       172\t|\tloss: 1.14279\n",
      "Evaluating Epoch 16  75.6% | batch:       130 of       172\t|\tloss: 0.48591\n",
      "Evaluating Epoch 16  76.2% | batch:       131 of       172\t|\tloss: 0.958949\n",
      "Evaluating Epoch 16  76.7% | batch:       132 of       172\t|\tloss: 0.658926\n",
      "Evaluating Epoch 16  77.3% | batch:       133 of       172\t|\tloss: 0.148451\n",
      "Evaluating Epoch 16  77.9% | batch:       134 of       172\t|\tloss: 0.358061\n",
      "Evaluating Epoch 16  78.5% | batch:       135 of       172\t|\tloss: 0.173954\n",
      "Evaluating Epoch 16  79.1% | batch:       136 of       172\t|\tloss: 0.248617\n",
      "Evaluating Epoch 16  79.7% | batch:       137 of       172\t|\tloss: 0.295275\n",
      "Evaluating Epoch 16  80.2% | batch:       138 of       172\t|\tloss: 0.323408\n",
      "Evaluating Epoch 16  80.8% | batch:       139 of       172\t|\tloss: 0.262065\n",
      "Evaluating Epoch 16  81.4% | batch:       140 of       172\t|\tloss: 0.319543\n",
      "Evaluating Epoch 16  82.0% | batch:       141 of       172\t|\tloss: 0.367354\n",
      "Evaluating Epoch 16  82.6% | batch:       142 of       172\t|\tloss: 0.405279\n",
      "Evaluating Epoch 16  83.1% | batch:       143 of       172\t|\tloss: 0.324768\n",
      "Evaluating Epoch 16  83.7% | batch:       144 of       172\t|\tloss: 0.331269\n",
      "Evaluating Epoch 16  84.3% | batch:       145 of       172\t|\tloss: 0.138644\n",
      "Evaluating Epoch 16  84.9% | batch:       146 of       172\t|\tloss: 0.368664\n",
      "Evaluating Epoch 16  85.5% | batch:       147 of       172\t|\tloss: 0.187117\n",
      "Evaluating Epoch 16  86.0% | batch:       148 of       172\t|\tloss: 0.338643\n",
      "Evaluating Epoch 16  86.6% | batch:       149 of       172\t|\tloss: 0.176801\n",
      "Evaluating Epoch 16  87.2% | batch:       150 of       172\t|\tloss: 0.576937\n",
      "Evaluating Epoch 16  87.8% | batch:       151 of       172\t|\tloss: 1.71937\n",
      "Evaluating Epoch 16  88.4% | batch:       152 of       172\t|\tloss: 0.93011\n",
      "Evaluating Epoch 16  89.0% | batch:       153 of       172\t|\tloss: 0.998334\n",
      "Evaluating Epoch 16  89.5% | batch:       154 of       172\t|\tloss: 1.76355\n",
      "Evaluating Epoch 16  90.1% | batch:       155 of       172\t|\tloss: 0.668653\n",
      "Evaluating Epoch 16  90.7% | batch:       156 of       172\t|\tloss: 1.68706\n",
      "Evaluating Epoch 16  91.3% | batch:       157 of       172\t|\tloss: 1.89291\n",
      "Evaluating Epoch 16  91.9% | batch:       158 of       172\t|\tloss: 0.671624\n",
      "Evaluating Epoch 16  92.4% | batch:       159 of       172\t|\tloss: 2.35737\n",
      "Evaluating Epoch 16  93.0% | batch:       160 of       172\t|\tloss: 1.15442\n",
      "Evaluating Epoch 16  93.6% | batch:       161 of       172\t|\tloss: 2.3802\n",
      "Evaluating Epoch 16  94.2% | batch:       162 of       172\t|\tloss: 1.81989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:07:18,711 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 4.560094118118286 seconds\n",
      "\n",
      "2023-05-25 22:07:18,715 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 3.9872553909526154 seconds\n",
      "2023-05-25 22:07:18,716 | INFO : Avg batch val. time: 0.023181717389259393 seconds\n",
      "2023-05-25 22:07:18,716 | INFO : Avg sample val. time: 0.00018159381477217358 seconds\n",
      "2023-05-25 22:07:18,717 | INFO : Epoch 16 Validation Summary: epoch: 16.000000 | loss: 1.443762 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 16  94.8% | batch:       163 of       172\t|\tloss: 0.854049\n",
      "Evaluating Epoch 16  95.3% | batch:       164 of       172\t|\tloss: 1.5556\n",
      "Evaluating Epoch 16  95.9% | batch:       165 of       172\t|\tloss: 1.45928\n",
      "Evaluating Epoch 16  96.5% | batch:       166 of       172\t|\tloss: 0.458167\n",
      "Evaluating Epoch 16  97.1% | batch:       167 of       172\t|\tloss: 2.02286\n",
      "Evaluating Epoch 16  97.7% | batch:       168 of       172\t|\tloss: 1.34345\n",
      "Evaluating Epoch 16  98.3% | batch:       169 of       172\t|\tloss: 0.869276\n",
      "Evaluating Epoch 16  98.8% | batch:       170 of       172\t|\tloss: 2.02366\n",
      "Evaluating Epoch 16  99.4% | batch:       171 of       172\t|\tloss: 1.62351\n",
      "\n",
      "Training Epoch 17   0.0% | batch:         0 of       686\t|\tloss: 2.42386\n",
      "Training Epoch 17   0.1% | batch:         1 of       686\t|\tloss: 2.52012\n",
      "Training Epoch 17   0.3% | batch:         2 of       686\t|\tloss: 3.05181\n",
      "Training Epoch 17   0.4% | batch:         3 of       686\t|\tloss: 2.75008\n",
      "Training Epoch 17   0.6% | batch:         4 of       686\t|\tloss: 2.5627\n",
      "Training Epoch 17   0.7% | batch:         5 of       686\t|\tloss: 2.50677\n",
      "Training Epoch 17   0.9% | batch:         6 of       686\t|\tloss: 3.25798\n",
      "Training Epoch 17   1.0% | batch:         7 of       686\t|\tloss: 2.85302\n",
      "Training Epoch 17   1.2% | batch:         8 of       686\t|\tloss: 3.47299\n",
      "Training Epoch 17   1.3% | batch:         9 of       686\t|\tloss: 2.38203\n",
      "Training Epoch 17   1.5% | batch:        10 of       686\t|\tloss: 2.46963\n",
      "Training Epoch 17   1.6% | batch:        11 of       686\t|\tloss: 3.055\n",
      "Training Epoch 17   1.7% | batch:        12 of       686\t|\tloss: 2.52423\n",
      "Training Epoch 17   1.9% | batch:        13 of       686\t|\tloss: 2.45369\n",
      "Training Epoch 17   2.0% | batch:        14 of       686\t|\tloss: 3.02765\n",
      "Training Epoch 17   2.2% | batch:        15 of       686\t|\tloss: 2.86863\n",
      "Training Epoch 17   2.3% | batch:        16 of       686\t|\tloss: 3.2444\n",
      "Training Epoch 17   2.5% | batch:        17 of       686\t|\tloss: 2.19159\n",
      "Training Epoch 17   2.6% | batch:        18 of       686\t|\tloss: 2.17362\n",
      "Training Epoch 17   2.8% | batch:        19 of       686\t|\tloss: 2.94765\n",
      "Training Epoch 17   2.9% | batch:        20 of       686\t|\tloss: 2.04572\n",
      "Training Epoch 17   3.1% | batch:        21 of       686\t|\tloss: 2.72982\n",
      "Training Epoch 17   3.2% | batch:        22 of       686\t|\tloss: 2.90618\n",
      "Training Epoch 17   3.4% | batch:        23 of       686\t|\tloss: 2.27741\n",
      "Training Epoch 17   3.5% | batch:        24 of       686\t|\tloss: 2.10517\n",
      "Training Epoch 17   3.6% | batch:        25 of       686\t|\tloss: 2.46033\n",
      "Training Epoch 17   3.8% | batch:        26 of       686\t|\tloss: 2.66447\n",
      "Training Epoch 17   3.9% | batch:        27 of       686\t|\tloss: 3.12272\n",
      "Training Epoch 17   4.1% | batch:        28 of       686\t|\tloss: 2.70631\n",
      "Training Epoch 17   4.2% | batch:        29 of       686\t|\tloss: 3.17459\n",
      "Training Epoch 17   4.4% | batch:        30 of       686\t|\tloss: 2.88177\n",
      "Training Epoch 17   4.5% | batch:        31 of       686\t|\tloss: 2.57607\n",
      "Training Epoch 17   4.7% | batch:        32 of       686\t|\tloss: 2.69476\n",
      "Training Epoch 17   4.8% | batch:        33 of       686\t|\tloss: 3.00794\n",
      "Training Epoch 17   5.0% | batch:        34 of       686\t|\tloss: 2.77467\n",
      "Training Epoch 17   5.1% | batch:        35 of       686\t|\tloss: 2.29889\n",
      "Training Epoch 17   5.2% | batch:        36 of       686\t|\tloss: 2.9951\n",
      "Training Epoch 17   5.4% | batch:        37 of       686\t|\tloss: 2.52753\n",
      "Training Epoch 17   5.5% | batch:        38 of       686\t|\tloss: 2.41908\n",
      "Training Epoch 17   5.7% | batch:        39 of       686\t|\tloss: 3.23831\n",
      "Training Epoch 17   5.8% | batch:        40 of       686\t|\tloss: 2.9156\n",
      "Training Epoch 17   6.0% | batch:        41 of       686\t|\tloss: 2.99551\n",
      "Training Epoch 17   6.1% | batch:        42 of       686\t|\tloss: 2.47114\n",
      "Training Epoch 17   6.3% | batch:        43 of       686\t|\tloss: 2.55665\n",
      "Training Epoch 17   6.4% | batch:        44 of       686\t|\tloss: 2.74263\n",
      "Training Epoch 17   6.6% | batch:        45 of       686\t|\tloss: 3.22786\n",
      "Training Epoch 17   6.7% | batch:        46 of       686\t|\tloss: 2.62233\n",
      "Training Epoch 17   6.9% | batch:        47 of       686\t|\tloss: 2.70124\n",
      "Training Epoch 17   7.0% | batch:        48 of       686\t|\tloss: 3.14079\n",
      "Training Epoch 17   7.1% | batch:        49 of       686\t|\tloss: 3.40227\n",
      "Training Epoch 17   7.3% | batch:        50 of       686\t|\tloss: 2.14723\n",
      "Training Epoch 17   7.4% | batch:        51 of       686\t|\tloss: 2.42002\n",
      "Training Epoch 17   7.6% | batch:        52 of       686\t|\tloss: 2.4714\n",
      "Training Epoch 17   7.7% | batch:        53 of       686\t|\tloss: 2.315\n",
      "Training Epoch 17   7.9% | batch:        54 of       686\t|\tloss: 2.66493\n",
      "Training Epoch 17   8.0% | batch:        55 of       686\t|\tloss: 3.87459\n",
      "Training Epoch 17   8.2% | batch:        56 of       686\t|\tloss: 3.44574\n",
      "Training Epoch 17   8.3% | batch:        57 of       686\t|\tloss: 2.41558\n",
      "Training Epoch 17   8.5% | batch:        58 of       686\t|\tloss: 2.27611\n",
      "Training Epoch 17   8.6% | batch:        59 of       686\t|\tloss: 2.81007\n",
      "Training Epoch 17   8.7% | batch:        60 of       686\t|\tloss: 2.5788\n",
      "Training Epoch 17   8.9% | batch:        61 of       686\t|\tloss: 3.87809\n",
      "Training Epoch 17   9.0% | batch:        62 of       686\t|\tloss: 2.52944\n",
      "Training Epoch 17   9.2% | batch:        63 of       686\t|\tloss: 2.96758\n",
      "Training Epoch 17   9.3% | batch:        64 of       686\t|\tloss: 2.96438\n",
      "Training Epoch 17   9.5% | batch:        65 of       686\t|\tloss: 2.84386\n",
      "Training Epoch 17   9.6% | batch:        66 of       686\t|\tloss: 2.69365\n",
      "Training Epoch 17   9.8% | batch:        67 of       686\t|\tloss: 2.45066\n",
      "Training Epoch 17   9.9% | batch:        68 of       686\t|\tloss: 2.60827\n",
      "Training Epoch 17  10.1% | batch:        69 of       686\t|\tloss: 2.54978\n",
      "Training Epoch 17  10.2% | batch:        70 of       686\t|\tloss: 2.67764\n",
      "Training Epoch 17  10.3% | batch:        71 of       686\t|\tloss: 2.57395\n",
      "Training Epoch 17  10.5% | batch:        72 of       686\t|\tloss: 2.30126\n",
      "Training Epoch 17  10.6% | batch:        73 of       686\t|\tloss: 3.01336\n",
      "Training Epoch 17  10.8% | batch:        74 of       686\t|\tloss: 2.60789\n",
      "Training Epoch 17  10.9% | batch:        75 of       686\t|\tloss: 3.26034\n",
      "Training Epoch 17  11.1% | batch:        76 of       686\t|\tloss: 2.68288\n",
      "Training Epoch 17  11.2% | batch:        77 of       686\t|\tloss: 2.7484\n",
      "Training Epoch 17  11.4% | batch:        78 of       686\t|\tloss: 3.32419\n",
      "Training Epoch 17  11.5% | batch:        79 of       686\t|\tloss: 2.44209\n",
      "Training Epoch 17  11.7% | batch:        80 of       686\t|\tloss: 2.75873\n",
      "Training Epoch 17  11.8% | batch:        81 of       686\t|\tloss: 2.47568\n",
      "Training Epoch 17  12.0% | batch:        82 of       686\t|\tloss: 2.85985\n",
      "Training Epoch 17  12.1% | batch:        83 of       686\t|\tloss: 3.05613\n",
      "Training Epoch 17  12.2% | batch:        84 of       686\t|\tloss: 2.8582\n",
      "Training Epoch 17  12.4% | batch:        85 of       686\t|\tloss: 3.04098\n",
      "Training Epoch 17  12.5% | batch:        86 of       686\t|\tloss: 2.87289\n",
      "Training Epoch 17  12.7% | batch:        87 of       686\t|\tloss: 2.72941\n",
      "Training Epoch 17  12.8% | batch:        88 of       686\t|\tloss: 2.49321\n",
      "Training Epoch 17  13.0% | batch:        89 of       686\t|\tloss: 3.00329\n",
      "Training Epoch 17  13.1% | batch:        90 of       686\t|\tloss: 2.90965\n",
      "Training Epoch 17  13.3% | batch:        91 of       686\t|\tloss: 3.37633\n",
      "Training Epoch 17  13.4% | batch:        92 of       686\t|\tloss: 2.78807\n",
      "Training Epoch 17  13.6% | batch:        93 of       686\t|\tloss: 2.67377\n",
      "Training Epoch 17  13.7% | batch:        94 of       686\t|\tloss: 2.3989\n",
      "Training Epoch 17  13.8% | batch:        95 of       686\t|\tloss: 2.88644\n",
      "Training Epoch 17  14.0% | batch:        96 of       686\t|\tloss: 2.22845\n",
      "Training Epoch 17  14.1% | batch:        97 of       686\t|\tloss: 2.58797\n",
      "Training Epoch 17  14.3% | batch:        98 of       686\t|\tloss: 3.36668\n",
      "Training Epoch 17  14.4% | batch:        99 of       686\t|\tloss: 2.49941\n",
      "Training Epoch 17  14.6% | batch:       100 of       686\t|\tloss: 2.6508\n",
      "Training Epoch 17  14.7% | batch:       101 of       686\t|\tloss: 2.43373\n",
      "Training Epoch 17  14.9% | batch:       102 of       686\t|\tloss: 2.41288\n",
      "Training Epoch 17  15.0% | batch:       103 of       686\t|\tloss: 2.64737\n",
      "Training Epoch 17  15.2% | batch:       104 of       686\t|\tloss: 3.07681\n",
      "Training Epoch 17  15.3% | batch:       105 of       686\t|\tloss: 3.49028\n",
      "Training Epoch 17  15.5% | batch:       106 of       686\t|\tloss: 2.77379\n",
      "Training Epoch 17  15.6% | batch:       107 of       686\t|\tloss: 2.72944\n",
      "Training Epoch 17  15.7% | batch:       108 of       686\t|\tloss: 2.99127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 17  15.9% | batch:       109 of       686\t|\tloss: 3.24413\n",
      "Training Epoch 17  16.0% | batch:       110 of       686\t|\tloss: 2.93607\n",
      "Training Epoch 17  16.2% | batch:       111 of       686\t|\tloss: 2.32174\n",
      "Training Epoch 17  16.3% | batch:       112 of       686\t|\tloss: 3.28315\n",
      "Training Epoch 17  16.5% | batch:       113 of       686\t|\tloss: 2.4545\n",
      "Training Epoch 17  16.6% | batch:       114 of       686\t|\tloss: 3.45307\n",
      "Training Epoch 17  16.8% | batch:       115 of       686\t|\tloss: 2.78593\n",
      "Training Epoch 17  16.9% | batch:       116 of       686\t|\tloss: 2.82622\n",
      "Training Epoch 17  17.1% | batch:       117 of       686\t|\tloss: 2.50309\n",
      "Training Epoch 17  17.2% | batch:       118 of       686\t|\tloss: 2.43655\n",
      "Training Epoch 17  17.3% | batch:       119 of       686\t|\tloss: 2.40424\n",
      "Training Epoch 17  17.5% | batch:       120 of       686\t|\tloss: 2.58872\n",
      "Training Epoch 17  17.6% | batch:       121 of       686\t|\tloss: 2.60307\n",
      "Training Epoch 17  17.8% | batch:       122 of       686\t|\tloss: 3.53884\n",
      "Training Epoch 17  17.9% | batch:       123 of       686\t|\tloss: 2.67683\n",
      "Training Epoch 17  18.1% | batch:       124 of       686\t|\tloss: 2.69415\n",
      "Training Epoch 17  18.2% | batch:       125 of       686\t|\tloss: 3.02766\n",
      "Training Epoch 17  18.4% | batch:       126 of       686\t|\tloss: 3.00534\n",
      "Training Epoch 17  18.5% | batch:       127 of       686\t|\tloss: 2.66204\n",
      "Training Epoch 17  18.7% | batch:       128 of       686\t|\tloss: 3.28145\n",
      "Training Epoch 17  18.8% | batch:       129 of       686\t|\tloss: 2.17038\n",
      "Training Epoch 17  19.0% | batch:       130 of       686\t|\tloss: 3.35277\n",
      "Training Epoch 17  19.1% | batch:       131 of       686\t|\tloss: 2.71066\n",
      "Training Epoch 17  19.2% | batch:       132 of       686\t|\tloss: 2.91518\n",
      "Training Epoch 17  19.4% | batch:       133 of       686\t|\tloss: 2.69509\n",
      "Training Epoch 17  19.5% | batch:       134 of       686\t|\tloss: 2.56396\n",
      "Training Epoch 17  19.7% | batch:       135 of       686\t|\tloss: 3.17946\n",
      "Training Epoch 17  19.8% | batch:       136 of       686\t|\tloss: 2.90009\n",
      "Training Epoch 17  20.0% | batch:       137 of       686\t|\tloss: 2.46865\n",
      "Training Epoch 17  20.1% | batch:       138 of       686\t|\tloss: 2.66491\n",
      "Training Epoch 17  20.3% | batch:       139 of       686\t|\tloss: 2.47692\n",
      "Training Epoch 17  20.4% | batch:       140 of       686\t|\tloss: 2.87157\n",
      "Training Epoch 17  20.6% | batch:       141 of       686\t|\tloss: 2.33322\n",
      "Training Epoch 17  20.7% | batch:       142 of       686\t|\tloss: 2.41334\n",
      "Training Epoch 17  20.8% | batch:       143 of       686\t|\tloss: 2.72789\n",
      "Training Epoch 17  21.0% | batch:       144 of       686\t|\tloss: 3.13542\n",
      "Training Epoch 17  21.1% | batch:       145 of       686\t|\tloss: 2.77348\n",
      "Training Epoch 17  21.3% | batch:       146 of       686\t|\tloss: 2.31078\n",
      "Training Epoch 17  21.4% | batch:       147 of       686\t|\tloss: 2.98515\n",
      "Training Epoch 17  21.6% | batch:       148 of       686\t|\tloss: 2.72213\n",
      "Training Epoch 17  21.7% | batch:       149 of       686\t|\tloss: 2.94893\n",
      "Training Epoch 17  21.9% | batch:       150 of       686\t|\tloss: 2.64137\n",
      "Training Epoch 17  22.0% | batch:       151 of       686\t|\tloss: 2.65542\n",
      "Training Epoch 17  22.2% | batch:       152 of       686\t|\tloss: 2.09851\n",
      "Training Epoch 17  22.3% | batch:       153 of       686\t|\tloss: 2.9649\n",
      "Training Epoch 17  22.4% | batch:       154 of       686\t|\tloss: 3.33769\n",
      "Training Epoch 17  22.6% | batch:       155 of       686\t|\tloss: 2.25916\n",
      "Training Epoch 17  22.7% | batch:       156 of       686\t|\tloss: 2.43078\n",
      "Training Epoch 17  22.9% | batch:       157 of       686\t|\tloss: 2.23783\n",
      "Training Epoch 17  23.0% | batch:       158 of       686\t|\tloss: 3.07427\n",
      "Training Epoch 17  23.2% | batch:       159 of       686\t|\tloss: 2.3825\n",
      "Training Epoch 17  23.3% | batch:       160 of       686\t|\tloss: 2.23134\n",
      "Training Epoch 17  23.5% | batch:       161 of       686\t|\tloss: 2.74358\n",
      "Training Epoch 17  23.6% | batch:       162 of       686\t|\tloss: 2.76099\n",
      "Training Epoch 17  23.8% | batch:       163 of       686\t|\tloss: 3.73431\n",
      "Training Epoch 17  23.9% | batch:       164 of       686\t|\tloss: 3.00697\n",
      "Training Epoch 17  24.1% | batch:       165 of       686\t|\tloss: 2.86784\n",
      "Training Epoch 17  24.2% | batch:       166 of       686\t|\tloss: 2.04107\n",
      "Training Epoch 17  24.3% | batch:       167 of       686\t|\tloss: 2.68831\n",
      "Training Epoch 17  24.5% | batch:       168 of       686\t|\tloss: 2.91998\n",
      "Training Epoch 17  24.6% | batch:       169 of       686\t|\tloss: 3.03783\n",
      "Training Epoch 17  24.8% | batch:       170 of       686\t|\tloss: 2.30106\n",
      "Training Epoch 17  24.9% | batch:       171 of       686\t|\tloss: 2.45814\n",
      "Training Epoch 17  25.1% | batch:       172 of       686\t|\tloss: 3.10575\n",
      "Training Epoch 17  25.2% | batch:       173 of       686\t|\tloss: 2.69471\n",
      "Training Epoch 17  25.4% | batch:       174 of       686\t|\tloss: 3.0063\n",
      "Training Epoch 17  25.5% | batch:       175 of       686\t|\tloss: 2.61546\n",
      "Training Epoch 17  25.7% | batch:       176 of       686\t|\tloss: 3.44802\n",
      "Training Epoch 17  25.8% | batch:       177 of       686\t|\tloss: 2.33228\n",
      "Training Epoch 17  25.9% | batch:       178 of       686\t|\tloss: 3.02834\n",
      "Training Epoch 17  26.1% | batch:       179 of       686\t|\tloss: 2.8206\n",
      "Training Epoch 17  26.2% | batch:       180 of       686\t|\tloss: 2.33116\n",
      "Training Epoch 17  26.4% | batch:       181 of       686\t|\tloss: 3.19313\n",
      "Training Epoch 17  26.5% | batch:       182 of       686\t|\tloss: 2.839\n",
      "Training Epoch 17  26.7% | batch:       183 of       686\t|\tloss: 2.54459\n",
      "Training Epoch 17  26.8% | batch:       184 of       686\t|\tloss: 2.21939\n",
      "Training Epoch 17  27.0% | batch:       185 of       686\t|\tloss: 3.09743\n",
      "Training Epoch 17  27.1% | batch:       186 of       686\t|\tloss: 2.19635\n",
      "Training Epoch 17  27.3% | batch:       187 of       686\t|\tloss: 2.91837\n",
      "Training Epoch 17  27.4% | batch:       188 of       686\t|\tloss: 2.14079\n",
      "Training Epoch 17  27.6% | batch:       189 of       686\t|\tloss: 2.18833\n",
      "Training Epoch 17  27.7% | batch:       190 of       686\t|\tloss: 2.36075\n",
      "Training Epoch 17  27.8% | batch:       191 of       686\t|\tloss: 2.60979\n",
      "Training Epoch 17  28.0% | batch:       192 of       686\t|\tloss: 2.15015\n",
      "Training Epoch 17  28.1% | batch:       193 of       686\t|\tloss: 2.94745\n",
      "Training Epoch 17  28.3% | batch:       194 of       686\t|\tloss: 3.01311\n",
      "Training Epoch 17  28.4% | batch:       195 of       686\t|\tloss: 2.56658\n",
      "Training Epoch 17  28.6% | batch:       196 of       686\t|\tloss: 2.14589\n",
      "Training Epoch 17  28.7% | batch:       197 of       686\t|\tloss: 2.68502\n",
      "Training Epoch 17  28.9% | batch:       198 of       686\t|\tloss: 2.74281\n",
      "Training Epoch 17  29.0% | batch:       199 of       686\t|\tloss: 2.31207\n",
      "Training Epoch 17  29.2% | batch:       200 of       686\t|\tloss: 2.8994\n",
      "Training Epoch 17  29.3% | batch:       201 of       686\t|\tloss: 3.07314\n",
      "Training Epoch 17  29.4% | batch:       202 of       686\t|\tloss: 3.47035\n",
      "Training Epoch 17  29.6% | batch:       203 of       686\t|\tloss: 2.92655\n",
      "Training Epoch 17  29.7% | batch:       204 of       686\t|\tloss: 2.45165\n",
      "Training Epoch 17  29.9% | batch:       205 of       686\t|\tloss: 2.61129\n",
      "Training Epoch 17  30.0% | batch:       206 of       686\t|\tloss: 2.24881\n",
      "Training Epoch 17  30.2% | batch:       207 of       686\t|\tloss: 2.89125\n",
      "Training Epoch 17  30.3% | batch:       208 of       686\t|\tloss: 2.76058\n",
      "Training Epoch 17  30.5% | batch:       209 of       686\t|\tloss: 2.6425\n",
      "Training Epoch 17  30.6% | batch:       210 of       686\t|\tloss: 2.26993\n",
      "Training Epoch 17  30.8% | batch:       211 of       686\t|\tloss: 2.76214\n",
      "Training Epoch 17  30.9% | batch:       212 of       686\t|\tloss: 3.61795\n",
      "Training Epoch 17  31.0% | batch:       213 of       686\t|\tloss: 3.07509\n",
      "Training Epoch 17  31.2% | batch:       214 of       686\t|\tloss: 2.98469\n",
      "Training Epoch 17  31.3% | batch:       215 of       686\t|\tloss: 2.83715\n",
      "Training Epoch 17  31.5% | batch:       216 of       686\t|\tloss: 1.9089\n",
      "Training Epoch 17  31.6% | batch:       217 of       686\t|\tloss: 3.25782\n",
      "Training Epoch 17  31.8% | batch:       218 of       686\t|\tloss: 3.93359\n",
      "Training Epoch 17  31.9% | batch:       219 of       686\t|\tloss: 3.13992\n",
      "Training Epoch 17  32.1% | batch:       220 of       686\t|\tloss: 2.31109\n",
      "Training Epoch 17  32.2% | batch:       221 of       686\t|\tloss: 2.81989\n",
      "Training Epoch 17  32.4% | batch:       222 of       686\t|\tloss: 2.42503\n",
      "Training Epoch 17  32.5% | batch:       223 of       686\t|\tloss: 2.26849\n",
      "Training Epoch 17  32.7% | batch:       224 of       686\t|\tloss: 2.56738\n",
      "Training Epoch 17  32.8% | batch:       225 of       686\t|\tloss: 2.85977\n",
      "Training Epoch 17  32.9% | batch:       226 of       686\t|\tloss: 3.18015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 17  33.1% | batch:       227 of       686\t|\tloss: 2.89487\n",
      "Training Epoch 17  33.2% | batch:       228 of       686\t|\tloss: 2.41321\n",
      "Training Epoch 17  33.4% | batch:       229 of       686\t|\tloss: 2.91144\n",
      "Training Epoch 17  33.5% | batch:       230 of       686\t|\tloss: 2.26515\n",
      "Training Epoch 17  33.7% | batch:       231 of       686\t|\tloss: 2.49355\n",
      "Training Epoch 17  33.8% | batch:       232 of       686\t|\tloss: 2.85523\n",
      "Training Epoch 17  34.0% | batch:       233 of       686\t|\tloss: 2.25925\n",
      "Training Epoch 17  34.1% | batch:       234 of       686\t|\tloss: 2.71221\n",
      "Training Epoch 17  34.3% | batch:       235 of       686\t|\tloss: 2.72213\n",
      "Training Epoch 17  34.4% | batch:       236 of       686\t|\tloss: 1.4863\n",
      "Training Epoch 17  34.5% | batch:       237 of       686\t|\tloss: 2.3997\n",
      "Training Epoch 17  34.7% | batch:       238 of       686\t|\tloss: 2.83713\n",
      "Training Epoch 17  34.8% | batch:       239 of       686\t|\tloss: 2.87454\n",
      "Training Epoch 17  35.0% | batch:       240 of       686\t|\tloss: 3.38013\n",
      "Training Epoch 17  35.1% | batch:       241 of       686\t|\tloss: 2.80443\n",
      "Training Epoch 17  35.3% | batch:       242 of       686\t|\tloss: 2.62852\n",
      "Training Epoch 17  35.4% | batch:       243 of       686\t|\tloss: 2.31417\n",
      "Training Epoch 17  35.6% | batch:       244 of       686\t|\tloss: 2.70011\n",
      "Training Epoch 17  35.7% | batch:       245 of       686\t|\tloss: 2.66077\n",
      "Training Epoch 17  35.9% | batch:       246 of       686\t|\tloss: 2.29741\n",
      "Training Epoch 17  36.0% | batch:       247 of       686\t|\tloss: 2.60062\n",
      "Training Epoch 17  36.2% | batch:       248 of       686\t|\tloss: 2.8648\n",
      "Training Epoch 17  36.3% | batch:       249 of       686\t|\tloss: 3.49961\n",
      "Training Epoch 17  36.4% | batch:       250 of       686\t|\tloss: 2.74336\n",
      "Training Epoch 17  36.6% | batch:       251 of       686\t|\tloss: 2.19004\n",
      "Training Epoch 17  36.7% | batch:       252 of       686\t|\tloss: 2.52319\n",
      "Training Epoch 17  36.9% | batch:       253 of       686\t|\tloss: 2.93861\n",
      "Training Epoch 17  37.0% | batch:       254 of       686\t|\tloss: 2.34066\n",
      "Training Epoch 17  37.2% | batch:       255 of       686\t|\tloss: 1.99636\n",
      "Training Epoch 17  37.3% | batch:       256 of       686\t|\tloss: 2.54352\n",
      "Training Epoch 17  37.5% | batch:       257 of       686\t|\tloss: 2.51348\n",
      "Training Epoch 17  37.6% | batch:       258 of       686\t|\tloss: 2.30104\n",
      "Training Epoch 17  37.8% | batch:       259 of       686\t|\tloss: 2.85658\n",
      "Training Epoch 17  37.9% | batch:       260 of       686\t|\tloss: 2.18525\n",
      "Training Epoch 17  38.0% | batch:       261 of       686\t|\tloss: 3.27535\n",
      "Training Epoch 17  38.2% | batch:       262 of       686\t|\tloss: 2.99805\n",
      "Training Epoch 17  38.3% | batch:       263 of       686\t|\tloss: 2.55231\n",
      "Training Epoch 17  38.5% | batch:       264 of       686\t|\tloss: 2.07004\n",
      "Training Epoch 17  38.6% | batch:       265 of       686\t|\tloss: 2.49496\n",
      "Training Epoch 17  38.8% | batch:       266 of       686\t|\tloss: 2.55599\n",
      "Training Epoch 17  38.9% | batch:       267 of       686\t|\tloss: 2.4755\n",
      "Training Epoch 17  39.1% | batch:       268 of       686\t|\tloss: 3.21525\n",
      "Training Epoch 17  39.2% | batch:       269 of       686\t|\tloss: 2.25154\n",
      "Training Epoch 17  39.4% | batch:       270 of       686\t|\tloss: 2.72502\n",
      "Training Epoch 17  39.5% | batch:       271 of       686\t|\tloss: 2.02397\n",
      "Training Epoch 17  39.7% | batch:       272 of       686\t|\tloss: 2.84045\n",
      "Training Epoch 17  39.8% | batch:       273 of       686\t|\tloss: 3.43753\n",
      "Training Epoch 17  39.9% | batch:       274 of       686\t|\tloss: 3.35317\n",
      "Training Epoch 17  40.1% | batch:       275 of       686\t|\tloss: 2.85925\n",
      "Training Epoch 17  40.2% | batch:       276 of       686\t|\tloss: 2.02039\n",
      "Training Epoch 17  40.4% | batch:       277 of       686\t|\tloss: 2.21668\n",
      "Training Epoch 17  40.5% | batch:       278 of       686\t|\tloss: 2.57984\n",
      "Training Epoch 17  40.7% | batch:       279 of       686\t|\tloss: 2.53653\n",
      "Training Epoch 17  40.8% | batch:       280 of       686\t|\tloss: 2.67999\n",
      "Training Epoch 17  41.0% | batch:       281 of       686\t|\tloss: 2.41286\n",
      "Training Epoch 17  41.1% | batch:       282 of       686\t|\tloss: 2.98409\n",
      "Training Epoch 17  41.3% | batch:       283 of       686\t|\tloss: 2.78052\n",
      "Training Epoch 17  41.4% | batch:       284 of       686\t|\tloss: 2.23671\n",
      "Training Epoch 17  41.5% | batch:       285 of       686\t|\tloss: 2.05416\n",
      "Training Epoch 17  41.7% | batch:       286 of       686\t|\tloss: 1.99449\n",
      "Training Epoch 17  41.8% | batch:       287 of       686\t|\tloss: 2.69962\n",
      "Training Epoch 17  42.0% | batch:       288 of       686\t|\tloss: 2.30986\n",
      "Training Epoch 17  42.1% | batch:       289 of       686\t|\tloss: 2.93603\n",
      "Training Epoch 17  42.3% | batch:       290 of       686\t|\tloss: 1.77954\n",
      "Training Epoch 17  42.4% | batch:       291 of       686\t|\tloss: 2.81999\n",
      "Training Epoch 17  42.6% | batch:       292 of       686\t|\tloss: 2.61134\n",
      "Training Epoch 17  42.7% | batch:       293 of       686\t|\tloss: 2.46361\n",
      "Training Epoch 17  42.9% | batch:       294 of       686\t|\tloss: 2.59828\n",
      "Training Epoch 17  43.0% | batch:       295 of       686\t|\tloss: 2.45734\n",
      "Training Epoch 17  43.1% | batch:       296 of       686\t|\tloss: 2.08927\n",
      "Training Epoch 17  43.3% | batch:       297 of       686\t|\tloss: 2.29145\n",
      "Training Epoch 17  43.4% | batch:       298 of       686\t|\tloss: 2.7528\n",
      "Training Epoch 17  43.6% | batch:       299 of       686\t|\tloss: 2.88367\n",
      "Training Epoch 17  43.7% | batch:       300 of       686\t|\tloss: 2.46454\n",
      "Training Epoch 17  43.9% | batch:       301 of       686\t|\tloss: 2.65561\n",
      "Training Epoch 17  44.0% | batch:       302 of       686\t|\tloss: 2.33214\n",
      "Training Epoch 17  44.2% | batch:       303 of       686\t|\tloss: 2.67725\n",
      "Training Epoch 17  44.3% | batch:       304 of       686\t|\tloss: 2.47536\n",
      "Training Epoch 17  44.5% | batch:       305 of       686\t|\tloss: 2.9318\n",
      "Training Epoch 17  44.6% | batch:       306 of       686\t|\tloss: 2.51497\n",
      "Training Epoch 17  44.8% | batch:       307 of       686\t|\tloss: 2.5952\n",
      "Training Epoch 17  44.9% | batch:       308 of       686\t|\tloss: 2.37812\n",
      "Training Epoch 17  45.0% | batch:       309 of       686\t|\tloss: 2.50077\n",
      "Training Epoch 17  45.2% | batch:       310 of       686\t|\tloss: 3.03145\n",
      "Training Epoch 17  45.3% | batch:       311 of       686\t|\tloss: 3.16798\n",
      "Training Epoch 17  45.5% | batch:       312 of       686\t|\tloss: 3.15961\n",
      "Training Epoch 17  45.6% | batch:       313 of       686\t|\tloss: 2.23398\n",
      "Training Epoch 17  45.8% | batch:       314 of       686\t|\tloss: 2.8117\n",
      "Training Epoch 17  45.9% | batch:       315 of       686\t|\tloss: 2.42345\n",
      "Training Epoch 17  46.1% | batch:       316 of       686\t|\tloss: 2.64175\n",
      "Training Epoch 17  46.2% | batch:       317 of       686\t|\tloss: 3.02495\n",
      "Training Epoch 17  46.4% | batch:       318 of       686\t|\tloss: 2.72951\n",
      "Training Epoch 17  46.5% | batch:       319 of       686\t|\tloss: 2.77831\n",
      "Training Epoch 17  46.6% | batch:       320 of       686\t|\tloss: 2.16766\n",
      "Training Epoch 17  46.8% | batch:       321 of       686\t|\tloss: 2.44071\n",
      "Training Epoch 17  46.9% | batch:       322 of       686\t|\tloss: 2.75864\n",
      "Training Epoch 17  47.1% | batch:       323 of       686\t|\tloss: 2.17541\n",
      "Training Epoch 17  47.2% | batch:       324 of       686\t|\tloss: 1.75656\n",
      "Training Epoch 17  47.4% | batch:       325 of       686\t|\tloss: 2.18208\n",
      "Training Epoch 17  47.5% | batch:       326 of       686\t|\tloss: 2.24587\n",
      "Training Epoch 17  47.7% | batch:       327 of       686\t|\tloss: 2.66198\n",
      "Training Epoch 17  47.8% | batch:       328 of       686\t|\tloss: 2.23134\n",
      "Training Epoch 17  48.0% | batch:       329 of       686\t|\tloss: 2.76213\n",
      "Training Epoch 17  48.1% | batch:       330 of       686\t|\tloss: 2.8562\n",
      "Training Epoch 17  48.3% | batch:       331 of       686\t|\tloss: 2.93912\n",
      "Training Epoch 17  48.4% | batch:       332 of       686\t|\tloss: 2.60738\n",
      "Training Epoch 17  48.5% | batch:       333 of       686\t|\tloss: 3.4634\n",
      "Training Epoch 17  48.7% | batch:       334 of       686\t|\tloss: 2.23449\n",
      "Training Epoch 17  48.8% | batch:       335 of       686\t|\tloss: 2.95731\n",
      "Training Epoch 17  49.0% | batch:       336 of       686\t|\tloss: 2.19795\n",
      "Training Epoch 17  49.1% | batch:       337 of       686\t|\tloss: 2.58359\n",
      "Training Epoch 17  49.3% | batch:       338 of       686\t|\tloss: 2.24906\n",
      "Training Epoch 17  49.4% | batch:       339 of       686\t|\tloss: 2.30763\n",
      "Training Epoch 17  49.6% | batch:       340 of       686\t|\tloss: 2.61572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 17  49.7% | batch:       341 of       686\t|\tloss: 2.1798\n",
      "Training Epoch 17  49.9% | batch:       342 of       686\t|\tloss: 2.69179\n",
      "Training Epoch 17  50.0% | batch:       343 of       686\t|\tloss: 2.9584\n",
      "Training Epoch 17  50.1% | batch:       344 of       686\t|\tloss: 2.79912\n",
      "Training Epoch 17  50.3% | batch:       345 of       686\t|\tloss: 1.82554\n",
      "Training Epoch 17  50.4% | batch:       346 of       686\t|\tloss: 2.40732\n",
      "Training Epoch 17  50.6% | batch:       347 of       686\t|\tloss: 1.99421\n",
      "Training Epoch 17  50.7% | batch:       348 of       686\t|\tloss: 2.22547\n",
      "Training Epoch 17  50.9% | batch:       349 of       686\t|\tloss: 2.886\n",
      "Training Epoch 17  51.0% | batch:       350 of       686\t|\tloss: 2.5836\n",
      "Training Epoch 17  51.2% | batch:       351 of       686\t|\tloss: 2.7456\n",
      "Training Epoch 17  51.3% | batch:       352 of       686\t|\tloss: 2.23681\n",
      "Training Epoch 17  51.5% | batch:       353 of       686\t|\tloss: 2.69516\n",
      "Training Epoch 17  51.6% | batch:       354 of       686\t|\tloss: 1.93101\n",
      "Training Epoch 17  51.7% | batch:       355 of       686\t|\tloss: 2.96043\n",
      "Training Epoch 17  51.9% | batch:       356 of       686\t|\tloss: 2.54417\n",
      "Training Epoch 17  52.0% | batch:       357 of       686\t|\tloss: 3.40245\n",
      "Training Epoch 17  52.2% | batch:       358 of       686\t|\tloss: 2.6671\n",
      "Training Epoch 17  52.3% | batch:       359 of       686\t|\tloss: 2.456\n",
      "Training Epoch 17  52.5% | batch:       360 of       686\t|\tloss: 2.50604\n",
      "Training Epoch 17  52.6% | batch:       361 of       686\t|\tloss: 2.23751\n",
      "Training Epoch 17  52.8% | batch:       362 of       686\t|\tloss: 3.12185\n",
      "Training Epoch 17  52.9% | batch:       363 of       686\t|\tloss: 2.45779\n",
      "Training Epoch 17  53.1% | batch:       364 of       686\t|\tloss: 2.67007\n",
      "Training Epoch 17  53.2% | batch:       365 of       686\t|\tloss: 2.6141\n",
      "Training Epoch 17  53.4% | batch:       366 of       686\t|\tloss: 2.44844\n",
      "Training Epoch 17  53.5% | batch:       367 of       686\t|\tloss: 2.47344\n",
      "Training Epoch 17  53.6% | batch:       368 of       686\t|\tloss: 2.34042\n",
      "Training Epoch 17  53.8% | batch:       369 of       686\t|\tloss: 2.33591\n",
      "Training Epoch 17  53.9% | batch:       370 of       686\t|\tloss: 2.50441\n",
      "Training Epoch 17  54.1% | batch:       371 of       686\t|\tloss: 2.56443\n",
      "Training Epoch 17  54.2% | batch:       372 of       686\t|\tloss: 3.16789\n",
      "Training Epoch 17  54.4% | batch:       373 of       686\t|\tloss: 1.92094\n",
      "Training Epoch 17  54.5% | batch:       374 of       686\t|\tloss: 2.15739\n",
      "Training Epoch 17  54.7% | batch:       375 of       686\t|\tloss: 2.13765\n",
      "Training Epoch 17  54.8% | batch:       376 of       686\t|\tloss: 2.33374\n",
      "Training Epoch 17  55.0% | batch:       377 of       686\t|\tloss: 3.13884\n",
      "Training Epoch 17  55.1% | batch:       378 of       686\t|\tloss: 1.96491\n",
      "Training Epoch 17  55.2% | batch:       379 of       686\t|\tloss: 2.15436\n",
      "Training Epoch 17  55.4% | batch:       380 of       686\t|\tloss: 2.1038\n",
      "Training Epoch 17  55.5% | batch:       381 of       686\t|\tloss: 2.70639\n",
      "Training Epoch 17  55.7% | batch:       382 of       686\t|\tloss: 2.6954\n",
      "Training Epoch 17  55.8% | batch:       383 of       686\t|\tloss: 2.94358\n",
      "Training Epoch 17  56.0% | batch:       384 of       686\t|\tloss: 2.52488\n",
      "Training Epoch 17  56.1% | batch:       385 of       686\t|\tloss: 2.34954\n",
      "Training Epoch 17  56.3% | batch:       386 of       686\t|\tloss: 2.99842\n",
      "Training Epoch 17  56.4% | batch:       387 of       686\t|\tloss: 2.8564\n",
      "Training Epoch 17  56.6% | batch:       388 of       686\t|\tloss: 2.02731\n",
      "Training Epoch 17  56.7% | batch:       389 of       686\t|\tloss: 2.71217\n",
      "Training Epoch 17  56.9% | batch:       390 of       686\t|\tloss: 2.45287\n",
      "Training Epoch 17  57.0% | batch:       391 of       686\t|\tloss: 2.03855\n",
      "Training Epoch 17  57.1% | batch:       392 of       686\t|\tloss: 1.83202\n",
      "Training Epoch 17  57.3% | batch:       393 of       686\t|\tloss: 4.04682\n",
      "Training Epoch 17  57.4% | batch:       394 of       686\t|\tloss: 2.31166\n",
      "Training Epoch 17  57.6% | batch:       395 of       686\t|\tloss: 2.88836\n",
      "Training Epoch 17  57.7% | batch:       396 of       686\t|\tloss: 2.15979\n",
      "Training Epoch 17  57.9% | batch:       397 of       686\t|\tloss: 2.57433\n",
      "Training Epoch 17  58.0% | batch:       398 of       686\t|\tloss: 2.46563\n",
      "Training Epoch 17  58.2% | batch:       399 of       686\t|\tloss: 1.97224\n",
      "Training Epoch 17  58.3% | batch:       400 of       686\t|\tloss: 2.28952\n",
      "Training Epoch 17  58.5% | batch:       401 of       686\t|\tloss: 1.90744\n",
      "Training Epoch 17  58.6% | batch:       402 of       686\t|\tloss: 2.89703\n",
      "Training Epoch 17  58.7% | batch:       403 of       686\t|\tloss: 3.01718\n",
      "Training Epoch 17  58.9% | batch:       404 of       686\t|\tloss: 2.25412\n",
      "Training Epoch 17  59.0% | batch:       405 of       686\t|\tloss: 2.73702\n",
      "Training Epoch 17  59.2% | batch:       406 of       686\t|\tloss: 2.3403\n",
      "Training Epoch 17  59.3% | batch:       407 of       686\t|\tloss: 2.13405\n",
      "Training Epoch 17  59.5% | batch:       408 of       686\t|\tloss: 3.05725\n",
      "Training Epoch 17  59.6% | batch:       409 of       686\t|\tloss: 2.81584\n",
      "Training Epoch 17  59.8% | batch:       410 of       686\t|\tloss: 2.00362\n",
      "Training Epoch 17  59.9% | batch:       411 of       686\t|\tloss: 3.01542\n",
      "Training Epoch 17  60.1% | batch:       412 of       686\t|\tloss: 2.49577\n",
      "Training Epoch 17  60.2% | batch:       413 of       686\t|\tloss: 2.48723\n",
      "Training Epoch 17  60.3% | batch:       414 of       686\t|\tloss: 2.28871\n",
      "Training Epoch 17  60.5% | batch:       415 of       686\t|\tloss: 2.40635\n",
      "Training Epoch 17  60.6% | batch:       416 of       686\t|\tloss: 2.77224\n",
      "Training Epoch 17  60.8% | batch:       417 of       686\t|\tloss: 2.58706\n",
      "Training Epoch 17  60.9% | batch:       418 of       686\t|\tloss: 2.71422\n",
      "Training Epoch 17  61.1% | batch:       419 of       686\t|\tloss: 2.25562\n",
      "Training Epoch 17  61.2% | batch:       420 of       686\t|\tloss: 2.59097\n",
      "Training Epoch 17  61.4% | batch:       421 of       686\t|\tloss: 2.86813\n",
      "Training Epoch 17  61.5% | batch:       422 of       686\t|\tloss: 2.62288\n",
      "Training Epoch 17  61.7% | batch:       423 of       686\t|\tloss: 3.4256\n",
      "Training Epoch 17  61.8% | batch:       424 of       686\t|\tloss: 2.57754\n",
      "Training Epoch 17  62.0% | batch:       425 of       686\t|\tloss: 2.69276\n",
      "Training Epoch 17  62.1% | batch:       426 of       686\t|\tloss: 3.03537\n",
      "Training Epoch 17  62.2% | batch:       427 of       686\t|\tloss: 2.25513\n",
      "Training Epoch 17  62.4% | batch:       428 of       686\t|\tloss: 2.58601\n",
      "Training Epoch 17  62.5% | batch:       429 of       686\t|\tloss: 2.19213\n",
      "Training Epoch 17  62.7% | batch:       430 of       686\t|\tloss: 2.08294\n",
      "Training Epoch 17  62.8% | batch:       431 of       686\t|\tloss: 2.40278\n",
      "Training Epoch 17  63.0% | batch:       432 of       686\t|\tloss: 2.75004\n",
      "Training Epoch 17  63.1% | batch:       433 of       686\t|\tloss: 3.54093\n",
      "Training Epoch 17  63.3% | batch:       434 of       686\t|\tloss: 2.57631\n",
      "Training Epoch 17  63.4% | batch:       435 of       686\t|\tloss: 2.60977\n",
      "Training Epoch 17  63.6% | batch:       436 of       686\t|\tloss: 2.59571\n",
      "Training Epoch 17  63.7% | batch:       437 of       686\t|\tloss: 2.23416\n",
      "Training Epoch 17  63.8% | batch:       438 of       686\t|\tloss: 2.36205\n",
      "Training Epoch 17  64.0% | batch:       439 of       686\t|\tloss: 3.54715\n",
      "Training Epoch 17  64.1% | batch:       440 of       686\t|\tloss: 3.24235\n",
      "Training Epoch 17  64.3% | batch:       441 of       686\t|\tloss: 2.73072\n",
      "Training Epoch 17  64.4% | batch:       442 of       686\t|\tloss: 2.51867\n",
      "Training Epoch 17  64.6% | batch:       443 of       686\t|\tloss: 2.18346\n",
      "Training Epoch 17  64.7% | batch:       444 of       686\t|\tloss: 2.98942\n",
      "Training Epoch 17  64.9% | batch:       445 of       686\t|\tloss: 2.96048\n",
      "Training Epoch 17  65.0% | batch:       446 of       686\t|\tloss: 2.26866\n",
      "Training Epoch 17  65.2% | batch:       447 of       686\t|\tloss: 2.88535\n",
      "Training Epoch 17  65.3% | batch:       448 of       686\t|\tloss: 3.95638\n",
      "Training Epoch 17  65.5% | batch:       449 of       686\t|\tloss: 2.53949\n",
      "Training Epoch 17  65.6% | batch:       450 of       686\t|\tloss: 2.19322\n",
      "Training Epoch 17  65.7% | batch:       451 of       686\t|\tloss: 2.79844\n",
      "Training Epoch 17  65.9% | batch:       452 of       686\t|\tloss: 2.85517\n",
      "Training Epoch 17  66.0% | batch:       453 of       686\t|\tloss: 2.73523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 17  66.2% | batch:       454 of       686\t|\tloss: 2.48857\n",
      "Training Epoch 17  66.3% | batch:       455 of       686\t|\tloss: 1.83816\n",
      "Training Epoch 17  66.5% | batch:       456 of       686\t|\tloss: 2.50291\n",
      "Training Epoch 17  66.6% | batch:       457 of       686\t|\tloss: 3.84773\n",
      "Training Epoch 17  66.8% | batch:       458 of       686\t|\tloss: 3.28843\n",
      "Training Epoch 17  66.9% | batch:       459 of       686\t|\tloss: 3.71089\n",
      "Training Epoch 17  67.1% | batch:       460 of       686\t|\tloss: 3.36671\n",
      "Training Epoch 17  67.2% | batch:       461 of       686\t|\tloss: 2.63087\n",
      "Training Epoch 17  67.3% | batch:       462 of       686\t|\tloss: 2.74169\n",
      "Training Epoch 17  67.5% | batch:       463 of       686\t|\tloss: 2.36299\n",
      "Training Epoch 17  67.6% | batch:       464 of       686\t|\tloss: 2.32441\n",
      "Training Epoch 17  67.8% | batch:       465 of       686\t|\tloss: 2.80465\n",
      "Training Epoch 17  67.9% | batch:       466 of       686\t|\tloss: 3.28305\n",
      "Training Epoch 17  68.1% | batch:       467 of       686\t|\tloss: 2.4237\n",
      "Training Epoch 17  68.2% | batch:       468 of       686\t|\tloss: 2.59002\n",
      "Training Epoch 17  68.4% | batch:       469 of       686\t|\tloss: 3.11045\n",
      "Training Epoch 17  68.5% | batch:       470 of       686\t|\tloss: 2.74613\n",
      "Training Epoch 17  68.7% | batch:       471 of       686\t|\tloss: 2.87416\n",
      "Training Epoch 17  68.8% | batch:       472 of       686\t|\tloss: 3.16633\n",
      "Training Epoch 17  69.0% | batch:       473 of       686\t|\tloss: 2.97032\n",
      "Training Epoch 17  69.1% | batch:       474 of       686\t|\tloss: 2.47824\n",
      "Training Epoch 17  69.2% | batch:       475 of       686\t|\tloss: 2.62765\n",
      "Training Epoch 17  69.4% | batch:       476 of       686\t|\tloss: 2.54402\n",
      "Training Epoch 17  69.5% | batch:       477 of       686\t|\tloss: 2.04247\n",
      "Training Epoch 17  69.7% | batch:       478 of       686\t|\tloss: 2.64462\n",
      "Training Epoch 17  69.8% | batch:       479 of       686\t|\tloss: 3.64857\n",
      "Training Epoch 17  70.0% | batch:       480 of       686\t|\tloss: 2.81154\n",
      "Training Epoch 17  70.1% | batch:       481 of       686\t|\tloss: 2.29947\n",
      "Training Epoch 17  70.3% | batch:       482 of       686\t|\tloss: 2.53196\n",
      "Training Epoch 17  70.4% | batch:       483 of       686\t|\tloss: 3.27829\n",
      "Training Epoch 17  70.6% | batch:       484 of       686\t|\tloss: 2.64885\n",
      "Training Epoch 17  70.7% | batch:       485 of       686\t|\tloss: 2.2184\n",
      "Training Epoch 17  70.8% | batch:       486 of       686\t|\tloss: 2.2379\n",
      "Training Epoch 17  71.0% | batch:       487 of       686\t|\tloss: 3.25443\n",
      "Training Epoch 17  71.1% | batch:       488 of       686\t|\tloss: 2.60955\n",
      "Training Epoch 17  71.3% | batch:       489 of       686\t|\tloss: 2.38802\n",
      "Training Epoch 17  71.4% | batch:       490 of       686\t|\tloss: 3.03664\n",
      "Training Epoch 17  71.6% | batch:       491 of       686\t|\tloss: 2.51774\n",
      "Training Epoch 17  71.7% | batch:       492 of       686\t|\tloss: 3.37179\n",
      "Training Epoch 17  71.9% | batch:       493 of       686\t|\tloss: 3.28823\n",
      "Training Epoch 17  72.0% | batch:       494 of       686\t|\tloss: 2.43915\n",
      "Training Epoch 17  72.2% | batch:       495 of       686\t|\tloss: 2.30773\n",
      "Training Epoch 17  72.3% | batch:       496 of       686\t|\tloss: 2.34114\n",
      "Training Epoch 17  72.4% | batch:       497 of       686\t|\tloss: 2.14694\n",
      "Training Epoch 17  72.6% | batch:       498 of       686\t|\tloss: 2.75385\n",
      "Training Epoch 17  72.7% | batch:       499 of       686\t|\tloss: 3.70903\n",
      "Training Epoch 17  72.9% | batch:       500 of       686\t|\tloss: 2.41332\n",
      "Training Epoch 17  73.0% | batch:       501 of       686\t|\tloss: 2.76242\n",
      "Training Epoch 17  73.2% | batch:       502 of       686\t|\tloss: 2.07112\n",
      "Training Epoch 17  73.3% | batch:       503 of       686\t|\tloss: 2.2992\n",
      "Training Epoch 17  73.5% | batch:       504 of       686\t|\tloss: 3.7293\n",
      "Training Epoch 17  73.6% | batch:       505 of       686\t|\tloss: 2.14873\n",
      "Training Epoch 17  73.8% | batch:       506 of       686\t|\tloss: 1.89858\n",
      "Training Epoch 17  73.9% | batch:       507 of       686\t|\tloss: 2.84084\n",
      "Training Epoch 17  74.1% | batch:       508 of       686\t|\tloss: 1.86692\n",
      "Training Epoch 17  74.2% | batch:       509 of       686\t|\tloss: 2.92229\n",
      "Training Epoch 17  74.3% | batch:       510 of       686\t|\tloss: 2.71224\n",
      "Training Epoch 17  74.5% | batch:       511 of       686\t|\tloss: 2.03194\n",
      "Training Epoch 17  74.6% | batch:       512 of       686\t|\tloss: 2.28778\n",
      "Training Epoch 17  74.8% | batch:       513 of       686\t|\tloss: 2.97472\n",
      "Training Epoch 17  74.9% | batch:       514 of       686\t|\tloss: 3.29686\n",
      "Training Epoch 17  75.1% | batch:       515 of       686\t|\tloss: 2.9229\n",
      "Training Epoch 17  75.2% | batch:       516 of       686\t|\tloss: 3.24316\n",
      "Training Epoch 17  75.4% | batch:       517 of       686\t|\tloss: 2.50664\n",
      "Training Epoch 17  75.5% | batch:       518 of       686\t|\tloss: 3.29832\n",
      "Training Epoch 17  75.7% | batch:       519 of       686\t|\tloss: 2.46183\n",
      "Training Epoch 17  75.8% | batch:       520 of       686\t|\tloss: 2.92824\n",
      "Training Epoch 17  75.9% | batch:       521 of       686\t|\tloss: 2.98567\n",
      "Training Epoch 17  76.1% | batch:       522 of       686\t|\tloss: 3.4824\n",
      "Training Epoch 17  76.2% | batch:       523 of       686\t|\tloss: 1.86616\n",
      "Training Epoch 17  76.4% | batch:       524 of       686\t|\tloss: 3.07664\n",
      "Training Epoch 17  76.5% | batch:       525 of       686\t|\tloss: 3.05723\n",
      "Training Epoch 17  76.7% | batch:       526 of       686\t|\tloss: 2.17175\n",
      "Training Epoch 17  76.8% | batch:       527 of       686\t|\tloss: 2.07361\n",
      "Training Epoch 17  77.0% | batch:       528 of       686\t|\tloss: 2.5427\n",
      "Training Epoch 17  77.1% | batch:       529 of       686\t|\tloss: 1.97909\n",
      "Training Epoch 17  77.3% | batch:       530 of       686\t|\tloss: 2.05242\n",
      "Training Epoch 17  77.4% | batch:       531 of       686\t|\tloss: 2.63284\n",
      "Training Epoch 17  77.6% | batch:       532 of       686\t|\tloss: 2.04434\n",
      "Training Epoch 17  77.7% | batch:       533 of       686\t|\tloss: 2.68611\n",
      "Training Epoch 17  77.8% | batch:       534 of       686\t|\tloss: 1.85115\n",
      "Training Epoch 17  78.0% | batch:       535 of       686\t|\tloss: 2.43537\n",
      "Training Epoch 17  78.1% | batch:       536 of       686\t|\tloss: 2.92398\n",
      "Training Epoch 17  78.3% | batch:       537 of       686\t|\tloss: 2.93124\n",
      "Training Epoch 17  78.4% | batch:       538 of       686\t|\tloss: 3.11691\n",
      "Training Epoch 17  78.6% | batch:       539 of       686\t|\tloss: 2.64519\n",
      "Training Epoch 17  78.7% | batch:       540 of       686\t|\tloss: 2.26535\n",
      "Training Epoch 17  78.9% | batch:       541 of       686\t|\tloss: 2.10104\n",
      "Training Epoch 17  79.0% | batch:       542 of       686\t|\tloss: 2.66152\n",
      "Training Epoch 17  79.2% | batch:       543 of       686\t|\tloss: 2.587\n",
      "Training Epoch 17  79.3% | batch:       544 of       686\t|\tloss: 2.25142\n",
      "Training Epoch 17  79.4% | batch:       545 of       686\t|\tloss: 2.38867\n",
      "Training Epoch 17  79.6% | batch:       546 of       686\t|\tloss: 2.53768\n",
      "Training Epoch 17  79.7% | batch:       547 of       686\t|\tloss: 1.86003\n",
      "Training Epoch 17  79.9% | batch:       548 of       686\t|\tloss: 1.92647\n",
      "Training Epoch 17  80.0% | batch:       549 of       686\t|\tloss: 2.66182\n",
      "Training Epoch 17  80.2% | batch:       550 of       686\t|\tloss: 2.64337\n",
      "Training Epoch 17  80.3% | batch:       551 of       686\t|\tloss: 2.55062\n",
      "Training Epoch 17  80.5% | batch:       552 of       686\t|\tloss: 2.52778\n",
      "Training Epoch 17  80.6% | batch:       553 of       686\t|\tloss: 3.07631\n",
      "Training Epoch 17  80.8% | batch:       554 of       686\t|\tloss: 2.8957\n",
      "Training Epoch 17  80.9% | batch:       555 of       686\t|\tloss: 2.68798\n",
      "Training Epoch 17  81.0% | batch:       556 of       686\t|\tloss: 2.58101\n",
      "Training Epoch 17  81.2% | batch:       557 of       686\t|\tloss: 2.72852\n",
      "Training Epoch 17  81.3% | batch:       558 of       686\t|\tloss: 2.94686\n",
      "Training Epoch 17  81.5% | batch:       559 of       686\t|\tloss: 3.50798\n",
      "Training Epoch 17  81.6% | batch:       560 of       686\t|\tloss: 2.87475\n",
      "Training Epoch 17  81.8% | batch:       561 of       686\t|\tloss: 2.23818\n",
      "Training Epoch 17  81.9% | batch:       562 of       686\t|\tloss: 2.32488\n",
      "Training Epoch 17  82.1% | batch:       563 of       686\t|\tloss: 2.96396\n",
      "Training Epoch 17  82.2% | batch:       564 of       686\t|\tloss: 2.35831\n",
      "Training Epoch 17  82.4% | batch:       565 of       686\t|\tloss: 2.23039\n",
      "Training Epoch 17  82.5% | batch:       566 of       686\t|\tloss: 2.42953\n",
      "Training Epoch 17  82.7% | batch:       567 of       686\t|\tloss: 2.31825\n",
      "Training Epoch 17  82.8% | batch:       568 of       686\t|\tloss: 2.66937\n",
      "Training Epoch 17  82.9% | batch:       569 of       686\t|\tloss: 1.69492\n",
      "Training Epoch 17  83.1% | batch:       570 of       686\t|\tloss: 2.19527\n",
      "Training Epoch 17  83.2% | batch:       571 of       686\t|\tloss: 1.8841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 17  83.4% | batch:       572 of       686\t|\tloss: 2.67079\n",
      "Training Epoch 17  83.5% | batch:       573 of       686\t|\tloss: 2.31019\n",
      "Training Epoch 17  83.7% | batch:       574 of       686\t|\tloss: 2.05809\n",
      "Training Epoch 17  83.8% | batch:       575 of       686\t|\tloss: 2.33183\n",
      "Training Epoch 17  84.0% | batch:       576 of       686\t|\tloss: 2.39252\n",
      "Training Epoch 17  84.1% | batch:       577 of       686\t|\tloss: 2.1502\n",
      "Training Epoch 17  84.3% | batch:       578 of       686\t|\tloss: 2.28317\n",
      "Training Epoch 17  84.4% | batch:       579 of       686\t|\tloss: 2.16891\n",
      "Training Epoch 17  84.5% | batch:       580 of       686\t|\tloss: 2.15955\n",
      "Training Epoch 17  84.7% | batch:       581 of       686\t|\tloss: 2.93755\n",
      "Training Epoch 17  84.8% | batch:       582 of       686\t|\tloss: 2.06428\n",
      "Training Epoch 17  85.0% | batch:       583 of       686\t|\tloss: 2.20735\n",
      "Training Epoch 17  85.1% | batch:       584 of       686\t|\tloss: 3.04037\n",
      "Training Epoch 17  85.3% | batch:       585 of       686\t|\tloss: 2.20397\n",
      "Training Epoch 17  85.4% | batch:       586 of       686\t|\tloss: 2.83226\n",
      "Training Epoch 17  85.6% | batch:       587 of       686\t|\tloss: 2.67658\n",
      "Training Epoch 17  85.7% | batch:       588 of       686\t|\tloss: 2.14013\n",
      "Training Epoch 17  85.9% | batch:       589 of       686\t|\tloss: 3.39349\n",
      "Training Epoch 17  86.0% | batch:       590 of       686\t|\tloss: 3.10233\n",
      "Training Epoch 17  86.2% | batch:       591 of       686\t|\tloss: 2.59084\n",
      "Training Epoch 17  86.3% | batch:       592 of       686\t|\tloss: 3.15138\n",
      "Training Epoch 17  86.4% | batch:       593 of       686\t|\tloss: 1.74991\n",
      "Training Epoch 17  86.6% | batch:       594 of       686\t|\tloss: 2.93189\n",
      "Training Epoch 17  86.7% | batch:       595 of       686\t|\tloss: 2.05198\n",
      "Training Epoch 17  86.9% | batch:       596 of       686\t|\tloss: 2.61299\n",
      "Training Epoch 17  87.0% | batch:       597 of       686\t|\tloss: 2.33009\n",
      "Training Epoch 17  87.2% | batch:       598 of       686\t|\tloss: 1.96212\n",
      "Training Epoch 17  87.3% | batch:       599 of       686\t|\tloss: 2.59814\n",
      "Training Epoch 17  87.5% | batch:       600 of       686\t|\tloss: 3.49088\n",
      "Training Epoch 17  87.6% | batch:       601 of       686\t|\tloss: 1.93328\n",
      "Training Epoch 17  87.8% | batch:       602 of       686\t|\tloss: 2.15764\n",
      "Training Epoch 17  87.9% | batch:       603 of       686\t|\tloss: 4.26776\n",
      "Training Epoch 17  88.0% | batch:       604 of       686\t|\tloss: 2.52038\n",
      "Training Epoch 17  88.2% | batch:       605 of       686\t|\tloss: 2.52988\n",
      "Training Epoch 17  88.3% | batch:       606 of       686\t|\tloss: 2.86902\n",
      "Training Epoch 17  88.5% | batch:       607 of       686\t|\tloss: 2.37966\n",
      "Training Epoch 17  88.6% | batch:       608 of       686\t|\tloss: 2.58096\n",
      "Training Epoch 17  88.8% | batch:       609 of       686\t|\tloss: 2.89318\n",
      "Training Epoch 17  88.9% | batch:       610 of       686\t|\tloss: 2.69837\n",
      "Training Epoch 17  89.1% | batch:       611 of       686\t|\tloss: 2.19428\n",
      "Training Epoch 17  89.2% | batch:       612 of       686\t|\tloss: 3.07024\n",
      "Training Epoch 17  89.4% | batch:       613 of       686\t|\tloss: 2.87049\n",
      "Training Epoch 17  89.5% | batch:       614 of       686\t|\tloss: 2.3132\n",
      "Training Epoch 17  89.7% | batch:       615 of       686\t|\tloss: 3.7526\n",
      "Training Epoch 17  89.8% | batch:       616 of       686\t|\tloss: 2.50253\n",
      "Training Epoch 17  89.9% | batch:       617 of       686\t|\tloss: 2.33326\n",
      "Training Epoch 17  90.1% | batch:       618 of       686\t|\tloss: 2.40964\n",
      "Training Epoch 17  90.2% | batch:       619 of       686\t|\tloss: 2.05956\n",
      "Training Epoch 17  90.4% | batch:       620 of       686\t|\tloss: 2.50192\n",
      "Training Epoch 17  90.5% | batch:       621 of       686\t|\tloss: 2.21884\n",
      "Training Epoch 17  90.7% | batch:       622 of       686\t|\tloss: 2.45975\n",
      "Training Epoch 17  90.8% | batch:       623 of       686\t|\tloss: 2.71232\n",
      "Training Epoch 17  91.0% | batch:       624 of       686\t|\tloss: 2.44723\n",
      "Training Epoch 17  91.1% | batch:       625 of       686\t|\tloss: 2.26188\n",
      "Training Epoch 17  91.3% | batch:       626 of       686\t|\tloss: 2.14028\n",
      "Training Epoch 17  91.4% | batch:       627 of       686\t|\tloss: 2.51581\n",
      "Training Epoch 17  91.5% | batch:       628 of       686\t|\tloss: 2.95416\n",
      "Training Epoch 17  91.7% | batch:       629 of       686\t|\tloss: 2.79656\n",
      "Training Epoch 17  91.8% | batch:       630 of       686\t|\tloss: 3.04482\n",
      "Training Epoch 17  92.0% | batch:       631 of       686\t|\tloss: 2.21117\n",
      "Training Epoch 17  92.1% | batch:       632 of       686\t|\tloss: 2.44909\n",
      "Training Epoch 17  92.3% | batch:       633 of       686\t|\tloss: 2.41436\n",
      "Training Epoch 17  92.4% | batch:       634 of       686\t|\tloss: 2.35144\n",
      "Training Epoch 17  92.6% | batch:       635 of       686\t|\tloss: 2.78211\n",
      "Training Epoch 17  92.7% | batch:       636 of       686\t|\tloss: 2.27198\n",
      "Training Epoch 17  92.9% | batch:       637 of       686\t|\tloss: 2.63602\n",
      "Training Epoch 17  93.0% | batch:       638 of       686\t|\tloss: 2.16108\n",
      "Training Epoch 17  93.1% | batch:       639 of       686\t|\tloss: 2.2902\n",
      "Training Epoch 17  93.3% | batch:       640 of       686\t|\tloss: 1.94449\n",
      "Training Epoch 17  93.4% | batch:       641 of       686\t|\tloss: 2.2483\n",
      "Training Epoch 17  93.6% | batch:       642 of       686\t|\tloss: 2.73628\n",
      "Training Epoch 17  93.7% | batch:       643 of       686\t|\tloss: 2.08689\n",
      "Training Epoch 17  93.9% | batch:       644 of       686\t|\tloss: 2.47024\n",
      "Training Epoch 17  94.0% | batch:       645 of       686\t|\tloss: 3.32523\n",
      "Training Epoch 17  94.2% | batch:       646 of       686\t|\tloss: 2.30859\n",
      "Training Epoch 17  94.3% | batch:       647 of       686\t|\tloss: 3.10511\n",
      "Training Epoch 17  94.5% | batch:       648 of       686\t|\tloss: 2.28217\n",
      "Training Epoch 17  94.6% | batch:       649 of       686\t|\tloss: 2.75491\n",
      "Training Epoch 17  94.8% | batch:       650 of       686\t|\tloss: 2.84075\n",
      "Training Epoch 17  94.9% | batch:       651 of       686\t|\tloss: 2.06389\n",
      "Training Epoch 17  95.0% | batch:       652 of       686\t|\tloss: 2.50573\n",
      "Training Epoch 17  95.2% | batch:       653 of       686\t|\tloss: 2.90798\n",
      "Training Epoch 17  95.3% | batch:       654 of       686\t|\tloss: 2.66626\n",
      "Training Epoch 17  95.5% | batch:       655 of       686\t|\tloss: 2.34676\n",
      "Training Epoch 17  95.6% | batch:       656 of       686\t|\tloss: 2.62619\n",
      "Training Epoch 17  95.8% | batch:       657 of       686\t|\tloss: 2.18622\n",
      "Training Epoch 17  95.9% | batch:       658 of       686\t|\tloss: 1.98548\n",
      "Training Epoch 17  96.1% | batch:       659 of       686\t|\tloss: 2.53395\n",
      "Training Epoch 17  96.2% | batch:       660 of       686\t|\tloss: 2.32022\n",
      "Training Epoch 17  96.4% | batch:       661 of       686\t|\tloss: 2.285\n",
      "Training Epoch 17  96.5% | batch:       662 of       686\t|\tloss: 2.03958\n",
      "Training Epoch 17  96.6% | batch:       663 of       686\t|\tloss: 1.76234\n",
      "Training Epoch 17  96.8% | batch:       664 of       686\t|\tloss: 2.02246\n",
      "Training Epoch 17  96.9% | batch:       665 of       686\t|\tloss: 2.18549\n",
      "Training Epoch 17  97.1% | batch:       666 of       686\t|\tloss: 2.52482\n",
      "Training Epoch 17  97.2% | batch:       667 of       686\t|\tloss: 2.6043\n",
      "Training Epoch 17  97.4% | batch:       668 of       686\t|\tloss: 2.55895\n",
      "Training Epoch 17  97.5% | batch:       669 of       686\t|\tloss: 1.99417\n",
      "Training Epoch 17  97.7% | batch:       670 of       686\t|\tloss: 2.59654\n",
      "Training Epoch 17  97.8% | batch:       671 of       686\t|\tloss: 2.74167\n",
      "Training Epoch 17  98.0% | batch:       672 of       686\t|\tloss: 2.29024\n",
      "Training Epoch 17  98.1% | batch:       673 of       686\t|\tloss: 1.6762\n",
      "Training Epoch 17  98.3% | batch:       674 of       686\t|\tloss: 2.71867\n",
      "Training Epoch 17  98.4% | batch:       675 of       686\t|\tloss: 2.54279\n",
      "Training Epoch 17  98.5% | batch:       676 of       686\t|\tloss: 2.76666\n",
      "Training Epoch 17  98.7% | batch:       677 of       686\t|\tloss: 2.29362\n",
      "Training Epoch 17  98.8% | batch:       678 of       686\t|\tloss: 2.10342\n",
      "Training Epoch 17  99.0% | batch:       679 of       686\t|\tloss: 2.17978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:07:42,465 | INFO : Epoch 17 Training Summary: epoch: 17.000000 | loss: 2.628014 | \n",
      "2023-05-25 22:07:42,466 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 23.65118670463562 seconds\n",
      "\n",
      "2023-05-25 22:07:42,466 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.902793379390943 seconds\n",
      "2023-05-25 22:07:42,467 | INFO : Avg batch train. time: 0.03484372212739204 seconds\n",
      "2023-05-25 22:07:42,467 | INFO : Avg sample train. time: 0.00027256734567981006 seconds\n",
      "2023-05-25 22:07:42,468 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 17  99.1% | batch:       680 of       686\t|\tloss: 2.03818\n",
      "Training Epoch 17  99.3% | batch:       681 of       686\t|\tloss: 2.09049\n",
      "Training Epoch 17  99.4% | batch:       682 of       686\t|\tloss: 2.11428\n",
      "Training Epoch 17  99.6% | batch:       683 of       686\t|\tloss: 2.25072\n",
      "Training Epoch 17  99.7% | batch:       684 of       686\t|\tloss: 2.59592\n",
      "Training Epoch 17  99.9% | batch:       685 of       686\t|\tloss: 3.49292\n",
      "\n",
      "Evaluating Epoch 17   0.0% | batch:         0 of       172\t|\tloss: 1.5872\n",
      "Evaluating Epoch 17   0.6% | batch:         1 of       172\t|\tloss: 2.38049\n",
      "Evaluating Epoch 17   1.2% | batch:         2 of       172\t|\tloss: 1.89706\n",
      "Evaluating Epoch 17   1.7% | batch:         3 of       172\t|\tloss: 3.51747\n",
      "Evaluating Epoch 17   2.3% | batch:         4 of       172\t|\tloss: 2.01961\n",
      "Evaluating Epoch 17   2.9% | batch:         5 of       172\t|\tloss: 1.64565\n",
      "Evaluating Epoch 17   3.5% | batch:         6 of       172\t|\tloss: 2.49236\n",
      "Evaluating Epoch 17   4.1% | batch:         7 of       172\t|\tloss: 3.55869\n",
      "Evaluating Epoch 17   4.7% | batch:         8 of       172\t|\tloss: 1.62603\n",
      "Evaluating Epoch 17   5.2% | batch:         9 of       172\t|\tloss: 2.05852\n",
      "Evaluating Epoch 17   5.8% | batch:        10 of       172\t|\tloss: 2.53194\n",
      "Evaluating Epoch 17   6.4% | batch:        11 of       172\t|\tloss: 2.23614\n",
      "Evaluating Epoch 17   7.0% | batch:        12 of       172\t|\tloss: 1.64341\n",
      "Evaluating Epoch 17   7.6% | batch:        13 of       172\t|\tloss: 2.3736\n",
      "Evaluating Epoch 17   8.1% | batch:        14 of       172\t|\tloss: 2.81321\n",
      "Evaluating Epoch 17   8.7% | batch:        15 of       172\t|\tloss: 2.07166\n",
      "Evaluating Epoch 17   9.3% | batch:        16 of       172\t|\tloss: 2.66692\n",
      "Evaluating Epoch 17   9.9% | batch:        17 of       172\t|\tloss: 1.80424\n",
      "Evaluating Epoch 17  10.5% | batch:        18 of       172\t|\tloss: 18.4047\n",
      "Evaluating Epoch 17  11.0% | batch:        19 of       172\t|\tloss: 1.74256\n",
      "Evaluating Epoch 17  11.6% | batch:        20 of       172\t|\tloss: 1.52153\n",
      "Evaluating Epoch 17  12.2% | batch:        21 of       172\t|\tloss: 0.296091\n",
      "Evaluating Epoch 17  12.8% | batch:        22 of       172\t|\tloss: 3.88266\n",
      "Evaluating Epoch 17  13.4% | batch:        23 of       172\t|\tloss: 2.94753\n",
      "Evaluating Epoch 17  14.0% | batch:        24 of       172\t|\tloss: 1.55846\n",
      "Evaluating Epoch 17  14.5% | batch:        25 of       172\t|\tloss: 2.06209\n",
      "Evaluating Epoch 17  15.1% | batch:        26 of       172\t|\tloss: 8.74347\n",
      "Evaluating Epoch 17  15.7% | batch:        27 of       172\t|\tloss: 16.7336\n",
      "Evaluating Epoch 17  16.3% | batch:        28 of       172\t|\tloss: 0.674859\n",
      "Evaluating Epoch 17  16.9% | batch:        29 of       172\t|\tloss: 1.04776\n",
      "Evaluating Epoch 17  17.4% | batch:        30 of       172\t|\tloss: 0.440224\n",
      "Evaluating Epoch 17  18.0% | batch:        31 of       172\t|\tloss: 0.169989\n",
      "Evaluating Epoch 17  18.6% | batch:        32 of       172\t|\tloss: 1.48526\n",
      "Evaluating Epoch 17  19.2% | batch:        33 of       172\t|\tloss: 0.742865\n",
      "Evaluating Epoch 17  19.8% | batch:        34 of       172\t|\tloss: 0.900115\n",
      "Evaluating Epoch 17  20.3% | batch:        35 of       172\t|\tloss: 0.452933\n",
      "Evaluating Epoch 17  20.9% | batch:        36 of       172\t|\tloss: 4.02424\n",
      "Evaluating Epoch 17  21.5% | batch:        37 of       172\t|\tloss: 5.75612\n",
      "Evaluating Epoch 17  22.1% | batch:        38 of       172\t|\tloss: 4.00119\n",
      "Evaluating Epoch 17  22.7% | batch:        39 of       172\t|\tloss: 7.43162\n",
      "Evaluating Epoch 17  23.3% | batch:        40 of       172\t|\tloss: 1.30833\n",
      "Evaluating Epoch 17  23.8% | batch:        41 of       172\t|\tloss: 0.706908\n",
      "Evaluating Epoch 17  24.4% | batch:        42 of       172\t|\tloss: 1.36898\n",
      "Evaluating Epoch 17  25.0% | batch:        43 of       172\t|\tloss: 19.7236\n",
      "Evaluating Epoch 17  25.6% | batch:        44 of       172\t|\tloss: 1.70368\n",
      "Evaluating Epoch 17  26.2% | batch:        45 of       172\t|\tloss: 0.62549\n",
      "Evaluating Epoch 17  26.7% | batch:        46 of       172\t|\tloss: 0.147936\n",
      "Evaluating Epoch 17  27.3% | batch:        47 of       172\t|\tloss: 0.359416\n",
      "Evaluating Epoch 17  27.9% | batch:        48 of       172\t|\tloss: 0.491807\n",
      "Evaluating Epoch 17  28.5% | batch:        49 of       172\t|\tloss: 1.76144\n",
      "Evaluating Epoch 17  29.1% | batch:        50 of       172\t|\tloss: 1.12216\n",
      "Evaluating Epoch 17  29.7% | batch:        51 of       172\t|\tloss: 0.545562\n",
      "Evaluating Epoch 17  30.2% | batch:        52 of       172\t|\tloss: 0.294732\n",
      "Evaluating Epoch 17  30.8% | batch:        53 of       172\t|\tloss: 1.06677\n",
      "Evaluating Epoch 17  31.4% | batch:        54 of       172\t|\tloss: 0.513458\n",
      "Evaluating Epoch 17  32.0% | batch:        55 of       172\t|\tloss: 0.419237\n",
      "Evaluating Epoch 17  32.6% | batch:        56 of       172\t|\tloss: 1.13422\n",
      "Evaluating Epoch 17  33.1% | batch:        57 of       172\t|\tloss: 0.752627\n",
      "Evaluating Epoch 17  33.7% | batch:        58 of       172\t|\tloss: 0.591666\n",
      "Evaluating Epoch 17  34.3% | batch:        59 of       172\t|\tloss: 0.467153\n",
      "Evaluating Epoch 17  34.9% | batch:        60 of       172\t|\tloss: 0.532522\n",
      "Evaluating Epoch 17  35.5% | batch:        61 of       172\t|\tloss: 0.82365\n",
      "Evaluating Epoch 17  36.0% | batch:        62 of       172\t|\tloss: 0.317131\n",
      "Evaluating Epoch 17  36.6% | batch:        63 of       172\t|\tloss: 0.943565\n",
      "Evaluating Epoch 17  37.2% | batch:        64 of       172\t|\tloss: 0.422358\n",
      "Evaluating Epoch 17  37.8% | batch:        65 of       172\t|\tloss: 0.77646\n",
      "Evaluating Epoch 17  38.4% | batch:        66 of       172\t|\tloss: 0.738579\n",
      "Evaluating Epoch 17  39.0% | batch:        67 of       172\t|\tloss: 0.394327\n",
      "Evaluating Epoch 17  39.5% | batch:        68 of       172\t|\tloss: 1.4106\n",
      "Evaluating Epoch 17  40.1% | batch:        69 of       172\t|\tloss: 0.703368\n",
      "Evaluating Epoch 17  40.7% | batch:        70 of       172\t|\tloss: 0.388314\n",
      "Evaluating Epoch 17  41.3% | batch:        71 of       172\t|\tloss: 0.569989\n",
      "Evaluating Epoch 17  41.9% | batch:        72 of       172\t|\tloss: 0.694455\n",
      "Evaluating Epoch 17  42.4% | batch:        73 of       172\t|\tloss: 0.770195\n",
      "Evaluating Epoch 17  43.0% | batch:        74 of       172\t|\tloss: 0.122457\n",
      "Evaluating Epoch 17  43.6% | batch:        75 of       172\t|\tloss: 0.144773\n",
      "Evaluating Epoch 17  44.2% | batch:        76 of       172\t|\tloss: 0.200012\n",
      "Evaluating Epoch 17  44.8% | batch:        77 of       172\t|\tloss: 0.119618\n",
      "Evaluating Epoch 17  45.3% | batch:        78 of       172\t|\tloss: 0.209512\n",
      "Evaluating Epoch 17  45.9% | batch:        79 of       172\t|\tloss: 0.269881\n",
      "Evaluating Epoch 17  46.5% | batch:        80 of       172\t|\tloss: 0.153275\n",
      "Evaluating Epoch 17  47.1% | batch:        81 of       172\t|\tloss: 0.216523\n",
      "Evaluating Epoch 17  47.7% | batch:        82 of       172\t|\tloss: 0.183221\n",
      "Evaluating Epoch 17  48.3% | batch:        83 of       172\t|\tloss: 0.433765\n",
      "Evaluating Epoch 17  48.8% | batch:        84 of       172\t|\tloss: 0.570207\n",
      "Evaluating Epoch 17  49.4% | batch:        85 of       172\t|\tloss: 0.398534\n",
      "Evaluating Epoch 17  50.0% | batch:        86 of       172\t|\tloss: 0.499017\n",
      "Evaluating Epoch 17  50.6% | batch:        87 of       172\t|\tloss: 0.562719\n",
      "Evaluating Epoch 17  51.2% | batch:        88 of       172\t|\tloss: 0.569306\n",
      "Evaluating Epoch 17  51.7% | batch:        89 of       172\t|\tloss: 0.45941\n",
      "Evaluating Epoch 17  52.3% | batch:        90 of       172\t|\tloss: 0.598503\n",
      "Evaluating Epoch 17  52.9% | batch:        91 of       172\t|\tloss: 0.77115\n",
      "Evaluating Epoch 17  53.5% | batch:        92 of       172\t|\tloss: 0.647183\n",
      "Evaluating Epoch 17  54.1% | batch:        93 of       172\t|\tloss: 0.52643\n",
      "Evaluating Epoch 17  54.7% | batch:        94 of       172\t|\tloss: 1.02858\n",
      "Evaluating Epoch 17  55.2% | batch:        95 of       172\t|\tloss: 0.447165\n",
      "Evaluating Epoch 17  55.8% | batch:        96 of       172\t|\tloss: 0.425345\n",
      "Evaluating Epoch 17  56.4% | batch:        97 of       172\t|\tloss: 0.613173\n",
      "Evaluating Epoch 17  57.0% | batch:        98 of       172\t|\tloss: 0.944953\n",
      "Evaluating Epoch 17  57.6% | batch:        99 of       172\t|\tloss: 0.569322\n",
      "Evaluating Epoch 17  58.1% | batch:       100 of       172\t|\tloss: 0.549718\n",
      "Evaluating Epoch 17  58.7% | batch:       101 of       172\t|\tloss: 0.549659\n",
      "Evaluating Epoch 17  59.3% | batch:       102 of       172\t|\tloss: 0.748077\n",
      "Evaluating Epoch 17  59.9% | batch:       103 of       172\t|\tloss: 0.526674\n",
      "Evaluating Epoch 17  60.5% | batch:       104 of       172\t|\tloss: 0.641804\n",
      "Evaluating Epoch 17  61.0% | batch:       105 of       172\t|\tloss: 0.86882\n",
      "Evaluating Epoch 17  61.6% | batch:       106 of       172\t|\tloss: 0.721179\n",
      "Evaluating Epoch 17  62.2% | batch:       107 of       172\t|\tloss: 0.603282\n",
      "Evaluating Epoch 17  62.8% | batch:       108 of       172\t|\tloss: 0.555984\n",
      "Evaluating Epoch 17  63.4% | batch:       109 of       172\t|\tloss: 0.767556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 17  64.0% | batch:       110 of       172\t|\tloss: 0.610751\n",
      "Evaluating Epoch 17  64.5% | batch:       111 of       172\t|\tloss: 0.559525\n",
      "Evaluating Epoch 17  65.1% | batch:       112 of       172\t|\tloss: 0.565884\n",
      "Evaluating Epoch 17  65.7% | batch:       113 of       172\t|\tloss: 0.739303\n",
      "Evaluating Epoch 17  66.3% | batch:       114 of       172\t|\tloss: 0.488873\n",
      "Evaluating Epoch 17  66.9% | batch:       115 of       172\t|\tloss: 0.920784\n",
      "Evaluating Epoch 17  67.4% | batch:       116 of       172\t|\tloss: 0.906698\n",
      "Evaluating Epoch 17  68.0% | batch:       117 of       172\t|\tloss: 0.962069\n",
      "Evaluating Epoch 17  68.6% | batch:       118 of       172\t|\tloss: 0.440924\n",
      "Evaluating Epoch 17  69.2% | batch:       119 of       172\t|\tloss: 0.53072\n",
      "Evaluating Epoch 17  69.8% | batch:       120 of       172\t|\tloss: 0.336152\n",
      "Evaluating Epoch 17  70.3% | batch:       121 of       172\t|\tloss: 0.593504\n",
      "Evaluating Epoch 17  70.9% | batch:       122 of       172\t|\tloss: 0.484134\n",
      "Evaluating Epoch 17  71.5% | batch:       123 of       172\t|\tloss: 0.84824\n",
      "Evaluating Epoch 17  72.1% | batch:       124 of       172\t|\tloss: 1.56677\n",
      "Evaluating Epoch 17  72.7% | batch:       125 of       172\t|\tloss: 0.751519\n",
      "Evaluating Epoch 17  73.3% | batch:       126 of       172\t|\tloss: 0.612757\n",
      "Evaluating Epoch 17  73.8% | batch:       127 of       172\t|\tloss: 0.421437\n",
      "Evaluating Epoch 17  74.4% | batch:       128 of       172\t|\tloss: 0.477628\n",
      "Evaluating Epoch 17  75.0% | batch:       129 of       172\t|\tloss: 0.503043\n",
      "Evaluating Epoch 17  75.6% | batch:       130 of       172\t|\tloss: 0.329206\n",
      "Evaluating Epoch 17  76.2% | batch:       131 of       172\t|\tloss: 0.543833\n",
      "Evaluating Epoch 17  76.7% | batch:       132 of       172\t|\tloss: 0.531236\n",
      "Evaluating Epoch 17  77.3% | batch:       133 of       172\t|\tloss: 0.157285\n",
      "Evaluating Epoch 17  77.9% | batch:       134 of       172\t|\tloss: 0.450238\n",
      "Evaluating Epoch 17  78.5% | batch:       135 of       172\t|\tloss: 0.125154\n",
      "Evaluating Epoch 17  79.1% | batch:       136 of       172\t|\tloss: 0.265713\n",
      "Evaluating Epoch 17  79.7% | batch:       137 of       172\t|\tloss: 0.124446\n",
      "Evaluating Epoch 17  80.2% | batch:       138 of       172\t|\tloss: 0.389915\n",
      "Evaluating Epoch 17  80.8% | batch:       139 of       172\t|\tloss: 0.213492\n",
      "Evaluating Epoch 17  81.4% | batch:       140 of       172\t|\tloss: 0.405398\n",
      "Evaluating Epoch 17  82.0% | batch:       141 of       172\t|\tloss: 0.118819\n",
      "Evaluating Epoch 17  82.6% | batch:       142 of       172\t|\tloss: 0.289991\n",
      "Evaluating Epoch 17  83.1% | batch:       143 of       172\t|\tloss: 0.179043\n",
      "Evaluating Epoch 17  83.7% | batch:       144 of       172\t|\tloss: 0.409092\n",
      "Evaluating Epoch 17  84.3% | batch:       145 of       172\t|\tloss: 0.146438\n",
      "Evaluating Epoch 17  84.9% | batch:       146 of       172\t|\tloss: 0.339964\n",
      "Evaluating Epoch 17  85.5% | batch:       147 of       172\t|\tloss: 0.143281\n",
      "Evaluating Epoch 17  86.0% | batch:       148 of       172\t|\tloss: 0.316689\n",
      "Evaluating Epoch 17  86.6% | batch:       149 of       172\t|\tloss: 0.185042\n",
      "Evaluating Epoch 17  87.2% | batch:       150 of       172\t|\tloss: 0.544045\n",
      "Evaluating Epoch 17  87.8% | batch:       151 of       172\t|\tloss: 1.10717\n",
      "Evaluating Epoch 17  88.4% | batch:       152 of       172\t|\tloss: 0.517347\n",
      "Evaluating Epoch 17  89.0% | batch:       153 of       172\t|\tloss: 0.665748\n",
      "Evaluating Epoch 17  89.5% | batch:       154 of       172\t|\tloss: 1.11721\n",
      "Evaluating Epoch 17  90.1% | batch:       155 of       172\t|\tloss: 0.481104\n",
      "Evaluating Epoch 17  90.7% | batch:       156 of       172\t|\tloss: 1.15129\n",
      "Evaluating Epoch 17  91.3% | batch:       157 of       172\t|\tloss: 1.26016\n",
      "Evaluating Epoch 17  91.9% | batch:       158 of       172\t|\tloss: 0.580073\n",
      "Evaluating Epoch 17  92.4% | batch:       159 of       172\t|\tloss: 1.73451\n",
      "Evaluating Epoch 17  93.0% | batch:       160 of       172\t|\tloss: 0.847166\n",
      "Evaluating Epoch 17  93.6% | batch:       161 of       172\t|\tloss: 2.01199\n",
      "Evaluating Epoch 17  94.2% | batch:       162 of       172\t|\tloss: 1.18064\n",
      "Evaluating Epoch 17  94.8% | batch:       163 of       172\t|\tloss: 0.687968\n",
      "Evaluating Epoch 17  95.3% | batch:       164 of       172\t|\tloss: 1.0215\n",
      "Evaluating Epoch 17  95.9% | batch:       165 of       172\t|\tloss: 0.841139\n",
      "Evaluating Epoch 17  96.5% | batch:       166 of       172\t|\tloss: 0.385357\n",
      "Evaluating Epoch 17  97.1% | batch:       167 of       172\t|\tloss: 1.41127\n",
      "Evaluating Epoch 17  97.7% | batch:       168 of       172\t|\tloss: 0.812335\n",
      "Evaluating Epoch 17  98.3% | batch:       169 of       172\t|\tloss: 0.681787\n",
      "Evaluating Epoch 17  98.8% | batch:       170 of       172\t|\tloss: 1.42302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:07:46,554 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 4.085539102554321 seconds\n",
      "\n",
      "2023-05-25 22:07:46,554 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 3.99271559715271 seconds\n",
      "2023-05-25 22:07:46,555 | INFO : Avg batch val. time: 0.023213462774143663 seconds\n",
      "2023-05-25 22:07:46,555 | INFO : Avg sample val. time: 0.00018184249201405974 seconds\n",
      "2023-05-25 22:07:46,556 | INFO : Epoch 17 Validation Summary: epoch: 17.000000 | loss: 1.333371 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 17  99.4% | batch:       171 of       172\t|\tloss: 0.975179\n",
      "\n",
      "Training Epoch 18   0.0% | batch:         0 of       686\t|\tloss: 2.194\n",
      "Training Epoch 18   0.1% | batch:         1 of       686\t|\tloss: 2.56567\n",
      "Training Epoch 18   0.3% | batch:         2 of       686\t|\tloss: 2.529\n",
      "Training Epoch 18   0.4% | batch:         3 of       686\t|\tloss: 2.7198\n",
      "Training Epoch 18   0.6% | batch:         4 of       686\t|\tloss: 2.22746\n",
      "Training Epoch 18   0.7% | batch:         5 of       686\t|\tloss: 2.72788\n",
      "Training Epoch 18   0.9% | batch:         6 of       686\t|\tloss: 2.33477\n",
      "Training Epoch 18   1.0% | batch:         7 of       686\t|\tloss: 3.13594\n",
      "Training Epoch 18   1.2% | batch:         8 of       686\t|\tloss: 2.06622\n",
      "Training Epoch 18   1.3% | batch:         9 of       686\t|\tloss: 2.44743\n",
      "Training Epoch 18   1.5% | batch:        10 of       686\t|\tloss: 2.35578\n",
      "Training Epoch 18   1.6% | batch:        11 of       686\t|\tloss: 2.26913\n",
      "Training Epoch 18   1.7% | batch:        12 of       686\t|\tloss: 1.98207\n",
      "Training Epoch 18   1.9% | batch:        13 of       686\t|\tloss: 2.46661\n",
      "Training Epoch 18   2.0% | batch:        14 of       686\t|\tloss: 2.95868\n",
      "Training Epoch 18   2.2% | batch:        15 of       686\t|\tloss: 2.21717\n",
      "Training Epoch 18   2.3% | batch:        16 of       686\t|\tloss: 1.87994\n",
      "Training Epoch 18   2.5% | batch:        17 of       686\t|\tloss: 3.17414\n",
      "Training Epoch 18   2.6% | batch:        18 of       686\t|\tloss: 2.77022\n",
      "Training Epoch 18   2.8% | batch:        19 of       686\t|\tloss: 2.27653\n",
      "Training Epoch 18   2.9% | batch:        20 of       686\t|\tloss: 2.74452\n",
      "Training Epoch 18   3.1% | batch:        21 of       686\t|\tloss: 2.32374\n",
      "Training Epoch 18   3.2% | batch:        22 of       686\t|\tloss: 2.84998\n",
      "Training Epoch 18   3.4% | batch:        23 of       686\t|\tloss: 2.38172\n",
      "Training Epoch 18   3.5% | batch:        24 of       686\t|\tloss: 2.58023\n",
      "Training Epoch 18   3.6% | batch:        25 of       686\t|\tloss: 2.29793\n",
      "Training Epoch 18   3.8% | batch:        26 of       686\t|\tloss: 2.15351\n",
      "Training Epoch 18   3.9% | batch:        27 of       686\t|\tloss: 3.54859\n",
      "Training Epoch 18   4.1% | batch:        28 of       686\t|\tloss: 2.1076\n",
      "Training Epoch 18   4.2% | batch:        29 of       686\t|\tloss: 1.90937\n",
      "Training Epoch 18   4.4% | batch:        30 of       686\t|\tloss: 2.15297\n",
      "Training Epoch 18   4.5% | batch:        31 of       686\t|\tloss: 2.34245\n",
      "Training Epoch 18   4.7% | batch:        32 of       686\t|\tloss: 1.51405\n",
      "Training Epoch 18   4.8% | batch:        33 of       686\t|\tloss: 2.37915\n",
      "Training Epoch 18   5.0% | batch:        34 of       686\t|\tloss: 2.80832\n",
      "Training Epoch 18   5.1% | batch:        35 of       686\t|\tloss: 2.5549\n",
      "Training Epoch 18   5.2% | batch:        36 of       686\t|\tloss: 2.21725\n",
      "Training Epoch 18   5.4% | batch:        37 of       686\t|\tloss: 1.9603\n",
      "Training Epoch 18   5.5% | batch:        38 of       686\t|\tloss: 2.25171\n",
      "Training Epoch 18   5.7% | batch:        39 of       686\t|\tloss: 2.5306\n",
      "Training Epoch 18   5.8% | batch:        40 of       686\t|\tloss: 2.00999\n",
      "Training Epoch 18   6.0% | batch:        41 of       686\t|\tloss: 1.97457\n",
      "Training Epoch 18   6.1% | batch:        42 of       686\t|\tloss: 3.42547\n",
      "Training Epoch 18   6.3% | batch:        43 of       686\t|\tloss: 2.28145\n",
      "Training Epoch 18   6.4% | batch:        44 of       686\t|\tloss: 2.43889\n",
      "Training Epoch 18   6.6% | batch:        45 of       686\t|\tloss: 2.29025\n",
      "Training Epoch 18   6.7% | batch:        46 of       686\t|\tloss: 2.34447\n",
      "Training Epoch 18   6.9% | batch:        47 of       686\t|\tloss: 1.86517\n",
      "Training Epoch 18   7.0% | batch:        48 of       686\t|\tloss: 2.22669\n",
      "Training Epoch 18   7.1% | batch:        49 of       686\t|\tloss: 2.20885\n",
      "Training Epoch 18   7.3% | batch:        50 of       686\t|\tloss: 2.06645\n",
      "Training Epoch 18   7.4% | batch:        51 of       686\t|\tloss: 2.46018\n",
      "Training Epoch 18   7.6% | batch:        52 of       686\t|\tloss: 1.7146\n",
      "Training Epoch 18   7.7% | batch:        53 of       686\t|\tloss: 2.49031\n",
      "Training Epoch 18   7.9% | batch:        54 of       686\t|\tloss: 2.05073\n",
      "Training Epoch 18   8.0% | batch:        55 of       686\t|\tloss: 2.21269\n",
      "Training Epoch 18   8.2% | batch:        56 of       686\t|\tloss: 2.54253\n",
      "Training Epoch 18   8.3% | batch:        57 of       686\t|\tloss: 2.14385\n",
      "Training Epoch 18   8.5% | batch:        58 of       686\t|\tloss: 2.48139\n",
      "Training Epoch 18   8.6% | batch:        59 of       686\t|\tloss: 2.28003\n",
      "Training Epoch 18   8.7% | batch:        60 of       686\t|\tloss: 2.61325\n",
      "Training Epoch 18   8.9% | batch:        61 of       686\t|\tloss: 2.39736\n",
      "Training Epoch 18   9.0% | batch:        62 of       686\t|\tloss: 2.49847\n",
      "Training Epoch 18   9.2% | batch:        63 of       686\t|\tloss: 1.94563\n",
      "Training Epoch 18   9.3% | batch:        64 of       686\t|\tloss: 2.4496\n",
      "Training Epoch 18   9.5% | batch:        65 of       686\t|\tloss: 2.24211\n",
      "Training Epoch 18   9.6% | batch:        66 of       686\t|\tloss: 2.30606\n",
      "Training Epoch 18   9.8% | batch:        67 of       686\t|\tloss: 2.44783\n",
      "Training Epoch 18   9.9% | batch:        68 of       686\t|\tloss: 1.94962\n",
      "Training Epoch 18  10.1% | batch:        69 of       686\t|\tloss: 2.38008\n",
      "Training Epoch 18  10.2% | batch:        70 of       686\t|\tloss: 2.13923\n",
      "Training Epoch 18  10.3% | batch:        71 of       686\t|\tloss: 2.5378\n",
      "Training Epoch 18  10.5% | batch:        72 of       686\t|\tloss: 2.78868\n",
      "Training Epoch 18  10.6% | batch:        73 of       686\t|\tloss: 2.97469\n",
      "Training Epoch 18  10.8% | batch:        74 of       686\t|\tloss: 2.44147\n",
      "Training Epoch 18  10.9% | batch:        75 of       686\t|\tloss: 2.19787\n",
      "Training Epoch 18  11.1% | batch:        76 of       686\t|\tloss: 2.57152\n",
      "Training Epoch 18  11.2% | batch:        77 of       686\t|\tloss: 2.11321\n",
      "Training Epoch 18  11.4% | batch:        78 of       686\t|\tloss: 2.53326\n",
      "Training Epoch 18  11.5% | batch:        79 of       686\t|\tloss: 2.44567\n",
      "Training Epoch 18  11.7% | batch:        80 of       686\t|\tloss: 2.82478\n",
      "Training Epoch 18  11.8% | batch:        81 of       686\t|\tloss: 2.23788\n",
      "Training Epoch 18  12.0% | batch:        82 of       686\t|\tloss: 2.04663\n",
      "Training Epoch 18  12.1% | batch:        83 of       686\t|\tloss: 3.78674\n",
      "Training Epoch 18  12.2% | batch:        84 of       686\t|\tloss: 1.98422\n",
      "Training Epoch 18  12.4% | batch:        85 of       686\t|\tloss: 2.29401\n",
      "Training Epoch 18  12.5% | batch:        86 of       686\t|\tloss: 2.44971\n",
      "Training Epoch 18  12.7% | batch:        87 of       686\t|\tloss: 2.96257\n",
      "Training Epoch 18  12.8% | batch:        88 of       686\t|\tloss: 2.78705\n",
      "Training Epoch 18  13.0% | batch:        89 of       686\t|\tloss: 3.15079\n",
      "Training Epoch 18  13.1% | batch:        90 of       686\t|\tloss: 3.27261\n",
      "Training Epoch 18  13.3% | batch:        91 of       686\t|\tloss: 2.3216\n",
      "Training Epoch 18  13.4% | batch:        92 of       686\t|\tloss: 2.53858\n",
      "Training Epoch 18  13.6% | batch:        93 of       686\t|\tloss: 2.54471\n",
      "Training Epoch 18  13.7% | batch:        94 of       686\t|\tloss: 1.7402\n",
      "Training Epoch 18  13.8% | batch:        95 of       686\t|\tloss: 2.09103\n",
      "Training Epoch 18  14.0% | batch:        96 of       686\t|\tloss: 2.02017\n",
      "Training Epoch 18  14.1% | batch:        97 of       686\t|\tloss: 2.10007\n",
      "Training Epoch 18  14.3% | batch:        98 of       686\t|\tloss: 2.66656\n",
      "Training Epoch 18  14.4% | batch:        99 of       686\t|\tloss: 2.58693\n",
      "Training Epoch 18  14.6% | batch:       100 of       686\t|\tloss: 2.29684\n",
      "Training Epoch 18  14.7% | batch:       101 of       686\t|\tloss: 2.55008\n",
      "Training Epoch 18  14.9% | batch:       102 of       686\t|\tloss: 2.8466\n",
      "Training Epoch 18  15.0% | batch:       103 of       686\t|\tloss: 2.07615\n",
      "Training Epoch 18  15.2% | batch:       104 of       686\t|\tloss: 2.54781\n",
      "Training Epoch 18  15.3% | batch:       105 of       686\t|\tloss: 1.78191\n",
      "Training Epoch 18  15.5% | batch:       106 of       686\t|\tloss: 2.20863\n",
      "Training Epoch 18  15.6% | batch:       107 of       686\t|\tloss: 2.04631\n",
      "Training Epoch 18  15.7% | batch:       108 of       686\t|\tloss: 2.26426\n",
      "Training Epoch 18  15.9% | batch:       109 of       686\t|\tloss: 2.67867\n",
      "Training Epoch 18  16.0% | batch:       110 of       686\t|\tloss: 2.83361\n",
      "Training Epoch 18  16.2% | batch:       111 of       686\t|\tloss: 2.02043\n",
      "Training Epoch 18  16.3% | batch:       112 of       686\t|\tloss: 1.91327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 18  16.5% | batch:       113 of       686\t|\tloss: 2.42192\n",
      "Training Epoch 18  16.6% | batch:       114 of       686\t|\tloss: 1.91889\n",
      "Training Epoch 18  16.8% | batch:       115 of       686\t|\tloss: 2.35368\n",
      "Training Epoch 18  16.9% | batch:       116 of       686\t|\tloss: 2.71501\n",
      "Training Epoch 18  17.1% | batch:       117 of       686\t|\tloss: 2.54099\n",
      "Training Epoch 18  17.2% | batch:       118 of       686\t|\tloss: 2.07655\n",
      "Training Epoch 18  17.3% | batch:       119 of       686\t|\tloss: 2.4064\n",
      "Training Epoch 18  17.5% | batch:       120 of       686\t|\tloss: 2.7043\n",
      "Training Epoch 18  17.6% | batch:       121 of       686\t|\tloss: 2.11863\n",
      "Training Epoch 18  17.8% | batch:       122 of       686\t|\tloss: 2.58734\n",
      "Training Epoch 18  17.9% | batch:       123 of       686\t|\tloss: 3.03357\n",
      "Training Epoch 18  18.1% | batch:       124 of       686\t|\tloss: 2.26532\n",
      "Training Epoch 18  18.2% | batch:       125 of       686\t|\tloss: 2.29872\n",
      "Training Epoch 18  18.4% | batch:       126 of       686\t|\tloss: 2.7124\n",
      "Training Epoch 18  18.5% | batch:       127 of       686\t|\tloss: 2.23356\n",
      "Training Epoch 18  18.7% | batch:       128 of       686\t|\tloss: 2.68532\n",
      "Training Epoch 18  18.8% | batch:       129 of       686\t|\tloss: 2.38993\n",
      "Training Epoch 18  19.0% | batch:       130 of       686\t|\tloss: 2.25305\n",
      "Training Epoch 18  19.1% | batch:       131 of       686\t|\tloss: 1.80102\n",
      "Training Epoch 18  19.2% | batch:       132 of       686\t|\tloss: 2.59769\n",
      "Training Epoch 18  19.4% | batch:       133 of       686\t|\tloss: 2.33429\n",
      "Training Epoch 18  19.5% | batch:       134 of       686\t|\tloss: 2.74935\n",
      "Training Epoch 18  19.7% | batch:       135 of       686\t|\tloss: 1.66582\n",
      "Training Epoch 18  19.8% | batch:       136 of       686\t|\tloss: 2.65686\n",
      "Training Epoch 18  20.0% | batch:       137 of       686\t|\tloss: 2.78539\n",
      "Training Epoch 18  20.1% | batch:       138 of       686\t|\tloss: 2.00117\n",
      "Training Epoch 18  20.3% | batch:       139 of       686\t|\tloss: 2.15129\n",
      "Training Epoch 18  20.4% | batch:       140 of       686\t|\tloss: 1.83797\n",
      "Training Epoch 18  20.6% | batch:       141 of       686\t|\tloss: 2.31618\n",
      "Training Epoch 18  20.7% | batch:       142 of       686\t|\tloss: 1.53538\n",
      "Training Epoch 18  20.8% | batch:       143 of       686\t|\tloss: 2.2791\n",
      "Training Epoch 18  21.0% | batch:       144 of       686\t|\tloss: 2.13513\n",
      "Training Epoch 18  21.1% | batch:       145 of       686\t|\tloss: 2.52082\n",
      "Training Epoch 18  21.3% | batch:       146 of       686\t|\tloss: 2.12139\n",
      "Training Epoch 18  21.4% | batch:       147 of       686\t|\tloss: 1.89457\n",
      "Training Epoch 18  21.6% | batch:       148 of       686\t|\tloss: 2.24115\n",
      "Training Epoch 18  21.7% | batch:       149 of       686\t|\tloss: 2.25605\n",
      "Training Epoch 18  21.9% | batch:       150 of       686\t|\tloss: 2.17173\n",
      "Training Epoch 18  22.0% | batch:       151 of       686\t|\tloss: 2.95363\n",
      "Training Epoch 18  22.2% | batch:       152 of       686\t|\tloss: 2.87188\n",
      "Training Epoch 18  22.3% | batch:       153 of       686\t|\tloss: 2.4436\n",
      "Training Epoch 18  22.4% | batch:       154 of       686\t|\tloss: 2.27297\n",
      "Training Epoch 18  22.6% | batch:       155 of       686\t|\tloss: 2.91407\n",
      "Training Epoch 18  22.7% | batch:       156 of       686\t|\tloss: 3.3328\n",
      "Training Epoch 18  22.9% | batch:       157 of       686\t|\tloss: 2.67352\n",
      "Training Epoch 18  23.0% | batch:       158 of       686\t|\tloss: 2.0058\n",
      "Training Epoch 18  23.2% | batch:       159 of       686\t|\tloss: 2.02613\n",
      "Training Epoch 18  23.3% | batch:       160 of       686\t|\tloss: 2.81353\n",
      "Training Epoch 18  23.5% | batch:       161 of       686\t|\tloss: 2.67714\n",
      "Training Epoch 18  23.6% | batch:       162 of       686\t|\tloss: 2.47988\n",
      "Training Epoch 18  23.8% | batch:       163 of       686\t|\tloss: 2.25146\n",
      "Training Epoch 18  23.9% | batch:       164 of       686\t|\tloss: 2.96973\n",
      "Training Epoch 18  24.1% | batch:       165 of       686\t|\tloss: 1.99678\n",
      "Training Epoch 18  24.2% | batch:       166 of       686\t|\tloss: 2.64596\n",
      "Training Epoch 18  24.3% | batch:       167 of       686\t|\tloss: 1.90369\n",
      "Training Epoch 18  24.5% | batch:       168 of       686\t|\tloss: 2.01625\n",
      "Training Epoch 18  24.6% | batch:       169 of       686\t|\tloss: 2.42342\n",
      "Training Epoch 18  24.8% | batch:       170 of       686\t|\tloss: 2.95998\n",
      "Training Epoch 18  24.9% | batch:       171 of       686\t|\tloss: 2.38064\n",
      "Training Epoch 18  25.1% | batch:       172 of       686\t|\tloss: 2.68274\n",
      "Training Epoch 18  25.2% | batch:       173 of       686\t|\tloss: 1.67882\n",
      "Training Epoch 18  25.4% | batch:       174 of       686\t|\tloss: 2.64141\n",
      "Training Epoch 18  25.5% | batch:       175 of       686\t|\tloss: 2.20041\n",
      "Training Epoch 18  25.7% | batch:       176 of       686\t|\tloss: 3.49986\n",
      "Training Epoch 18  25.8% | batch:       177 of       686\t|\tloss: 3.27189\n",
      "Training Epoch 18  25.9% | batch:       178 of       686\t|\tloss: 3.45903\n",
      "Training Epoch 18  26.1% | batch:       179 of       686\t|\tloss: 2.76053\n",
      "Training Epoch 18  26.2% | batch:       180 of       686\t|\tloss: 2.75696\n",
      "Training Epoch 18  26.4% | batch:       181 of       686\t|\tloss: 2.01475\n",
      "Training Epoch 18  26.5% | batch:       182 of       686\t|\tloss: 2.45931\n",
      "Training Epoch 18  26.7% | batch:       183 of       686\t|\tloss: 2.17808\n",
      "Training Epoch 18  26.8% | batch:       184 of       686\t|\tloss: 2.6795\n",
      "Training Epoch 18  27.0% | batch:       185 of       686\t|\tloss: 2.28845\n",
      "Training Epoch 18  27.1% | batch:       186 of       686\t|\tloss: 2.35244\n",
      "Training Epoch 18  27.3% | batch:       187 of       686\t|\tloss: 2.44868\n",
      "Training Epoch 18  27.4% | batch:       188 of       686\t|\tloss: 1.90764\n",
      "Training Epoch 18  27.6% | batch:       189 of       686\t|\tloss: 2.28565\n",
      "Training Epoch 18  27.7% | batch:       190 of       686\t|\tloss: 2.39818\n",
      "Training Epoch 18  27.8% | batch:       191 of       686\t|\tloss: 2.32273\n",
      "Training Epoch 18  28.0% | batch:       192 of       686\t|\tloss: 2.75028\n",
      "Training Epoch 18  28.1% | batch:       193 of       686\t|\tloss: 2.65524\n",
      "Training Epoch 18  28.3% | batch:       194 of       686\t|\tloss: 2.45754\n",
      "Training Epoch 18  28.4% | batch:       195 of       686\t|\tloss: 2.36534\n",
      "Training Epoch 18  28.6% | batch:       196 of       686\t|\tloss: 2.88944\n",
      "Training Epoch 18  28.7% | batch:       197 of       686\t|\tloss: 2.47108\n",
      "Training Epoch 18  28.9% | batch:       198 of       686\t|\tloss: 2.44535\n",
      "Training Epoch 18  29.0% | batch:       199 of       686\t|\tloss: 2.47228\n",
      "Training Epoch 18  29.2% | batch:       200 of       686\t|\tloss: 2.05548\n",
      "Training Epoch 18  29.3% | batch:       201 of       686\t|\tloss: 3.30757\n",
      "Training Epoch 18  29.4% | batch:       202 of       686\t|\tloss: 2.24321\n",
      "Training Epoch 18  29.6% | batch:       203 of       686\t|\tloss: 2.69553\n",
      "Training Epoch 18  29.7% | batch:       204 of       686\t|\tloss: 2.16964\n",
      "Training Epoch 18  29.9% | batch:       205 of       686\t|\tloss: 2.13515\n",
      "Training Epoch 18  30.0% | batch:       206 of       686\t|\tloss: 2.17413\n",
      "Training Epoch 18  30.2% | batch:       207 of       686\t|\tloss: 2.35459\n",
      "Training Epoch 18  30.3% | batch:       208 of       686\t|\tloss: 2.0476\n",
      "Training Epoch 18  30.5% | batch:       209 of       686\t|\tloss: 2.3089\n",
      "Training Epoch 18  30.6% | batch:       210 of       686\t|\tloss: 2.69173\n",
      "Training Epoch 18  30.8% | batch:       211 of       686\t|\tloss: 2.12276\n",
      "Training Epoch 18  30.9% | batch:       212 of       686\t|\tloss: 2.37129\n",
      "Training Epoch 18  31.0% | batch:       213 of       686\t|\tloss: 2.21348\n",
      "Training Epoch 18  31.2% | batch:       214 of       686\t|\tloss: 2.06875\n",
      "Training Epoch 18  31.3% | batch:       215 of       686\t|\tloss: 1.74725\n",
      "Training Epoch 18  31.5% | batch:       216 of       686\t|\tloss: 2.11396\n",
      "Training Epoch 18  31.6% | batch:       217 of       686\t|\tloss: 2.25123\n",
      "Training Epoch 18  31.8% | batch:       218 of       686\t|\tloss: 2.32475\n",
      "Training Epoch 18  31.9% | batch:       219 of       686\t|\tloss: 2.56551\n",
      "Training Epoch 18  32.1% | batch:       220 of       686\t|\tloss: 2.08956\n",
      "Training Epoch 18  32.2% | batch:       221 of       686\t|\tloss: 2.71828\n",
      "Training Epoch 18  32.4% | batch:       222 of       686\t|\tloss: 2.53948\n",
      "Training Epoch 18  32.5% | batch:       223 of       686\t|\tloss: 2.08062\n",
      "Training Epoch 18  32.7% | batch:       224 of       686\t|\tloss: 2.79801\n",
      "Training Epoch 18  32.8% | batch:       225 of       686\t|\tloss: 2.22153\n",
      "Training Epoch 18  32.9% | batch:       226 of       686\t|\tloss: 1.94451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 18  33.1% | batch:       227 of       686\t|\tloss: 1.88588\n",
      "Training Epoch 18  33.2% | batch:       228 of       686\t|\tloss: 2.32687\n",
      "Training Epoch 18  33.4% | batch:       229 of       686\t|\tloss: 2.32584\n",
      "Training Epoch 18  33.5% | batch:       230 of       686\t|\tloss: 2.15978\n",
      "Training Epoch 18  33.7% | batch:       231 of       686\t|\tloss: 2.81514\n",
      "Training Epoch 18  33.8% | batch:       232 of       686\t|\tloss: 2.45798\n",
      "Training Epoch 18  34.0% | batch:       233 of       686\t|\tloss: 2.20534\n",
      "Training Epoch 18  34.1% | batch:       234 of       686\t|\tloss: 2.51156\n",
      "Training Epoch 18  34.3% | batch:       235 of       686\t|\tloss: 1.96356\n",
      "Training Epoch 18  34.4% | batch:       236 of       686\t|\tloss: 2.43186\n",
      "Training Epoch 18  34.5% | batch:       237 of       686\t|\tloss: 2.80288\n",
      "Training Epoch 18  34.7% | batch:       238 of       686\t|\tloss: 2.12649\n",
      "Training Epoch 18  34.8% | batch:       239 of       686\t|\tloss: 2.64347\n",
      "Training Epoch 18  35.0% | batch:       240 of       686\t|\tloss: 2.38776\n",
      "Training Epoch 18  35.1% | batch:       241 of       686\t|\tloss: 2.5455\n",
      "Training Epoch 18  35.3% | batch:       242 of       686\t|\tloss: 3.72224\n",
      "Training Epoch 18  35.4% | batch:       243 of       686\t|\tloss: 3.12966\n",
      "Training Epoch 18  35.6% | batch:       244 of       686\t|\tloss: 2.20418\n",
      "Training Epoch 18  35.7% | batch:       245 of       686\t|\tloss: 2.29864\n",
      "Training Epoch 18  35.9% | batch:       246 of       686\t|\tloss: 2.07188\n",
      "Training Epoch 18  36.0% | batch:       247 of       686\t|\tloss: 2.39557\n",
      "Training Epoch 18  36.2% | batch:       248 of       686\t|\tloss: 2.58429\n",
      "Training Epoch 18  36.3% | batch:       249 of       686\t|\tloss: 2.86825\n",
      "Training Epoch 18  36.4% | batch:       250 of       686\t|\tloss: 3.5336\n",
      "Training Epoch 18  36.6% | batch:       251 of       686\t|\tloss: 1.88487\n",
      "Training Epoch 18  36.7% | batch:       252 of       686\t|\tloss: 2.25827\n",
      "Training Epoch 18  36.9% | batch:       253 of       686\t|\tloss: 1.95475\n",
      "Training Epoch 18  37.0% | batch:       254 of       686\t|\tloss: 2.12762\n",
      "Training Epoch 18  37.2% | batch:       255 of       686\t|\tloss: 2.63599\n",
      "Training Epoch 18  37.3% | batch:       256 of       686\t|\tloss: 1.97568\n",
      "Training Epoch 18  37.5% | batch:       257 of       686\t|\tloss: 2.20228\n",
      "Training Epoch 18  37.6% | batch:       258 of       686\t|\tloss: 2.19888\n",
      "Training Epoch 18  37.8% | batch:       259 of       686\t|\tloss: 2.37441\n",
      "Training Epoch 18  37.9% | batch:       260 of       686\t|\tloss: 2.42384\n",
      "Training Epoch 18  38.0% | batch:       261 of       686\t|\tloss: 2.34272\n",
      "Training Epoch 18  38.2% | batch:       262 of       686\t|\tloss: 1.80489\n",
      "Training Epoch 18  38.3% | batch:       263 of       686\t|\tloss: 1.84258\n",
      "Training Epoch 18  38.5% | batch:       264 of       686\t|\tloss: 1.6494\n",
      "Training Epoch 18  38.6% | batch:       265 of       686\t|\tloss: 1.94358\n",
      "Training Epoch 18  38.8% | batch:       266 of       686\t|\tloss: 2.33258\n",
      "Training Epoch 18  38.9% | batch:       267 of       686\t|\tloss: 3.04587\n",
      "Training Epoch 18  39.1% | batch:       268 of       686\t|\tloss: 3.05071\n",
      "Training Epoch 18  39.2% | batch:       269 of       686\t|\tloss: 2.60147\n",
      "Training Epoch 18  39.4% | batch:       270 of       686\t|\tloss: 2.12457\n",
      "Training Epoch 18  39.5% | batch:       271 of       686\t|\tloss: 2.13442\n",
      "Training Epoch 18  39.7% | batch:       272 of       686\t|\tloss: 1.82416\n",
      "Training Epoch 18  39.8% | batch:       273 of       686\t|\tloss: 2.79359\n",
      "Training Epoch 18  39.9% | batch:       274 of       686\t|\tloss: 1.88659\n",
      "Training Epoch 18  40.1% | batch:       275 of       686\t|\tloss: 2.3952\n",
      "Training Epoch 18  40.2% | batch:       276 of       686\t|\tloss: 2.20965\n",
      "Training Epoch 18  40.4% | batch:       277 of       686\t|\tloss: 2.63554\n",
      "Training Epoch 18  40.5% | batch:       278 of       686\t|\tloss: 2.41709\n",
      "Training Epoch 18  40.7% | batch:       279 of       686\t|\tloss: 2.06724\n",
      "Training Epoch 18  40.8% | batch:       280 of       686\t|\tloss: 2.44156\n",
      "Training Epoch 18  41.0% | batch:       281 of       686\t|\tloss: 2.71978\n",
      "Training Epoch 18  41.1% | batch:       282 of       686\t|\tloss: 3.11996\n",
      "Training Epoch 18  41.3% | batch:       283 of       686\t|\tloss: 1.78719\n",
      "Training Epoch 18  41.4% | batch:       284 of       686\t|\tloss: 2.03365\n",
      "Training Epoch 18  41.5% | batch:       285 of       686\t|\tloss: 2.20593\n",
      "Training Epoch 18  41.7% | batch:       286 of       686\t|\tloss: 1.96561\n",
      "Training Epoch 18  41.8% | batch:       287 of       686\t|\tloss: 2.41815\n",
      "Training Epoch 18  42.0% | batch:       288 of       686\t|\tloss: 1.93579\n",
      "Training Epoch 18  42.1% | batch:       289 of       686\t|\tloss: 3.69542\n",
      "Training Epoch 18  42.3% | batch:       290 of       686\t|\tloss: 2.30828\n",
      "Training Epoch 18  42.4% | batch:       291 of       686\t|\tloss: 2.34177\n",
      "Training Epoch 18  42.6% | batch:       292 of       686\t|\tloss: 2.11801\n",
      "Training Epoch 18  42.7% | batch:       293 of       686\t|\tloss: 2.48309\n",
      "Training Epoch 18  42.9% | batch:       294 of       686\t|\tloss: 3.09437\n",
      "Training Epoch 18  43.0% | batch:       295 of       686\t|\tloss: 2.41245\n",
      "Training Epoch 18  43.1% | batch:       296 of       686\t|\tloss: 2.17666\n",
      "Training Epoch 18  43.3% | batch:       297 of       686\t|\tloss: 2.11797\n",
      "Training Epoch 18  43.4% | batch:       298 of       686\t|\tloss: 1.76747\n",
      "Training Epoch 18  43.6% | batch:       299 of       686\t|\tloss: 2.13747\n",
      "Training Epoch 18  43.7% | batch:       300 of       686\t|\tloss: 2.29168\n",
      "Training Epoch 18  43.9% | batch:       301 of       686\t|\tloss: 1.76633\n",
      "Training Epoch 18  44.0% | batch:       302 of       686\t|\tloss: 2.87351\n",
      "Training Epoch 18  44.2% | batch:       303 of       686\t|\tloss: 1.93582\n",
      "Training Epoch 18  44.3% | batch:       304 of       686\t|\tloss: 1.66482\n",
      "Training Epoch 18  44.5% | batch:       305 of       686\t|\tloss: 2.20355\n",
      "Training Epoch 18  44.6% | batch:       306 of       686\t|\tloss: 1.87547\n",
      "Training Epoch 18  44.8% | batch:       307 of       686\t|\tloss: 2.14797\n",
      "Training Epoch 18  44.9% | batch:       308 of       686\t|\tloss: 2.43395\n",
      "Training Epoch 18  45.0% | batch:       309 of       686\t|\tloss: 2.683\n",
      "Training Epoch 18  45.2% | batch:       310 of       686\t|\tloss: 2.35095\n",
      "Training Epoch 18  45.3% | batch:       311 of       686\t|\tloss: 2.24522\n",
      "Training Epoch 18  45.5% | batch:       312 of       686\t|\tloss: 2.31851\n",
      "Training Epoch 18  45.6% | batch:       313 of       686\t|\tloss: 2.11596\n",
      "Training Epoch 18  45.8% | batch:       314 of       686\t|\tloss: 2.70371\n",
      "Training Epoch 18  45.9% | batch:       315 of       686\t|\tloss: 2.30768\n",
      "Training Epoch 18  46.1% | batch:       316 of       686\t|\tloss: 2.44582\n",
      "Training Epoch 18  46.2% | batch:       317 of       686\t|\tloss: 2.28486\n",
      "Training Epoch 18  46.4% | batch:       318 of       686\t|\tloss: 1.76738\n",
      "Training Epoch 18  46.5% | batch:       319 of       686\t|\tloss: 2.13519\n",
      "Training Epoch 18  46.6% | batch:       320 of       686\t|\tloss: 2.31483\n",
      "Training Epoch 18  46.8% | batch:       321 of       686\t|\tloss: 2.23409\n",
      "Training Epoch 18  46.9% | batch:       322 of       686\t|\tloss: 2.83642\n",
      "Training Epoch 18  47.1% | batch:       323 of       686\t|\tloss: 2.91591\n",
      "Training Epoch 18  47.2% | batch:       324 of       686\t|\tloss: 1.98699\n",
      "Training Epoch 18  47.4% | batch:       325 of       686\t|\tloss: 2.17168\n",
      "Training Epoch 18  47.5% | batch:       326 of       686\t|\tloss: 2.96609\n",
      "Training Epoch 18  47.7% | batch:       327 of       686\t|\tloss: 2.64021\n",
      "Training Epoch 18  47.8% | batch:       328 of       686\t|\tloss: 2.32899\n",
      "Training Epoch 18  48.0% | batch:       329 of       686\t|\tloss: 2.17527\n",
      "Training Epoch 18  48.1% | batch:       330 of       686\t|\tloss: 2.98886\n",
      "Training Epoch 18  48.3% | batch:       331 of       686\t|\tloss: 2.42067\n",
      "Training Epoch 18  48.4% | batch:       332 of       686\t|\tloss: 1.8934\n",
      "Training Epoch 18  48.5% | batch:       333 of       686\t|\tloss: 2.93865\n",
      "Training Epoch 18  48.7% | batch:       334 of       686\t|\tloss: 2.44614\n",
      "Training Epoch 18  48.8% | batch:       335 of       686\t|\tloss: 1.93019\n",
      "Training Epoch 18  49.0% | batch:       336 of       686\t|\tloss: 2.65694\n",
      "Training Epoch 18  49.1% | batch:       337 of       686\t|\tloss: 2.03877\n",
      "Training Epoch 18  49.3% | batch:       338 of       686\t|\tloss: 2.14422\n",
      "Training Epoch 18  49.4% | batch:       339 of       686\t|\tloss: 2.03886\n",
      "Training Epoch 18  49.6% | batch:       340 of       686\t|\tloss: 2.30111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 18  49.7% | batch:       341 of       686\t|\tloss: 2.23915\n",
      "Training Epoch 18  49.9% | batch:       342 of       686\t|\tloss: 1.85595\n",
      "Training Epoch 18  50.0% | batch:       343 of       686\t|\tloss: 2.1303\n",
      "Training Epoch 18  50.1% | batch:       344 of       686\t|\tloss: 2.28823\n",
      "Training Epoch 18  50.3% | batch:       345 of       686\t|\tloss: 2.58888\n",
      "Training Epoch 18  50.4% | batch:       346 of       686\t|\tloss: 2.22931\n",
      "Training Epoch 18  50.6% | batch:       347 of       686\t|\tloss: 2.11326\n",
      "Training Epoch 18  50.7% | batch:       348 of       686\t|\tloss: 2.1523\n",
      "Training Epoch 18  50.9% | batch:       349 of       686\t|\tloss: 1.79896\n",
      "Training Epoch 18  51.0% | batch:       350 of       686\t|\tloss: 2.42918\n",
      "Training Epoch 18  51.2% | batch:       351 of       686\t|\tloss: 2.19377\n",
      "Training Epoch 18  51.3% | batch:       352 of       686\t|\tloss: 2.3587\n",
      "Training Epoch 18  51.5% | batch:       353 of       686\t|\tloss: 1.95472\n",
      "Training Epoch 18  51.6% | batch:       354 of       686\t|\tloss: 2.33457\n",
      "Training Epoch 18  51.7% | batch:       355 of       686\t|\tloss: 2.65799\n",
      "Training Epoch 18  51.9% | batch:       356 of       686\t|\tloss: 1.60819\n",
      "Training Epoch 18  52.0% | batch:       357 of       686\t|\tloss: 2.4771\n",
      "Training Epoch 18  52.2% | batch:       358 of       686\t|\tloss: 2.15796\n",
      "Training Epoch 18  52.3% | batch:       359 of       686\t|\tloss: 1.83433\n",
      "Training Epoch 18  52.5% | batch:       360 of       686\t|\tloss: 2.04116\n",
      "Training Epoch 18  52.6% | batch:       361 of       686\t|\tloss: 2.42907\n",
      "Training Epoch 18  52.8% | batch:       362 of       686\t|\tloss: 2.4617\n",
      "Training Epoch 18  52.9% | batch:       363 of       686\t|\tloss: 2.22128\n",
      "Training Epoch 18  53.1% | batch:       364 of       686\t|\tloss: 2.02607\n",
      "Training Epoch 18  53.2% | batch:       365 of       686\t|\tloss: 2.51107\n",
      "Training Epoch 18  53.4% | batch:       366 of       686\t|\tloss: 2.97299\n",
      "Training Epoch 18  53.5% | batch:       367 of       686\t|\tloss: 1.91204\n",
      "Training Epoch 18  53.6% | batch:       368 of       686\t|\tloss: 2.12882\n",
      "Training Epoch 18  53.8% | batch:       369 of       686\t|\tloss: 2.05882\n",
      "Training Epoch 18  53.9% | batch:       370 of       686\t|\tloss: 1.73578\n",
      "Training Epoch 18  54.1% | batch:       371 of       686\t|\tloss: 1.87278\n",
      "Training Epoch 18  54.2% | batch:       372 of       686\t|\tloss: 1.84904\n",
      "Training Epoch 18  54.4% | batch:       373 of       686\t|\tloss: 2.27227\n",
      "Training Epoch 18  54.5% | batch:       374 of       686\t|\tloss: 1.99854\n",
      "Training Epoch 18  54.7% | batch:       375 of       686\t|\tloss: 1.86778\n",
      "Training Epoch 18  54.8% | batch:       376 of       686\t|\tloss: 2.31573\n",
      "Training Epoch 18  55.0% | batch:       377 of       686\t|\tloss: 1.90387\n",
      "Training Epoch 18  55.1% | batch:       378 of       686\t|\tloss: 2.38764\n",
      "Training Epoch 18  55.2% | batch:       379 of       686\t|\tloss: 2.26197\n",
      "Training Epoch 18  55.4% | batch:       380 of       686\t|\tloss: 2.79299\n",
      "Training Epoch 18  55.5% | batch:       381 of       686\t|\tloss: 2.4705\n",
      "Training Epoch 18  55.7% | batch:       382 of       686\t|\tloss: 2.11329\n",
      "Training Epoch 18  55.8% | batch:       383 of       686\t|\tloss: 2.21697\n",
      "Training Epoch 18  56.0% | batch:       384 of       686\t|\tloss: 1.45963\n",
      "Training Epoch 18  56.1% | batch:       385 of       686\t|\tloss: 1.86691\n",
      "Training Epoch 18  56.3% | batch:       386 of       686\t|\tloss: 3.27226\n",
      "Training Epoch 18  56.4% | batch:       387 of       686\t|\tloss: 2.08046\n",
      "Training Epoch 18  56.6% | batch:       388 of       686\t|\tloss: 2.34142\n",
      "Training Epoch 18  56.7% | batch:       389 of       686\t|\tloss: 2.15351\n",
      "Training Epoch 18  56.9% | batch:       390 of       686\t|\tloss: 1.70981\n",
      "Training Epoch 18  57.0% | batch:       391 of       686\t|\tloss: 2.02668\n",
      "Training Epoch 18  57.1% | batch:       392 of       686\t|\tloss: 2.69986\n",
      "Training Epoch 18  57.3% | batch:       393 of       686\t|\tloss: 2.13987\n",
      "Training Epoch 18  57.4% | batch:       394 of       686\t|\tloss: 1.5631\n",
      "Training Epoch 18  57.6% | batch:       395 of       686\t|\tloss: 2.06596\n",
      "Training Epoch 18  57.7% | batch:       396 of       686\t|\tloss: 2.67108\n",
      "Training Epoch 18  57.9% | batch:       397 of       686\t|\tloss: 2.68756\n",
      "Training Epoch 18  58.0% | batch:       398 of       686\t|\tloss: 1.98264\n",
      "Training Epoch 18  58.2% | batch:       399 of       686\t|\tloss: 2.05056\n",
      "Training Epoch 18  58.3% | batch:       400 of       686\t|\tloss: 1.99242\n",
      "Training Epoch 18  58.5% | batch:       401 of       686\t|\tloss: 3.08066\n",
      "Training Epoch 18  58.6% | batch:       402 of       686\t|\tloss: 2.00636\n",
      "Training Epoch 18  58.7% | batch:       403 of       686\t|\tloss: 1.95641\n",
      "Training Epoch 18  58.9% | batch:       404 of       686\t|\tloss: 2.72154\n",
      "Training Epoch 18  59.0% | batch:       405 of       686\t|\tloss: 1.99662\n",
      "Training Epoch 18  59.2% | batch:       406 of       686\t|\tloss: 2.17029\n",
      "Training Epoch 18  59.3% | batch:       407 of       686\t|\tloss: 1.92077\n",
      "Training Epoch 18  59.5% | batch:       408 of       686\t|\tloss: 1.84988\n",
      "Training Epoch 18  59.6% | batch:       409 of       686\t|\tloss: 1.83462\n",
      "Training Epoch 18  59.8% | batch:       410 of       686\t|\tloss: 2.26936\n",
      "Training Epoch 18  59.9% | batch:       411 of       686\t|\tloss: 2.27205\n",
      "Training Epoch 18  60.1% | batch:       412 of       686\t|\tloss: 2.64825\n",
      "Training Epoch 18  60.2% | batch:       413 of       686\t|\tloss: 1.86535\n",
      "Training Epoch 18  60.3% | batch:       414 of       686\t|\tloss: 2.15112\n",
      "Training Epoch 18  60.5% | batch:       415 of       686\t|\tloss: 2.38239\n",
      "Training Epoch 18  60.6% | batch:       416 of       686\t|\tloss: 2.63774\n",
      "Training Epoch 18  60.8% | batch:       417 of       686\t|\tloss: 2.02435\n",
      "Training Epoch 18  60.9% | batch:       418 of       686\t|\tloss: 2.12756\n",
      "Training Epoch 18  61.1% | batch:       419 of       686\t|\tloss: 1.90726\n",
      "Training Epoch 18  61.2% | batch:       420 of       686\t|\tloss: 2.37482\n",
      "Training Epoch 18  61.4% | batch:       421 of       686\t|\tloss: 3.21404\n",
      "Training Epoch 18  61.5% | batch:       422 of       686\t|\tloss: 2.47714\n",
      "Training Epoch 18  61.7% | batch:       423 of       686\t|\tloss: 2.42328\n",
      "Training Epoch 18  61.8% | batch:       424 of       686\t|\tloss: 2.10845\n",
      "Training Epoch 18  62.0% | batch:       425 of       686\t|\tloss: 2.37259\n",
      "Training Epoch 18  62.1% | batch:       426 of       686\t|\tloss: 1.84498\n",
      "Training Epoch 18  62.2% | batch:       427 of       686\t|\tloss: 2.11556\n",
      "Training Epoch 18  62.4% | batch:       428 of       686\t|\tloss: 2.8879\n",
      "Training Epoch 18  62.5% | batch:       429 of       686\t|\tloss: 2.47223\n",
      "Training Epoch 18  62.7% | batch:       430 of       686\t|\tloss: 2.4375\n",
      "Training Epoch 18  62.8% | batch:       431 of       686\t|\tloss: 2.11003\n",
      "Training Epoch 18  63.0% | batch:       432 of       686\t|\tloss: 2.4935\n",
      "Training Epoch 18  63.1% | batch:       433 of       686\t|\tloss: 2.09009\n",
      "Training Epoch 18  63.3% | batch:       434 of       686\t|\tloss: 2.2119\n",
      "Training Epoch 18  63.4% | batch:       435 of       686\t|\tloss: 2.12027\n",
      "Training Epoch 18  63.6% | batch:       436 of       686\t|\tloss: 2.15853\n",
      "Training Epoch 18  63.7% | batch:       437 of       686\t|\tloss: 2.49382\n",
      "Training Epoch 18  63.8% | batch:       438 of       686\t|\tloss: 2.15468\n",
      "Training Epoch 18  64.0% | batch:       439 of       686\t|\tloss: 2.40123\n",
      "Training Epoch 18  64.1% | batch:       440 of       686\t|\tloss: 1.87586\n",
      "Training Epoch 18  64.3% | batch:       441 of       686\t|\tloss: 2.24834\n",
      "Training Epoch 18  64.4% | batch:       442 of       686\t|\tloss: 2.60519\n",
      "Training Epoch 18  64.6% | batch:       443 of       686\t|\tloss: 1.99331\n",
      "Training Epoch 18  64.7% | batch:       444 of       686\t|\tloss: 2.79205\n",
      "Training Epoch 18  64.9% | batch:       445 of       686\t|\tloss: 2.37414\n",
      "Training Epoch 18  65.0% | batch:       446 of       686\t|\tloss: 1.90087\n",
      "Training Epoch 18  65.2% | batch:       447 of       686\t|\tloss: 2.0678\n",
      "Training Epoch 18  65.3% | batch:       448 of       686\t|\tloss: 2.21104\n",
      "Training Epoch 18  65.5% | batch:       449 of       686\t|\tloss: 2.26399\n",
      "Training Epoch 18  65.6% | batch:       450 of       686\t|\tloss: 2.01889\n",
      "Training Epoch 18  65.7% | batch:       451 of       686\t|\tloss: 2.53228\n",
      "Training Epoch 18  65.9% | batch:       452 of       686\t|\tloss: 2.43205\n",
      "Training Epoch 18  66.0% | batch:       453 of       686\t|\tloss: 2.33123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 18  66.2% | batch:       454 of       686\t|\tloss: 2.29644\n",
      "Training Epoch 18  66.3% | batch:       455 of       686\t|\tloss: 1.97736\n",
      "Training Epoch 18  66.5% | batch:       456 of       686\t|\tloss: 2.57268\n",
      "Training Epoch 18  66.6% | batch:       457 of       686\t|\tloss: 2.79181\n",
      "Training Epoch 18  66.8% | batch:       458 of       686\t|\tloss: 2.52802\n",
      "Training Epoch 18  66.9% | batch:       459 of       686\t|\tloss: 1.66605\n",
      "Training Epoch 18  67.1% | batch:       460 of       686\t|\tloss: 1.81229\n",
      "Training Epoch 18  67.2% | batch:       461 of       686\t|\tloss: 2.37208\n",
      "Training Epoch 18  67.3% | batch:       462 of       686\t|\tloss: 2.32005\n",
      "Training Epoch 18  67.5% | batch:       463 of       686\t|\tloss: 3.43907\n",
      "Training Epoch 18  67.6% | batch:       464 of       686\t|\tloss: 1.65023\n",
      "Training Epoch 18  67.8% | batch:       465 of       686\t|\tloss: 1.67862\n",
      "Training Epoch 18  67.9% | batch:       466 of       686\t|\tloss: 2.09116\n",
      "Training Epoch 18  68.1% | batch:       467 of       686\t|\tloss: 2.65536\n",
      "Training Epoch 18  68.2% | batch:       468 of       686\t|\tloss: 2.22021\n",
      "Training Epoch 18  68.4% | batch:       469 of       686\t|\tloss: 1.8942\n",
      "Training Epoch 18  68.5% | batch:       470 of       686\t|\tloss: 2.56274\n",
      "Training Epoch 18  68.7% | batch:       471 of       686\t|\tloss: 2.01902\n",
      "Training Epoch 18  68.8% | batch:       472 of       686\t|\tloss: 2.0197\n",
      "Training Epoch 18  69.0% | batch:       473 of       686\t|\tloss: 2.20671\n",
      "Training Epoch 18  69.1% | batch:       474 of       686\t|\tloss: 3.12977\n",
      "Training Epoch 18  69.2% | batch:       475 of       686\t|\tloss: 2.08522\n",
      "Training Epoch 18  69.4% | batch:       476 of       686\t|\tloss: 2.57183\n",
      "Training Epoch 18  69.5% | batch:       477 of       686\t|\tloss: 2.09416\n",
      "Training Epoch 18  69.7% | batch:       478 of       686\t|\tloss: 1.96725\n",
      "Training Epoch 18  69.8% | batch:       479 of       686\t|\tloss: 2.09749\n",
      "Training Epoch 18  70.0% | batch:       480 of       686\t|\tloss: 2.36844\n",
      "Training Epoch 18  70.1% | batch:       481 of       686\t|\tloss: 1.95765\n",
      "Training Epoch 18  70.3% | batch:       482 of       686\t|\tloss: 2.98072\n",
      "Training Epoch 18  70.4% | batch:       483 of       686\t|\tloss: 2.51322\n",
      "Training Epoch 18  70.6% | batch:       484 of       686\t|\tloss: 1.81342\n",
      "Training Epoch 18  70.7% | batch:       485 of       686\t|\tloss: 2.15704\n",
      "Training Epoch 18  70.8% | batch:       486 of       686\t|\tloss: 2.41634\n",
      "Training Epoch 18  71.0% | batch:       487 of       686\t|\tloss: 1.69368\n",
      "Training Epoch 18  71.1% | batch:       488 of       686\t|\tloss: 1.56559\n",
      "Training Epoch 18  71.3% | batch:       489 of       686\t|\tloss: 2.10888\n",
      "Training Epoch 18  71.4% | batch:       490 of       686\t|\tloss: 2.27968\n",
      "Training Epoch 18  71.6% | batch:       491 of       686\t|\tloss: 2.26545\n",
      "Training Epoch 18  71.7% | batch:       492 of       686\t|\tloss: 2.14096\n",
      "Training Epoch 18  71.9% | batch:       493 of       686\t|\tloss: 2.81244\n",
      "Training Epoch 18  72.0% | batch:       494 of       686\t|\tloss: 2.52935\n",
      "Training Epoch 18  72.2% | batch:       495 of       686\t|\tloss: 1.87521\n",
      "Training Epoch 18  72.3% | batch:       496 of       686\t|\tloss: 2.19705\n",
      "Training Epoch 18  72.4% | batch:       497 of       686\t|\tloss: 2.35271\n",
      "Training Epoch 18  72.6% | batch:       498 of       686\t|\tloss: 1.51987\n",
      "Training Epoch 18  72.7% | batch:       499 of       686\t|\tloss: 2.02808\n",
      "Training Epoch 18  72.9% | batch:       500 of       686\t|\tloss: 2.29625\n",
      "Training Epoch 18  73.0% | batch:       501 of       686\t|\tloss: 2.21538\n",
      "Training Epoch 18  73.2% | batch:       502 of       686\t|\tloss: 2.23798\n",
      "Training Epoch 18  73.3% | batch:       503 of       686\t|\tloss: 2.23938\n",
      "Training Epoch 18  73.5% | batch:       504 of       686\t|\tloss: 2.48051\n",
      "Training Epoch 18  73.6% | batch:       505 of       686\t|\tloss: 2.24508\n",
      "Training Epoch 18  73.8% | batch:       506 of       686\t|\tloss: 2.21386\n",
      "Training Epoch 18  73.9% | batch:       507 of       686\t|\tloss: 2.12552\n",
      "Training Epoch 18  74.1% | batch:       508 of       686\t|\tloss: 2.01836\n",
      "Training Epoch 18  74.2% | batch:       509 of       686\t|\tloss: 2.68892\n",
      "Training Epoch 18  74.3% | batch:       510 of       686\t|\tloss: 2.42267\n",
      "Training Epoch 18  74.5% | batch:       511 of       686\t|\tloss: 1.65743\n",
      "Training Epoch 18  74.6% | batch:       512 of       686\t|\tloss: 2.0059\n",
      "Training Epoch 18  74.8% | batch:       513 of       686\t|\tloss: 2.00265\n",
      "Training Epoch 18  74.9% | batch:       514 of       686\t|\tloss: 3.26915\n",
      "Training Epoch 18  75.1% | batch:       515 of       686\t|\tloss: 1.67357\n",
      "Training Epoch 18  75.2% | batch:       516 of       686\t|\tloss: 2.17445\n",
      "Training Epoch 18  75.4% | batch:       517 of       686\t|\tloss: 2.15529\n",
      "Training Epoch 18  75.5% | batch:       518 of       686\t|\tloss: 1.97875\n",
      "Training Epoch 18  75.7% | batch:       519 of       686\t|\tloss: 2.01683\n",
      "Training Epoch 18  75.8% | batch:       520 of       686\t|\tloss: 2.18716\n",
      "Training Epoch 18  75.9% | batch:       521 of       686\t|\tloss: 2.38812\n",
      "Training Epoch 18  76.1% | batch:       522 of       686\t|\tloss: 2.59476\n",
      "Training Epoch 18  76.2% | batch:       523 of       686\t|\tloss: 2.57024\n",
      "Training Epoch 18  76.4% | batch:       524 of       686\t|\tloss: 2.22364\n",
      "Training Epoch 18  76.5% | batch:       525 of       686\t|\tloss: 1.93335\n",
      "Training Epoch 18  76.7% | batch:       526 of       686\t|\tloss: 2.52147\n",
      "Training Epoch 18  76.8% | batch:       527 of       686\t|\tloss: 2.2046\n",
      "Training Epoch 18  77.0% | batch:       528 of       686\t|\tloss: 2.64822\n",
      "Training Epoch 18  77.1% | batch:       529 of       686\t|\tloss: 1.89188\n",
      "Training Epoch 18  77.3% | batch:       530 of       686\t|\tloss: 1.61639\n",
      "Training Epoch 18  77.4% | batch:       531 of       686\t|\tloss: 1.69505\n",
      "Training Epoch 18  77.6% | batch:       532 of       686\t|\tloss: 1.81303\n",
      "Training Epoch 18  77.7% | batch:       533 of       686\t|\tloss: 3.117\n",
      "Training Epoch 18  77.8% | batch:       534 of       686\t|\tloss: 1.59945\n",
      "Training Epoch 18  78.0% | batch:       535 of       686\t|\tloss: 2.04141\n",
      "Training Epoch 18  78.1% | batch:       536 of       686\t|\tloss: 2.36535\n",
      "Training Epoch 18  78.3% | batch:       537 of       686\t|\tloss: 1.62827\n",
      "Training Epoch 18  78.4% | batch:       538 of       686\t|\tloss: 2.39292\n",
      "Training Epoch 18  78.6% | batch:       539 of       686\t|\tloss: 2.14346\n",
      "Training Epoch 18  78.7% | batch:       540 of       686\t|\tloss: 1.74518\n",
      "Training Epoch 18  78.9% | batch:       541 of       686\t|\tloss: 2.47484\n",
      "Training Epoch 18  79.0% | batch:       542 of       686\t|\tloss: 2.02627\n",
      "Training Epoch 18  79.2% | batch:       543 of       686\t|\tloss: 1.58262\n",
      "Training Epoch 18  79.3% | batch:       544 of       686\t|\tloss: 2.31175\n",
      "Training Epoch 18  79.4% | batch:       545 of       686\t|\tloss: 2.28693\n",
      "Training Epoch 18  79.6% | batch:       546 of       686\t|\tloss: 2.52124\n",
      "Training Epoch 18  79.7% | batch:       547 of       686\t|\tloss: 2.62198\n",
      "Training Epoch 18  79.9% | batch:       548 of       686\t|\tloss: 2.764\n",
      "Training Epoch 18  80.0% | batch:       549 of       686\t|\tloss: 1.88061\n",
      "Training Epoch 18  80.2% | batch:       550 of       686\t|\tloss: 2.11476\n",
      "Training Epoch 18  80.3% | batch:       551 of       686\t|\tloss: 2.24998\n",
      "Training Epoch 18  80.5% | batch:       552 of       686\t|\tloss: 1.97918\n",
      "Training Epoch 18  80.6% | batch:       553 of       686\t|\tloss: 2.38137\n",
      "Training Epoch 18  80.8% | batch:       554 of       686\t|\tloss: 2.52927\n",
      "Training Epoch 18  80.9% | batch:       555 of       686\t|\tloss: 2.30558\n",
      "Training Epoch 18  81.0% | batch:       556 of       686\t|\tloss: 2.55842\n",
      "Training Epoch 18  81.2% | batch:       557 of       686\t|\tloss: 1.73257\n",
      "Training Epoch 18  81.3% | batch:       558 of       686\t|\tloss: 2.09076\n",
      "Training Epoch 18  81.5% | batch:       559 of       686\t|\tloss: 2.2352\n",
      "Training Epoch 18  81.6% | batch:       560 of       686\t|\tloss: 2.03659\n",
      "Training Epoch 18  81.8% | batch:       561 of       686\t|\tloss: 1.92746\n",
      "Training Epoch 18  81.9% | batch:       562 of       686\t|\tloss: 2.33261\n",
      "Training Epoch 18  82.1% | batch:       563 of       686\t|\tloss: 2.56393\n",
      "Training Epoch 18  82.2% | batch:       564 of       686\t|\tloss: 2.51954\n",
      "Training Epoch 18  82.4% | batch:       565 of       686\t|\tloss: 1.84297\n",
      "Training Epoch 18  82.5% | batch:       566 of       686\t|\tloss: 1.98518\n",
      "Training Epoch 18  82.7% | batch:       567 of       686\t|\tloss: 2.39036\n",
      "Training Epoch 18  82.8% | batch:       568 of       686\t|\tloss: 1.80429\n",
      "Training Epoch 18  82.9% | batch:       569 of       686\t|\tloss: 2.61875\n",
      "Training Epoch 18  83.1% | batch:       570 of       686\t|\tloss: 1.75034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 18  83.2% | batch:       571 of       686\t|\tloss: 2.50284\n",
      "Training Epoch 18  83.4% | batch:       572 of       686\t|\tloss: 2.32576\n",
      "Training Epoch 18  83.5% | batch:       573 of       686\t|\tloss: 1.95041\n",
      "Training Epoch 18  83.7% | batch:       574 of       686\t|\tloss: 2.99501\n",
      "Training Epoch 18  83.8% | batch:       575 of       686\t|\tloss: 2.2689\n",
      "Training Epoch 18  84.0% | batch:       576 of       686\t|\tloss: 2.05118\n",
      "Training Epoch 18  84.1% | batch:       577 of       686\t|\tloss: 2.30068\n",
      "Training Epoch 18  84.3% | batch:       578 of       686\t|\tloss: 1.8915\n",
      "Training Epoch 18  84.4% | batch:       579 of       686\t|\tloss: 2.36539\n",
      "Training Epoch 18  84.5% | batch:       580 of       686\t|\tloss: 2.21952\n",
      "Training Epoch 18  84.7% | batch:       581 of       686\t|\tloss: 2.18466\n",
      "Training Epoch 18  84.8% | batch:       582 of       686\t|\tloss: 1.84783\n",
      "Training Epoch 18  85.0% | batch:       583 of       686\t|\tloss: 3.06841\n",
      "Training Epoch 18  85.1% | batch:       584 of       686\t|\tloss: 2.96812\n",
      "Training Epoch 18  85.3% | batch:       585 of       686\t|\tloss: 2.97953\n",
      "Training Epoch 18  85.4% | batch:       586 of       686\t|\tloss: 2.46657\n",
      "Training Epoch 18  85.6% | batch:       587 of       686\t|\tloss: 2.53161\n",
      "Training Epoch 18  85.7% | batch:       588 of       686\t|\tloss: 2.07239\n",
      "Training Epoch 18  85.9% | batch:       589 of       686\t|\tloss: 1.94833\n",
      "Training Epoch 18  86.0% | batch:       590 of       686\t|\tloss: 2.25594\n",
      "Training Epoch 18  86.2% | batch:       591 of       686\t|\tloss: 2.32671\n",
      "Training Epoch 18  86.3% | batch:       592 of       686\t|\tloss: 1.99766\n",
      "Training Epoch 18  86.4% | batch:       593 of       686\t|\tloss: 2.81868\n",
      "Training Epoch 18  86.6% | batch:       594 of       686\t|\tloss: 2.08584\n",
      "Training Epoch 18  86.7% | batch:       595 of       686\t|\tloss: 2.32199\n",
      "Training Epoch 18  86.9% | batch:       596 of       686\t|\tloss: 2.04187\n",
      "Training Epoch 18  87.0% | batch:       597 of       686\t|\tloss: 2.34009\n",
      "Training Epoch 18  87.2% | batch:       598 of       686\t|\tloss: 2.15118\n",
      "Training Epoch 18  87.3% | batch:       599 of       686\t|\tloss: 2.17704\n",
      "Training Epoch 18  87.5% | batch:       600 of       686\t|\tloss: 2.17246\n",
      "Training Epoch 18  87.6% | batch:       601 of       686\t|\tloss: 1.49604\n",
      "Training Epoch 18  87.8% | batch:       602 of       686\t|\tloss: 1.95598\n",
      "Training Epoch 18  87.9% | batch:       603 of       686\t|\tloss: 1.84072\n",
      "Training Epoch 18  88.0% | batch:       604 of       686\t|\tloss: 2.01748\n",
      "Training Epoch 18  88.2% | batch:       605 of       686\t|\tloss: 2.46836\n",
      "Training Epoch 18  88.3% | batch:       606 of       686\t|\tloss: 2.63686\n",
      "Training Epoch 18  88.5% | batch:       607 of       686\t|\tloss: 1.67774\n",
      "Training Epoch 18  88.6% | batch:       608 of       686\t|\tloss: 2.13484\n",
      "Training Epoch 18  88.8% | batch:       609 of       686\t|\tloss: 2.69707\n",
      "Training Epoch 18  88.9% | batch:       610 of       686\t|\tloss: 1.94905\n",
      "Training Epoch 18  89.1% | batch:       611 of       686\t|\tloss: 2.07306\n",
      "Training Epoch 18  89.2% | batch:       612 of       686\t|\tloss: 2.33335\n",
      "Training Epoch 18  89.4% | batch:       613 of       686\t|\tloss: 2.8755\n",
      "Training Epoch 18  89.5% | batch:       614 of       686\t|\tloss: 2.35034\n",
      "Training Epoch 18  89.7% | batch:       615 of       686\t|\tloss: 1.88676\n",
      "Training Epoch 18  89.8% | batch:       616 of       686\t|\tloss: 2.70866\n",
      "Training Epoch 18  89.9% | batch:       617 of       686\t|\tloss: 1.96248\n",
      "Training Epoch 18  90.1% | batch:       618 of       686\t|\tloss: 1.73556\n",
      "Training Epoch 18  90.2% | batch:       619 of       686\t|\tloss: 1.77595\n",
      "Training Epoch 18  90.4% | batch:       620 of       686\t|\tloss: 1.82331\n",
      "Training Epoch 18  90.5% | batch:       621 of       686\t|\tloss: 2.12829\n",
      "Training Epoch 18  90.7% | batch:       622 of       686\t|\tloss: 2.28975\n",
      "Training Epoch 18  90.8% | batch:       623 of       686\t|\tloss: 1.80344\n",
      "Training Epoch 18  91.0% | batch:       624 of       686\t|\tloss: 2.25517\n",
      "Training Epoch 18  91.1% | batch:       625 of       686\t|\tloss: 1.70852\n",
      "Training Epoch 18  91.3% | batch:       626 of       686\t|\tloss: 1.52487\n",
      "Training Epoch 18  91.4% | batch:       627 of       686\t|\tloss: 2.06711\n",
      "Training Epoch 18  91.5% | batch:       628 of       686\t|\tloss: 2.17405\n",
      "Training Epoch 18  91.7% | batch:       629 of       686\t|\tloss: 2.46447\n",
      "Training Epoch 18  91.8% | batch:       630 of       686\t|\tloss: 2.32869\n",
      "Training Epoch 18  92.0% | batch:       631 of       686\t|\tloss: 2.06617\n",
      "Training Epoch 18  92.1% | batch:       632 of       686\t|\tloss: 1.52986\n",
      "Training Epoch 18  92.3% | batch:       633 of       686\t|\tloss: 1.57188\n",
      "Training Epoch 18  92.4% | batch:       634 of       686\t|\tloss: 2.24075\n",
      "Training Epoch 18  92.6% | batch:       635 of       686\t|\tloss: 1.65572\n",
      "Training Epoch 18  92.7% | batch:       636 of       686\t|\tloss: 2.0307\n",
      "Training Epoch 18  92.9% | batch:       637 of       686\t|\tloss: 2.35738\n",
      "Training Epoch 18  93.0% | batch:       638 of       686\t|\tloss: 1.93804\n",
      "Training Epoch 18  93.1% | batch:       639 of       686\t|\tloss: 2.15599\n",
      "Training Epoch 18  93.3% | batch:       640 of       686\t|\tloss: 1.96749\n",
      "Training Epoch 18  93.4% | batch:       641 of       686\t|\tloss: 2.30744\n",
      "Training Epoch 18  93.6% | batch:       642 of       686\t|\tloss: 2.22764\n",
      "Training Epoch 18  93.7% | batch:       643 of       686\t|\tloss: 2.50209\n",
      "Training Epoch 18  93.9% | batch:       644 of       686\t|\tloss: 2.40843\n",
      "Training Epoch 18  94.0% | batch:       645 of       686\t|\tloss: 2.25746\n",
      "Training Epoch 18  94.2% | batch:       646 of       686\t|\tloss: 2.36438\n",
      "Training Epoch 18  94.3% | batch:       647 of       686\t|\tloss: 2.48708\n",
      "Training Epoch 18  94.5% | batch:       648 of       686\t|\tloss: 2.28993\n",
      "Training Epoch 18  94.6% | batch:       649 of       686\t|\tloss: 1.77987\n",
      "Training Epoch 18  94.8% | batch:       650 of       686\t|\tloss: 1.91988\n",
      "Training Epoch 18  94.9% | batch:       651 of       686\t|\tloss: 1.98375\n",
      "Training Epoch 18  95.0% | batch:       652 of       686\t|\tloss: 2.72907\n",
      "Training Epoch 18  95.2% | batch:       653 of       686\t|\tloss: 2.03615\n",
      "Training Epoch 18  95.3% | batch:       654 of       686\t|\tloss: 2.45329\n",
      "Training Epoch 18  95.5% | batch:       655 of       686\t|\tloss: 2.1047\n",
      "Training Epoch 18  95.6% | batch:       656 of       686\t|\tloss: 2.49213\n",
      "Training Epoch 18  95.8% | batch:       657 of       686\t|\tloss: 2.2603\n",
      "Training Epoch 18  95.9% | batch:       658 of       686\t|\tloss: 1.68446\n",
      "Training Epoch 18  96.1% | batch:       659 of       686\t|\tloss: 1.81587\n",
      "Training Epoch 18  96.2% | batch:       660 of       686\t|\tloss: 1.92118\n",
      "Training Epoch 18  96.4% | batch:       661 of       686\t|\tloss: 2.37618\n",
      "Training Epoch 18  96.5% | batch:       662 of       686\t|\tloss: 2.1777\n",
      "Training Epoch 18  96.6% | batch:       663 of       686\t|\tloss: 2.35558\n",
      "Training Epoch 18  96.8% | batch:       664 of       686\t|\tloss: 3.39537\n",
      "Training Epoch 18  96.9% | batch:       665 of       686\t|\tloss: 1.73944\n",
      "Training Epoch 18  97.1% | batch:       666 of       686\t|\tloss: 2.34244\n",
      "Training Epoch 18  97.2% | batch:       667 of       686\t|\tloss: 1.79294\n",
      "Training Epoch 18  97.4% | batch:       668 of       686\t|\tloss: 2.69048\n",
      "Training Epoch 18  97.5% | batch:       669 of       686\t|\tloss: 2.05057\n",
      "Training Epoch 18  97.7% | batch:       670 of       686\t|\tloss: 2.1443\n",
      "Training Epoch 18  97.8% | batch:       671 of       686\t|\tloss: 1.82001\n",
      "Training Epoch 18  98.0% | batch:       672 of       686\t|\tloss: 2.34301\n",
      "Training Epoch 18  98.1% | batch:       673 of       686\t|\tloss: 1.94791\n",
      "Training Epoch 18  98.3% | batch:       674 of       686\t|\tloss: 2.34592\n",
      "Training Epoch 18  98.4% | batch:       675 of       686\t|\tloss: 2.0601\n",
      "Training Epoch 18  98.5% | batch:       676 of       686\t|\tloss: 2.28146\n",
      "Training Epoch 18  98.7% | batch:       677 of       686\t|\tloss: 2.26328\n",
      "Training Epoch 18  98.8% | batch:       678 of       686\t|\tloss: 2.13739\n",
      "Training Epoch 18  99.0% | batch:       679 of       686\t|\tloss: 2.35683\n",
      "Training Epoch 18  99.1% | batch:       680 of       686\t|\tloss: 3.00984\n",
      "Training Epoch 18  99.3% | batch:       681 of       686\t|\tloss: 1.85701\n",
      "Training Epoch 18  99.4% | batch:       682 of       686\t|\tloss: 2.09043\n",
      "Training Epoch 18  99.6% | batch:       683 of       686\t|\tloss: 2.90757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:08:11,551 | INFO : Epoch 18 Training Summary: epoch: 18.000000 | loss: 2.296882 | \n",
      "2023-05-25 22:08:11,554 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 24.875190496444702 seconds\n",
      "\n",
      "2023-05-25 22:08:11,555 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.956815441449482 seconds\n",
      "2023-05-25 22:08:11,556 | INFO : Avg batch train. time: 0.03492247148899341 seconds\n",
      "2023-05-25 22:08:11,557 | INFO : Avg sample train. time: 0.00027318336782541176 seconds\n",
      "2023-05-25 22:08:11,558 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 18  99.7% | batch:       684 of       686\t|\tloss: 2.445\n",
      "Training Epoch 18  99.9% | batch:       685 of       686\t|\tloss: 2.48253\n",
      "\n",
      "Evaluating Epoch 18   0.0% | batch:         0 of       172\t|\tloss: 1.69202\n",
      "Evaluating Epoch 18   0.6% | batch:         1 of       172\t|\tloss: 2.2422\n",
      "Evaluating Epoch 18   1.2% | batch:         2 of       172\t|\tloss: 1.67617\n",
      "Evaluating Epoch 18   1.7% | batch:         3 of       172\t|\tloss: 3.36314\n",
      "Evaluating Epoch 18   2.3% | batch:         4 of       172\t|\tloss: 1.86784\n",
      "Evaluating Epoch 18   2.9% | batch:         5 of       172\t|\tloss: 1.57443\n",
      "Evaluating Epoch 18   3.5% | batch:         6 of       172\t|\tloss: 2.20106\n",
      "Evaluating Epoch 18   4.1% | batch:         7 of       172\t|\tloss: 3.65097\n",
      "Evaluating Epoch 18   4.7% | batch:         8 of       172\t|\tloss: 1.42669\n",
      "Evaluating Epoch 18   5.2% | batch:         9 of       172\t|\tloss: 2.0946\n",
      "Evaluating Epoch 18   5.8% | batch:        10 of       172\t|\tloss: 2.30017\n",
      "Evaluating Epoch 18   6.4% | batch:        11 of       172\t|\tloss: 2.08551\n",
      "Evaluating Epoch 18   7.0% | batch:        12 of       172\t|\tloss: 1.56214\n",
      "Evaluating Epoch 18   7.6% | batch:        13 of       172\t|\tloss: 2.23519\n",
      "Evaluating Epoch 18   8.1% | batch:        14 of       172\t|\tloss: 2.48049\n",
      "Evaluating Epoch 18   8.7% | batch:        15 of       172\t|\tloss: 1.93255\n",
      "Evaluating Epoch 18   9.3% | batch:        16 of       172\t|\tloss: 2.56437\n",
      "Evaluating Epoch 18   9.9% | batch:        17 of       172\t|\tloss: 1.57009\n",
      "Evaluating Epoch 18  10.5% | batch:        18 of       172\t|\tloss: 17.8844\n",
      "Evaluating Epoch 18  11.0% | batch:        19 of       172\t|\tloss: 1.8651\n",
      "Evaluating Epoch 18  11.6% | batch:        20 of       172\t|\tloss: 1.53271\n",
      "Evaluating Epoch 18  12.2% | batch:        21 of       172\t|\tloss: 0.209486\n",
      "Evaluating Epoch 18  12.8% | batch:        22 of       172\t|\tloss: 3.68432\n",
      "Evaluating Epoch 18  13.4% | batch:        23 of       172\t|\tloss: 3.21287\n",
      "Evaluating Epoch 18  14.0% | batch:        24 of       172\t|\tloss: 1.32931\n",
      "Evaluating Epoch 18  14.5% | batch:        25 of       172\t|\tloss: 1.82911\n",
      "Evaluating Epoch 18  15.1% | batch:        26 of       172\t|\tloss: 8.66908\n",
      "Evaluating Epoch 18  15.7% | batch:        27 of       172\t|\tloss: 16.4419\n",
      "Evaluating Epoch 18  16.3% | batch:        28 of       172\t|\tloss: 0.532693\n",
      "Evaluating Epoch 18  16.9% | batch:        29 of       172\t|\tloss: 0.958704\n",
      "Evaluating Epoch 18  17.4% | batch:        30 of       172\t|\tloss: 0.489708\n",
      "Evaluating Epoch 18  18.0% | batch:        31 of       172\t|\tloss: 0.300383\n",
      "Evaluating Epoch 18  18.6% | batch:        32 of       172\t|\tloss: 1.24014\n",
      "Evaluating Epoch 18  19.2% | batch:        33 of       172\t|\tloss: 0.644053\n",
      "Evaluating Epoch 18  19.8% | batch:        34 of       172\t|\tloss: 0.662643\n",
      "Evaluating Epoch 18  20.3% | batch:        35 of       172\t|\tloss: 0.471884\n",
      "Evaluating Epoch 18  20.9% | batch:        36 of       172\t|\tloss: 3.98331\n",
      "Evaluating Epoch 18  21.5% | batch:        37 of       172\t|\tloss: 5.51992\n",
      "Evaluating Epoch 18  22.1% | batch:        38 of       172\t|\tloss: 3.96702\n",
      "Evaluating Epoch 18  22.7% | batch:        39 of       172\t|\tloss: 7.29412\n",
      "Evaluating Epoch 18  23.3% | batch:        40 of       172\t|\tloss: 1.11454\n",
      "Evaluating Epoch 18  23.8% | batch:        41 of       172\t|\tloss: 0.5859\n",
      "Evaluating Epoch 18  24.4% | batch:        42 of       172\t|\tloss: 1.24682\n",
      "Evaluating Epoch 18  25.0% | batch:        43 of       172\t|\tloss: 19.3447\n",
      "Evaluating Epoch 18  25.6% | batch:        44 of       172\t|\tloss: 1.68828\n",
      "Evaluating Epoch 18  26.2% | batch:        45 of       172\t|\tloss: 0.55113\n",
      "Evaluating Epoch 18  26.7% | batch:        46 of       172\t|\tloss: 0.121372\n",
      "Evaluating Epoch 18  27.3% | batch:        47 of       172\t|\tloss: 0.366323\n",
      "Evaluating Epoch 18  27.9% | batch:        48 of       172\t|\tloss: 0.462588\n",
      "Evaluating Epoch 18  28.5% | batch:        49 of       172\t|\tloss: 1.6589\n",
      "Evaluating Epoch 18  29.1% | batch:        50 of       172\t|\tloss: 0.998422\n",
      "Evaluating Epoch 18  29.7% | batch:        51 of       172\t|\tloss: 0.638018\n",
      "Evaluating Epoch 18  30.2% | batch:        52 of       172\t|\tloss: 0.271755\n",
      "Evaluating Epoch 18  30.8% | batch:        53 of       172\t|\tloss: 1.394\n",
      "Evaluating Epoch 18  31.4% | batch:        54 of       172\t|\tloss: 0.658492\n",
      "Evaluating Epoch 18  32.0% | batch:        55 of       172\t|\tloss: 0.32789\n",
      "Evaluating Epoch 18  32.6% | batch:        56 of       172\t|\tloss: 1.72678\n",
      "Evaluating Epoch 18  33.1% | batch:        57 of       172\t|\tloss: 0.543158\n",
      "Evaluating Epoch 18  33.7% | batch:        58 of       172\t|\tloss: 1.12044\n",
      "Evaluating Epoch 18  34.3% | batch:        59 of       172\t|\tloss: 0.693837\n",
      "Evaluating Epoch 18  34.9% | batch:        60 of       172\t|\tloss: 0.577059\n",
      "Evaluating Epoch 18  35.5% | batch:        61 of       172\t|\tloss: 1.01571\n",
      "Evaluating Epoch 18  36.0% | batch:        62 of       172\t|\tloss: 0.333111\n",
      "Evaluating Epoch 18  36.6% | batch:        63 of       172\t|\tloss: 1.58033\n",
      "Evaluating Epoch 18  37.2% | batch:        64 of       172\t|\tloss: 0.463072\n",
      "Evaluating Epoch 18  37.8% | batch:        65 of       172\t|\tloss: 1.10043\n",
      "Evaluating Epoch 18  38.4% | batch:        66 of       172\t|\tloss: 1.19321\n",
      "Evaluating Epoch 18  39.0% | batch:        67 of       172\t|\tloss: 0.294631\n",
      "Evaluating Epoch 18  39.5% | batch:        68 of       172\t|\tloss: 1.5006\n",
      "Evaluating Epoch 18  40.1% | batch:        69 of       172\t|\tloss: 0.682062\n",
      "Evaluating Epoch 18  40.7% | batch:        70 of       172\t|\tloss: 0.638601\n",
      "Evaluating Epoch 18  41.3% | batch:        71 of       172\t|\tloss: 0.915091\n",
      "Evaluating Epoch 18  41.9% | batch:        72 of       172\t|\tloss: 0.584478\n",
      "Evaluating Epoch 18  42.4% | batch:        73 of       172\t|\tloss: 1.33375\n",
      "Evaluating Epoch 18  43.0% | batch:        74 of       172\t|\tloss: 0.623541\n",
      "Evaluating Epoch 18  43.6% | batch:        75 of       172\t|\tloss: 0.68757\n",
      "Evaluating Epoch 18  44.2% | batch:        76 of       172\t|\tloss: 0.61273\n",
      "Evaluating Epoch 18  44.8% | batch:        77 of       172\t|\tloss: 0.648672\n",
      "Evaluating Epoch 18  45.3% | batch:        78 of       172\t|\tloss: 0.803262\n",
      "Evaluating Epoch 18  45.9% | batch:        79 of       172\t|\tloss: 0.513885\n",
      "Evaluating Epoch 18  46.5% | batch:        80 of       172\t|\tloss: 0.640501\n",
      "Evaluating Epoch 18  47.1% | batch:        81 of       172\t|\tloss: 0.593427\n",
      "Evaluating Epoch 18  47.7% | batch:        82 of       172\t|\tloss: 0.579954\n",
      "Evaluating Epoch 18  48.3% | batch:        83 of       172\t|\tloss: 0.624544\n",
      "Evaluating Epoch 18  48.8% | batch:        84 of       172\t|\tloss: 0.460328\n",
      "Evaluating Epoch 18  49.4% | batch:        85 of       172\t|\tloss: 0.417001\n",
      "Evaluating Epoch 18  50.0% | batch:        86 of       172\t|\tloss: 0.56363\n",
      "Evaluating Epoch 18  50.6% | batch:        87 of       172\t|\tloss: 0.526723\n",
      "Evaluating Epoch 18  51.2% | batch:        88 of       172\t|\tloss: 0.308103\n",
      "Evaluating Epoch 18  51.7% | batch:        89 of       172\t|\tloss: 0.508046\n",
      "Evaluating Epoch 18  52.3% | batch:        90 of       172\t|\tloss: 0.604624\n",
      "Evaluating Epoch 18  52.9% | batch:        91 of       172\t|\tloss: 0.473586\n",
      "Evaluating Epoch 18  53.5% | batch:        92 of       172\t|\tloss: 0.347221\n",
      "Evaluating Epoch 18  54.1% | batch:        93 of       172\t|\tloss: 0.731962\n",
      "Evaluating Epoch 18  54.7% | batch:        94 of       172\t|\tloss: 0.810736\n",
      "Evaluating Epoch 18  55.2% | batch:        95 of       172\t|\tloss: 0.314385\n",
      "Evaluating Epoch 18  55.8% | batch:        96 of       172\t|\tloss: 0.60729\n",
      "Evaluating Epoch 18  56.4% | batch:        97 of       172\t|\tloss: 0.553752\n",
      "Evaluating Epoch 18  57.0% | batch:        98 of       172\t|\tloss: 0.581274\n",
      "Evaluating Epoch 18  57.6% | batch:        99 of       172\t|\tloss: 0.378754\n",
      "Evaluating Epoch 18  58.1% | batch:       100 of       172\t|\tloss: 0.568955\n",
      "Evaluating Epoch 18  58.7% | batch:       101 of       172\t|\tloss: 0.439399\n",
      "Evaluating Epoch 18  59.3% | batch:       102 of       172\t|\tloss: 0.452112\n",
      "Evaluating Epoch 18  59.9% | batch:       103 of       172\t|\tloss: 0.727974\n",
      "Evaluating Epoch 18  60.5% | batch:       104 of       172\t|\tloss: 0.60493\n",
      "Evaluating Epoch 18  61.0% | batch:       105 of       172\t|\tloss: 0.529019\n",
      "Evaluating Epoch 18  61.6% | batch:       106 of       172\t|\tloss: 0.454611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 18  62.2% | batch:       107 of       172\t|\tloss: 0.866711\n",
      "Evaluating Epoch 18  62.8% | batch:       108 of       172\t|\tloss: 0.46029\n",
      "Evaluating Epoch 18  63.4% | batch:       109 of       172\t|\tloss: 0.435473\n",
      "Evaluating Epoch 18  64.0% | batch:       110 of       172\t|\tloss: 0.733878\n",
      "Evaluating Epoch 18  64.5% | batch:       111 of       172\t|\tloss: 0.541015\n",
      "Evaluating Epoch 18  65.1% | batch:       112 of       172\t|\tloss: 0.45518\n",
      "Evaluating Epoch 18  65.7% | batch:       113 of       172\t|\tloss: 0.710795\n",
      "Evaluating Epoch 18  66.3% | batch:       114 of       172\t|\tloss: 0.85568\n",
      "Evaluating Epoch 18  66.9% | batch:       115 of       172\t|\tloss: 1.03513\n",
      "Evaluating Epoch 18  67.4% | batch:       116 of       172\t|\tloss: 1.04596\n",
      "Evaluating Epoch 18  68.0% | batch:       117 of       172\t|\tloss: 1.00972\n",
      "Evaluating Epoch 18  68.6% | batch:       118 of       172\t|\tloss: 0.436592\n",
      "Evaluating Epoch 18  69.2% | batch:       119 of       172\t|\tloss: 0.61602\n",
      "Evaluating Epoch 18  69.8% | batch:       120 of       172\t|\tloss: 0.317396\n",
      "Evaluating Epoch 18  70.3% | batch:       121 of       172\t|\tloss: 0.571296\n",
      "Evaluating Epoch 18  70.9% | batch:       122 of       172\t|\tloss: 0.494412\n",
      "Evaluating Epoch 18  71.5% | batch:       123 of       172\t|\tloss: 0.786041\n",
      "Evaluating Epoch 18  72.1% | batch:       124 of       172\t|\tloss: 1.5196\n",
      "Evaluating Epoch 18  72.7% | batch:       125 of       172\t|\tloss: 0.89102\n",
      "Evaluating Epoch 18  73.3% | batch:       126 of       172\t|\tloss: 0.814359\n",
      "Evaluating Epoch 18  73.8% | batch:       127 of       172\t|\tloss: 0.517109\n",
      "Evaluating Epoch 18  74.4% | batch:       128 of       172\t|\tloss: 0.512996\n",
      "Evaluating Epoch 18  75.0% | batch:       129 of       172\t|\tloss: 0.596129\n",
      "Evaluating Epoch 18  75.6% | batch:       130 of       172\t|\tloss: 0.29299\n",
      "Evaluating Epoch 18  76.2% | batch:       131 of       172\t|\tloss: 0.682246\n",
      "Evaluating Epoch 18  76.7% | batch:       132 of       172\t|\tloss: 0.478334\n",
      "Evaluating Epoch 18  77.3% | batch:       133 of       172\t|\tloss: 0.131016\n",
      "Evaluating Epoch 18  77.9% | batch:       134 of       172\t|\tloss: 0.246828\n",
      "Evaluating Epoch 18  78.5% | batch:       135 of       172\t|\tloss: 0.157849\n",
      "Evaluating Epoch 18  79.1% | batch:       136 of       172\t|\tloss: 0.18305\n",
      "Evaluating Epoch 18  79.7% | batch:       137 of       172\t|\tloss: 0.135893\n",
      "Evaluating Epoch 18  80.2% | batch:       138 of       172\t|\tloss: 0.31624\n",
      "Evaluating Epoch 18  80.8% | batch:       139 of       172\t|\tloss: 0.168104\n",
      "Evaluating Epoch 18  81.4% | batch:       140 of       172\t|\tloss: 0.230202\n",
      "Evaluating Epoch 18  82.0% | batch:       141 of       172\t|\tloss: 0.175646\n",
      "Evaluating Epoch 18  82.6% | batch:       142 of       172\t|\tloss: 0.250951\n",
      "Evaluating Epoch 18  83.1% | batch:       143 of       172\t|\tloss: 0.19949\n",
      "Evaluating Epoch 18  83.7% | batch:       144 of       172\t|\tloss: 0.233661\n",
      "Evaluating Epoch 18  84.3% | batch:       145 of       172\t|\tloss: 0.119168\n",
      "Evaluating Epoch 18  84.9% | batch:       146 of       172\t|\tloss: 0.297107\n",
      "Evaluating Epoch 18  85.5% | batch:       147 of       172\t|\tloss: 0.149179\n",
      "Evaluating Epoch 18  86.0% | batch:       148 of       172\t|\tloss: 0.235657\n",
      "Evaluating Epoch 18  86.6% | batch:       149 of       172\t|\tloss: 0.111291\n",
      "Evaluating Epoch 18  87.2% | batch:       150 of       172\t|\tloss: 0.496436\n",
      "Evaluating Epoch 18  87.8% | batch:       151 of       172\t|\tloss: 0.925538\n",
      "Evaluating Epoch 18  88.4% | batch:       152 of       172\t|\tloss: 0.474781\n",
      "Evaluating Epoch 18  89.0% | batch:       153 of       172\t|\tloss: 0.562309\n",
      "Evaluating Epoch 18  89.5% | batch:       154 of       172\t|\tloss: 0.862614\n",
      "Evaluating Epoch 18  90.1% | batch:       155 of       172\t|\tloss: 0.327519\n",
      "Evaluating Epoch 18  90.7% | batch:       156 of       172\t|\tloss: 0.974517\n",
      "Evaluating Epoch 18  91.3% | batch:       157 of       172\t|\tloss: 1.02718\n",
      "Evaluating Epoch 18  91.9% | batch:       158 of       172\t|\tloss: 0.534316\n",
      "Evaluating Epoch 18  92.4% | batch:       159 of       172\t|\tloss: 1.43851\n",
      "Evaluating Epoch 18  93.0% | batch:       160 of       172\t|\tloss: 0.74622\n",
      "Evaluating Epoch 18  93.6% | batch:       161 of       172\t|\tloss: 1.94853\n",
      "Evaluating Epoch 18  94.2% | batch:       162 of       172\t|\tloss: 0.928651\n",
      "Evaluating Epoch 18  94.8% | batch:       163 of       172\t|\tloss: 0.617771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:08:15,529 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.970027446746826 seconds\n",
      "\n",
      "2023-05-25 22:08:15,530 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 3.991521483973453 seconds\n",
      "2023-05-25 22:08:15,531 | INFO : Avg batch val. time: 0.02320652025565961 seconds\n",
      "2023-05-25 22:08:15,531 | INFO : Avg sample val. time: 0.00018178810784594675 seconds\n",
      "2023-05-25 22:08:15,532 | INFO : Epoch 18 Validation Summary: epoch: 18.000000 | loss: 1.319080 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 18  95.3% | batch:       164 of       172\t|\tloss: 0.958093\n",
      "Evaluating Epoch 18  95.9% | batch:       165 of       172\t|\tloss: 0.696833\n",
      "Evaluating Epoch 18  96.5% | batch:       166 of       172\t|\tloss: 0.324591\n",
      "Evaluating Epoch 18  97.1% | batch:       167 of       172\t|\tloss: 1.18312\n",
      "Evaluating Epoch 18  97.7% | batch:       168 of       172\t|\tloss: 0.681865\n",
      "Evaluating Epoch 18  98.3% | batch:       169 of       172\t|\tloss: 0.623485\n",
      "Evaluating Epoch 18  98.8% | batch:       170 of       172\t|\tloss: 1.16446\n",
      "Evaluating Epoch 18  99.4% | batch:       171 of       172\t|\tloss: 0.924058\n",
      "\n",
      "Training Epoch 19   0.0% | batch:         0 of       686\t|\tloss: 2.07801\n",
      "Training Epoch 19   0.1% | batch:         1 of       686\t|\tloss: 1.87662\n",
      "Training Epoch 19   0.3% | batch:         2 of       686\t|\tloss: 1.77978\n",
      "Training Epoch 19   0.4% | batch:         3 of       686\t|\tloss: 2.84917\n",
      "Training Epoch 19   0.6% | batch:         4 of       686\t|\tloss: 2.72739\n",
      "Training Epoch 19   0.7% | batch:         5 of       686\t|\tloss: 2.47726\n",
      "Training Epoch 19   0.9% | batch:         6 of       686\t|\tloss: 2.2446\n",
      "Training Epoch 19   1.0% | batch:         7 of       686\t|\tloss: 3.56635\n",
      "Training Epoch 19   1.2% | batch:         8 of       686\t|\tloss: 2.38707\n",
      "Training Epoch 19   1.3% | batch:         9 of       686\t|\tloss: 2.17383\n",
      "Training Epoch 19   1.5% | batch:        10 of       686\t|\tloss: 2.36172\n",
      "Training Epoch 19   1.6% | batch:        11 of       686\t|\tloss: 2.33003\n",
      "Training Epoch 19   1.7% | batch:        12 of       686\t|\tloss: 2.2183\n",
      "Training Epoch 19   1.9% | batch:        13 of       686\t|\tloss: 2.02022\n",
      "Training Epoch 19   2.0% | batch:        14 of       686\t|\tloss: 2.63591\n",
      "Training Epoch 19   2.2% | batch:        15 of       686\t|\tloss: 2.33024\n",
      "Training Epoch 19   2.3% | batch:        16 of       686\t|\tloss: 2.28542\n",
      "Training Epoch 19   2.5% | batch:        17 of       686\t|\tloss: 2.0874\n",
      "Training Epoch 19   2.6% | batch:        18 of       686\t|\tloss: 1.80709\n",
      "Training Epoch 19   2.8% | batch:        19 of       686\t|\tloss: 2.62883\n",
      "Training Epoch 19   2.9% | batch:        20 of       686\t|\tloss: 1.98013\n",
      "Training Epoch 19   3.1% | batch:        21 of       686\t|\tloss: 2.15543\n",
      "Training Epoch 19   3.2% | batch:        22 of       686\t|\tloss: 1.89458\n",
      "Training Epoch 19   3.4% | batch:        23 of       686\t|\tloss: 2.04257\n",
      "Training Epoch 19   3.5% | batch:        24 of       686\t|\tloss: 1.83394\n",
      "Training Epoch 19   3.6% | batch:        25 of       686\t|\tloss: 2.28861\n",
      "Training Epoch 19   3.8% | batch:        26 of       686\t|\tloss: 1.50149\n",
      "Training Epoch 19   3.9% | batch:        27 of       686\t|\tloss: 1.32192\n",
      "Training Epoch 19   4.1% | batch:        28 of       686\t|\tloss: 1.45122\n",
      "Training Epoch 19   4.2% | batch:        29 of       686\t|\tloss: 1.70923\n",
      "Training Epoch 19   4.4% | batch:        30 of       686\t|\tloss: 2.25663\n",
      "Training Epoch 19   4.5% | batch:        31 of       686\t|\tloss: 2.09698\n",
      "Training Epoch 19   4.7% | batch:        32 of       686\t|\tloss: 2.13003\n",
      "Training Epoch 19   4.8% | batch:        33 of       686\t|\tloss: 1.59953\n",
      "Training Epoch 19   5.0% | batch:        34 of       686\t|\tloss: 1.9214\n",
      "Training Epoch 19   5.1% | batch:        35 of       686\t|\tloss: 2.39049\n",
      "Training Epoch 19   5.2% | batch:        36 of       686\t|\tloss: 2.17519\n",
      "Training Epoch 19   5.4% | batch:        37 of       686\t|\tloss: 2.29016\n",
      "Training Epoch 19   5.5% | batch:        38 of       686\t|\tloss: 2.01127\n",
      "Training Epoch 19   5.7% | batch:        39 of       686\t|\tloss: 2.00055\n",
      "Training Epoch 19   5.8% | batch:        40 of       686\t|\tloss: 1.88194\n",
      "Training Epoch 19   6.0% | batch:        41 of       686\t|\tloss: 1.67257\n",
      "Training Epoch 19   6.1% | batch:        42 of       686\t|\tloss: 2.04773\n",
      "Training Epoch 19   6.3% | batch:        43 of       686\t|\tloss: 2.37633\n",
      "Training Epoch 19   6.4% | batch:        44 of       686\t|\tloss: 1.71165\n",
      "Training Epoch 19   6.6% | batch:        45 of       686\t|\tloss: 2.10477\n",
      "Training Epoch 19   6.7% | batch:        46 of       686\t|\tloss: 2.60721\n",
      "Training Epoch 19   6.9% | batch:        47 of       686\t|\tloss: 1.9263\n",
      "Training Epoch 19   7.0% | batch:        48 of       686\t|\tloss: 2.10159\n",
      "Training Epoch 19   7.1% | batch:        49 of       686\t|\tloss: 2.12517\n",
      "Training Epoch 19   7.3% | batch:        50 of       686\t|\tloss: 3.05326\n",
      "Training Epoch 19   7.4% | batch:        51 of       686\t|\tloss: 2.16909\n",
      "Training Epoch 19   7.6% | batch:        52 of       686\t|\tloss: 2.19501\n",
      "Training Epoch 19   7.7% | batch:        53 of       686\t|\tloss: 2.15194\n",
      "Training Epoch 19   7.9% | batch:        54 of       686\t|\tloss: 1.65729\n",
      "Training Epoch 19   8.0% | batch:        55 of       686\t|\tloss: 1.7848\n",
      "Training Epoch 19   8.2% | batch:        56 of       686\t|\tloss: 1.66018\n",
      "Training Epoch 19   8.3% | batch:        57 of       686\t|\tloss: 2.25552\n",
      "Training Epoch 19   8.5% | batch:        58 of       686\t|\tloss: 2.16471\n",
      "Training Epoch 19   8.6% | batch:        59 of       686\t|\tloss: 2.97133\n",
      "Training Epoch 19   8.7% | batch:        60 of       686\t|\tloss: 2.41643\n",
      "Training Epoch 19   8.9% | batch:        61 of       686\t|\tloss: 1.69843\n",
      "Training Epoch 19   9.0% | batch:        62 of       686\t|\tloss: 1.75452\n",
      "Training Epoch 19   9.2% | batch:        63 of       686\t|\tloss: 1.8732\n",
      "Training Epoch 19   9.3% | batch:        64 of       686\t|\tloss: 1.97968\n",
      "Training Epoch 19   9.5% | batch:        65 of       686\t|\tloss: 2.28824\n",
      "Training Epoch 19   9.6% | batch:        66 of       686\t|\tloss: 1.68729\n",
      "Training Epoch 19   9.8% | batch:        67 of       686\t|\tloss: 2.15913\n",
      "Training Epoch 19   9.9% | batch:        68 of       686\t|\tloss: 2.59587\n",
      "Training Epoch 19  10.1% | batch:        69 of       686\t|\tloss: 1.34514\n",
      "Training Epoch 19  10.2% | batch:        70 of       686\t|\tloss: 2.17366\n",
      "Training Epoch 19  10.3% | batch:        71 of       686\t|\tloss: 1.92225\n",
      "Training Epoch 19  10.5% | batch:        72 of       686\t|\tloss: 2.20356\n",
      "Training Epoch 19  10.6% | batch:        73 of       686\t|\tloss: 1.68982\n",
      "Training Epoch 19  10.8% | batch:        74 of       686\t|\tloss: 2.11053\n",
      "Training Epoch 19  10.9% | batch:        75 of       686\t|\tloss: 1.62125\n",
      "Training Epoch 19  11.1% | batch:        76 of       686\t|\tloss: 2.54872\n",
      "Training Epoch 19  11.2% | batch:        77 of       686\t|\tloss: 2.02306\n",
      "Training Epoch 19  11.4% | batch:        78 of       686\t|\tloss: 2.53068\n",
      "Training Epoch 19  11.5% | batch:        79 of       686\t|\tloss: 1.89608\n",
      "Training Epoch 19  11.7% | batch:        80 of       686\t|\tloss: 2.88802\n",
      "Training Epoch 19  11.8% | batch:        81 of       686\t|\tloss: 2.43141\n",
      "Training Epoch 19  12.0% | batch:        82 of       686\t|\tloss: 1.79419\n",
      "Training Epoch 19  12.1% | batch:        83 of       686\t|\tloss: 1.8726\n",
      "Training Epoch 19  12.2% | batch:        84 of       686\t|\tloss: 1.72913\n",
      "Training Epoch 19  12.4% | batch:        85 of       686\t|\tloss: 2.50771\n",
      "Training Epoch 19  12.5% | batch:        86 of       686\t|\tloss: 1.78502\n",
      "Training Epoch 19  12.7% | batch:        87 of       686\t|\tloss: 2.21536\n",
      "Training Epoch 19  12.8% | batch:        88 of       686\t|\tloss: 1.88381\n",
      "Training Epoch 19  13.0% | batch:        89 of       686\t|\tloss: 1.8164\n",
      "Training Epoch 19  13.1% | batch:        90 of       686\t|\tloss: 2.04247\n",
      "Training Epoch 19  13.3% | batch:        91 of       686\t|\tloss: 2.26857\n",
      "Training Epoch 19  13.4% | batch:        92 of       686\t|\tloss: 2.23717\n",
      "Training Epoch 19  13.6% | batch:        93 of       686\t|\tloss: 1.878\n",
      "Training Epoch 19  13.7% | batch:        94 of       686\t|\tloss: 1.60757\n",
      "Training Epoch 19  13.8% | batch:        95 of       686\t|\tloss: 1.42789\n",
      "Training Epoch 19  14.0% | batch:        96 of       686\t|\tloss: 2.11739\n",
      "Training Epoch 19  14.1% | batch:        97 of       686\t|\tloss: 2.23269\n",
      "Training Epoch 19  14.3% | batch:        98 of       686\t|\tloss: 2.11881\n",
      "Training Epoch 19  14.4% | batch:        99 of       686\t|\tloss: 1.86377\n",
      "Training Epoch 19  14.6% | batch:       100 of       686\t|\tloss: 2.0134\n",
      "Training Epoch 19  14.7% | batch:       101 of       686\t|\tloss: 1.88199\n",
      "Training Epoch 19  14.9% | batch:       102 of       686\t|\tloss: 2.24346\n",
      "Training Epoch 19  15.0% | batch:       103 of       686\t|\tloss: 1.62554\n",
      "Training Epoch 19  15.2% | batch:       104 of       686\t|\tloss: 1.94449\n",
      "Training Epoch 19  15.3% | batch:       105 of       686\t|\tloss: 1.79286\n",
      "Training Epoch 19  15.5% | batch:       106 of       686\t|\tloss: 2.09399\n",
      "Training Epoch 19  15.6% | batch:       107 of       686\t|\tloss: 2.07348\n",
      "Training Epoch 19  15.7% | batch:       108 of       686\t|\tloss: 2.15788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 19  15.9% | batch:       109 of       686\t|\tloss: 1.75372\n",
      "Training Epoch 19  16.0% | batch:       110 of       686\t|\tloss: 1.80472\n",
      "Training Epoch 19  16.2% | batch:       111 of       686\t|\tloss: 2.07593\n",
      "Training Epoch 19  16.3% | batch:       112 of       686\t|\tloss: 1.80676\n",
      "Training Epoch 19  16.5% | batch:       113 of       686\t|\tloss: 1.85692\n",
      "Training Epoch 19  16.6% | batch:       114 of       686\t|\tloss: 1.70168\n",
      "Training Epoch 19  16.8% | batch:       115 of       686\t|\tloss: 1.91782\n",
      "Training Epoch 19  16.9% | batch:       116 of       686\t|\tloss: 1.91978\n",
      "Training Epoch 19  17.1% | batch:       117 of       686\t|\tloss: 2.50908\n",
      "Training Epoch 19  17.2% | batch:       118 of       686\t|\tloss: 2.28229\n",
      "Training Epoch 19  17.3% | batch:       119 of       686\t|\tloss: 2.10696\n",
      "Training Epoch 19  17.5% | batch:       120 of       686\t|\tloss: 2.91564\n",
      "Training Epoch 19  17.6% | batch:       121 of       686\t|\tloss: 1.8274\n",
      "Training Epoch 19  17.8% | batch:       122 of       686\t|\tloss: 1.89262\n",
      "Training Epoch 19  17.9% | batch:       123 of       686\t|\tloss: 1.82929\n",
      "Training Epoch 19  18.1% | batch:       124 of       686\t|\tloss: 1.90709\n",
      "Training Epoch 19  18.2% | batch:       125 of       686\t|\tloss: 1.94034\n",
      "Training Epoch 19  18.4% | batch:       126 of       686\t|\tloss: 3.09907\n",
      "Training Epoch 19  18.5% | batch:       127 of       686\t|\tloss: 2.67693\n",
      "Training Epoch 19  18.7% | batch:       128 of       686\t|\tloss: 1.98909\n",
      "Training Epoch 19  18.8% | batch:       129 of       686\t|\tloss: 2.02979\n",
      "Training Epoch 19  19.0% | batch:       130 of       686\t|\tloss: 1.71042\n",
      "Training Epoch 19  19.1% | batch:       131 of       686\t|\tloss: 1.87956\n",
      "Training Epoch 19  19.2% | batch:       132 of       686\t|\tloss: 2.23526\n",
      "Training Epoch 19  19.4% | batch:       133 of       686\t|\tloss: 2.06546\n",
      "Training Epoch 19  19.5% | batch:       134 of       686\t|\tloss: 1.79855\n",
      "Training Epoch 19  19.7% | batch:       135 of       686\t|\tloss: 1.70801\n",
      "Training Epoch 19  19.8% | batch:       136 of       686\t|\tloss: 1.59176\n",
      "Training Epoch 19  20.0% | batch:       137 of       686\t|\tloss: 1.89576\n",
      "Training Epoch 19  20.1% | batch:       138 of       686\t|\tloss: 2.01551\n",
      "Training Epoch 19  20.3% | batch:       139 of       686\t|\tloss: 1.69898\n",
      "Training Epoch 19  20.4% | batch:       140 of       686\t|\tloss: 2.68878\n",
      "Training Epoch 19  20.6% | batch:       141 of       686\t|\tloss: 2.11506\n",
      "Training Epoch 19  20.7% | batch:       142 of       686\t|\tloss: 2.27936\n",
      "Training Epoch 19  20.8% | batch:       143 of       686\t|\tloss: 1.6133\n",
      "Training Epoch 19  21.0% | batch:       144 of       686\t|\tloss: 2.65005\n",
      "Training Epoch 19  21.1% | batch:       145 of       686\t|\tloss: 2.73985\n",
      "Training Epoch 19  21.3% | batch:       146 of       686\t|\tloss: 2.39042\n",
      "Training Epoch 19  21.4% | batch:       147 of       686\t|\tloss: 1.57491\n",
      "Training Epoch 19  21.6% | batch:       148 of       686\t|\tloss: 1.68961\n",
      "Training Epoch 19  21.7% | batch:       149 of       686\t|\tloss: 2.55274\n",
      "Training Epoch 19  21.9% | batch:       150 of       686\t|\tloss: 3.1414\n",
      "Training Epoch 19  22.0% | batch:       151 of       686\t|\tloss: 2.49737\n",
      "Training Epoch 19  22.2% | batch:       152 of       686\t|\tloss: 1.73773\n",
      "Training Epoch 19  22.3% | batch:       153 of       686\t|\tloss: 2.1957\n",
      "Training Epoch 19  22.4% | batch:       154 of       686\t|\tloss: 2.41775\n",
      "Training Epoch 19  22.6% | batch:       155 of       686\t|\tloss: 1.92968\n",
      "Training Epoch 19  22.7% | batch:       156 of       686\t|\tloss: 2.64317\n",
      "Training Epoch 19  22.9% | batch:       157 of       686\t|\tloss: 1.96056\n",
      "Training Epoch 19  23.0% | batch:       158 of       686\t|\tloss: 1.78318\n",
      "Training Epoch 19  23.2% | batch:       159 of       686\t|\tloss: 1.51493\n",
      "Training Epoch 19  23.3% | batch:       160 of       686\t|\tloss: 2.3357\n",
      "Training Epoch 19  23.5% | batch:       161 of       686\t|\tloss: 2.05469\n",
      "Training Epoch 19  23.6% | batch:       162 of       686\t|\tloss: 2.21838\n",
      "Training Epoch 19  23.8% | batch:       163 of       686\t|\tloss: 2.34275\n",
      "Training Epoch 19  23.9% | batch:       164 of       686\t|\tloss: 2.30245\n",
      "Training Epoch 19  24.1% | batch:       165 of       686\t|\tloss: 2.386\n",
      "Training Epoch 19  24.2% | batch:       166 of       686\t|\tloss: 1.6421\n",
      "Training Epoch 19  24.3% | batch:       167 of       686\t|\tloss: 2.27426\n",
      "Training Epoch 19  24.5% | batch:       168 of       686\t|\tloss: 1.84602\n",
      "Training Epoch 19  24.6% | batch:       169 of       686\t|\tloss: 2.06768\n",
      "Training Epoch 19  24.8% | batch:       170 of       686\t|\tloss: 2.13142\n",
      "Training Epoch 19  24.9% | batch:       171 of       686\t|\tloss: 2.14501\n",
      "Training Epoch 19  25.1% | batch:       172 of       686\t|\tloss: 1.66729\n",
      "Training Epoch 19  25.2% | batch:       173 of       686\t|\tloss: 2.39214\n",
      "Training Epoch 19  25.4% | batch:       174 of       686\t|\tloss: 1.84442\n",
      "Training Epoch 19  25.5% | batch:       175 of       686\t|\tloss: 2.37099\n",
      "Training Epoch 19  25.7% | batch:       176 of       686\t|\tloss: 1.92631\n",
      "Training Epoch 19  25.8% | batch:       177 of       686\t|\tloss: 3.96658\n",
      "Training Epoch 19  25.9% | batch:       178 of       686\t|\tloss: 2.35609\n",
      "Training Epoch 19  26.1% | batch:       179 of       686\t|\tloss: 2.02653\n",
      "Training Epoch 19  26.2% | batch:       180 of       686\t|\tloss: 1.94516\n",
      "Training Epoch 19  26.4% | batch:       181 of       686\t|\tloss: 1.65493\n",
      "Training Epoch 19  26.5% | batch:       182 of       686\t|\tloss: 1.70949\n",
      "Training Epoch 19  26.7% | batch:       183 of       686\t|\tloss: 2.12092\n",
      "Training Epoch 19  26.8% | batch:       184 of       686\t|\tloss: 2.21733\n",
      "Training Epoch 19  27.0% | batch:       185 of       686\t|\tloss: 2.14262\n",
      "Training Epoch 19  27.1% | batch:       186 of       686\t|\tloss: 2.12364\n",
      "Training Epoch 19  27.3% | batch:       187 of       686\t|\tloss: 1.87482\n",
      "Training Epoch 19  27.4% | batch:       188 of       686\t|\tloss: 1.86686\n",
      "Training Epoch 19  27.6% | batch:       189 of       686\t|\tloss: 2.46839\n",
      "Training Epoch 19  27.7% | batch:       190 of       686\t|\tloss: 2.30111\n",
      "Training Epoch 19  27.8% | batch:       191 of       686\t|\tloss: 2.06218\n",
      "Training Epoch 19  28.0% | batch:       192 of       686\t|\tloss: 1.95174\n",
      "Training Epoch 19  28.1% | batch:       193 of       686\t|\tloss: 2.32048\n",
      "Training Epoch 19  28.3% | batch:       194 of       686\t|\tloss: 1.88327\n",
      "Training Epoch 19  28.4% | batch:       195 of       686\t|\tloss: 2.42353\n",
      "Training Epoch 19  28.6% | batch:       196 of       686\t|\tloss: 1.8872\n",
      "Training Epoch 19  28.7% | batch:       197 of       686\t|\tloss: 2.69078\n",
      "Training Epoch 19  28.9% | batch:       198 of       686\t|\tloss: 1.82011\n",
      "Training Epoch 19  29.0% | batch:       199 of       686\t|\tloss: 1.84088\n",
      "Training Epoch 19  29.2% | batch:       200 of       686\t|\tloss: 1.87042\n",
      "Training Epoch 19  29.3% | batch:       201 of       686\t|\tloss: 2.34893\n",
      "Training Epoch 19  29.4% | batch:       202 of       686\t|\tloss: 2.05514\n",
      "Training Epoch 19  29.6% | batch:       203 of       686\t|\tloss: 2.02543\n",
      "Training Epoch 19  29.7% | batch:       204 of       686\t|\tloss: 1.87471\n",
      "Training Epoch 19  29.9% | batch:       205 of       686\t|\tloss: 2.54016\n",
      "Training Epoch 19  30.0% | batch:       206 of       686\t|\tloss: 2.59537\n",
      "Training Epoch 19  30.2% | batch:       207 of       686\t|\tloss: 2.17144\n",
      "Training Epoch 19  30.3% | batch:       208 of       686\t|\tloss: 2.25306\n",
      "Training Epoch 19  30.5% | batch:       209 of       686\t|\tloss: 2.08363\n",
      "Training Epoch 19  30.6% | batch:       210 of       686\t|\tloss: 2.25458\n",
      "Training Epoch 19  30.8% | batch:       211 of       686\t|\tloss: 1.89188\n",
      "Training Epoch 19  30.9% | batch:       212 of       686\t|\tloss: 1.70038\n",
      "Training Epoch 19  31.0% | batch:       213 of       686\t|\tloss: 2.12258\n",
      "Training Epoch 19  31.2% | batch:       214 of       686\t|\tloss: 1.845\n",
      "Training Epoch 19  31.3% | batch:       215 of       686\t|\tloss: 1.77075\n",
      "Training Epoch 19  31.5% | batch:       216 of       686\t|\tloss: 2.46461\n",
      "Training Epoch 19  31.6% | batch:       217 of       686\t|\tloss: 2.05192\n",
      "Training Epoch 19  31.8% | batch:       218 of       686\t|\tloss: 2.38025\n",
      "Training Epoch 19  31.9% | batch:       219 of       686\t|\tloss: 1.85529\n",
      "Training Epoch 19  32.1% | batch:       220 of       686\t|\tloss: 2.04571\n",
      "Training Epoch 19  32.2% | batch:       221 of       686\t|\tloss: 1.74542\n",
      "Training Epoch 19  32.4% | batch:       222 of       686\t|\tloss: 2.28239\n",
      "Training Epoch 19  32.5% | batch:       223 of       686\t|\tloss: 2.31051\n",
      "Training Epoch 19  32.7% | batch:       224 of       686\t|\tloss: 1.97485\n",
      "Training Epoch 19  32.8% | batch:       225 of       686\t|\tloss: 2.91476\n",
      "Training Epoch 19  32.9% | batch:       226 of       686\t|\tloss: 2.09054\n",
      "Training Epoch 19  33.1% | batch:       227 of       686\t|\tloss: 2.01724\n",
      "Training Epoch 19  33.2% | batch:       228 of       686\t|\tloss: 3.35091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 19  33.4% | batch:       229 of       686\t|\tloss: 1.56279\n",
      "Training Epoch 19  33.5% | batch:       230 of       686\t|\tloss: 2.20978\n",
      "Training Epoch 19  33.7% | batch:       231 of       686\t|\tloss: 1.96268\n",
      "Training Epoch 19  33.8% | batch:       232 of       686\t|\tloss: 1.94401\n",
      "Training Epoch 19  34.0% | batch:       233 of       686\t|\tloss: 1.94678\n",
      "Training Epoch 19  34.1% | batch:       234 of       686\t|\tloss: 2.20907\n",
      "Training Epoch 19  34.3% | batch:       235 of       686\t|\tloss: 1.55254\n",
      "Training Epoch 19  34.4% | batch:       236 of       686\t|\tloss: 1.8147\n",
      "Training Epoch 19  34.5% | batch:       237 of       686\t|\tloss: 1.9612\n",
      "Training Epoch 19  34.7% | batch:       238 of       686\t|\tloss: 2.5238\n",
      "Training Epoch 19  34.8% | batch:       239 of       686\t|\tloss: 1.85392\n",
      "Training Epoch 19  35.0% | batch:       240 of       686\t|\tloss: 1.63826\n",
      "Training Epoch 19  35.1% | batch:       241 of       686\t|\tloss: 1.64506\n",
      "Training Epoch 19  35.3% | batch:       242 of       686\t|\tloss: 1.83366\n",
      "Training Epoch 19  35.4% | batch:       243 of       686\t|\tloss: 1.82941\n",
      "Training Epoch 19  35.6% | batch:       244 of       686\t|\tloss: 2.38874\n",
      "Training Epoch 19  35.7% | batch:       245 of       686\t|\tloss: 2.11526\n",
      "Training Epoch 19  35.9% | batch:       246 of       686\t|\tloss: 1.81766\n",
      "Training Epoch 19  36.0% | batch:       247 of       686\t|\tloss: 2.03433\n",
      "Training Epoch 19  36.2% | batch:       248 of       686\t|\tloss: 2.02146\n",
      "Training Epoch 19  36.3% | batch:       249 of       686\t|\tloss: 1.65653\n",
      "Training Epoch 19  36.4% | batch:       250 of       686\t|\tloss: 2.09199\n",
      "Training Epoch 19  36.6% | batch:       251 of       686\t|\tloss: 2.22854\n",
      "Training Epoch 19  36.7% | batch:       252 of       686\t|\tloss: 2.07716\n",
      "Training Epoch 19  36.9% | batch:       253 of       686\t|\tloss: 1.96833\n",
      "Training Epoch 19  37.0% | batch:       254 of       686\t|\tloss: 1.88435\n",
      "Training Epoch 19  37.2% | batch:       255 of       686\t|\tloss: 1.85842\n",
      "Training Epoch 19  37.3% | batch:       256 of       686\t|\tloss: 1.6617\n",
      "Training Epoch 19  37.5% | batch:       257 of       686\t|\tloss: 2.18481\n",
      "Training Epoch 19  37.6% | batch:       258 of       686\t|\tloss: 1.74518\n",
      "Training Epoch 19  37.8% | batch:       259 of       686\t|\tloss: 1.87694\n",
      "Training Epoch 19  37.9% | batch:       260 of       686\t|\tloss: 1.82019\n",
      "Training Epoch 19  38.0% | batch:       261 of       686\t|\tloss: 1.51889\n",
      "Training Epoch 19  38.2% | batch:       262 of       686\t|\tloss: 1.95309\n",
      "Training Epoch 19  38.3% | batch:       263 of       686\t|\tloss: 1.83805\n",
      "Training Epoch 19  38.5% | batch:       264 of       686\t|\tloss: 2.01135\n",
      "Training Epoch 19  38.6% | batch:       265 of       686\t|\tloss: 1.79806\n",
      "Training Epoch 19  38.8% | batch:       266 of       686\t|\tloss: 1.50895\n",
      "Training Epoch 19  38.9% | batch:       267 of       686\t|\tloss: 1.64571\n",
      "Training Epoch 19  39.1% | batch:       268 of       686\t|\tloss: 1.87979\n",
      "Training Epoch 19  39.2% | batch:       269 of       686\t|\tloss: 1.88772\n",
      "Training Epoch 19  39.4% | batch:       270 of       686\t|\tloss: 2.02632\n",
      "Training Epoch 19  39.5% | batch:       271 of       686\t|\tloss: 1.64727\n",
      "Training Epoch 19  39.7% | batch:       272 of       686\t|\tloss: 2.30918\n",
      "Training Epoch 19  39.8% | batch:       273 of       686\t|\tloss: 2.33664\n",
      "Training Epoch 19  39.9% | batch:       274 of       686\t|\tloss: 2.33042\n",
      "Training Epoch 19  40.1% | batch:       275 of       686\t|\tloss: 1.80271\n",
      "Training Epoch 19  40.2% | batch:       276 of       686\t|\tloss: 2.10123\n",
      "Training Epoch 19  40.4% | batch:       277 of       686\t|\tloss: 3.16055\n",
      "Training Epoch 19  40.5% | batch:       278 of       686\t|\tloss: 2.17662\n",
      "Training Epoch 19  40.7% | batch:       279 of       686\t|\tloss: 2.04292\n",
      "Training Epoch 19  40.8% | batch:       280 of       686\t|\tloss: 1.53202\n",
      "Training Epoch 19  41.0% | batch:       281 of       686\t|\tloss: 1.56084\n",
      "Training Epoch 19  41.1% | batch:       282 of       686\t|\tloss: 2.0869\n",
      "Training Epoch 19  41.3% | batch:       283 of       686\t|\tloss: 1.69684\n",
      "Training Epoch 19  41.4% | batch:       284 of       686\t|\tloss: 1.98014\n",
      "Training Epoch 19  41.5% | batch:       285 of       686\t|\tloss: 2.67528\n",
      "Training Epoch 19  41.7% | batch:       286 of       686\t|\tloss: 1.82988\n",
      "Training Epoch 19  41.8% | batch:       287 of       686\t|\tloss: 1.84754\n",
      "Training Epoch 19  42.0% | batch:       288 of       686\t|\tloss: 1.80511\n",
      "Training Epoch 19  42.1% | batch:       289 of       686\t|\tloss: 1.91787\n",
      "Training Epoch 19  42.3% | batch:       290 of       686\t|\tloss: 1.75766\n",
      "Training Epoch 19  42.4% | batch:       291 of       686\t|\tloss: 2.17624\n",
      "Training Epoch 19  42.6% | batch:       292 of       686\t|\tloss: 2.20504\n",
      "Training Epoch 19  42.7% | batch:       293 of       686\t|\tloss: 1.34653\n",
      "Training Epoch 19  42.9% | batch:       294 of       686\t|\tloss: 1.66106\n",
      "Training Epoch 19  43.0% | batch:       295 of       686\t|\tloss: 1.7617\n",
      "Training Epoch 19  43.1% | batch:       296 of       686\t|\tloss: 1.91021\n",
      "Training Epoch 19  43.3% | batch:       297 of       686\t|\tloss: 1.76963\n",
      "Training Epoch 19  43.4% | batch:       298 of       686\t|\tloss: 2.10871\n",
      "Training Epoch 19  43.6% | batch:       299 of       686\t|\tloss: 2.06955\n",
      "Training Epoch 19  43.7% | batch:       300 of       686\t|\tloss: 1.99046\n",
      "Training Epoch 19  43.9% | batch:       301 of       686\t|\tloss: 2.25817\n",
      "Training Epoch 19  44.0% | batch:       302 of       686\t|\tloss: 2.08075\n",
      "Training Epoch 19  44.2% | batch:       303 of       686\t|\tloss: 2.16013\n",
      "Training Epoch 19  44.3% | batch:       304 of       686\t|\tloss: 1.8077\n",
      "Training Epoch 19  44.5% | batch:       305 of       686\t|\tloss: 1.91099\n",
      "Training Epoch 19  44.6% | batch:       306 of       686\t|\tloss: 2.49496\n",
      "Training Epoch 19  44.8% | batch:       307 of       686\t|\tloss: 1.92568\n",
      "Training Epoch 19  44.9% | batch:       308 of       686\t|\tloss: 1.54694\n",
      "Training Epoch 19  45.0% | batch:       309 of       686\t|\tloss: 1.79456\n",
      "Training Epoch 19  45.2% | batch:       310 of       686\t|\tloss: 2.24432\n",
      "Training Epoch 19  45.3% | batch:       311 of       686\t|\tloss: 2.0791\n",
      "Training Epoch 19  45.5% | batch:       312 of       686\t|\tloss: 1.62661\n",
      "Training Epoch 19  45.6% | batch:       313 of       686\t|\tloss: 2.64852\n",
      "Training Epoch 19  45.8% | batch:       314 of       686\t|\tloss: 1.89658\n",
      "Training Epoch 19  45.9% | batch:       315 of       686\t|\tloss: 2.46992\n",
      "Training Epoch 19  46.1% | batch:       316 of       686\t|\tloss: 1.80116\n",
      "Training Epoch 19  46.2% | batch:       317 of       686\t|\tloss: 1.88442\n",
      "Training Epoch 19  46.4% | batch:       318 of       686\t|\tloss: 1.93962\n",
      "Training Epoch 19  46.5% | batch:       319 of       686\t|\tloss: 2.01546\n",
      "Training Epoch 19  46.6% | batch:       320 of       686\t|\tloss: 2.0451\n",
      "Training Epoch 19  46.8% | batch:       321 of       686\t|\tloss: 1.79376\n",
      "Training Epoch 19  46.9% | batch:       322 of       686\t|\tloss: 2.0402\n",
      "Training Epoch 19  47.1% | batch:       323 of       686\t|\tloss: 1.43681\n",
      "Training Epoch 19  47.2% | batch:       324 of       686\t|\tloss: 1.59052\n",
      "Training Epoch 19  47.4% | batch:       325 of       686\t|\tloss: 1.87097\n",
      "Training Epoch 19  47.5% | batch:       326 of       686\t|\tloss: 2.1815\n",
      "Training Epoch 19  47.7% | batch:       327 of       686\t|\tloss: 2.39607\n",
      "Training Epoch 19  47.8% | batch:       328 of       686\t|\tloss: 3.07761\n",
      "Training Epoch 19  48.0% | batch:       329 of       686\t|\tloss: 2.19545\n",
      "Training Epoch 19  48.1% | batch:       330 of       686\t|\tloss: 1.66338\n",
      "Training Epoch 19  48.3% | batch:       331 of       686\t|\tloss: 1.9674\n",
      "Training Epoch 19  48.4% | batch:       332 of       686\t|\tloss: 1.92946\n",
      "Training Epoch 19  48.5% | batch:       333 of       686\t|\tloss: 1.8354\n",
      "Training Epoch 19  48.7% | batch:       334 of       686\t|\tloss: 1.94342\n",
      "Training Epoch 19  48.8% | batch:       335 of       686\t|\tloss: 1.7271\n",
      "Training Epoch 19  49.0% | batch:       336 of       686\t|\tloss: 1.53319\n",
      "Training Epoch 19  49.1% | batch:       337 of       686\t|\tloss: 2.37984\n",
      "Training Epoch 19  49.3% | batch:       338 of       686\t|\tloss: 1.79418\n",
      "Training Epoch 19  49.4% | batch:       339 of       686\t|\tloss: 2.058\n",
      "Training Epoch 19  49.6% | batch:       340 of       686\t|\tloss: 1.61415\n",
      "Training Epoch 19  49.7% | batch:       341 of       686\t|\tloss: 1.73436\n",
      "Training Epoch 19  49.9% | batch:       342 of       686\t|\tloss: 1.67764\n",
      "Training Epoch 19  50.0% | batch:       343 of       686\t|\tloss: 1.51373\n",
      "Training Epoch 19  50.1% | batch:       344 of       686\t|\tloss: 2.15305\n",
      "Training Epoch 19  50.3% | batch:       345 of       686\t|\tloss: 1.78561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 19  50.4% | batch:       346 of       686\t|\tloss: 2.13006\n",
      "Training Epoch 19  50.6% | batch:       347 of       686\t|\tloss: 2.09804\n",
      "Training Epoch 19  50.7% | batch:       348 of       686\t|\tloss: 1.77695\n",
      "Training Epoch 19  50.9% | batch:       349 of       686\t|\tloss: 1.54984\n",
      "Training Epoch 19  51.0% | batch:       350 of       686\t|\tloss: 2.49439\n",
      "Training Epoch 19  51.2% | batch:       351 of       686\t|\tloss: 1.30681\n",
      "Training Epoch 19  51.3% | batch:       352 of       686\t|\tloss: 1.56037\n",
      "Training Epoch 19  51.5% | batch:       353 of       686\t|\tloss: 1.74881\n",
      "Training Epoch 19  51.6% | batch:       354 of       686\t|\tloss: 1.75855\n",
      "Training Epoch 19  51.7% | batch:       355 of       686\t|\tloss: 1.9799\n",
      "Training Epoch 19  51.9% | batch:       356 of       686\t|\tloss: 2.11893\n",
      "Training Epoch 19  52.0% | batch:       357 of       686\t|\tloss: 2.18321\n",
      "Training Epoch 19  52.2% | batch:       358 of       686\t|\tloss: 1.83606\n",
      "Training Epoch 19  52.3% | batch:       359 of       686\t|\tloss: 1.66532\n",
      "Training Epoch 19  52.5% | batch:       360 of       686\t|\tloss: 1.78184\n",
      "Training Epoch 19  52.6% | batch:       361 of       686\t|\tloss: 1.67495\n",
      "Training Epoch 19  52.8% | batch:       362 of       686\t|\tloss: 2.20781\n",
      "Training Epoch 19  52.9% | batch:       363 of       686\t|\tloss: 1.82909\n",
      "Training Epoch 19  53.1% | batch:       364 of       686\t|\tloss: 1.70324\n",
      "Training Epoch 19  53.2% | batch:       365 of       686\t|\tloss: 2.38012\n",
      "Training Epoch 19  53.4% | batch:       366 of       686\t|\tloss: 2.64432\n",
      "Training Epoch 19  53.5% | batch:       367 of       686\t|\tloss: 2.18544\n",
      "Training Epoch 19  53.6% | batch:       368 of       686\t|\tloss: 2.87015\n",
      "Training Epoch 19  53.8% | batch:       369 of       686\t|\tloss: 1.61481\n",
      "Training Epoch 19  53.9% | batch:       370 of       686\t|\tloss: 1.55377\n",
      "Training Epoch 19  54.1% | batch:       371 of       686\t|\tloss: 1.7843\n",
      "Training Epoch 19  54.2% | batch:       372 of       686\t|\tloss: 1.54058\n",
      "Training Epoch 19  54.4% | batch:       373 of       686\t|\tloss: 1.89418\n",
      "Training Epoch 19  54.5% | batch:       374 of       686\t|\tloss: 1.78487\n",
      "Training Epoch 19  54.7% | batch:       375 of       686\t|\tloss: 2.51584\n",
      "Training Epoch 19  54.8% | batch:       376 of       686\t|\tloss: 2.35329\n",
      "Training Epoch 19  55.0% | batch:       377 of       686\t|\tloss: 1.55412\n",
      "Training Epoch 19  55.1% | batch:       378 of       686\t|\tloss: 1.67859\n",
      "Training Epoch 19  55.2% | batch:       379 of       686\t|\tloss: 1.95583\n",
      "Training Epoch 19  55.4% | batch:       380 of       686\t|\tloss: 2.20131\n",
      "Training Epoch 19  55.5% | batch:       381 of       686\t|\tloss: 1.62948\n",
      "Training Epoch 19  55.7% | batch:       382 of       686\t|\tloss: 2.13524\n",
      "Training Epoch 19  55.8% | batch:       383 of       686\t|\tloss: 2.36917\n",
      "Training Epoch 19  56.0% | batch:       384 of       686\t|\tloss: 1.6096\n",
      "Training Epoch 19  56.1% | batch:       385 of       686\t|\tloss: 2.05249\n",
      "Training Epoch 19  56.3% | batch:       386 of       686\t|\tloss: 1.41429\n",
      "Training Epoch 19  56.4% | batch:       387 of       686\t|\tloss: 1.66748\n",
      "Training Epoch 19  56.6% | batch:       388 of       686\t|\tloss: 2.01462\n",
      "Training Epoch 19  56.7% | batch:       389 of       686\t|\tloss: 1.94449\n",
      "Training Epoch 19  56.9% | batch:       390 of       686\t|\tloss: 2.38312\n",
      "Training Epoch 19  57.0% | batch:       391 of       686\t|\tloss: 2.03651\n",
      "Training Epoch 19  57.1% | batch:       392 of       686\t|\tloss: 1.7624\n",
      "Training Epoch 19  57.3% | batch:       393 of       686\t|\tloss: 2.35753\n",
      "Training Epoch 19  57.4% | batch:       394 of       686\t|\tloss: 1.95322\n",
      "Training Epoch 19  57.6% | batch:       395 of       686\t|\tloss: 1.90311\n",
      "Training Epoch 19  57.7% | batch:       396 of       686\t|\tloss: 1.92747\n",
      "Training Epoch 19  57.9% | batch:       397 of       686\t|\tloss: 1.7913\n",
      "Training Epoch 19  58.0% | batch:       398 of       686\t|\tloss: 1.71609\n",
      "Training Epoch 19  58.2% | batch:       399 of       686\t|\tloss: 2.28256\n",
      "Training Epoch 19  58.3% | batch:       400 of       686\t|\tloss: 1.73113\n",
      "Training Epoch 19  58.5% | batch:       401 of       686\t|\tloss: 1.65456\n",
      "Training Epoch 19  58.6% | batch:       402 of       686\t|\tloss: 2.51404\n",
      "Training Epoch 19  58.7% | batch:       403 of       686\t|\tloss: 1.38868\n",
      "Training Epoch 19  58.9% | batch:       404 of       686\t|\tloss: 2.26932\n",
      "Training Epoch 19  59.0% | batch:       405 of       686\t|\tloss: 1.63884\n",
      "Training Epoch 19  59.2% | batch:       406 of       686\t|\tloss: 1.55826\n",
      "Training Epoch 19  59.3% | batch:       407 of       686\t|\tloss: 1.4567\n",
      "Training Epoch 19  59.5% | batch:       408 of       686\t|\tloss: 2.48262\n",
      "Training Epoch 19  59.6% | batch:       409 of       686\t|\tloss: 1.49163\n",
      "Training Epoch 19  59.8% | batch:       410 of       686\t|\tloss: 2.16543\n",
      "Training Epoch 19  59.9% | batch:       411 of       686\t|\tloss: 2.10133\n",
      "Training Epoch 19  60.1% | batch:       412 of       686\t|\tloss: 1.81386\n",
      "Training Epoch 19  60.2% | batch:       413 of       686\t|\tloss: 1.72076\n",
      "Training Epoch 19  60.3% | batch:       414 of       686\t|\tloss: 1.57409\n",
      "Training Epoch 19  60.5% | batch:       415 of       686\t|\tloss: 2.03063\n",
      "Training Epoch 19  60.6% | batch:       416 of       686\t|\tloss: 2.26917\n",
      "Training Epoch 19  60.8% | batch:       417 of       686\t|\tloss: 2.13035\n",
      "Training Epoch 19  60.9% | batch:       418 of       686\t|\tloss: 1.95206\n",
      "Training Epoch 19  61.1% | batch:       419 of       686\t|\tloss: 1.26554\n",
      "Training Epoch 19  61.2% | batch:       420 of       686\t|\tloss: 1.38562\n",
      "Training Epoch 19  61.4% | batch:       421 of       686\t|\tloss: 1.86842\n",
      "Training Epoch 19  61.5% | batch:       422 of       686\t|\tloss: 1.7613\n",
      "Training Epoch 19  61.7% | batch:       423 of       686\t|\tloss: 2.09787\n",
      "Training Epoch 19  61.8% | batch:       424 of       686\t|\tloss: 1.9632\n",
      "Training Epoch 19  62.0% | batch:       425 of       686\t|\tloss: 1.67278\n",
      "Training Epoch 19  62.1% | batch:       426 of       686\t|\tloss: 1.55462\n",
      "Training Epoch 19  62.2% | batch:       427 of       686\t|\tloss: 2.21266\n",
      "Training Epoch 19  62.4% | batch:       428 of       686\t|\tloss: 2.68071\n",
      "Training Epoch 19  62.5% | batch:       429 of       686\t|\tloss: 1.56771\n",
      "Training Epoch 19  62.7% | batch:       430 of       686\t|\tloss: 1.76412\n",
      "Training Epoch 19  62.8% | batch:       431 of       686\t|\tloss: 1.74947\n",
      "Training Epoch 19  63.0% | batch:       432 of       686\t|\tloss: 2.07536\n",
      "Training Epoch 19  63.1% | batch:       433 of       686\t|\tloss: 1.81946\n",
      "Training Epoch 19  63.3% | batch:       434 of       686\t|\tloss: 1.37397\n",
      "Training Epoch 19  63.4% | batch:       435 of       686\t|\tloss: 1.75636\n",
      "Training Epoch 19  63.6% | batch:       436 of       686\t|\tloss: 1.92697\n",
      "Training Epoch 19  63.7% | batch:       437 of       686\t|\tloss: 1.97961\n",
      "Training Epoch 19  63.8% | batch:       438 of       686\t|\tloss: 2.48489\n",
      "Training Epoch 19  64.0% | batch:       439 of       686\t|\tloss: 2.03632\n",
      "Training Epoch 19  64.1% | batch:       440 of       686\t|\tloss: 2.69301\n",
      "Training Epoch 19  64.3% | batch:       441 of       686\t|\tloss: 1.79102\n",
      "Training Epoch 19  64.4% | batch:       442 of       686\t|\tloss: 1.93963\n",
      "Training Epoch 19  64.6% | batch:       443 of       686\t|\tloss: 2.22382\n",
      "Training Epoch 19  64.7% | batch:       444 of       686\t|\tloss: 1.83738\n",
      "Training Epoch 19  64.9% | batch:       445 of       686\t|\tloss: 1.73404\n",
      "Training Epoch 19  65.0% | batch:       446 of       686\t|\tloss: 1.88645\n",
      "Training Epoch 19  65.2% | batch:       447 of       686\t|\tloss: 1.88388\n",
      "Training Epoch 19  65.3% | batch:       448 of       686\t|\tloss: 2.79994\n",
      "Training Epoch 19  65.5% | batch:       449 of       686\t|\tloss: 2.06891\n",
      "Training Epoch 19  65.6% | batch:       450 of       686\t|\tloss: 1.63465\n",
      "Training Epoch 19  65.7% | batch:       451 of       686\t|\tloss: 2.27428\n",
      "Training Epoch 19  65.9% | batch:       452 of       686\t|\tloss: 1.81246\n",
      "Training Epoch 19  66.0% | batch:       453 of       686\t|\tloss: 1.62314\n",
      "Training Epoch 19  66.2% | batch:       454 of       686\t|\tloss: 2.12675\n",
      "Training Epoch 19  66.3% | batch:       455 of       686\t|\tloss: 2.49598\n",
      "Training Epoch 19  66.5% | batch:       456 of       686\t|\tloss: 1.77325\n",
      "Training Epoch 19  66.6% | batch:       457 of       686\t|\tloss: 2.34041\n",
      "Training Epoch 19  66.8% | batch:       458 of       686\t|\tloss: 1.83797\n",
      "Training Epoch 19  66.9% | batch:       459 of       686\t|\tloss: 1.76367\n",
      "Training Epoch 19  67.1% | batch:       460 of       686\t|\tloss: 1.40509\n",
      "Training Epoch 19  67.2% | batch:       461 of       686\t|\tloss: 2.31015\n",
      "Training Epoch 19  67.3% | batch:       462 of       686\t|\tloss: 1.66953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 19  67.5% | batch:       463 of       686\t|\tloss: 2.64114\n",
      "Training Epoch 19  67.6% | batch:       464 of       686\t|\tloss: 2.15351\n",
      "Training Epoch 19  67.8% | batch:       465 of       686\t|\tloss: 1.86769\n",
      "Training Epoch 19  67.9% | batch:       466 of       686\t|\tloss: 1.72044\n",
      "Training Epoch 19  68.1% | batch:       467 of       686\t|\tloss: 2.18714\n",
      "Training Epoch 19  68.2% | batch:       468 of       686\t|\tloss: 1.80533\n",
      "Training Epoch 19  68.4% | batch:       469 of       686\t|\tloss: 1.50997\n",
      "Training Epoch 19  68.5% | batch:       470 of       686\t|\tloss: 2.18456\n",
      "Training Epoch 19  68.7% | batch:       471 of       686\t|\tloss: 1.54934\n",
      "Training Epoch 19  68.8% | batch:       472 of       686\t|\tloss: 2.41107\n",
      "Training Epoch 19  69.0% | batch:       473 of       686\t|\tloss: 1.85482\n",
      "Training Epoch 19  69.1% | batch:       474 of       686\t|\tloss: 1.66548\n",
      "Training Epoch 19  69.2% | batch:       475 of       686\t|\tloss: 1.62967\n",
      "Training Epoch 19  69.4% | batch:       476 of       686\t|\tloss: 2.02819\n",
      "Training Epoch 19  69.5% | batch:       477 of       686\t|\tloss: 1.48694\n",
      "Training Epoch 19  69.7% | batch:       478 of       686\t|\tloss: 1.57441\n",
      "Training Epoch 19  69.8% | batch:       479 of       686\t|\tloss: 1.89616\n",
      "Training Epoch 19  70.0% | batch:       480 of       686\t|\tloss: 1.70276\n",
      "Training Epoch 19  70.1% | batch:       481 of       686\t|\tloss: 1.86137\n",
      "Training Epoch 19  70.3% | batch:       482 of       686\t|\tloss: 1.98515\n",
      "Training Epoch 19  70.4% | batch:       483 of       686\t|\tloss: 1.59905\n",
      "Training Epoch 19  70.6% | batch:       484 of       686\t|\tloss: 1.74842\n",
      "Training Epoch 19  70.7% | batch:       485 of       686\t|\tloss: 2.091\n",
      "Training Epoch 19  70.8% | batch:       486 of       686\t|\tloss: 1.58292\n",
      "Training Epoch 19  71.0% | batch:       487 of       686\t|\tloss: 2.37677\n",
      "Training Epoch 19  71.1% | batch:       488 of       686\t|\tloss: 1.80704\n",
      "Training Epoch 19  71.3% | batch:       489 of       686\t|\tloss: 1.64856\n",
      "Training Epoch 19  71.4% | batch:       490 of       686\t|\tloss: 2.15096\n",
      "Training Epoch 19  71.6% | batch:       491 of       686\t|\tloss: 2.49269\n",
      "Training Epoch 19  71.7% | batch:       492 of       686\t|\tloss: 2.35832\n",
      "Training Epoch 19  71.9% | batch:       493 of       686\t|\tloss: 1.91702\n",
      "Training Epoch 19  72.0% | batch:       494 of       686\t|\tloss: 2.03602\n",
      "Training Epoch 19  72.2% | batch:       495 of       686\t|\tloss: 1.66431\n",
      "Training Epoch 19  72.3% | batch:       496 of       686\t|\tloss: 1.85924\n",
      "Training Epoch 19  72.4% | batch:       497 of       686\t|\tloss: 2.06636\n",
      "Training Epoch 19  72.6% | batch:       498 of       686\t|\tloss: 1.968\n",
      "Training Epoch 19  72.7% | batch:       499 of       686\t|\tloss: 1.89211\n",
      "Training Epoch 19  72.9% | batch:       500 of       686\t|\tloss: 2.19365\n",
      "Training Epoch 19  73.0% | batch:       501 of       686\t|\tloss: 1.82532\n",
      "Training Epoch 19  73.2% | batch:       502 of       686\t|\tloss: 1.93277\n",
      "Training Epoch 19  73.3% | batch:       503 of       686\t|\tloss: 2.47819\n",
      "Training Epoch 19  73.5% | batch:       504 of       686\t|\tloss: 2.2721\n",
      "Training Epoch 19  73.6% | batch:       505 of       686\t|\tloss: 2.18602\n",
      "Training Epoch 19  73.8% | batch:       506 of       686\t|\tloss: 1.73252\n",
      "Training Epoch 19  73.9% | batch:       507 of       686\t|\tloss: 1.9904\n",
      "Training Epoch 19  74.1% | batch:       508 of       686\t|\tloss: 1.77818\n",
      "Training Epoch 19  74.2% | batch:       509 of       686\t|\tloss: 1.90946\n",
      "Training Epoch 19  74.3% | batch:       510 of       686\t|\tloss: 2.08172\n",
      "Training Epoch 19  74.5% | batch:       511 of       686\t|\tloss: 1.9726\n",
      "Training Epoch 19  74.6% | batch:       512 of       686\t|\tloss: 1.80578\n",
      "Training Epoch 19  74.8% | batch:       513 of       686\t|\tloss: 1.5887\n",
      "Training Epoch 19  74.9% | batch:       514 of       686\t|\tloss: 1.73742\n",
      "Training Epoch 19  75.1% | batch:       515 of       686\t|\tloss: 1.91313\n",
      "Training Epoch 19  75.2% | batch:       516 of       686\t|\tloss: 2.093\n",
      "Training Epoch 19  75.4% | batch:       517 of       686\t|\tloss: 1.82485\n",
      "Training Epoch 19  75.5% | batch:       518 of       686\t|\tloss: 2.44379\n",
      "Training Epoch 19  75.7% | batch:       519 of       686\t|\tloss: 1.83925\n",
      "Training Epoch 19  75.8% | batch:       520 of       686\t|\tloss: 1.48681\n",
      "Training Epoch 19  75.9% | batch:       521 of       686\t|\tloss: 1.82723\n",
      "Training Epoch 19  76.1% | batch:       522 of       686\t|\tloss: 1.41456\n",
      "Training Epoch 19  76.2% | batch:       523 of       686\t|\tloss: 1.86728\n",
      "Training Epoch 19  76.4% | batch:       524 of       686\t|\tloss: 1.47044\n",
      "Training Epoch 19  76.5% | batch:       525 of       686\t|\tloss: 1.81906\n",
      "Training Epoch 19  76.7% | batch:       526 of       686\t|\tloss: 2.03198\n",
      "Training Epoch 19  76.8% | batch:       527 of       686\t|\tloss: 2.16555\n",
      "Training Epoch 19  77.0% | batch:       528 of       686\t|\tloss: 1.59828\n",
      "Training Epoch 19  77.1% | batch:       529 of       686\t|\tloss: 2.63904\n",
      "Training Epoch 19  77.3% | batch:       530 of       686\t|\tloss: 1.59606\n",
      "Training Epoch 19  77.4% | batch:       531 of       686\t|\tloss: 1.79628\n",
      "Training Epoch 19  77.6% | batch:       532 of       686\t|\tloss: 1.67224\n",
      "Training Epoch 19  77.7% | batch:       533 of       686\t|\tloss: 1.62983\n",
      "Training Epoch 19  77.8% | batch:       534 of       686\t|\tloss: 1.81537\n",
      "Training Epoch 19  78.0% | batch:       535 of       686\t|\tloss: 1.84306\n",
      "Training Epoch 19  78.1% | batch:       536 of       686\t|\tloss: 1.66592\n",
      "Training Epoch 19  78.3% | batch:       537 of       686\t|\tloss: 1.94528\n",
      "Training Epoch 19  78.4% | batch:       538 of       686\t|\tloss: 1.99684\n",
      "Training Epoch 19  78.6% | batch:       539 of       686\t|\tloss: 1.77848\n",
      "Training Epoch 19  78.7% | batch:       540 of       686\t|\tloss: 2.4713\n",
      "Training Epoch 19  78.9% | batch:       541 of       686\t|\tloss: 1.9223\n",
      "Training Epoch 19  79.0% | batch:       542 of       686\t|\tloss: 2.59188\n",
      "Training Epoch 19  79.2% | batch:       543 of       686\t|\tloss: 1.93125\n",
      "Training Epoch 19  79.3% | batch:       544 of       686\t|\tloss: 1.66383\n",
      "Training Epoch 19  79.4% | batch:       545 of       686\t|\tloss: 1.76046\n",
      "Training Epoch 19  79.6% | batch:       546 of       686\t|\tloss: 1.80775\n",
      "Training Epoch 19  79.7% | batch:       547 of       686\t|\tloss: 2.04633\n",
      "Training Epoch 19  79.9% | batch:       548 of       686\t|\tloss: 1.39153\n",
      "Training Epoch 19  80.0% | batch:       549 of       686\t|\tloss: 1.46451\n",
      "Training Epoch 19  80.2% | batch:       550 of       686\t|\tloss: 1.73653\n",
      "Training Epoch 19  80.3% | batch:       551 of       686\t|\tloss: 2.13118\n",
      "Training Epoch 19  80.5% | batch:       552 of       686\t|\tloss: 1.72078\n",
      "Training Epoch 19  80.6% | batch:       553 of       686\t|\tloss: 2.13043\n",
      "Training Epoch 19  80.8% | batch:       554 of       686\t|\tloss: 1.49985\n",
      "Training Epoch 19  80.9% | batch:       555 of       686\t|\tloss: 1.37918\n",
      "Training Epoch 19  81.0% | batch:       556 of       686\t|\tloss: 1.94814\n",
      "Training Epoch 19  81.2% | batch:       557 of       686\t|\tloss: 2.26481\n",
      "Training Epoch 19  81.3% | batch:       558 of       686\t|\tloss: 1.59399\n",
      "Training Epoch 19  81.5% | batch:       559 of       686\t|\tloss: 1.86359\n",
      "Training Epoch 19  81.6% | batch:       560 of       686\t|\tloss: 2.40526\n",
      "Training Epoch 19  81.8% | batch:       561 of       686\t|\tloss: 1.62897\n",
      "Training Epoch 19  81.9% | batch:       562 of       686\t|\tloss: 2.89231\n",
      "Training Epoch 19  82.1% | batch:       563 of       686\t|\tloss: 1.87776\n",
      "Training Epoch 19  82.2% | batch:       564 of       686\t|\tloss: 2.04702\n",
      "Training Epoch 19  82.4% | batch:       565 of       686\t|\tloss: 1.65215\n",
      "Training Epoch 19  82.5% | batch:       566 of       686\t|\tloss: 1.82097\n",
      "Training Epoch 19  82.7% | batch:       567 of       686\t|\tloss: 1.78523\n",
      "Training Epoch 19  82.8% | batch:       568 of       686\t|\tloss: 1.9316\n",
      "Training Epoch 19  82.9% | batch:       569 of       686\t|\tloss: 2.6441\n",
      "Training Epoch 19  83.1% | batch:       570 of       686\t|\tloss: 2.04506\n",
      "Training Epoch 19  83.2% | batch:       571 of       686\t|\tloss: 2.07182\n",
      "Training Epoch 19  83.4% | batch:       572 of       686\t|\tloss: 2.04979\n",
      "Training Epoch 19  83.5% | batch:       573 of       686\t|\tloss: 1.76118\n",
      "Training Epoch 19  83.7% | batch:       574 of       686\t|\tloss: 1.83371\n",
      "Training Epoch 19  83.8% | batch:       575 of       686\t|\tloss: 1.94202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 19  84.0% | batch:       576 of       686\t|\tloss: 1.67601\n",
      "Training Epoch 19  84.1% | batch:       577 of       686\t|\tloss: 1.88218\n",
      "Training Epoch 19  84.3% | batch:       578 of       686\t|\tloss: 1.94037\n",
      "Training Epoch 19  84.4% | batch:       579 of       686\t|\tloss: 2.14987\n",
      "Training Epoch 19  84.5% | batch:       580 of       686\t|\tloss: 1.83041\n",
      "Training Epoch 19  84.7% | batch:       581 of       686\t|\tloss: 1.64098\n",
      "Training Epoch 19  84.8% | batch:       582 of       686\t|\tloss: 1.72764\n",
      "Training Epoch 19  85.0% | batch:       583 of       686\t|\tloss: 1.57935\n",
      "Training Epoch 19  85.1% | batch:       584 of       686\t|\tloss: 1.70146\n",
      "Training Epoch 19  85.3% | batch:       585 of       686\t|\tloss: 2.01549\n",
      "Training Epoch 19  85.4% | batch:       586 of       686\t|\tloss: 1.61511\n",
      "Training Epoch 19  85.6% | batch:       587 of       686\t|\tloss: 2.31937\n",
      "Training Epoch 19  85.7% | batch:       588 of       686\t|\tloss: 1.58879\n",
      "Training Epoch 19  85.9% | batch:       589 of       686\t|\tloss: 1.64738\n",
      "Training Epoch 19  86.0% | batch:       590 of       686\t|\tloss: 1.68454\n",
      "Training Epoch 19  86.2% | batch:       591 of       686\t|\tloss: 1.48601\n",
      "Training Epoch 19  86.3% | batch:       592 of       686\t|\tloss: 2.40215\n",
      "Training Epoch 19  86.4% | batch:       593 of       686\t|\tloss: 1.68247\n",
      "Training Epoch 19  86.6% | batch:       594 of       686\t|\tloss: 2.01988\n",
      "Training Epoch 19  86.7% | batch:       595 of       686\t|\tloss: 1.81581\n",
      "Training Epoch 19  86.9% | batch:       596 of       686\t|\tloss: 1.4601\n",
      "Training Epoch 19  87.0% | batch:       597 of       686\t|\tloss: 2.12837\n",
      "Training Epoch 19  87.2% | batch:       598 of       686\t|\tloss: 1.7637\n",
      "Training Epoch 19  87.3% | batch:       599 of       686\t|\tloss: 1.8293\n",
      "Training Epoch 19  87.5% | batch:       600 of       686\t|\tloss: 1.79697\n",
      "Training Epoch 19  87.6% | batch:       601 of       686\t|\tloss: 1.9626\n",
      "Training Epoch 19  87.8% | batch:       602 of       686\t|\tloss: 2.06461\n",
      "Training Epoch 19  87.9% | batch:       603 of       686\t|\tloss: 1.46556\n",
      "Training Epoch 19  88.0% | batch:       604 of       686\t|\tloss: 1.96654\n",
      "Training Epoch 19  88.2% | batch:       605 of       686\t|\tloss: 1.73233\n",
      "Training Epoch 19  88.3% | batch:       606 of       686\t|\tloss: 1.86424\n",
      "Training Epoch 19  88.5% | batch:       607 of       686\t|\tloss: 2.07313\n",
      "Training Epoch 19  88.6% | batch:       608 of       686\t|\tloss: 1.5871\n",
      "Training Epoch 19  88.8% | batch:       609 of       686\t|\tloss: 1.83215\n",
      "Training Epoch 19  88.9% | batch:       610 of       686\t|\tloss: 1.7159\n",
      "Training Epoch 19  89.1% | batch:       611 of       686\t|\tloss: 1.77367\n",
      "Training Epoch 19  89.2% | batch:       612 of       686\t|\tloss: 1.7024\n",
      "Training Epoch 19  89.4% | batch:       613 of       686\t|\tloss: 1.96583\n",
      "Training Epoch 19  89.5% | batch:       614 of       686\t|\tloss: 1.4953\n",
      "Training Epoch 19  89.7% | batch:       615 of       686\t|\tloss: 1.90617\n",
      "Training Epoch 19  89.8% | batch:       616 of       686\t|\tloss: 1.59059\n",
      "Training Epoch 19  89.9% | batch:       617 of       686\t|\tloss: 1.65544\n",
      "Training Epoch 19  90.1% | batch:       618 of       686\t|\tloss: 2.13547\n",
      "Training Epoch 19  90.2% | batch:       619 of       686\t|\tloss: 1.66689\n",
      "Training Epoch 19  90.4% | batch:       620 of       686\t|\tloss: 1.86339\n",
      "Training Epoch 19  90.5% | batch:       621 of       686\t|\tloss: 1.50549\n",
      "Training Epoch 19  90.7% | batch:       622 of       686\t|\tloss: 2.18099\n",
      "Training Epoch 19  90.8% | batch:       623 of       686\t|\tloss: 2.17822\n",
      "Training Epoch 19  91.0% | batch:       624 of       686\t|\tloss: 1.53141\n",
      "Training Epoch 19  91.1% | batch:       625 of       686\t|\tloss: 2.14022\n",
      "Training Epoch 19  91.3% | batch:       626 of       686\t|\tloss: 1.91112\n",
      "Training Epoch 19  91.4% | batch:       627 of       686\t|\tloss: 1.95004\n",
      "Training Epoch 19  91.5% | batch:       628 of       686\t|\tloss: 2.12612\n",
      "Training Epoch 19  91.7% | batch:       629 of       686\t|\tloss: 1.66813\n",
      "Training Epoch 19  91.8% | batch:       630 of       686\t|\tloss: 1.90274\n",
      "Training Epoch 19  92.0% | batch:       631 of       686\t|\tloss: 2.08783\n",
      "Training Epoch 19  92.1% | batch:       632 of       686\t|\tloss: 1.50317\n",
      "Training Epoch 19  92.3% | batch:       633 of       686\t|\tloss: 1.83326\n",
      "Training Epoch 19  92.4% | batch:       634 of       686\t|\tloss: 1.85847\n",
      "Training Epoch 19  92.6% | batch:       635 of       686\t|\tloss: 1.64007\n",
      "Training Epoch 19  92.7% | batch:       636 of       686\t|\tloss: 1.70908\n",
      "Training Epoch 19  92.9% | batch:       637 of       686\t|\tloss: 1.76363\n",
      "Training Epoch 19  93.0% | batch:       638 of       686\t|\tloss: 1.93936\n",
      "Training Epoch 19  93.1% | batch:       639 of       686\t|\tloss: 1.97063\n",
      "Training Epoch 19  93.3% | batch:       640 of       686\t|\tloss: 1.84406\n",
      "Training Epoch 19  93.4% | batch:       641 of       686\t|\tloss: 1.56058\n",
      "Training Epoch 19  93.6% | batch:       642 of       686\t|\tloss: 1.55154\n",
      "Training Epoch 19  93.7% | batch:       643 of       686\t|\tloss: 1.46435\n",
      "Training Epoch 19  93.9% | batch:       644 of       686\t|\tloss: 2.06126\n",
      "Training Epoch 19  94.0% | batch:       645 of       686\t|\tloss: 2.27227\n",
      "Training Epoch 19  94.2% | batch:       646 of       686\t|\tloss: 1.45038\n",
      "Training Epoch 19  94.3% | batch:       647 of       686\t|\tloss: 1.7047\n",
      "Training Epoch 19  94.5% | batch:       648 of       686\t|\tloss: 2.04543\n",
      "Training Epoch 19  94.6% | batch:       649 of       686\t|\tloss: 1.93125\n",
      "Training Epoch 19  94.8% | batch:       650 of       686\t|\tloss: 1.49254\n",
      "Training Epoch 19  94.9% | batch:       651 of       686\t|\tloss: 1.83381\n",
      "Training Epoch 19  95.0% | batch:       652 of       686\t|\tloss: 2.07134\n",
      "Training Epoch 19  95.2% | batch:       653 of       686\t|\tloss: 1.88689\n",
      "Training Epoch 19  95.3% | batch:       654 of       686\t|\tloss: 1.91587\n",
      "Training Epoch 19  95.5% | batch:       655 of       686\t|\tloss: 1.86716\n",
      "Training Epoch 19  95.6% | batch:       656 of       686\t|\tloss: 1.52553\n",
      "Training Epoch 19  95.8% | batch:       657 of       686\t|\tloss: 1.91185\n",
      "Training Epoch 19  95.9% | batch:       658 of       686\t|\tloss: 2.43044\n",
      "Training Epoch 19  96.1% | batch:       659 of       686\t|\tloss: 1.94971\n",
      "Training Epoch 19  96.2% | batch:       660 of       686\t|\tloss: 1.71042\n",
      "Training Epoch 19  96.4% | batch:       661 of       686\t|\tloss: 1.71575\n",
      "Training Epoch 19  96.5% | batch:       662 of       686\t|\tloss: 1.68216\n",
      "Training Epoch 19  96.6% | batch:       663 of       686\t|\tloss: 1.90541\n",
      "Training Epoch 19  96.8% | batch:       664 of       686\t|\tloss: 1.82954\n",
      "Training Epoch 19  96.9% | batch:       665 of       686\t|\tloss: 1.61563\n",
      "Training Epoch 19  97.1% | batch:       666 of       686\t|\tloss: 1.29472\n",
      "Training Epoch 19  97.2% | batch:       667 of       686\t|\tloss: 1.74822\n",
      "Training Epoch 19  97.4% | batch:       668 of       686\t|\tloss: 2.02067\n",
      "Training Epoch 19  97.5% | batch:       669 of       686\t|\tloss: 1.96741\n",
      "Training Epoch 19  97.7% | batch:       670 of       686\t|\tloss: 2.1473\n",
      "Training Epoch 19  97.8% | batch:       671 of       686\t|\tloss: 1.87214\n",
      "Training Epoch 19  98.0% | batch:       672 of       686\t|\tloss: 1.60496\n",
      "Training Epoch 19  98.1% | batch:       673 of       686\t|\tloss: 1.34261\n",
      "Training Epoch 19  98.3% | batch:       674 of       686\t|\tloss: 1.86319\n",
      "Training Epoch 19  98.4% | batch:       675 of       686\t|\tloss: 1.80857\n",
      "Training Epoch 19  98.5% | batch:       676 of       686\t|\tloss: 1.55189\n",
      "Training Epoch 19  98.7% | batch:       677 of       686\t|\tloss: 1.85561\n",
      "Training Epoch 19  98.8% | batch:       678 of       686\t|\tloss: 1.70903\n",
      "Training Epoch 19  99.0% | batch:       679 of       686\t|\tloss: 1.83236\n",
      "Training Epoch 19  99.1% | batch:       680 of       686\t|\tloss: 1.74699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:08:39,802 | INFO : Epoch 19 Training Summary: epoch: 19.000000 | loss: 1.971140 | \n",
      "2023-05-25 22:08:39,803 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 24.19196629524231 seconds\n",
      "\n",
      "2023-05-25 22:08:39,803 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.96919180217542 seconds\n",
      "2023-05-25 22:08:39,804 | INFO : Avg batch train. time: 0.0349405128311595 seconds\n",
      "2023-05-25 22:08:39,804 | INFO : Avg sample train. time: 0.00027332449743058807 seconds\n",
      "2023-05-25 22:08:39,804 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 19  99.3% | batch:       681 of       686\t|\tloss: 1.45203\n",
      "Training Epoch 19  99.4% | batch:       682 of       686\t|\tloss: 2.11138\n",
      "Training Epoch 19  99.6% | batch:       683 of       686\t|\tloss: 1.64339\n",
      "Training Epoch 19  99.7% | batch:       684 of       686\t|\tloss: 1.86705\n",
      "Training Epoch 19  99.9% | batch:       685 of       686\t|\tloss: 1.40203\n",
      "\n",
      "Evaluating Epoch 19   0.0% | batch:         0 of       172\t|\tloss: 1.92182\n",
      "Evaluating Epoch 19   0.6% | batch:         1 of       172\t|\tloss: 2.3007\n",
      "Evaluating Epoch 19   1.2% | batch:         2 of       172\t|\tloss: 1.64762\n",
      "Evaluating Epoch 19   1.7% | batch:         3 of       172\t|\tloss: 3.89864\n",
      "Evaluating Epoch 19   2.3% | batch:         4 of       172\t|\tloss: 1.84253\n",
      "Evaluating Epoch 19   2.9% | batch:         5 of       172\t|\tloss: 1.72912\n",
      "Evaluating Epoch 19   3.5% | batch:         6 of       172\t|\tloss: 2.22703\n",
      "Evaluating Epoch 19   4.1% | batch:         7 of       172\t|\tloss: 4.09388\n",
      "Evaluating Epoch 19   4.7% | batch:         8 of       172\t|\tloss: 1.38623\n",
      "Evaluating Epoch 19   5.2% | batch:         9 of       172\t|\tloss: 2.43061\n",
      "Evaluating Epoch 19   5.8% | batch:        10 of       172\t|\tloss: 2.3439\n",
      "Evaluating Epoch 19   6.4% | batch:        11 of       172\t|\tloss: 2.21635\n",
      "Evaluating Epoch 19   7.0% | batch:        12 of       172\t|\tloss: 1.66832\n",
      "Evaluating Epoch 19   7.6% | batch:        13 of       172\t|\tloss: 2.42302\n",
      "Evaluating Epoch 19   8.1% | batch:        14 of       172\t|\tloss: 2.60745\n",
      "Evaluating Epoch 19   8.7% | batch:        15 of       172\t|\tloss: 2.08116\n",
      "Evaluating Epoch 19   9.3% | batch:        16 of       172\t|\tloss: 2.78328\n",
      "Evaluating Epoch 19   9.9% | batch:        17 of       172\t|\tloss: 1.55406\n",
      "Evaluating Epoch 19  10.5% | batch:        18 of       172\t|\tloss: 14.8653\n",
      "Evaluating Epoch 19  11.0% | batch:        19 of       172\t|\tloss: 1.80649\n",
      "Evaluating Epoch 19  11.6% | batch:        20 of       172\t|\tloss: 2.25774\n",
      "Evaluating Epoch 19  12.2% | batch:        21 of       172\t|\tloss: 0.487141\n",
      "Evaluating Epoch 19  12.8% | batch:        22 of       172\t|\tloss: 2.66698\n",
      "Evaluating Epoch 19  13.4% | batch:        23 of       172\t|\tloss: 2.94562\n",
      "Evaluating Epoch 19  14.0% | batch:        24 of       172\t|\tloss: 1.22124\n",
      "Evaluating Epoch 19  14.5% | batch:        25 of       172\t|\tloss: 2.31271\n",
      "Evaluating Epoch 19  15.1% | batch:        26 of       172\t|\tloss: 7.01233\n",
      "Evaluating Epoch 19  15.7% | batch:        27 of       172\t|\tloss: 14.7425\n",
      "Evaluating Epoch 19  16.3% | batch:        28 of       172\t|\tloss: 0.182314\n",
      "Evaluating Epoch 19  16.9% | batch:        29 of       172\t|\tloss: 1.80179\n",
      "Evaluating Epoch 19  17.4% | batch:        30 of       172\t|\tloss: 0.881367\n",
      "Evaluating Epoch 19  18.0% | batch:        31 of       172\t|\tloss: 0.789166\n",
      "Evaluating Epoch 19  18.6% | batch:        32 of       172\t|\tloss: 0.719741\n",
      "Evaluating Epoch 19  19.2% | batch:        33 of       172\t|\tloss: 0.356011\n",
      "Evaluating Epoch 19  19.8% | batch:        34 of       172\t|\tloss: 0.288311\n",
      "Evaluating Epoch 19  20.3% | batch:        35 of       172\t|\tloss: 0.412947\n",
      "Evaluating Epoch 19  20.9% | batch:        36 of       172\t|\tloss: 3.09954\n",
      "Evaluating Epoch 19  21.5% | batch:        37 of       172\t|\tloss: 3.87364\n",
      "Evaluating Epoch 19  22.1% | batch:        38 of       172\t|\tloss: 2.90779\n",
      "Evaluating Epoch 19  22.7% | batch:        39 of       172\t|\tloss: 6.28312\n",
      "Evaluating Epoch 19  23.3% | batch:        40 of       172\t|\tloss: 0.510276\n",
      "Evaluating Epoch 19  23.8% | batch:        41 of       172\t|\tloss: 1.15564\n",
      "Evaluating Epoch 19  24.4% | batch:        42 of       172\t|\tloss: 0.814287\n",
      "Evaluating Epoch 19  25.0% | batch:        43 of       172\t|\tloss: 16.1688\n",
      "Evaluating Epoch 19  25.6% | batch:        44 of       172\t|\tloss: 1.63499\n",
      "Evaluating Epoch 19  26.2% | batch:        45 of       172\t|\tloss: 0.978456\n",
      "Evaluating Epoch 19  26.7% | batch:        46 of       172\t|\tloss: 0.266021\n",
      "Evaluating Epoch 19  27.3% | batch:        47 of       172\t|\tloss: 1.03092\n",
      "Evaluating Epoch 19  27.9% | batch:        48 of       172\t|\tloss: 0.306434\n",
      "Evaluating Epoch 19  28.5% | batch:        49 of       172\t|\tloss: 1.01491\n",
      "Evaluating Epoch 19  29.1% | batch:        50 of       172\t|\tloss: 0.42087\n",
      "Evaluating Epoch 19  29.7% | batch:        51 of       172\t|\tloss: 0.524832\n",
      "Evaluating Epoch 19  30.2% | batch:        52 of       172\t|\tloss: 0.864343\n",
      "Evaluating Epoch 19  30.8% | batch:        53 of       172\t|\tloss: 1.47622\n",
      "Evaluating Epoch 19  31.4% | batch:        54 of       172\t|\tloss: 0.953007\n",
      "Evaluating Epoch 19  32.0% | batch:        55 of       172\t|\tloss: 1.10224\n",
      "Evaluating Epoch 19  32.6% | batch:        56 of       172\t|\tloss: 1.96079\n",
      "Evaluating Epoch 19  33.1% | batch:        57 of       172\t|\tloss: 1.45671\n",
      "Evaluating Epoch 19  33.7% | batch:        58 of       172\t|\tloss: 1.00939\n",
      "Evaluating Epoch 19  34.3% | batch:        59 of       172\t|\tloss: 1.70643\n",
      "Evaluating Epoch 19  34.9% | batch:        60 of       172\t|\tloss: 0.616382\n",
      "Evaluating Epoch 19  35.5% | batch:        61 of       172\t|\tloss: 2.23902\n",
      "Evaluating Epoch 19  36.0% | batch:        62 of       172\t|\tloss: 0.361953\n",
      "Evaluating Epoch 19  36.6% | batch:        63 of       172\t|\tloss: 1.60428\n",
      "Evaluating Epoch 19  37.2% | batch:        64 of       172\t|\tloss: 1.16506\n",
      "Evaluating Epoch 19  37.8% | batch:        65 of       172\t|\tloss: 1.1659\n",
      "Evaluating Epoch 19  38.4% | batch:        66 of       172\t|\tloss: 2.1633\n",
      "Evaluating Epoch 19  39.0% | batch:        67 of       172\t|\tloss: 0.693465\n",
      "Evaluating Epoch 19  39.5% | batch:        68 of       172\t|\tloss: 1.6333\n",
      "Evaluating Epoch 19  40.1% | batch:        69 of       172\t|\tloss: 2.09597\n",
      "Evaluating Epoch 19  40.7% | batch:        70 of       172\t|\tloss: 0.641455\n",
      "Evaluating Epoch 19  41.3% | batch:        71 of       172\t|\tloss: 1.11882\n",
      "Evaluating Epoch 19  41.9% | batch:        72 of       172\t|\tloss: 1.14865\n",
      "Evaluating Epoch 19  42.4% | batch:        73 of       172\t|\tloss: 1.16631\n",
      "Evaluating Epoch 19  43.0% | batch:        74 of       172\t|\tloss: 0.333029\n",
      "Evaluating Epoch 19  43.6% | batch:        75 of       172\t|\tloss: 0.296638\n",
      "Evaluating Epoch 19  44.2% | batch:        76 of       172\t|\tloss: 0.407415\n",
      "Evaluating Epoch 19  44.8% | batch:        77 of       172\t|\tloss: 0.332459\n",
      "Evaluating Epoch 19  45.3% | batch:        78 of       172\t|\tloss: 0.311972\n",
      "Evaluating Epoch 19  45.9% | batch:        79 of       172\t|\tloss: 0.451592\n",
      "Evaluating Epoch 19  46.5% | batch:        80 of       172\t|\tloss: 0.307812\n",
      "Evaluating Epoch 19  47.1% | batch:        81 of       172\t|\tloss: 0.367339\n",
      "Evaluating Epoch 19  47.7% | batch:        82 of       172\t|\tloss: 0.369843\n",
      "Evaluating Epoch 19  48.3% | batch:        83 of       172\t|\tloss: 0.500428\n",
      "Evaluating Epoch 19  48.8% | batch:        84 of       172\t|\tloss: 0.568681\n",
      "Evaluating Epoch 19  49.4% | batch:        85 of       172\t|\tloss: 0.590932\n",
      "Evaluating Epoch 19  50.0% | batch:        86 of       172\t|\tloss: 0.375752\n",
      "Evaluating Epoch 19  50.6% | batch:        87 of       172\t|\tloss: 0.533149\n",
      "Evaluating Epoch 19  51.2% | batch:        88 of       172\t|\tloss: 0.630196\n",
      "Evaluating Epoch 19  51.7% | batch:        89 of       172\t|\tloss: 0.744403\n",
      "Evaluating Epoch 19  52.3% | batch:        90 of       172\t|\tloss: 0.679543\n",
      "Evaluating Epoch 19  52.9% | batch:        91 of       172\t|\tloss: 0.767527\n",
      "Evaluating Epoch 19  53.5% | batch:        92 of       172\t|\tloss: 1.04354\n",
      "Evaluating Epoch 19  54.1% | batch:        93 of       172\t|\tloss: 0.733158\n",
      "Evaluating Epoch 19  54.7% | batch:        94 of       172\t|\tloss: 0.786162\n",
      "Evaluating Epoch 19  55.2% | batch:        95 of       172\t|\tloss: 0.681495\n",
      "Evaluating Epoch 19  55.8% | batch:        96 of       172\t|\tloss: 0.783824\n",
      "Evaluating Epoch 19  56.4% | batch:        97 of       172\t|\tloss: 0.622676\n",
      "Evaluating Epoch 19  57.0% | batch:        98 of       172\t|\tloss: 0.990492\n",
      "Evaluating Epoch 19  57.6% | batch:        99 of       172\t|\tloss: 0.891611\n",
      "Evaluating Epoch 19  58.1% | batch:       100 of       172\t|\tloss: 0.278317\n",
      "Evaluating Epoch 19  58.7% | batch:       101 of       172\t|\tloss: 0.58838\n",
      "Evaluating Epoch 19  59.3% | batch:       102 of       172\t|\tloss: 0.830373\n",
      "Evaluating Epoch 19  59.9% | batch:       103 of       172\t|\tloss: 0.806338\n",
      "Evaluating Epoch 19  60.5% | batch:       104 of       172\t|\tloss: 0.516602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 19  61.0% | batch:       105 of       172\t|\tloss: 0.822957\n",
      "Evaluating Epoch 19  61.6% | batch:       106 of       172\t|\tloss: 1.06966\n",
      "Evaluating Epoch 19  62.2% | batch:       107 of       172\t|\tloss: 0.721884\n",
      "Evaluating Epoch 19  62.8% | batch:       108 of       172\t|\tloss: 0.571683\n",
      "Evaluating Epoch 19  63.4% | batch:       109 of       172\t|\tloss: 0.859572\n",
      "Evaluating Epoch 19  64.0% | batch:       110 of       172\t|\tloss: 0.967164\n",
      "Evaluating Epoch 19  64.5% | batch:       111 of       172\t|\tloss: 0.505501\n",
      "Evaluating Epoch 19  65.1% | batch:       112 of       172\t|\tloss: 0.46906\n",
      "Evaluating Epoch 19  65.7% | batch:       113 of       172\t|\tloss: 0.56123\n",
      "Evaluating Epoch 19  66.3% | batch:       114 of       172\t|\tloss: 0.598788\n",
      "Evaluating Epoch 19  66.9% | batch:       115 of       172\t|\tloss: 0.653491\n",
      "Evaluating Epoch 19  67.4% | batch:       116 of       172\t|\tloss: 0.533669\n",
      "Evaluating Epoch 19  68.0% | batch:       117 of       172\t|\tloss: 0.672486\n",
      "Evaluating Epoch 19  68.6% | batch:       118 of       172\t|\tloss: 0.539948\n",
      "Evaluating Epoch 19  69.2% | batch:       119 of       172\t|\tloss: 0.308378\n",
      "Evaluating Epoch 19  69.8% | batch:       120 of       172\t|\tloss: 0.386119\n",
      "Evaluating Epoch 19  70.3% | batch:       121 of       172\t|\tloss: 0.624355\n",
      "Evaluating Epoch 19  70.9% | batch:       122 of       172\t|\tloss: 0.54189\n",
      "Evaluating Epoch 19  71.5% | batch:       123 of       172\t|\tloss: 0.889588\n",
      "Evaluating Epoch 19  72.1% | batch:       124 of       172\t|\tloss: 0.941862\n",
      "Evaluating Epoch 19  72.7% | batch:       125 of       172\t|\tloss: 0.47423\n",
      "Evaluating Epoch 19  73.3% | batch:       126 of       172\t|\tloss: 0.459889\n",
      "Evaluating Epoch 19  73.8% | batch:       127 of       172\t|\tloss: 0.351746\n",
      "Evaluating Epoch 19  74.4% | batch:       128 of       172\t|\tloss: 0.500917\n",
      "Evaluating Epoch 19  75.0% | batch:       129 of       172\t|\tloss: 0.202261\n",
      "Evaluating Epoch 19  75.6% | batch:       130 of       172\t|\tloss: 0.563788\n",
      "Evaluating Epoch 19  76.2% | batch:       131 of       172\t|\tloss: 0.472672\n",
      "Evaluating Epoch 19  76.7% | batch:       132 of       172\t|\tloss: 0.32857\n",
      "Evaluating Epoch 19  77.3% | batch:       133 of       172\t|\tloss: 0.442324\n",
      "Evaluating Epoch 19  77.9% | batch:       134 of       172\t|\tloss: 0.388701\n",
      "Evaluating Epoch 19  78.5% | batch:       135 of       172\t|\tloss: 0.345661\n",
      "Evaluating Epoch 19  79.1% | batch:       136 of       172\t|\tloss: 0.363767\n",
      "Evaluating Epoch 19  79.7% | batch:       137 of       172\t|\tloss: 0.288016\n",
      "Evaluating Epoch 19  80.2% | batch:       138 of       172\t|\tloss: 0.424484\n",
      "Evaluating Epoch 19  80.8% | batch:       139 of       172\t|\tloss: 0.488315\n",
      "Evaluating Epoch 19  81.4% | batch:       140 of       172\t|\tloss: 0.390424\n",
      "Evaluating Epoch 19  82.0% | batch:       141 of       172\t|\tloss: 0.238048\n",
      "Evaluating Epoch 19  82.6% | batch:       142 of       172\t|\tloss: 0.288088\n",
      "Evaluating Epoch 19  83.1% | batch:       143 of       172\t|\tloss: 0.385095\n",
      "Evaluating Epoch 19  83.7% | batch:       144 of       172\t|\tloss: 0.326566\n",
      "Evaluating Epoch 19  84.3% | batch:       145 of       172\t|\tloss: 0.462095\n",
      "Evaluating Epoch 19  84.9% | batch:       146 of       172\t|\tloss: 0.400648\n",
      "Evaluating Epoch 19  85.5% | batch:       147 of       172\t|\tloss: 0.398138\n",
      "Evaluating Epoch 19  86.0% | batch:       148 of       172\t|\tloss: 0.350997\n",
      "Evaluating Epoch 19  86.6% | batch:       149 of       172\t|\tloss: 0.419753\n",
      "Evaluating Epoch 19  87.2% | batch:       150 of       172\t|\tloss: 0.367867\n",
      "Evaluating Epoch 19  87.8% | batch:       151 of       172\t|\tloss: 0.37369\n",
      "Evaluating Epoch 19  88.4% | batch:       152 of       172\t|\tloss: 0.198797\n",
      "Evaluating Epoch 19  89.0% | batch:       153 of       172\t|\tloss: 0.321058\n",
      "Evaluating Epoch 19  89.5% | batch:       154 of       172\t|\tloss: 0.333418\n",
      "Evaluating Epoch 19  90.1% | batch:       155 of       172\t|\tloss: 0.246611\n",
      "Evaluating Epoch 19  90.7% | batch:       156 of       172\t|\tloss: 0.518484\n",
      "Evaluating Epoch 19  91.3% | batch:       157 of       172\t|\tloss: 0.472896\n",
      "Evaluating Epoch 19  91.9% | batch:       158 of       172\t|\tloss: 0.366503\n",
      "Evaluating Epoch 19  92.4% | batch:       159 of       172\t|\tloss: 0.767162\n",
      "Evaluating Epoch 19  93.0% | batch:       160 of       172\t|\tloss: 0.499404\n",
      "Evaluating Epoch 19  93.6% | batch:       161 of       172\t|\tloss: 1.25255\n",
      "Evaluating Epoch 19  94.2% | batch:       162 of       172\t|\tloss: 0.385399\n",
      "Evaluating Epoch 19  94.8% | batch:       163 of       172\t|\tloss: 0.347535\n",
      "Evaluating Epoch 19  95.3% | batch:       164 of       172\t|\tloss: 0.441663\n",
      "Evaluating Epoch 19  95.9% | batch:       165 of       172\t|\tloss: 0.22332\n",
      "Evaluating Epoch 19  96.5% | batch:       166 of       172\t|\tloss: 0.238905\n",
      "Evaluating Epoch 19  97.1% | batch:       167 of       172\t|\tloss: 0.622554\n",
      "Evaluating Epoch 19  97.7% | batch:       168 of       172\t|\tloss: 0.237393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:08:43,866 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 4.061175107955933 seconds\n",
      "\n",
      "2023-05-25 22:08:43,870 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 3.9950041651725767 seconds\n",
      "2023-05-25 22:08:43,871 | INFO : Avg batch val. time: 0.023226768402166142 seconds\n",
      "2023-05-25 22:08:43,871 | INFO : Avg sample val. time: 0.00018194672155451915 seconds\n",
      "2023-05-25 22:08:43,872 | INFO : Epoch 19 Validation Summary: epoch: 19.000000 | loss: 1.263479 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 19  98.3% | batch:       169 of       172\t|\tloss: 0.416401\n",
      "Evaluating Epoch 19  98.8% | batch:       170 of       172\t|\tloss: 0.556834\n",
      "Evaluating Epoch 19  99.4% | batch:       171 of       172\t|\tloss: 0.357304\n",
      "\n",
      "Training Epoch 20   0.0% | batch:         0 of       686\t|\tloss: 1.59619\n",
      "Training Epoch 20   0.1% | batch:         1 of       686\t|\tloss: 1.60468\n",
      "Training Epoch 20   0.3% | batch:         2 of       686\t|\tloss: 1.67492\n",
      "Training Epoch 20   0.4% | batch:         3 of       686\t|\tloss: 1.83695\n",
      "Training Epoch 20   0.6% | batch:         4 of       686\t|\tloss: 2.23883\n",
      "Training Epoch 20   0.7% | batch:         5 of       686\t|\tloss: 1.40283\n",
      "Training Epoch 20   0.9% | batch:         6 of       686\t|\tloss: 1.91428\n",
      "Training Epoch 20   1.0% | batch:         7 of       686\t|\tloss: 1.63658\n",
      "Training Epoch 20   1.2% | batch:         8 of       686\t|\tloss: 2.30835\n",
      "Training Epoch 20   1.3% | batch:         9 of       686\t|\tloss: 1.56042\n",
      "Training Epoch 20   1.5% | batch:        10 of       686\t|\tloss: 1.74234\n",
      "Training Epoch 20   1.6% | batch:        11 of       686\t|\tloss: 1.26937\n",
      "Training Epoch 20   1.7% | batch:        12 of       686\t|\tloss: 1.49828\n",
      "Training Epoch 20   1.9% | batch:        13 of       686\t|\tloss: 2.12446\n",
      "Training Epoch 20   2.0% | batch:        14 of       686\t|\tloss: 1.20742\n",
      "Training Epoch 20   2.2% | batch:        15 of       686\t|\tloss: 1.59632\n",
      "Training Epoch 20   2.3% | batch:        16 of       686\t|\tloss: 1.74004\n",
      "Training Epoch 20   2.5% | batch:        17 of       686\t|\tloss: 1.73948\n",
      "Training Epoch 20   2.6% | batch:        18 of       686\t|\tloss: 1.97652\n",
      "Training Epoch 20   2.8% | batch:        19 of       686\t|\tloss: 1.95118\n",
      "Training Epoch 20   2.9% | batch:        20 of       686\t|\tloss: 1.402\n",
      "Training Epoch 20   3.1% | batch:        21 of       686\t|\tloss: 1.69676\n",
      "Training Epoch 20   3.2% | batch:        22 of       686\t|\tloss: 1.46026\n",
      "Training Epoch 20   3.4% | batch:        23 of       686\t|\tloss: 1.80722\n",
      "Training Epoch 20   3.5% | batch:        24 of       686\t|\tloss: 1.74456\n",
      "Training Epoch 20   3.6% | batch:        25 of       686\t|\tloss: 1.76218\n",
      "Training Epoch 20   3.8% | batch:        26 of       686\t|\tloss: 1.61109\n",
      "Training Epoch 20   3.9% | batch:        27 of       686\t|\tloss: 1.62268\n",
      "Training Epoch 20   4.1% | batch:        28 of       686\t|\tloss: 1.36482\n",
      "Training Epoch 20   4.2% | batch:        29 of       686\t|\tloss: 1.58112\n",
      "Training Epoch 20   4.4% | batch:        30 of       686\t|\tloss: 1.86087\n",
      "Training Epoch 20   4.5% | batch:        31 of       686\t|\tloss: 1.74617\n",
      "Training Epoch 20   4.7% | batch:        32 of       686\t|\tloss: 2.02902\n",
      "Training Epoch 20   4.8% | batch:        33 of       686\t|\tloss: 2.05445\n",
      "Training Epoch 20   5.0% | batch:        34 of       686\t|\tloss: 1.62393\n",
      "Training Epoch 20   5.1% | batch:        35 of       686\t|\tloss: 1.82356\n",
      "Training Epoch 20   5.2% | batch:        36 of       686\t|\tloss: 1.88938\n",
      "Training Epoch 20   5.4% | batch:        37 of       686\t|\tloss: 2.08949\n",
      "Training Epoch 20   5.5% | batch:        38 of       686\t|\tloss: 1.48559\n",
      "Training Epoch 20   5.7% | batch:        39 of       686\t|\tloss: 2.2776\n",
      "Training Epoch 20   5.8% | batch:        40 of       686\t|\tloss: 2.06469\n",
      "Training Epoch 20   6.0% | batch:        41 of       686\t|\tloss: 1.35852\n",
      "Training Epoch 20   6.1% | batch:        42 of       686\t|\tloss: 1.49335\n",
      "Training Epoch 20   6.3% | batch:        43 of       686\t|\tloss: 1.62216\n",
      "Training Epoch 20   6.4% | batch:        44 of       686\t|\tloss: 1.37606\n",
      "Training Epoch 20   6.6% | batch:        45 of       686\t|\tloss: 1.52219\n",
      "Training Epoch 20   6.7% | batch:        46 of       686\t|\tloss: 2.24434\n",
      "Training Epoch 20   6.9% | batch:        47 of       686\t|\tloss: 1.97606\n",
      "Training Epoch 20   7.0% | batch:        48 of       686\t|\tloss: 1.90185\n",
      "Training Epoch 20   7.1% | batch:        49 of       686\t|\tloss: 1.78663\n",
      "Training Epoch 20   7.3% | batch:        50 of       686\t|\tloss: 1.75855\n",
      "Training Epoch 20   7.4% | batch:        51 of       686\t|\tloss: 2.21508\n",
      "Training Epoch 20   7.6% | batch:        52 of       686\t|\tloss: 1.60534\n",
      "Training Epoch 20   7.7% | batch:        53 of       686\t|\tloss: 1.43452\n",
      "Training Epoch 20   7.9% | batch:        54 of       686\t|\tloss: 1.48602\n",
      "Training Epoch 20   8.0% | batch:        55 of       686\t|\tloss: 1.61963\n",
      "Training Epoch 20   8.2% | batch:        56 of       686\t|\tloss: 2.25631\n",
      "Training Epoch 20   8.3% | batch:        57 of       686\t|\tloss: 1.74132\n",
      "Training Epoch 20   8.5% | batch:        58 of       686\t|\tloss: 2.19035\n",
      "Training Epoch 20   8.6% | batch:        59 of       686\t|\tloss: 2.10657\n",
      "Training Epoch 20   8.7% | batch:        60 of       686\t|\tloss: 1.61583\n",
      "Training Epoch 20   8.9% | batch:        61 of       686\t|\tloss: 1.35424\n",
      "Training Epoch 20   9.0% | batch:        62 of       686\t|\tloss: 2.04366\n",
      "Training Epoch 20   9.2% | batch:        63 of       686\t|\tloss: 2.12688\n",
      "Training Epoch 20   9.3% | batch:        64 of       686\t|\tloss: 1.89172\n",
      "Training Epoch 20   9.5% | batch:        65 of       686\t|\tloss: 2.13993\n",
      "Training Epoch 20   9.6% | batch:        66 of       686\t|\tloss: 1.58557\n",
      "Training Epoch 20   9.8% | batch:        67 of       686\t|\tloss: 1.50671\n",
      "Training Epoch 20   9.9% | batch:        68 of       686\t|\tloss: 2.22809\n",
      "Training Epoch 20  10.1% | batch:        69 of       686\t|\tloss: 1.59865\n",
      "Training Epoch 20  10.2% | batch:        70 of       686\t|\tloss: 1.91021\n",
      "Training Epoch 20  10.3% | batch:        71 of       686\t|\tloss: 1.74757\n",
      "Training Epoch 20  10.5% | batch:        72 of       686\t|\tloss: 1.82939\n",
      "Training Epoch 20  10.6% | batch:        73 of       686\t|\tloss: 1.88281\n",
      "Training Epoch 20  10.8% | batch:        74 of       686\t|\tloss: 1.52486\n",
      "Training Epoch 20  10.9% | batch:        75 of       686\t|\tloss: 1.30542\n",
      "Training Epoch 20  11.1% | batch:        76 of       686\t|\tloss: 1.41889\n",
      "Training Epoch 20  11.2% | batch:        77 of       686\t|\tloss: 1.96075\n",
      "Training Epoch 20  11.4% | batch:        78 of       686\t|\tloss: 1.68766\n",
      "Training Epoch 20  11.5% | batch:        79 of       686\t|\tloss: 1.67507\n",
      "Training Epoch 20  11.7% | batch:        80 of       686\t|\tloss: 1.58053\n",
      "Training Epoch 20  11.8% | batch:        81 of       686\t|\tloss: 1.82763\n",
      "Training Epoch 20  12.0% | batch:        82 of       686\t|\tloss: 1.7941\n",
      "Training Epoch 20  12.1% | batch:        83 of       686\t|\tloss: 2.11493\n",
      "Training Epoch 20  12.2% | batch:        84 of       686\t|\tloss: 1.8655\n",
      "Training Epoch 20  12.4% | batch:        85 of       686\t|\tloss: 1.69175\n",
      "Training Epoch 20  12.5% | batch:        86 of       686\t|\tloss: 1.50625\n",
      "Training Epoch 20  12.7% | batch:        87 of       686\t|\tloss: 1.84984\n",
      "Training Epoch 20  12.8% | batch:        88 of       686\t|\tloss: 1.28282\n",
      "Training Epoch 20  13.0% | batch:        89 of       686\t|\tloss: 2.14077\n",
      "Training Epoch 20  13.1% | batch:        90 of       686\t|\tloss: 1.73695\n",
      "Training Epoch 20  13.3% | batch:        91 of       686\t|\tloss: 1.49748\n",
      "Training Epoch 20  13.4% | batch:        92 of       686\t|\tloss: 1.53131\n",
      "Training Epoch 20  13.6% | batch:        93 of       686\t|\tloss: 1.81772\n",
      "Training Epoch 20  13.7% | batch:        94 of       686\t|\tloss: 2.26249\n",
      "Training Epoch 20  13.8% | batch:        95 of       686\t|\tloss: 1.7565\n",
      "Training Epoch 20  14.0% | batch:        96 of       686\t|\tloss: 1.80851\n",
      "Training Epoch 20  14.1% | batch:        97 of       686\t|\tloss: 1.59085\n",
      "Training Epoch 20  14.3% | batch:        98 of       686\t|\tloss: 1.48063\n",
      "Training Epoch 20  14.4% | batch:        99 of       686\t|\tloss: 1.5851\n",
      "Training Epoch 20  14.6% | batch:       100 of       686\t|\tloss: 1.85703\n",
      "Training Epoch 20  14.7% | batch:       101 of       686\t|\tloss: 1.47957\n",
      "Training Epoch 20  14.9% | batch:       102 of       686\t|\tloss: 1.62817\n",
      "Training Epoch 20  15.0% | batch:       103 of       686\t|\tloss: 1.67517\n",
      "Training Epoch 20  15.2% | batch:       104 of       686\t|\tloss: 2.00574\n",
      "Training Epoch 20  15.3% | batch:       105 of       686\t|\tloss: 1.4674\n",
      "Training Epoch 20  15.5% | batch:       106 of       686\t|\tloss: 2.24293\n",
      "Training Epoch 20  15.6% | batch:       107 of       686\t|\tloss: 2.15975\n",
      "Training Epoch 20  15.7% | batch:       108 of       686\t|\tloss: 2.18987\n",
      "Training Epoch 20  15.9% | batch:       109 of       686\t|\tloss: 1.67201\n",
      "Training Epoch 20  16.0% | batch:       110 of       686\t|\tloss: 1.9554\n",
      "Training Epoch 20  16.2% | batch:       111 of       686\t|\tloss: 2.24873\n",
      "Training Epoch 20  16.3% | batch:       112 of       686\t|\tloss: 1.58654\n",
      "Training Epoch 20  16.5% | batch:       113 of       686\t|\tloss: 1.35449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 20  16.6% | batch:       114 of       686\t|\tloss: 2.60261\n",
      "Training Epoch 20  16.8% | batch:       115 of       686\t|\tloss: 1.94187\n",
      "Training Epoch 20  16.9% | batch:       116 of       686\t|\tloss: 1.95945\n",
      "Training Epoch 20  17.1% | batch:       117 of       686\t|\tloss: 1.99484\n",
      "Training Epoch 20  17.2% | batch:       118 of       686\t|\tloss: 2.0391\n",
      "Training Epoch 20  17.3% | batch:       119 of       686\t|\tloss: 1.81526\n",
      "Training Epoch 20  17.5% | batch:       120 of       686\t|\tloss: 1.3942\n",
      "Training Epoch 20  17.6% | batch:       121 of       686\t|\tloss: 1.64998\n",
      "Training Epoch 20  17.8% | batch:       122 of       686\t|\tloss: 1.42496\n",
      "Training Epoch 20  17.9% | batch:       123 of       686\t|\tloss: 1.83782\n",
      "Training Epoch 20  18.1% | batch:       124 of       686\t|\tloss: 1.89107\n",
      "Training Epoch 20  18.2% | batch:       125 of       686\t|\tloss: 1.89462\n",
      "Training Epoch 20  18.4% | batch:       126 of       686\t|\tloss: 2.11243\n",
      "Training Epoch 20  18.5% | batch:       127 of       686\t|\tloss: 1.40464\n",
      "Training Epoch 20  18.7% | batch:       128 of       686\t|\tloss: 2.25232\n",
      "Training Epoch 20  18.8% | batch:       129 of       686\t|\tloss: 1.95091\n",
      "Training Epoch 20  19.0% | batch:       130 of       686\t|\tloss: 1.79805\n",
      "Training Epoch 20  19.1% | batch:       131 of       686\t|\tloss: 1.62935\n",
      "Training Epoch 20  19.2% | batch:       132 of       686\t|\tloss: 1.71895\n",
      "Training Epoch 20  19.4% | batch:       133 of       686\t|\tloss: 1.31419\n",
      "Training Epoch 20  19.5% | batch:       134 of       686\t|\tloss: 1.65639\n",
      "Training Epoch 20  19.7% | batch:       135 of       686\t|\tloss: 2.00369\n",
      "Training Epoch 20  19.8% | batch:       136 of       686\t|\tloss: 1.92236\n",
      "Training Epoch 20  20.0% | batch:       137 of       686\t|\tloss: 2.00297\n",
      "Training Epoch 20  20.1% | batch:       138 of       686\t|\tloss: 2.04563\n",
      "Training Epoch 20  20.3% | batch:       139 of       686\t|\tloss: 2.19696\n",
      "Training Epoch 20  20.4% | batch:       140 of       686\t|\tloss: 1.83182\n",
      "Training Epoch 20  20.6% | batch:       141 of       686\t|\tloss: 1.48439\n",
      "Training Epoch 20  20.7% | batch:       142 of       686\t|\tloss: 1.60715\n",
      "Training Epoch 20  20.8% | batch:       143 of       686\t|\tloss: 1.81609\n",
      "Training Epoch 20  21.0% | batch:       144 of       686\t|\tloss: 1.7429\n",
      "Training Epoch 20  21.1% | batch:       145 of       686\t|\tloss: 1.48395\n",
      "Training Epoch 20  21.3% | batch:       146 of       686\t|\tloss: 1.43703\n",
      "Training Epoch 20  21.4% | batch:       147 of       686\t|\tloss: 1.25691\n",
      "Training Epoch 20  21.6% | batch:       148 of       686\t|\tloss: 1.87468\n",
      "Training Epoch 20  21.7% | batch:       149 of       686\t|\tloss: 1.82753\n",
      "Training Epoch 20  21.9% | batch:       150 of       686\t|\tloss: 2.15925\n",
      "Training Epoch 20  22.0% | batch:       151 of       686\t|\tloss: 1.39902\n",
      "Training Epoch 20  22.2% | batch:       152 of       686\t|\tloss: 2.0043\n",
      "Training Epoch 20  22.3% | batch:       153 of       686\t|\tloss: 1.79809\n",
      "Training Epoch 20  22.4% | batch:       154 of       686\t|\tloss: 1.72138\n",
      "Training Epoch 20  22.6% | batch:       155 of       686\t|\tloss: 1.68534\n",
      "Training Epoch 20  22.7% | batch:       156 of       686\t|\tloss: 1.80185\n",
      "Training Epoch 20  22.9% | batch:       157 of       686\t|\tloss: 1.75042\n",
      "Training Epoch 20  23.0% | batch:       158 of       686\t|\tloss: 2.36912\n",
      "Training Epoch 20  23.2% | batch:       159 of       686\t|\tloss: 1.72309\n",
      "Training Epoch 20  23.3% | batch:       160 of       686\t|\tloss: 1.59302\n",
      "Training Epoch 20  23.5% | batch:       161 of       686\t|\tloss: 1.37878\n",
      "Training Epoch 20  23.6% | batch:       162 of       686\t|\tloss: 1.81462\n",
      "Training Epoch 20  23.8% | batch:       163 of       686\t|\tloss: 1.65863\n",
      "Training Epoch 20  23.9% | batch:       164 of       686\t|\tloss: 2.58687\n",
      "Training Epoch 20  24.1% | batch:       165 of       686\t|\tloss: 2.32112\n",
      "Training Epoch 20  24.2% | batch:       166 of       686\t|\tloss: 1.5467\n",
      "Training Epoch 20  24.3% | batch:       167 of       686\t|\tloss: 1.82311\n",
      "Training Epoch 20  24.5% | batch:       168 of       686\t|\tloss: 1.69759\n",
      "Training Epoch 20  24.6% | batch:       169 of       686\t|\tloss: 1.64609\n",
      "Training Epoch 20  24.8% | batch:       170 of       686\t|\tloss: 1.61323\n",
      "Training Epoch 20  24.9% | batch:       171 of       686\t|\tloss: 3.09593\n",
      "Training Epoch 20  25.1% | batch:       172 of       686\t|\tloss: 1.36731\n",
      "Training Epoch 20  25.2% | batch:       173 of       686\t|\tloss: 1.73505\n",
      "Training Epoch 20  25.4% | batch:       174 of       686\t|\tloss: 1.30838\n",
      "Training Epoch 20  25.5% | batch:       175 of       686\t|\tloss: 2.00113\n",
      "Training Epoch 20  25.7% | batch:       176 of       686\t|\tloss: 1.60512\n",
      "Training Epoch 20  25.8% | batch:       177 of       686\t|\tloss: 1.69845\n",
      "Training Epoch 20  25.9% | batch:       178 of       686\t|\tloss: 1.48613\n",
      "Training Epoch 20  26.1% | batch:       179 of       686\t|\tloss: 2.01145\n",
      "Training Epoch 20  26.2% | batch:       180 of       686\t|\tloss: 2.38014\n",
      "Training Epoch 20  26.4% | batch:       181 of       686\t|\tloss: 1.49029\n",
      "Training Epoch 20  26.5% | batch:       182 of       686\t|\tloss: 1.2397\n",
      "Training Epoch 20  26.7% | batch:       183 of       686\t|\tloss: 2.05069\n",
      "Training Epoch 20  26.8% | batch:       184 of       686\t|\tloss: 1.70649\n",
      "Training Epoch 20  27.0% | batch:       185 of       686\t|\tloss: 1.50475\n",
      "Training Epoch 20  27.1% | batch:       186 of       686\t|\tloss: 1.68887\n",
      "Training Epoch 20  27.3% | batch:       187 of       686\t|\tloss: 1.74313\n",
      "Training Epoch 20  27.4% | batch:       188 of       686\t|\tloss: 1.59191\n",
      "Training Epoch 20  27.6% | batch:       189 of       686\t|\tloss: 1.65521\n",
      "Training Epoch 20  27.7% | batch:       190 of       686\t|\tloss: 1.73697\n",
      "Training Epoch 20  27.8% | batch:       191 of       686\t|\tloss: 1.70243\n",
      "Training Epoch 20  28.0% | batch:       192 of       686\t|\tloss: 2.12893\n",
      "Training Epoch 20  28.1% | batch:       193 of       686\t|\tloss: 1.89597\n",
      "Training Epoch 20  28.3% | batch:       194 of       686\t|\tloss: 1.65762\n",
      "Training Epoch 20  28.4% | batch:       195 of       686\t|\tloss: 1.60206\n",
      "Training Epoch 20  28.6% | batch:       196 of       686\t|\tloss: 2.4471\n",
      "Training Epoch 20  28.7% | batch:       197 of       686\t|\tloss: 1.46976\n",
      "Training Epoch 20  28.9% | batch:       198 of       686\t|\tloss: 2.16583\n",
      "Training Epoch 20  29.0% | batch:       199 of       686\t|\tloss: 1.78891\n",
      "Training Epoch 20  29.2% | batch:       200 of       686\t|\tloss: 1.60204\n",
      "Training Epoch 20  29.3% | batch:       201 of       686\t|\tloss: 1.85612\n",
      "Training Epoch 20  29.4% | batch:       202 of       686\t|\tloss: 1.37156\n",
      "Training Epoch 20  29.6% | batch:       203 of       686\t|\tloss: 1.61272\n",
      "Training Epoch 20  29.7% | batch:       204 of       686\t|\tloss: 2.14027\n",
      "Training Epoch 20  29.9% | batch:       205 of       686\t|\tloss: 1.81062\n",
      "Training Epoch 20  30.0% | batch:       206 of       686\t|\tloss: 1.69681\n",
      "Training Epoch 20  30.2% | batch:       207 of       686\t|\tloss: 1.7558\n",
      "Training Epoch 20  30.3% | batch:       208 of       686\t|\tloss: 1.71657\n",
      "Training Epoch 20  30.5% | batch:       209 of       686\t|\tloss: 1.58002\n",
      "Training Epoch 20  30.6% | batch:       210 of       686\t|\tloss: 1.79405\n",
      "Training Epoch 20  30.8% | batch:       211 of       686\t|\tloss: 2.12318\n",
      "Training Epoch 20  30.9% | batch:       212 of       686\t|\tloss: 2.84973\n",
      "Training Epoch 20  31.0% | batch:       213 of       686\t|\tloss: 1.70515\n",
      "Training Epoch 20  31.2% | batch:       214 of       686\t|\tloss: 1.28222\n",
      "Training Epoch 20  31.3% | batch:       215 of       686\t|\tloss: 1.73535\n",
      "Training Epoch 20  31.5% | batch:       216 of       686\t|\tloss: 1.57423\n",
      "Training Epoch 20  31.6% | batch:       217 of       686\t|\tloss: 1.62761\n",
      "Training Epoch 20  31.8% | batch:       218 of       686\t|\tloss: 1.39259\n",
      "Training Epoch 20  31.9% | batch:       219 of       686\t|\tloss: 2.20838\n",
      "Training Epoch 20  32.1% | batch:       220 of       686\t|\tloss: 1.45252\n",
      "Training Epoch 20  32.2% | batch:       221 of       686\t|\tloss: 1.48638\n",
      "Training Epoch 20  32.4% | batch:       222 of       686\t|\tloss: 1.98444\n",
      "Training Epoch 20  32.5% | batch:       223 of       686\t|\tloss: 1.98291\n",
      "Training Epoch 20  32.7% | batch:       224 of       686\t|\tloss: 1.60847\n",
      "Training Epoch 20  32.8% | batch:       225 of       686\t|\tloss: 1.8112\n",
      "Training Epoch 20  32.9% | batch:       226 of       686\t|\tloss: 1.99911\n",
      "Training Epoch 20  33.1% | batch:       227 of       686\t|\tloss: 1.62334\n",
      "Training Epoch 20  33.2% | batch:       228 of       686\t|\tloss: 1.58724\n",
      "Training Epoch 20  33.4% | batch:       229 of       686\t|\tloss: 1.6979\n",
      "Training Epoch 20  33.5% | batch:       230 of       686\t|\tloss: 1.65355\n",
      "Training Epoch 20  33.7% | batch:       231 of       686\t|\tloss: 1.72718\n",
      "Training Epoch 20  33.8% | batch:       232 of       686\t|\tloss: 1.42927\n",
      "Training Epoch 20  34.0% | batch:       233 of       686\t|\tloss: 1.35532\n",
      "Training Epoch 20  34.1% | batch:       234 of       686\t|\tloss: 1.61587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 20  34.3% | batch:       235 of       686\t|\tloss: 1.73022\n",
      "Training Epoch 20  34.4% | batch:       236 of       686\t|\tloss: 1.69751\n",
      "Training Epoch 20  34.5% | batch:       237 of       686\t|\tloss: 1.77551\n",
      "Training Epoch 20  34.7% | batch:       238 of       686\t|\tloss: 2.10291\n",
      "Training Epoch 20  34.8% | batch:       239 of       686\t|\tloss: 2.12101\n",
      "Training Epoch 20  35.0% | batch:       240 of       686\t|\tloss: 1.42304\n",
      "Training Epoch 20  35.1% | batch:       241 of       686\t|\tloss: 1.97822\n",
      "Training Epoch 20  35.3% | batch:       242 of       686\t|\tloss: 1.82378\n",
      "Training Epoch 20  35.4% | batch:       243 of       686\t|\tloss: 2.40363\n",
      "Training Epoch 20  35.6% | batch:       244 of       686\t|\tloss: 1.71538\n",
      "Training Epoch 20  35.7% | batch:       245 of       686\t|\tloss: 1.48788\n",
      "Training Epoch 20  35.9% | batch:       246 of       686\t|\tloss: 1.86412\n",
      "Training Epoch 20  36.0% | batch:       247 of       686\t|\tloss: 1.94552\n",
      "Training Epoch 20  36.2% | batch:       248 of       686\t|\tloss: 1.5478\n",
      "Training Epoch 20  36.3% | batch:       249 of       686\t|\tloss: 1.758\n",
      "Training Epoch 20  36.4% | batch:       250 of       686\t|\tloss: 1.16519\n",
      "Training Epoch 20  36.6% | batch:       251 of       686\t|\tloss: 1.82118\n",
      "Training Epoch 20  36.7% | batch:       252 of       686\t|\tloss: 1.68055\n",
      "Training Epoch 20  36.9% | batch:       253 of       686\t|\tloss: 1.61602\n",
      "Training Epoch 20  37.0% | batch:       254 of       686\t|\tloss: 2.1786\n",
      "Training Epoch 20  37.2% | batch:       255 of       686\t|\tloss: 2.28012\n",
      "Training Epoch 20  37.3% | batch:       256 of       686\t|\tloss: 2.55736\n",
      "Training Epoch 20  37.5% | batch:       257 of       686\t|\tloss: 1.57264\n",
      "Training Epoch 20  37.6% | batch:       258 of       686\t|\tloss: 1.76752\n",
      "Training Epoch 20  37.8% | batch:       259 of       686\t|\tloss: 1.63235\n",
      "Training Epoch 20  37.9% | batch:       260 of       686\t|\tloss: 2.04637\n",
      "Training Epoch 20  38.0% | batch:       261 of       686\t|\tloss: 1.49347\n",
      "Training Epoch 20  38.2% | batch:       262 of       686\t|\tloss: 1.51754\n",
      "Training Epoch 20  38.3% | batch:       263 of       686\t|\tloss: 1.82744\n",
      "Training Epoch 20  38.5% | batch:       264 of       686\t|\tloss: 1.39988\n",
      "Training Epoch 20  38.6% | batch:       265 of       686\t|\tloss: 1.5887\n",
      "Training Epoch 20  38.8% | batch:       266 of       686\t|\tloss: 1.44925\n",
      "Training Epoch 20  38.9% | batch:       267 of       686\t|\tloss: 1.5789\n",
      "Training Epoch 20  39.1% | batch:       268 of       686\t|\tloss: 1.66751\n",
      "Training Epoch 20  39.2% | batch:       269 of       686\t|\tloss: 1.6438\n",
      "Training Epoch 20  39.4% | batch:       270 of       686\t|\tloss: 1.75551\n",
      "Training Epoch 20  39.5% | batch:       271 of       686\t|\tloss: 1.8572\n",
      "Training Epoch 20  39.7% | batch:       272 of       686\t|\tloss: 1.48393\n",
      "Training Epoch 20  39.8% | batch:       273 of       686\t|\tloss: 2.00445\n",
      "Training Epoch 20  39.9% | batch:       274 of       686\t|\tloss: 2.03717\n",
      "Training Epoch 20  40.1% | batch:       275 of       686\t|\tloss: 1.66215\n",
      "Training Epoch 20  40.2% | batch:       276 of       686\t|\tloss: 1.6987\n",
      "Training Epoch 20  40.4% | batch:       277 of       686\t|\tloss: 2.07281\n",
      "Training Epoch 20  40.5% | batch:       278 of       686\t|\tloss: 2.13627\n",
      "Training Epoch 20  40.7% | batch:       279 of       686\t|\tloss: 1.87052\n",
      "Training Epoch 20  40.8% | batch:       280 of       686\t|\tloss: 1.41492\n",
      "Training Epoch 20  41.0% | batch:       281 of       686\t|\tloss: 1.77586\n",
      "Training Epoch 20  41.1% | batch:       282 of       686\t|\tloss: 1.4363\n",
      "Training Epoch 20  41.3% | batch:       283 of       686\t|\tloss: 1.47004\n",
      "Training Epoch 20  41.4% | batch:       284 of       686\t|\tloss: 1.7246\n",
      "Training Epoch 20  41.5% | batch:       285 of       686\t|\tloss: 1.74167\n",
      "Training Epoch 20  41.7% | batch:       286 of       686\t|\tloss: 1.91991\n",
      "Training Epoch 20  41.8% | batch:       287 of       686\t|\tloss: 1.35236\n",
      "Training Epoch 20  42.0% | batch:       288 of       686\t|\tloss: 1.69443\n",
      "Training Epoch 20  42.1% | batch:       289 of       686\t|\tloss: 1.64545\n",
      "Training Epoch 20  42.3% | batch:       290 of       686\t|\tloss: 1.56667\n",
      "Training Epoch 20  42.4% | batch:       291 of       686\t|\tloss: 1.78206\n",
      "Training Epoch 20  42.6% | batch:       292 of       686\t|\tloss: 2.03246\n",
      "Training Epoch 20  42.7% | batch:       293 of       686\t|\tloss: 1.77655\n",
      "Training Epoch 20  42.9% | batch:       294 of       686\t|\tloss: 1.47322\n",
      "Training Epoch 20  43.0% | batch:       295 of       686\t|\tloss: 1.97829\n",
      "Training Epoch 20  43.1% | batch:       296 of       686\t|\tloss: 1.23784\n",
      "Training Epoch 20  43.3% | batch:       297 of       686\t|\tloss: 1.23492\n",
      "Training Epoch 20  43.4% | batch:       298 of       686\t|\tloss: 1.58343\n",
      "Training Epoch 20  43.6% | batch:       299 of       686\t|\tloss: 1.76653\n",
      "Training Epoch 20  43.7% | batch:       300 of       686\t|\tloss: 1.87741\n",
      "Training Epoch 20  43.9% | batch:       301 of       686\t|\tloss: 2.45292\n",
      "Training Epoch 20  44.0% | batch:       302 of       686\t|\tloss: 1.70614\n",
      "Training Epoch 20  44.2% | batch:       303 of       686\t|\tloss: 1.42182\n",
      "Training Epoch 20  44.3% | batch:       304 of       686\t|\tloss: 1.57497\n",
      "Training Epoch 20  44.5% | batch:       305 of       686\t|\tloss: 1.92835\n",
      "Training Epoch 20  44.6% | batch:       306 of       686\t|\tloss: 1.76493\n",
      "Training Epoch 20  44.8% | batch:       307 of       686\t|\tloss: 1.90518\n",
      "Training Epoch 20  44.9% | batch:       308 of       686\t|\tloss: 2.87599\n",
      "Training Epoch 20  45.0% | batch:       309 of       686\t|\tloss: 1.83581\n",
      "Training Epoch 20  45.2% | batch:       310 of       686\t|\tloss: 1.43269\n",
      "Training Epoch 20  45.3% | batch:       311 of       686\t|\tloss: 1.41762\n",
      "Training Epoch 20  45.5% | batch:       312 of       686\t|\tloss: 2.17035\n",
      "Training Epoch 20  45.6% | batch:       313 of       686\t|\tloss: 1.35644\n",
      "Training Epoch 20  45.8% | batch:       314 of       686\t|\tloss: 1.67322\n",
      "Training Epoch 20  45.9% | batch:       315 of       686\t|\tloss: 1.86133\n",
      "Training Epoch 20  46.1% | batch:       316 of       686\t|\tloss: 1.51812\n",
      "Training Epoch 20  46.2% | batch:       317 of       686\t|\tloss: 1.20843\n",
      "Training Epoch 20  46.4% | batch:       318 of       686\t|\tloss: 1.63631\n",
      "Training Epoch 20  46.5% | batch:       319 of       686\t|\tloss: 1.44117\n",
      "Training Epoch 20  46.6% | batch:       320 of       686\t|\tloss: 2.08007\n",
      "Training Epoch 20  46.8% | batch:       321 of       686\t|\tloss: 1.82552\n",
      "Training Epoch 20  46.9% | batch:       322 of       686\t|\tloss: 1.75551\n",
      "Training Epoch 20  47.1% | batch:       323 of       686\t|\tloss: 1.60442\n",
      "Training Epoch 20  47.2% | batch:       324 of       686\t|\tloss: 2.17474\n",
      "Training Epoch 20  47.4% | batch:       325 of       686\t|\tloss: 1.13354\n",
      "Training Epoch 20  47.5% | batch:       326 of       686\t|\tloss: 1.71957\n",
      "Training Epoch 20  47.7% | batch:       327 of       686\t|\tloss: 1.41241\n",
      "Training Epoch 20  47.8% | batch:       328 of       686\t|\tloss: 1.57231\n",
      "Training Epoch 20  48.0% | batch:       329 of       686\t|\tloss: 1.40144\n",
      "Training Epoch 20  48.1% | batch:       330 of       686\t|\tloss: 1.69342\n",
      "Training Epoch 20  48.3% | batch:       331 of       686\t|\tloss: 1.9637\n",
      "Training Epoch 20  48.4% | batch:       332 of       686\t|\tloss: 1.94225\n",
      "Training Epoch 20  48.5% | batch:       333 of       686\t|\tloss: 1.80625\n",
      "Training Epoch 20  48.7% | batch:       334 of       686\t|\tloss: 1.49906\n",
      "Training Epoch 20  48.8% | batch:       335 of       686\t|\tloss: 1.93009\n",
      "Training Epoch 20  49.0% | batch:       336 of       686\t|\tloss: 1.57075\n",
      "Training Epoch 20  49.1% | batch:       337 of       686\t|\tloss: 1.60366\n",
      "Training Epoch 20  49.3% | batch:       338 of       686\t|\tloss: 1.30699\n",
      "Training Epoch 20  49.4% | batch:       339 of       686\t|\tloss: 1.72166\n",
      "Training Epoch 20  49.6% | batch:       340 of       686\t|\tloss: 1.81241\n",
      "Training Epoch 20  49.7% | batch:       341 of       686\t|\tloss: 1.88007\n",
      "Training Epoch 20  49.9% | batch:       342 of       686\t|\tloss: 1.93944\n",
      "Training Epoch 20  50.0% | batch:       343 of       686\t|\tloss: 2.04552\n",
      "Training Epoch 20  50.1% | batch:       344 of       686\t|\tloss: 2.05921\n",
      "Training Epoch 20  50.3% | batch:       345 of       686\t|\tloss: 2.06607\n",
      "Training Epoch 20  50.4% | batch:       346 of       686\t|\tloss: 1.51766\n",
      "Training Epoch 20  50.6% | batch:       347 of       686\t|\tloss: 1.49168\n",
      "Training Epoch 20  50.7% | batch:       348 of       686\t|\tloss: 1.92307\n",
      "Training Epoch 20  50.9% | batch:       349 of       686\t|\tloss: 1.59652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 20  51.0% | batch:       350 of       686\t|\tloss: 2.05823\n",
      "Training Epoch 20  51.2% | batch:       351 of       686\t|\tloss: 1.50405\n",
      "Training Epoch 20  51.3% | batch:       352 of       686\t|\tloss: 1.59664\n",
      "Training Epoch 20  51.5% | batch:       353 of       686\t|\tloss: 1.77704\n",
      "Training Epoch 20  51.6% | batch:       354 of       686\t|\tloss: 1.83132\n",
      "Training Epoch 20  51.7% | batch:       355 of       686\t|\tloss: 1.83358\n",
      "Training Epoch 20  51.9% | batch:       356 of       686\t|\tloss: 1.36129\n",
      "Training Epoch 20  52.0% | batch:       357 of       686\t|\tloss: 1.46854\n",
      "Training Epoch 20  52.2% | batch:       358 of       686\t|\tloss: 1.41323\n",
      "Training Epoch 20  52.3% | batch:       359 of       686\t|\tloss: 1.48624\n",
      "Training Epoch 20  52.5% | batch:       360 of       686\t|\tloss: 1.69542\n",
      "Training Epoch 20  52.6% | batch:       361 of       686\t|\tloss: 1.35987\n",
      "Training Epoch 20  52.8% | batch:       362 of       686\t|\tloss: 1.5154\n",
      "Training Epoch 20  52.9% | batch:       363 of       686\t|\tloss: 1.31726\n",
      "Training Epoch 20  53.1% | batch:       364 of       686\t|\tloss: 2.00933\n",
      "Training Epoch 20  53.2% | batch:       365 of       686\t|\tloss: 1.84827\n",
      "Training Epoch 20  53.4% | batch:       366 of       686\t|\tloss: 1.58589\n",
      "Training Epoch 20  53.5% | batch:       367 of       686\t|\tloss: 2.28808\n",
      "Training Epoch 20  53.6% | batch:       368 of       686\t|\tloss: 1.93504\n",
      "Training Epoch 20  53.8% | batch:       369 of       686\t|\tloss: 1.44613\n",
      "Training Epoch 20  53.9% | batch:       370 of       686\t|\tloss: 1.54604\n",
      "Training Epoch 20  54.1% | batch:       371 of       686\t|\tloss: 1.99158\n",
      "Training Epoch 20  54.2% | batch:       372 of       686\t|\tloss: 1.5765\n",
      "Training Epoch 20  54.4% | batch:       373 of       686\t|\tloss: 1.48613\n",
      "Training Epoch 20  54.5% | batch:       374 of       686\t|\tloss: 1.51027\n",
      "Training Epoch 20  54.7% | batch:       375 of       686\t|\tloss: 1.98004\n",
      "Training Epoch 20  54.8% | batch:       376 of       686\t|\tloss: 2.21443\n",
      "Training Epoch 20  55.0% | batch:       377 of       686\t|\tloss: 2.33633\n",
      "Training Epoch 20  55.1% | batch:       378 of       686\t|\tloss: 1.69749\n",
      "Training Epoch 20  55.2% | batch:       379 of       686\t|\tloss: 1.47076\n",
      "Training Epoch 20  55.4% | batch:       380 of       686\t|\tloss: 1.29175\n",
      "Training Epoch 20  55.5% | batch:       381 of       686\t|\tloss: 1.55548\n",
      "Training Epoch 20  55.7% | batch:       382 of       686\t|\tloss: 1.70364\n",
      "Training Epoch 20  55.8% | batch:       383 of       686\t|\tloss: 1.63723\n",
      "Training Epoch 20  56.0% | batch:       384 of       686\t|\tloss: 1.68588\n",
      "Training Epoch 20  56.1% | batch:       385 of       686\t|\tloss: 1.65034\n",
      "Training Epoch 20  56.3% | batch:       386 of       686\t|\tloss: 1.23392\n",
      "Training Epoch 20  56.4% | batch:       387 of       686\t|\tloss: 1.46574\n",
      "Training Epoch 20  56.6% | batch:       388 of       686\t|\tloss: 1.81919\n",
      "Training Epoch 20  56.7% | batch:       389 of       686\t|\tloss: 1.85054\n",
      "Training Epoch 20  56.9% | batch:       390 of       686\t|\tloss: 1.42602\n",
      "Training Epoch 20  57.0% | batch:       391 of       686\t|\tloss: 1.63414\n",
      "Training Epoch 20  57.1% | batch:       392 of       686\t|\tloss: 1.11613\n",
      "Training Epoch 20  57.3% | batch:       393 of       686\t|\tloss: 2.18923\n",
      "Training Epoch 20  57.4% | batch:       394 of       686\t|\tloss: 1.91561\n",
      "Training Epoch 20  57.6% | batch:       395 of       686\t|\tloss: 1.8078\n",
      "Training Epoch 20  57.7% | batch:       396 of       686\t|\tloss: 1.1584\n",
      "Training Epoch 20  57.9% | batch:       397 of       686\t|\tloss: 1.93784\n",
      "Training Epoch 20  58.0% | batch:       398 of       686\t|\tloss: 1.46555\n",
      "Training Epoch 20  58.2% | batch:       399 of       686\t|\tloss: 2.11443\n",
      "Training Epoch 20  58.3% | batch:       400 of       686\t|\tloss: 1.15971\n",
      "Training Epoch 20  58.5% | batch:       401 of       686\t|\tloss: 1.98228\n",
      "Training Epoch 20  58.6% | batch:       402 of       686\t|\tloss: 2.09984\n",
      "Training Epoch 20  58.7% | batch:       403 of       686\t|\tloss: 1.57568\n",
      "Training Epoch 20  58.9% | batch:       404 of       686\t|\tloss: 1.33261\n",
      "Training Epoch 20  59.0% | batch:       405 of       686\t|\tloss: 1.95567\n",
      "Training Epoch 20  59.2% | batch:       406 of       686\t|\tloss: 1.70499\n",
      "Training Epoch 20  59.3% | batch:       407 of       686\t|\tloss: 1.55776\n",
      "Training Epoch 20  59.5% | batch:       408 of       686\t|\tloss: 1.37238\n",
      "Training Epoch 20  59.6% | batch:       409 of       686\t|\tloss: 1.76564\n",
      "Training Epoch 20  59.8% | batch:       410 of       686\t|\tloss: 1.78444\n",
      "Training Epoch 20  59.9% | batch:       411 of       686\t|\tloss: 1.90549\n",
      "Training Epoch 20  60.1% | batch:       412 of       686\t|\tloss: 1.49335\n",
      "Training Epoch 20  60.2% | batch:       413 of       686\t|\tloss: 2.0253\n",
      "Training Epoch 20  60.3% | batch:       414 of       686\t|\tloss: 2.42791\n",
      "Training Epoch 20  60.5% | batch:       415 of       686\t|\tloss: 1.52144\n",
      "Training Epoch 20  60.6% | batch:       416 of       686\t|\tloss: 1.43292\n",
      "Training Epoch 20  60.8% | batch:       417 of       686\t|\tloss: 1.34578\n",
      "Training Epoch 20  60.9% | batch:       418 of       686\t|\tloss: 1.5407\n",
      "Training Epoch 20  61.1% | batch:       419 of       686\t|\tloss: 1.83892\n",
      "Training Epoch 20  61.2% | batch:       420 of       686\t|\tloss: 1.55441\n",
      "Training Epoch 20  61.4% | batch:       421 of       686\t|\tloss: 1.73318\n",
      "Training Epoch 20  61.5% | batch:       422 of       686\t|\tloss: 1.64601\n",
      "Training Epoch 20  61.7% | batch:       423 of       686\t|\tloss: 1.47902\n",
      "Training Epoch 20  61.8% | batch:       424 of       686\t|\tloss: 1.15029\n",
      "Training Epoch 20  62.0% | batch:       425 of       686\t|\tloss: 1.93806\n",
      "Training Epoch 20  62.1% | batch:       426 of       686\t|\tloss: 1.56652\n",
      "Training Epoch 20  62.2% | batch:       427 of       686\t|\tloss: 1.32576\n",
      "Training Epoch 20  62.4% | batch:       428 of       686\t|\tloss: 1.66459\n",
      "Training Epoch 20  62.5% | batch:       429 of       686\t|\tloss: 2.02611\n",
      "Training Epoch 20  62.7% | batch:       430 of       686\t|\tloss: 1.89302\n",
      "Training Epoch 20  62.8% | batch:       431 of       686\t|\tloss: 1.67308\n",
      "Training Epoch 20  63.0% | batch:       432 of       686\t|\tloss: 2.19979\n",
      "Training Epoch 20  63.1% | batch:       433 of       686\t|\tloss: 1.54602\n",
      "Training Epoch 20  63.3% | batch:       434 of       686\t|\tloss: 1.68397\n",
      "Training Epoch 20  63.4% | batch:       435 of       686\t|\tloss: 1.92477\n",
      "Training Epoch 20  63.6% | batch:       436 of       686\t|\tloss: 2.01021\n",
      "Training Epoch 20  63.7% | batch:       437 of       686\t|\tloss: 1.48489\n",
      "Training Epoch 20  63.8% | batch:       438 of       686\t|\tloss: 1.44766\n",
      "Training Epoch 20  64.0% | batch:       439 of       686\t|\tloss: 1.55385\n",
      "Training Epoch 20  64.1% | batch:       440 of       686\t|\tloss: 1.803\n",
      "Training Epoch 20  64.3% | batch:       441 of       686\t|\tloss: 1.56347\n",
      "Training Epoch 20  64.4% | batch:       442 of       686\t|\tloss: 1.53942\n",
      "Training Epoch 20  64.6% | batch:       443 of       686\t|\tloss: 2.42546\n",
      "Training Epoch 20  64.7% | batch:       444 of       686\t|\tloss: 1.58046\n",
      "Training Epoch 20  64.9% | batch:       445 of       686\t|\tloss: 1.66999\n",
      "Training Epoch 20  65.0% | batch:       446 of       686\t|\tloss: 1.95251\n",
      "Training Epoch 20  65.2% | batch:       447 of       686\t|\tloss: 1.61025\n",
      "Training Epoch 20  65.3% | batch:       448 of       686\t|\tloss: 1.39996\n",
      "Training Epoch 20  65.5% | batch:       449 of       686\t|\tloss: 1.38889\n",
      "Training Epoch 20  65.6% | batch:       450 of       686\t|\tloss: 1.3899\n",
      "Training Epoch 20  65.7% | batch:       451 of       686\t|\tloss: 1.77449\n",
      "Training Epoch 20  65.9% | batch:       452 of       686\t|\tloss: 1.71626\n",
      "Training Epoch 20  66.0% | batch:       453 of       686\t|\tloss: 1.42481\n",
      "Training Epoch 20  66.2% | batch:       454 of       686\t|\tloss: 1.66552\n",
      "Training Epoch 20  66.3% | batch:       455 of       686\t|\tloss: 1.85515\n",
      "Training Epoch 20  66.5% | batch:       456 of       686\t|\tloss: 1.722\n",
      "Training Epoch 20  66.6% | batch:       457 of       686\t|\tloss: 1.5542\n",
      "Training Epoch 20  66.8% | batch:       458 of       686\t|\tloss: 2.07638\n",
      "Training Epoch 20  66.9% | batch:       459 of       686\t|\tloss: 2.33248\n",
      "Training Epoch 20  67.1% | batch:       460 of       686\t|\tloss: 1.63926\n",
      "Training Epoch 20  67.2% | batch:       461 of       686\t|\tloss: 2.04384\n",
      "Training Epoch 20  67.3% | batch:       462 of       686\t|\tloss: 1.5021\n",
      "Training Epoch 20  67.5% | batch:       463 of       686\t|\tloss: 1.9085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 20  67.6% | batch:       464 of       686\t|\tloss: 1.66873\n",
      "Training Epoch 20  67.8% | batch:       465 of       686\t|\tloss: 1.37833\n",
      "Training Epoch 20  67.9% | batch:       466 of       686\t|\tloss: 1.58972\n",
      "Training Epoch 20  68.1% | batch:       467 of       686\t|\tloss: 1.36682\n",
      "Training Epoch 20  68.2% | batch:       468 of       686\t|\tloss: 2.04385\n",
      "Training Epoch 20  68.4% | batch:       469 of       686\t|\tloss: 1.90924\n",
      "Training Epoch 20  68.5% | batch:       470 of       686\t|\tloss: 1.47279\n",
      "Training Epoch 20  68.7% | batch:       471 of       686\t|\tloss: 1.60572\n",
      "Training Epoch 20  68.8% | batch:       472 of       686\t|\tloss: 1.51435\n",
      "Training Epoch 20  69.0% | batch:       473 of       686\t|\tloss: 1.36791\n",
      "Training Epoch 20  69.1% | batch:       474 of       686\t|\tloss: 1.61196\n",
      "Training Epoch 20  69.2% | batch:       475 of       686\t|\tloss: 1.57352\n",
      "Training Epoch 20  69.4% | batch:       476 of       686\t|\tloss: 1.31721\n",
      "Training Epoch 20  69.5% | batch:       477 of       686\t|\tloss: 1.25367\n",
      "Training Epoch 20  69.7% | batch:       478 of       686\t|\tloss: 1.55855\n",
      "Training Epoch 20  69.8% | batch:       479 of       686\t|\tloss: 1.5701\n",
      "Training Epoch 20  70.0% | batch:       480 of       686\t|\tloss: 1.94115\n",
      "Training Epoch 20  70.1% | batch:       481 of       686\t|\tloss: 1.65131\n",
      "Training Epoch 20  70.3% | batch:       482 of       686\t|\tloss: 1.67412\n",
      "Training Epoch 20  70.4% | batch:       483 of       686\t|\tloss: 1.70379\n",
      "Training Epoch 20  70.6% | batch:       484 of       686\t|\tloss: 1.37159\n",
      "Training Epoch 20  70.7% | batch:       485 of       686\t|\tloss: 1.66362\n",
      "Training Epoch 20  70.8% | batch:       486 of       686\t|\tloss: 1.73253\n",
      "Training Epoch 20  71.0% | batch:       487 of       686\t|\tloss: 1.54439\n",
      "Training Epoch 20  71.1% | batch:       488 of       686\t|\tloss: 1.5048\n",
      "Training Epoch 20  71.3% | batch:       489 of       686\t|\tloss: 1.73775\n",
      "Training Epoch 20  71.4% | batch:       490 of       686\t|\tloss: 2.31841\n",
      "Training Epoch 20  71.6% | batch:       491 of       686\t|\tloss: 1.91289\n",
      "Training Epoch 20  71.7% | batch:       492 of       686\t|\tloss: 2.19325\n",
      "Training Epoch 20  71.9% | batch:       493 of       686\t|\tloss: 1.67918\n",
      "Training Epoch 20  72.0% | batch:       494 of       686\t|\tloss: 1.4625\n",
      "Training Epoch 20  72.2% | batch:       495 of       686\t|\tloss: 1.49834\n",
      "Training Epoch 20  72.3% | batch:       496 of       686\t|\tloss: 1.80484\n",
      "Training Epoch 20  72.4% | batch:       497 of       686\t|\tloss: 1.46613\n",
      "Training Epoch 20  72.6% | batch:       498 of       686\t|\tloss: 1.75505\n",
      "Training Epoch 20  72.7% | batch:       499 of       686\t|\tloss: 2.11251\n",
      "Training Epoch 20  72.9% | batch:       500 of       686\t|\tloss: 1.59932\n",
      "Training Epoch 20  73.0% | batch:       501 of       686\t|\tloss: 1.78321\n",
      "Training Epoch 20  73.2% | batch:       502 of       686\t|\tloss: 1.53842\n",
      "Training Epoch 20  73.3% | batch:       503 of       686\t|\tloss: 1.31238\n",
      "Training Epoch 20  73.5% | batch:       504 of       686\t|\tloss: 1.28709\n",
      "Training Epoch 20  73.6% | batch:       505 of       686\t|\tloss: 1.4069\n",
      "Training Epoch 20  73.8% | batch:       506 of       686\t|\tloss: 1.45337\n",
      "Training Epoch 20  73.9% | batch:       507 of       686\t|\tloss: 1.43633\n",
      "Training Epoch 20  74.1% | batch:       508 of       686\t|\tloss: 1.55849\n",
      "Training Epoch 20  74.2% | batch:       509 of       686\t|\tloss: 1.48998\n",
      "Training Epoch 20  74.3% | batch:       510 of       686\t|\tloss: 1.33476\n",
      "Training Epoch 20  74.5% | batch:       511 of       686\t|\tloss: 1.61293\n",
      "Training Epoch 20  74.6% | batch:       512 of       686\t|\tloss: 1.48583\n",
      "Training Epoch 20  74.8% | batch:       513 of       686\t|\tloss: 1.68831\n",
      "Training Epoch 20  74.9% | batch:       514 of       686\t|\tloss: 1.63603\n",
      "Training Epoch 20  75.1% | batch:       515 of       686\t|\tloss: 1.87552\n",
      "Training Epoch 20  75.2% | batch:       516 of       686\t|\tloss: 1.56045\n",
      "Training Epoch 20  75.4% | batch:       517 of       686\t|\tloss: 1.76951\n",
      "Training Epoch 20  75.5% | batch:       518 of       686\t|\tloss: 1.47573\n",
      "Training Epoch 20  75.7% | batch:       519 of       686\t|\tloss: 1.98163\n",
      "Training Epoch 20  75.8% | batch:       520 of       686\t|\tloss: 1.56442\n",
      "Training Epoch 20  75.9% | batch:       521 of       686\t|\tloss: 1.70926\n",
      "Training Epoch 20  76.1% | batch:       522 of       686\t|\tloss: 1.42894\n",
      "Training Epoch 20  76.2% | batch:       523 of       686\t|\tloss: 1.70002\n",
      "Training Epoch 20  76.4% | batch:       524 of       686\t|\tloss: 1.95875\n",
      "Training Epoch 20  76.5% | batch:       525 of       686\t|\tloss: 1.52106\n",
      "Training Epoch 20  76.7% | batch:       526 of       686\t|\tloss: 1.6707\n",
      "Training Epoch 20  76.8% | batch:       527 of       686\t|\tloss: 1.40428\n",
      "Training Epoch 20  77.0% | batch:       528 of       686\t|\tloss: 1.40772\n",
      "Training Epoch 20  77.1% | batch:       529 of       686\t|\tloss: 1.82964\n",
      "Training Epoch 20  77.3% | batch:       530 of       686\t|\tloss: 2.1535\n",
      "Training Epoch 20  77.4% | batch:       531 of       686\t|\tloss: 1.51565\n",
      "Training Epoch 20  77.6% | batch:       532 of       686\t|\tloss: 1.88129\n",
      "Training Epoch 20  77.7% | batch:       533 of       686\t|\tloss: 1.84756\n",
      "Training Epoch 20  77.8% | batch:       534 of       686\t|\tloss: 1.40041\n",
      "Training Epoch 20  78.0% | batch:       535 of       686\t|\tloss: 1.57711\n",
      "Training Epoch 20  78.1% | batch:       536 of       686\t|\tloss: 1.29997\n",
      "Training Epoch 20  78.3% | batch:       537 of       686\t|\tloss: 1.81364\n",
      "Training Epoch 20  78.4% | batch:       538 of       686\t|\tloss: 1.4536\n",
      "Training Epoch 20  78.6% | batch:       539 of       686\t|\tloss: 1.96024\n",
      "Training Epoch 20  78.7% | batch:       540 of       686\t|\tloss: 1.47439\n",
      "Training Epoch 20  78.9% | batch:       541 of       686\t|\tloss: 1.43255\n",
      "Training Epoch 20  79.0% | batch:       542 of       686\t|\tloss: 1.64644\n",
      "Training Epoch 20  79.2% | batch:       543 of       686\t|\tloss: 1.67341\n",
      "Training Epoch 20  79.3% | batch:       544 of       686\t|\tloss: 1.14872\n",
      "Training Epoch 20  79.4% | batch:       545 of       686\t|\tloss: 1.72125\n",
      "Training Epoch 20  79.6% | batch:       546 of       686\t|\tloss: 1.33073\n",
      "Training Epoch 20  79.7% | batch:       547 of       686\t|\tloss: 1.76874\n",
      "Training Epoch 20  79.9% | batch:       548 of       686\t|\tloss: 1.4996\n",
      "Training Epoch 20  80.0% | batch:       549 of       686\t|\tloss: 1.54265\n",
      "Training Epoch 20  80.2% | batch:       550 of       686\t|\tloss: 1.51839\n",
      "Training Epoch 20  80.3% | batch:       551 of       686\t|\tloss: 1.46309\n",
      "Training Epoch 20  80.5% | batch:       552 of       686\t|\tloss: 2.22001\n",
      "Training Epoch 20  80.6% | batch:       553 of       686\t|\tloss: 1.22629\n",
      "Training Epoch 20  80.8% | batch:       554 of       686\t|\tloss: 1.92888\n",
      "Training Epoch 20  80.9% | batch:       555 of       686\t|\tloss: 1.55888\n",
      "Training Epoch 20  81.0% | batch:       556 of       686\t|\tloss: 1.42107\n",
      "Training Epoch 20  81.2% | batch:       557 of       686\t|\tloss: 1.56648\n",
      "Training Epoch 20  81.3% | batch:       558 of       686\t|\tloss: 1.56062\n",
      "Training Epoch 20  81.5% | batch:       559 of       686\t|\tloss: 1.49851\n",
      "Training Epoch 20  81.6% | batch:       560 of       686\t|\tloss: 1.69178\n",
      "Training Epoch 20  81.8% | batch:       561 of       686\t|\tloss: 1.57943\n",
      "Training Epoch 20  81.9% | batch:       562 of       686\t|\tloss: 1.75137\n",
      "Training Epoch 20  82.1% | batch:       563 of       686\t|\tloss: 1.71801\n",
      "Training Epoch 20  82.2% | batch:       564 of       686\t|\tloss: 1.82094\n",
      "Training Epoch 20  82.4% | batch:       565 of       686\t|\tloss: 1.93884\n",
      "Training Epoch 20  82.5% | batch:       566 of       686\t|\tloss: 1.34172\n",
      "Training Epoch 20  82.7% | batch:       567 of       686\t|\tloss: 1.77652\n",
      "Training Epoch 20  82.8% | batch:       568 of       686\t|\tloss: 1.63552\n",
      "Training Epoch 20  82.9% | batch:       569 of       686\t|\tloss: 1.68084\n",
      "Training Epoch 20  83.1% | batch:       570 of       686\t|\tloss: 1.53039\n",
      "Training Epoch 20  83.2% | batch:       571 of       686\t|\tloss: 1.50782\n",
      "Training Epoch 20  83.4% | batch:       572 of       686\t|\tloss: 1.8628\n",
      "Training Epoch 20  83.5% | batch:       573 of       686\t|\tloss: 1.61764\n",
      "Training Epoch 20  83.7% | batch:       574 of       686\t|\tloss: 1.94082\n",
      "Training Epoch 20  83.8% | batch:       575 of       686\t|\tloss: 1.48667\n",
      "Training Epoch 20  84.0% | batch:       576 of       686\t|\tloss: 2.49352\n",
      "Training Epoch 20  84.1% | batch:       577 of       686\t|\tloss: 1.53891\n",
      "Training Epoch 20  84.3% | batch:       578 of       686\t|\tloss: 1.94176\n",
      "Training Epoch 20  84.4% | batch:       579 of       686\t|\tloss: 1.78049\n",
      "Training Epoch 20  84.5% | batch:       580 of       686\t|\tloss: 1.67757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 20  84.7% | batch:       581 of       686\t|\tloss: 1.0918\n",
      "Training Epoch 20  84.8% | batch:       582 of       686\t|\tloss: 1.62983\n",
      "Training Epoch 20  85.0% | batch:       583 of       686\t|\tloss: 1.7099\n",
      "Training Epoch 20  85.1% | batch:       584 of       686\t|\tloss: 1.74259\n",
      "Training Epoch 20  85.3% | batch:       585 of       686\t|\tloss: 1.60195\n",
      "Training Epoch 20  85.4% | batch:       586 of       686\t|\tloss: 1.49161\n",
      "Training Epoch 20  85.6% | batch:       587 of       686\t|\tloss: 1.43652\n",
      "Training Epoch 20  85.7% | batch:       588 of       686\t|\tloss: 1.90028\n",
      "Training Epoch 20  85.9% | batch:       589 of       686\t|\tloss: 1.45249\n",
      "Training Epoch 20  86.0% | batch:       590 of       686\t|\tloss: 1.68978\n",
      "Training Epoch 20  86.2% | batch:       591 of       686\t|\tloss: 1.38665\n",
      "Training Epoch 20  86.3% | batch:       592 of       686\t|\tloss: 1.36084\n",
      "Training Epoch 20  86.4% | batch:       593 of       686\t|\tloss: 1.58542\n",
      "Training Epoch 20  86.6% | batch:       594 of       686\t|\tloss: 1.28058\n",
      "Training Epoch 20  86.7% | batch:       595 of       686\t|\tloss: 1.6673\n",
      "Training Epoch 20  86.9% | batch:       596 of       686\t|\tloss: 2.05515\n",
      "Training Epoch 20  87.0% | batch:       597 of       686\t|\tloss: 1.63443\n",
      "Training Epoch 20  87.2% | batch:       598 of       686\t|\tloss: 1.51622\n",
      "Training Epoch 20  87.3% | batch:       599 of       686\t|\tloss: 1.70244\n",
      "Training Epoch 20  87.5% | batch:       600 of       686\t|\tloss: 1.50439\n",
      "Training Epoch 20  87.6% | batch:       601 of       686\t|\tloss: 1.46992\n",
      "Training Epoch 20  87.8% | batch:       602 of       686\t|\tloss: 1.49699\n",
      "Training Epoch 20  87.9% | batch:       603 of       686\t|\tloss: 1.64244\n",
      "Training Epoch 20  88.0% | batch:       604 of       686\t|\tloss: 1.71238\n",
      "Training Epoch 20  88.2% | batch:       605 of       686\t|\tloss: 1.26326\n",
      "Training Epoch 20  88.3% | batch:       606 of       686\t|\tloss: 1.53253\n",
      "Training Epoch 20  88.5% | batch:       607 of       686\t|\tloss: 1.80534\n",
      "Training Epoch 20  88.6% | batch:       608 of       686\t|\tloss: 1.55729\n",
      "Training Epoch 20  88.8% | batch:       609 of       686\t|\tloss: 1.32445\n",
      "Training Epoch 20  88.9% | batch:       610 of       686\t|\tloss: 1.76256\n",
      "Training Epoch 20  89.1% | batch:       611 of       686\t|\tloss: 1.33419\n",
      "Training Epoch 20  89.2% | batch:       612 of       686\t|\tloss: 1.78644\n",
      "Training Epoch 20  89.4% | batch:       613 of       686\t|\tloss: 1.56354\n",
      "Training Epoch 20  89.5% | batch:       614 of       686\t|\tloss: 1.53259\n",
      "Training Epoch 20  89.7% | batch:       615 of       686\t|\tloss: 1.3088\n",
      "Training Epoch 20  89.8% | batch:       616 of       686\t|\tloss: 1.6775\n",
      "Training Epoch 20  89.9% | batch:       617 of       686\t|\tloss: 1.8579\n",
      "Training Epoch 20  90.1% | batch:       618 of       686\t|\tloss: 1.52331\n",
      "Training Epoch 20  90.2% | batch:       619 of       686\t|\tloss: 1.454\n",
      "Training Epoch 20  90.4% | batch:       620 of       686\t|\tloss: 1.50623\n",
      "Training Epoch 20  90.5% | batch:       621 of       686\t|\tloss: 1.85018\n",
      "Training Epoch 20  90.7% | batch:       622 of       686\t|\tloss: 1.66487\n",
      "Training Epoch 20  90.8% | batch:       623 of       686\t|\tloss: 1.58284\n",
      "Training Epoch 20  91.0% | batch:       624 of       686\t|\tloss: 1.51339\n",
      "Training Epoch 20  91.1% | batch:       625 of       686\t|\tloss: 1.51103\n",
      "Training Epoch 20  91.3% | batch:       626 of       686\t|\tloss: 1.41686\n",
      "Training Epoch 20  91.4% | batch:       627 of       686\t|\tloss: 1.66342\n",
      "Training Epoch 20  91.5% | batch:       628 of       686\t|\tloss: 1.66142\n",
      "Training Epoch 20  91.7% | batch:       629 of       686\t|\tloss: 1.57187\n",
      "Training Epoch 20  91.8% | batch:       630 of       686\t|\tloss: 1.76042\n",
      "Training Epoch 20  92.0% | batch:       631 of       686\t|\tloss: 1.59894\n",
      "Training Epoch 20  92.1% | batch:       632 of       686\t|\tloss: 1.1233\n",
      "Training Epoch 20  92.3% | batch:       633 of       686\t|\tloss: 1.6778\n",
      "Training Epoch 20  92.4% | batch:       634 of       686\t|\tloss: 1.52926\n",
      "Training Epoch 20  92.6% | batch:       635 of       686\t|\tloss: 1.37089\n",
      "Training Epoch 20  92.7% | batch:       636 of       686\t|\tloss: 1.63346\n",
      "Training Epoch 20  92.9% | batch:       637 of       686\t|\tloss: 1.63303\n",
      "Training Epoch 20  93.0% | batch:       638 of       686\t|\tloss: 1.85764\n",
      "Training Epoch 20  93.1% | batch:       639 of       686\t|\tloss: 1.37966\n",
      "Training Epoch 20  93.3% | batch:       640 of       686\t|\tloss: 1.06716\n",
      "Training Epoch 20  93.4% | batch:       641 of       686\t|\tloss: 1.42162\n",
      "Training Epoch 20  93.6% | batch:       642 of       686\t|\tloss: 1.35477\n",
      "Training Epoch 20  93.7% | batch:       643 of       686\t|\tloss: 1.70982\n",
      "Training Epoch 20  93.9% | batch:       644 of       686\t|\tloss: 1.74448\n",
      "Training Epoch 20  94.0% | batch:       645 of       686\t|\tloss: 1.36058\n",
      "Training Epoch 20  94.2% | batch:       646 of       686\t|\tloss: 1.4311\n",
      "Training Epoch 20  94.3% | batch:       647 of       686\t|\tloss: 1.89443\n",
      "Training Epoch 20  94.5% | batch:       648 of       686\t|\tloss: 1.34267\n",
      "Training Epoch 20  94.6% | batch:       649 of       686\t|\tloss: 1.63415\n",
      "Training Epoch 20  94.8% | batch:       650 of       686\t|\tloss: 1.60195\n",
      "Training Epoch 20  94.9% | batch:       651 of       686\t|\tloss: 1.6787\n",
      "Training Epoch 20  95.0% | batch:       652 of       686\t|\tloss: 1.60227\n",
      "Training Epoch 20  95.2% | batch:       653 of       686\t|\tloss: 1.59448\n",
      "Training Epoch 20  95.3% | batch:       654 of       686\t|\tloss: 1.58826\n",
      "Training Epoch 20  95.5% | batch:       655 of       686\t|\tloss: 1.6053\n",
      "Training Epoch 20  95.6% | batch:       656 of       686\t|\tloss: 1.49966\n",
      "Training Epoch 20  95.8% | batch:       657 of       686\t|\tloss: 1.43963\n",
      "Training Epoch 20  95.9% | batch:       658 of       686\t|\tloss: 2.59766\n",
      "Training Epoch 20  96.1% | batch:       659 of       686\t|\tloss: 1.64794\n",
      "Training Epoch 20  96.2% | batch:       660 of       686\t|\tloss: 1.34028\n",
      "Training Epoch 20  96.4% | batch:       661 of       686\t|\tloss: 1.7134\n",
      "Training Epoch 20  96.5% | batch:       662 of       686\t|\tloss: 1.88806\n",
      "Training Epoch 20  96.6% | batch:       663 of       686\t|\tloss: 2.11136\n",
      "Training Epoch 20  96.8% | batch:       664 of       686\t|\tloss: 1.60635\n",
      "Training Epoch 20  96.9% | batch:       665 of       686\t|\tloss: 1.26591\n",
      "Training Epoch 20  97.1% | batch:       666 of       686\t|\tloss: 2.15113\n",
      "Training Epoch 20  97.2% | batch:       667 of       686\t|\tloss: 1.6551\n",
      "Training Epoch 20  97.4% | batch:       668 of       686\t|\tloss: 1.38768\n",
      "Training Epoch 20  97.5% | batch:       669 of       686\t|\tloss: 1.52242\n",
      "Training Epoch 20  97.7% | batch:       670 of       686\t|\tloss: 1.68117\n",
      "Training Epoch 20  97.8% | batch:       671 of       686\t|\tloss: 1.84902\n",
      "Training Epoch 20  98.0% | batch:       672 of       686\t|\tloss: 1.41248\n",
      "Training Epoch 20  98.1% | batch:       673 of       686\t|\tloss: 1.49534\n",
      "Training Epoch 20  98.3% | batch:       674 of       686\t|\tloss: 1.60979\n",
      "Training Epoch 20  98.4% | batch:       675 of       686\t|\tloss: 2.09737\n",
      "Training Epoch 20  98.5% | batch:       676 of       686\t|\tloss: 1.40105\n",
      "Training Epoch 20  98.7% | batch:       677 of       686\t|\tloss: 1.41004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:09:07,131 | INFO : Epoch 20 Training Summary: epoch: 20.000000 | loss: 1.700294 | \n",
      "2023-05-25 22:09:07,132 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 23.18257999420166 seconds\n",
      "\n",
      "2023-05-25 22:09:07,133 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.929861211776732 seconds\n",
      "2023-05-25 22:09:07,133 | INFO : Avg batch train. time: 0.03488317960900398 seconds\n",
      "2023-05-25 22:09:07,134 | INFO : Avg sample train. time: 0.00027287600446749226 seconds\n",
      "2023-05-25 22:09:07,134 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 20  98.8% | batch:       678 of       686\t|\tloss: 2.01673\n",
      "Training Epoch 20  99.0% | batch:       679 of       686\t|\tloss: 1.12581\n",
      "Training Epoch 20  99.1% | batch:       680 of       686\t|\tloss: 1.10486\n",
      "Training Epoch 20  99.3% | batch:       681 of       686\t|\tloss: 1.59906\n",
      "Training Epoch 20  99.4% | batch:       682 of       686\t|\tloss: 1.72995\n",
      "Training Epoch 20  99.6% | batch:       683 of       686\t|\tloss: 1.47564\n",
      "Training Epoch 20  99.7% | batch:       684 of       686\t|\tloss: 1.56938\n",
      "Training Epoch 20  99.9% | batch:       685 of       686\t|\tloss: 1.40662\n",
      "\n",
      "Evaluating Epoch 20   0.0% | batch:         0 of       172\t|\tloss: 1.91274\n",
      "Evaluating Epoch 20   0.6% | batch:         1 of       172\t|\tloss: 2.122\n",
      "Evaluating Epoch 20   1.2% | batch:         2 of       172\t|\tloss: 1.45297\n",
      "Evaluating Epoch 20   1.7% | batch:         3 of       172\t|\tloss: 3.56556\n",
      "Evaluating Epoch 20   2.3% | batch:         4 of       172\t|\tloss: 1.72247\n",
      "Evaluating Epoch 20   2.9% | batch:         5 of       172\t|\tloss: 1.53448\n",
      "Evaluating Epoch 20   3.5% | batch:         6 of       172\t|\tloss: 1.99454\n",
      "Evaluating Epoch 20   4.1% | batch:         7 of       172\t|\tloss: 3.94306\n",
      "Evaluating Epoch 20   4.7% | batch:         8 of       172\t|\tloss: 1.21291\n",
      "Evaluating Epoch 20   5.2% | batch:         9 of       172\t|\tloss: 2.32796\n",
      "Evaluating Epoch 20   5.8% | batch:        10 of       172\t|\tloss: 2.06278\n",
      "Evaluating Epoch 20   6.4% | batch:        11 of       172\t|\tloss: 2.01152\n",
      "Evaluating Epoch 20   7.0% | batch:        12 of       172\t|\tloss: 1.53471\n",
      "Evaluating Epoch 20   7.6% | batch:        13 of       172\t|\tloss: 2.19564\n",
      "Evaluating Epoch 20   8.1% | batch:        14 of       172\t|\tloss: 2.3485\n",
      "Evaluating Epoch 20   8.7% | batch:        15 of       172\t|\tloss: 1.91468\n",
      "Evaluating Epoch 20   9.3% | batch:        16 of       172\t|\tloss: 2.61546\n",
      "Evaluating Epoch 20   9.9% | batch:        17 of       172\t|\tloss: 1.39361\n",
      "Evaluating Epoch 20  10.5% | batch:        18 of       172\t|\tloss: 15.5933\n",
      "Evaluating Epoch 20  11.0% | batch:        19 of       172\t|\tloss: 1.79269\n",
      "Evaluating Epoch 20  11.6% | batch:        20 of       172\t|\tloss: 2.36483\n",
      "Evaluating Epoch 20  12.2% | batch:        21 of       172\t|\tloss: 0.401088\n",
      "Evaluating Epoch 20  12.8% | batch:        22 of       172\t|\tloss: 3.14529\n",
      "Evaluating Epoch 20  13.4% | batch:        23 of       172\t|\tloss: 2.91519\n",
      "Evaluating Epoch 20  14.0% | batch:        24 of       172\t|\tloss: 1.20007\n",
      "Evaluating Epoch 20  14.5% | batch:        25 of       172\t|\tloss: 2.28664\n",
      "Evaluating Epoch 20  15.1% | batch:        26 of       172\t|\tloss: 7.32546\n",
      "Evaluating Epoch 20  15.7% | batch:        27 of       172\t|\tloss: 15.0154\n",
      "Evaluating Epoch 20  16.3% | batch:        28 of       172\t|\tloss: 0.221102\n",
      "Evaluating Epoch 20  16.9% | batch:        29 of       172\t|\tloss: 1.77436\n",
      "Evaluating Epoch 20  17.4% | batch:        30 of       172\t|\tloss: 0.842917\n",
      "Evaluating Epoch 20  18.0% | batch:        31 of       172\t|\tloss: 0.512465\n",
      "Evaluating Epoch 20  18.6% | batch:        32 of       172\t|\tloss: 0.706699\n",
      "Evaluating Epoch 20  19.2% | batch:        33 of       172\t|\tloss: 0.450385\n",
      "Evaluating Epoch 20  19.8% | batch:        34 of       172\t|\tloss: 0.273262\n",
      "Evaluating Epoch 20  20.3% | batch:        35 of       172\t|\tloss: 0.414687\n",
      "Evaluating Epoch 20  20.9% | batch:        36 of       172\t|\tloss: 3.30217\n",
      "Evaluating Epoch 20  21.5% | batch:        37 of       172\t|\tloss: 3.86541\n",
      "Evaluating Epoch 20  22.1% | batch:        38 of       172\t|\tloss: 3.17792\n",
      "Evaluating Epoch 20  22.7% | batch:        39 of       172\t|\tloss: 6.67115\n",
      "Evaluating Epoch 20  23.3% | batch:        40 of       172\t|\tloss: 0.608858\n",
      "Evaluating Epoch 20  23.8% | batch:        41 of       172\t|\tloss: 1.16768\n",
      "Evaluating Epoch 20  24.4% | batch:        42 of       172\t|\tloss: 0.813554\n",
      "Evaluating Epoch 20  25.0% | batch:        43 of       172\t|\tloss: 17.1205\n",
      "Evaluating Epoch 20  25.6% | batch:        44 of       172\t|\tloss: 1.56071\n",
      "Evaluating Epoch 20  26.2% | batch:        45 of       172\t|\tloss: 0.977973\n",
      "Evaluating Epoch 20  26.7% | batch:        46 of       172\t|\tloss: 0.25165\n",
      "Evaluating Epoch 20  27.3% | batch:        47 of       172\t|\tloss: 0.726158\n",
      "Evaluating Epoch 20  27.9% | batch:        48 of       172\t|\tloss: 0.299863\n",
      "Evaluating Epoch 20  28.5% | batch:        49 of       172\t|\tloss: 1.1341\n",
      "Evaluating Epoch 20  29.1% | batch:        50 of       172\t|\tloss: 0.491688\n",
      "Evaluating Epoch 20  29.7% | batch:        51 of       172\t|\tloss: 0.599237\n",
      "Evaluating Epoch 20  30.2% | batch:        52 of       172\t|\tloss: 1.01293\n",
      "Evaluating Epoch 20  30.8% | batch:        53 of       172\t|\tloss: 1.71845\n",
      "Evaluating Epoch 20  31.4% | batch:        54 of       172\t|\tloss: 1.27577\n",
      "Evaluating Epoch 20  32.0% | batch:        55 of       172\t|\tloss: 1.14214\n",
      "Evaluating Epoch 20  32.6% | batch:        56 of       172\t|\tloss: 2.39996\n",
      "Evaluating Epoch 20  33.1% | batch:        57 of       172\t|\tloss: 1.63876\n",
      "Evaluating Epoch 20  33.7% | batch:        58 of       172\t|\tloss: 1.36431\n",
      "Evaluating Epoch 20  34.3% | batch:        59 of       172\t|\tloss: 1.96288\n",
      "Evaluating Epoch 20  34.9% | batch:        60 of       172\t|\tloss: 0.577097\n",
      "Evaluating Epoch 20  35.5% | batch:        61 of       172\t|\tloss: 2.66996\n",
      "Evaluating Epoch 20  36.0% | batch:        62 of       172\t|\tloss: 0.525177\n",
      "Evaluating Epoch 20  36.6% | batch:        63 of       172\t|\tloss: 2.05115\n",
      "Evaluating Epoch 20  37.2% | batch:        64 of       172\t|\tloss: 1.27057\n",
      "Evaluating Epoch 20  37.8% | batch:        65 of       172\t|\tloss: 1.40798\n",
      "Evaluating Epoch 20  38.4% | batch:        66 of       172\t|\tloss: 2.50862\n",
      "Evaluating Epoch 20  39.0% | batch:        67 of       172\t|\tloss: 0.648463\n",
      "Evaluating Epoch 20  39.5% | batch:        68 of       172\t|\tloss: 1.6206\n",
      "Evaluating Epoch 20  40.1% | batch:        69 of       172\t|\tloss: 2.28489\n",
      "Evaluating Epoch 20  40.7% | batch:        70 of       172\t|\tloss: 0.887023\n",
      "Evaluating Epoch 20  41.3% | batch:        71 of       172\t|\tloss: 1.45216\n",
      "Evaluating Epoch 20  41.9% | batch:        72 of       172\t|\tloss: 1.06575\n",
      "Evaluating Epoch 20  42.4% | batch:        73 of       172\t|\tloss: 1.61282\n",
      "Evaluating Epoch 20  43.0% | batch:        74 of       172\t|\tloss: 0.38583\n",
      "Evaluating Epoch 20  43.6% | batch:        75 of       172\t|\tloss: 0.465054\n",
      "Evaluating Epoch 20  44.2% | batch:        76 of       172\t|\tloss: 0.428642\n",
      "Evaluating Epoch 20  44.8% | batch:        77 of       172\t|\tloss: 0.567953\n",
      "Evaluating Epoch 20  45.3% | batch:        78 of       172\t|\tloss: 0.556754\n",
      "Evaluating Epoch 20  45.9% | batch:        79 of       172\t|\tloss: 0.361753\n",
      "Evaluating Epoch 20  46.5% | batch:        80 of       172\t|\tloss: 0.42478\n",
      "Evaluating Epoch 20  47.1% | batch:        81 of       172\t|\tloss: 0.448947\n",
      "Evaluating Epoch 20  47.7% | batch:        82 of       172\t|\tloss: 0.368617\n",
      "Evaluating Epoch 20  48.3% | batch:        83 of       172\t|\tloss: 0.496591\n",
      "Evaluating Epoch 20  48.8% | batch:        84 of       172\t|\tloss: 0.43402\n",
      "Evaluating Epoch 20  49.4% | batch:        85 of       172\t|\tloss: 0.381567\n",
      "Evaluating Epoch 20  50.0% | batch:        86 of       172\t|\tloss: 0.481447\n",
      "Evaluating Epoch 20  50.6% | batch:        87 of       172\t|\tloss: 0.417228\n",
      "Evaluating Epoch 20  51.2% | batch:        88 of       172\t|\tloss: 0.216584\n",
      "Evaluating Epoch 20  51.7% | batch:        89 of       172\t|\tloss: 0.492046\n",
      "Evaluating Epoch 20  52.3% | batch:        90 of       172\t|\tloss: 0.540467\n",
      "Evaluating Epoch 20  52.9% | batch:        91 of       172\t|\tloss: 0.307489\n",
      "Evaluating Epoch 20  53.5% | batch:        92 of       172\t|\tloss: 0.277354\n",
      "Evaluating Epoch 20  54.1% | batch:        93 of       172\t|\tloss: 0.771789\n",
      "Evaluating Epoch 20  54.7% | batch:        94 of       172\t|\tloss: 0.552362\n",
      "Evaluating Epoch 20  55.2% | batch:        95 of       172\t|\tloss: 0.214193\n",
      "Evaluating Epoch 20  55.8% | batch:        96 of       172\t|\tloss: 0.660145\n",
      "Evaluating Epoch 20  56.4% | batch:        97 of       172\t|\tloss: 0.37185\n",
      "Evaluating Epoch 20  57.0% | batch:        98 of       172\t|\tloss: 0.444572\n",
      "Evaluating Epoch 20  57.6% | batch:        99 of       172\t|\tloss: 0.317985\n",
      "Evaluating Epoch 20  58.1% | batch:       100 of       172\t|\tloss: 0.43372\n",
      "Evaluating Epoch 20  58.7% | batch:       101 of       172\t|\tloss: 0.384598\n",
      "Evaluating Epoch 20  59.3% | batch:       102 of       172\t|\tloss: 0.36456\n",
      "Evaluating Epoch 20  59.9% | batch:       103 of       172\t|\tloss: 0.848141\n",
      "Evaluating Epoch 20  60.5% | batch:       104 of       172\t|\tloss: 0.36491\n",
      "Evaluating Epoch 20  61.0% | batch:       105 of       172\t|\tloss: 0.355884\n",
      "Evaluating Epoch 20  61.6% | batch:       106 of       172\t|\tloss: 0.390861\n",
      "Evaluating Epoch 20  62.2% | batch:       107 of       172\t|\tloss: 0.848117\n",
      "Evaluating Epoch 20  62.8% | batch:       108 of       172\t|\tloss: 0.321646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 20  63.4% | batch:       109 of       172\t|\tloss: 0.292766\n",
      "Evaluating Epoch 20  64.0% | batch:       110 of       172\t|\tloss: 0.804214\n",
      "Evaluating Epoch 20  64.5% | batch:       111 of       172\t|\tloss: 0.380914\n",
      "Evaluating Epoch 20  65.1% | batch:       112 of       172\t|\tloss: 0.444532\n",
      "Evaluating Epoch 20  65.7% | batch:       113 of       172\t|\tloss: 0.720188\n",
      "Evaluating Epoch 20  66.3% | batch:       114 of       172\t|\tloss: 0.640286\n",
      "Evaluating Epoch 20  66.9% | batch:       115 of       172\t|\tloss: 0.614586\n",
      "Evaluating Epoch 20  67.4% | batch:       116 of       172\t|\tloss: 0.347341\n",
      "Evaluating Epoch 20  68.0% | batch:       117 of       172\t|\tloss: 0.591291\n",
      "Evaluating Epoch 20  68.6% | batch:       118 of       172\t|\tloss: 0.436328\n",
      "Evaluating Epoch 20  69.2% | batch:       119 of       172\t|\tloss: 0.251636\n",
      "Evaluating Epoch 20  69.8% | batch:       120 of       172\t|\tloss: 0.383076\n",
      "Evaluating Epoch 20  70.3% | batch:       121 of       172\t|\tloss: 0.620578\n",
      "Evaluating Epoch 20  70.9% | batch:       122 of       172\t|\tloss: 0.503095\n",
      "Evaluating Epoch 20  71.5% | batch:       123 of       172\t|\tloss: 0.722624\n",
      "Evaluating Epoch 20  72.1% | batch:       124 of       172\t|\tloss: 1.17826\n",
      "Evaluating Epoch 20  72.7% | batch:       125 of       172\t|\tloss: 0.409296\n",
      "Evaluating Epoch 20  73.3% | batch:       126 of       172\t|\tloss: 0.344797\n",
      "Evaluating Epoch 20  73.8% | batch:       127 of       172\t|\tloss: 0.436502\n",
      "Evaluating Epoch 20  74.4% | batch:       128 of       172\t|\tloss: 0.481549\n",
      "Evaluating Epoch 20  75.0% | batch:       129 of       172\t|\tloss: 0.330596\n",
      "Evaluating Epoch 20  75.6% | batch:       130 of       172\t|\tloss: 0.61299\n",
      "Evaluating Epoch 20  76.2% | batch:       131 of       172\t|\tloss: 0.442339\n",
      "Evaluating Epoch 20  76.7% | batch:       132 of       172\t|\tloss: 0.303318\n",
      "Evaluating Epoch 20  77.3% | batch:       133 of       172\t|\tloss: 0.257918\n",
      "Evaluating Epoch 20  77.9% | batch:       134 of       172\t|\tloss: 0.221308\n",
      "Evaluating Epoch 20  78.5% | batch:       135 of       172\t|\tloss: 0.276855\n",
      "Evaluating Epoch 20  79.1% | batch:       136 of       172\t|\tloss: 0.213315\n",
      "Evaluating Epoch 20  79.7% | batch:       137 of       172\t|\tloss: 0.115004\n",
      "Evaluating Epoch 20  80.2% | batch:       138 of       172\t|\tloss: 0.384825\n",
      "Evaluating Epoch 20  80.8% | batch:       139 of       172\t|\tloss: 0.292731\n",
      "Evaluating Epoch 20  81.4% | batch:       140 of       172\t|\tloss: 0.216867\n",
      "Evaluating Epoch 20  82.0% | batch:       141 of       172\t|\tloss: 0.107479\n",
      "Evaluating Epoch 20  82.6% | batch:       142 of       172\t|\tloss: 0.268181\n",
      "Evaluating Epoch 20  83.1% | batch:       143 of       172\t|\tloss: 0.184173\n",
      "Evaluating Epoch 20  83.7% | batch:       144 of       172\t|\tloss: 0.209278\n",
      "Evaluating Epoch 20  84.3% | batch:       145 of       172\t|\tloss: 0.2669\n",
      "Evaluating Epoch 20  84.9% | batch:       146 of       172\t|\tloss: 0.280063\n",
      "Evaluating Epoch 20  85.5% | batch:       147 of       172\t|\tloss: 0.230153\n",
      "Evaluating Epoch 20  86.0% | batch:       148 of       172\t|\tloss: 0.244837\n",
      "Evaluating Epoch 20  86.6% | batch:       149 of       172\t|\tloss: 0.181821\n",
      "Evaluating Epoch 20  87.2% | batch:       150 of       172\t|\tloss: 0.449052\n",
      "Evaluating Epoch 20  87.8% | batch:       151 of       172\t|\tloss: 0.562648\n",
      "Evaluating Epoch 20  88.4% | batch:       152 of       172\t|\tloss: 0.238632\n",
      "Evaluating Epoch 20  89.0% | batch:       153 of       172\t|\tloss: 0.466961\n",
      "Evaluating Epoch 20  89.5% | batch:       154 of       172\t|\tloss: 0.431294\n",
      "Evaluating Epoch 20  90.1% | batch:       155 of       172\t|\tloss: 0.291456\n",
      "Evaluating Epoch 20  90.7% | batch:       156 of       172\t|\tloss: 0.689698\n",
      "Evaluating Epoch 20  91.3% | batch:       157 of       172\t|\tloss: 0.556052\n",
      "Evaluating Epoch 20  91.9% | batch:       158 of       172\t|\tloss: 0.476796\n",
      "Evaluating Epoch 20  92.4% | batch:       159 of       172\t|\tloss: 0.89037\n",
      "Evaluating Epoch 20  93.0% | batch:       160 of       172\t|\tloss: 0.543922\n",
      "Evaluating Epoch 20  93.6% | batch:       161 of       172\t|\tloss: 1.28887\n",
      "Evaluating Epoch 20  94.2% | batch:       162 of       172\t|\tloss: 0.514942\n",
      "Evaluating Epoch 20  94.8% | batch:       163 of       172\t|\tloss: 0.374441\n",
      "Evaluating Epoch 20  95.3% | batch:       164 of       172\t|\tloss: 0.676379\n",
      "Evaluating Epoch 20  95.9% | batch:       165 of       172\t|\tloss: 0.387829\n",
      "Evaluating Epoch 20  96.5% | batch:       166 of       172\t|\tloss: 0.363755\n",
      "Evaluating Epoch 20  97.1% | batch:       167 of       172\t|\tloss: 0.779232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:09:10,654 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.519026517868042 seconds\n",
      "\n",
      "2023-05-25 22:09:10,655 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 3.97233856291998 seconds\n",
      "2023-05-25 22:09:10,655 | INFO : Avg batch val. time: 0.023094991644883604 seconds\n",
      "2023-05-25 22:09:10,655 | INFO : Avg sample val. time: 0.00018091444928359886 seconds\n",
      "2023-05-25 22:09:10,656 | INFO : Epoch 20 Validation Summary: epoch: 20.000000 | loss: 1.252281 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 20  97.7% | batch:       168 of       172\t|\tloss: 0.371247\n",
      "Evaluating Epoch 20  98.3% | batch:       169 of       172\t|\tloss: 0.554307\n",
      "Evaluating Epoch 20  98.8% | batch:       170 of       172\t|\tloss: 0.700218\n",
      "Evaluating Epoch 20  99.4% | batch:       171 of       172\t|\tloss: 0.549638\n",
      "\n",
      "Training Epoch 21   0.0% | batch:         0 of       686\t|\tloss: 1.85133\n",
      "Training Epoch 21   0.1% | batch:         1 of       686\t|\tloss: 1.68651\n",
      "Training Epoch 21   0.3% | batch:         2 of       686\t|\tloss: 1.83067\n",
      "Training Epoch 21   0.4% | batch:         3 of       686\t|\tloss: 1.31643\n",
      "Training Epoch 21   0.6% | batch:         4 of       686\t|\tloss: 1.46332\n",
      "Training Epoch 21   0.7% | batch:         5 of       686\t|\tloss: 1.47994\n",
      "Training Epoch 21   0.9% | batch:         6 of       686\t|\tloss: 1.79434\n",
      "Training Epoch 21   1.0% | batch:         7 of       686\t|\tloss: 2.23371\n",
      "Training Epoch 21   1.2% | batch:         8 of       686\t|\tloss: 1.73957\n",
      "Training Epoch 21   1.3% | batch:         9 of       686\t|\tloss: 1.75471\n",
      "Training Epoch 21   1.5% | batch:        10 of       686\t|\tloss: 1.32251\n",
      "Training Epoch 21   1.6% | batch:        11 of       686\t|\tloss: 1.19496\n",
      "Training Epoch 21   1.7% | batch:        12 of       686\t|\tloss: 2.08345\n",
      "Training Epoch 21   1.9% | batch:        13 of       686\t|\tloss: 2.04094\n",
      "Training Epoch 21   2.0% | batch:        14 of       686\t|\tloss: 1.30642\n",
      "Training Epoch 21   2.2% | batch:        15 of       686\t|\tloss: 1.83526\n",
      "Training Epoch 21   2.3% | batch:        16 of       686\t|\tloss: 1.55609\n",
      "Training Epoch 21   2.5% | batch:        17 of       686\t|\tloss: 1.45668\n",
      "Training Epoch 21   2.6% | batch:        18 of       686\t|\tloss: 1.45369\n",
      "Training Epoch 21   2.8% | batch:        19 of       686\t|\tloss: 1.39881\n",
      "Training Epoch 21   2.9% | batch:        20 of       686\t|\tloss: 1.73727\n",
      "Training Epoch 21   3.1% | batch:        21 of       686\t|\tloss: 1.24257\n",
      "Training Epoch 21   3.2% | batch:        22 of       686\t|\tloss: 1.52537\n",
      "Training Epoch 21   3.4% | batch:        23 of       686\t|\tloss: 1.49918\n",
      "Training Epoch 21   3.5% | batch:        24 of       686\t|\tloss: 2.28148\n",
      "Training Epoch 21   3.6% | batch:        25 of       686\t|\tloss: 1.80755\n",
      "Training Epoch 21   3.8% | batch:        26 of       686\t|\tloss: 1.3986\n",
      "Training Epoch 21   3.9% | batch:        27 of       686\t|\tloss: 1.61857\n",
      "Training Epoch 21   4.1% | batch:        28 of       686\t|\tloss: 1.84054\n",
      "Training Epoch 21   4.2% | batch:        29 of       686\t|\tloss: 1.9789\n",
      "Training Epoch 21   4.4% | batch:        30 of       686\t|\tloss: 1.53331\n",
      "Training Epoch 21   4.5% | batch:        31 of       686\t|\tloss: 1.45509\n",
      "Training Epoch 21   4.7% | batch:        32 of       686\t|\tloss: 1.62208\n",
      "Training Epoch 21   4.8% | batch:        33 of       686\t|\tloss: 1.83076\n",
      "Training Epoch 21   5.0% | batch:        34 of       686\t|\tloss: 1.14526\n",
      "Training Epoch 21   5.1% | batch:        35 of       686\t|\tloss: 2.10118\n",
      "Training Epoch 21   5.2% | batch:        36 of       686\t|\tloss: 2.02394\n",
      "Training Epoch 21   5.4% | batch:        37 of       686\t|\tloss: 1.39086\n",
      "Training Epoch 21   5.5% | batch:        38 of       686\t|\tloss: 1.29011\n",
      "Training Epoch 21   5.7% | batch:        39 of       686\t|\tloss: 1.233\n",
      "Training Epoch 21   5.8% | batch:        40 of       686\t|\tloss: 1.62517\n",
      "Training Epoch 21   6.0% | batch:        41 of       686\t|\tloss: 1.83304\n",
      "Training Epoch 21   6.1% | batch:        42 of       686\t|\tloss: 1.3307\n",
      "Training Epoch 21   6.3% | batch:        43 of       686\t|\tloss: 0.962193\n",
      "Training Epoch 21   6.4% | batch:        44 of       686\t|\tloss: 1.68538\n",
      "Training Epoch 21   6.6% | batch:        45 of       686\t|\tloss: 1.48727\n",
      "Training Epoch 21   6.7% | batch:        46 of       686\t|\tloss: 1.43204\n",
      "Training Epoch 21   6.9% | batch:        47 of       686\t|\tloss: 1.56212\n",
      "Training Epoch 21   7.0% | batch:        48 of       686\t|\tloss: 1.8922\n",
      "Training Epoch 21   7.1% | batch:        49 of       686\t|\tloss: 1.36597\n",
      "Training Epoch 21   7.3% | batch:        50 of       686\t|\tloss: 1.4579\n",
      "Training Epoch 21   7.4% | batch:        51 of       686\t|\tloss: 1.53725\n",
      "Training Epoch 21   7.6% | batch:        52 of       686\t|\tloss: 2.15505\n",
      "Training Epoch 21   7.7% | batch:        53 of       686\t|\tloss: 2.18488\n",
      "Training Epoch 21   7.9% | batch:        54 of       686\t|\tloss: 1.46667\n",
      "Training Epoch 21   8.0% | batch:        55 of       686\t|\tloss: 1.90005\n",
      "Training Epoch 21   8.2% | batch:        56 of       686\t|\tloss: 1.49586\n",
      "Training Epoch 21   8.3% | batch:        57 of       686\t|\tloss: 1.87027\n",
      "Training Epoch 21   8.5% | batch:        58 of       686\t|\tloss: 1.74576\n",
      "Training Epoch 21   8.6% | batch:        59 of       686\t|\tloss: 1.75833\n",
      "Training Epoch 21   8.7% | batch:        60 of       686\t|\tloss: 1.30174\n",
      "Training Epoch 21   8.9% | batch:        61 of       686\t|\tloss: 1.2851\n",
      "Training Epoch 21   9.0% | batch:        62 of       686\t|\tloss: 1.24375\n",
      "Training Epoch 21   9.2% | batch:        63 of       686\t|\tloss: 1.61964\n",
      "Training Epoch 21   9.3% | batch:        64 of       686\t|\tloss: 1.38781\n",
      "Training Epoch 21   9.5% | batch:        65 of       686\t|\tloss: 1.64976\n",
      "Training Epoch 21   9.6% | batch:        66 of       686\t|\tloss: 1.71273\n",
      "Training Epoch 21   9.8% | batch:        67 of       686\t|\tloss: 1.3057\n",
      "Training Epoch 21   9.9% | batch:        68 of       686\t|\tloss: 1.44911\n",
      "Training Epoch 21  10.1% | batch:        69 of       686\t|\tloss: 1.64135\n",
      "Training Epoch 21  10.2% | batch:        70 of       686\t|\tloss: 1.014\n",
      "Training Epoch 21  10.3% | batch:        71 of       686\t|\tloss: 1.63414\n",
      "Training Epoch 21  10.5% | batch:        72 of       686\t|\tloss: 1.51865\n",
      "Training Epoch 21  10.6% | batch:        73 of       686\t|\tloss: 1.76448\n",
      "Training Epoch 21  10.8% | batch:        74 of       686\t|\tloss: 1.38236\n",
      "Training Epoch 21  10.9% | batch:        75 of       686\t|\tloss: 1.43285\n",
      "Training Epoch 21  11.1% | batch:        76 of       686\t|\tloss: 2.36347\n",
      "Training Epoch 21  11.2% | batch:        77 of       686\t|\tloss: 1.4439\n",
      "Training Epoch 21  11.4% | batch:        78 of       686\t|\tloss: 1.23927\n",
      "Training Epoch 21  11.5% | batch:        79 of       686\t|\tloss: 1.84742\n",
      "Training Epoch 21  11.7% | batch:        80 of       686\t|\tloss: 1.55868\n",
      "Training Epoch 21  11.8% | batch:        81 of       686\t|\tloss: 1.47301\n",
      "Training Epoch 21  12.0% | batch:        82 of       686\t|\tloss: 1.8557\n",
      "Training Epoch 21  12.1% | batch:        83 of       686\t|\tloss: 1.46657\n",
      "Training Epoch 21  12.2% | batch:        84 of       686\t|\tloss: 1.66895\n",
      "Training Epoch 21  12.4% | batch:        85 of       686\t|\tloss: 1.72274\n",
      "Training Epoch 21  12.5% | batch:        86 of       686\t|\tloss: 1.51266\n",
      "Training Epoch 21  12.7% | batch:        87 of       686\t|\tloss: 1.70473\n",
      "Training Epoch 21  12.8% | batch:        88 of       686\t|\tloss: 1.17966\n",
      "Training Epoch 21  13.0% | batch:        89 of       686\t|\tloss: 1.66639\n",
      "Training Epoch 21  13.1% | batch:        90 of       686\t|\tloss: 1.88787\n",
      "Training Epoch 21  13.3% | batch:        91 of       686\t|\tloss: 1.38033\n",
      "Training Epoch 21  13.4% | batch:        92 of       686\t|\tloss: 1.34298\n",
      "Training Epoch 21  13.6% | batch:        93 of       686\t|\tloss: 1.7221\n",
      "Training Epoch 21  13.7% | batch:        94 of       686\t|\tloss: 1.60012\n",
      "Training Epoch 21  13.8% | batch:        95 of       686\t|\tloss: 1.46407\n",
      "Training Epoch 21  14.0% | batch:        96 of       686\t|\tloss: 1.98248\n",
      "Training Epoch 21  14.1% | batch:        97 of       686\t|\tloss: 1.40872\n",
      "Training Epoch 21  14.3% | batch:        98 of       686\t|\tloss: 1.4289\n",
      "Training Epoch 21  14.4% | batch:        99 of       686\t|\tloss: 1.31189\n",
      "Training Epoch 21  14.6% | batch:       100 of       686\t|\tloss: 1.61934\n",
      "Training Epoch 21  14.7% | batch:       101 of       686\t|\tloss: 1.44485\n",
      "Training Epoch 21  14.9% | batch:       102 of       686\t|\tloss: 1.68697\n",
      "Training Epoch 21  15.0% | batch:       103 of       686\t|\tloss: 1.87511\n",
      "Training Epoch 21  15.2% | batch:       104 of       686\t|\tloss: 1.41457\n",
      "Training Epoch 21  15.3% | batch:       105 of       686\t|\tloss: 1.64108\n",
      "Training Epoch 21  15.5% | batch:       106 of       686\t|\tloss: 1.57358\n",
      "Training Epoch 21  15.6% | batch:       107 of       686\t|\tloss: 1.33812\n",
      "Training Epoch 21  15.7% | batch:       108 of       686\t|\tloss: 1.50169\n",
      "Training Epoch 21  15.9% | batch:       109 of       686\t|\tloss: 1.72662\n",
      "Training Epoch 21  16.0% | batch:       110 of       686\t|\tloss: 1.80275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 21  16.2% | batch:       111 of       686\t|\tloss: 1.65648\n",
      "Training Epoch 21  16.3% | batch:       112 of       686\t|\tloss: 1.42674\n",
      "Training Epoch 21  16.5% | batch:       113 of       686\t|\tloss: 1.47279\n",
      "Training Epoch 21  16.6% | batch:       114 of       686\t|\tloss: 1.67499\n",
      "Training Epoch 21  16.8% | batch:       115 of       686\t|\tloss: 1.63298\n",
      "Training Epoch 21  16.9% | batch:       116 of       686\t|\tloss: 1.41381\n",
      "Training Epoch 21  17.1% | batch:       117 of       686\t|\tloss: 1.04035\n",
      "Training Epoch 21  17.2% | batch:       118 of       686\t|\tloss: 1.37273\n",
      "Training Epoch 21  17.3% | batch:       119 of       686\t|\tloss: 1.60119\n",
      "Training Epoch 21  17.5% | batch:       120 of       686\t|\tloss: 1.49456\n",
      "Training Epoch 21  17.6% | batch:       121 of       686\t|\tloss: 1.60381\n",
      "Training Epoch 21  17.8% | batch:       122 of       686\t|\tloss: 1.45105\n",
      "Training Epoch 21  17.9% | batch:       123 of       686\t|\tloss: 1.68505\n",
      "Training Epoch 21  18.1% | batch:       124 of       686\t|\tloss: 2.23267\n",
      "Training Epoch 21  18.2% | batch:       125 of       686\t|\tloss: 1.89456\n",
      "Training Epoch 21  18.4% | batch:       126 of       686\t|\tloss: 1.33987\n",
      "Training Epoch 21  18.5% | batch:       127 of       686\t|\tloss: 1.39902\n",
      "Training Epoch 21  18.7% | batch:       128 of       686\t|\tloss: 1.38961\n",
      "Training Epoch 21  18.8% | batch:       129 of       686\t|\tloss: 1.51497\n",
      "Training Epoch 21  19.0% | batch:       130 of       686\t|\tloss: 1.86891\n",
      "Training Epoch 21  19.1% | batch:       131 of       686\t|\tloss: 1.70223\n",
      "Training Epoch 21  19.2% | batch:       132 of       686\t|\tloss: 1.43693\n",
      "Training Epoch 21  19.4% | batch:       133 of       686\t|\tloss: 1.7438\n",
      "Training Epoch 21  19.5% | batch:       134 of       686\t|\tloss: 1.2325\n",
      "Training Epoch 21  19.7% | batch:       135 of       686\t|\tloss: 2.02789\n",
      "Training Epoch 21  19.8% | batch:       136 of       686\t|\tloss: 1.58732\n",
      "Training Epoch 21  20.0% | batch:       137 of       686\t|\tloss: 1.29404\n",
      "Training Epoch 21  20.1% | batch:       138 of       686\t|\tloss: 1.4537\n",
      "Training Epoch 21  20.3% | batch:       139 of       686\t|\tloss: 1.56684\n",
      "Training Epoch 21  20.4% | batch:       140 of       686\t|\tloss: 1.37459\n",
      "Training Epoch 21  20.6% | batch:       141 of       686\t|\tloss: 1.08652\n",
      "Training Epoch 21  20.7% | batch:       142 of       686\t|\tloss: 1.46108\n",
      "Training Epoch 21  20.8% | batch:       143 of       686\t|\tloss: 1.41518\n",
      "Training Epoch 21  21.0% | batch:       144 of       686\t|\tloss: 1.82562\n",
      "Training Epoch 21  21.1% | batch:       145 of       686\t|\tloss: 1.25626\n",
      "Training Epoch 21  21.3% | batch:       146 of       686\t|\tloss: 1.30539\n",
      "Training Epoch 21  21.4% | batch:       147 of       686\t|\tloss: 1.42813\n",
      "Training Epoch 21  21.6% | batch:       148 of       686\t|\tloss: 1.43622\n",
      "Training Epoch 21  21.7% | batch:       149 of       686\t|\tloss: 1.35755\n",
      "Training Epoch 21  21.9% | batch:       150 of       686\t|\tloss: 1.14744\n",
      "Training Epoch 21  22.0% | batch:       151 of       686\t|\tloss: 1.70222\n",
      "Training Epoch 21  22.2% | batch:       152 of       686\t|\tloss: 1.81379\n",
      "Training Epoch 21  22.3% | batch:       153 of       686\t|\tloss: 1.61977\n",
      "Training Epoch 21  22.4% | batch:       154 of       686\t|\tloss: 1.61207\n",
      "Training Epoch 21  22.6% | batch:       155 of       686\t|\tloss: 1.19212\n",
      "Training Epoch 21  22.7% | batch:       156 of       686\t|\tloss: 1.91341\n",
      "Training Epoch 21  22.9% | batch:       157 of       686\t|\tloss: 1.27356\n",
      "Training Epoch 21  23.0% | batch:       158 of       686\t|\tloss: 1.78691\n",
      "Training Epoch 21  23.2% | batch:       159 of       686\t|\tloss: 1.36496\n",
      "Training Epoch 21  23.3% | batch:       160 of       686\t|\tloss: 1.49249\n",
      "Training Epoch 21  23.5% | batch:       161 of       686\t|\tloss: 1.82782\n",
      "Training Epoch 21  23.6% | batch:       162 of       686\t|\tloss: 1.36018\n",
      "Training Epoch 21  23.8% | batch:       163 of       686\t|\tloss: 1.3656\n",
      "Training Epoch 21  23.9% | batch:       164 of       686\t|\tloss: 1.43895\n",
      "Training Epoch 21  24.1% | batch:       165 of       686\t|\tloss: 1.47198\n",
      "Training Epoch 21  24.2% | batch:       166 of       686\t|\tloss: 1.58291\n",
      "Training Epoch 21  24.3% | batch:       167 of       686\t|\tloss: 1.60787\n",
      "Training Epoch 21  24.5% | batch:       168 of       686\t|\tloss: 1.36876\n",
      "Training Epoch 21  24.6% | batch:       169 of       686\t|\tloss: 1.30425\n",
      "Training Epoch 21  24.8% | batch:       170 of       686\t|\tloss: 1.61388\n",
      "Training Epoch 21  24.9% | batch:       171 of       686\t|\tloss: 1.4374\n",
      "Training Epoch 21  25.1% | batch:       172 of       686\t|\tloss: 1.63582\n",
      "Training Epoch 21  25.2% | batch:       173 of       686\t|\tloss: 1.38154\n",
      "Training Epoch 21  25.4% | batch:       174 of       686\t|\tloss: 1.52709\n",
      "Training Epoch 21  25.5% | batch:       175 of       686\t|\tloss: 1.94581\n",
      "Training Epoch 21  25.7% | batch:       176 of       686\t|\tloss: 1.29824\n",
      "Training Epoch 21  25.8% | batch:       177 of       686\t|\tloss: 1.2242\n",
      "Training Epoch 21  25.9% | batch:       178 of       686\t|\tloss: 1.21339\n",
      "Training Epoch 21  26.1% | batch:       179 of       686\t|\tloss: 1.42872\n",
      "Training Epoch 21  26.2% | batch:       180 of       686\t|\tloss: 1.54899\n",
      "Training Epoch 21  26.4% | batch:       181 of       686\t|\tloss: 1.53233\n",
      "Training Epoch 21  26.5% | batch:       182 of       686\t|\tloss: 2.07419\n",
      "Training Epoch 21  26.7% | batch:       183 of       686\t|\tloss: 1.62589\n",
      "Training Epoch 21  26.8% | batch:       184 of       686\t|\tloss: 1.26174\n",
      "Training Epoch 21  27.0% | batch:       185 of       686\t|\tloss: 1.28508\n",
      "Training Epoch 21  27.1% | batch:       186 of       686\t|\tloss: 1.47414\n",
      "Training Epoch 21  27.3% | batch:       187 of       686\t|\tloss: 1.58955\n",
      "Training Epoch 21  27.4% | batch:       188 of       686\t|\tloss: 1.76366\n",
      "Training Epoch 21  27.6% | batch:       189 of       686\t|\tloss: 1.65299\n",
      "Training Epoch 21  27.7% | batch:       190 of       686\t|\tloss: 1.14946\n",
      "Training Epoch 21  27.8% | batch:       191 of       686\t|\tloss: 1.46601\n",
      "Training Epoch 21  28.0% | batch:       192 of       686\t|\tloss: 1.86837\n",
      "Training Epoch 21  28.1% | batch:       193 of       686\t|\tloss: 2.05404\n",
      "Training Epoch 21  28.3% | batch:       194 of       686\t|\tloss: 1.72818\n",
      "Training Epoch 21  28.4% | batch:       195 of       686\t|\tloss: 1.56203\n",
      "Training Epoch 21  28.6% | batch:       196 of       686\t|\tloss: 1.07645\n",
      "Training Epoch 21  28.7% | batch:       197 of       686\t|\tloss: 1.69829\n",
      "Training Epoch 21  28.9% | batch:       198 of       686\t|\tloss: 1.35118\n",
      "Training Epoch 21  29.0% | batch:       199 of       686\t|\tloss: 1.39234\n",
      "Training Epoch 21  29.2% | batch:       200 of       686\t|\tloss: 1.13622\n",
      "Training Epoch 21  29.3% | batch:       201 of       686\t|\tloss: 1.76645\n",
      "Training Epoch 21  29.4% | batch:       202 of       686\t|\tloss: 1.60282\n",
      "Training Epoch 21  29.6% | batch:       203 of       686\t|\tloss: 1.25632\n",
      "Training Epoch 21  29.7% | batch:       204 of       686\t|\tloss: 1.47848\n",
      "Training Epoch 21  29.9% | batch:       205 of       686\t|\tloss: 1.79763\n",
      "Training Epoch 21  30.0% | batch:       206 of       686\t|\tloss: 1.84164\n",
      "Training Epoch 21  30.2% | batch:       207 of       686\t|\tloss: 1.22113\n",
      "Training Epoch 21  30.3% | batch:       208 of       686\t|\tloss: 1.55834\n",
      "Training Epoch 21  30.5% | batch:       209 of       686\t|\tloss: 1.67844\n",
      "Training Epoch 21  30.6% | batch:       210 of       686\t|\tloss: 1.25915\n",
      "Training Epoch 21  30.8% | batch:       211 of       686\t|\tloss: 1.76986\n",
      "Training Epoch 21  30.9% | batch:       212 of       686\t|\tloss: 1.62158\n",
      "Training Epoch 21  31.0% | batch:       213 of       686\t|\tloss: 1.63733\n",
      "Training Epoch 21  31.2% | batch:       214 of       686\t|\tloss: 1.07176\n",
      "Training Epoch 21  31.3% | batch:       215 of       686\t|\tloss: 1.84568\n",
      "Training Epoch 21  31.5% | batch:       216 of       686\t|\tloss: 1.72347\n",
      "Training Epoch 21  31.6% | batch:       217 of       686\t|\tloss: 1.73029\n",
      "Training Epoch 21  31.8% | batch:       218 of       686\t|\tloss: 1.37815\n",
      "Training Epoch 21  31.9% | batch:       219 of       686\t|\tloss: 1.50198\n",
      "Training Epoch 21  32.1% | batch:       220 of       686\t|\tloss: 1.56261\n",
      "Training Epoch 21  32.2% | batch:       221 of       686\t|\tloss: 1.65481\n",
      "Training Epoch 21  32.4% | batch:       222 of       686\t|\tloss: 1.37706\n",
      "Training Epoch 21  32.5% | batch:       223 of       686\t|\tloss: 1.8852\n",
      "Training Epoch 21  32.7% | batch:       224 of       686\t|\tloss: 1.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 21  32.8% | batch:       225 of       686\t|\tloss: 1.40776\n",
      "Training Epoch 21  32.9% | batch:       226 of       686\t|\tloss: 1.48948\n",
      "Training Epoch 21  33.1% | batch:       227 of       686\t|\tloss: 1.40926\n",
      "Training Epoch 21  33.2% | batch:       228 of       686\t|\tloss: 1.3602\n",
      "Training Epoch 21  33.4% | batch:       229 of       686\t|\tloss: 1.61947\n",
      "Training Epoch 21  33.5% | batch:       230 of       686\t|\tloss: 1.36557\n",
      "Training Epoch 21  33.7% | batch:       231 of       686\t|\tloss: 1.10023\n",
      "Training Epoch 21  33.8% | batch:       232 of       686\t|\tloss: 2.09996\n",
      "Training Epoch 21  34.0% | batch:       233 of       686\t|\tloss: 1.20901\n",
      "Training Epoch 21  34.1% | batch:       234 of       686\t|\tloss: 1.33096\n",
      "Training Epoch 21  34.3% | batch:       235 of       686\t|\tloss: 1.31487\n",
      "Training Epoch 21  34.4% | batch:       236 of       686\t|\tloss: 1.26118\n",
      "Training Epoch 21  34.5% | batch:       237 of       686\t|\tloss: 1.37585\n",
      "Training Epoch 21  34.7% | batch:       238 of       686\t|\tloss: 2.12015\n",
      "Training Epoch 21  34.8% | batch:       239 of       686\t|\tloss: 1.29229\n",
      "Training Epoch 21  35.0% | batch:       240 of       686\t|\tloss: 1.11618\n",
      "Training Epoch 21  35.1% | batch:       241 of       686\t|\tloss: 1.14973\n",
      "Training Epoch 21  35.3% | batch:       242 of       686\t|\tloss: 1.7678\n",
      "Training Epoch 21  35.4% | batch:       243 of       686\t|\tloss: 1.47382\n",
      "Training Epoch 21  35.6% | batch:       244 of       686\t|\tloss: 1.5443\n",
      "Training Epoch 21  35.7% | batch:       245 of       686\t|\tloss: 1.53953\n",
      "Training Epoch 21  35.9% | batch:       246 of       686\t|\tloss: 1.66397\n",
      "Training Epoch 21  36.0% | batch:       247 of       686\t|\tloss: 1.75511\n",
      "Training Epoch 21  36.2% | batch:       248 of       686\t|\tloss: 1.43678\n",
      "Training Epoch 21  36.3% | batch:       249 of       686\t|\tloss: 2.00895\n",
      "Training Epoch 21  36.4% | batch:       250 of       686\t|\tloss: 2.17424\n",
      "Training Epoch 21  36.6% | batch:       251 of       686\t|\tloss: 1.54445\n",
      "Training Epoch 21  36.7% | batch:       252 of       686\t|\tloss: 1.2392\n",
      "Training Epoch 21  36.9% | batch:       253 of       686\t|\tloss: 1.51662\n",
      "Training Epoch 21  37.0% | batch:       254 of       686\t|\tloss: 1.48859\n",
      "Training Epoch 21  37.2% | batch:       255 of       686\t|\tloss: 1.30262\n",
      "Training Epoch 21  37.3% | batch:       256 of       686\t|\tloss: 1.75758\n",
      "Training Epoch 21  37.5% | batch:       257 of       686\t|\tloss: 1.58336\n",
      "Training Epoch 21  37.6% | batch:       258 of       686\t|\tloss: 1.22209\n",
      "Training Epoch 21  37.8% | batch:       259 of       686\t|\tloss: 1.45288\n",
      "Training Epoch 21  37.9% | batch:       260 of       686\t|\tloss: 1.40874\n",
      "Training Epoch 21  38.0% | batch:       261 of       686\t|\tloss: 1.77725\n",
      "Training Epoch 21  38.2% | batch:       262 of       686\t|\tloss: 1.36807\n",
      "Training Epoch 21  38.3% | batch:       263 of       686\t|\tloss: 1.89668\n",
      "Training Epoch 21  38.5% | batch:       264 of       686\t|\tloss: 1.43573\n",
      "Training Epoch 21  38.6% | batch:       265 of       686\t|\tloss: 1.2465\n",
      "Training Epoch 21  38.8% | batch:       266 of       686\t|\tloss: 1.3336\n",
      "Training Epoch 21  38.9% | batch:       267 of       686\t|\tloss: 1.88611\n",
      "Training Epoch 21  39.1% | batch:       268 of       686\t|\tloss: 1.70824\n",
      "Training Epoch 21  39.2% | batch:       269 of       686\t|\tloss: 1.47171\n",
      "Training Epoch 21  39.4% | batch:       270 of       686\t|\tloss: 1.61365\n",
      "Training Epoch 21  39.5% | batch:       271 of       686\t|\tloss: 1.54664\n",
      "Training Epoch 21  39.7% | batch:       272 of       686\t|\tloss: 1.52985\n",
      "Training Epoch 21  39.8% | batch:       273 of       686\t|\tloss: 1.58818\n",
      "Training Epoch 21  39.9% | batch:       274 of       686\t|\tloss: 1.22968\n",
      "Training Epoch 21  40.1% | batch:       275 of       686\t|\tloss: 1.50106\n",
      "Training Epoch 21  40.2% | batch:       276 of       686\t|\tloss: 1.33603\n",
      "Training Epoch 21  40.4% | batch:       277 of       686\t|\tloss: 1.74693\n",
      "Training Epoch 21  40.5% | batch:       278 of       686\t|\tloss: 1.6678\n",
      "Training Epoch 21  40.7% | batch:       279 of       686\t|\tloss: 1.62704\n",
      "Training Epoch 21  40.8% | batch:       280 of       686\t|\tloss: 1.38318\n",
      "Training Epoch 21  41.0% | batch:       281 of       686\t|\tloss: 1.18938\n",
      "Training Epoch 21  41.1% | batch:       282 of       686\t|\tloss: 1.47558\n",
      "Training Epoch 21  41.3% | batch:       283 of       686\t|\tloss: 1.43076\n",
      "Training Epoch 21  41.4% | batch:       284 of       686\t|\tloss: 1.11532\n",
      "Training Epoch 21  41.5% | batch:       285 of       686\t|\tloss: 1.72541\n",
      "Training Epoch 21  41.7% | batch:       286 of       686\t|\tloss: 1.43541\n",
      "Training Epoch 21  41.8% | batch:       287 of       686\t|\tloss: 1.30845\n",
      "Training Epoch 21  42.0% | batch:       288 of       686\t|\tloss: 1.82792\n",
      "Training Epoch 21  42.1% | batch:       289 of       686\t|\tloss: 1.61639\n",
      "Training Epoch 21  42.3% | batch:       290 of       686\t|\tloss: 1.5434\n",
      "Training Epoch 21  42.4% | batch:       291 of       686\t|\tloss: 1.57442\n",
      "Training Epoch 21  42.6% | batch:       292 of       686\t|\tloss: 1.17207\n",
      "Training Epoch 21  42.7% | batch:       293 of       686\t|\tloss: 1.81054\n",
      "Training Epoch 21  42.9% | batch:       294 of       686\t|\tloss: 1.67727\n",
      "Training Epoch 21  43.0% | batch:       295 of       686\t|\tloss: 1.66513\n",
      "Training Epoch 21  43.1% | batch:       296 of       686\t|\tloss: 1.75675\n",
      "Training Epoch 21  43.3% | batch:       297 of       686\t|\tloss: 1.35872\n",
      "Training Epoch 21  43.4% | batch:       298 of       686\t|\tloss: 1.38785\n",
      "Training Epoch 21  43.6% | batch:       299 of       686\t|\tloss: 1.07799\n",
      "Training Epoch 21  43.7% | batch:       300 of       686\t|\tloss: 1.39691\n",
      "Training Epoch 21  43.9% | batch:       301 of       686\t|\tloss: 1.4516\n",
      "Training Epoch 21  44.0% | batch:       302 of       686\t|\tloss: 1.54523\n",
      "Training Epoch 21  44.2% | batch:       303 of       686\t|\tloss: 1.85932\n",
      "Training Epoch 21  44.3% | batch:       304 of       686\t|\tloss: 1.16784\n",
      "Training Epoch 21  44.5% | batch:       305 of       686\t|\tloss: 1.57679\n",
      "Training Epoch 21  44.6% | batch:       306 of       686\t|\tloss: 1.70612\n",
      "Training Epoch 21  44.8% | batch:       307 of       686\t|\tloss: 1.59962\n",
      "Training Epoch 21  44.9% | batch:       308 of       686\t|\tloss: 1.79988\n",
      "Training Epoch 21  45.0% | batch:       309 of       686\t|\tloss: 1.44667\n",
      "Training Epoch 21  45.2% | batch:       310 of       686\t|\tloss: 1.47216\n",
      "Training Epoch 21  45.3% | batch:       311 of       686\t|\tloss: 1.7694\n",
      "Training Epoch 21  45.5% | batch:       312 of       686\t|\tloss: 1.55683\n",
      "Training Epoch 21  45.6% | batch:       313 of       686\t|\tloss: 1.17351\n",
      "Training Epoch 21  45.8% | batch:       314 of       686\t|\tloss: 1.35249\n",
      "Training Epoch 21  45.9% | batch:       315 of       686\t|\tloss: 1.27905\n",
      "Training Epoch 21  46.1% | batch:       316 of       686\t|\tloss: 1.32454\n",
      "Training Epoch 21  46.2% | batch:       317 of       686\t|\tloss: 1.51594\n",
      "Training Epoch 21  46.4% | batch:       318 of       686\t|\tloss: 1.30533\n",
      "Training Epoch 21  46.5% | batch:       319 of       686\t|\tloss: 1.57498\n",
      "Training Epoch 21  46.6% | batch:       320 of       686\t|\tloss: 1.39004\n",
      "Training Epoch 21  46.8% | batch:       321 of       686\t|\tloss: 1.50285\n",
      "Training Epoch 21  46.9% | batch:       322 of       686\t|\tloss: 2.10733\n",
      "Training Epoch 21  47.1% | batch:       323 of       686\t|\tloss: 1.30767\n",
      "Training Epoch 21  47.2% | batch:       324 of       686\t|\tloss: 1.13294\n",
      "Training Epoch 21  47.4% | batch:       325 of       686\t|\tloss: 1.69768\n",
      "Training Epoch 21  47.5% | batch:       326 of       686\t|\tloss: 1.43273\n",
      "Training Epoch 21  47.7% | batch:       327 of       686\t|\tloss: 1.52963\n",
      "Training Epoch 21  47.8% | batch:       328 of       686\t|\tloss: 1.33293\n",
      "Training Epoch 21  48.0% | batch:       329 of       686\t|\tloss: 1.1323\n",
      "Training Epoch 21  48.1% | batch:       330 of       686\t|\tloss: 1.34387\n",
      "Training Epoch 21  48.3% | batch:       331 of       686\t|\tloss: 1.4077\n",
      "Training Epoch 21  48.4% | batch:       332 of       686\t|\tloss: 1.68291\n",
      "Training Epoch 21  48.5% | batch:       333 of       686\t|\tloss: 1.65261\n",
      "Training Epoch 21  48.7% | batch:       334 of       686\t|\tloss: 1.46471\n",
      "Training Epoch 21  48.8% | batch:       335 of       686\t|\tloss: 1.19809\n",
      "Training Epoch 21  49.0% | batch:       336 of       686\t|\tloss: 1.53877\n",
      "Training Epoch 21  49.1% | batch:       337 of       686\t|\tloss: 1.56775\n",
      "Training Epoch 21  49.3% | batch:       338 of       686\t|\tloss: 1.51901\n",
      "Training Epoch 21  49.4% | batch:       339 of       686\t|\tloss: 1.5168\n",
      "Training Epoch 21  49.6% | batch:       340 of       686\t|\tloss: 1.90871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 21  49.7% | batch:       341 of       686\t|\tloss: 1.24731\n",
      "Training Epoch 21  49.9% | batch:       342 of       686\t|\tloss: 1.21093\n",
      "Training Epoch 21  50.0% | batch:       343 of       686\t|\tloss: 1.30254\n",
      "Training Epoch 21  50.1% | batch:       344 of       686\t|\tloss: 1.43306\n",
      "Training Epoch 21  50.3% | batch:       345 of       686\t|\tloss: 1.56045\n",
      "Training Epoch 21  50.4% | batch:       346 of       686\t|\tloss: 1.60981\n",
      "Training Epoch 21  50.6% | batch:       347 of       686\t|\tloss: 1.14521\n",
      "Training Epoch 21  50.7% | batch:       348 of       686\t|\tloss: 1.2802\n",
      "Training Epoch 21  50.9% | batch:       349 of       686\t|\tloss: 1.65601\n",
      "Training Epoch 21  51.0% | batch:       350 of       686\t|\tloss: 1.41574\n",
      "Training Epoch 21  51.2% | batch:       351 of       686\t|\tloss: 1.35384\n",
      "Training Epoch 21  51.3% | batch:       352 of       686\t|\tloss: 1.12943\n",
      "Training Epoch 21  51.5% | batch:       353 of       686\t|\tloss: 1.66452\n",
      "Training Epoch 21  51.6% | batch:       354 of       686\t|\tloss: 1.60575\n",
      "Training Epoch 21  51.7% | batch:       355 of       686\t|\tloss: 1.66449\n",
      "Training Epoch 21  51.9% | batch:       356 of       686\t|\tloss: 1.42097\n",
      "Training Epoch 21  52.0% | batch:       357 of       686\t|\tloss: 1.38\n",
      "Training Epoch 21  52.2% | batch:       358 of       686\t|\tloss: 1.48479\n",
      "Training Epoch 21  52.3% | batch:       359 of       686\t|\tloss: 2.06527\n",
      "Training Epoch 21  52.5% | batch:       360 of       686\t|\tloss: 1.26272\n",
      "Training Epoch 21  52.6% | batch:       361 of       686\t|\tloss: 1.88376\n",
      "Training Epoch 21  52.8% | batch:       362 of       686\t|\tloss: 1.37539\n",
      "Training Epoch 21  52.9% | batch:       363 of       686\t|\tloss: 1.45936\n",
      "Training Epoch 21  53.1% | batch:       364 of       686\t|\tloss: 1.32126\n",
      "Training Epoch 21  53.2% | batch:       365 of       686\t|\tloss: 1.78016\n",
      "Training Epoch 21  53.4% | batch:       366 of       686\t|\tloss: 1.73498\n",
      "Training Epoch 21  53.5% | batch:       367 of       686\t|\tloss: 1.35199\n",
      "Training Epoch 21  53.6% | batch:       368 of       686\t|\tloss: 1.33543\n",
      "Training Epoch 21  53.8% | batch:       369 of       686\t|\tloss: 1.554\n",
      "Training Epoch 21  53.9% | batch:       370 of       686\t|\tloss: 2.4481\n",
      "Training Epoch 21  54.1% | batch:       371 of       686\t|\tloss: 1.4412\n",
      "Training Epoch 21  54.2% | batch:       372 of       686\t|\tloss: 1.32108\n",
      "Training Epoch 21  54.4% | batch:       373 of       686\t|\tloss: 1.60116\n",
      "Training Epoch 21  54.5% | batch:       374 of       686\t|\tloss: 1.17277\n",
      "Training Epoch 21  54.7% | batch:       375 of       686\t|\tloss: 1.69025\n",
      "Training Epoch 21  54.8% | batch:       376 of       686\t|\tloss: 1.67957\n",
      "Training Epoch 21  55.0% | batch:       377 of       686\t|\tloss: 1.58853\n",
      "Training Epoch 21  55.1% | batch:       378 of       686\t|\tloss: 1.77555\n",
      "Training Epoch 21  55.2% | batch:       379 of       686\t|\tloss: 1.54194\n",
      "Training Epoch 21  55.4% | batch:       380 of       686\t|\tloss: 1.14522\n",
      "Training Epoch 21  55.5% | batch:       381 of       686\t|\tloss: 1.34851\n",
      "Training Epoch 21  55.7% | batch:       382 of       686\t|\tloss: 1.86506\n",
      "Training Epoch 21  55.8% | batch:       383 of       686\t|\tloss: 1.2096\n",
      "Training Epoch 21  56.0% | batch:       384 of       686\t|\tloss: 1.23607\n",
      "Training Epoch 21  56.1% | batch:       385 of       686\t|\tloss: 1.24183\n",
      "Training Epoch 21  56.3% | batch:       386 of       686\t|\tloss: 1.36821\n",
      "Training Epoch 21  56.4% | batch:       387 of       686\t|\tloss: 1.48688\n",
      "Training Epoch 21  56.6% | batch:       388 of       686\t|\tloss: 1.40446\n",
      "Training Epoch 21  56.7% | batch:       389 of       686\t|\tloss: 1.89596\n",
      "Training Epoch 21  56.9% | batch:       390 of       686\t|\tloss: 1.33776\n",
      "Training Epoch 21  57.0% | batch:       391 of       686\t|\tloss: 1.53523\n",
      "Training Epoch 21  57.1% | batch:       392 of       686\t|\tloss: 1.31912\n",
      "Training Epoch 21  57.3% | batch:       393 of       686\t|\tloss: 1.3571\n",
      "Training Epoch 21  57.4% | batch:       394 of       686\t|\tloss: 1.53852\n",
      "Training Epoch 21  57.6% | batch:       395 of       686\t|\tloss: 1.67517\n",
      "Training Epoch 21  57.7% | batch:       396 of       686\t|\tloss: 1.617\n",
      "Training Epoch 21  57.9% | batch:       397 of       686\t|\tloss: 1.26982\n",
      "Training Epoch 21  58.0% | batch:       398 of       686\t|\tloss: 1.70168\n",
      "Training Epoch 21  58.2% | batch:       399 of       686\t|\tloss: 1.5852\n",
      "Training Epoch 21  58.3% | batch:       400 of       686\t|\tloss: 1.6015\n",
      "Training Epoch 21  58.5% | batch:       401 of       686\t|\tloss: 1.34865\n",
      "Training Epoch 21  58.6% | batch:       402 of       686\t|\tloss: 1.50483\n",
      "Training Epoch 21  58.7% | batch:       403 of       686\t|\tloss: 1.72636\n",
      "Training Epoch 21  58.9% | batch:       404 of       686\t|\tloss: 1.32455\n",
      "Training Epoch 21  59.0% | batch:       405 of       686\t|\tloss: 1.97806\n",
      "Training Epoch 21  59.2% | batch:       406 of       686\t|\tloss: 1.63215\n",
      "Training Epoch 21  59.3% | batch:       407 of       686\t|\tloss: 1.87455\n",
      "Training Epoch 21  59.5% | batch:       408 of       686\t|\tloss: 1.54181\n",
      "Training Epoch 21  59.6% | batch:       409 of       686\t|\tloss: 1.65788\n",
      "Training Epoch 21  59.8% | batch:       410 of       686\t|\tloss: 1.39733\n",
      "Training Epoch 21  59.9% | batch:       411 of       686\t|\tloss: 1.67005\n",
      "Training Epoch 21  60.1% | batch:       412 of       686\t|\tloss: 1.25074\n",
      "Training Epoch 21  60.2% | batch:       413 of       686\t|\tloss: 1.63421\n",
      "Training Epoch 21  60.3% | batch:       414 of       686\t|\tloss: 1.22555\n",
      "Training Epoch 21  60.5% | batch:       415 of       686\t|\tloss: 1.40002\n",
      "Training Epoch 21  60.6% | batch:       416 of       686\t|\tloss: 1.61282\n",
      "Training Epoch 21  60.8% | batch:       417 of       686\t|\tloss: 1.34161\n",
      "Training Epoch 21  60.9% | batch:       418 of       686\t|\tloss: 1.14971\n",
      "Training Epoch 21  61.1% | batch:       419 of       686\t|\tloss: 1.54751\n",
      "Training Epoch 21  61.2% | batch:       420 of       686\t|\tloss: 1.60131\n",
      "Training Epoch 21  61.4% | batch:       421 of       686\t|\tloss: 1.21519\n",
      "Training Epoch 21  61.5% | batch:       422 of       686\t|\tloss: 1.49687\n",
      "Training Epoch 21  61.7% | batch:       423 of       686\t|\tloss: 1.25862\n",
      "Training Epoch 21  61.8% | batch:       424 of       686\t|\tloss: 1.44148\n",
      "Training Epoch 21  62.0% | batch:       425 of       686\t|\tloss: 1.58909\n",
      "Training Epoch 21  62.1% | batch:       426 of       686\t|\tloss: 1.79786\n",
      "Training Epoch 21  62.2% | batch:       427 of       686\t|\tloss: 1.52152\n",
      "Training Epoch 21  62.4% | batch:       428 of       686\t|\tloss: 1.31281\n",
      "Training Epoch 21  62.5% | batch:       429 of       686\t|\tloss: 1.4508\n",
      "Training Epoch 21  62.7% | batch:       430 of       686\t|\tloss: 1.85775\n",
      "Training Epoch 21  62.8% | batch:       431 of       686\t|\tloss: 1.35252\n",
      "Training Epoch 21  63.0% | batch:       432 of       686\t|\tloss: 1.16284\n",
      "Training Epoch 21  63.1% | batch:       433 of       686\t|\tloss: 1.4151\n",
      "Training Epoch 21  63.3% | batch:       434 of       686\t|\tloss: 1.27243\n",
      "Training Epoch 21  63.4% | batch:       435 of       686\t|\tloss: 1.21787\n",
      "Training Epoch 21  63.6% | batch:       436 of       686\t|\tloss: 1.66607\n",
      "Training Epoch 21  63.7% | batch:       437 of       686\t|\tloss: 1.71668\n",
      "Training Epoch 21  63.8% | batch:       438 of       686\t|\tloss: 1.12027\n",
      "Training Epoch 21  64.0% | batch:       439 of       686\t|\tloss: 1.53008\n",
      "Training Epoch 21  64.1% | batch:       440 of       686\t|\tloss: 1.15559\n",
      "Training Epoch 21  64.3% | batch:       441 of       686\t|\tloss: 1.63848\n",
      "Training Epoch 21  64.4% | batch:       442 of       686\t|\tloss: 1.2736\n",
      "Training Epoch 21  64.6% | batch:       443 of       686\t|\tloss: 1.22613\n",
      "Training Epoch 21  64.7% | batch:       444 of       686\t|\tloss: 0.986307\n",
      "Training Epoch 21  64.9% | batch:       445 of       686\t|\tloss: 1.34492\n",
      "Training Epoch 21  65.0% | batch:       446 of       686\t|\tloss: 1.67879\n",
      "Training Epoch 21  65.2% | batch:       447 of       686\t|\tloss: 1.25492\n",
      "Training Epoch 21  65.3% | batch:       448 of       686\t|\tloss: 1.46548\n",
      "Training Epoch 21  65.5% | batch:       449 of       686\t|\tloss: 1.5959\n",
      "Training Epoch 21  65.6% | batch:       450 of       686\t|\tloss: 1.50369\n",
      "Training Epoch 21  65.7% | batch:       451 of       686\t|\tloss: 2.03387\n",
      "Training Epoch 21  65.9% | batch:       452 of       686\t|\tloss: 1.48081\n",
      "Training Epoch 21  66.0% | batch:       453 of       686\t|\tloss: 1.32527\n",
      "Training Epoch 21  66.2% | batch:       454 of       686\t|\tloss: 1.47032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 21  66.3% | batch:       455 of       686\t|\tloss: 1.2159\n",
      "Training Epoch 21  66.5% | batch:       456 of       686\t|\tloss: 1.64952\n",
      "Training Epoch 21  66.6% | batch:       457 of       686\t|\tloss: 1.13235\n",
      "Training Epoch 21  66.8% | batch:       458 of       686\t|\tloss: 1.23216\n",
      "Training Epoch 21  66.9% | batch:       459 of       686\t|\tloss: 1.53226\n",
      "Training Epoch 21  67.1% | batch:       460 of       686\t|\tloss: 1.67524\n",
      "Training Epoch 21  67.2% | batch:       461 of       686\t|\tloss: 1.39517\n",
      "Training Epoch 21  67.3% | batch:       462 of       686\t|\tloss: 1.48166\n",
      "Training Epoch 21  67.5% | batch:       463 of       686\t|\tloss: 1.17047\n",
      "Training Epoch 21  67.6% | batch:       464 of       686\t|\tloss: 1.42281\n",
      "Training Epoch 21  67.8% | batch:       465 of       686\t|\tloss: 1.51012\n",
      "Training Epoch 21  67.9% | batch:       466 of       686\t|\tloss: 1.77193\n",
      "Training Epoch 21  68.1% | batch:       467 of       686\t|\tloss: 1.59098\n",
      "Training Epoch 21  68.2% | batch:       468 of       686\t|\tloss: 1.86401\n",
      "Training Epoch 21  68.4% | batch:       469 of       686\t|\tloss: 1.12514\n",
      "Training Epoch 21  68.5% | batch:       470 of       686\t|\tloss: 1.44081\n",
      "Training Epoch 21  68.7% | batch:       471 of       686\t|\tloss: 1.24394\n",
      "Training Epoch 21  68.8% | batch:       472 of       686\t|\tloss: 1.78865\n",
      "Training Epoch 21  69.0% | batch:       473 of       686\t|\tloss: 1.12572\n",
      "Training Epoch 21  69.1% | batch:       474 of       686\t|\tloss: 1.52434\n",
      "Training Epoch 21  69.2% | batch:       475 of       686\t|\tloss: 1.52106\n",
      "Training Epoch 21  69.4% | batch:       476 of       686\t|\tloss: 1.20135\n",
      "Training Epoch 21  69.5% | batch:       477 of       686\t|\tloss: 1.61225\n",
      "Training Epoch 21  69.7% | batch:       478 of       686\t|\tloss: 1.64997\n",
      "Training Epoch 21  69.8% | batch:       479 of       686\t|\tloss: 1.67742\n",
      "Training Epoch 21  70.0% | batch:       480 of       686\t|\tloss: 1.83998\n",
      "Training Epoch 21  70.1% | batch:       481 of       686\t|\tloss: 1.52783\n",
      "Training Epoch 21  70.3% | batch:       482 of       686\t|\tloss: 1.6785\n",
      "Training Epoch 21  70.4% | batch:       483 of       686\t|\tloss: 1.60827\n",
      "Training Epoch 21  70.6% | batch:       484 of       686\t|\tloss: 1.15883\n",
      "Training Epoch 21  70.7% | batch:       485 of       686\t|\tloss: 1.65784\n",
      "Training Epoch 21  70.8% | batch:       486 of       686\t|\tloss: 1.57578\n",
      "Training Epoch 21  71.0% | batch:       487 of       686\t|\tloss: 1.43588\n",
      "Training Epoch 21  71.1% | batch:       488 of       686\t|\tloss: 1.4228\n",
      "Training Epoch 21  71.3% | batch:       489 of       686\t|\tloss: 1.32386\n",
      "Training Epoch 21  71.4% | batch:       490 of       686\t|\tloss: 1.48155\n",
      "Training Epoch 21  71.6% | batch:       491 of       686\t|\tloss: 1.4274\n",
      "Training Epoch 21  71.7% | batch:       492 of       686\t|\tloss: 1.29812\n",
      "Training Epoch 21  71.9% | batch:       493 of       686\t|\tloss: 1.81203\n",
      "Training Epoch 21  72.0% | batch:       494 of       686\t|\tloss: 1.65743\n",
      "Training Epoch 21  72.2% | batch:       495 of       686\t|\tloss: 1.32483\n",
      "Training Epoch 21  72.3% | batch:       496 of       686\t|\tloss: 1.55937\n",
      "Training Epoch 21  72.4% | batch:       497 of       686\t|\tloss: 1.82385\n",
      "Training Epoch 21  72.6% | batch:       498 of       686\t|\tloss: 1.28482\n",
      "Training Epoch 21  72.7% | batch:       499 of       686\t|\tloss: 1.14556\n",
      "Training Epoch 21  72.9% | batch:       500 of       686\t|\tloss: 1.5493\n",
      "Training Epoch 21  73.0% | batch:       501 of       686\t|\tloss: 1.09973\n",
      "Training Epoch 21  73.2% | batch:       502 of       686\t|\tloss: 1.02256\n",
      "Training Epoch 21  73.3% | batch:       503 of       686\t|\tloss: 1.18744\n",
      "Training Epoch 21  73.5% | batch:       504 of       686\t|\tloss: 1.59569\n",
      "Training Epoch 21  73.6% | batch:       505 of       686\t|\tloss: 1.4855\n",
      "Training Epoch 21  73.8% | batch:       506 of       686\t|\tloss: 1.53955\n",
      "Training Epoch 21  73.9% | batch:       507 of       686\t|\tloss: 1.4734\n",
      "Training Epoch 21  74.1% | batch:       508 of       686\t|\tloss: 1.17155\n",
      "Training Epoch 21  74.2% | batch:       509 of       686\t|\tloss: 1.34621\n",
      "Training Epoch 21  74.3% | batch:       510 of       686\t|\tloss: 1.49851\n",
      "Training Epoch 21  74.5% | batch:       511 of       686\t|\tloss: 1.44458\n",
      "Training Epoch 21  74.6% | batch:       512 of       686\t|\tloss: 2.19914\n",
      "Training Epoch 21  74.8% | batch:       513 of       686\t|\tloss: 1.34702\n",
      "Training Epoch 21  74.9% | batch:       514 of       686\t|\tloss: 1.56602\n",
      "Training Epoch 21  75.1% | batch:       515 of       686\t|\tloss: 1.50031\n",
      "Training Epoch 21  75.2% | batch:       516 of       686\t|\tloss: 1.2135\n",
      "Training Epoch 21  75.4% | batch:       517 of       686\t|\tloss: 1.60084\n",
      "Training Epoch 21  75.5% | batch:       518 of       686\t|\tloss: 1.52427\n",
      "Training Epoch 21  75.7% | batch:       519 of       686\t|\tloss: 1.61403\n",
      "Training Epoch 21  75.8% | batch:       520 of       686\t|\tloss: 1.64376\n",
      "Training Epoch 21  75.9% | batch:       521 of       686\t|\tloss: 1.77325\n",
      "Training Epoch 21  76.1% | batch:       522 of       686\t|\tloss: 1.14346\n",
      "Training Epoch 21  76.2% | batch:       523 of       686\t|\tloss: 1.81522\n",
      "Training Epoch 21  76.4% | batch:       524 of       686\t|\tloss: 1.16704\n",
      "Training Epoch 21  76.5% | batch:       525 of       686\t|\tloss: 1.17165\n",
      "Training Epoch 21  76.7% | batch:       526 of       686\t|\tloss: 1.26086\n",
      "Training Epoch 21  76.8% | batch:       527 of       686\t|\tloss: 1.5441\n",
      "Training Epoch 21  77.0% | batch:       528 of       686\t|\tloss: 1.3234\n",
      "Training Epoch 21  77.1% | batch:       529 of       686\t|\tloss: 1.14816\n",
      "Training Epoch 21  77.3% | batch:       530 of       686\t|\tloss: 1.00744\n",
      "Training Epoch 21  77.4% | batch:       531 of       686\t|\tloss: 1.33793\n",
      "Training Epoch 21  77.6% | batch:       532 of       686\t|\tloss: 1.37517\n",
      "Training Epoch 21  77.7% | batch:       533 of       686\t|\tloss: 2.51138\n",
      "Training Epoch 21  77.8% | batch:       534 of       686\t|\tloss: 1.31349\n",
      "Training Epoch 21  78.0% | batch:       535 of       686\t|\tloss: 1.69911\n",
      "Training Epoch 21  78.1% | batch:       536 of       686\t|\tloss: 1.4462\n",
      "Training Epoch 21  78.3% | batch:       537 of       686\t|\tloss: 1.46546\n",
      "Training Epoch 21  78.4% | batch:       538 of       686\t|\tloss: 1.78547\n",
      "Training Epoch 21  78.6% | batch:       539 of       686\t|\tloss: 1.15383\n",
      "Training Epoch 21  78.7% | batch:       540 of       686\t|\tloss: 1.76449\n",
      "Training Epoch 21  78.9% | batch:       541 of       686\t|\tloss: 1.87525\n",
      "Training Epoch 21  79.0% | batch:       542 of       686\t|\tloss: 1.28577\n",
      "Training Epoch 21  79.2% | batch:       543 of       686\t|\tloss: 1.71491\n",
      "Training Epoch 21  79.3% | batch:       544 of       686\t|\tloss: 1.46518\n",
      "Training Epoch 21  79.4% | batch:       545 of       686\t|\tloss: 1.84701\n",
      "Training Epoch 21  79.6% | batch:       546 of       686\t|\tloss: 1.61704\n",
      "Training Epoch 21  79.7% | batch:       547 of       686\t|\tloss: 1.30607\n",
      "Training Epoch 21  79.9% | batch:       548 of       686\t|\tloss: 1.31478\n",
      "Training Epoch 21  80.0% | batch:       549 of       686\t|\tloss: 1.59021\n",
      "Training Epoch 21  80.2% | batch:       550 of       686\t|\tloss: 1.2763\n",
      "Training Epoch 21  80.3% | batch:       551 of       686\t|\tloss: 1.47727\n",
      "Training Epoch 21  80.5% | batch:       552 of       686\t|\tloss: 1.50591\n",
      "Training Epoch 21  80.6% | batch:       553 of       686\t|\tloss: 1.62869\n",
      "Training Epoch 21  80.8% | batch:       554 of       686\t|\tloss: 1.64606\n",
      "Training Epoch 21  80.9% | batch:       555 of       686\t|\tloss: 1.79328\n",
      "Training Epoch 21  81.0% | batch:       556 of       686\t|\tloss: 1.14465\n",
      "Training Epoch 21  81.2% | batch:       557 of       686\t|\tloss: 1.71678\n",
      "Training Epoch 21  81.3% | batch:       558 of       686\t|\tloss: 1.44159\n",
      "Training Epoch 21  81.5% | batch:       559 of       686\t|\tloss: 1.33495\n",
      "Training Epoch 21  81.6% | batch:       560 of       686\t|\tloss: 1.14989\n",
      "Training Epoch 21  81.8% | batch:       561 of       686\t|\tloss: 1.45989\n",
      "Training Epoch 21  81.9% | batch:       562 of       686\t|\tloss: 1.29466\n",
      "Training Epoch 21  82.1% | batch:       563 of       686\t|\tloss: 1.26996\n",
      "Training Epoch 21  82.2% | batch:       564 of       686\t|\tloss: 1.32806\n",
      "Training Epoch 21  82.4% | batch:       565 of       686\t|\tloss: 1.43451\n",
      "Training Epoch 21  82.5% | batch:       566 of       686\t|\tloss: 1.4473\n",
      "Training Epoch 21  82.7% | batch:       567 of       686\t|\tloss: 1.15511\n",
      "Training Epoch 21  82.8% | batch:       568 of       686\t|\tloss: 1.39171\n",
      "Training Epoch 21  82.9% | batch:       569 of       686\t|\tloss: 1.25454\n",
      "Training Epoch 21  83.1% | batch:       570 of       686\t|\tloss: 1.18577\n",
      "Training Epoch 21  83.2% | batch:       571 of       686\t|\tloss: 1.60413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 21  83.4% | batch:       572 of       686\t|\tloss: 1.22708\n",
      "Training Epoch 21  83.5% | batch:       573 of       686\t|\tloss: 1.57394\n",
      "Training Epoch 21  83.7% | batch:       574 of       686\t|\tloss: 1.41552\n",
      "Training Epoch 21  83.8% | batch:       575 of       686\t|\tloss: 1.84445\n",
      "Training Epoch 21  84.0% | batch:       576 of       686\t|\tloss: 1.40829\n",
      "Training Epoch 21  84.1% | batch:       577 of       686\t|\tloss: 1.40547\n",
      "Training Epoch 21  84.3% | batch:       578 of       686\t|\tloss: 1.18145\n",
      "Training Epoch 21  84.4% | batch:       579 of       686\t|\tloss: 1.58339\n",
      "Training Epoch 21  84.5% | batch:       580 of       686\t|\tloss: 1.44362\n",
      "Training Epoch 21  84.7% | batch:       581 of       686\t|\tloss: 1.34196\n",
      "Training Epoch 21  84.8% | batch:       582 of       686\t|\tloss: 1.57929\n",
      "Training Epoch 21  85.0% | batch:       583 of       686\t|\tloss: 1.96266\n",
      "Training Epoch 21  85.1% | batch:       584 of       686\t|\tloss: 1.08652\n",
      "Training Epoch 21  85.3% | batch:       585 of       686\t|\tloss: 1.43045\n",
      "Training Epoch 21  85.4% | batch:       586 of       686\t|\tloss: 0.98163\n",
      "Training Epoch 21  85.6% | batch:       587 of       686\t|\tloss: 1.52654\n",
      "Training Epoch 21  85.7% | batch:       588 of       686\t|\tloss: 1.3868\n",
      "Training Epoch 21  85.9% | batch:       589 of       686\t|\tloss: 1.25695\n",
      "Training Epoch 21  86.0% | batch:       590 of       686\t|\tloss: 1.0085\n",
      "Training Epoch 21  86.2% | batch:       591 of       686\t|\tloss: 1.33294\n",
      "Training Epoch 21  86.3% | batch:       592 of       686\t|\tloss: 1.55192\n",
      "Training Epoch 21  86.4% | batch:       593 of       686\t|\tloss: 1.53532\n",
      "Training Epoch 21  86.6% | batch:       594 of       686\t|\tloss: 1.0824\n",
      "Training Epoch 21  86.7% | batch:       595 of       686\t|\tloss: 1.47074\n",
      "Training Epoch 21  86.9% | batch:       596 of       686\t|\tloss: 1.37552\n",
      "Training Epoch 21  87.0% | batch:       597 of       686\t|\tloss: 1.31842\n",
      "Training Epoch 21  87.2% | batch:       598 of       686\t|\tloss: 1.36287\n",
      "Training Epoch 21  87.3% | batch:       599 of       686\t|\tloss: 1.27314\n",
      "Training Epoch 21  87.5% | batch:       600 of       686\t|\tloss: 1.42764\n",
      "Training Epoch 21  87.6% | batch:       601 of       686\t|\tloss: 1.53344\n",
      "Training Epoch 21  87.8% | batch:       602 of       686\t|\tloss: 1.17131\n",
      "Training Epoch 21  87.9% | batch:       603 of       686\t|\tloss: 1.40287\n",
      "Training Epoch 21  88.0% | batch:       604 of       686\t|\tloss: 1.32273\n",
      "Training Epoch 21  88.2% | batch:       605 of       686\t|\tloss: 1.32204\n",
      "Training Epoch 21  88.3% | batch:       606 of       686\t|\tloss: 1.46074\n",
      "Training Epoch 21  88.5% | batch:       607 of       686\t|\tloss: 1.36268\n",
      "Training Epoch 21  88.6% | batch:       608 of       686\t|\tloss: 1.56703\n",
      "Training Epoch 21  88.8% | batch:       609 of       686\t|\tloss: 1.38191\n",
      "Training Epoch 21  88.9% | batch:       610 of       686\t|\tloss: 1.39567\n",
      "Training Epoch 21  89.1% | batch:       611 of       686\t|\tloss: 1.45941\n",
      "Training Epoch 21  89.2% | batch:       612 of       686\t|\tloss: 1.16019\n",
      "Training Epoch 21  89.4% | batch:       613 of       686\t|\tloss: 1.86251\n",
      "Training Epoch 21  89.5% | batch:       614 of       686\t|\tloss: 1.94102\n",
      "Training Epoch 21  89.7% | batch:       615 of       686\t|\tloss: 1.07859\n",
      "Training Epoch 21  89.8% | batch:       616 of       686\t|\tloss: 0.922566\n",
      "Training Epoch 21  89.9% | batch:       617 of       686\t|\tloss: 1.74407\n",
      "Training Epoch 21  90.1% | batch:       618 of       686\t|\tloss: 1.91452\n",
      "Training Epoch 21  90.2% | batch:       619 of       686\t|\tloss: 1.54003\n",
      "Training Epoch 21  90.4% | batch:       620 of       686\t|\tloss: 1.25631\n",
      "Training Epoch 21  90.5% | batch:       621 of       686\t|\tloss: 1.2328\n",
      "Training Epoch 21  90.7% | batch:       622 of       686\t|\tloss: 1.60693\n",
      "Training Epoch 21  90.8% | batch:       623 of       686\t|\tloss: 1.91061\n",
      "Training Epoch 21  91.0% | batch:       624 of       686\t|\tloss: 1.61878\n",
      "Training Epoch 21  91.1% | batch:       625 of       686\t|\tloss: 1.61151\n",
      "Training Epoch 21  91.3% | batch:       626 of       686\t|\tloss: 1.78184\n",
      "Training Epoch 21  91.4% | batch:       627 of       686\t|\tloss: 1.24042\n",
      "Training Epoch 21  91.5% | batch:       628 of       686\t|\tloss: 0.981422\n",
      "Training Epoch 21  91.7% | batch:       629 of       686\t|\tloss: 1.29637\n",
      "Training Epoch 21  91.8% | batch:       630 of       686\t|\tloss: 1.68104\n",
      "Training Epoch 21  92.0% | batch:       631 of       686\t|\tloss: 1.70049\n",
      "Training Epoch 21  92.1% | batch:       632 of       686\t|\tloss: 1.36919\n",
      "Training Epoch 21  92.3% | batch:       633 of       686\t|\tloss: 1.24128\n",
      "Training Epoch 21  92.4% | batch:       634 of       686\t|\tloss: 1.37728\n",
      "Training Epoch 21  92.6% | batch:       635 of       686\t|\tloss: 1.35519\n",
      "Training Epoch 21  92.7% | batch:       636 of       686\t|\tloss: 1.14642\n",
      "Training Epoch 21  92.9% | batch:       637 of       686\t|\tloss: 1.12979\n",
      "Training Epoch 21  93.0% | batch:       638 of       686\t|\tloss: 1.58985\n",
      "Training Epoch 21  93.1% | batch:       639 of       686\t|\tloss: 1.38226\n",
      "Training Epoch 21  93.3% | batch:       640 of       686\t|\tloss: 1.47279\n",
      "Training Epoch 21  93.4% | batch:       641 of       686\t|\tloss: 1.4243\n",
      "Training Epoch 21  93.6% | batch:       642 of       686\t|\tloss: 1.30266\n",
      "Training Epoch 21  93.7% | batch:       643 of       686\t|\tloss: 1.25238\n",
      "Training Epoch 21  93.9% | batch:       644 of       686\t|\tloss: 1.46923\n",
      "Training Epoch 21  94.0% | batch:       645 of       686\t|\tloss: 1.63002\n",
      "Training Epoch 21  94.2% | batch:       646 of       686\t|\tloss: 1.54756\n",
      "Training Epoch 21  94.3% | batch:       647 of       686\t|\tloss: 1.64359\n",
      "Training Epoch 21  94.5% | batch:       648 of       686\t|\tloss: 1.59187\n",
      "Training Epoch 21  94.6% | batch:       649 of       686\t|\tloss: 1.52868\n",
      "Training Epoch 21  94.8% | batch:       650 of       686\t|\tloss: 1.27722\n",
      "Training Epoch 21  94.9% | batch:       651 of       686\t|\tloss: 1.37731\n",
      "Training Epoch 21  95.0% | batch:       652 of       686\t|\tloss: 1.88655\n",
      "Training Epoch 21  95.2% | batch:       653 of       686\t|\tloss: 1.88988\n",
      "Training Epoch 21  95.3% | batch:       654 of       686\t|\tloss: 1.49144\n",
      "Training Epoch 21  95.5% | batch:       655 of       686\t|\tloss: 1.21948\n",
      "Training Epoch 21  95.6% | batch:       656 of       686\t|\tloss: 1.03464\n",
      "Training Epoch 21  95.8% | batch:       657 of       686\t|\tloss: 1.15626\n",
      "Training Epoch 21  95.9% | batch:       658 of       686\t|\tloss: 1.37947\n",
      "Training Epoch 21  96.1% | batch:       659 of       686\t|\tloss: 1.4931\n",
      "Training Epoch 21  96.2% | batch:       660 of       686\t|\tloss: 1.12192\n",
      "Training Epoch 21  96.4% | batch:       661 of       686\t|\tloss: 1.22001\n",
      "Training Epoch 21  96.5% | batch:       662 of       686\t|\tloss: 1.20759\n",
      "Training Epoch 21  96.6% | batch:       663 of       686\t|\tloss: 1.58758\n",
      "Training Epoch 21  96.8% | batch:       664 of       686\t|\tloss: 1.43173\n",
      "Training Epoch 21  96.9% | batch:       665 of       686\t|\tloss: 1.228\n",
      "Training Epoch 21  97.1% | batch:       666 of       686\t|\tloss: 1.63052\n",
      "Training Epoch 21  97.2% | batch:       667 of       686\t|\tloss: 1.43612\n",
      "Training Epoch 21  97.4% | batch:       668 of       686\t|\tloss: 1.41303\n",
      "Training Epoch 21  97.5% | batch:       669 of       686\t|\tloss: 1.22405\n",
      "Training Epoch 21  97.7% | batch:       670 of       686\t|\tloss: 1.27193\n",
      "Training Epoch 21  97.8% | batch:       671 of       686\t|\tloss: 1.01693\n",
      "Training Epoch 21  98.0% | batch:       672 of       686\t|\tloss: 1.32624\n",
      "Training Epoch 21  98.1% | batch:       673 of       686\t|\tloss: 2.15721\n",
      "Training Epoch 21  98.3% | batch:       674 of       686\t|\tloss: 1.32113\n",
      "Training Epoch 21  98.4% | batch:       675 of       686\t|\tloss: 1.49687\n",
      "Training Epoch 21  98.5% | batch:       676 of       686\t|\tloss: 1.42126\n",
      "Training Epoch 21  98.7% | batch:       677 of       686\t|\tloss: 1.40275\n",
      "Training Epoch 21  98.8% | batch:       678 of       686\t|\tloss: 1.72523\n",
      "Training Epoch 21  99.0% | batch:       679 of       686\t|\tloss: 1.57676\n",
      "Training Epoch 21  99.1% | batch:       680 of       686\t|\tloss: 1.19922\n",
      "Training Epoch 21  99.3% | batch:       681 of       686\t|\tloss: 1.43908\n",
      "Training Epoch 21  99.4% | batch:       682 of       686\t|\tloss: 1.54682\n",
      "Training Epoch 21  99.6% | batch:       683 of       686\t|\tloss: 1.50944\n",
      "Training Epoch 21  99.7% | batch:       684 of       686\t|\tloss: 1.53238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:09:33,123 | INFO : Epoch 21 Training Summary: epoch: 21.000000 | loss: 1.499251 | \n",
      "2023-05-25 22:09:33,125 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 22.391666412353516 seconds\n",
      "\n",
      "2023-05-25 22:09:33,127 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.856613840375626 seconds\n",
      "2023-05-25 22:09:33,128 | INFO : Avg batch train. time: 0.03477640501512482 seconds\n",
      "2023-05-25 22:09:33,130 | INFO : Avg sample train. time: 0.00027204075306888224 seconds\n",
      "2023-05-25 22:09:33,131 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 21  99.9% | batch:       685 of       686\t|\tloss: 2.16301\n",
      "\n",
      "Evaluating Epoch 21   0.0% | batch:         0 of       172\t|\tloss: 1.39356\n",
      "Evaluating Epoch 21   0.6% | batch:         1 of       172\t|\tloss: 1.71421\n",
      "Evaluating Epoch 21   1.2% | batch:         2 of       172\t|\tloss: 1.04885\n",
      "Evaluating Epoch 21   1.7% | batch:         3 of       172\t|\tloss: 2.7294\n",
      "Evaluating Epoch 21   2.3% | batch:         4 of       172\t|\tloss: 1.46131\n",
      "Evaluating Epoch 21   2.9% | batch:         5 of       172\t|\tloss: 1.34379\n",
      "Evaluating Epoch 21   3.5% | batch:         6 of       172\t|\tloss: 1.5204\n",
      "Evaluating Epoch 21   4.1% | batch:         7 of       172\t|\tloss: 3.27378\n",
      "Evaluating Epoch 21   4.7% | batch:         8 of       172\t|\tloss: 0.858071\n",
      "Evaluating Epoch 21   5.2% | batch:         9 of       172\t|\tloss: 1.76999\n",
      "Evaluating Epoch 21   5.8% | batch:        10 of       172\t|\tloss: 1.58995\n",
      "Evaluating Epoch 21   6.4% | batch:        11 of       172\t|\tloss: 1.43095\n",
      "Evaluating Epoch 21   7.0% | batch:        12 of       172\t|\tloss: 1.49584\n",
      "Evaluating Epoch 21   7.6% | batch:        13 of       172\t|\tloss: 1.58762\n",
      "Evaluating Epoch 21   8.1% | batch:        14 of       172\t|\tloss: 2.0038\n",
      "Evaluating Epoch 21   8.7% | batch:        15 of       172\t|\tloss: 1.44527\n",
      "Evaluating Epoch 21   9.3% | batch:        16 of       172\t|\tloss: 2.25784\n",
      "Evaluating Epoch 21   9.9% | batch:        17 of       172\t|\tloss: 1.03277\n",
      "Evaluating Epoch 21  10.5% | batch:        18 of       172\t|\tloss: 19.1894\n",
      "Evaluating Epoch 21  11.0% | batch:        19 of       172\t|\tloss: 1.51599\n",
      "Evaluating Epoch 21  11.6% | batch:        20 of       172\t|\tloss: 3.04595\n",
      "Evaluating Epoch 21  12.2% | batch:        21 of       172\t|\tloss: 0.442513\n",
      "Evaluating Epoch 21  12.8% | batch:        22 of       172\t|\tloss: 4.63323\n",
      "Evaluating Epoch 21  13.4% | batch:        23 of       172\t|\tloss: 2.89906\n",
      "Evaluating Epoch 21  14.0% | batch:        24 of       172\t|\tloss: 1.87117\n",
      "Evaluating Epoch 21  14.5% | batch:        25 of       172\t|\tloss: 3.24525\n",
      "Evaluating Epoch 21  15.1% | batch:        26 of       172\t|\tloss: 8.47848\n",
      "Evaluating Epoch 21  15.7% | batch:        27 of       172\t|\tloss: 17.1544\n",
      "Evaluating Epoch 21  16.3% | batch:        28 of       172\t|\tloss: 0.649117\n",
      "Evaluating Epoch 21  16.9% | batch:        29 of       172\t|\tloss: 2.43486\n",
      "Evaluating Epoch 21  17.4% | batch:        30 of       172\t|\tloss: 0.562428\n",
      "Evaluating Epoch 21  18.0% | batch:        31 of       172\t|\tloss: 0.341172\n",
      "Evaluating Epoch 21  18.6% | batch:        32 of       172\t|\tloss: 1.12039\n",
      "Evaluating Epoch 21  19.2% | batch:        33 of       172\t|\tloss: 0.498015\n",
      "Evaluating Epoch 21  19.8% | batch:        34 of       172\t|\tloss: 0.756308\n",
      "Evaluating Epoch 21  20.3% | batch:        35 of       172\t|\tloss: 0.384248\n",
      "Evaluating Epoch 21  20.9% | batch:        36 of       172\t|\tloss: 3.01797\n",
      "Evaluating Epoch 21  21.5% | batch:        37 of       172\t|\tloss: 3.53348\n",
      "Evaluating Epoch 21  22.1% | batch:        38 of       172\t|\tloss: 3.94505\n",
      "Evaluating Epoch 21  22.7% | batch:        39 of       172\t|\tloss: 8.16429\n",
      "Evaluating Epoch 21  23.3% | batch:        40 of       172\t|\tloss: 1.18289\n",
      "Evaluating Epoch 21  23.8% | batch:        41 of       172\t|\tloss: 1.71245\n",
      "Evaluating Epoch 21  24.4% | batch:        42 of       172\t|\tloss: 0.916851\n",
      "Evaluating Epoch 21  25.0% | batch:        43 of       172\t|\tloss: 20.7796\n",
      "Evaluating Epoch 21  25.6% | batch:        44 of       172\t|\tloss: 1.46792\n",
      "Evaluating Epoch 21  26.2% | batch:        45 of       172\t|\tloss: 1.64728\n",
      "Evaluating Epoch 21  26.7% | batch:        46 of       172\t|\tloss: 0.159984\n",
      "Evaluating Epoch 21  27.3% | batch:        47 of       172\t|\tloss: 0.777399\n",
      "Evaluating Epoch 21  27.9% | batch:        48 of       172\t|\tloss: 0.411341\n",
      "Evaluating Epoch 21  28.5% | batch:        49 of       172\t|\tloss: 1.09028\n",
      "Evaluating Epoch 21  29.1% | batch:        50 of       172\t|\tloss: 0.63271\n",
      "Evaluating Epoch 21  29.7% | batch:        51 of       172\t|\tloss: 0.856305\n",
      "Evaluating Epoch 21  30.2% | batch:        52 of       172\t|\tloss: 0.41229\n",
      "Evaluating Epoch 21  30.8% | batch:        53 of       172\t|\tloss: 2.14565\n",
      "Evaluating Epoch 21  31.4% | batch:        54 of       172\t|\tloss: 1.1049\n",
      "Evaluating Epoch 21  32.0% | batch:        55 of       172\t|\tloss: 0.374479\n",
      "Evaluating Epoch 21  32.6% | batch:        56 of       172\t|\tloss: 2.69151\n",
      "Evaluating Epoch 21  33.1% | batch:        57 of       172\t|\tloss: 0.688099\n",
      "Evaluating Epoch 21  33.7% | batch:        58 of       172\t|\tloss: 1.86293\n",
      "Evaluating Epoch 21  34.3% | batch:        59 of       172\t|\tloss: 1.04983\n",
      "Evaluating Epoch 21  34.9% | batch:        60 of       172\t|\tloss: 0.785038\n",
      "Evaluating Epoch 21  35.5% | batch:        61 of       172\t|\tloss: 1.69221\n",
      "Evaluating Epoch 21  36.0% | batch:        62 of       172\t|\tloss: 0.636219\n",
      "Evaluating Epoch 21  36.6% | batch:        63 of       172\t|\tloss: 2.56626\n",
      "Evaluating Epoch 21  37.2% | batch:        64 of       172\t|\tloss: 0.521824\n",
      "Evaluating Epoch 21  37.8% | batch:        65 of       172\t|\tloss: 1.78948\n",
      "Evaluating Epoch 21  38.4% | batch:        66 of       172\t|\tloss: 1.65084\n",
      "Evaluating Epoch 21  39.0% | batch:        67 of       172\t|\tloss: 0.236592\n",
      "Evaluating Epoch 21  39.5% | batch:        68 of       172\t|\tloss: 1.80324\n",
      "Evaluating Epoch 21  40.1% | batch:        69 of       172\t|\tloss: 0.838734\n",
      "Evaluating Epoch 21  40.7% | batch:        70 of       172\t|\tloss: 1.30531\n",
      "Evaluating Epoch 21  41.3% | batch:        71 of       172\t|\tloss: 1.5117\n",
      "Evaluating Epoch 21  41.9% | batch:        72 of       172\t|\tloss: 0.479341\n",
      "Evaluating Epoch 21  42.4% | batch:        73 of       172\t|\tloss: 2.1412\n",
      "Evaluating Epoch 21  43.0% | batch:        74 of       172\t|\tloss: 0.821627\n",
      "Evaluating Epoch 21  43.6% | batch:        75 of       172\t|\tloss: 0.845895\n",
      "Evaluating Epoch 21  44.2% | batch:        76 of       172\t|\tloss: 0.813876\n",
      "Evaluating Epoch 21  44.8% | batch:        77 of       172\t|\tloss: 1.04191\n",
      "Evaluating Epoch 21  45.3% | batch:        78 of       172\t|\tloss: 0.987728\n",
      "Evaluating Epoch 21  45.9% | batch:        79 of       172\t|\tloss: 0.656492\n",
      "Evaluating Epoch 21  46.5% | batch:        80 of       172\t|\tloss: 0.824932\n",
      "Evaluating Epoch 21  47.1% | batch:        81 of       172\t|\tloss: 0.923458\n",
      "Evaluating Epoch 21  47.7% | batch:        82 of       172\t|\tloss: 0.798957\n",
      "Evaluating Epoch 21  48.3% | batch:        83 of       172\t|\tloss: 0.796759\n",
      "Evaluating Epoch 21  48.8% | batch:        84 of       172\t|\tloss: 0.369723\n",
      "Evaluating Epoch 21  49.4% | batch:        85 of       172\t|\tloss: 0.439797\n",
      "Evaluating Epoch 21  50.0% | batch:        86 of       172\t|\tloss: 0.673667\n",
      "Evaluating Epoch 21  50.6% | batch:        87 of       172\t|\tloss: 0.389897\n",
      "Evaluating Epoch 21  51.2% | batch:        88 of       172\t|\tloss: 0.326572\n",
      "Evaluating Epoch 21  51.7% | batch:        89 of       172\t|\tloss: 0.859919\n",
      "Evaluating Epoch 21  52.3% | batch:        90 of       172\t|\tloss: 0.71262\n",
      "Evaluating Epoch 21  52.9% | batch:        91 of       172\t|\tloss: 0.446293\n",
      "Evaluating Epoch 21  53.5% | batch:        92 of       172\t|\tloss: 0.4518\n",
      "Evaluating Epoch 21  54.1% | batch:        93 of       172\t|\tloss: 1.10904\n",
      "Evaluating Epoch 21  54.7% | batch:        94 of       172\t|\tloss: 0.483443\n",
      "Evaluating Epoch 21  55.2% | batch:        95 of       172\t|\tloss: 0.524727\n",
      "Evaluating Epoch 21  55.8% | batch:        96 of       172\t|\tloss: 0.920983\n",
      "Evaluating Epoch 21  56.4% | batch:        97 of       172\t|\tloss: 0.502993\n",
      "Evaluating Epoch 21  57.0% | batch:        98 of       172\t|\tloss: 0.462837\n",
      "Evaluating Epoch 21  57.6% | batch:        99 of       172\t|\tloss: 0.577053\n",
      "Evaluating Epoch 21  58.1% | batch:       100 of       172\t|\tloss: 0.562496\n",
      "Evaluating Epoch 21  58.7% | batch:       101 of       172\t|\tloss: 0.329209\n",
      "Evaluating Epoch 21  59.3% | batch:       102 of       172\t|\tloss: 0.649532\n",
      "Evaluating Epoch 21  59.9% | batch:       103 of       172\t|\tloss: 1.19269\n",
      "Evaluating Epoch 21  60.5% | batch:       104 of       172\t|\tloss: 0.467575\n",
      "Evaluating Epoch 21  61.0% | batch:       105 of       172\t|\tloss: 0.537357\n",
      "Evaluating Epoch 21  61.6% | batch:       106 of       172\t|\tloss: 0.643732\n",
      "Evaluating Epoch 21  62.2% | batch:       107 of       172\t|\tloss: 1.18451\n",
      "Evaluating Epoch 21  62.8% | batch:       108 of       172\t|\tloss: 0.327343\n",
      "Evaluating Epoch 21  63.4% | batch:       109 of       172\t|\tloss: 0.582777\n",
      "Evaluating Epoch 21  64.0% | batch:       110 of       172\t|\tloss: 1.09563\n",
      "Evaluating Epoch 21  64.5% | batch:       111 of       172\t|\tloss: 0.419619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 21  65.1% | batch:       112 of       172\t|\tloss: 0.273468\n",
      "Evaluating Epoch 21  65.7% | batch:       113 of       172\t|\tloss: 0.993854\n",
      "Evaluating Epoch 21  66.3% | batch:       114 of       172\t|\tloss: 1.28773\n",
      "Evaluating Epoch 21  66.9% | batch:       115 of       172\t|\tloss: 1.23749\n",
      "Evaluating Epoch 21  67.4% | batch:       116 of       172\t|\tloss: 1.16364\n",
      "Evaluating Epoch 21  68.0% | batch:       117 of       172\t|\tloss: 1.11726\n",
      "Evaluating Epoch 21  68.6% | batch:       118 of       172\t|\tloss: 0.525066\n",
      "Evaluating Epoch 21  69.2% | batch:       119 of       172\t|\tloss: 0.784063\n",
      "Evaluating Epoch 21  69.8% | batch:       120 of       172\t|\tloss: 0.399115\n",
      "Evaluating Epoch 21  70.3% | batch:       121 of       172\t|\tloss: 0.826906\n",
      "Evaluating Epoch 21  70.9% | batch:       122 of       172\t|\tloss: 0.716738\n",
      "Evaluating Epoch 21  71.5% | batch:       123 of       172\t|\tloss: 0.755856\n",
      "Evaluating Epoch 21  72.1% | batch:       124 of       172\t|\tloss: 2.0599\n",
      "Evaluating Epoch 21  72.7% | batch:       125 of       172\t|\tloss: 1.00654\n",
      "Evaluating Epoch 21  73.3% | batch:       126 of       172\t|\tloss: 1.0631\n",
      "Evaluating Epoch 21  73.8% | batch:       127 of       172\t|\tloss: 0.589661\n",
      "Evaluating Epoch 21  74.4% | batch:       128 of       172\t|\tloss: 0.692202\n",
      "Evaluating Epoch 21  75.0% | batch:       129 of       172\t|\tloss: 0.761766\n",
      "Evaluating Epoch 21  75.6% | batch:       130 of       172\t|\tloss: 0.260324\n",
      "Evaluating Epoch 21  76.2% | batch:       131 of       172\t|\tloss: 0.906322\n",
      "Evaluating Epoch 21  76.7% | batch:       132 of       172\t|\tloss: 0.505413\n",
      "Evaluating Epoch 21  77.3% | batch:       133 of       172\t|\tloss: 0.131387\n",
      "Evaluating Epoch 21  77.9% | batch:       134 of       172\t|\tloss: 0.210841\n",
      "Evaluating Epoch 21  78.5% | batch:       135 of       172\t|\tloss: 0.18467\n",
      "Evaluating Epoch 21  79.1% | batch:       136 of       172\t|\tloss: 0.222298\n",
      "Evaluating Epoch 21  79.7% | batch:       137 of       172\t|\tloss: 0.136934\n",
      "Evaluating Epoch 21  80.2% | batch:       138 of       172\t|\tloss: 0.368686\n",
      "Evaluating Epoch 21  80.8% | batch:       139 of       172\t|\tloss: 0.147498\n",
      "Evaluating Epoch 21  81.4% | batch:       140 of       172\t|\tloss: 0.215046\n",
      "Evaluating Epoch 21  82.0% | batch:       141 of       172\t|\tloss: 0.260073\n",
      "Evaluating Epoch 21  82.6% | batch:       142 of       172\t|\tloss: 0.345181\n",
      "Evaluating Epoch 21  83.1% | batch:       143 of       172\t|\tloss: 0.188982\n",
      "Evaluating Epoch 21  83.7% | batch:       144 of       172\t|\tloss: 0.237156\n",
      "Evaluating Epoch 21  84.3% | batch:       145 of       172\t|\tloss: 0.120583\n",
      "Evaluating Epoch 21  84.9% | batch:       146 of       172\t|\tloss: 0.272453\n",
      "Evaluating Epoch 21  85.5% | batch:       147 of       172\t|\tloss: 0.161122\n",
      "Evaluating Epoch 21  86.0% | batch:       148 of       172\t|\tloss: 0.246046\n",
      "Evaluating Epoch 21  86.6% | batch:       149 of       172\t|\tloss: 0.07285\n",
      "Evaluating Epoch 21  87.2% | batch:       150 of       172\t|\tloss: 0.480292\n",
      "Evaluating Epoch 21  87.8% | batch:       151 of       172\t|\tloss: 0.703545\n",
      "Evaluating Epoch 21  88.4% | batch:       152 of       172\t|\tloss: 0.360554\n",
      "Evaluating Epoch 21  89.0% | batch:       153 of       172\t|\tloss: 0.482257\n",
      "Evaluating Epoch 21  89.5% | batch:       154 of       172\t|\tloss: 0.588675\n",
      "Evaluating Epoch 21  90.1% | batch:       155 of       172\t|\tloss: 0.289891\n",
      "Evaluating Epoch 21  90.7% | batch:       156 of       172\t|\tloss: 0.80273\n",
      "Evaluating Epoch 21  91.3% | batch:       157 of       172\t|\tloss: 0.724461\n",
      "Evaluating Epoch 21  91.9% | batch:       158 of       172\t|\tloss: 0.526587\n",
      "Evaluating Epoch 21  92.4% | batch:       159 of       172\t|\tloss: 1.10027\n",
      "Evaluating Epoch 21  93.0% | batch:       160 of       172\t|\tloss: 0.48609\n",
      "Evaluating Epoch 21  93.6% | batch:       161 of       172\t|\tloss: 1.42296\n",
      "Evaluating Epoch 21  94.2% | batch:       162 of       172\t|\tloss: 0.671927\n",
      "Evaluating Epoch 21  94.8% | batch:       163 of       172\t|\tloss: 0.479834\n",
      "Evaluating Epoch 21  95.3% | batch:       164 of       172\t|\tloss: 0.835363\n",
      "Evaluating Epoch 21  95.9% | batch:       165 of       172\t|\tloss: 0.524636\n",
      "Evaluating Epoch 21  96.5% | batch:       166 of       172\t|\tloss: 0.355771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:09:37,706 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 4.572960138320923 seconds\n",
      "\n",
      "2023-05-25 22:09:37,708 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 3.999639543620023 seconds\n",
      "2023-05-25 22:09:37,709 | INFO : Avg batch val. time: 0.0232537182768606 seconds\n",
      "2023-05-25 22:09:37,709 | INFO : Avg sample val. time: 0.00018215783320216892 seconds\n",
      "2023-05-25 22:09:37,710 | INFO : Epoch 21 Validation Summary: epoch: 21.000000 | loss: 1.393942 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 21  97.1% | batch:       167 of       172\t|\tloss: 0.925384\n",
      "Evaluating Epoch 21  97.7% | batch:       168 of       172\t|\tloss: 0.517573\n",
      "Evaluating Epoch 21  98.3% | batch:       169 of       172\t|\tloss: 0.564427\n",
      "Evaluating Epoch 21  98.8% | batch:       170 of       172\t|\tloss: 0.898084\n",
      "Evaluating Epoch 21  99.4% | batch:       171 of       172\t|\tloss: 0.756228\n",
      "\n",
      "Training Epoch 22   0.0% | batch:         0 of       686\t|\tloss: 1.91852\n",
      "Training Epoch 22   0.1% | batch:         1 of       686\t|\tloss: 1.4262\n",
      "Training Epoch 22   0.3% | batch:         2 of       686\t|\tloss: 1.38687\n",
      "Training Epoch 22   0.4% | batch:         3 of       686\t|\tloss: 1.37155\n",
      "Training Epoch 22   0.6% | batch:         4 of       686\t|\tloss: 1.74454\n",
      "Training Epoch 22   0.7% | batch:         5 of       686\t|\tloss: 1.09068\n",
      "Training Epoch 22   0.9% | batch:         6 of       686\t|\tloss: 1.22543\n",
      "Training Epoch 22   1.0% | batch:         7 of       686\t|\tloss: 1.16912\n",
      "Training Epoch 22   1.2% | batch:         8 of       686\t|\tloss: 1.48614\n",
      "Training Epoch 22   1.3% | batch:         9 of       686\t|\tloss: 1.77627\n",
      "Training Epoch 22   1.5% | batch:        10 of       686\t|\tloss: 1.51928\n",
      "Training Epoch 22   1.6% | batch:        11 of       686\t|\tloss: 1.04788\n",
      "Training Epoch 22   1.7% | batch:        12 of       686\t|\tloss: 1.28707\n",
      "Training Epoch 22   1.9% | batch:        13 of       686\t|\tloss: 1.27289\n",
      "Training Epoch 22   2.0% | batch:        14 of       686\t|\tloss: 1.16324\n",
      "Training Epoch 22   2.2% | batch:        15 of       686\t|\tloss: 1.33073\n",
      "Training Epoch 22   2.3% | batch:        16 of       686\t|\tloss: 1.483\n",
      "Training Epoch 22   2.5% | batch:        17 of       686\t|\tloss: 1.52358\n",
      "Training Epoch 22   2.6% | batch:        18 of       686\t|\tloss: 1.32601\n",
      "Training Epoch 22   2.8% | batch:        19 of       686\t|\tloss: 1.25713\n",
      "Training Epoch 22   2.9% | batch:        20 of       686\t|\tloss: 1.39015\n",
      "Training Epoch 22   3.1% | batch:        21 of       686\t|\tloss: 1.48666\n",
      "Training Epoch 22   3.2% | batch:        22 of       686\t|\tloss: 1.17329\n",
      "Training Epoch 22   3.4% | batch:        23 of       686\t|\tloss: 1.68529\n",
      "Training Epoch 22   3.5% | batch:        24 of       686\t|\tloss: 1.42964\n",
      "Training Epoch 22   3.6% | batch:        25 of       686\t|\tloss: 1.47293\n",
      "Training Epoch 22   3.8% | batch:        26 of       686\t|\tloss: 1.13082\n",
      "Training Epoch 22   3.9% | batch:        27 of       686\t|\tloss: 1.30904\n",
      "Training Epoch 22   4.1% | batch:        28 of       686\t|\tloss: 1.30819\n",
      "Training Epoch 22   4.2% | batch:        29 of       686\t|\tloss: 1.37225\n",
      "Training Epoch 22   4.4% | batch:        30 of       686\t|\tloss: 1.23811\n",
      "Training Epoch 22   4.5% | batch:        31 of       686\t|\tloss: 1.28881\n",
      "Training Epoch 22   4.7% | batch:        32 of       686\t|\tloss: 1.38999\n",
      "Training Epoch 22   4.8% | batch:        33 of       686\t|\tloss: 1.23989\n",
      "Training Epoch 22   5.0% | batch:        34 of       686\t|\tloss: 1.32846\n",
      "Training Epoch 22   5.1% | batch:        35 of       686\t|\tloss: 1.57498\n",
      "Training Epoch 22   5.2% | batch:        36 of       686\t|\tloss: 1.27393\n",
      "Training Epoch 22   5.4% | batch:        37 of       686\t|\tloss: 1.28725\n",
      "Training Epoch 22   5.5% | batch:        38 of       686\t|\tloss: 1.44804\n",
      "Training Epoch 22   5.7% | batch:        39 of       686\t|\tloss: 1.43393\n",
      "Training Epoch 22   5.8% | batch:        40 of       686\t|\tloss: 1.53216\n",
      "Training Epoch 22   6.0% | batch:        41 of       686\t|\tloss: 1.10271\n",
      "Training Epoch 22   6.1% | batch:        42 of       686\t|\tloss: 1.35637\n",
      "Training Epoch 22   6.3% | batch:        43 of       686\t|\tloss: 1.48064\n",
      "Training Epoch 22   6.4% | batch:        44 of       686\t|\tloss: 1.43066\n",
      "Training Epoch 22   6.6% | batch:        45 of       686\t|\tloss: 1.02967\n",
      "Training Epoch 22   6.7% | batch:        46 of       686\t|\tloss: 1.41445\n",
      "Training Epoch 22   6.9% | batch:        47 of       686\t|\tloss: 1.5032\n",
      "Training Epoch 22   7.0% | batch:        48 of       686\t|\tloss: 1.37675\n",
      "Training Epoch 22   7.1% | batch:        49 of       686\t|\tloss: 1.51133\n",
      "Training Epoch 22   7.3% | batch:        50 of       686\t|\tloss: 1.56689\n",
      "Training Epoch 22   7.4% | batch:        51 of       686\t|\tloss: 1.52462\n",
      "Training Epoch 22   7.6% | batch:        52 of       686\t|\tloss: 1.48152\n",
      "Training Epoch 22   7.7% | batch:        53 of       686\t|\tloss: 1.44633\n",
      "Training Epoch 22   7.9% | batch:        54 of       686\t|\tloss: 1.49682\n",
      "Training Epoch 22   8.0% | batch:        55 of       686\t|\tloss: 1.33149\n",
      "Training Epoch 22   8.2% | batch:        56 of       686\t|\tloss: 1.195\n",
      "Training Epoch 22   8.3% | batch:        57 of       686\t|\tloss: 1.30249\n",
      "Training Epoch 22   8.5% | batch:        58 of       686\t|\tloss: 1.09408\n",
      "Training Epoch 22   8.6% | batch:        59 of       686\t|\tloss: 1.43558\n",
      "Training Epoch 22   8.7% | batch:        60 of       686\t|\tloss: 1.5286\n",
      "Training Epoch 22   8.9% | batch:        61 of       686\t|\tloss: 1.76279\n",
      "Training Epoch 22   9.0% | batch:        62 of       686\t|\tloss: 1.5279\n",
      "Training Epoch 22   9.2% | batch:        63 of       686\t|\tloss: 1.63677\n",
      "Training Epoch 22   9.3% | batch:        64 of       686\t|\tloss: 1.86013\n",
      "Training Epoch 22   9.5% | batch:        65 of       686\t|\tloss: 1.13545\n",
      "Training Epoch 22   9.6% | batch:        66 of       686\t|\tloss: 1.38455\n",
      "Training Epoch 22   9.8% | batch:        67 of       686\t|\tloss: 1.58116\n",
      "Training Epoch 22   9.9% | batch:        68 of       686\t|\tloss: 1.20438\n",
      "Training Epoch 22  10.1% | batch:        69 of       686\t|\tloss: 1.22906\n",
      "Training Epoch 22  10.2% | batch:        70 of       686\t|\tloss: 1.15266\n",
      "Training Epoch 22  10.3% | batch:        71 of       686\t|\tloss: 1.17852\n",
      "Training Epoch 22  10.5% | batch:        72 of       686\t|\tloss: 1.53402\n",
      "Training Epoch 22  10.6% | batch:        73 of       686\t|\tloss: 0.932684\n",
      "Training Epoch 22  10.8% | batch:        74 of       686\t|\tloss: 1.41956\n",
      "Training Epoch 22  10.9% | batch:        75 of       686\t|\tloss: 1.45696\n",
      "Training Epoch 22  11.1% | batch:        76 of       686\t|\tloss: 1.81749\n",
      "Training Epoch 22  11.2% | batch:        77 of       686\t|\tloss: 1.34794\n",
      "Training Epoch 22  11.4% | batch:        78 of       686\t|\tloss: 1.60384\n",
      "Training Epoch 22  11.5% | batch:        79 of       686\t|\tloss: 1.438\n",
      "Training Epoch 22  11.7% | batch:        80 of       686\t|\tloss: 1.3669\n",
      "Training Epoch 22  11.8% | batch:        81 of       686\t|\tloss: 1.05739\n",
      "Training Epoch 22  12.0% | batch:        82 of       686\t|\tloss: 1.3493\n",
      "Training Epoch 22  12.1% | batch:        83 of       686\t|\tloss: 1.28742\n",
      "Training Epoch 22  12.2% | batch:        84 of       686\t|\tloss: 1.40425\n",
      "Training Epoch 22  12.4% | batch:        85 of       686\t|\tloss: 1.23267\n",
      "Training Epoch 22  12.5% | batch:        86 of       686\t|\tloss: 1.57686\n",
      "Training Epoch 22  12.7% | batch:        87 of       686\t|\tloss: 1.19496\n",
      "Training Epoch 22  12.8% | batch:        88 of       686\t|\tloss: 1.14358\n",
      "Training Epoch 22  13.0% | batch:        89 of       686\t|\tloss: 1.34145\n",
      "Training Epoch 22  13.1% | batch:        90 of       686\t|\tloss: 1.33145\n",
      "Training Epoch 22  13.3% | batch:        91 of       686\t|\tloss: 1.51706\n",
      "Training Epoch 22  13.4% | batch:        92 of       686\t|\tloss: 1.11334\n",
      "Training Epoch 22  13.6% | batch:        93 of       686\t|\tloss: 1.40567\n",
      "Training Epoch 22  13.7% | batch:        94 of       686\t|\tloss: 1.34002\n",
      "Training Epoch 22  13.8% | batch:        95 of       686\t|\tloss: 1.35061\n",
      "Training Epoch 22  14.0% | batch:        96 of       686\t|\tloss: 1.20757\n",
      "Training Epoch 22  14.1% | batch:        97 of       686\t|\tloss: 1.59328\n",
      "Training Epoch 22  14.3% | batch:        98 of       686\t|\tloss: 1.24044\n",
      "Training Epoch 22  14.4% | batch:        99 of       686\t|\tloss: 1.29379\n",
      "Training Epoch 22  14.6% | batch:       100 of       686\t|\tloss: 1.32753\n",
      "Training Epoch 22  14.7% | batch:       101 of       686\t|\tloss: 1.43637\n",
      "Training Epoch 22  14.9% | batch:       102 of       686\t|\tloss: 1.52702\n",
      "Training Epoch 22  15.0% | batch:       103 of       686\t|\tloss: 1.46956\n",
      "Training Epoch 22  15.2% | batch:       104 of       686\t|\tloss: 1.53618\n",
      "Training Epoch 22  15.3% | batch:       105 of       686\t|\tloss: 1.17703\n",
      "Training Epoch 22  15.5% | batch:       106 of       686\t|\tloss: 1.41776\n",
      "Training Epoch 22  15.6% | batch:       107 of       686\t|\tloss: 1.76161\n",
      "Training Epoch 22  15.7% | batch:       108 of       686\t|\tloss: 1.33492\n",
      "Training Epoch 22  15.9% | batch:       109 of       686\t|\tloss: 1.36301\n",
      "Training Epoch 22  16.0% | batch:       110 of       686\t|\tloss: 1.22672\n",
      "Training Epoch 22  16.2% | batch:       111 of       686\t|\tloss: 1.34394\n",
      "Training Epoch 22  16.3% | batch:       112 of       686\t|\tloss: 1.69997\n",
      "Training Epoch 22  16.5% | batch:       113 of       686\t|\tloss: 1.5003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 22  16.6% | batch:       114 of       686\t|\tloss: 1.48421\n",
      "Training Epoch 22  16.8% | batch:       115 of       686\t|\tloss: 1.50561\n",
      "Training Epoch 22  16.9% | batch:       116 of       686\t|\tloss: 1.77076\n",
      "Training Epoch 22  17.1% | batch:       117 of       686\t|\tloss: 1.64655\n",
      "Training Epoch 22  17.2% | batch:       118 of       686\t|\tloss: 1.22664\n",
      "Training Epoch 22  17.3% | batch:       119 of       686\t|\tloss: 1.41402\n",
      "Training Epoch 22  17.5% | batch:       120 of       686\t|\tloss: 1.56133\n",
      "Training Epoch 22  17.6% | batch:       121 of       686\t|\tloss: 1.57991\n",
      "Training Epoch 22  17.8% | batch:       122 of       686\t|\tloss: 2.05394\n",
      "Training Epoch 22  17.9% | batch:       123 of       686\t|\tloss: 1.22947\n",
      "Training Epoch 22  18.1% | batch:       124 of       686\t|\tloss: 1.3583\n",
      "Training Epoch 22  18.2% | batch:       125 of       686\t|\tloss: 1.41034\n",
      "Training Epoch 22  18.4% | batch:       126 of       686\t|\tloss: 1.42656\n",
      "Training Epoch 22  18.5% | batch:       127 of       686\t|\tloss: 1.23802\n",
      "Training Epoch 22  18.7% | batch:       128 of       686\t|\tloss: 1.49214\n",
      "Training Epoch 22  18.8% | batch:       129 of       686\t|\tloss: 1.24133\n",
      "Training Epoch 22  19.0% | batch:       130 of       686\t|\tloss: 1.21929\n",
      "Training Epoch 22  19.1% | batch:       131 of       686\t|\tloss: 0.965267\n",
      "Training Epoch 22  19.2% | batch:       132 of       686\t|\tloss: 1.20467\n",
      "Training Epoch 22  19.4% | batch:       133 of       686\t|\tloss: 0.920481\n",
      "Training Epoch 22  19.5% | batch:       134 of       686\t|\tloss: 1.51321\n",
      "Training Epoch 22  19.7% | batch:       135 of       686\t|\tloss: 1.04196\n",
      "Training Epoch 22  19.8% | batch:       136 of       686\t|\tloss: 1.43121\n",
      "Training Epoch 22  20.0% | batch:       137 of       686\t|\tloss: 1.73655\n",
      "Training Epoch 22  20.1% | batch:       138 of       686\t|\tloss: 1.50813\n",
      "Training Epoch 22  20.3% | batch:       139 of       686\t|\tloss: 1.43318\n",
      "Training Epoch 22  20.4% | batch:       140 of       686\t|\tloss: 1.07705\n",
      "Training Epoch 22  20.6% | batch:       141 of       686\t|\tloss: 0.965422\n",
      "Training Epoch 22  20.7% | batch:       142 of       686\t|\tloss: 1.2597\n",
      "Training Epoch 22  20.8% | batch:       143 of       686\t|\tloss: 1.02577\n",
      "Training Epoch 22  21.0% | batch:       144 of       686\t|\tloss: 1.24388\n",
      "Training Epoch 22  21.1% | batch:       145 of       686\t|\tloss: 1.66318\n",
      "Training Epoch 22  21.3% | batch:       146 of       686\t|\tloss: 1.23803\n",
      "Training Epoch 22  21.4% | batch:       147 of       686\t|\tloss: 1.53212\n",
      "Training Epoch 22  21.6% | batch:       148 of       686\t|\tloss: 1.07253\n",
      "Training Epoch 22  21.7% | batch:       149 of       686\t|\tloss: 1.1878\n",
      "Training Epoch 22  21.9% | batch:       150 of       686\t|\tloss: 1.32853\n",
      "Training Epoch 22  22.0% | batch:       151 of       686\t|\tloss: 1.31686\n",
      "Training Epoch 22  22.2% | batch:       152 of       686\t|\tloss: 1.53182\n",
      "Training Epoch 22  22.3% | batch:       153 of       686\t|\tloss: 1.29059\n",
      "Training Epoch 22  22.4% | batch:       154 of       686\t|\tloss: 1.18667\n",
      "Training Epoch 22  22.6% | batch:       155 of       686\t|\tloss: 1.32693\n",
      "Training Epoch 22  22.7% | batch:       156 of       686\t|\tloss: 1.34243\n",
      "Training Epoch 22  22.9% | batch:       157 of       686\t|\tloss: 0.942351\n",
      "Training Epoch 22  23.0% | batch:       158 of       686\t|\tloss: 1.33955\n",
      "Training Epoch 22  23.2% | batch:       159 of       686\t|\tloss: 1.15546\n",
      "Training Epoch 22  23.3% | batch:       160 of       686\t|\tloss: 1.80972\n",
      "Training Epoch 22  23.5% | batch:       161 of       686\t|\tloss: 1.09204\n",
      "Training Epoch 22  23.6% | batch:       162 of       686\t|\tloss: 1.31463\n",
      "Training Epoch 22  23.8% | batch:       163 of       686\t|\tloss: 2.16241\n",
      "Training Epoch 22  23.9% | batch:       164 of       686\t|\tloss: 1.29342\n",
      "Training Epoch 22  24.1% | batch:       165 of       686\t|\tloss: 1.60052\n",
      "Training Epoch 22  24.2% | batch:       166 of       686\t|\tloss: 1.31116\n",
      "Training Epoch 22  24.3% | batch:       167 of       686\t|\tloss: 1.28091\n",
      "Training Epoch 22  24.5% | batch:       168 of       686\t|\tloss: 1.52398\n",
      "Training Epoch 22  24.6% | batch:       169 of       686\t|\tloss: 1.13095\n",
      "Training Epoch 22  24.8% | batch:       170 of       686\t|\tloss: 1.15322\n",
      "Training Epoch 22  24.9% | batch:       171 of       686\t|\tloss: 1.44487\n",
      "Training Epoch 22  25.1% | batch:       172 of       686\t|\tloss: 1.35365\n",
      "Training Epoch 22  25.2% | batch:       173 of       686\t|\tloss: 1.35748\n",
      "Training Epoch 22  25.4% | batch:       174 of       686\t|\tloss: 1.42154\n",
      "Training Epoch 22  25.5% | batch:       175 of       686\t|\tloss: 1.04904\n",
      "Training Epoch 22  25.7% | batch:       176 of       686\t|\tloss: 1.09482\n",
      "Training Epoch 22  25.8% | batch:       177 of       686\t|\tloss: 1.40651\n",
      "Training Epoch 22  25.9% | batch:       178 of       686\t|\tloss: 1.16417\n",
      "Training Epoch 22  26.1% | batch:       179 of       686\t|\tloss: 1.25861\n",
      "Training Epoch 22  26.2% | batch:       180 of       686\t|\tloss: 1.10433\n",
      "Training Epoch 22  26.4% | batch:       181 of       686\t|\tloss: 1.11772\n",
      "Training Epoch 22  26.5% | batch:       182 of       686\t|\tloss: 1.35743\n",
      "Training Epoch 22  26.7% | batch:       183 of       686\t|\tloss: 1.23408\n",
      "Training Epoch 22  26.8% | batch:       184 of       686\t|\tloss: 1.21596\n",
      "Training Epoch 22  27.0% | batch:       185 of       686\t|\tloss: 1.04449\n",
      "Training Epoch 22  27.1% | batch:       186 of       686\t|\tloss: 1.26354\n",
      "Training Epoch 22  27.3% | batch:       187 of       686\t|\tloss: 1.8536\n",
      "Training Epoch 22  27.4% | batch:       188 of       686\t|\tloss: 1.07947\n",
      "Training Epoch 22  27.6% | batch:       189 of       686\t|\tloss: 1.32328\n",
      "Training Epoch 22  27.7% | batch:       190 of       686\t|\tloss: 1.44146\n",
      "Training Epoch 22  27.8% | batch:       191 of       686\t|\tloss: 1.43728\n",
      "Training Epoch 22  28.0% | batch:       192 of       686\t|\tloss: 1.31956\n",
      "Training Epoch 22  28.1% | batch:       193 of       686\t|\tloss: 1.47278\n",
      "Training Epoch 22  28.3% | batch:       194 of       686\t|\tloss: 1.55213\n",
      "Training Epoch 22  28.4% | batch:       195 of       686\t|\tloss: 1.47051\n",
      "Training Epoch 22  28.6% | batch:       196 of       686\t|\tloss: 1.61731\n",
      "Training Epoch 22  28.7% | batch:       197 of       686\t|\tloss: 1.47244\n",
      "Training Epoch 22  28.9% | batch:       198 of       686\t|\tloss: 1.40227\n",
      "Training Epoch 22  29.0% | batch:       199 of       686\t|\tloss: 1.27115\n",
      "Training Epoch 22  29.2% | batch:       200 of       686\t|\tloss: 1.24469\n",
      "Training Epoch 22  29.3% | batch:       201 of       686\t|\tloss: 1.35396\n",
      "Training Epoch 22  29.4% | batch:       202 of       686\t|\tloss: 1.0826\n",
      "Training Epoch 22  29.6% | batch:       203 of       686\t|\tloss: 1.67778\n",
      "Training Epoch 22  29.7% | batch:       204 of       686\t|\tloss: 1.21315\n",
      "Training Epoch 22  29.9% | batch:       205 of       686\t|\tloss: 1.31056\n",
      "Training Epoch 22  30.0% | batch:       206 of       686\t|\tloss: 1.55215\n",
      "Training Epoch 22  30.2% | batch:       207 of       686\t|\tloss: 1.48313\n",
      "Training Epoch 22  30.3% | batch:       208 of       686\t|\tloss: 1.54115\n",
      "Training Epoch 22  30.5% | batch:       209 of       686\t|\tloss: 1.27364\n",
      "Training Epoch 22  30.6% | batch:       210 of       686\t|\tloss: 1.15877\n",
      "Training Epoch 22  30.8% | batch:       211 of       686\t|\tloss: 1.09393\n",
      "Training Epoch 22  30.9% | batch:       212 of       686\t|\tloss: 1.07866\n",
      "Training Epoch 22  31.0% | batch:       213 of       686\t|\tloss: 1.07776\n",
      "Training Epoch 22  31.2% | batch:       214 of       686\t|\tloss: 1.49614\n",
      "Training Epoch 22  31.3% | batch:       215 of       686\t|\tloss: 1.543\n",
      "Training Epoch 22  31.5% | batch:       216 of       686\t|\tloss: 1.11572\n",
      "Training Epoch 22  31.6% | batch:       217 of       686\t|\tloss: 1.15204\n",
      "Training Epoch 22  31.8% | batch:       218 of       686\t|\tloss: 1.44625\n",
      "Training Epoch 22  31.9% | batch:       219 of       686\t|\tloss: 1.4165\n",
      "Training Epoch 22  32.1% | batch:       220 of       686\t|\tloss: 1.79551\n",
      "Training Epoch 22  32.2% | batch:       221 of       686\t|\tloss: 1.41285\n",
      "Training Epoch 22  32.4% | batch:       222 of       686\t|\tloss: 1.18056\n",
      "Training Epoch 22  32.5% | batch:       223 of       686\t|\tloss: 1.2349\n",
      "Training Epoch 22  32.7% | batch:       224 of       686\t|\tloss: 1.03267\n",
      "Training Epoch 22  32.8% | batch:       225 of       686\t|\tloss: 1.27033\n",
      "Training Epoch 22  32.9% | batch:       226 of       686\t|\tloss: 1.29947\n",
      "Training Epoch 22  33.1% | batch:       227 of       686\t|\tloss: 1.7823\n",
      "Training Epoch 22  33.2% | batch:       228 of       686\t|\tloss: 1.45214\n",
      "Training Epoch 22  33.4% | batch:       229 of       686\t|\tloss: 1.55874\n",
      "Training Epoch 22  33.5% | batch:       230 of       686\t|\tloss: 1.42257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 22  33.7% | batch:       231 of       686\t|\tloss: 1.29857\n",
      "Training Epoch 22  33.8% | batch:       232 of       686\t|\tloss: 1.16586\n",
      "Training Epoch 22  34.0% | batch:       233 of       686\t|\tloss: 1.18256\n",
      "Training Epoch 22  34.1% | batch:       234 of       686\t|\tloss: 1.11984\n",
      "Training Epoch 22  34.3% | batch:       235 of       686\t|\tloss: 1.39101\n",
      "Training Epoch 22  34.4% | batch:       236 of       686\t|\tloss: 1.18225\n",
      "Training Epoch 22  34.5% | batch:       237 of       686\t|\tloss: 1.25702\n",
      "Training Epoch 22  34.7% | batch:       238 of       686\t|\tloss: 1.75578\n",
      "Training Epoch 22  34.8% | batch:       239 of       686\t|\tloss: 1.51826\n",
      "Training Epoch 22  35.0% | batch:       240 of       686\t|\tloss: 1.31747\n",
      "Training Epoch 22  35.1% | batch:       241 of       686\t|\tloss: 1.31096\n",
      "Training Epoch 22  35.3% | batch:       242 of       686\t|\tloss: 1.41857\n",
      "Training Epoch 22  35.4% | batch:       243 of       686\t|\tloss: 1.46285\n",
      "Training Epoch 22  35.6% | batch:       244 of       686\t|\tloss: 1.26764\n",
      "Training Epoch 22  35.7% | batch:       245 of       686\t|\tloss: 1.54426\n",
      "Training Epoch 22  35.9% | batch:       246 of       686\t|\tloss: 1.42643\n",
      "Training Epoch 22  36.0% | batch:       247 of       686\t|\tloss: 1.00395\n",
      "Training Epoch 22  36.2% | batch:       248 of       686\t|\tloss: 1.47499\n",
      "Training Epoch 22  36.3% | batch:       249 of       686\t|\tloss: 1.58591\n",
      "Training Epoch 22  36.4% | batch:       250 of       686\t|\tloss: 0.90791\n",
      "Training Epoch 22  36.6% | batch:       251 of       686\t|\tloss: 1.09279\n",
      "Training Epoch 22  36.7% | batch:       252 of       686\t|\tloss: 1.32259\n",
      "Training Epoch 22  36.9% | batch:       253 of       686\t|\tloss: 1.30704\n",
      "Training Epoch 22  37.0% | batch:       254 of       686\t|\tloss: 1.91679\n",
      "Training Epoch 22  37.2% | batch:       255 of       686\t|\tloss: 1.29975\n",
      "Training Epoch 22  37.3% | batch:       256 of       686\t|\tloss: 0.95772\n",
      "Training Epoch 22  37.5% | batch:       257 of       686\t|\tloss: 1.54547\n",
      "Training Epoch 22  37.6% | batch:       258 of       686\t|\tloss: 1.59799\n",
      "Training Epoch 22  37.8% | batch:       259 of       686\t|\tloss: 1.18786\n",
      "Training Epoch 22  37.9% | batch:       260 of       686\t|\tloss: 1.46017\n",
      "Training Epoch 22  38.0% | batch:       261 of       686\t|\tloss: 1.26778\n",
      "Training Epoch 22  38.2% | batch:       262 of       686\t|\tloss: 1.21992\n",
      "Training Epoch 22  38.3% | batch:       263 of       686\t|\tloss: 1.25789\n",
      "Training Epoch 22  38.5% | batch:       264 of       686\t|\tloss: 1.19921\n",
      "Training Epoch 22  38.6% | batch:       265 of       686\t|\tloss: 1.35694\n",
      "Training Epoch 22  38.8% | batch:       266 of       686\t|\tloss: 1.04081\n",
      "Training Epoch 22  38.9% | batch:       267 of       686\t|\tloss: 1.22973\n",
      "Training Epoch 22  39.1% | batch:       268 of       686\t|\tloss: 1.32758\n",
      "Training Epoch 22  39.2% | batch:       269 of       686\t|\tloss: 1.4149\n",
      "Training Epoch 22  39.4% | batch:       270 of       686\t|\tloss: 1.15544\n",
      "Training Epoch 22  39.5% | batch:       271 of       686\t|\tloss: 1.32343\n",
      "Training Epoch 22  39.7% | batch:       272 of       686\t|\tloss: 1.87264\n",
      "Training Epoch 22  39.8% | batch:       273 of       686\t|\tloss: 1.38892\n",
      "Training Epoch 22  39.9% | batch:       274 of       686\t|\tloss: 1.64901\n",
      "Training Epoch 22  40.1% | batch:       275 of       686\t|\tloss: 1.17276\n",
      "Training Epoch 22  40.2% | batch:       276 of       686\t|\tloss: 1.55699\n",
      "Training Epoch 22  40.4% | batch:       277 of       686\t|\tloss: 1.23306\n",
      "Training Epoch 22  40.5% | batch:       278 of       686\t|\tloss: 1.52395\n",
      "Training Epoch 22  40.7% | batch:       279 of       686\t|\tloss: 1.26789\n",
      "Training Epoch 22  40.8% | batch:       280 of       686\t|\tloss: 1.96175\n",
      "Training Epoch 22  41.0% | batch:       281 of       686\t|\tloss: 1.25706\n",
      "Training Epoch 22  41.1% | batch:       282 of       686\t|\tloss: 1.23754\n",
      "Training Epoch 22  41.3% | batch:       283 of       686\t|\tloss: 1.02775\n",
      "Training Epoch 22  41.4% | batch:       284 of       686\t|\tloss: 1.45551\n",
      "Training Epoch 22  41.5% | batch:       285 of       686\t|\tloss: 1.18226\n",
      "Training Epoch 22  41.7% | batch:       286 of       686\t|\tloss: 1.15701\n",
      "Training Epoch 22  41.8% | batch:       287 of       686\t|\tloss: 0.985316\n",
      "Training Epoch 22  42.0% | batch:       288 of       686\t|\tloss: 1.25836\n",
      "Training Epoch 22  42.1% | batch:       289 of       686\t|\tloss: 1.66838\n",
      "Training Epoch 22  42.3% | batch:       290 of       686\t|\tloss: 1.75508\n",
      "Training Epoch 22  42.4% | batch:       291 of       686\t|\tloss: 1.32404\n",
      "Training Epoch 22  42.6% | batch:       292 of       686\t|\tloss: 1.56195\n",
      "Training Epoch 22  42.7% | batch:       293 of       686\t|\tloss: 1.22712\n",
      "Training Epoch 22  42.9% | batch:       294 of       686\t|\tloss: 1.1263\n",
      "Training Epoch 22  43.0% | batch:       295 of       686\t|\tloss: 1.26893\n",
      "Training Epoch 22  43.1% | batch:       296 of       686\t|\tloss: 1.17533\n",
      "Training Epoch 22  43.3% | batch:       297 of       686\t|\tloss: 1.15656\n",
      "Training Epoch 22  43.4% | batch:       298 of       686\t|\tloss: 1.28464\n",
      "Training Epoch 22  43.6% | batch:       299 of       686\t|\tloss: 1.33267\n",
      "Training Epoch 22  43.7% | batch:       300 of       686\t|\tloss: 1.30771\n",
      "Training Epoch 22  43.9% | batch:       301 of       686\t|\tloss: 1.37987\n",
      "Training Epoch 22  44.0% | batch:       302 of       686\t|\tloss: 1.20433\n",
      "Training Epoch 22  44.2% | batch:       303 of       686\t|\tloss: 1.87941\n",
      "Training Epoch 22  44.3% | batch:       304 of       686\t|\tloss: 1.77368\n",
      "Training Epoch 22  44.5% | batch:       305 of       686\t|\tloss: 1.23037\n",
      "Training Epoch 22  44.6% | batch:       306 of       686\t|\tloss: 1.17694\n",
      "Training Epoch 22  44.8% | batch:       307 of       686\t|\tloss: 1.30837\n",
      "Training Epoch 22  44.9% | batch:       308 of       686\t|\tloss: 1.50728\n",
      "Training Epoch 22  45.0% | batch:       309 of       686\t|\tloss: 1.07549\n",
      "Training Epoch 22  45.2% | batch:       310 of       686\t|\tloss: 1.03083\n",
      "Training Epoch 22  45.3% | batch:       311 of       686\t|\tloss: 1.19318\n",
      "Training Epoch 22  45.5% | batch:       312 of       686\t|\tloss: 1.3304\n",
      "Training Epoch 22  45.6% | batch:       313 of       686\t|\tloss: 1.53187\n",
      "Training Epoch 22  45.8% | batch:       314 of       686\t|\tloss: 1.72698\n",
      "Training Epoch 22  45.9% | batch:       315 of       686\t|\tloss: 1.46105\n",
      "Training Epoch 22  46.1% | batch:       316 of       686\t|\tloss: 1.41894\n",
      "Training Epoch 22  46.2% | batch:       317 of       686\t|\tloss: 1.23388\n",
      "Training Epoch 22  46.4% | batch:       318 of       686\t|\tloss: 1.32727\n",
      "Training Epoch 22  46.5% | batch:       319 of       686\t|\tloss: 1.67938\n",
      "Training Epoch 22  46.6% | batch:       320 of       686\t|\tloss: 1.30216\n",
      "Training Epoch 22  46.8% | batch:       321 of       686\t|\tloss: 1.49166\n",
      "Training Epoch 22  46.9% | batch:       322 of       686\t|\tloss: 1.39507\n",
      "Training Epoch 22  47.1% | batch:       323 of       686\t|\tloss: 1.10958\n",
      "Training Epoch 22  47.2% | batch:       324 of       686\t|\tloss: 1.33922\n",
      "Training Epoch 22  47.4% | batch:       325 of       686\t|\tloss: 1.39173\n",
      "Training Epoch 22  47.5% | batch:       326 of       686\t|\tloss: 1.2534\n",
      "Training Epoch 22  47.7% | batch:       327 of       686\t|\tloss: 1.34908\n",
      "Training Epoch 22  47.8% | batch:       328 of       686\t|\tloss: 1.22864\n",
      "Training Epoch 22  48.0% | batch:       329 of       686\t|\tloss: 1.15766\n",
      "Training Epoch 22  48.1% | batch:       330 of       686\t|\tloss: 1.09314\n",
      "Training Epoch 22  48.3% | batch:       331 of       686\t|\tloss: 1.33384\n",
      "Training Epoch 22  48.4% | batch:       332 of       686\t|\tloss: 1.6979\n",
      "Training Epoch 22  48.5% | batch:       333 of       686\t|\tloss: 0.942256\n",
      "Training Epoch 22  48.7% | batch:       334 of       686\t|\tloss: 1.32224\n",
      "Training Epoch 22  48.8% | batch:       335 of       686\t|\tloss: 1.30933\n",
      "Training Epoch 22  49.0% | batch:       336 of       686\t|\tloss: 1.01821\n",
      "Training Epoch 22  49.1% | batch:       337 of       686\t|\tloss: 1.07012\n",
      "Training Epoch 22  49.3% | batch:       338 of       686\t|\tloss: 1.54819\n",
      "Training Epoch 22  49.4% | batch:       339 of       686\t|\tloss: 1.21264\n",
      "Training Epoch 22  49.6% | batch:       340 of       686\t|\tloss: 1.34021\n",
      "Training Epoch 22  49.7% | batch:       341 of       686\t|\tloss: 1.59212\n",
      "Training Epoch 22  49.9% | batch:       342 of       686\t|\tloss: 1.52685\n",
      "Training Epoch 22  50.0% | batch:       343 of       686\t|\tloss: 1.49951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 22  50.1% | batch:       344 of       686\t|\tloss: 1.44678\n",
      "Training Epoch 22  50.3% | batch:       345 of       686\t|\tloss: 1.2762\n",
      "Training Epoch 22  50.4% | batch:       346 of       686\t|\tloss: 1.35955\n",
      "Training Epoch 22  50.6% | batch:       347 of       686\t|\tloss: 1.19316\n",
      "Training Epoch 22  50.7% | batch:       348 of       686\t|\tloss: 1.27439\n",
      "Training Epoch 22  50.9% | batch:       349 of       686\t|\tloss: 1.66508\n",
      "Training Epoch 22  51.0% | batch:       350 of       686\t|\tloss: 1.0946\n",
      "Training Epoch 22  51.2% | batch:       351 of       686\t|\tloss: 1.3035\n",
      "Training Epoch 22  51.3% | batch:       352 of       686\t|\tloss: 1.20412\n",
      "Training Epoch 22  51.5% | batch:       353 of       686\t|\tloss: 1.33037\n",
      "Training Epoch 22  51.6% | batch:       354 of       686\t|\tloss: 1.13217\n",
      "Training Epoch 22  51.7% | batch:       355 of       686\t|\tloss: 1.50631\n",
      "Training Epoch 22  51.9% | batch:       356 of       686\t|\tloss: 1.26628\n",
      "Training Epoch 22  52.0% | batch:       357 of       686\t|\tloss: 1.64074\n",
      "Training Epoch 22  52.2% | batch:       358 of       686\t|\tloss: 1.28514\n",
      "Training Epoch 22  52.3% | batch:       359 of       686\t|\tloss: 1.35788\n",
      "Training Epoch 22  52.5% | batch:       360 of       686\t|\tloss: 1.47966\n",
      "Training Epoch 22  52.6% | batch:       361 of       686\t|\tloss: 1.45858\n",
      "Training Epoch 22  52.8% | batch:       362 of       686\t|\tloss: 1.2878\n",
      "Training Epoch 22  52.9% | batch:       363 of       686\t|\tloss: 1.13069\n",
      "Training Epoch 22  53.1% | batch:       364 of       686\t|\tloss: 1.24661\n",
      "Training Epoch 22  53.2% | batch:       365 of       686\t|\tloss: 1.01172\n",
      "Training Epoch 22  53.4% | batch:       366 of       686\t|\tloss: 1.40901\n",
      "Training Epoch 22  53.5% | batch:       367 of       686\t|\tloss: 1.21095\n",
      "Training Epoch 22  53.6% | batch:       368 of       686\t|\tloss: 1.20819\n",
      "Training Epoch 22  53.8% | batch:       369 of       686\t|\tloss: 0.932582\n",
      "Training Epoch 22  53.9% | batch:       370 of       686\t|\tloss: 1.31466\n",
      "Training Epoch 22  54.1% | batch:       371 of       686\t|\tloss: 1.15981\n",
      "Training Epoch 22  54.2% | batch:       372 of       686\t|\tloss: 1.25221\n",
      "Training Epoch 22  54.4% | batch:       373 of       686\t|\tloss: 0.997602\n",
      "Training Epoch 22  54.5% | batch:       374 of       686\t|\tloss: 1.20499\n",
      "Training Epoch 22  54.7% | batch:       375 of       686\t|\tloss: 1.03317\n",
      "Training Epoch 22  54.8% | batch:       376 of       686\t|\tloss: 1.24999\n",
      "Training Epoch 22  55.0% | batch:       377 of       686\t|\tloss: 1.50671\n",
      "Training Epoch 22  55.1% | batch:       378 of       686\t|\tloss: 1.33141\n",
      "Training Epoch 22  55.2% | batch:       379 of       686\t|\tloss: 1.01764\n",
      "Training Epoch 22  55.4% | batch:       380 of       686\t|\tloss: 1.23635\n",
      "Training Epoch 22  55.5% | batch:       381 of       686\t|\tloss: 1.2133\n",
      "Training Epoch 22  55.7% | batch:       382 of       686\t|\tloss: 1.29507\n",
      "Training Epoch 22  55.8% | batch:       383 of       686\t|\tloss: 1.48918\n",
      "Training Epoch 22  56.0% | batch:       384 of       686\t|\tloss: 1.25157\n",
      "Training Epoch 22  56.1% | batch:       385 of       686\t|\tloss: 1.145\n",
      "Training Epoch 22  56.3% | batch:       386 of       686\t|\tloss: 1.25023\n",
      "Training Epoch 22  56.4% | batch:       387 of       686\t|\tloss: 1.18565\n",
      "Training Epoch 22  56.6% | batch:       388 of       686\t|\tloss: 1.28566\n",
      "Training Epoch 22  56.7% | batch:       389 of       686\t|\tloss: 1.17157\n",
      "Training Epoch 22  56.9% | batch:       390 of       686\t|\tloss: 1.47626\n",
      "Training Epoch 22  57.0% | batch:       391 of       686\t|\tloss: 1.50195\n",
      "Training Epoch 22  57.1% | batch:       392 of       686\t|\tloss: 1.83691\n",
      "Training Epoch 22  57.3% | batch:       393 of       686\t|\tloss: 1.23462\n",
      "Training Epoch 22  57.4% | batch:       394 of       686\t|\tloss: 1.66381\n",
      "Training Epoch 22  57.6% | batch:       395 of       686\t|\tloss: 1.24789\n",
      "Training Epoch 22  57.7% | batch:       396 of       686\t|\tloss: 1.45954\n",
      "Training Epoch 22  57.9% | batch:       397 of       686\t|\tloss: 1.32243\n",
      "Training Epoch 22  58.0% | batch:       398 of       686\t|\tloss: 1.47082\n",
      "Training Epoch 22  58.2% | batch:       399 of       686\t|\tloss: 1.42815\n",
      "Training Epoch 22  58.3% | batch:       400 of       686\t|\tloss: 1.14708\n",
      "Training Epoch 22  58.5% | batch:       401 of       686\t|\tloss: 1.34205\n",
      "Training Epoch 22  58.6% | batch:       402 of       686\t|\tloss: 1.60295\n",
      "Training Epoch 22  58.7% | batch:       403 of       686\t|\tloss: 1.10875\n",
      "Training Epoch 22  58.9% | batch:       404 of       686\t|\tloss: 1.58463\n",
      "Training Epoch 22  59.0% | batch:       405 of       686\t|\tloss: 1.36321\n",
      "Training Epoch 22  59.2% | batch:       406 of       686\t|\tloss: 1.37911\n",
      "Training Epoch 22  59.3% | batch:       407 of       686\t|\tloss: 1.73445\n",
      "Training Epoch 22  59.5% | batch:       408 of       686\t|\tloss: 1.19885\n",
      "Training Epoch 22  59.6% | batch:       409 of       686\t|\tloss: 1.10864\n",
      "Training Epoch 22  59.8% | batch:       410 of       686\t|\tloss: 0.892092\n",
      "Training Epoch 22  59.9% | batch:       411 of       686\t|\tloss: 1.2237\n",
      "Training Epoch 22  60.1% | batch:       412 of       686\t|\tloss: 1.20817\n",
      "Training Epoch 22  60.2% | batch:       413 of       686\t|\tloss: 1.21928\n",
      "Training Epoch 22  60.3% | batch:       414 of       686\t|\tloss: 1.28105\n",
      "Training Epoch 22  60.5% | batch:       415 of       686\t|\tloss: 1.23512\n",
      "Training Epoch 22  60.6% | batch:       416 of       686\t|\tloss: 1.62976\n",
      "Training Epoch 22  60.8% | batch:       417 of       686\t|\tloss: 1.12476\n",
      "Training Epoch 22  60.9% | batch:       418 of       686\t|\tloss: 0.800144\n",
      "Training Epoch 22  61.1% | batch:       419 of       686\t|\tloss: 1.20137\n",
      "Training Epoch 22  61.2% | batch:       420 of       686\t|\tloss: 1.98153\n",
      "Training Epoch 22  61.4% | batch:       421 of       686\t|\tloss: 1.41598\n",
      "Training Epoch 22  61.5% | batch:       422 of       686\t|\tloss: 1.37769\n",
      "Training Epoch 22  61.7% | batch:       423 of       686\t|\tloss: 0.975895\n",
      "Training Epoch 22  61.8% | batch:       424 of       686\t|\tloss: 1.21811\n",
      "Training Epoch 22  62.0% | batch:       425 of       686\t|\tloss: 1.23869\n",
      "Training Epoch 22  62.1% | batch:       426 of       686\t|\tloss: 1.17459\n",
      "Training Epoch 22  62.2% | batch:       427 of       686\t|\tloss: 1.50406\n",
      "Training Epoch 22  62.4% | batch:       428 of       686\t|\tloss: 1.24972\n",
      "Training Epoch 22  62.5% | batch:       429 of       686\t|\tloss: 1.54661\n",
      "Training Epoch 22  62.7% | batch:       430 of       686\t|\tloss: 1.16222\n",
      "Training Epoch 22  62.8% | batch:       431 of       686\t|\tloss: 1.06305\n",
      "Training Epoch 22  63.0% | batch:       432 of       686\t|\tloss: 1.50713\n",
      "Training Epoch 22  63.1% | batch:       433 of       686\t|\tloss: 1.16701\n",
      "Training Epoch 22  63.3% | batch:       434 of       686\t|\tloss: 1.09134\n",
      "Training Epoch 22  63.4% | batch:       435 of       686\t|\tloss: 1.07106\n",
      "Training Epoch 22  63.6% | batch:       436 of       686\t|\tloss: 1.23153\n",
      "Training Epoch 22  63.7% | batch:       437 of       686\t|\tloss: 1.13316\n",
      "Training Epoch 22  63.8% | batch:       438 of       686\t|\tloss: 1.39436\n",
      "Training Epoch 22  64.0% | batch:       439 of       686\t|\tloss: 1.19269\n",
      "Training Epoch 22  64.1% | batch:       440 of       686\t|\tloss: 1.24375\n",
      "Training Epoch 22  64.3% | batch:       441 of       686\t|\tloss: 1.27567\n",
      "Training Epoch 22  64.4% | batch:       442 of       686\t|\tloss: 1.38007\n",
      "Training Epoch 22  64.6% | batch:       443 of       686\t|\tloss: 1.33728\n",
      "Training Epoch 22  64.7% | batch:       444 of       686\t|\tloss: 1.37142\n",
      "Training Epoch 22  64.9% | batch:       445 of       686\t|\tloss: 1.45652\n",
      "Training Epoch 22  65.0% | batch:       446 of       686\t|\tloss: 1.18726\n",
      "Training Epoch 22  65.2% | batch:       447 of       686\t|\tloss: 1.56483\n",
      "Training Epoch 22  65.3% | batch:       448 of       686\t|\tloss: 1.27764\n",
      "Training Epoch 22  65.5% | batch:       449 of       686\t|\tloss: 1.06788\n",
      "Training Epoch 22  65.6% | batch:       450 of       686\t|\tloss: 1.18946\n",
      "Training Epoch 22  65.7% | batch:       451 of       686\t|\tloss: 1.04816\n",
      "Training Epoch 22  65.9% | batch:       452 of       686\t|\tloss: 1.34629\n",
      "Training Epoch 22  66.0% | batch:       453 of       686\t|\tloss: 1.4517\n",
      "Training Epoch 22  66.2% | batch:       454 of       686\t|\tloss: 0.810195\n",
      "Training Epoch 22  66.3% | batch:       455 of       686\t|\tloss: 1.26494\n",
      "Training Epoch 22  66.5% | batch:       456 of       686\t|\tloss: 1.1647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 22  66.6% | batch:       457 of       686\t|\tloss: 1.08311\n",
      "Training Epoch 22  66.8% | batch:       458 of       686\t|\tloss: 1.3122\n",
      "Training Epoch 22  66.9% | batch:       459 of       686\t|\tloss: 1.02964\n",
      "Training Epoch 22  67.1% | batch:       460 of       686\t|\tloss: 1.27461\n",
      "Training Epoch 22  67.2% | batch:       461 of       686\t|\tloss: 1.06752\n",
      "Training Epoch 22  67.3% | batch:       462 of       686\t|\tloss: 1.18866\n",
      "Training Epoch 22  67.5% | batch:       463 of       686\t|\tloss: 1.31626\n",
      "Training Epoch 22  67.6% | batch:       464 of       686\t|\tloss: 1.1275\n",
      "Training Epoch 22  67.8% | batch:       465 of       686\t|\tloss: 1.55202\n",
      "Training Epoch 22  67.9% | batch:       466 of       686\t|\tloss: 1.3248\n",
      "Training Epoch 22  68.1% | batch:       467 of       686\t|\tloss: 1.57686\n",
      "Training Epoch 22  68.2% | batch:       468 of       686\t|\tloss: 1.30646\n",
      "Training Epoch 22  68.4% | batch:       469 of       686\t|\tloss: 1.34569\n",
      "Training Epoch 22  68.5% | batch:       470 of       686\t|\tloss: 1.49992\n",
      "Training Epoch 22  68.7% | batch:       471 of       686\t|\tloss: 1.10758\n",
      "Training Epoch 22  68.8% | batch:       472 of       686\t|\tloss: 1.10528\n",
      "Training Epoch 22  69.0% | batch:       473 of       686\t|\tloss: 1.28764\n",
      "Training Epoch 22  69.1% | batch:       474 of       686\t|\tloss: 1.46029\n",
      "Training Epoch 22  69.2% | batch:       475 of       686\t|\tloss: 0.915318\n",
      "Training Epoch 22  69.4% | batch:       476 of       686\t|\tloss: 1.17945\n",
      "Training Epoch 22  69.5% | batch:       477 of       686\t|\tloss: 1.35544\n",
      "Training Epoch 22  69.7% | batch:       478 of       686\t|\tloss: 1.21051\n",
      "Training Epoch 22  69.8% | batch:       479 of       686\t|\tloss: 1.2497\n",
      "Training Epoch 22  70.0% | batch:       480 of       686\t|\tloss: 1.94466\n",
      "Training Epoch 22  70.1% | batch:       481 of       686\t|\tloss: 1.18308\n",
      "Training Epoch 22  70.3% | batch:       482 of       686\t|\tloss: 1.19967\n",
      "Training Epoch 22  70.4% | batch:       483 of       686\t|\tloss: 1.23086\n",
      "Training Epoch 22  70.6% | batch:       484 of       686\t|\tloss: 1.09545\n",
      "Training Epoch 22  70.7% | batch:       485 of       686\t|\tloss: 0.942116\n",
      "Training Epoch 22  70.8% | batch:       486 of       686\t|\tloss: 1.01081\n",
      "Training Epoch 22  71.0% | batch:       487 of       686\t|\tloss: 1.3565\n",
      "Training Epoch 22  71.1% | batch:       488 of       686\t|\tloss: 1.06964\n",
      "Training Epoch 22  71.3% | batch:       489 of       686\t|\tloss: 1.51421\n",
      "Training Epoch 22  71.4% | batch:       490 of       686\t|\tloss: 1.19179\n",
      "Training Epoch 22  71.6% | batch:       491 of       686\t|\tloss: 1.14827\n",
      "Training Epoch 22  71.7% | batch:       492 of       686\t|\tloss: 1.14859\n",
      "Training Epoch 22  71.9% | batch:       493 of       686\t|\tloss: 1.1137\n",
      "Training Epoch 22  72.0% | batch:       494 of       686\t|\tloss: 1.59404\n",
      "Training Epoch 22  72.2% | batch:       495 of       686\t|\tloss: 1.08227\n",
      "Training Epoch 22  72.3% | batch:       496 of       686\t|\tloss: 0.993429\n",
      "Training Epoch 22  72.4% | batch:       497 of       686\t|\tloss: 1.02683\n",
      "Training Epoch 22  72.6% | batch:       498 of       686\t|\tloss: 1.09038\n",
      "Training Epoch 22  72.7% | batch:       499 of       686\t|\tloss: 2.13993\n",
      "Training Epoch 22  72.9% | batch:       500 of       686\t|\tloss: 1.1829\n",
      "Training Epoch 22  73.0% | batch:       501 of       686\t|\tloss: 1.39401\n",
      "Training Epoch 22  73.2% | batch:       502 of       686\t|\tloss: 1.24826\n",
      "Training Epoch 22  73.3% | batch:       503 of       686\t|\tloss: 1.27092\n",
      "Training Epoch 22  73.5% | batch:       504 of       686\t|\tloss: 1.72138\n",
      "Training Epoch 22  73.6% | batch:       505 of       686\t|\tloss: 1.24612\n",
      "Training Epoch 22  73.8% | batch:       506 of       686\t|\tloss: 1.27962\n",
      "Training Epoch 22  73.9% | batch:       507 of       686\t|\tloss: 1.19162\n",
      "Training Epoch 22  74.1% | batch:       508 of       686\t|\tloss: 1.43019\n",
      "Training Epoch 22  74.2% | batch:       509 of       686\t|\tloss: 1.33574\n",
      "Training Epoch 22  74.3% | batch:       510 of       686\t|\tloss: 1.10139\n",
      "Training Epoch 22  74.5% | batch:       511 of       686\t|\tloss: 1.13881\n",
      "Training Epoch 22  74.6% | batch:       512 of       686\t|\tloss: 1.2128\n",
      "Training Epoch 22  74.8% | batch:       513 of       686\t|\tloss: 1.16047\n",
      "Training Epoch 22  74.9% | batch:       514 of       686\t|\tloss: 1.39811\n",
      "Training Epoch 22  75.1% | batch:       515 of       686\t|\tloss: 1.54251\n",
      "Training Epoch 22  75.2% | batch:       516 of       686\t|\tloss: 1.20243\n",
      "Training Epoch 22  75.4% | batch:       517 of       686\t|\tloss: 1.58351\n",
      "Training Epoch 22  75.5% | batch:       518 of       686\t|\tloss: 1.37938\n",
      "Training Epoch 22  75.7% | batch:       519 of       686\t|\tloss: 1.27865\n",
      "Training Epoch 22  75.8% | batch:       520 of       686\t|\tloss: 1.22781\n",
      "Training Epoch 22  75.9% | batch:       521 of       686\t|\tloss: 1.35053\n",
      "Training Epoch 22  76.1% | batch:       522 of       686\t|\tloss: 1.3208\n",
      "Training Epoch 22  76.2% | batch:       523 of       686\t|\tloss: 1.52278\n",
      "Training Epoch 22  76.4% | batch:       524 of       686\t|\tloss: 1.31507\n",
      "Training Epoch 22  76.5% | batch:       525 of       686\t|\tloss: 1.40944\n",
      "Training Epoch 22  76.7% | batch:       526 of       686\t|\tloss: 1.06267\n",
      "Training Epoch 22  76.8% | batch:       527 of       686\t|\tloss: 1.18429\n",
      "Training Epoch 22  77.0% | batch:       528 of       686\t|\tloss: 1.52486\n",
      "Training Epoch 22  77.1% | batch:       529 of       686\t|\tloss: 1.4962\n",
      "Training Epoch 22  77.3% | batch:       530 of       686\t|\tloss: 1.31769\n",
      "Training Epoch 22  77.4% | batch:       531 of       686\t|\tloss: 1.18375\n",
      "Training Epoch 22  77.6% | batch:       532 of       686\t|\tloss: 1.36472\n",
      "Training Epoch 22  77.7% | batch:       533 of       686\t|\tloss: 1.57684\n",
      "Training Epoch 22  77.8% | batch:       534 of       686\t|\tloss: 1.456\n",
      "Training Epoch 22  78.0% | batch:       535 of       686\t|\tloss: 1.14573\n",
      "Training Epoch 22  78.1% | batch:       536 of       686\t|\tloss: 0.999595\n",
      "Training Epoch 22  78.3% | batch:       537 of       686\t|\tloss: 1.04856\n",
      "Training Epoch 22  78.4% | batch:       538 of       686\t|\tloss: 1.00897\n",
      "Training Epoch 22  78.6% | batch:       539 of       686\t|\tloss: 1.43071\n",
      "Training Epoch 22  78.7% | batch:       540 of       686\t|\tloss: 1.63582\n",
      "Training Epoch 22  78.9% | batch:       541 of       686\t|\tloss: 1.40757\n",
      "Training Epoch 22  79.0% | batch:       542 of       686\t|\tloss: 1.53748\n",
      "Training Epoch 22  79.2% | batch:       543 of       686\t|\tloss: 1.42874\n",
      "Training Epoch 22  79.3% | batch:       544 of       686\t|\tloss: 1.36103\n",
      "Training Epoch 22  79.4% | batch:       545 of       686\t|\tloss: 0.882174\n",
      "Training Epoch 22  79.6% | batch:       546 of       686\t|\tloss: 1.51702\n",
      "Training Epoch 22  79.7% | batch:       547 of       686\t|\tloss: 1.28768\n",
      "Training Epoch 22  79.9% | batch:       548 of       686\t|\tloss: 1.29148\n",
      "Training Epoch 22  80.0% | batch:       549 of       686\t|\tloss: 1.04109\n",
      "Training Epoch 22  80.2% | batch:       550 of       686\t|\tloss: 1.21934\n",
      "Training Epoch 22  80.3% | batch:       551 of       686\t|\tloss: 1.58222\n",
      "Training Epoch 22  80.5% | batch:       552 of       686\t|\tloss: 1.59848\n",
      "Training Epoch 22  80.6% | batch:       553 of       686\t|\tloss: 1.21123\n",
      "Training Epoch 22  80.8% | batch:       554 of       686\t|\tloss: 1.31873\n",
      "Training Epoch 22  80.9% | batch:       555 of       686\t|\tloss: 1.26382\n",
      "Training Epoch 22  81.0% | batch:       556 of       686\t|\tloss: 1.25857\n",
      "Training Epoch 22  81.2% | batch:       557 of       686\t|\tloss: 1.18151\n",
      "Training Epoch 22  81.3% | batch:       558 of       686\t|\tloss: 1.28322\n",
      "Training Epoch 22  81.5% | batch:       559 of       686\t|\tloss: 1.21104\n",
      "Training Epoch 22  81.6% | batch:       560 of       686\t|\tloss: 0.919114\n",
      "Training Epoch 22  81.8% | batch:       561 of       686\t|\tloss: 1.46616\n",
      "Training Epoch 22  81.9% | batch:       562 of       686\t|\tloss: 1.0962\n",
      "Training Epoch 22  82.1% | batch:       563 of       686\t|\tloss: 1.06764\n",
      "Training Epoch 22  82.2% | batch:       564 of       686\t|\tloss: 1.09337\n",
      "Training Epoch 22  82.4% | batch:       565 of       686\t|\tloss: 1.19199\n",
      "Training Epoch 22  82.5% | batch:       566 of       686\t|\tloss: 1.26187\n",
      "Training Epoch 22  82.7% | batch:       567 of       686\t|\tloss: 1.29506\n",
      "Training Epoch 22  82.8% | batch:       568 of       686\t|\tloss: 1.39979\n",
      "Training Epoch 22  82.9% | batch:       569 of       686\t|\tloss: 1.16705\n",
      "Training Epoch 22  83.1% | batch:       570 of       686\t|\tloss: 1.23487\n",
      "Training Epoch 22  83.2% | batch:       571 of       686\t|\tloss: 1.43884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 22  83.4% | batch:       572 of       686\t|\tloss: 1.22864\n",
      "Training Epoch 22  83.5% | batch:       573 of       686\t|\tloss: 1.46954\n",
      "Training Epoch 22  83.7% | batch:       574 of       686\t|\tloss: 1.05334\n",
      "Training Epoch 22  83.8% | batch:       575 of       686\t|\tloss: 1.13607\n",
      "Training Epoch 22  84.0% | batch:       576 of       686\t|\tloss: 1.20655\n",
      "Training Epoch 22  84.1% | batch:       577 of       686\t|\tloss: 1.14259\n",
      "Training Epoch 22  84.3% | batch:       578 of       686\t|\tloss: 1.02654\n",
      "Training Epoch 22  84.4% | batch:       579 of       686\t|\tloss: 1.46465\n",
      "Training Epoch 22  84.5% | batch:       580 of       686\t|\tloss: 1.35419\n",
      "Training Epoch 22  84.7% | batch:       581 of       686\t|\tloss: 1.21201\n",
      "Training Epoch 22  84.8% | batch:       582 of       686\t|\tloss: 1.03151\n",
      "Training Epoch 22  85.0% | batch:       583 of       686\t|\tloss: 1.26888\n",
      "Training Epoch 22  85.1% | batch:       584 of       686\t|\tloss: 1.35272\n",
      "Training Epoch 22  85.3% | batch:       585 of       686\t|\tloss: 1.15618\n",
      "Training Epoch 22  85.4% | batch:       586 of       686\t|\tloss: 1.15103\n",
      "Training Epoch 22  85.6% | batch:       587 of       686\t|\tloss: 1.21274\n",
      "Training Epoch 22  85.7% | batch:       588 of       686\t|\tloss: 0.92405\n",
      "Training Epoch 22  85.9% | batch:       589 of       686\t|\tloss: 1.58152\n",
      "Training Epoch 22  86.0% | batch:       590 of       686\t|\tloss: 1.10929\n",
      "Training Epoch 22  86.2% | batch:       591 of       686\t|\tloss: 1.29979\n",
      "Training Epoch 22  86.3% | batch:       592 of       686\t|\tloss: 1.33678\n",
      "Training Epoch 22  86.4% | batch:       593 of       686\t|\tloss: 1.27082\n",
      "Training Epoch 22  86.6% | batch:       594 of       686\t|\tloss: 1.07906\n",
      "Training Epoch 22  86.7% | batch:       595 of       686\t|\tloss: 1.01585\n",
      "Training Epoch 22  86.9% | batch:       596 of       686\t|\tloss: 1.31306\n",
      "Training Epoch 22  87.0% | batch:       597 of       686\t|\tloss: 1.07358\n",
      "Training Epoch 22  87.2% | batch:       598 of       686\t|\tloss: 1.37809\n",
      "Training Epoch 22  87.3% | batch:       599 of       686\t|\tloss: 1.13122\n",
      "Training Epoch 22  87.5% | batch:       600 of       686\t|\tloss: 1.29133\n",
      "Training Epoch 22  87.6% | batch:       601 of       686\t|\tloss: 1.50862\n",
      "Training Epoch 22  87.8% | batch:       602 of       686\t|\tloss: 1.27926\n",
      "Training Epoch 22  87.9% | batch:       603 of       686\t|\tloss: 0.949789\n",
      "Training Epoch 22  88.0% | batch:       604 of       686\t|\tloss: 1.07865\n",
      "Training Epoch 22  88.2% | batch:       605 of       686\t|\tloss: 1.22489\n",
      "Training Epoch 22  88.3% | batch:       606 of       686\t|\tloss: 1.0805\n",
      "Training Epoch 22  88.5% | batch:       607 of       686\t|\tloss: 1.33193\n",
      "Training Epoch 22  88.6% | batch:       608 of       686\t|\tloss: 1.26642\n",
      "Training Epoch 22  88.8% | batch:       609 of       686\t|\tloss: 1.06641\n",
      "Training Epoch 22  88.9% | batch:       610 of       686\t|\tloss: 1.24213\n",
      "Training Epoch 22  89.1% | batch:       611 of       686\t|\tloss: 1.09279\n",
      "Training Epoch 22  89.2% | batch:       612 of       686\t|\tloss: 1.36927\n",
      "Training Epoch 22  89.4% | batch:       613 of       686\t|\tloss: 1.23947\n",
      "Training Epoch 22  89.5% | batch:       614 of       686\t|\tloss: 1.54882\n",
      "Training Epoch 22  89.7% | batch:       615 of       686\t|\tloss: 1.3047\n",
      "Training Epoch 22  89.8% | batch:       616 of       686\t|\tloss: 1.03977\n",
      "Training Epoch 22  89.9% | batch:       617 of       686\t|\tloss: 1.41218\n",
      "Training Epoch 22  90.1% | batch:       618 of       686\t|\tloss: 1.16594\n",
      "Training Epoch 22  90.2% | batch:       619 of       686\t|\tloss: 1.15431\n",
      "Training Epoch 22  90.4% | batch:       620 of       686\t|\tloss: 0.857332\n",
      "Training Epoch 22  90.5% | batch:       621 of       686\t|\tloss: 1.23608\n",
      "Training Epoch 22  90.7% | batch:       622 of       686\t|\tloss: 1.52113\n",
      "Training Epoch 22  90.8% | batch:       623 of       686\t|\tloss: 1.30395\n",
      "Training Epoch 22  91.0% | batch:       624 of       686\t|\tloss: 1.40412\n",
      "Training Epoch 22  91.1% | batch:       625 of       686\t|\tloss: 1.15655\n",
      "Training Epoch 22  91.3% | batch:       626 of       686\t|\tloss: 1.58686\n",
      "Training Epoch 22  91.4% | batch:       627 of       686\t|\tloss: 1.17874\n",
      "Training Epoch 22  91.5% | batch:       628 of       686\t|\tloss: 1.60985\n",
      "Training Epoch 22  91.7% | batch:       629 of       686\t|\tloss: 1.35902\n",
      "Training Epoch 22  91.8% | batch:       630 of       686\t|\tloss: 1.07019\n",
      "Training Epoch 22  92.0% | batch:       631 of       686\t|\tloss: 1.069\n",
      "Training Epoch 22  92.1% | batch:       632 of       686\t|\tloss: 1.10233\n",
      "Training Epoch 22  92.3% | batch:       633 of       686\t|\tloss: 0.943244\n",
      "Training Epoch 22  92.4% | batch:       634 of       686\t|\tloss: 1.33968\n",
      "Training Epoch 22  92.6% | batch:       635 of       686\t|\tloss: 1.10776\n",
      "Training Epoch 22  92.7% | batch:       636 of       686\t|\tloss: 1.15602\n",
      "Training Epoch 22  92.9% | batch:       637 of       686\t|\tloss: 1.0038\n",
      "Training Epoch 22  93.0% | batch:       638 of       686\t|\tloss: 1.04145\n",
      "Training Epoch 22  93.1% | batch:       639 of       686\t|\tloss: 1.00363\n",
      "Training Epoch 22  93.3% | batch:       640 of       686\t|\tloss: 1.34908\n",
      "Training Epoch 22  93.4% | batch:       641 of       686\t|\tloss: 1.08773\n",
      "Training Epoch 22  93.6% | batch:       642 of       686\t|\tloss: 0.955322\n",
      "Training Epoch 22  93.7% | batch:       643 of       686\t|\tloss: 1.32067\n",
      "Training Epoch 22  93.9% | batch:       644 of       686\t|\tloss: 1.52623\n",
      "Training Epoch 22  94.0% | batch:       645 of       686\t|\tloss: 1.48062\n",
      "Training Epoch 22  94.2% | batch:       646 of       686\t|\tloss: 1.19886\n",
      "Training Epoch 22  94.3% | batch:       647 of       686\t|\tloss: 1.0779\n",
      "Training Epoch 22  94.5% | batch:       648 of       686\t|\tloss: 1.00682\n",
      "Training Epoch 22  94.6% | batch:       649 of       686\t|\tloss: 1.57482\n",
      "Training Epoch 22  94.8% | batch:       650 of       686\t|\tloss: 1.78849\n",
      "Training Epoch 22  94.9% | batch:       651 of       686\t|\tloss: 1.31435\n",
      "Training Epoch 22  95.0% | batch:       652 of       686\t|\tloss: 1.23445\n",
      "Training Epoch 22  95.2% | batch:       653 of       686\t|\tloss: 1.27328\n",
      "Training Epoch 22  95.3% | batch:       654 of       686\t|\tloss: 1.37745\n",
      "Training Epoch 22  95.5% | batch:       655 of       686\t|\tloss: 1.17754\n",
      "Training Epoch 22  95.6% | batch:       656 of       686\t|\tloss: 1.2889\n",
      "Training Epoch 22  95.8% | batch:       657 of       686\t|\tloss: 1.12401\n",
      "Training Epoch 22  95.9% | batch:       658 of       686\t|\tloss: 1.18509\n",
      "Training Epoch 22  96.1% | batch:       659 of       686\t|\tloss: 1.53123\n",
      "Training Epoch 22  96.2% | batch:       660 of       686\t|\tloss: 1.45017\n",
      "Training Epoch 22  96.4% | batch:       661 of       686\t|\tloss: 0.963687\n",
      "Training Epoch 22  96.5% | batch:       662 of       686\t|\tloss: 1.17803\n",
      "Training Epoch 22  96.6% | batch:       663 of       686\t|\tloss: 1.74163\n",
      "Training Epoch 22  96.8% | batch:       664 of       686\t|\tloss: 1.46571\n",
      "Training Epoch 22  96.9% | batch:       665 of       686\t|\tloss: 1.22687\n",
      "Training Epoch 22  97.1% | batch:       666 of       686\t|\tloss: 1.27068\n",
      "Training Epoch 22  97.2% | batch:       667 of       686\t|\tloss: 1.38344\n",
      "Training Epoch 22  97.4% | batch:       668 of       686\t|\tloss: 1.38334\n",
      "Training Epoch 22  97.5% | batch:       669 of       686\t|\tloss: 1.51753\n",
      "Training Epoch 22  97.7% | batch:       670 of       686\t|\tloss: 1.13366\n",
      "Training Epoch 22  97.8% | batch:       671 of       686\t|\tloss: 1.10371\n",
      "Training Epoch 22  98.0% | batch:       672 of       686\t|\tloss: 1.45408\n",
      "Training Epoch 22  98.1% | batch:       673 of       686\t|\tloss: 1.11703\n",
      "Training Epoch 22  98.3% | batch:       674 of       686\t|\tloss: 1.15588\n",
      "Training Epoch 22  98.4% | batch:       675 of       686\t|\tloss: 1.0747\n",
      "Training Epoch 22  98.5% | batch:       676 of       686\t|\tloss: 1.35276\n",
      "Training Epoch 22  98.7% | batch:       677 of       686\t|\tloss: 1.64471\n",
      "Training Epoch 22  98.8% | batch:       678 of       686\t|\tloss: 1.66437\n",
      "Training Epoch 22  99.0% | batch:       679 of       686\t|\tloss: 1.32566\n",
      "Training Epoch 22  99.1% | batch:       680 of       686\t|\tloss: 1.50736\n",
      "Training Epoch 22  99.3% | batch:       681 of       686\t|\tloss: 1.5715\n",
      "Training Epoch 22  99.4% | batch:       682 of       686\t|\tloss: 1.08734\n",
      "Training Epoch 22  99.6% | batch:       683 of       686\t|\tloss: 1.18171\n",
      "Training Epoch 22  99.7% | batch:       684 of       686\t|\tloss: 1.30588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:10:02,288 | INFO : Epoch 22 Training Summary: epoch: 22.000000 | loss: 1.316505 | \n",
      "2023-05-25 22:10:02,289 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 24.55124592781067 seconds\n",
      "\n",
      "2023-05-25 22:10:02,291 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.88818802616813 seconds\n",
      "2023-05-25 22:10:02,291 | INFO : Avg batch train. time: 0.034822431525026426 seconds\n",
      "2023-05-25 22:10:02,292 | INFO : Avg sample train. time: 0.0002724007985195066 seconds\n",
      "2023-05-25 22:10:02,292 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 22  99.9% | batch:       685 of       686\t|\tloss: 2.88628\n",
      "\n",
      "Evaluating Epoch 22   0.0% | batch:         0 of       172\t|\tloss: 1.43477\n",
      "Evaluating Epoch 22   0.6% | batch:         1 of       172\t|\tloss: 1.60139\n",
      "Evaluating Epoch 22   1.2% | batch:         2 of       172\t|\tloss: 0.849563\n",
      "Evaluating Epoch 22   1.7% | batch:         3 of       172\t|\tloss: 2.78007\n",
      "Evaluating Epoch 22   2.3% | batch:         4 of       172\t|\tloss: 1.27333\n",
      "Evaluating Epoch 22   2.9% | batch:         5 of       172\t|\tloss: 1.33327\n",
      "Evaluating Epoch 22   3.5% | batch:         6 of       172\t|\tloss: 1.27536\n",
      "Evaluating Epoch 22   4.1% | batch:         7 of       172\t|\tloss: 3.33022\n",
      "Evaluating Epoch 22   4.7% | batch:         8 of       172\t|\tloss: 0.709289\n",
      "Evaluating Epoch 22   5.2% | batch:         9 of       172\t|\tloss: 1.87988\n",
      "Evaluating Epoch 22   5.8% | batch:        10 of       172\t|\tloss: 1.46267\n",
      "Evaluating Epoch 22   6.4% | batch:        11 of       172\t|\tloss: 1.29014\n",
      "Evaluating Epoch 22   7.0% | batch:        12 of       172\t|\tloss: 1.48052\n",
      "Evaluating Epoch 22   7.6% | batch:        13 of       172\t|\tloss: 1.36357\n",
      "Evaluating Epoch 22   8.1% | batch:        14 of       172\t|\tloss: 1.86477\n",
      "Evaluating Epoch 22   8.7% | batch:        15 of       172\t|\tloss: 1.37399\n",
      "Evaluating Epoch 22   9.3% | batch:        16 of       172\t|\tloss: 2.29118\n",
      "Evaluating Epoch 22   9.9% | batch:        17 of       172\t|\tloss: 0.882459\n",
      "Evaluating Epoch 22  10.5% | batch:        18 of       172\t|\tloss: 24.4899\n",
      "Evaluating Epoch 22  11.0% | batch:        19 of       172\t|\tloss: 1.77677\n",
      "Evaluating Epoch 22  11.6% | batch:        20 of       172\t|\tloss: 1.38091\n",
      "Evaluating Epoch 22  12.2% | batch:        21 of       172\t|\tloss: 0.404292\n",
      "Evaluating Epoch 22  12.8% | batch:        22 of       172\t|\tloss: 6.57128\n",
      "Evaluating Epoch 22  13.4% | batch:        23 of       172\t|\tloss: 4.2787\n",
      "Evaluating Epoch 22  14.0% | batch:        24 of       172\t|\tloss: 0.922128\n",
      "Evaluating Epoch 22  14.5% | batch:        25 of       172\t|\tloss: 1.59282\n",
      "Evaluating Epoch 22  15.1% | batch:        26 of       172\t|\tloss: 10.1168\n",
      "Evaluating Epoch 22  15.7% | batch:        27 of       172\t|\tloss: 21.7148\n",
      "Evaluating Epoch 22  16.3% | batch:        28 of       172\t|\tloss: 0.318754\n",
      "Evaluating Epoch 22  16.9% | batch:        29 of       172\t|\tloss: 0.981209\n",
      "Evaluating Epoch 22  17.4% | batch:        30 of       172\t|\tloss: 0.788499\n",
      "Evaluating Epoch 22  18.0% | batch:        31 of       172\t|\tloss: 0.985279\n",
      "Evaluating Epoch 22  18.6% | batch:        32 of       172\t|\tloss: 0.672118\n",
      "Evaluating Epoch 22  19.2% | batch:        33 of       172\t|\tloss: 0.659331\n",
      "Evaluating Epoch 22  19.8% | batch:        34 of       172\t|\tloss: 0.3398\n",
      "Evaluating Epoch 22  20.3% | batch:        35 of       172\t|\tloss: 0.783745\n",
      "Evaluating Epoch 22  20.9% | batch:        36 of       172\t|\tloss: 3.53243\n",
      "Evaluating Epoch 22  21.5% | batch:        37 of       172\t|\tloss: 5.23893\n",
      "Evaluating Epoch 22  22.1% | batch:        38 of       172\t|\tloss: 4.86077\n",
      "Evaluating Epoch 22  22.7% | batch:        39 of       172\t|\tloss: 11.1917\n",
      "Evaluating Epoch 22  23.3% | batch:        40 of       172\t|\tloss: 0.707764\n",
      "Evaluating Epoch 22  23.8% | batch:        41 of       172\t|\tloss: 0.507613\n",
      "Evaluating Epoch 22  24.4% | batch:        42 of       172\t|\tloss: 0.681217\n",
      "Evaluating Epoch 22  25.0% | batch:        43 of       172\t|\tloss: 26.1829\n",
      "Evaluating Epoch 22  25.6% | batch:        44 of       172\t|\tloss: 1.46155\n",
      "Evaluating Epoch 22  26.2% | batch:        45 of       172\t|\tloss: 0.563943\n",
      "Evaluating Epoch 22  26.7% | batch:        46 of       172\t|\tloss: 0.203274\n",
      "Evaluating Epoch 22  27.3% | batch:        47 of       172\t|\tloss: 1.11972\n",
      "Evaluating Epoch 22  27.9% | batch:        48 of       172\t|\tloss: 0.51021\n",
      "Evaluating Epoch 22  28.5% | batch:        49 of       172\t|\tloss: 1.36762\n",
      "Evaluating Epoch 22  29.1% | batch:        50 of       172\t|\tloss: 0.654294\n",
      "Evaluating Epoch 22  29.7% | batch:        51 of       172\t|\tloss: 0.810601\n",
      "Evaluating Epoch 22  30.2% | batch:        52 of       172\t|\tloss: 0.705113\n",
      "Evaluating Epoch 22  30.8% | batch:        53 of       172\t|\tloss: 2.55104\n",
      "Evaluating Epoch 22  31.4% | batch:        54 of       172\t|\tloss: 0.969383\n",
      "Evaluating Epoch 22  32.0% | batch:        55 of       172\t|\tloss: 0.344018\n",
      "Evaluating Epoch 22  32.6% | batch:        56 of       172\t|\tloss: 2.99233\n",
      "Evaluating Epoch 22  33.1% | batch:        57 of       172\t|\tloss: 0.287883\n",
      "Evaluating Epoch 22  33.7% | batch:        58 of       172\t|\tloss: 2.13292\n",
      "Evaluating Epoch 22  34.3% | batch:        59 of       172\t|\tloss: 1.11128\n",
      "Evaluating Epoch 22  34.9% | batch:        60 of       172\t|\tloss: 1.15403\n",
      "Evaluating Epoch 22  35.5% | batch:        61 of       172\t|\tloss: 1.36556\n",
      "Evaluating Epoch 22  36.0% | batch:        62 of       172\t|\tloss: 0.751683\n",
      "Evaluating Epoch 22  36.6% | batch:        63 of       172\t|\tloss: 2.85353\n",
      "Evaluating Epoch 22  37.2% | batch:        64 of       172\t|\tloss: 0.721794\n",
      "Evaluating Epoch 22  37.8% | batch:        65 of       172\t|\tloss: 2.15683\n",
      "Evaluating Epoch 22  38.4% | batch:        66 of       172\t|\tloss: 1.4059\n",
      "Evaluating Epoch 22  39.0% | batch:        67 of       172\t|\tloss: 0.443787\n",
      "Evaluating Epoch 22  39.5% | batch:        68 of       172\t|\tloss: 2.21223\n",
      "Evaluating Epoch 22  40.1% | batch:        69 of       172\t|\tloss: 0.589735\n",
      "Evaluating Epoch 22  40.7% | batch:        70 of       172\t|\tloss: 1.62797\n",
      "Evaluating Epoch 22  41.3% | batch:        71 of       172\t|\tloss: 1.59475\n",
      "Evaluating Epoch 22  41.9% | batch:        72 of       172\t|\tloss: 0.668439\n",
      "Evaluating Epoch 22  42.4% | batch:        73 of       172\t|\tloss: 2.40497\n",
      "Evaluating Epoch 22  43.0% | batch:        74 of       172\t|\tloss: 0.325537\n",
      "Evaluating Epoch 22  43.6% | batch:        75 of       172\t|\tloss: 0.329628\n",
      "Evaluating Epoch 22  44.2% | batch:        76 of       172\t|\tloss: 0.393961\n",
      "Evaluating Epoch 22  44.8% | batch:        77 of       172\t|\tloss: 0.48927\n",
      "Evaluating Epoch 22  45.3% | batch:        78 of       172\t|\tloss: 0.459463\n",
      "Evaluating Epoch 22  45.9% | batch:        79 of       172\t|\tloss: 0.334098\n",
      "Evaluating Epoch 22  46.5% | batch:        80 of       172\t|\tloss: 0.282013\n",
      "Evaluating Epoch 22  47.1% | batch:        81 of       172\t|\tloss: 0.418857\n",
      "Evaluating Epoch 22  47.7% | batch:        82 of       172\t|\tloss: 0.287415\n",
      "Evaluating Epoch 22  48.3% | batch:        83 of       172\t|\tloss: 0.650869\n",
      "Evaluating Epoch 22  48.8% | batch:        84 of       172\t|\tloss: 0.530562\n",
      "Evaluating Epoch 22  49.4% | batch:        85 of       172\t|\tloss: 0.931121\n",
      "Evaluating Epoch 22  50.0% | batch:        86 of       172\t|\tloss: 0.616226\n",
      "Evaluating Epoch 22  50.6% | batch:        87 of       172\t|\tloss: 0.835081\n",
      "Evaluating Epoch 22  51.2% | batch:        88 of       172\t|\tloss: 0.396002\n",
      "Evaluating Epoch 22  51.7% | batch:        89 of       172\t|\tloss: 0.782634\n",
      "Evaluating Epoch 22  52.3% | batch:        90 of       172\t|\tloss: 0.824777\n",
      "Evaluating Epoch 22  52.9% | batch:        91 of       172\t|\tloss: 0.202564\n",
      "Evaluating Epoch 22  53.5% | batch:        92 of       172\t|\tloss: 0.611951\n",
      "Evaluating Epoch 22  54.1% | batch:        93 of       172\t|\tloss: 0.860802\n",
      "Evaluating Epoch 22  54.7% | batch:        94 of       172\t|\tloss: 0.613373\n",
      "Evaluating Epoch 22  55.2% | batch:        95 of       172\t|\tloss: 0.547284\n",
      "Evaluating Epoch 22  55.8% | batch:        96 of       172\t|\tloss: 0.7211\n",
      "Evaluating Epoch 22  56.4% | batch:        97 of       172\t|\tloss: 0.698686\n",
      "Evaluating Epoch 22  57.0% | batch:        98 of       172\t|\tloss: 0.403823\n",
      "Evaluating Epoch 22  57.6% | batch:        99 of       172\t|\tloss: 0.663224\n",
      "Evaluating Epoch 22  58.1% | batch:       100 of       172\t|\tloss: 0.681535\n",
      "Evaluating Epoch 22  58.7% | batch:       101 of       172\t|\tloss: 0.407913\n",
      "Evaluating Epoch 22  59.3% | batch:       102 of       172\t|\tloss: 0.616627\n",
      "Evaluating Epoch 22  59.9% | batch:       103 of       172\t|\tloss: 0.898044\n",
      "Evaluating Epoch 22  60.5% | batch:       104 of       172\t|\tloss: 0.657029\n",
      "Evaluating Epoch 22  61.0% | batch:       105 of       172\t|\tloss: 0.253551\n",
      "Evaluating Epoch 22  61.6% | batch:       106 of       172\t|\tloss: 0.687461\n",
      "Evaluating Epoch 22  62.2% | batch:       107 of       172\t|\tloss: 1.06799\n",
      "Evaluating Epoch 22  62.8% | batch:       108 of       172\t|\tloss: 0.488694\n",
      "Evaluating Epoch 22  63.4% | batch:       109 of       172\t|\tloss: 0.555947\n",
      "Evaluating Epoch 22  64.0% | batch:       110 of       172\t|\tloss: 0.837472\n",
      "Evaluating Epoch 22  64.5% | batch:       111 of       172\t|\tloss: 0.728834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 22  65.1% | batch:       112 of       172\t|\tloss: 0.528822\n",
      "Evaluating Epoch 22  65.7% | batch:       113 of       172\t|\tloss: 0.94839\n",
      "Evaluating Epoch 22  66.3% | batch:       114 of       172\t|\tloss: 0.637173\n",
      "Evaluating Epoch 22  66.9% | batch:       115 of       172\t|\tloss: 0.608522\n",
      "Evaluating Epoch 22  67.4% | batch:       116 of       172\t|\tloss: 0.261437\n",
      "Evaluating Epoch 22  68.0% | batch:       117 of       172\t|\tloss: 0.549718\n",
      "Evaluating Epoch 22  68.6% | batch:       118 of       172\t|\tloss: 0.445791\n",
      "Evaluating Epoch 22  69.2% | batch:       119 of       172\t|\tloss: 0.239159\n",
      "Evaluating Epoch 22  69.8% | batch:       120 of       172\t|\tloss: 0.44072\n",
      "Evaluating Epoch 22  70.3% | batch:       121 of       172\t|\tloss: 0.693265\n",
      "Evaluating Epoch 22  70.9% | batch:       122 of       172\t|\tloss: 0.650675\n",
      "Evaluating Epoch 22  71.5% | batch:       123 of       172\t|\tloss: 0.616002\n",
      "Evaluating Epoch 22  72.1% | batch:       124 of       172\t|\tloss: 1.11957\n",
      "Evaluating Epoch 22  72.7% | batch:       125 of       172\t|\tloss: 0.416401\n",
      "Evaluating Epoch 22  73.3% | batch:       126 of       172\t|\tloss: 0.293182\n",
      "Evaluating Epoch 22  73.8% | batch:       127 of       172\t|\tloss: 0.573898\n",
      "Evaluating Epoch 22  74.4% | batch:       128 of       172\t|\tloss: 0.60243\n",
      "Evaluating Epoch 22  75.0% | batch:       129 of       172\t|\tloss: 0.445859\n",
      "Evaluating Epoch 22  75.6% | batch:       130 of       172\t|\tloss: 0.750751\n",
      "Evaluating Epoch 22  76.2% | batch:       131 of       172\t|\tloss: 0.559368\n",
      "Evaluating Epoch 22  76.7% | batch:       132 of       172\t|\tloss: 0.283002\n",
      "Evaluating Epoch 22  77.3% | batch:       133 of       172\t|\tloss: 0.329201\n",
      "Evaluating Epoch 22  77.9% | batch:       134 of       172\t|\tloss: 0.249655\n",
      "Evaluating Epoch 22  78.5% | batch:       135 of       172\t|\tloss: 0.315717\n",
      "Evaluating Epoch 22  79.1% | batch:       136 of       172\t|\tloss: 0.296103\n",
      "Evaluating Epoch 22  79.7% | batch:       137 of       172\t|\tloss: 0.156018\n",
      "Evaluating Epoch 22  80.2% | batch:       138 of       172\t|\tloss: 0.398395\n",
      "Evaluating Epoch 22  80.8% | batch:       139 of       172\t|\tloss: 0.320398\n",
      "Evaluating Epoch 22  81.4% | batch:       140 of       172\t|\tloss: 0.244386\n",
      "Evaluating Epoch 22  82.0% | batch:       141 of       172\t|\tloss: 0.144981\n",
      "Evaluating Epoch 22  82.6% | batch:       142 of       172\t|\tloss: 0.267456\n",
      "Evaluating Epoch 22  83.1% | batch:       143 of       172\t|\tloss: 0.261303\n",
      "Evaluating Epoch 22  83.7% | batch:       144 of       172\t|\tloss: 0.20799\n",
      "Evaluating Epoch 22  84.3% | batch:       145 of       172\t|\tloss: 0.333568\n",
      "Evaluating Epoch 22  84.9% | batch:       146 of       172\t|\tloss: 0.331471\n",
      "Evaluating Epoch 22  85.5% | batch:       147 of       172\t|\tloss: 0.274433\n",
      "Evaluating Epoch 22  86.0% | batch:       148 of       172\t|\tloss: 0.277972\n",
      "Evaluating Epoch 22  86.6% | batch:       149 of       172\t|\tloss: 0.247279\n",
      "Evaluating Epoch 22  87.2% | batch:       150 of       172\t|\tloss: 0.381262\n",
      "Evaluating Epoch 22  87.8% | batch:       151 of       172\t|\tloss: 0.308562\n",
      "Evaluating Epoch 22  88.4% | batch:       152 of       172\t|\tloss: 0.253686\n",
      "Evaluating Epoch 22  89.0% | batch:       153 of       172\t|\tloss: 0.332458\n",
      "Evaluating Epoch 22  89.5% | batch:       154 of       172\t|\tloss: 0.156515\n",
      "Evaluating Epoch 22  90.1% | batch:       155 of       172\t|\tloss: 0.171994\n",
      "Evaluating Epoch 22  90.7% | batch:       156 of       172\t|\tloss: 0.438109\n",
      "Evaluating Epoch 22  91.3% | batch:       157 of       172\t|\tloss: 0.387598\n",
      "Evaluating Epoch 22  91.9% | batch:       158 of       172\t|\tloss: 0.413818\n",
      "Evaluating Epoch 22  92.4% | batch:       159 of       172\t|\tloss: 0.568614\n",
      "Evaluating Epoch 22  93.0% | batch:       160 of       172\t|\tloss: 0.354477\n",
      "Evaluating Epoch 22  93.6% | batch:       161 of       172\t|\tloss: 1.11643\n",
      "Evaluating Epoch 22  94.2% | batch:       162 of       172\t|\tloss: 0.24162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:10:06,825 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 4.532223463058472 seconds\n",
      "\n",
      "2023-05-25 22:10:06,826 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.022795366204304 seconds\n",
      "2023-05-25 22:10:06,826 | INFO : Avg batch val. time: 0.023388345152350602 seconds\n",
      "2023-05-25 22:10:06,827 | INFO : Avg sample val. time: 0.00018321243185336357 seconds\n",
      "2023-05-25 22:10:06,827 | INFO : Epoch 22 Validation Summary: epoch: 22.000000 | loss: 1.424465 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 22  94.8% | batch:       163 of       172\t|\tloss: 0.304691\n",
      "Evaluating Epoch 22  95.3% | batch:       164 of       172\t|\tloss: 0.483671\n",
      "Evaluating Epoch 22  95.9% | batch:       165 of       172\t|\tloss: 0.186704\n",
      "Evaluating Epoch 22  96.5% | batch:       166 of       172\t|\tloss: 0.274332\n",
      "Evaluating Epoch 22  97.1% | batch:       167 of       172\t|\tloss: 0.448899\n",
      "Evaluating Epoch 22  97.7% | batch:       168 of       172\t|\tloss: 0.212871\n",
      "Evaluating Epoch 22  98.3% | batch:       169 of       172\t|\tloss: 0.419932\n",
      "Evaluating Epoch 22  98.8% | batch:       170 of       172\t|\tloss: 0.396593\n",
      "Evaluating Epoch 22  99.4% | batch:       171 of       172\t|\tloss: 0.358807\n",
      "\n",
      "Training Epoch 23   0.0% | batch:         0 of       686\t|\tloss: 1.76373\n",
      "Training Epoch 23   0.1% | batch:         1 of       686\t|\tloss: 1.01727\n",
      "Training Epoch 23   0.3% | batch:         2 of       686\t|\tloss: 1.04118\n",
      "Training Epoch 23   0.4% | batch:         3 of       686\t|\tloss: 0.960833\n",
      "Training Epoch 23   0.6% | batch:         4 of       686\t|\tloss: 1.25662\n",
      "Training Epoch 23   0.7% | batch:         5 of       686\t|\tloss: 1.05989\n",
      "Training Epoch 23   0.9% | batch:         6 of       686\t|\tloss: 1.11014\n",
      "Training Epoch 23   1.0% | batch:         7 of       686\t|\tloss: 1.57007\n",
      "Training Epoch 23   1.2% | batch:         8 of       686\t|\tloss: 1.23702\n",
      "Training Epoch 23   1.3% | batch:         9 of       686\t|\tloss: 1.42227\n",
      "Training Epoch 23   1.5% | batch:        10 of       686\t|\tloss: 1.19387\n",
      "Training Epoch 23   1.6% | batch:        11 of       686\t|\tloss: 1.31287\n",
      "Training Epoch 23   1.7% | batch:        12 of       686\t|\tloss: 1.26823\n",
      "Training Epoch 23   1.9% | batch:        13 of       686\t|\tloss: 1.33147\n",
      "Training Epoch 23   2.0% | batch:        14 of       686\t|\tloss: 1.45961\n",
      "Training Epoch 23   2.2% | batch:        15 of       686\t|\tloss: 1.2936\n",
      "Training Epoch 23   2.3% | batch:        16 of       686\t|\tloss: 1.44887\n",
      "Training Epoch 23   2.5% | batch:        17 of       686\t|\tloss: 1.34584\n",
      "Training Epoch 23   2.6% | batch:        18 of       686\t|\tloss: 1.32874\n",
      "Training Epoch 23   2.8% | batch:        19 of       686\t|\tloss: 1.30242\n",
      "Training Epoch 23   2.9% | batch:        20 of       686\t|\tloss: 1.38215\n",
      "Training Epoch 23   3.1% | batch:        21 of       686\t|\tloss: 1.19891\n",
      "Training Epoch 23   3.2% | batch:        22 of       686\t|\tloss: 1.69831\n",
      "Training Epoch 23   3.4% | batch:        23 of       686\t|\tloss: 1.06526\n",
      "Training Epoch 23   3.5% | batch:        24 of       686\t|\tloss: 1.2718\n",
      "Training Epoch 23   3.6% | batch:        25 of       686\t|\tloss: 1.37911\n",
      "Training Epoch 23   3.8% | batch:        26 of       686\t|\tloss: 1.50686\n",
      "Training Epoch 23   3.9% | batch:        27 of       686\t|\tloss: 1.15874\n",
      "Training Epoch 23   4.1% | batch:        28 of       686\t|\tloss: 1.13889\n",
      "Training Epoch 23   4.2% | batch:        29 of       686\t|\tloss: 0.975592\n",
      "Training Epoch 23   4.4% | batch:        30 of       686\t|\tloss: 1.04303\n",
      "Training Epoch 23   4.5% | batch:        31 of       686\t|\tloss: 1.44629\n",
      "Training Epoch 23   4.7% | batch:        32 of       686\t|\tloss: 1.32152\n",
      "Training Epoch 23   4.8% | batch:        33 of       686\t|\tloss: 1.17329\n",
      "Training Epoch 23   5.0% | batch:        34 of       686\t|\tloss: 1.40709\n",
      "Training Epoch 23   5.1% | batch:        35 of       686\t|\tloss: 0.888495\n",
      "Training Epoch 23   5.2% | batch:        36 of       686\t|\tloss: 1.61039\n",
      "Training Epoch 23   5.4% | batch:        37 of       686\t|\tloss: 1.25746\n",
      "Training Epoch 23   5.5% | batch:        38 of       686\t|\tloss: 1.7368\n",
      "Training Epoch 23   5.7% | batch:        39 of       686\t|\tloss: 1.04037\n",
      "Training Epoch 23   5.8% | batch:        40 of       686\t|\tloss: 1.193\n",
      "Training Epoch 23   6.0% | batch:        41 of       686\t|\tloss: 1.12608\n",
      "Training Epoch 23   6.1% | batch:        42 of       686\t|\tloss: 1.1895\n",
      "Training Epoch 23   6.3% | batch:        43 of       686\t|\tloss: 1.47844\n",
      "Training Epoch 23   6.4% | batch:        44 of       686\t|\tloss: 1.07919\n",
      "Training Epoch 23   6.6% | batch:        45 of       686\t|\tloss: 1.54287\n",
      "Training Epoch 23   6.7% | batch:        46 of       686\t|\tloss: 1.18865\n",
      "Training Epoch 23   6.9% | batch:        47 of       686\t|\tloss: 1.29689\n",
      "Training Epoch 23   7.0% | batch:        48 of       686\t|\tloss: 1.18667\n",
      "Training Epoch 23   7.1% | batch:        49 of       686\t|\tloss: 1.01747\n",
      "Training Epoch 23   7.3% | batch:        50 of       686\t|\tloss: 1.10674\n",
      "Training Epoch 23   7.4% | batch:        51 of       686\t|\tloss: 1.17018\n",
      "Training Epoch 23   7.6% | batch:        52 of       686\t|\tloss: 0.887045\n",
      "Training Epoch 23   7.7% | batch:        53 of       686\t|\tloss: 1.00605\n",
      "Training Epoch 23   7.9% | batch:        54 of       686\t|\tloss: 1.33338\n",
      "Training Epoch 23   8.0% | batch:        55 of       686\t|\tloss: 1.42438\n",
      "Training Epoch 23   8.2% | batch:        56 of       686\t|\tloss: 1.6386\n",
      "Training Epoch 23   8.3% | batch:        57 of       686\t|\tloss: 1.30274\n",
      "Training Epoch 23   8.5% | batch:        58 of       686\t|\tloss: 1.05526\n",
      "Training Epoch 23   8.6% | batch:        59 of       686\t|\tloss: 1.43925\n",
      "Training Epoch 23   8.7% | batch:        60 of       686\t|\tloss: 1.05857\n",
      "Training Epoch 23   8.9% | batch:        61 of       686\t|\tloss: 1.49202\n",
      "Training Epoch 23   9.0% | batch:        62 of       686\t|\tloss: 1.38311\n",
      "Training Epoch 23   9.2% | batch:        63 of       686\t|\tloss: 1.35356\n",
      "Training Epoch 23   9.3% | batch:        64 of       686\t|\tloss: 1.2088\n",
      "Training Epoch 23   9.5% | batch:        65 of       686\t|\tloss: 1.27599\n",
      "Training Epoch 23   9.6% | batch:        66 of       686\t|\tloss: 1.15482\n",
      "Training Epoch 23   9.8% | batch:        67 of       686\t|\tloss: 1.34134\n",
      "Training Epoch 23   9.9% | batch:        68 of       686\t|\tloss: 1.11441\n",
      "Training Epoch 23  10.1% | batch:        69 of       686\t|\tloss: 1.4897\n",
      "Training Epoch 23  10.2% | batch:        70 of       686\t|\tloss: 1.09127\n",
      "Training Epoch 23  10.3% | batch:        71 of       686\t|\tloss: 1.11851\n",
      "Training Epoch 23  10.5% | batch:        72 of       686\t|\tloss: 1.07413\n",
      "Training Epoch 23  10.6% | batch:        73 of       686\t|\tloss: 1.08438\n",
      "Training Epoch 23  10.8% | batch:        74 of       686\t|\tloss: 1.17186\n",
      "Training Epoch 23  10.9% | batch:        75 of       686\t|\tloss: 1.18295\n",
      "Training Epoch 23  11.1% | batch:        76 of       686\t|\tloss: 1.43058\n",
      "Training Epoch 23  11.2% | batch:        77 of       686\t|\tloss: 1.06428\n",
      "Training Epoch 23  11.4% | batch:        78 of       686\t|\tloss: 1.23773\n",
      "Training Epoch 23  11.5% | batch:        79 of       686\t|\tloss: 1.23468\n",
      "Training Epoch 23  11.7% | batch:        80 of       686\t|\tloss: 1.39727\n",
      "Training Epoch 23  11.8% | batch:        81 of       686\t|\tloss: 1.21608\n",
      "Training Epoch 23  12.0% | batch:        82 of       686\t|\tloss: 1.08836\n",
      "Training Epoch 23  12.1% | batch:        83 of       686\t|\tloss: 0.955116\n",
      "Training Epoch 23  12.2% | batch:        84 of       686\t|\tloss: 1.03794\n",
      "Training Epoch 23  12.4% | batch:        85 of       686\t|\tloss: 1.07296\n",
      "Training Epoch 23  12.5% | batch:        86 of       686\t|\tloss: 1.131\n",
      "Training Epoch 23  12.7% | batch:        87 of       686\t|\tloss: 0.952464\n",
      "Training Epoch 23  12.8% | batch:        88 of       686\t|\tloss: 1.14766\n",
      "Training Epoch 23  13.0% | batch:        89 of       686\t|\tloss: 1.29772\n",
      "Training Epoch 23  13.1% | batch:        90 of       686\t|\tloss: 1.0544\n",
      "Training Epoch 23  13.3% | batch:        91 of       686\t|\tloss: 1.20557\n",
      "Training Epoch 23  13.4% | batch:        92 of       686\t|\tloss: 1.19096\n",
      "Training Epoch 23  13.6% | batch:        93 of       686\t|\tloss: 1.33369\n",
      "Training Epoch 23  13.7% | batch:        94 of       686\t|\tloss: 1.29395\n",
      "Training Epoch 23  13.8% | batch:        95 of       686\t|\tloss: 1.21518\n",
      "Training Epoch 23  14.0% | batch:        96 of       686\t|\tloss: 1.29389\n",
      "Training Epoch 23  14.1% | batch:        97 of       686\t|\tloss: 1.47645\n",
      "Training Epoch 23  14.3% | batch:        98 of       686\t|\tloss: 1.07629\n",
      "Training Epoch 23  14.4% | batch:        99 of       686\t|\tloss: 1.23262\n",
      "Training Epoch 23  14.6% | batch:       100 of       686\t|\tloss: 1.23758\n",
      "Training Epoch 23  14.7% | batch:       101 of       686\t|\tloss: 1.22275\n",
      "Training Epoch 23  14.9% | batch:       102 of       686\t|\tloss: 1.37195\n",
      "Training Epoch 23  15.0% | batch:       103 of       686\t|\tloss: 1.65901\n",
      "Training Epoch 23  15.2% | batch:       104 of       686\t|\tloss: 1.3496\n",
      "Training Epoch 23  15.3% | batch:       105 of       686\t|\tloss: 1.15788\n",
      "Training Epoch 23  15.5% | batch:       106 of       686\t|\tloss: 1.25087\n",
      "Training Epoch 23  15.6% | batch:       107 of       686\t|\tloss: 1.25837\n",
      "Training Epoch 23  15.7% | batch:       108 of       686\t|\tloss: 0.933472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 23  15.9% | batch:       109 of       686\t|\tloss: 0.991497\n",
      "Training Epoch 23  16.0% | batch:       110 of       686\t|\tloss: 1.23226\n",
      "Training Epoch 23  16.2% | batch:       111 of       686\t|\tloss: 1.64629\n",
      "Training Epoch 23  16.3% | batch:       112 of       686\t|\tloss: 1.53989\n",
      "Training Epoch 23  16.5% | batch:       113 of       686\t|\tloss: 1.28177\n",
      "Training Epoch 23  16.6% | batch:       114 of       686\t|\tloss: 1.07043\n",
      "Training Epoch 23  16.8% | batch:       115 of       686\t|\tloss: 1.26882\n",
      "Training Epoch 23  16.9% | batch:       116 of       686\t|\tloss: 1.5448\n",
      "Training Epoch 23  17.1% | batch:       117 of       686\t|\tloss: 1.35965\n",
      "Training Epoch 23  17.2% | batch:       118 of       686\t|\tloss: 1.17699\n",
      "Training Epoch 23  17.3% | batch:       119 of       686\t|\tloss: 1.10497\n",
      "Training Epoch 23  17.5% | batch:       120 of       686\t|\tloss: 1.14279\n",
      "Training Epoch 23  17.6% | batch:       121 of       686\t|\tloss: 0.965499\n",
      "Training Epoch 23  17.8% | batch:       122 of       686\t|\tloss: 1.38834\n",
      "Training Epoch 23  17.9% | batch:       123 of       686\t|\tloss: 1.04723\n",
      "Training Epoch 23  18.1% | batch:       124 of       686\t|\tloss: 1.21284\n",
      "Training Epoch 23  18.2% | batch:       125 of       686\t|\tloss: 1.55278\n",
      "Training Epoch 23  18.4% | batch:       126 of       686\t|\tloss: 1.34222\n",
      "Training Epoch 23  18.5% | batch:       127 of       686\t|\tloss: 1.00734\n",
      "Training Epoch 23  18.7% | batch:       128 of       686\t|\tloss: 1.2472\n",
      "Training Epoch 23  18.8% | batch:       129 of       686\t|\tloss: 1.89869\n",
      "Training Epoch 23  19.0% | batch:       130 of       686\t|\tloss: 1.48274\n",
      "Training Epoch 23  19.1% | batch:       131 of       686\t|\tloss: 1.28297\n",
      "Training Epoch 23  19.2% | batch:       132 of       686\t|\tloss: 1.15078\n",
      "Training Epoch 23  19.4% | batch:       133 of       686\t|\tloss: 1.28804\n",
      "Training Epoch 23  19.5% | batch:       134 of       686\t|\tloss: 0.955937\n",
      "Training Epoch 23  19.7% | batch:       135 of       686\t|\tloss: 1.26588\n",
      "Training Epoch 23  19.8% | batch:       136 of       686\t|\tloss: 1.22222\n",
      "Training Epoch 23  20.0% | batch:       137 of       686\t|\tloss: 1.30655\n",
      "Training Epoch 23  20.1% | batch:       138 of       686\t|\tloss: 0.938663\n",
      "Training Epoch 23  20.3% | batch:       139 of       686\t|\tloss: 1.26774\n",
      "Training Epoch 23  20.4% | batch:       140 of       686\t|\tloss: 1.01746\n",
      "Training Epoch 23  20.6% | batch:       141 of       686\t|\tloss: 1.19527\n",
      "Training Epoch 23  20.7% | batch:       142 of       686\t|\tloss: 1.25205\n",
      "Training Epoch 23  20.8% | batch:       143 of       686\t|\tloss: 1.30878\n",
      "Training Epoch 23  21.0% | batch:       144 of       686\t|\tloss: 1.37278\n",
      "Training Epoch 23  21.1% | batch:       145 of       686\t|\tloss: 1.10847\n",
      "Training Epoch 23  21.3% | batch:       146 of       686\t|\tloss: 1.75157\n",
      "Training Epoch 23  21.4% | batch:       147 of       686\t|\tloss: 1.41345\n",
      "Training Epoch 23  21.6% | batch:       148 of       686\t|\tloss: 1.13306\n",
      "Training Epoch 23  21.7% | batch:       149 of       686\t|\tloss: 0.913415\n",
      "Training Epoch 23  21.9% | batch:       150 of       686\t|\tloss: 1.1379\n",
      "Training Epoch 23  22.0% | batch:       151 of       686\t|\tloss: 0.948163\n",
      "Training Epoch 23  22.2% | batch:       152 of       686\t|\tloss: 1.51595\n",
      "Training Epoch 23  22.3% | batch:       153 of       686\t|\tloss: 1.11796\n",
      "Training Epoch 23  22.4% | batch:       154 of       686\t|\tloss: 1.21885\n",
      "Training Epoch 23  22.6% | batch:       155 of       686\t|\tloss: 1.31903\n",
      "Training Epoch 23  22.7% | batch:       156 of       686\t|\tloss: 0.935576\n",
      "Training Epoch 23  22.9% | batch:       157 of       686\t|\tloss: 0.978511\n",
      "Training Epoch 23  23.0% | batch:       158 of       686\t|\tloss: 0.99525\n",
      "Training Epoch 23  23.2% | batch:       159 of       686\t|\tloss: 1.15301\n",
      "Training Epoch 23  23.3% | batch:       160 of       686\t|\tloss: 1.1614\n",
      "Training Epoch 23  23.5% | batch:       161 of       686\t|\tloss: 1.12054\n",
      "Training Epoch 23  23.6% | batch:       162 of       686\t|\tloss: 0.934708\n",
      "Training Epoch 23  23.8% | batch:       163 of       686\t|\tloss: 1.16845\n",
      "Training Epoch 23  23.9% | batch:       164 of       686\t|\tloss: 1.25753\n",
      "Training Epoch 23  24.1% | batch:       165 of       686\t|\tloss: 1.4338\n",
      "Training Epoch 23  24.2% | batch:       166 of       686\t|\tloss: 0.873421\n",
      "Training Epoch 23  24.3% | batch:       167 of       686\t|\tloss: 1.17838\n",
      "Training Epoch 23  24.5% | batch:       168 of       686\t|\tloss: 1.30283\n",
      "Training Epoch 23  24.6% | batch:       169 of       686\t|\tloss: 1.31596\n",
      "Training Epoch 23  24.8% | batch:       170 of       686\t|\tloss: 1.30841\n",
      "Training Epoch 23  24.9% | batch:       171 of       686\t|\tloss: 1.25365\n",
      "Training Epoch 23  25.1% | batch:       172 of       686\t|\tloss: 1.20192\n",
      "Training Epoch 23  25.2% | batch:       173 of       686\t|\tloss: 1.76862\n",
      "Training Epoch 23  25.4% | batch:       174 of       686\t|\tloss: 0.875257\n",
      "Training Epoch 23  25.5% | batch:       175 of       686\t|\tloss: 1.33393\n",
      "Training Epoch 23  25.7% | batch:       176 of       686\t|\tloss: 1.1842\n",
      "Training Epoch 23  25.8% | batch:       177 of       686\t|\tloss: 0.862633\n",
      "Training Epoch 23  25.9% | batch:       178 of       686\t|\tloss: 1.16407\n",
      "Training Epoch 23  26.1% | batch:       179 of       686\t|\tloss: 1.34699\n",
      "Training Epoch 23  26.2% | batch:       180 of       686\t|\tloss: 1.26321\n",
      "Training Epoch 23  26.4% | batch:       181 of       686\t|\tloss: 1.22689\n",
      "Training Epoch 23  26.5% | batch:       182 of       686\t|\tloss: 1.00322\n",
      "Training Epoch 23  26.7% | batch:       183 of       686\t|\tloss: 1.21671\n",
      "Training Epoch 23  26.8% | batch:       184 of       686\t|\tloss: 1.24367\n",
      "Training Epoch 23  27.0% | batch:       185 of       686\t|\tloss: 1.08141\n",
      "Training Epoch 23  27.1% | batch:       186 of       686\t|\tloss: 1.22656\n",
      "Training Epoch 23  27.3% | batch:       187 of       686\t|\tloss: 1.02422\n",
      "Training Epoch 23  27.4% | batch:       188 of       686\t|\tloss: 1.17925\n",
      "Training Epoch 23  27.6% | batch:       189 of       686\t|\tloss: 1.14357\n",
      "Training Epoch 23  27.7% | batch:       190 of       686\t|\tloss: 1.02688\n",
      "Training Epoch 23  27.8% | batch:       191 of       686\t|\tloss: 1.3346\n",
      "Training Epoch 23  28.0% | batch:       192 of       686\t|\tloss: 1.45991\n",
      "Training Epoch 23  28.1% | batch:       193 of       686\t|\tloss: 1.11863\n",
      "Training Epoch 23  28.3% | batch:       194 of       686\t|\tloss: 1.09772\n",
      "Training Epoch 23  28.4% | batch:       195 of       686\t|\tloss: 1.13125\n",
      "Training Epoch 23  28.6% | batch:       196 of       686\t|\tloss: 1.06096\n",
      "Training Epoch 23  28.7% | batch:       197 of       686\t|\tloss: 1.47778\n",
      "Training Epoch 23  28.9% | batch:       198 of       686\t|\tloss: 1.19014\n",
      "Training Epoch 23  29.0% | batch:       199 of       686\t|\tloss: 1.62874\n",
      "Training Epoch 23  29.2% | batch:       200 of       686\t|\tloss: 1.12406\n",
      "Training Epoch 23  29.3% | batch:       201 of       686\t|\tloss: 1.09577\n",
      "Training Epoch 23  29.4% | batch:       202 of       686\t|\tloss: 1.31709\n",
      "Training Epoch 23  29.6% | batch:       203 of       686\t|\tloss: 1.3807\n",
      "Training Epoch 23  29.7% | batch:       204 of       686\t|\tloss: 0.961732\n",
      "Training Epoch 23  29.9% | batch:       205 of       686\t|\tloss: 1.16226\n",
      "Training Epoch 23  30.0% | batch:       206 of       686\t|\tloss: 1.32447\n",
      "Training Epoch 23  30.2% | batch:       207 of       686\t|\tloss: 1.09241\n",
      "Training Epoch 23  30.3% | batch:       208 of       686\t|\tloss: 1.52709\n",
      "Training Epoch 23  30.5% | batch:       209 of       686\t|\tloss: 1.03368\n",
      "Training Epoch 23  30.6% | batch:       210 of       686\t|\tloss: 1.20831\n",
      "Training Epoch 23  30.8% | batch:       211 of       686\t|\tloss: 1.28523\n",
      "Training Epoch 23  30.9% | batch:       212 of       686\t|\tloss: 1.37\n",
      "Training Epoch 23  31.0% | batch:       213 of       686\t|\tloss: 1.09001\n",
      "Training Epoch 23  31.2% | batch:       214 of       686\t|\tloss: 1.21107\n",
      "Training Epoch 23  31.3% | batch:       215 of       686\t|\tloss: 1.14991\n",
      "Training Epoch 23  31.5% | batch:       216 of       686\t|\tloss: 1.24274\n",
      "Training Epoch 23  31.6% | batch:       217 of       686\t|\tloss: 1.03435\n",
      "Training Epoch 23  31.8% | batch:       218 of       686\t|\tloss: 1.17461\n",
      "Training Epoch 23  31.9% | batch:       219 of       686\t|\tloss: 1.45992\n",
      "Training Epoch 23  32.1% | batch:       220 of       686\t|\tloss: 1.16346\n",
      "Training Epoch 23  32.2% | batch:       221 of       686\t|\tloss: 1.05568\n",
      "Training Epoch 23  32.4% | batch:       222 of       686\t|\tloss: 1.2367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 23  32.5% | batch:       223 of       686\t|\tloss: 1.22642\n",
      "Training Epoch 23  32.7% | batch:       224 of       686\t|\tloss: 1.19481\n",
      "Training Epoch 23  32.8% | batch:       225 of       686\t|\tloss: 1.32943\n",
      "Training Epoch 23  32.9% | batch:       226 of       686\t|\tloss: 0.962215\n",
      "Training Epoch 23  33.1% | batch:       227 of       686\t|\tloss: 1.33111\n",
      "Training Epoch 23  33.2% | batch:       228 of       686\t|\tloss: 1.47566\n",
      "Training Epoch 23  33.4% | batch:       229 of       686\t|\tloss: 1.22928\n",
      "Training Epoch 23  33.5% | batch:       230 of       686\t|\tloss: 0.937155\n",
      "Training Epoch 23  33.7% | batch:       231 of       686\t|\tloss: 1.04366\n",
      "Training Epoch 23  33.8% | batch:       232 of       686\t|\tloss: 1.18309\n",
      "Training Epoch 23  34.0% | batch:       233 of       686\t|\tloss: 1.50817\n",
      "Training Epoch 23  34.1% | batch:       234 of       686\t|\tloss: 1.14704\n",
      "Training Epoch 23  34.3% | batch:       235 of       686\t|\tloss: 1.05425\n",
      "Training Epoch 23  34.4% | batch:       236 of       686\t|\tloss: 1.37647\n",
      "Training Epoch 23  34.5% | batch:       237 of       686\t|\tloss: 1.31355\n",
      "Training Epoch 23  34.7% | batch:       238 of       686\t|\tloss: 1.42141\n",
      "Training Epoch 23  34.8% | batch:       239 of       686\t|\tloss: 1.16546\n",
      "Training Epoch 23  35.0% | batch:       240 of       686\t|\tloss: 1.37746\n",
      "Training Epoch 23  35.1% | batch:       241 of       686\t|\tloss: 1.30663\n",
      "Training Epoch 23  35.3% | batch:       242 of       686\t|\tloss: 1.50428\n",
      "Training Epoch 23  35.4% | batch:       243 of       686\t|\tloss: 1.138\n",
      "Training Epoch 23  35.6% | batch:       244 of       686\t|\tloss: 1.13657\n",
      "Training Epoch 23  35.7% | batch:       245 of       686\t|\tloss: 1.00932\n",
      "Training Epoch 23  35.9% | batch:       246 of       686\t|\tloss: 1.18033\n",
      "Training Epoch 23  36.0% | batch:       247 of       686\t|\tloss: 1.01162\n",
      "Training Epoch 23  36.2% | batch:       248 of       686\t|\tloss: 0.870924\n",
      "Training Epoch 23  36.3% | batch:       249 of       686\t|\tloss: 1.05695\n",
      "Training Epoch 23  36.4% | batch:       250 of       686\t|\tloss: 1.31782\n",
      "Training Epoch 23  36.6% | batch:       251 of       686\t|\tloss: 0.904493\n",
      "Training Epoch 23  36.7% | batch:       252 of       686\t|\tloss: 0.86576\n",
      "Training Epoch 23  36.9% | batch:       253 of       686\t|\tloss: 0.979667\n",
      "Training Epoch 23  37.0% | batch:       254 of       686\t|\tloss: 1.23915\n",
      "Training Epoch 23  37.2% | batch:       255 of       686\t|\tloss: 1.37134\n",
      "Training Epoch 23  37.3% | batch:       256 of       686\t|\tloss: 0.942263\n",
      "Training Epoch 23  37.5% | batch:       257 of       686\t|\tloss: 1.3111\n",
      "Training Epoch 23  37.6% | batch:       258 of       686\t|\tloss: 1.19736\n",
      "Training Epoch 23  37.8% | batch:       259 of       686\t|\tloss: 1.2987\n",
      "Training Epoch 23  37.9% | batch:       260 of       686\t|\tloss: 0.901861\n",
      "Training Epoch 23  38.0% | batch:       261 of       686\t|\tloss: 1.31288\n",
      "Training Epoch 23  38.2% | batch:       262 of       686\t|\tloss: 1.0172\n",
      "Training Epoch 23  38.3% | batch:       263 of       686\t|\tloss: 1.12012\n",
      "Training Epoch 23  38.5% | batch:       264 of       686\t|\tloss: 1.08378\n",
      "Training Epoch 23  38.6% | batch:       265 of       686\t|\tloss: 1.14041\n",
      "Training Epoch 23  38.8% | batch:       266 of       686\t|\tloss: 1.21637\n",
      "Training Epoch 23  38.9% | batch:       267 of       686\t|\tloss: 1.21302\n",
      "Training Epoch 23  39.1% | batch:       268 of       686\t|\tloss: 1.1831\n",
      "Training Epoch 23  39.2% | batch:       269 of       686\t|\tloss: 1.33363\n",
      "Training Epoch 23  39.4% | batch:       270 of       686\t|\tloss: 1.2813\n",
      "Training Epoch 23  39.5% | batch:       271 of       686\t|\tloss: 0.982234\n",
      "Training Epoch 23  39.7% | batch:       272 of       686\t|\tloss: 0.967545\n",
      "Training Epoch 23  39.8% | batch:       273 of       686\t|\tloss: 1.28419\n",
      "Training Epoch 23  39.9% | batch:       274 of       686\t|\tloss: 1.28731\n",
      "Training Epoch 23  40.1% | batch:       275 of       686\t|\tloss: 1.36416\n",
      "Training Epoch 23  40.2% | batch:       276 of       686\t|\tloss: 1.30075\n",
      "Training Epoch 23  40.4% | batch:       277 of       686\t|\tloss: 1.28845\n",
      "Training Epoch 23  40.5% | batch:       278 of       686\t|\tloss: 1.23462\n",
      "Training Epoch 23  40.7% | batch:       279 of       686\t|\tloss: 1.01249\n",
      "Training Epoch 23  40.8% | batch:       280 of       686\t|\tloss: 0.931546\n",
      "Training Epoch 23  41.0% | batch:       281 of       686\t|\tloss: 1.18148\n",
      "Training Epoch 23  41.1% | batch:       282 of       686\t|\tloss: 1.308\n",
      "Training Epoch 23  41.3% | batch:       283 of       686\t|\tloss: 1.23561\n",
      "Training Epoch 23  41.4% | batch:       284 of       686\t|\tloss: 1.3842\n",
      "Training Epoch 23  41.5% | batch:       285 of       686\t|\tloss: 1.34667\n",
      "Training Epoch 23  41.7% | batch:       286 of       686\t|\tloss: 1.13792\n",
      "Training Epoch 23  41.8% | batch:       287 of       686\t|\tloss: 0.976205\n",
      "Training Epoch 23  42.0% | batch:       288 of       686\t|\tloss: 1.21167\n",
      "Training Epoch 23  42.1% | batch:       289 of       686\t|\tloss: 1.10893\n",
      "Training Epoch 23  42.3% | batch:       290 of       686\t|\tloss: 1.24209\n",
      "Training Epoch 23  42.4% | batch:       291 of       686\t|\tloss: 1.33267\n",
      "Training Epoch 23  42.6% | batch:       292 of       686\t|\tloss: 1.166\n",
      "Training Epoch 23  42.7% | batch:       293 of       686\t|\tloss: 1.25669\n",
      "Training Epoch 23  42.9% | batch:       294 of       686\t|\tloss: 1.06087\n",
      "Training Epoch 23  43.0% | batch:       295 of       686\t|\tloss: 1.2713\n",
      "Training Epoch 23  43.1% | batch:       296 of       686\t|\tloss: 0.977255\n",
      "Training Epoch 23  43.3% | batch:       297 of       686\t|\tloss: 1.21164\n",
      "Training Epoch 23  43.4% | batch:       298 of       686\t|\tloss: 1.13779\n",
      "Training Epoch 23  43.6% | batch:       299 of       686\t|\tloss: 1.33504\n",
      "Training Epoch 23  43.7% | batch:       300 of       686\t|\tloss: 1.30718\n",
      "Training Epoch 23  43.9% | batch:       301 of       686\t|\tloss: 1.17594\n",
      "Training Epoch 23  44.0% | batch:       302 of       686\t|\tloss: 1.16061\n",
      "Training Epoch 23  44.2% | batch:       303 of       686\t|\tloss: 1.28715\n",
      "Training Epoch 23  44.3% | batch:       304 of       686\t|\tloss: 1.06288\n",
      "Training Epoch 23  44.5% | batch:       305 of       686\t|\tloss: 1.09339\n",
      "Training Epoch 23  44.6% | batch:       306 of       686\t|\tloss: 1.5022\n",
      "Training Epoch 23  44.8% | batch:       307 of       686\t|\tloss: 0.930362\n",
      "Training Epoch 23  44.9% | batch:       308 of       686\t|\tloss: 1.34256\n",
      "Training Epoch 23  45.0% | batch:       309 of       686\t|\tloss: 1.06093\n",
      "Training Epoch 23  45.2% | batch:       310 of       686\t|\tloss: 1.08909\n",
      "Training Epoch 23  45.3% | batch:       311 of       686\t|\tloss: 1.36226\n",
      "Training Epoch 23  45.5% | batch:       312 of       686\t|\tloss: 0.992018\n",
      "Training Epoch 23  45.6% | batch:       313 of       686\t|\tloss: 1.14\n",
      "Training Epoch 23  45.8% | batch:       314 of       686\t|\tloss: 1.25871\n",
      "Training Epoch 23  45.9% | batch:       315 of       686\t|\tloss: 0.944941\n",
      "Training Epoch 23  46.1% | batch:       316 of       686\t|\tloss: 1.26453\n",
      "Training Epoch 23  46.2% | batch:       317 of       686\t|\tloss: 1.06578\n",
      "Training Epoch 23  46.4% | batch:       318 of       686\t|\tloss: 1.09092\n",
      "Training Epoch 23  46.5% | batch:       319 of       686\t|\tloss: 0.862058\n",
      "Training Epoch 23  46.6% | batch:       320 of       686\t|\tloss: 1.34564\n",
      "Training Epoch 23  46.8% | batch:       321 of       686\t|\tloss: 1.38074\n",
      "Training Epoch 23  46.9% | batch:       322 of       686\t|\tloss: 0.86852\n",
      "Training Epoch 23  47.1% | batch:       323 of       686\t|\tloss: 0.955057\n",
      "Training Epoch 23  47.2% | batch:       324 of       686\t|\tloss: 1.0577\n",
      "Training Epoch 23  47.4% | batch:       325 of       686\t|\tloss: 0.826087\n",
      "Training Epoch 23  47.5% | batch:       326 of       686\t|\tloss: 1.19534\n",
      "Training Epoch 23  47.7% | batch:       327 of       686\t|\tloss: 1.19575\n",
      "Training Epoch 23  47.8% | batch:       328 of       686\t|\tloss: 1.03691\n",
      "Training Epoch 23  48.0% | batch:       329 of       686\t|\tloss: 1.14582\n",
      "Training Epoch 23  48.1% | batch:       330 of       686\t|\tloss: 1.13279\n",
      "Training Epoch 23  48.3% | batch:       331 of       686\t|\tloss: 0.858838\n",
      "Training Epoch 23  48.4% | batch:       332 of       686\t|\tloss: 1.13288\n",
      "Training Epoch 23  48.5% | batch:       333 of       686\t|\tloss: 0.864529\n",
      "Training Epoch 23  48.7% | batch:       334 of       686\t|\tloss: 1.41744\n",
      "Training Epoch 23  48.8% | batch:       335 of       686\t|\tloss: 1.12938\n",
      "Training Epoch 23  49.0% | batch:       336 of       686\t|\tloss: 1.0778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 23  49.1% | batch:       337 of       686\t|\tloss: 0.911238\n",
      "Training Epoch 23  49.3% | batch:       338 of       686\t|\tloss: 1.13707\n",
      "Training Epoch 23  49.4% | batch:       339 of       686\t|\tloss: 1.18082\n",
      "Training Epoch 23  49.6% | batch:       340 of       686\t|\tloss: 0.816864\n",
      "Training Epoch 23  49.7% | batch:       341 of       686\t|\tloss: 1.09725\n",
      "Training Epoch 23  49.9% | batch:       342 of       686\t|\tloss: 0.938754\n",
      "Training Epoch 23  50.0% | batch:       343 of       686\t|\tloss: 1.02484\n",
      "Training Epoch 23  50.1% | batch:       344 of       686\t|\tloss: 1.12201\n",
      "Training Epoch 23  50.3% | batch:       345 of       686\t|\tloss: 1.18925\n",
      "Training Epoch 23  50.4% | batch:       346 of       686\t|\tloss: 1.17592\n",
      "Training Epoch 23  50.6% | batch:       347 of       686\t|\tloss: 1.73769\n",
      "Training Epoch 23  50.7% | batch:       348 of       686\t|\tloss: 0.926378\n",
      "Training Epoch 23  50.9% | batch:       349 of       686\t|\tloss: 0.883085\n",
      "Training Epoch 23  51.0% | batch:       350 of       686\t|\tloss: 1.27852\n",
      "Training Epoch 23  51.2% | batch:       351 of       686\t|\tloss: 1.29016\n",
      "Training Epoch 23  51.3% | batch:       352 of       686\t|\tloss: 1.02959\n",
      "Training Epoch 23  51.5% | batch:       353 of       686\t|\tloss: 0.91894\n",
      "Training Epoch 23  51.6% | batch:       354 of       686\t|\tloss: 1.70891\n",
      "Training Epoch 23  51.7% | batch:       355 of       686\t|\tloss: 1.17653\n",
      "Training Epoch 23  51.9% | batch:       356 of       686\t|\tloss: 1.70642\n",
      "Training Epoch 23  52.0% | batch:       357 of       686\t|\tloss: 0.947315\n",
      "Training Epoch 23  52.2% | batch:       358 of       686\t|\tloss: 1.05878\n",
      "Training Epoch 23  52.3% | batch:       359 of       686\t|\tloss: 1.07908\n",
      "Training Epoch 23  52.5% | batch:       360 of       686\t|\tloss: 0.954825\n",
      "Training Epoch 23  52.6% | batch:       361 of       686\t|\tloss: 1.3279\n",
      "Training Epoch 23  52.8% | batch:       362 of       686\t|\tloss: 0.842603\n",
      "Training Epoch 23  52.9% | batch:       363 of       686\t|\tloss: 1.08565\n",
      "Training Epoch 23  53.1% | batch:       364 of       686\t|\tloss: 1.07058\n",
      "Training Epoch 23  53.2% | batch:       365 of       686\t|\tloss: 1.14312\n",
      "Training Epoch 23  53.4% | batch:       366 of       686\t|\tloss: 1.19167\n",
      "Training Epoch 23  53.5% | batch:       367 of       686\t|\tloss: 1.12993\n",
      "Training Epoch 23  53.6% | batch:       368 of       686\t|\tloss: 1.22766\n",
      "Training Epoch 23  53.8% | batch:       369 of       686\t|\tloss: 1.02092\n",
      "Training Epoch 23  53.9% | batch:       370 of       686\t|\tloss: 0.982335\n",
      "Training Epoch 23  54.1% | batch:       371 of       686\t|\tloss: 1.32477\n",
      "Training Epoch 23  54.2% | batch:       372 of       686\t|\tloss: 1.13749\n",
      "Training Epoch 23  54.4% | batch:       373 of       686\t|\tloss: 0.957146\n",
      "Training Epoch 23  54.5% | batch:       374 of       686\t|\tloss: 0.990233\n",
      "Training Epoch 23  54.7% | batch:       375 of       686\t|\tloss: 1.01315\n",
      "Training Epoch 23  54.8% | batch:       376 of       686\t|\tloss: 1.11166\n",
      "Training Epoch 23  55.0% | batch:       377 of       686\t|\tloss: 1.32095\n",
      "Training Epoch 23  55.1% | batch:       378 of       686\t|\tloss: 1.21711\n",
      "Training Epoch 23  55.2% | batch:       379 of       686\t|\tloss: 1.27786\n",
      "Training Epoch 23  55.4% | batch:       380 of       686\t|\tloss: 1.10376\n",
      "Training Epoch 23  55.5% | batch:       381 of       686\t|\tloss: 1.29279\n",
      "Training Epoch 23  55.7% | batch:       382 of       686\t|\tloss: 1.10101\n",
      "Training Epoch 23  55.8% | batch:       383 of       686\t|\tloss: 1.24776\n",
      "Training Epoch 23  56.0% | batch:       384 of       686\t|\tloss: 1.24587\n",
      "Training Epoch 23  56.1% | batch:       385 of       686\t|\tloss: 0.960958\n",
      "Training Epoch 23  56.3% | batch:       386 of       686\t|\tloss: 1.26513\n",
      "Training Epoch 23  56.4% | batch:       387 of       686\t|\tloss: 0.921467\n",
      "Training Epoch 23  56.6% | batch:       388 of       686\t|\tloss: 0.959385\n",
      "Training Epoch 23  56.7% | batch:       389 of       686\t|\tloss: 0.827354\n",
      "Training Epoch 23  56.9% | batch:       390 of       686\t|\tloss: 1.21135\n",
      "Training Epoch 23  57.0% | batch:       391 of       686\t|\tloss: 1.08163\n",
      "Training Epoch 23  57.1% | batch:       392 of       686\t|\tloss: 0.899331\n",
      "Training Epoch 23  57.3% | batch:       393 of       686\t|\tloss: 1.0404\n",
      "Training Epoch 23  57.4% | batch:       394 of       686\t|\tloss: 1.20061\n",
      "Training Epoch 23  57.6% | batch:       395 of       686\t|\tloss: 1.43317\n",
      "Training Epoch 23  57.7% | batch:       396 of       686\t|\tloss: 1.16055\n",
      "Training Epoch 23  57.9% | batch:       397 of       686\t|\tloss: 1.34219\n",
      "Training Epoch 23  58.0% | batch:       398 of       686\t|\tloss: 1.31046\n",
      "Training Epoch 23  58.2% | batch:       399 of       686\t|\tloss: 1.2527\n",
      "Training Epoch 23  58.3% | batch:       400 of       686\t|\tloss: 1.00829\n",
      "Training Epoch 23  58.5% | batch:       401 of       686\t|\tloss: 0.992251\n",
      "Training Epoch 23  58.6% | batch:       402 of       686\t|\tloss: 1.64294\n",
      "Training Epoch 23  58.7% | batch:       403 of       686\t|\tloss: 1.2952\n",
      "Training Epoch 23  58.9% | batch:       404 of       686\t|\tloss: 0.875396\n",
      "Training Epoch 23  59.0% | batch:       405 of       686\t|\tloss: 1.30088\n",
      "Training Epoch 23  59.2% | batch:       406 of       686\t|\tloss: 1.20467\n",
      "Training Epoch 23  59.3% | batch:       407 of       686\t|\tloss: 1.58779\n",
      "Training Epoch 23  59.5% | batch:       408 of       686\t|\tloss: 1.18218\n",
      "Training Epoch 23  59.6% | batch:       409 of       686\t|\tloss: 0.807048\n",
      "Training Epoch 23  59.8% | batch:       410 of       686\t|\tloss: 1.07392\n",
      "Training Epoch 23  59.9% | batch:       411 of       686\t|\tloss: 1.19626\n",
      "Training Epoch 23  60.1% | batch:       412 of       686\t|\tloss: 1.13035\n",
      "Training Epoch 23  60.2% | batch:       413 of       686\t|\tloss: 1.06317\n",
      "Training Epoch 23  60.3% | batch:       414 of       686\t|\tloss: 1.35258\n",
      "Training Epoch 23  60.5% | batch:       415 of       686\t|\tloss: 1.21457\n",
      "Training Epoch 23  60.6% | batch:       416 of       686\t|\tloss: 1.13579\n",
      "Training Epoch 23  60.8% | batch:       417 of       686\t|\tloss: 1.17599\n",
      "Training Epoch 23  60.9% | batch:       418 of       686\t|\tloss: 1.2323\n",
      "Training Epoch 23  61.1% | batch:       419 of       686\t|\tloss: 1.27632\n",
      "Training Epoch 23  61.2% | batch:       420 of       686\t|\tloss: 1.29498\n",
      "Training Epoch 23  61.4% | batch:       421 of       686\t|\tloss: 1.09164\n",
      "Training Epoch 23  61.5% | batch:       422 of       686\t|\tloss: 0.984281\n",
      "Training Epoch 23  61.7% | batch:       423 of       686\t|\tloss: 1.27528\n",
      "Training Epoch 23  61.8% | batch:       424 of       686\t|\tloss: 1.35429\n",
      "Training Epoch 23  62.0% | batch:       425 of       686\t|\tloss: 1.02953\n",
      "Training Epoch 23  62.1% | batch:       426 of       686\t|\tloss: 0.877024\n",
      "Training Epoch 23  62.2% | batch:       427 of       686\t|\tloss: 1.19075\n",
      "Training Epoch 23  62.4% | batch:       428 of       686\t|\tloss: 1.26607\n",
      "Training Epoch 23  62.5% | batch:       429 of       686\t|\tloss: 1.57012\n",
      "Training Epoch 23  62.7% | batch:       430 of       686\t|\tloss: 1.46402\n",
      "Training Epoch 23  62.8% | batch:       431 of       686\t|\tloss: 1.26445\n",
      "Training Epoch 23  63.0% | batch:       432 of       686\t|\tloss: 1.16617\n",
      "Training Epoch 23  63.1% | batch:       433 of       686\t|\tloss: 0.874377\n",
      "Training Epoch 23  63.3% | batch:       434 of       686\t|\tloss: 0.914425\n",
      "Training Epoch 23  63.4% | batch:       435 of       686\t|\tloss: 1.17072\n",
      "Training Epoch 23  63.6% | batch:       436 of       686\t|\tloss: 1.43493\n",
      "Training Epoch 23  63.7% | batch:       437 of       686\t|\tloss: 1.36387\n",
      "Training Epoch 23  63.8% | batch:       438 of       686\t|\tloss: 1.14788\n",
      "Training Epoch 23  64.0% | batch:       439 of       686\t|\tloss: 1.51461\n",
      "Training Epoch 23  64.1% | batch:       440 of       686\t|\tloss: 1.11817\n",
      "Training Epoch 23  64.3% | batch:       441 of       686\t|\tloss: 1.22927\n",
      "Training Epoch 23  64.4% | batch:       442 of       686\t|\tloss: 1.25153\n",
      "Training Epoch 23  64.6% | batch:       443 of       686\t|\tloss: 1.13037\n",
      "Training Epoch 23  64.7% | batch:       444 of       686\t|\tloss: 1.14239\n",
      "Training Epoch 23  64.9% | batch:       445 of       686\t|\tloss: 1.13481\n",
      "Training Epoch 23  65.0% | batch:       446 of       686\t|\tloss: 1.17772\n",
      "Training Epoch 23  65.2% | batch:       447 of       686\t|\tloss: 1.36753\n",
      "Training Epoch 23  65.3% | batch:       448 of       686\t|\tloss: 1.18541\n",
      "Training Epoch 23  65.5% | batch:       449 of       686\t|\tloss: 0.837759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 23  65.6% | batch:       450 of       686\t|\tloss: 0.979897\n",
      "Training Epoch 23  65.7% | batch:       451 of       686\t|\tloss: 0.945162\n",
      "Training Epoch 23  65.9% | batch:       452 of       686\t|\tloss: 1.10536\n",
      "Training Epoch 23  66.0% | batch:       453 of       686\t|\tloss: 1.12371\n",
      "Training Epoch 23  66.2% | batch:       454 of       686\t|\tloss: 1.00208\n",
      "Training Epoch 23  66.3% | batch:       455 of       686\t|\tloss: 1.19324\n",
      "Training Epoch 23  66.5% | batch:       456 of       686\t|\tloss: 1.22354\n",
      "Training Epoch 23  66.6% | batch:       457 of       686\t|\tloss: 1.11054\n",
      "Training Epoch 23  66.8% | batch:       458 of       686\t|\tloss: 0.975717\n",
      "Training Epoch 23  66.9% | batch:       459 of       686\t|\tloss: 1.8602\n",
      "Training Epoch 23  67.1% | batch:       460 of       686\t|\tloss: 1.14376\n",
      "Training Epoch 23  67.2% | batch:       461 of       686\t|\tloss: 1.4199\n",
      "Training Epoch 23  67.3% | batch:       462 of       686\t|\tloss: 1.27419\n",
      "Training Epoch 23  67.5% | batch:       463 of       686\t|\tloss: 1.09916\n",
      "Training Epoch 23  67.6% | batch:       464 of       686\t|\tloss: 1.00756\n",
      "Training Epoch 23  67.8% | batch:       465 of       686\t|\tloss: 1.07661\n",
      "Training Epoch 23  67.9% | batch:       466 of       686\t|\tloss: 1.19052\n",
      "Training Epoch 23  68.1% | batch:       467 of       686\t|\tloss: 1.04973\n",
      "Training Epoch 23  68.2% | batch:       468 of       686\t|\tloss: 1.15586\n",
      "Training Epoch 23  68.4% | batch:       469 of       686\t|\tloss: 1.3832\n",
      "Training Epoch 23  68.5% | batch:       470 of       686\t|\tloss: 1.00875\n",
      "Training Epoch 23  68.7% | batch:       471 of       686\t|\tloss: 1.07221\n",
      "Training Epoch 23  68.8% | batch:       472 of       686\t|\tloss: 1.09908\n",
      "Training Epoch 23  69.0% | batch:       473 of       686\t|\tloss: 1.02637\n",
      "Training Epoch 23  69.1% | batch:       474 of       686\t|\tloss: 1.28082\n",
      "Training Epoch 23  69.2% | batch:       475 of       686\t|\tloss: 0.983861\n",
      "Training Epoch 23  69.4% | batch:       476 of       686\t|\tloss: 1.22042\n",
      "Training Epoch 23  69.5% | batch:       477 of       686\t|\tloss: 1.23726\n",
      "Training Epoch 23  69.7% | batch:       478 of       686\t|\tloss: 1.35868\n",
      "Training Epoch 23  69.8% | batch:       479 of       686\t|\tloss: 0.897122\n",
      "Training Epoch 23  70.0% | batch:       480 of       686\t|\tloss: 1.10137\n",
      "Training Epoch 23  70.1% | batch:       481 of       686\t|\tloss: 1.10909\n",
      "Training Epoch 23  70.3% | batch:       482 of       686\t|\tloss: 0.976253\n",
      "Training Epoch 23  70.4% | batch:       483 of       686\t|\tloss: 1.0135\n",
      "Training Epoch 23  70.6% | batch:       484 of       686\t|\tloss: 1.10796\n",
      "Training Epoch 23  70.7% | batch:       485 of       686\t|\tloss: 0.903904\n",
      "Training Epoch 23  70.8% | batch:       486 of       686\t|\tloss: 0.923646\n",
      "Training Epoch 23  71.0% | batch:       487 of       686\t|\tloss: 1.29147\n",
      "Training Epoch 23  71.1% | batch:       488 of       686\t|\tloss: 0.94171\n",
      "Training Epoch 23  71.3% | batch:       489 of       686\t|\tloss: 1.05675\n",
      "Training Epoch 23  71.4% | batch:       490 of       686\t|\tloss: 1.14116\n",
      "Training Epoch 23  71.6% | batch:       491 of       686\t|\tloss: 1.03967\n",
      "Training Epoch 23  71.7% | batch:       492 of       686\t|\tloss: 0.91593\n",
      "Training Epoch 23  71.9% | batch:       493 of       686\t|\tloss: 1.61062\n",
      "Training Epoch 23  72.0% | batch:       494 of       686\t|\tloss: 1.25706\n",
      "Training Epoch 23  72.2% | batch:       495 of       686\t|\tloss: 0.849567\n",
      "Training Epoch 23  72.3% | batch:       496 of       686\t|\tloss: 1.13669\n",
      "Training Epoch 23  72.4% | batch:       497 of       686\t|\tloss: 1.27458\n",
      "Training Epoch 23  72.6% | batch:       498 of       686\t|\tloss: 1.05419\n",
      "Training Epoch 23  72.7% | batch:       499 of       686\t|\tloss: 1.19348\n",
      "Training Epoch 23  72.9% | batch:       500 of       686\t|\tloss: 0.936166\n",
      "Training Epoch 23  73.0% | batch:       501 of       686\t|\tloss: 1.43874\n",
      "Training Epoch 23  73.2% | batch:       502 of       686\t|\tloss: 1.12531\n",
      "Training Epoch 23  73.3% | batch:       503 of       686\t|\tloss: 1.10306\n",
      "Training Epoch 23  73.5% | batch:       504 of       686\t|\tloss: 1.0128\n",
      "Training Epoch 23  73.6% | batch:       505 of       686\t|\tloss: 0.827396\n",
      "Training Epoch 23  73.8% | batch:       506 of       686\t|\tloss: 0.807133\n",
      "Training Epoch 23  73.9% | batch:       507 of       686\t|\tloss: 1.17915\n",
      "Training Epoch 23  74.1% | batch:       508 of       686\t|\tloss: 1.23995\n",
      "Training Epoch 23  74.2% | batch:       509 of       686\t|\tloss: 0.830562\n",
      "Training Epoch 23  74.3% | batch:       510 of       686\t|\tloss: 0.753518\n",
      "Training Epoch 23  74.5% | batch:       511 of       686\t|\tloss: 1.58863\n",
      "Training Epoch 23  74.6% | batch:       512 of       686\t|\tloss: 1.56653\n",
      "Training Epoch 23  74.8% | batch:       513 of       686\t|\tloss: 1.03871\n",
      "Training Epoch 23  74.9% | batch:       514 of       686\t|\tloss: 1.03682\n",
      "Training Epoch 23  75.1% | batch:       515 of       686\t|\tloss: 0.760468\n",
      "Training Epoch 23  75.2% | batch:       516 of       686\t|\tloss: 0.953468\n",
      "Training Epoch 23  75.4% | batch:       517 of       686\t|\tloss: 1.04217\n",
      "Training Epoch 23  75.5% | batch:       518 of       686\t|\tloss: 1.10035\n",
      "Training Epoch 23  75.7% | batch:       519 of       686\t|\tloss: 1.06451\n",
      "Training Epoch 23  75.8% | batch:       520 of       686\t|\tloss: 1.23349\n",
      "Training Epoch 23  75.9% | batch:       521 of       686\t|\tloss: 0.883798\n",
      "Training Epoch 23  76.1% | batch:       522 of       686\t|\tloss: 1.083\n",
      "Training Epoch 23  76.2% | batch:       523 of       686\t|\tloss: 0.94244\n",
      "Training Epoch 23  76.4% | batch:       524 of       686\t|\tloss: 1.1724\n",
      "Training Epoch 23  76.5% | batch:       525 of       686\t|\tloss: 1.19182\n",
      "Training Epoch 23  76.7% | batch:       526 of       686\t|\tloss: 1.1927\n",
      "Training Epoch 23  76.8% | batch:       527 of       686\t|\tloss: 1.39044\n",
      "Training Epoch 23  77.0% | batch:       528 of       686\t|\tloss: 1.25976\n",
      "Training Epoch 23  77.1% | batch:       529 of       686\t|\tloss: 1.3727\n",
      "Training Epoch 23  77.3% | batch:       530 of       686\t|\tloss: 1.44859\n",
      "Training Epoch 23  77.4% | batch:       531 of       686\t|\tloss: 0.875383\n",
      "Training Epoch 23  77.6% | batch:       532 of       686\t|\tloss: 1.14642\n",
      "Training Epoch 23  77.7% | batch:       533 of       686\t|\tloss: 0.991823\n",
      "Training Epoch 23  77.8% | batch:       534 of       686\t|\tloss: 1.12852\n",
      "Training Epoch 23  78.0% | batch:       535 of       686\t|\tloss: 1.08023\n",
      "Training Epoch 23  78.1% | batch:       536 of       686\t|\tloss: 1.22185\n",
      "Training Epoch 23  78.3% | batch:       537 of       686\t|\tloss: 0.971714\n",
      "Training Epoch 23  78.4% | batch:       538 of       686\t|\tloss: 1.15928\n",
      "Training Epoch 23  78.6% | batch:       539 of       686\t|\tloss: 0.770019\n",
      "Training Epoch 23  78.7% | batch:       540 of       686\t|\tloss: 1.24282\n",
      "Training Epoch 23  78.9% | batch:       541 of       686\t|\tloss: 1.28966\n",
      "Training Epoch 23  79.0% | batch:       542 of       686\t|\tloss: 1.03413\n",
      "Training Epoch 23  79.2% | batch:       543 of       686\t|\tloss: 1.08256\n",
      "Training Epoch 23  79.3% | batch:       544 of       686\t|\tloss: 0.899467\n",
      "Training Epoch 23  79.4% | batch:       545 of       686\t|\tloss: 0.906476\n",
      "Training Epoch 23  79.6% | batch:       546 of       686\t|\tloss: 1.24458\n",
      "Training Epoch 23  79.7% | batch:       547 of       686\t|\tloss: 1.54263\n",
      "Training Epoch 23  79.9% | batch:       548 of       686\t|\tloss: 1.29627\n",
      "Training Epoch 23  80.0% | batch:       549 of       686\t|\tloss: 1.01977\n",
      "Training Epoch 23  80.2% | batch:       550 of       686\t|\tloss: 1.19151\n",
      "Training Epoch 23  80.3% | batch:       551 of       686\t|\tloss: 1.18391\n",
      "Training Epoch 23  80.5% | batch:       552 of       686\t|\tloss: 1.18283\n",
      "Training Epoch 23  80.6% | batch:       553 of       686\t|\tloss: 1.2998\n",
      "Training Epoch 23  80.8% | batch:       554 of       686\t|\tloss: 1.43652\n",
      "Training Epoch 23  80.9% | batch:       555 of       686\t|\tloss: 1.03637\n",
      "Training Epoch 23  81.0% | batch:       556 of       686\t|\tloss: 1.02406\n",
      "Training Epoch 23  81.2% | batch:       557 of       686\t|\tloss: 1.05254\n",
      "Training Epoch 23  81.3% | batch:       558 of       686\t|\tloss: 1.22504\n",
      "Training Epoch 23  81.5% | batch:       559 of       686\t|\tloss: 1.09759\n",
      "Training Epoch 23  81.6% | batch:       560 of       686\t|\tloss: 1.44571\n",
      "Training Epoch 23  81.8% | batch:       561 of       686\t|\tloss: 1.2915\n",
      "Training Epoch 23  81.9% | batch:       562 of       686\t|\tloss: 0.93921\n",
      "Training Epoch 23  82.1% | batch:       563 of       686\t|\tloss: 1.13211\n",
      "Training Epoch 23  82.2% | batch:       564 of       686\t|\tloss: 1.13816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 23  82.4% | batch:       565 of       686\t|\tloss: 1.23761\n",
      "Training Epoch 23  82.5% | batch:       566 of       686\t|\tloss: 0.923121\n",
      "Training Epoch 23  82.7% | batch:       567 of       686\t|\tloss: 1.38509\n",
      "Training Epoch 23  82.8% | batch:       568 of       686\t|\tloss: 0.978209\n",
      "Training Epoch 23  82.9% | batch:       569 of       686\t|\tloss: 1.17403\n",
      "Training Epoch 23  83.1% | batch:       570 of       686\t|\tloss: 1.04706\n",
      "Training Epoch 23  83.2% | batch:       571 of       686\t|\tloss: 1.02392\n",
      "Training Epoch 23  83.4% | batch:       572 of       686\t|\tloss: 1.00463\n",
      "Training Epoch 23  83.5% | batch:       573 of       686\t|\tloss: 1.15843\n",
      "Training Epoch 23  83.7% | batch:       574 of       686\t|\tloss: 1.09004\n",
      "Training Epoch 23  83.8% | batch:       575 of       686\t|\tloss: 1.35108\n",
      "Training Epoch 23  84.0% | batch:       576 of       686\t|\tloss: 1.24764\n",
      "Training Epoch 23  84.1% | batch:       577 of       686\t|\tloss: 1.47987\n",
      "Training Epoch 23  84.3% | batch:       578 of       686\t|\tloss: 0.955703\n",
      "Training Epoch 23  84.4% | batch:       579 of       686\t|\tloss: 1.36074\n",
      "Training Epoch 23  84.5% | batch:       580 of       686\t|\tloss: 1.16629\n",
      "Training Epoch 23  84.7% | batch:       581 of       686\t|\tloss: 1.20183\n",
      "Training Epoch 23  84.8% | batch:       582 of       686\t|\tloss: 1.24827\n",
      "Training Epoch 23  85.0% | batch:       583 of       686\t|\tloss: 1.24722\n",
      "Training Epoch 23  85.1% | batch:       584 of       686\t|\tloss: 1.12714\n",
      "Training Epoch 23  85.3% | batch:       585 of       686\t|\tloss: 1.27041\n",
      "Training Epoch 23  85.4% | batch:       586 of       686\t|\tloss: 0.999363\n",
      "Training Epoch 23  85.6% | batch:       587 of       686\t|\tloss: 0.949214\n",
      "Training Epoch 23  85.7% | batch:       588 of       686\t|\tloss: 0.988844\n",
      "Training Epoch 23  85.9% | batch:       589 of       686\t|\tloss: 1.216\n",
      "Training Epoch 23  86.0% | batch:       590 of       686\t|\tloss: 1.38919\n",
      "Training Epoch 23  86.2% | batch:       591 of       686\t|\tloss: 1.36107\n",
      "Training Epoch 23  86.3% | batch:       592 of       686\t|\tloss: 1.08041\n",
      "Training Epoch 23  86.4% | batch:       593 of       686\t|\tloss: 1.18164\n",
      "Training Epoch 23  86.6% | batch:       594 of       686\t|\tloss: 0.961963\n",
      "Training Epoch 23  86.7% | batch:       595 of       686\t|\tloss: 1.06935\n",
      "Training Epoch 23  86.9% | batch:       596 of       686\t|\tloss: 1.03197\n",
      "Training Epoch 23  87.0% | batch:       597 of       686\t|\tloss: 1.44776\n",
      "Training Epoch 23  87.2% | batch:       598 of       686\t|\tloss: 1.2059\n",
      "Training Epoch 23  87.3% | batch:       599 of       686\t|\tloss: 0.933181\n",
      "Training Epoch 23  87.5% | batch:       600 of       686\t|\tloss: 1.14877\n",
      "Training Epoch 23  87.6% | batch:       601 of       686\t|\tloss: 0.995034\n",
      "Training Epoch 23  87.8% | batch:       602 of       686\t|\tloss: 1.10465\n",
      "Training Epoch 23  87.9% | batch:       603 of       686\t|\tloss: 1.24042\n",
      "Training Epoch 23  88.0% | batch:       604 of       686\t|\tloss: 1.29182\n",
      "Training Epoch 23  88.2% | batch:       605 of       686\t|\tloss: 1.34181\n",
      "Training Epoch 23  88.3% | batch:       606 of       686\t|\tloss: 0.933589\n",
      "Training Epoch 23  88.5% | batch:       607 of       686\t|\tloss: 1.18658\n",
      "Training Epoch 23  88.6% | batch:       608 of       686\t|\tloss: 0.866449\n",
      "Training Epoch 23  88.8% | batch:       609 of       686\t|\tloss: 1.18026\n",
      "Training Epoch 23  88.9% | batch:       610 of       686\t|\tloss: 1.31411\n",
      "Training Epoch 23  89.1% | batch:       611 of       686\t|\tloss: 0.921657\n",
      "Training Epoch 23  89.2% | batch:       612 of       686\t|\tloss: 1.08464\n",
      "Training Epoch 23  89.4% | batch:       613 of       686\t|\tloss: 1.07242\n",
      "Training Epoch 23  89.5% | batch:       614 of       686\t|\tloss: 1.06037\n",
      "Training Epoch 23  89.7% | batch:       615 of       686\t|\tloss: 1.05467\n",
      "Training Epoch 23  89.8% | batch:       616 of       686\t|\tloss: 1.00644\n",
      "Training Epoch 23  89.9% | batch:       617 of       686\t|\tloss: 1.53438\n",
      "Training Epoch 23  90.1% | batch:       618 of       686\t|\tloss: 0.949359\n",
      "Training Epoch 23  90.2% | batch:       619 of       686\t|\tloss: 1.1028\n",
      "Training Epoch 23  90.4% | batch:       620 of       686\t|\tloss: 0.816781\n",
      "Training Epoch 23  90.5% | batch:       621 of       686\t|\tloss: 1.24141\n",
      "Training Epoch 23  90.7% | batch:       622 of       686\t|\tloss: 1.2929\n",
      "Training Epoch 23  90.8% | batch:       623 of       686\t|\tloss: 1.17636\n",
      "Training Epoch 23  91.0% | batch:       624 of       686\t|\tloss: 0.986906\n",
      "Training Epoch 23  91.1% | batch:       625 of       686\t|\tloss: 0.994738\n",
      "Training Epoch 23  91.3% | batch:       626 of       686\t|\tloss: 1.12891\n",
      "Training Epoch 23  91.4% | batch:       627 of       686\t|\tloss: 1.05894\n",
      "Training Epoch 23  91.5% | batch:       628 of       686\t|\tloss: 0.993302\n",
      "Training Epoch 23  91.7% | batch:       629 of       686\t|\tloss: 1.05682\n",
      "Training Epoch 23  91.8% | batch:       630 of       686\t|\tloss: 1.0065\n",
      "Training Epoch 23  92.0% | batch:       631 of       686\t|\tloss: 1.08749\n",
      "Training Epoch 23  92.1% | batch:       632 of       686\t|\tloss: 0.965797\n",
      "Training Epoch 23  92.3% | batch:       633 of       686\t|\tloss: 1.06157\n",
      "Training Epoch 23  92.4% | batch:       634 of       686\t|\tloss: 1.18753\n",
      "Training Epoch 23  92.6% | batch:       635 of       686\t|\tloss: 1.0986\n",
      "Training Epoch 23  92.7% | batch:       636 of       686\t|\tloss: 1.25368\n",
      "Training Epoch 23  92.9% | batch:       637 of       686\t|\tloss: 0.863403\n",
      "Training Epoch 23  93.0% | batch:       638 of       686\t|\tloss: 0.988434\n",
      "Training Epoch 23  93.1% | batch:       639 of       686\t|\tloss: 1.22066\n",
      "Training Epoch 23  93.3% | batch:       640 of       686\t|\tloss: 1.21123\n",
      "Training Epoch 23  93.4% | batch:       641 of       686\t|\tloss: 1.06508\n",
      "Training Epoch 23  93.6% | batch:       642 of       686\t|\tloss: 1.36514\n",
      "Training Epoch 23  93.7% | batch:       643 of       686\t|\tloss: 1.1503\n",
      "Training Epoch 23  93.9% | batch:       644 of       686\t|\tloss: 1.24135\n",
      "Training Epoch 23  94.0% | batch:       645 of       686\t|\tloss: 1.17401\n",
      "Training Epoch 23  94.2% | batch:       646 of       686\t|\tloss: 1.22842\n",
      "Training Epoch 23  94.3% | batch:       647 of       686\t|\tloss: 1.41967\n",
      "Training Epoch 23  94.5% | batch:       648 of       686\t|\tloss: 1.49246\n",
      "Training Epoch 23  94.6% | batch:       649 of       686\t|\tloss: 0.999257\n",
      "Training Epoch 23  94.8% | batch:       650 of       686\t|\tloss: 1.11199\n",
      "Training Epoch 23  94.9% | batch:       651 of       686\t|\tloss: 0.930739\n",
      "Training Epoch 23  95.0% | batch:       652 of       686\t|\tloss: 0.732145\n",
      "Training Epoch 23  95.2% | batch:       653 of       686\t|\tloss: 0.784854\n",
      "Training Epoch 23  95.3% | batch:       654 of       686\t|\tloss: 0.896823\n",
      "Training Epoch 23  95.5% | batch:       655 of       686\t|\tloss: 1.244\n",
      "Training Epoch 23  95.6% | batch:       656 of       686\t|\tloss: 1.1237\n",
      "Training Epoch 23  95.8% | batch:       657 of       686\t|\tloss: 1.21627\n",
      "Training Epoch 23  95.9% | batch:       658 of       686\t|\tloss: 0.992028\n",
      "Training Epoch 23  96.1% | batch:       659 of       686\t|\tloss: 1.23385\n",
      "Training Epoch 23  96.2% | batch:       660 of       686\t|\tloss: 1.02139\n",
      "Training Epoch 23  96.4% | batch:       661 of       686\t|\tloss: 1.2393\n",
      "Training Epoch 23  96.5% | batch:       662 of       686\t|\tloss: 1.21611\n",
      "Training Epoch 23  96.6% | batch:       663 of       686\t|\tloss: 1.12928\n",
      "Training Epoch 23  96.8% | batch:       664 of       686\t|\tloss: 1.23643\n",
      "Training Epoch 23  96.9% | batch:       665 of       686\t|\tloss: 1.16228\n",
      "Training Epoch 23  97.1% | batch:       666 of       686\t|\tloss: 1.04758\n",
      "Training Epoch 23  97.2% | batch:       667 of       686\t|\tloss: 0.83571\n",
      "Training Epoch 23  97.4% | batch:       668 of       686\t|\tloss: 1.14889\n",
      "Training Epoch 23  97.5% | batch:       669 of       686\t|\tloss: 1.18319\n",
      "Training Epoch 23  97.7% | batch:       670 of       686\t|\tloss: 1.3247\n",
      "Training Epoch 23  97.8% | batch:       671 of       686\t|\tloss: 1.10349\n",
      "Training Epoch 23  98.0% | batch:       672 of       686\t|\tloss: 1.18298\n",
      "Training Epoch 23  98.1% | batch:       673 of       686\t|\tloss: 1.18331\n",
      "Training Epoch 23  98.3% | batch:       674 of       686\t|\tloss: 0.808252\n",
      "Training Epoch 23  98.4% | batch:       675 of       686\t|\tloss: 1.03449\n",
      "Training Epoch 23  98.5% | batch:       676 of       686\t|\tloss: 1.16404\n",
      "Training Epoch 23  98.7% | batch:       677 of       686\t|\tloss: 1.01612\n",
      "Training Epoch 23  98.8% | batch:       678 of       686\t|\tloss: 1.26267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 23  99.0% | batch:       679 of       686\t|\tloss: 0.988999\n",
      "Training Epoch 23  99.1% | batch:       680 of       686\t|\tloss: 0.927436\n",
      "Training Epoch 23  99.3% | batch:       681 of       686\t|\tloss: 1.27568\n",
      "Training Epoch 23  99.4% | batch:       682 of       686\t|\tloss: 1.12142\n",
      "Training Epoch 23  99.6% | batch:       683 of       686\t|\tloss: 0.97611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:10:32,752 | INFO : Epoch 23 Training Summary: epoch: 23.000000 | loss: 1.171991 | \n",
      "2023-05-25 22:10:32,753 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 25.90280270576477 seconds\n",
      "\n",
      "2023-05-25 22:10:32,754 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.975779968759287 seconds\n",
      "2023-05-25 22:10:32,755 | INFO : Avg batch train. time: 0.034950116572535404 seconds\n",
      "2023-05-25 22:10:32,755 | INFO : Avg sample train. time: 0.0002733996233395209 seconds\n",
      "2023-05-25 22:10:32,756 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 23  99.7% | batch:       684 of       686\t|\tloss: 1.15452\n",
      "Training Epoch 23  99.9% | batch:       685 of       686\t|\tloss: 0.752412\n",
      "\n",
      "Evaluating Epoch 23   0.0% | batch:         0 of       172\t|\tloss: 1.1425\n",
      "Evaluating Epoch 23   0.6% | batch:         1 of       172\t|\tloss: 1.38573\n",
      "Evaluating Epoch 23   1.2% | batch:         2 of       172\t|\tloss: 0.770983\n",
      "Evaluating Epoch 23   1.7% | batch:         3 of       172\t|\tloss: 2.28606\n",
      "Evaluating Epoch 23   2.3% | batch:         4 of       172\t|\tloss: 1.25438\n",
      "Evaluating Epoch 23   2.9% | batch:         5 of       172\t|\tloss: 1.14798\n",
      "Evaluating Epoch 23   3.5% | batch:         6 of       172\t|\tloss: 1.19107\n",
      "Evaluating Epoch 23   4.1% | batch:         7 of       172\t|\tloss: 2.78949\n",
      "Evaluating Epoch 23   4.7% | batch:         8 of       172\t|\tloss: 0.666944\n",
      "Evaluating Epoch 23   5.2% | batch:         9 of       172\t|\tloss: 1.50015\n",
      "Evaluating Epoch 23   5.8% | batch:        10 of       172\t|\tloss: 1.25371\n",
      "Evaluating Epoch 23   6.4% | batch:        11 of       172\t|\tloss: 1.159\n",
      "Evaluating Epoch 23   7.0% | batch:        12 of       172\t|\tloss: 1.33176\n",
      "Evaluating Epoch 23   7.6% | batch:        13 of       172\t|\tloss: 1.24265\n",
      "Evaluating Epoch 23   8.1% | batch:        14 of       172\t|\tloss: 1.72817\n",
      "Evaluating Epoch 23   8.7% | batch:        15 of       172\t|\tloss: 1.2288\n",
      "Evaluating Epoch 23   9.3% | batch:        16 of       172\t|\tloss: 1.98775\n",
      "Evaluating Epoch 23   9.9% | batch:        17 of       172\t|\tloss: 0.824069\n",
      "Evaluating Epoch 23  10.5% | batch:        18 of       172\t|\tloss: 16.2791\n",
      "Evaluating Epoch 23  11.0% | batch:        19 of       172\t|\tloss: 1.28018\n",
      "Evaluating Epoch 23  11.6% | batch:        20 of       172\t|\tloss: 3.27257\n",
      "Evaluating Epoch 23  12.2% | batch:        21 of       172\t|\tloss: 0.619264\n",
      "Evaluating Epoch 23  12.8% | batch:        22 of       172\t|\tloss: 3.45226\n",
      "Evaluating Epoch 23  13.4% | batch:        23 of       172\t|\tloss: 2.43862\n",
      "Evaluating Epoch 23  14.0% | batch:        24 of       172\t|\tloss: 1.7294\n",
      "Evaluating Epoch 23  14.5% | batch:        25 of       172\t|\tloss: 3.44658\n",
      "Evaluating Epoch 23  15.1% | batch:        26 of       172\t|\tloss: 7.28486\n",
      "Evaluating Epoch 23  15.7% | batch:        27 of       172\t|\tloss: 14.8079\n",
      "Evaluating Epoch 23  16.3% | batch:        28 of       172\t|\tloss: 0.532048\n",
      "Evaluating Epoch 23  16.9% | batch:        29 of       172\t|\tloss: 2.71334\n",
      "Evaluating Epoch 23  17.4% | batch:        30 of       172\t|\tloss: 0.822491\n",
      "Evaluating Epoch 23  18.0% | batch:        31 of       172\t|\tloss: 0.288441\n",
      "Evaluating Epoch 23  18.6% | batch:        32 of       172\t|\tloss: 0.731438\n",
      "Evaluating Epoch 23  19.2% | batch:        33 of       172\t|\tloss: 0.575774\n",
      "Evaluating Epoch 23  19.8% | batch:        34 of       172\t|\tloss: 0.547154\n",
      "Evaluating Epoch 23  20.3% | batch:        35 of       172\t|\tloss: 0.267907\n",
      "Evaluating Epoch 23  20.9% | batch:        36 of       172\t|\tloss: 2.72438\n",
      "Evaluating Epoch 23  21.5% | batch:        37 of       172\t|\tloss: 3.15037\n",
      "Evaluating Epoch 23  22.1% | batch:        38 of       172\t|\tloss: 3.10188\n",
      "Evaluating Epoch 23  22.7% | batch:        39 of       172\t|\tloss: 6.68036\n",
      "Evaluating Epoch 23  23.3% | batch:        40 of       172\t|\tloss: 0.916407\n",
      "Evaluating Epoch 23  23.8% | batch:        41 of       172\t|\tloss: 1.86652\n",
      "Evaluating Epoch 23  24.4% | batch:        42 of       172\t|\tloss: 0.507347\n",
      "Evaluating Epoch 23  25.0% | batch:        43 of       172\t|\tloss: 17.8173\n",
      "Evaluating Epoch 23  25.6% | batch:        44 of       172\t|\tloss: 1.3025\n",
      "Evaluating Epoch 23  26.2% | batch:        45 of       172\t|\tloss: 1.81431\n",
      "Evaluating Epoch 23  26.7% | batch:        46 of       172\t|\tloss: 0.228842\n",
      "Evaluating Epoch 23  27.3% | batch:        47 of       172\t|\tloss: 0.858035\n",
      "Evaluating Epoch 23  27.9% | batch:        48 of       172\t|\tloss: 0.193818\n",
      "Evaluating Epoch 23  28.5% | batch:        49 of       172\t|\tloss: 0.899771\n",
      "Evaluating Epoch 23  29.1% | batch:        50 of       172\t|\tloss: 0.488067\n",
      "Evaluating Epoch 23  29.7% | batch:        51 of       172\t|\tloss: 0.671976\n",
      "Evaluating Epoch 23  30.2% | batch:        52 of       172\t|\tloss: 0.345431\n",
      "Evaluating Epoch 23  30.8% | batch:        53 of       172\t|\tloss: 2.43294\n",
      "Evaluating Epoch 23  31.4% | batch:        54 of       172\t|\tloss: 0.973275\n",
      "Evaluating Epoch 23  32.0% | batch:        55 of       172\t|\tloss: 0.398083\n",
      "Evaluating Epoch 23  32.6% | batch:        56 of       172\t|\tloss: 2.81317\n",
      "Evaluating Epoch 23  33.1% | batch:        57 of       172\t|\tloss: 0.521348\n",
      "Evaluating Epoch 23  33.7% | batch:        58 of       172\t|\tloss: 1.93345\n",
      "Evaluating Epoch 23  34.3% | batch:        59 of       172\t|\tloss: 0.994264\n",
      "Evaluating Epoch 23  34.9% | batch:        60 of       172\t|\tloss: 0.938993\n",
      "Evaluating Epoch 23  35.5% | batch:        61 of       172\t|\tloss: 1.65539\n",
      "Evaluating Epoch 23  36.0% | batch:        62 of       172\t|\tloss: 0.713841\n",
      "Evaluating Epoch 23  36.6% | batch:        63 of       172\t|\tloss: 2.64279\n",
      "Evaluating Epoch 23  37.2% | batch:        64 of       172\t|\tloss: 0.513407\n",
      "Evaluating Epoch 23  37.8% | batch:        65 of       172\t|\tloss: 1.97843\n",
      "Evaluating Epoch 23  38.4% | batch:        66 of       172\t|\tloss: 1.51979\n",
      "Evaluating Epoch 23  39.0% | batch:        67 of       172\t|\tloss: 0.280973\n",
      "Evaluating Epoch 23  39.5% | batch:        68 of       172\t|\tloss: 1.97001\n",
      "Evaluating Epoch 23  40.1% | batch:        69 of       172\t|\tloss: 0.855628\n",
      "Evaluating Epoch 23  40.7% | batch:        70 of       172\t|\tloss: 1.47224\n",
      "Evaluating Epoch 23  41.3% | batch:        71 of       172\t|\tloss: 1.39742\n",
      "Evaluating Epoch 23  41.9% | batch:        72 of       172\t|\tloss: 0.524469\n",
      "Evaluating Epoch 23  42.4% | batch:        73 of       172\t|\tloss: 2.31091\n",
      "Evaluating Epoch 23  43.0% | batch:        74 of       172\t|\tloss: 0.774013\n",
      "Evaluating Epoch 23  43.6% | batch:        75 of       172\t|\tloss: 0.767876\n",
      "Evaluating Epoch 23  44.2% | batch:        76 of       172\t|\tloss: 0.735152\n",
      "Evaluating Epoch 23  44.8% | batch:        77 of       172\t|\tloss: 0.963561\n",
      "Evaluating Epoch 23  45.3% | batch:        78 of       172\t|\tloss: 0.858653\n",
      "Evaluating Epoch 23  45.9% | batch:        79 of       172\t|\tloss: 0.580634\n",
      "Evaluating Epoch 23  46.5% | batch:        80 of       172\t|\tloss: 0.805171\n",
      "Evaluating Epoch 23  47.1% | batch:        81 of       172\t|\tloss: 0.873218\n",
      "Evaluating Epoch 23  47.7% | batch:        82 of       172\t|\tloss: 0.703553\n",
      "Evaluating Epoch 23  48.3% | batch:        83 of       172\t|\tloss: 0.812123\n",
      "Evaluating Epoch 23  48.8% | batch:        84 of       172\t|\tloss: 0.324696\n",
      "Evaluating Epoch 23  49.4% | batch:        85 of       172\t|\tloss: 0.573648\n",
      "Evaluating Epoch 23  50.0% | batch:        86 of       172\t|\tloss: 0.691592\n",
      "Evaluating Epoch 23  50.6% | batch:        87 of       172\t|\tloss: 0.595469\n",
      "Evaluating Epoch 23  51.2% | batch:        88 of       172\t|\tloss: 0.273522\n",
      "Evaluating Epoch 23  51.7% | batch:        89 of       172\t|\tloss: 0.778516\n",
      "Evaluating Epoch 23  52.3% | batch:        90 of       172\t|\tloss: 0.743001\n",
      "Evaluating Epoch 23  52.9% | batch:        91 of       172\t|\tloss: 0.155593\n",
      "Evaluating Epoch 23  53.5% | batch:        92 of       172\t|\tloss: 0.452075\n",
      "Evaluating Epoch 23  54.1% | batch:        93 of       172\t|\tloss: 1.06882\n",
      "Evaluating Epoch 23  54.7% | batch:        94 of       172\t|\tloss: 0.460962\n",
      "Evaluating Epoch 23  55.2% | batch:        95 of       172\t|\tloss: 0.412709\n",
      "Evaluating Epoch 23  55.8% | batch:        96 of       172\t|\tloss: 0.903693\n",
      "Evaluating Epoch 23  56.4% | batch:        97 of       172\t|\tloss: 0.544225\n",
      "Evaluating Epoch 23  57.0% | batch:        98 of       172\t|\tloss: 0.334802\n",
      "Evaluating Epoch 23  57.6% | batch:        99 of       172\t|\tloss: 0.534097\n",
      "Evaluating Epoch 23  58.1% | batch:       100 of       172\t|\tloss: 0.644001\n",
      "Evaluating Epoch 23  58.7% | batch:       101 of       172\t|\tloss: 0.281551\n",
      "Evaluating Epoch 23  59.3% | batch:       102 of       172\t|\tloss: 0.49599\n",
      "Evaluating Epoch 23  59.9% | batch:       103 of       172\t|\tloss: 1.14283\n",
      "Evaluating Epoch 23  60.5% | batch:       104 of       172\t|\tloss: 0.55643\n",
      "Evaluating Epoch 23  61.0% | batch:       105 of       172\t|\tloss: 0.307204\n",
      "Evaluating Epoch 23  61.6% | batch:       106 of       172\t|\tloss: 0.583089\n",
      "Evaluating Epoch 23  62.2% | batch:       107 of       172\t|\tloss: 1.16979\n",
      "Evaluating Epoch 23  62.8% | batch:       108 of       172\t|\tloss: 0.294577\n",
      "Evaluating Epoch 23  63.4% | batch:       109 of       172\t|\tloss: 0.428827\n",
      "Evaluating Epoch 23  64.0% | batch:       110 of       172\t|\tloss: 1.03004\n",
      "Evaluating Epoch 23  64.5% | batch:       111 of       172\t|\tloss: 0.539275\n",
      "Evaluating Epoch 23  65.1% | batch:       112 of       172\t|\tloss: 0.32588\n",
      "Evaluating Epoch 23  65.7% | batch:       113 of       172\t|\tloss: 0.950552\n",
      "Evaluating Epoch 23  66.3% | batch:       114 of       172\t|\tloss: 1.10111\n",
      "Evaluating Epoch 23  66.9% | batch:       115 of       172\t|\tloss: 1.14615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 23  67.4% | batch:       116 of       172\t|\tloss: 0.873434\n",
      "Evaluating Epoch 23  68.0% | batch:       117 of       172\t|\tloss: 1.05211\n",
      "Evaluating Epoch 23  68.6% | batch:       118 of       172\t|\tloss: 0.376815\n",
      "Evaluating Epoch 23  69.2% | batch:       119 of       172\t|\tloss: 0.773729\n",
      "Evaluating Epoch 23  69.8% | batch:       120 of       172\t|\tloss: 0.266062\n",
      "Evaluating Epoch 23  70.3% | batch:       121 of       172\t|\tloss: 0.664956\n",
      "Evaluating Epoch 23  70.9% | batch:       122 of       172\t|\tloss: 0.676334\n",
      "Evaluating Epoch 23  71.5% | batch:       123 of       172\t|\tloss: 0.516871\n",
      "Evaluating Epoch 23  72.1% | batch:       124 of       172\t|\tloss: 1.41288\n",
      "Evaluating Epoch 23  72.7% | batch:       125 of       172\t|\tloss: 0.93189\n",
      "Evaluating Epoch 23  73.3% | batch:       126 of       172\t|\tloss: 0.938108\n",
      "Evaluating Epoch 23  73.8% | batch:       127 of       172\t|\tloss: 0.618009\n",
      "Evaluating Epoch 23  74.4% | batch:       128 of       172\t|\tloss: 0.426635\n",
      "Evaluating Epoch 23  75.0% | batch:       129 of       172\t|\tloss: 0.813571\n",
      "Evaluating Epoch 23  75.6% | batch:       130 of       172\t|\tloss: 0.198576\n",
      "Evaluating Epoch 23  76.2% | batch:       131 of       172\t|\tloss: 0.892178\n",
      "Evaluating Epoch 23  76.7% | batch:       132 of       172\t|\tloss: 0.486601\n",
      "Evaluating Epoch 23  77.3% | batch:       133 of       172\t|\tloss: 0.184838\n",
      "Evaluating Epoch 23  77.9% | batch:       134 of       172\t|\tloss: 0.217357\n",
      "Evaluating Epoch 23  78.5% | batch:       135 of       172\t|\tloss: 0.181812\n",
      "Evaluating Epoch 23  79.1% | batch:       136 of       172\t|\tloss: 0.162068\n",
      "Evaluating Epoch 23  79.7% | batch:       137 of       172\t|\tloss: 0.107966\n",
      "Evaluating Epoch 23  80.2% | batch:       138 of       172\t|\tloss: 0.308845\n",
      "Evaluating Epoch 23  80.8% | batch:       139 of       172\t|\tloss: 0.211803\n",
      "Evaluating Epoch 23  81.4% | batch:       140 of       172\t|\tloss: 0.225225\n",
      "Evaluating Epoch 23  82.0% | batch:       141 of       172\t|\tloss: 0.217526\n",
      "Evaluating Epoch 23  82.6% | batch:       142 of       172\t|\tloss: 0.294424\n",
      "Evaluating Epoch 23  83.1% | batch:       143 of       172\t|\tloss: 0.165446\n",
      "Evaluating Epoch 23  83.7% | batch:       144 of       172\t|\tloss: 0.229329\n",
      "Evaluating Epoch 23  84.3% | batch:       145 of       172\t|\tloss: 0.127574\n",
      "Evaluating Epoch 23  84.9% | batch:       146 of       172\t|\tloss: 0.270518\n",
      "Evaluating Epoch 23  85.5% | batch:       147 of       172\t|\tloss: 0.169238\n",
      "Evaluating Epoch 23  86.0% | batch:       148 of       172\t|\tloss: 0.238801\n",
      "Evaluating Epoch 23  86.6% | batch:       149 of       172\t|\tloss: 0.0813243\n",
      "Evaluating Epoch 23  87.2% | batch:       150 of       172\t|\tloss: 0.392289\n",
      "Evaluating Epoch 23  87.8% | batch:       151 of       172\t|\tloss: 0.59023\n",
      "Evaluating Epoch 23  88.4% | batch:       152 of       172\t|\tloss: 0.342648\n",
      "Evaluating Epoch 23  89.0% | batch:       153 of       172\t|\tloss: 0.439207\n",
      "Evaluating Epoch 23  89.5% | batch:       154 of       172\t|\tloss: 0.518308\n",
      "Evaluating Epoch 23  90.1% | batch:       155 of       172\t|\tloss: 0.260811\n",
      "Evaluating Epoch 23  90.7% | batch:       156 of       172\t|\tloss: 0.612949\n",
      "Evaluating Epoch 23  91.3% | batch:       157 of       172\t|\tloss: 0.500498\n",
      "Evaluating Epoch 23  91.9% | batch:       158 of       172\t|\tloss: 0.430418\n",
      "Evaluating Epoch 23  92.4% | batch:       159 of       172\t|\tloss: 0.822726\n",
      "Evaluating Epoch 23  93.0% | batch:       160 of       172\t|\tloss: 0.501133\n",
      "Evaluating Epoch 23  93.6% | batch:       161 of       172\t|\tloss: 0.928616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:10:37,452 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 4.694823741912842 seconds\n",
      "\n",
      "2023-05-25 22:10:37,456 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.050796548525493 seconds\n",
      "2023-05-25 22:10:37,457 | INFO : Avg batch val. time: 0.02355114272398542 seconds\n",
      "2023-05-25 22:10:37,458 | INFO : Avg sample val. time: 0.0001844877054481711 seconds\n",
      "2023-05-25 22:10:37,463 | INFO : Epoch 23 Validation Summary: epoch: 23.000000 | loss: 1.239087 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 23  94.2% | batch:       162 of       172\t|\tloss: 0.539414\n",
      "Evaluating Epoch 23  94.8% | batch:       163 of       172\t|\tloss: 0.310068\n",
      "Evaluating Epoch 23  95.3% | batch:       164 of       172\t|\tloss: 0.632755\n",
      "Evaluating Epoch 23  95.9% | batch:       165 of       172\t|\tloss: 0.459598\n",
      "Evaluating Epoch 23  96.5% | batch:       166 of       172\t|\tloss: 0.295158\n",
      "Evaluating Epoch 23  97.1% | batch:       167 of       172\t|\tloss: 0.70349\n",
      "Evaluating Epoch 23  97.7% | batch:       168 of       172\t|\tloss: 0.43673\n",
      "Evaluating Epoch 23  98.3% | batch:       169 of       172\t|\tloss: 0.473497\n",
      "Evaluating Epoch 23  98.8% | batch:       170 of       172\t|\tloss: 0.648558\n",
      "Evaluating Epoch 23  99.4% | batch:       171 of       172\t|\tloss: 0.542519\n",
      "\n",
      "Training Epoch 24   0.0% | batch:         0 of       686\t|\tloss: 1.00482\n",
      "Training Epoch 24   0.1% | batch:         1 of       686\t|\tloss: 1.26671\n",
      "Training Epoch 24   0.3% | batch:         2 of       686\t|\tloss: 1.30256\n",
      "Training Epoch 24   0.4% | batch:         3 of       686\t|\tloss: 1.25652\n",
      "Training Epoch 24   0.6% | batch:         4 of       686\t|\tloss: 1.18804\n",
      "Training Epoch 24   0.7% | batch:         5 of       686\t|\tloss: 1.01258\n",
      "Training Epoch 24   0.9% | batch:         6 of       686\t|\tloss: 0.987316\n",
      "Training Epoch 24   1.0% | batch:         7 of       686\t|\tloss: 1.11436\n",
      "Training Epoch 24   1.2% | batch:         8 of       686\t|\tloss: 0.852766\n",
      "Training Epoch 24   1.3% | batch:         9 of       686\t|\tloss: 1.27768\n",
      "Training Epoch 24   1.5% | batch:        10 of       686\t|\tloss: 1.03397\n",
      "Training Epoch 24   1.6% | batch:        11 of       686\t|\tloss: 1.20013\n",
      "Training Epoch 24   1.7% | batch:        12 of       686\t|\tloss: 1.02466\n",
      "Training Epoch 24   1.9% | batch:        13 of       686\t|\tloss: 1.20012\n",
      "Training Epoch 24   2.0% | batch:        14 of       686\t|\tloss: 1.09418\n",
      "Training Epoch 24   2.2% | batch:        15 of       686\t|\tloss: 1.0854\n",
      "Training Epoch 24   2.3% | batch:        16 of       686\t|\tloss: 1.00028\n",
      "Training Epoch 24   2.5% | batch:        17 of       686\t|\tloss: 0.98868\n",
      "Training Epoch 24   2.6% | batch:        18 of       686\t|\tloss: 0.897664\n",
      "Training Epoch 24   2.8% | batch:        19 of       686\t|\tloss: 1.06618\n",
      "Training Epoch 24   2.9% | batch:        20 of       686\t|\tloss: 0.934837\n",
      "Training Epoch 24   3.1% | batch:        21 of       686\t|\tloss: 1.35631\n",
      "Training Epoch 24   3.2% | batch:        22 of       686\t|\tloss: 1.25784\n",
      "Training Epoch 24   3.4% | batch:        23 of       686\t|\tloss: 1.18775\n",
      "Training Epoch 24   3.5% | batch:        24 of       686\t|\tloss: 0.947853\n",
      "Training Epoch 24   3.6% | batch:        25 of       686\t|\tloss: 1.05319\n",
      "Training Epoch 24   3.8% | batch:        26 of       686\t|\tloss: 0.844035\n",
      "Training Epoch 24   3.9% | batch:        27 of       686\t|\tloss: 1.78018\n",
      "Training Epoch 24   4.1% | batch:        28 of       686\t|\tloss: 1.03065\n",
      "Training Epoch 24   4.2% | batch:        29 of       686\t|\tloss: 1.27268\n",
      "Training Epoch 24   4.4% | batch:        30 of       686\t|\tloss: 0.958785\n",
      "Training Epoch 24   4.5% | batch:        31 of       686\t|\tloss: 1.21952\n",
      "Training Epoch 24   4.7% | batch:        32 of       686\t|\tloss: 1.15925\n",
      "Training Epoch 24   4.8% | batch:        33 of       686\t|\tloss: 0.827192\n",
      "Training Epoch 24   5.0% | batch:        34 of       686\t|\tloss: 1.14136\n",
      "Training Epoch 24   5.1% | batch:        35 of       686\t|\tloss: 1.19339\n",
      "Training Epoch 24   5.2% | batch:        36 of       686\t|\tloss: 0.838407\n",
      "Training Epoch 24   5.4% | batch:        37 of       686\t|\tloss: 1.31501\n",
      "Training Epoch 24   5.5% | batch:        38 of       686\t|\tloss: 1.17058\n",
      "Training Epoch 24   5.7% | batch:        39 of       686\t|\tloss: 1.03935\n",
      "Training Epoch 24   5.8% | batch:        40 of       686\t|\tloss: 0.947903\n",
      "Training Epoch 24   6.0% | batch:        41 of       686\t|\tloss: 1.18406\n",
      "Training Epoch 24   6.1% | batch:        42 of       686\t|\tloss: 1.06025\n",
      "Training Epoch 24   6.3% | batch:        43 of       686\t|\tloss: 1.19921\n",
      "Training Epoch 24   6.4% | batch:        44 of       686\t|\tloss: 1.07726\n",
      "Training Epoch 24   6.6% | batch:        45 of       686\t|\tloss: 0.832102\n",
      "Training Epoch 24   6.7% | batch:        46 of       686\t|\tloss: 1.1279\n",
      "Training Epoch 24   6.9% | batch:        47 of       686\t|\tloss: 0.928557\n",
      "Training Epoch 24   7.0% | batch:        48 of       686\t|\tloss: 1.15335\n",
      "Training Epoch 24   7.1% | batch:        49 of       686\t|\tloss: 1.0925\n",
      "Training Epoch 24   7.3% | batch:        50 of       686\t|\tloss: 0.96245\n",
      "Training Epoch 24   7.4% | batch:        51 of       686\t|\tloss: 1.08501\n",
      "Training Epoch 24   7.6% | batch:        52 of       686\t|\tloss: 0.986169\n",
      "Training Epoch 24   7.7% | batch:        53 of       686\t|\tloss: 1.04256\n",
      "Training Epoch 24   7.9% | batch:        54 of       686\t|\tloss: 1.43491\n",
      "Training Epoch 24   8.0% | batch:        55 of       686\t|\tloss: 1.08019\n",
      "Training Epoch 24   8.2% | batch:        56 of       686\t|\tloss: 1.58587\n",
      "Training Epoch 24   8.3% | batch:        57 of       686\t|\tloss: 1.25591\n",
      "Training Epoch 24   8.5% | batch:        58 of       686\t|\tloss: 1.37086\n",
      "Training Epoch 24   8.6% | batch:        59 of       686\t|\tloss: 1.06291\n",
      "Training Epoch 24   8.7% | batch:        60 of       686\t|\tloss: 1.10489\n",
      "Training Epoch 24   8.9% | batch:        61 of       686\t|\tloss: 1.03637\n",
      "Training Epoch 24   9.0% | batch:        62 of       686\t|\tloss: 1.15677\n",
      "Training Epoch 24   9.2% | batch:        63 of       686\t|\tloss: 1.35045\n",
      "Training Epoch 24   9.3% | batch:        64 of       686\t|\tloss: 0.944614\n",
      "Training Epoch 24   9.5% | batch:        65 of       686\t|\tloss: 0.909809\n",
      "Training Epoch 24   9.6% | batch:        66 of       686\t|\tloss: 1.1392\n",
      "Training Epoch 24   9.8% | batch:        67 of       686\t|\tloss: 1.25241\n",
      "Training Epoch 24   9.9% | batch:        68 of       686\t|\tloss: 1.10917\n",
      "Training Epoch 24  10.1% | batch:        69 of       686\t|\tloss: 1.07827\n",
      "Training Epoch 24  10.2% | batch:        70 of       686\t|\tloss: 1.13428\n",
      "Training Epoch 24  10.3% | batch:        71 of       686\t|\tloss: 0.873857\n",
      "Training Epoch 24  10.5% | batch:        72 of       686\t|\tloss: 0.976148\n",
      "Training Epoch 24  10.6% | batch:        73 of       686\t|\tloss: 1.13813\n",
      "Training Epoch 24  10.8% | batch:        74 of       686\t|\tloss: 1.03783\n",
      "Training Epoch 24  10.9% | batch:        75 of       686\t|\tloss: 1.07105\n",
      "Training Epoch 24  11.1% | batch:        76 of       686\t|\tloss: 0.979141\n",
      "Training Epoch 24  11.2% | batch:        77 of       686\t|\tloss: 1.29048\n",
      "Training Epoch 24  11.4% | batch:        78 of       686\t|\tloss: 1.27244\n",
      "Training Epoch 24  11.5% | batch:        79 of       686\t|\tloss: 1.1414\n",
      "Training Epoch 24  11.7% | batch:        80 of       686\t|\tloss: 0.796654\n",
      "Training Epoch 24  11.8% | batch:        81 of       686\t|\tloss: 1.26915\n",
      "Training Epoch 24  12.0% | batch:        82 of       686\t|\tloss: 1.06633\n",
      "Training Epoch 24  12.1% | batch:        83 of       686\t|\tloss: 1.26212\n",
      "Training Epoch 24  12.2% | batch:        84 of       686\t|\tloss: 1.07209\n",
      "Training Epoch 24  12.4% | batch:        85 of       686\t|\tloss: 1.00248\n",
      "Training Epoch 24  12.5% | batch:        86 of       686\t|\tloss: 1.08065\n",
      "Training Epoch 24  12.7% | batch:        87 of       686\t|\tloss: 1.01462\n",
      "Training Epoch 24  12.8% | batch:        88 of       686\t|\tloss: 1.54744\n",
      "Training Epoch 24  13.0% | batch:        89 of       686\t|\tloss: 1.21604\n",
      "Training Epoch 24  13.1% | batch:        90 of       686\t|\tloss: 0.986187\n",
      "Training Epoch 24  13.3% | batch:        91 of       686\t|\tloss: 0.819363\n",
      "Training Epoch 24  13.4% | batch:        92 of       686\t|\tloss: 1.17857\n",
      "Training Epoch 24  13.6% | batch:        93 of       686\t|\tloss: 1.0762\n",
      "Training Epoch 24  13.7% | batch:        94 of       686\t|\tloss: 1.25326\n",
      "Training Epoch 24  13.8% | batch:        95 of       686\t|\tloss: 1.442\n",
      "Training Epoch 24  14.0% | batch:        96 of       686\t|\tloss: 0.885955\n",
      "Training Epoch 24  14.1% | batch:        97 of       686\t|\tloss: 1.17249\n",
      "Training Epoch 24  14.3% | batch:        98 of       686\t|\tloss: 1.16304\n",
      "Training Epoch 24  14.4% | batch:        99 of       686\t|\tloss: 1.07263\n",
      "Training Epoch 24  14.6% | batch:       100 of       686\t|\tloss: 1.08201\n",
      "Training Epoch 24  14.7% | batch:       101 of       686\t|\tloss: 0.877475\n",
      "Training Epoch 24  14.9% | batch:       102 of       686\t|\tloss: 1.00348\n",
      "Training Epoch 24  15.0% | batch:       103 of       686\t|\tloss: 0.843176\n",
      "Training Epoch 24  15.2% | batch:       104 of       686\t|\tloss: 1.30483\n",
      "Training Epoch 24  15.3% | batch:       105 of       686\t|\tloss: 0.848269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 24  15.5% | batch:       106 of       686\t|\tloss: 1.19026\n",
      "Training Epoch 24  15.6% | batch:       107 of       686\t|\tloss: 1.11477\n",
      "Training Epoch 24  15.7% | batch:       108 of       686\t|\tloss: 0.876002\n",
      "Training Epoch 24  15.9% | batch:       109 of       686\t|\tloss: 1.19203\n",
      "Training Epoch 24  16.0% | batch:       110 of       686\t|\tloss: 0.845678\n",
      "Training Epoch 24  16.2% | batch:       111 of       686\t|\tloss: 0.880366\n",
      "Training Epoch 24  16.3% | batch:       112 of       686\t|\tloss: 1.14794\n",
      "Training Epoch 24  16.5% | batch:       113 of       686\t|\tloss: 0.960822\n",
      "Training Epoch 24  16.6% | batch:       114 of       686\t|\tloss: 1.11051\n",
      "Training Epoch 24  16.8% | batch:       115 of       686\t|\tloss: 1.10657\n",
      "Training Epoch 24  16.9% | batch:       116 of       686\t|\tloss: 1.24029\n",
      "Training Epoch 24  17.1% | batch:       117 of       686\t|\tloss: 0.738343\n",
      "Training Epoch 24  17.2% | batch:       118 of       686\t|\tloss: 0.813213\n",
      "Training Epoch 24  17.3% | batch:       119 of       686\t|\tloss: 1.35657\n",
      "Training Epoch 24  17.5% | batch:       120 of       686\t|\tloss: 1.27817\n",
      "Training Epoch 24  17.6% | batch:       121 of       686\t|\tloss: 1.12344\n",
      "Training Epoch 24  17.8% | batch:       122 of       686\t|\tloss: 1.07937\n",
      "Training Epoch 24  17.9% | batch:       123 of       686\t|\tloss: 1.54779\n",
      "Training Epoch 24  18.1% | batch:       124 of       686\t|\tloss: 0.922334\n",
      "Training Epoch 24  18.2% | batch:       125 of       686\t|\tloss: 1.11681\n",
      "Training Epoch 24  18.4% | batch:       126 of       686\t|\tloss: 1.17704\n",
      "Training Epoch 24  18.5% | batch:       127 of       686\t|\tloss: 1.05538\n",
      "Training Epoch 24  18.7% | batch:       128 of       686\t|\tloss: 0.960966\n",
      "Training Epoch 24  18.8% | batch:       129 of       686\t|\tloss: 1.31417\n",
      "Training Epoch 24  19.0% | batch:       130 of       686\t|\tloss: 1.07076\n",
      "Training Epoch 24  19.1% | batch:       131 of       686\t|\tloss: 1.17\n",
      "Training Epoch 24  19.2% | batch:       132 of       686\t|\tloss: 0.901544\n",
      "Training Epoch 24  19.4% | batch:       133 of       686\t|\tloss: 1.0703\n",
      "Training Epoch 24  19.5% | batch:       134 of       686\t|\tloss: 1.15526\n",
      "Training Epoch 24  19.7% | batch:       135 of       686\t|\tloss: 0.890268\n",
      "Training Epoch 24  19.8% | batch:       136 of       686\t|\tloss: 0.785622\n",
      "Training Epoch 24  20.0% | batch:       137 of       686\t|\tloss: 0.899307\n",
      "Training Epoch 24  20.1% | batch:       138 of       686\t|\tloss: 1.04787\n",
      "Training Epoch 24  20.3% | batch:       139 of       686\t|\tloss: 0.840928\n",
      "Training Epoch 24  20.4% | batch:       140 of       686\t|\tloss: 0.840402\n",
      "Training Epoch 24  20.6% | batch:       141 of       686\t|\tloss: 0.938281\n",
      "Training Epoch 24  20.7% | batch:       142 of       686\t|\tloss: 0.959877\n",
      "Training Epoch 24  20.8% | batch:       143 of       686\t|\tloss: 0.953767\n",
      "Training Epoch 24  21.0% | batch:       144 of       686\t|\tloss: 1.26764\n",
      "Training Epoch 24  21.1% | batch:       145 of       686\t|\tloss: 1.13629\n",
      "Training Epoch 24  21.3% | batch:       146 of       686\t|\tloss: 1.40207\n",
      "Training Epoch 24  21.4% | batch:       147 of       686\t|\tloss: 1.29323\n",
      "Training Epoch 24  21.6% | batch:       148 of       686\t|\tloss: 0.910663\n",
      "Training Epoch 24  21.7% | batch:       149 of       686\t|\tloss: 1.00029\n",
      "Training Epoch 24  21.9% | batch:       150 of       686\t|\tloss: 0.979116\n",
      "Training Epoch 24  22.0% | batch:       151 of       686\t|\tloss: 1.10078\n",
      "Training Epoch 24  22.2% | batch:       152 of       686\t|\tloss: 1.13854\n",
      "Training Epoch 24  22.3% | batch:       153 of       686\t|\tloss: 1.19245\n",
      "Training Epoch 24  22.4% | batch:       154 of       686\t|\tloss: 1.42226\n",
      "Training Epoch 24  22.6% | batch:       155 of       686\t|\tloss: 1.15211\n",
      "Training Epoch 24  22.7% | batch:       156 of       686\t|\tloss: 1.15807\n",
      "Training Epoch 24  22.9% | batch:       157 of       686\t|\tloss: 0.916976\n",
      "Training Epoch 24  23.0% | batch:       158 of       686\t|\tloss: 1.15464\n",
      "Training Epoch 24  23.2% | batch:       159 of       686\t|\tloss: 0.910974\n",
      "Training Epoch 24  23.3% | batch:       160 of       686\t|\tloss: 1.13846\n",
      "Training Epoch 24  23.5% | batch:       161 of       686\t|\tloss: 1.36714\n",
      "Training Epoch 24  23.6% | batch:       162 of       686\t|\tloss: 1.065\n",
      "Training Epoch 24  23.8% | batch:       163 of       686\t|\tloss: 0.971944\n",
      "Training Epoch 24  23.9% | batch:       164 of       686\t|\tloss: 0.923246\n",
      "Training Epoch 24  24.1% | batch:       165 of       686\t|\tloss: 0.752865\n",
      "Training Epoch 24  24.2% | batch:       166 of       686\t|\tloss: 1.20341\n",
      "Training Epoch 24  24.3% | batch:       167 of       686\t|\tloss: 1.13422\n",
      "Training Epoch 24  24.5% | batch:       168 of       686\t|\tloss: 0.879892\n",
      "Training Epoch 24  24.6% | batch:       169 of       686\t|\tloss: 1.19067\n",
      "Training Epoch 24  24.8% | batch:       170 of       686\t|\tloss: 1.03304\n",
      "Training Epoch 24  24.9% | batch:       171 of       686\t|\tloss: 1.1085\n",
      "Training Epoch 24  25.1% | batch:       172 of       686\t|\tloss: 1.15921\n",
      "Training Epoch 24  25.2% | batch:       173 of       686\t|\tloss: 1.06915\n",
      "Training Epoch 24  25.4% | batch:       174 of       686\t|\tloss: 1.1735\n",
      "Training Epoch 24  25.5% | batch:       175 of       686\t|\tloss: 1.29731\n",
      "Training Epoch 24  25.7% | batch:       176 of       686\t|\tloss: 1.05668\n",
      "Training Epoch 24  25.8% | batch:       177 of       686\t|\tloss: 1.30113\n",
      "Training Epoch 24  25.9% | batch:       178 of       686\t|\tloss: 0.993035\n",
      "Training Epoch 24  26.1% | batch:       179 of       686\t|\tloss: 1.02932\n",
      "Training Epoch 24  26.2% | batch:       180 of       686\t|\tloss: 1.02156\n",
      "Training Epoch 24  26.4% | batch:       181 of       686\t|\tloss: 0.978421\n",
      "Training Epoch 24  26.5% | batch:       182 of       686\t|\tloss: 1.187\n",
      "Training Epoch 24  26.7% | batch:       183 of       686\t|\tloss: 1.02937\n",
      "Training Epoch 24  26.8% | batch:       184 of       686\t|\tloss: 1.20337\n",
      "Training Epoch 24  27.0% | batch:       185 of       686\t|\tloss: 1.21746\n",
      "Training Epoch 24  27.1% | batch:       186 of       686\t|\tloss: 1.30612\n",
      "Training Epoch 24  27.3% | batch:       187 of       686\t|\tloss: 0.931644\n",
      "Training Epoch 24  27.4% | batch:       188 of       686\t|\tloss: 0.976159\n",
      "Training Epoch 24  27.6% | batch:       189 of       686\t|\tloss: 1.17615\n",
      "Training Epoch 24  27.7% | batch:       190 of       686\t|\tloss: 0.994433\n",
      "Training Epoch 24  27.8% | batch:       191 of       686\t|\tloss: 0.956135\n",
      "Training Epoch 24  28.0% | batch:       192 of       686\t|\tloss: 0.907062\n",
      "Training Epoch 24  28.1% | batch:       193 of       686\t|\tloss: 1.2924\n",
      "Training Epoch 24  28.3% | batch:       194 of       686\t|\tloss: 0.964001\n",
      "Training Epoch 24  28.4% | batch:       195 of       686\t|\tloss: 1.16223\n",
      "Training Epoch 24  28.6% | batch:       196 of       686\t|\tloss: 0.860322\n",
      "Training Epoch 24  28.7% | batch:       197 of       686\t|\tloss: 1.20818\n",
      "Training Epoch 24  28.9% | batch:       198 of       686\t|\tloss: 1.07746\n",
      "Training Epoch 24  29.0% | batch:       199 of       686\t|\tloss: 1.38373\n",
      "Training Epoch 24  29.2% | batch:       200 of       686\t|\tloss: 0.963195\n",
      "Training Epoch 24  29.3% | batch:       201 of       686\t|\tloss: 0.838025\n",
      "Training Epoch 24  29.4% | batch:       202 of       686\t|\tloss: 1.22458\n",
      "Training Epoch 24  29.6% | batch:       203 of       686\t|\tloss: 1.21088\n",
      "Training Epoch 24  29.7% | batch:       204 of       686\t|\tloss: 0.854808\n",
      "Training Epoch 24  29.9% | batch:       205 of       686\t|\tloss: 1.06224\n",
      "Training Epoch 24  30.0% | batch:       206 of       686\t|\tloss: 1.15996\n",
      "Training Epoch 24  30.2% | batch:       207 of       686\t|\tloss: 0.974489\n",
      "Training Epoch 24  30.3% | batch:       208 of       686\t|\tloss: 1.36563\n",
      "Training Epoch 24  30.5% | batch:       209 of       686\t|\tloss: 1.15401\n",
      "Training Epoch 24  30.6% | batch:       210 of       686\t|\tloss: 1.15967\n",
      "Training Epoch 24  30.8% | batch:       211 of       686\t|\tloss: 1.21005\n",
      "Training Epoch 24  30.9% | batch:       212 of       686\t|\tloss: 1.19082\n",
      "Training Epoch 24  31.0% | batch:       213 of       686\t|\tloss: 0.98104\n",
      "Training Epoch 24  31.2% | batch:       214 of       686\t|\tloss: 1.06538\n",
      "Training Epoch 24  31.3% | batch:       215 of       686\t|\tloss: 1.03767\n",
      "Training Epoch 24  31.5% | batch:       216 of       686\t|\tloss: 0.873524\n",
      "Training Epoch 24  31.6% | batch:       217 of       686\t|\tloss: 1.0105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 24  31.8% | batch:       218 of       686\t|\tloss: 1.03564\n",
      "Training Epoch 24  31.9% | batch:       219 of       686\t|\tloss: 0.913487\n",
      "Training Epoch 24  32.1% | batch:       220 of       686\t|\tloss: 1.00833\n",
      "Training Epoch 24  32.2% | batch:       221 of       686\t|\tloss: 1.27215\n",
      "Training Epoch 24  32.4% | batch:       222 of       686\t|\tloss: 0.801525\n",
      "Training Epoch 24  32.5% | batch:       223 of       686\t|\tloss: 1.05731\n",
      "Training Epoch 24  32.7% | batch:       224 of       686\t|\tloss: 1.33536\n",
      "Training Epoch 24  32.8% | batch:       225 of       686\t|\tloss: 1.0921\n",
      "Training Epoch 24  32.9% | batch:       226 of       686\t|\tloss: 1.42558\n",
      "Training Epoch 24  33.1% | batch:       227 of       686\t|\tloss: 0.957893\n",
      "Training Epoch 24  33.2% | batch:       228 of       686\t|\tloss: 0.90617\n",
      "Training Epoch 24  33.4% | batch:       229 of       686\t|\tloss: 1.0393\n",
      "Training Epoch 24  33.5% | batch:       230 of       686\t|\tloss: 0.792278\n",
      "Training Epoch 24  33.7% | batch:       231 of       686\t|\tloss: 0.989654\n",
      "Training Epoch 24  33.8% | batch:       232 of       686\t|\tloss: 1.03754\n",
      "Training Epoch 24  34.0% | batch:       233 of       686\t|\tloss: 1.11856\n",
      "Training Epoch 24  34.1% | batch:       234 of       686\t|\tloss: 1.27643\n",
      "Training Epoch 24  34.3% | batch:       235 of       686\t|\tloss: 1.0374\n",
      "Training Epoch 24  34.4% | batch:       236 of       686\t|\tloss: 0.86669\n",
      "Training Epoch 24  34.5% | batch:       237 of       686\t|\tloss: 0.961806\n",
      "Training Epoch 24  34.7% | batch:       238 of       686\t|\tloss: 1.06934\n",
      "Training Epoch 24  34.8% | batch:       239 of       686\t|\tloss: 0.868785\n",
      "Training Epoch 24  35.0% | batch:       240 of       686\t|\tloss: 0.68205\n",
      "Training Epoch 24  35.1% | batch:       241 of       686\t|\tloss: 1.22145\n",
      "Training Epoch 24  35.3% | batch:       242 of       686\t|\tloss: 1.15258\n",
      "Training Epoch 24  35.4% | batch:       243 of       686\t|\tloss: 0.842836\n",
      "Training Epoch 24  35.6% | batch:       244 of       686\t|\tloss: 1.22568\n",
      "Training Epoch 24  35.7% | batch:       245 of       686\t|\tloss: 0.994873\n",
      "Training Epoch 24  35.9% | batch:       246 of       686\t|\tloss: 0.91199\n",
      "Training Epoch 24  36.0% | batch:       247 of       686\t|\tloss: 1.36715\n",
      "Training Epoch 24  36.2% | batch:       248 of       686\t|\tloss: 1.14\n",
      "Training Epoch 24  36.3% | batch:       249 of       686\t|\tloss: 0.893591\n",
      "Training Epoch 24  36.4% | batch:       250 of       686\t|\tloss: 1.06345\n",
      "Training Epoch 24  36.6% | batch:       251 of       686\t|\tloss: 1.05224\n",
      "Training Epoch 24  36.7% | batch:       252 of       686\t|\tloss: 0.902784\n",
      "Training Epoch 24  36.9% | batch:       253 of       686\t|\tloss: 0.864341\n",
      "Training Epoch 24  37.0% | batch:       254 of       686\t|\tloss: 0.890461\n",
      "Training Epoch 24  37.2% | batch:       255 of       686\t|\tloss: 0.950476\n",
      "Training Epoch 24  37.3% | batch:       256 of       686\t|\tloss: 1.1487\n",
      "Training Epoch 24  37.5% | batch:       257 of       686\t|\tloss: 0.984302\n",
      "Training Epoch 24  37.6% | batch:       258 of       686\t|\tloss: 1.62554\n",
      "Training Epoch 24  37.8% | batch:       259 of       686\t|\tloss: 1.29706\n",
      "Training Epoch 24  37.9% | batch:       260 of       686\t|\tloss: 1.23611\n",
      "Training Epoch 24  38.0% | batch:       261 of       686\t|\tloss: 0.86394\n",
      "Training Epoch 24  38.2% | batch:       262 of       686\t|\tloss: 1.46097\n",
      "Training Epoch 24  38.3% | batch:       263 of       686\t|\tloss: 1.29506\n",
      "Training Epoch 24  38.5% | batch:       264 of       686\t|\tloss: 0.899876\n",
      "Training Epoch 24  38.6% | batch:       265 of       686\t|\tloss: 1.11058\n",
      "Training Epoch 24  38.8% | batch:       266 of       686\t|\tloss: 1.17738\n",
      "Training Epoch 24  38.9% | batch:       267 of       686\t|\tloss: 1.15786\n",
      "Training Epoch 24  39.1% | batch:       268 of       686\t|\tloss: 0.894228\n",
      "Training Epoch 24  39.2% | batch:       269 of       686\t|\tloss: 0.892709\n",
      "Training Epoch 24  39.4% | batch:       270 of       686\t|\tloss: 1.2372\n",
      "Training Epoch 24  39.5% | batch:       271 of       686\t|\tloss: 1.12505\n",
      "Training Epoch 24  39.7% | batch:       272 of       686\t|\tloss: 1.06652\n",
      "Training Epoch 24  39.8% | batch:       273 of       686\t|\tloss: 0.913429\n",
      "Training Epoch 24  39.9% | batch:       274 of       686\t|\tloss: 1.11273\n",
      "Training Epoch 24  40.1% | batch:       275 of       686\t|\tloss: 1.40201\n",
      "Training Epoch 24  40.2% | batch:       276 of       686\t|\tloss: 0.908089\n",
      "Training Epoch 24  40.4% | batch:       277 of       686\t|\tloss: 1.26107\n",
      "Training Epoch 24  40.5% | batch:       278 of       686\t|\tloss: 1.37972\n",
      "Training Epoch 24  40.7% | batch:       279 of       686\t|\tloss: 1.20544\n",
      "Training Epoch 24  40.8% | batch:       280 of       686\t|\tloss: 0.90995\n",
      "Training Epoch 24  41.0% | batch:       281 of       686\t|\tloss: 0.953717\n",
      "Training Epoch 24  41.1% | batch:       282 of       686\t|\tloss: 1.03975\n",
      "Training Epoch 24  41.3% | batch:       283 of       686\t|\tloss: 1.25336\n",
      "Training Epoch 24  41.4% | batch:       284 of       686\t|\tloss: 0.692534\n",
      "Training Epoch 24  41.5% | batch:       285 of       686\t|\tloss: 1.16657\n",
      "Training Epoch 24  41.7% | batch:       286 of       686\t|\tloss: 1.24975\n",
      "Training Epoch 24  41.8% | batch:       287 of       686\t|\tloss: 1.13931\n",
      "Training Epoch 24  42.0% | batch:       288 of       686\t|\tloss: 1.29816\n",
      "Training Epoch 24  42.1% | batch:       289 of       686\t|\tloss: 1.17944\n",
      "Training Epoch 24  42.3% | batch:       290 of       686\t|\tloss: 0.907793\n",
      "Training Epoch 24  42.4% | batch:       291 of       686\t|\tloss: 1.02327\n",
      "Training Epoch 24  42.6% | batch:       292 of       686\t|\tloss: 1.16046\n",
      "Training Epoch 24  42.7% | batch:       293 of       686\t|\tloss: 1.02528\n",
      "Training Epoch 24  42.9% | batch:       294 of       686\t|\tloss: 0.947308\n",
      "Training Epoch 24  43.0% | batch:       295 of       686\t|\tloss: 1.02606\n",
      "Training Epoch 24  43.1% | batch:       296 of       686\t|\tloss: 1.39028\n",
      "Training Epoch 24  43.3% | batch:       297 of       686\t|\tloss: 0.998349\n",
      "Training Epoch 24  43.4% | batch:       298 of       686\t|\tloss: 1.14285\n",
      "Training Epoch 24  43.6% | batch:       299 of       686\t|\tloss: 0.851435\n",
      "Training Epoch 24  43.7% | batch:       300 of       686\t|\tloss: 0.957815\n",
      "Training Epoch 24  43.9% | batch:       301 of       686\t|\tloss: 1.33466\n",
      "Training Epoch 24  44.0% | batch:       302 of       686\t|\tloss: 1.14227\n",
      "Training Epoch 24  44.2% | batch:       303 of       686\t|\tloss: 1.00768\n",
      "Training Epoch 24  44.3% | batch:       304 of       686\t|\tloss: 0.780863\n",
      "Training Epoch 24  44.5% | batch:       305 of       686\t|\tloss: 0.853279\n",
      "Training Epoch 24  44.6% | batch:       306 of       686\t|\tloss: 1.03357\n",
      "Training Epoch 24  44.8% | batch:       307 of       686\t|\tloss: 1.1099\n",
      "Training Epoch 24  44.9% | batch:       308 of       686\t|\tloss: 0.969844\n",
      "Training Epoch 24  45.0% | batch:       309 of       686\t|\tloss: 0.95518\n",
      "Training Epoch 24  45.2% | batch:       310 of       686\t|\tloss: 1.20756\n",
      "Training Epoch 24  45.3% | batch:       311 of       686\t|\tloss: 0.928793\n",
      "Training Epoch 24  45.5% | batch:       312 of       686\t|\tloss: 0.881615\n",
      "Training Epoch 24  45.6% | batch:       313 of       686\t|\tloss: 1.00372\n",
      "Training Epoch 24  45.8% | batch:       314 of       686\t|\tloss: 1.16082\n",
      "Training Epoch 24  45.9% | batch:       315 of       686\t|\tloss: 1.33952\n",
      "Training Epoch 24  46.1% | batch:       316 of       686\t|\tloss: 1.3772\n",
      "Training Epoch 24  46.2% | batch:       317 of       686\t|\tloss: 1.76819\n",
      "Training Epoch 24  46.4% | batch:       318 of       686\t|\tloss: 0.869921\n",
      "Training Epoch 24  46.5% | batch:       319 of       686\t|\tloss: 0.863315\n",
      "Training Epoch 24  46.6% | batch:       320 of       686\t|\tloss: 1.11951\n",
      "Training Epoch 24  46.8% | batch:       321 of       686\t|\tloss: 1.00452\n",
      "Training Epoch 24  46.9% | batch:       322 of       686\t|\tloss: 0.982513\n",
      "Training Epoch 24  47.1% | batch:       323 of       686\t|\tloss: 1.17523\n",
      "Training Epoch 24  47.2% | batch:       324 of       686\t|\tloss: 1.0411\n",
      "Training Epoch 24  47.4% | batch:       325 of       686\t|\tloss: 1.27789\n",
      "Training Epoch 24  47.5% | batch:       326 of       686\t|\tloss: 1.12719\n",
      "Training Epoch 24  47.7% | batch:       327 of       686\t|\tloss: 1.20681\n",
      "Training Epoch 24  47.8% | batch:       328 of       686\t|\tloss: 0.988843\n",
      "Training Epoch 24  48.0% | batch:       329 of       686\t|\tloss: 1.11066\n",
      "Training Epoch 24  48.1% | batch:       330 of       686\t|\tloss: 1.05141\n",
      "Training Epoch 24  48.3% | batch:       331 of       686\t|\tloss: 1.16048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 24  48.4% | batch:       332 of       686\t|\tloss: 0.70922\n",
      "Training Epoch 24  48.5% | batch:       333 of       686\t|\tloss: 0.89938\n",
      "Training Epoch 24  48.7% | batch:       334 of       686\t|\tloss: 1.24591\n",
      "Training Epoch 24  48.8% | batch:       335 of       686\t|\tloss: 0.969678\n",
      "Training Epoch 24  49.0% | batch:       336 of       686\t|\tloss: 1.10594\n",
      "Training Epoch 24  49.1% | batch:       337 of       686\t|\tloss: 0.883331\n",
      "Training Epoch 24  49.3% | batch:       338 of       686\t|\tloss: 1.01239\n",
      "Training Epoch 24  49.4% | batch:       339 of       686\t|\tloss: 1.16104\n",
      "Training Epoch 24  49.6% | batch:       340 of       686\t|\tloss: 1.06315\n",
      "Training Epoch 24  49.7% | batch:       341 of       686\t|\tloss: 1.20458\n",
      "Training Epoch 24  49.9% | batch:       342 of       686\t|\tloss: 1.0234\n",
      "Training Epoch 24  50.0% | batch:       343 of       686\t|\tloss: 1.38293\n",
      "Training Epoch 24  50.1% | batch:       344 of       686\t|\tloss: 0.897396\n",
      "Training Epoch 24  50.3% | batch:       345 of       686\t|\tloss: 1.23822\n",
      "Training Epoch 24  50.4% | batch:       346 of       686\t|\tloss: 1.06666\n",
      "Training Epoch 24  50.6% | batch:       347 of       686\t|\tloss: 0.837944\n",
      "Training Epoch 24  50.7% | batch:       348 of       686\t|\tloss: 1.12851\n",
      "Training Epoch 24  50.9% | batch:       349 of       686\t|\tloss: 0.920196\n",
      "Training Epoch 24  51.0% | batch:       350 of       686\t|\tloss: 1.2281\n",
      "Training Epoch 24  51.2% | batch:       351 of       686\t|\tloss: 0.893959\n",
      "Training Epoch 24  51.3% | batch:       352 of       686\t|\tloss: 1.10055\n",
      "Training Epoch 24  51.5% | batch:       353 of       686\t|\tloss: 0.873856\n",
      "Training Epoch 24  51.6% | batch:       354 of       686\t|\tloss: 0.84189\n",
      "Training Epoch 24  51.7% | batch:       355 of       686\t|\tloss: 1.40881\n",
      "Training Epoch 24  51.9% | batch:       356 of       686\t|\tloss: 0.738178\n",
      "Training Epoch 24  52.0% | batch:       357 of       686\t|\tloss: 0.8845\n",
      "Training Epoch 24  52.2% | batch:       358 of       686\t|\tloss: 0.997159\n",
      "Training Epoch 24  52.3% | batch:       359 of       686\t|\tloss: 1.49493\n",
      "Training Epoch 24  52.5% | batch:       360 of       686\t|\tloss: 1.4634\n",
      "Training Epoch 24  52.6% | batch:       361 of       686\t|\tloss: 1.12044\n",
      "Training Epoch 24  52.8% | batch:       362 of       686\t|\tloss: 1.29275\n",
      "Training Epoch 24  52.9% | batch:       363 of       686\t|\tloss: 1.02908\n",
      "Training Epoch 24  53.1% | batch:       364 of       686\t|\tloss: 1.30504\n",
      "Training Epoch 24  53.2% | batch:       365 of       686\t|\tloss: 1.19783\n",
      "Training Epoch 24  53.4% | batch:       366 of       686\t|\tloss: 0.941475\n",
      "Training Epoch 24  53.5% | batch:       367 of       686\t|\tloss: 1.22496\n",
      "Training Epoch 24  53.6% | batch:       368 of       686\t|\tloss: 0.952573\n",
      "Training Epoch 24  53.8% | batch:       369 of       686\t|\tloss: 0.864593\n",
      "Training Epoch 24  53.9% | batch:       370 of       686\t|\tloss: 1.32653\n",
      "Training Epoch 24  54.1% | batch:       371 of       686\t|\tloss: 0.957817\n",
      "Training Epoch 24  54.2% | batch:       372 of       686\t|\tloss: 1.1011\n",
      "Training Epoch 24  54.4% | batch:       373 of       686\t|\tloss: 1.18388\n",
      "Training Epoch 24  54.5% | batch:       374 of       686\t|\tloss: 1.14612\n",
      "Training Epoch 24  54.7% | batch:       375 of       686\t|\tloss: 1.09341\n",
      "Training Epoch 24  54.8% | batch:       376 of       686\t|\tloss: 1.083\n",
      "Training Epoch 24  55.0% | batch:       377 of       686\t|\tloss: 1.22631\n",
      "Training Epoch 24  55.1% | batch:       378 of       686\t|\tloss: 0.953232\n",
      "Training Epoch 24  55.2% | batch:       379 of       686\t|\tloss: 0.998675\n",
      "Training Epoch 24  55.4% | batch:       380 of       686\t|\tloss: 1.04756\n",
      "Training Epoch 24  55.5% | batch:       381 of       686\t|\tloss: 1.03153\n",
      "Training Epoch 24  55.7% | batch:       382 of       686\t|\tloss: 0.976438\n",
      "Training Epoch 24  55.8% | batch:       383 of       686\t|\tloss: 0.868092\n",
      "Training Epoch 24  56.0% | batch:       384 of       686\t|\tloss: 1.20782\n",
      "Training Epoch 24  56.1% | batch:       385 of       686\t|\tloss: 0.994978\n",
      "Training Epoch 24  56.3% | batch:       386 of       686\t|\tloss: 1.26758\n",
      "Training Epoch 24  56.4% | batch:       387 of       686\t|\tloss: 1.10816\n",
      "Training Epoch 24  56.6% | batch:       388 of       686\t|\tloss: 1.07721\n",
      "Training Epoch 24  56.7% | batch:       389 of       686\t|\tloss: 0.849903\n",
      "Training Epoch 24  56.9% | batch:       390 of       686\t|\tloss: 1.5124\n",
      "Training Epoch 24  57.0% | batch:       391 of       686\t|\tloss: 1.1511\n",
      "Training Epoch 24  57.1% | batch:       392 of       686\t|\tloss: 0.82426\n",
      "Training Epoch 24  57.3% | batch:       393 of       686\t|\tloss: 0.933063\n",
      "Training Epoch 24  57.4% | batch:       394 of       686\t|\tloss: 0.852483\n",
      "Training Epoch 24  57.6% | batch:       395 of       686\t|\tloss: 0.804342\n",
      "Training Epoch 24  57.7% | batch:       396 of       686\t|\tloss: 0.969478\n",
      "Training Epoch 24  57.9% | batch:       397 of       686\t|\tloss: 1.08797\n",
      "Training Epoch 24  58.0% | batch:       398 of       686\t|\tloss: 1.10424\n",
      "Training Epoch 24  58.2% | batch:       399 of       686\t|\tloss: 0.739653\n",
      "Training Epoch 24  58.3% | batch:       400 of       686\t|\tloss: 1.25553\n",
      "Training Epoch 24  58.5% | batch:       401 of       686\t|\tloss: 0.99925\n",
      "Training Epoch 24  58.6% | batch:       402 of       686\t|\tloss: 0.951475\n",
      "Training Epoch 24  58.7% | batch:       403 of       686\t|\tloss: 0.812983\n",
      "Training Epoch 24  58.9% | batch:       404 of       686\t|\tloss: 1.01523\n",
      "Training Epoch 24  59.0% | batch:       405 of       686\t|\tloss: 0.697532\n",
      "Training Epoch 24  59.2% | batch:       406 of       686\t|\tloss: 1.133\n",
      "Training Epoch 24  59.3% | batch:       407 of       686\t|\tloss: 1.39613\n",
      "Training Epoch 24  59.5% | batch:       408 of       686\t|\tloss: 1.04716\n",
      "Training Epoch 24  59.6% | batch:       409 of       686\t|\tloss: 1.17513\n",
      "Training Epoch 24  59.8% | batch:       410 of       686\t|\tloss: 1.00724\n",
      "Training Epoch 24  59.9% | batch:       411 of       686\t|\tloss: 0.806906\n",
      "Training Epoch 24  60.1% | batch:       412 of       686\t|\tloss: 0.912804\n",
      "Training Epoch 24  60.2% | batch:       413 of       686\t|\tloss: 1.1364\n",
      "Training Epoch 24  60.3% | batch:       414 of       686\t|\tloss: 0.962153\n",
      "Training Epoch 24  60.5% | batch:       415 of       686\t|\tloss: 1.0412\n",
      "Training Epoch 24  60.6% | batch:       416 of       686\t|\tloss: 0.973429\n",
      "Training Epoch 24  60.8% | batch:       417 of       686\t|\tloss: 1.16315\n",
      "Training Epoch 24  60.9% | batch:       418 of       686\t|\tloss: 0.906586\n",
      "Training Epoch 24  61.1% | batch:       419 of       686\t|\tloss: 0.894326\n",
      "Training Epoch 24  61.2% | batch:       420 of       686\t|\tloss: 1.10354\n",
      "Training Epoch 24  61.4% | batch:       421 of       686\t|\tloss: 0.929241\n",
      "Training Epoch 24  61.5% | batch:       422 of       686\t|\tloss: 0.814643\n",
      "Training Epoch 24  61.7% | batch:       423 of       686\t|\tloss: 1.02431\n",
      "Training Epoch 24  61.8% | batch:       424 of       686\t|\tloss: 0.724523\n",
      "Training Epoch 24  62.0% | batch:       425 of       686\t|\tloss: 1.05998\n",
      "Training Epoch 24  62.1% | batch:       426 of       686\t|\tloss: 1.08479\n",
      "Training Epoch 24  62.2% | batch:       427 of       686\t|\tloss: 1.03386\n",
      "Training Epoch 24  62.4% | batch:       428 of       686\t|\tloss: 1.42299\n",
      "Training Epoch 24  62.5% | batch:       429 of       686\t|\tloss: 1.13087\n",
      "Training Epoch 24  62.7% | batch:       430 of       686\t|\tloss: 0.902842\n",
      "Training Epoch 24  62.8% | batch:       431 of       686\t|\tloss: 0.970145\n",
      "Training Epoch 24  63.0% | batch:       432 of       686\t|\tloss: 0.935242\n",
      "Training Epoch 24  63.1% | batch:       433 of       686\t|\tloss: 0.840518\n",
      "Training Epoch 24  63.3% | batch:       434 of       686\t|\tloss: 0.986747\n",
      "Training Epoch 24  63.4% | batch:       435 of       686\t|\tloss: 1.10894\n",
      "Training Epoch 24  63.6% | batch:       436 of       686\t|\tloss: 0.879279\n",
      "Training Epoch 24  63.7% | batch:       437 of       686\t|\tloss: 1.22166\n",
      "Training Epoch 24  63.8% | batch:       438 of       686\t|\tloss: 0.945021\n",
      "Training Epoch 24  64.0% | batch:       439 of       686\t|\tloss: 1.0894\n",
      "Training Epoch 24  64.1% | batch:       440 of       686\t|\tloss: 1.1527\n",
      "Training Epoch 24  64.3% | batch:       441 of       686\t|\tloss: 0.776217\n",
      "Training Epoch 24  64.4% | batch:       442 of       686\t|\tloss: 1.0289\n",
      "Training Epoch 24  64.6% | batch:       443 of       686\t|\tloss: 1.16857\n",
      "Training Epoch 24  64.7% | batch:       444 of       686\t|\tloss: 0.760366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 24  64.9% | batch:       445 of       686\t|\tloss: 0.873323\n",
      "Training Epoch 24  65.0% | batch:       446 of       686\t|\tloss: 1.07281\n",
      "Training Epoch 24  65.2% | batch:       447 of       686\t|\tloss: 0.926124\n",
      "Training Epoch 24  65.3% | batch:       448 of       686\t|\tloss: 0.868661\n",
      "Training Epoch 24  65.5% | batch:       449 of       686\t|\tloss: 1.07052\n",
      "Training Epoch 24  65.6% | batch:       450 of       686\t|\tloss: 1.02155\n",
      "Training Epoch 24  65.7% | batch:       451 of       686\t|\tloss: 1.05552\n",
      "Training Epoch 24  65.9% | batch:       452 of       686\t|\tloss: 0.971808\n",
      "Training Epoch 24  66.0% | batch:       453 of       686\t|\tloss: 1.15941\n",
      "Training Epoch 24  66.2% | batch:       454 of       686\t|\tloss: 1.0902\n",
      "Training Epoch 24  66.3% | batch:       455 of       686\t|\tloss: 0.907661\n",
      "Training Epoch 24  66.5% | batch:       456 of       686\t|\tloss: 0.868406\n",
      "Training Epoch 24  66.6% | batch:       457 of       686\t|\tloss: 0.888893\n",
      "Training Epoch 24  66.8% | batch:       458 of       686\t|\tloss: 0.841983\n",
      "Training Epoch 24  66.9% | batch:       459 of       686\t|\tloss: 1.02518\n",
      "Training Epoch 24  67.1% | batch:       460 of       686\t|\tloss: 0.753483\n",
      "Training Epoch 24  67.2% | batch:       461 of       686\t|\tloss: 0.929055\n",
      "Training Epoch 24  67.3% | batch:       462 of       686\t|\tloss: 1.15791\n",
      "Training Epoch 24  67.5% | batch:       463 of       686\t|\tloss: 0.818894\n",
      "Training Epoch 24  67.6% | batch:       464 of       686\t|\tloss: 1.06852\n",
      "Training Epoch 24  67.8% | batch:       465 of       686\t|\tloss: 0.958311\n",
      "Training Epoch 24  67.9% | batch:       466 of       686\t|\tloss: 1.07213\n",
      "Training Epoch 24  68.1% | batch:       467 of       686\t|\tloss: 0.828627\n",
      "Training Epoch 24  68.2% | batch:       468 of       686\t|\tloss: 0.819179\n",
      "Training Epoch 24  68.4% | batch:       469 of       686\t|\tloss: 1.00923\n",
      "Training Epoch 24  68.5% | batch:       470 of       686\t|\tloss: 0.823649\n",
      "Training Epoch 24  68.7% | batch:       471 of       686\t|\tloss: 0.924428\n",
      "Training Epoch 24  68.8% | batch:       472 of       686\t|\tloss: 0.910009\n",
      "Training Epoch 24  69.0% | batch:       473 of       686\t|\tloss: 0.946214\n",
      "Training Epoch 24  69.1% | batch:       474 of       686\t|\tloss: 0.970685\n",
      "Training Epoch 24  69.2% | batch:       475 of       686\t|\tloss: 0.819928\n",
      "Training Epoch 24  69.4% | batch:       476 of       686\t|\tloss: 1.03453\n",
      "Training Epoch 24  69.5% | batch:       477 of       686\t|\tloss: 0.978626\n",
      "Training Epoch 24  69.7% | batch:       478 of       686\t|\tloss: 1.13497\n",
      "Training Epoch 24  69.8% | batch:       479 of       686\t|\tloss: 0.990412\n",
      "Training Epoch 24  70.0% | batch:       480 of       686\t|\tloss: 0.924617\n",
      "Training Epoch 24  70.1% | batch:       481 of       686\t|\tloss: 0.917807\n",
      "Training Epoch 24  70.3% | batch:       482 of       686\t|\tloss: 0.839067\n",
      "Training Epoch 24  70.4% | batch:       483 of       686\t|\tloss: 0.973842\n",
      "Training Epoch 24  70.6% | batch:       484 of       686\t|\tloss: 0.954523\n",
      "Training Epoch 24  70.7% | batch:       485 of       686\t|\tloss: 1.37596\n",
      "Training Epoch 24  70.8% | batch:       486 of       686\t|\tloss: 1.29303\n",
      "Training Epoch 24  71.0% | batch:       487 of       686\t|\tloss: 0.97112\n",
      "Training Epoch 24  71.1% | batch:       488 of       686\t|\tloss: 0.938765\n",
      "Training Epoch 24  71.3% | batch:       489 of       686\t|\tloss: 1.06581\n",
      "Training Epoch 24  71.4% | batch:       490 of       686\t|\tloss: 0.944091\n",
      "Training Epoch 24  71.6% | batch:       491 of       686\t|\tloss: 0.830722\n",
      "Training Epoch 24  71.7% | batch:       492 of       686\t|\tloss: 1.32189\n",
      "Training Epoch 24  71.9% | batch:       493 of       686\t|\tloss: 1.00875\n",
      "Training Epoch 24  72.0% | batch:       494 of       686\t|\tloss: 1.15723\n",
      "Training Epoch 24  72.2% | batch:       495 of       686\t|\tloss: 0.984892\n",
      "Training Epoch 24  72.3% | batch:       496 of       686\t|\tloss: 0.865568\n",
      "Training Epoch 24  72.4% | batch:       497 of       686\t|\tloss: 1.16197\n",
      "Training Epoch 24  72.6% | batch:       498 of       686\t|\tloss: 1.02626\n",
      "Training Epoch 24  72.7% | batch:       499 of       686\t|\tloss: 1.07333\n",
      "Training Epoch 24  72.9% | batch:       500 of       686\t|\tloss: 0.820607\n",
      "Training Epoch 24  73.0% | batch:       501 of       686\t|\tloss: 0.786492\n",
      "Training Epoch 24  73.2% | batch:       502 of       686\t|\tloss: 0.986003\n",
      "Training Epoch 24  73.3% | batch:       503 of       686\t|\tloss: 1.28037\n",
      "Training Epoch 24  73.5% | batch:       504 of       686\t|\tloss: 1.18294\n",
      "Training Epoch 24  73.6% | batch:       505 of       686\t|\tloss: 0.815318\n",
      "Training Epoch 24  73.8% | batch:       506 of       686\t|\tloss: 1.03627\n",
      "Training Epoch 24  73.9% | batch:       507 of       686\t|\tloss: 0.936007\n",
      "Training Epoch 24  74.1% | batch:       508 of       686\t|\tloss: 1.1327\n",
      "Training Epoch 24  74.2% | batch:       509 of       686\t|\tloss: 1.2107\n",
      "Training Epoch 24  74.3% | batch:       510 of       686\t|\tloss: 0.944495\n",
      "Training Epoch 24  74.5% | batch:       511 of       686\t|\tloss: 0.987159\n",
      "Training Epoch 24  74.6% | batch:       512 of       686\t|\tloss: 1.17994\n",
      "Training Epoch 24  74.8% | batch:       513 of       686\t|\tloss: 1.48928\n",
      "Training Epoch 24  74.9% | batch:       514 of       686\t|\tloss: 1.34353\n",
      "Training Epoch 24  75.1% | batch:       515 of       686\t|\tloss: 1.0694\n",
      "Training Epoch 24  75.2% | batch:       516 of       686\t|\tloss: 1.0552\n",
      "Training Epoch 24  75.4% | batch:       517 of       686\t|\tloss: 0.96303\n",
      "Training Epoch 24  75.5% | batch:       518 of       686\t|\tloss: 0.962617\n",
      "Training Epoch 24  75.7% | batch:       519 of       686\t|\tloss: 1.13015\n",
      "Training Epoch 24  75.8% | batch:       520 of       686\t|\tloss: 1.02288\n",
      "Training Epoch 24  75.9% | batch:       521 of       686\t|\tloss: 0.928407\n",
      "Training Epoch 24  76.1% | batch:       522 of       686\t|\tloss: 1.01532\n",
      "Training Epoch 24  76.2% | batch:       523 of       686\t|\tloss: 1.00863\n",
      "Training Epoch 24  76.4% | batch:       524 of       686\t|\tloss: 0.964118\n",
      "Training Epoch 24  76.5% | batch:       525 of       686\t|\tloss: 0.786947\n",
      "Training Epoch 24  76.7% | batch:       526 of       686\t|\tloss: 0.885185\n",
      "Training Epoch 24  76.8% | batch:       527 of       686\t|\tloss: 0.750004\n",
      "Training Epoch 24  77.0% | batch:       528 of       686\t|\tloss: 1.08366\n",
      "Training Epoch 24  77.1% | batch:       529 of       686\t|\tloss: 1.16093\n",
      "Training Epoch 24  77.3% | batch:       530 of       686\t|\tloss: 1.18312\n",
      "Training Epoch 24  77.4% | batch:       531 of       686\t|\tloss: 1.11953\n",
      "Training Epoch 24  77.6% | batch:       532 of       686\t|\tloss: 1.1872\n",
      "Training Epoch 24  77.7% | batch:       533 of       686\t|\tloss: 1.01841\n",
      "Training Epoch 24  77.8% | batch:       534 of       686\t|\tloss: 0.813521\n",
      "Training Epoch 24  78.0% | batch:       535 of       686\t|\tloss: 0.946383\n",
      "Training Epoch 24  78.1% | batch:       536 of       686\t|\tloss: 0.979047\n",
      "Training Epoch 24  78.3% | batch:       537 of       686\t|\tloss: 1.21328\n",
      "Training Epoch 24  78.4% | batch:       538 of       686\t|\tloss: 1.04086\n",
      "Training Epoch 24  78.6% | batch:       539 of       686\t|\tloss: 0.904328\n",
      "Training Epoch 24  78.7% | batch:       540 of       686\t|\tloss: 1.25496\n",
      "Training Epoch 24  78.9% | batch:       541 of       686\t|\tloss: 0.895264\n",
      "Training Epoch 24  79.0% | batch:       542 of       686\t|\tloss: 1.53567\n",
      "Training Epoch 24  79.2% | batch:       543 of       686\t|\tloss: 1.37807\n",
      "Training Epoch 24  79.3% | batch:       544 of       686\t|\tloss: 0.913277\n",
      "Training Epoch 24  79.4% | batch:       545 of       686\t|\tloss: 0.98975\n",
      "Training Epoch 24  79.6% | batch:       546 of       686\t|\tloss: 0.827888\n",
      "Training Epoch 24  79.7% | batch:       547 of       686\t|\tloss: 1.03474\n",
      "Training Epoch 24  79.9% | batch:       548 of       686\t|\tloss: 0.696999\n",
      "Training Epoch 24  80.0% | batch:       549 of       686\t|\tloss: 1.23829\n",
      "Training Epoch 24  80.2% | batch:       550 of       686\t|\tloss: 1.10586\n",
      "Training Epoch 24  80.3% | batch:       551 of       686\t|\tloss: 1.07204\n",
      "Training Epoch 24  80.5% | batch:       552 of       686\t|\tloss: 0.900577\n",
      "Training Epoch 24  80.6% | batch:       553 of       686\t|\tloss: 1.08679\n",
      "Training Epoch 24  80.8% | batch:       554 of       686\t|\tloss: 1.18148\n",
      "Training Epoch 24  80.9% | batch:       555 of       686\t|\tloss: 1.05293\n",
      "Training Epoch 24  81.0% | batch:       556 of       686\t|\tloss: 0.9361\n",
      "Training Epoch 24  81.2% | batch:       557 of       686\t|\tloss: 0.835808\n",
      "Training Epoch 24  81.3% | batch:       558 of       686\t|\tloss: 0.980628\n",
      "Training Epoch 24  81.5% | batch:       559 of       686\t|\tloss: 0.954691\n",
      "Training Epoch 24  81.6% | batch:       560 of       686\t|\tloss: 0.966592\n",
      "Training Epoch 24  81.8% | batch:       561 of       686\t|\tloss: 0.921781\n",
      "Training Epoch 24  81.9% | batch:       562 of       686\t|\tloss: 1.28084\n",
      "Training Epoch 24  82.1% | batch:       563 of       686\t|\tloss: 0.815551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 24  82.2% | batch:       564 of       686\t|\tloss: 0.763876\n",
      "Training Epoch 24  82.4% | batch:       565 of       686\t|\tloss: 1.03283\n",
      "Training Epoch 24  82.5% | batch:       566 of       686\t|\tloss: 1.13679\n",
      "Training Epoch 24  82.7% | batch:       567 of       686\t|\tloss: 0.723686\n",
      "Training Epoch 24  82.8% | batch:       568 of       686\t|\tloss: 0.795674\n",
      "Training Epoch 24  82.9% | batch:       569 of       686\t|\tloss: 1.17536\n",
      "Training Epoch 24  83.1% | batch:       570 of       686\t|\tloss: 0.90717\n",
      "Training Epoch 24  83.2% | batch:       571 of       686\t|\tloss: 1.1842\n",
      "Training Epoch 24  83.4% | batch:       572 of       686\t|\tloss: 0.980725\n",
      "Training Epoch 24  83.5% | batch:       573 of       686\t|\tloss: 1.03357\n",
      "Training Epoch 24  83.7% | batch:       574 of       686\t|\tloss: 1.22642\n",
      "Training Epoch 24  83.8% | batch:       575 of       686\t|\tloss: 1.04422\n",
      "Training Epoch 24  84.0% | batch:       576 of       686\t|\tloss: 1.18418\n",
      "Training Epoch 24  84.1% | batch:       577 of       686\t|\tloss: 0.976232\n",
      "Training Epoch 24  84.3% | batch:       578 of       686\t|\tloss: 0.971488\n",
      "Training Epoch 24  84.4% | batch:       579 of       686\t|\tloss: 0.888028\n",
      "Training Epoch 24  84.5% | batch:       580 of       686\t|\tloss: 0.808946\n",
      "Training Epoch 24  84.7% | batch:       581 of       686\t|\tloss: 0.892223\n",
      "Training Epoch 24  84.8% | batch:       582 of       686\t|\tloss: 0.902092\n",
      "Training Epoch 24  85.0% | batch:       583 of       686\t|\tloss: 0.847715\n",
      "Training Epoch 24  85.1% | batch:       584 of       686\t|\tloss: 0.981838\n",
      "Training Epoch 24  85.3% | batch:       585 of       686\t|\tloss: 1.1983\n",
      "Training Epoch 24  85.4% | batch:       586 of       686\t|\tloss: 0.958472\n",
      "Training Epoch 24  85.6% | batch:       587 of       686\t|\tloss: 1.06633\n",
      "Training Epoch 24  85.7% | batch:       588 of       686\t|\tloss: 1.04374\n",
      "Training Epoch 24  85.9% | batch:       589 of       686\t|\tloss: 1.08242\n",
      "Training Epoch 24  86.0% | batch:       590 of       686\t|\tloss: 0.991363\n",
      "Training Epoch 24  86.2% | batch:       591 of       686\t|\tloss: 1.01727\n",
      "Training Epoch 24  86.3% | batch:       592 of       686\t|\tloss: 0.881008\n",
      "Training Epoch 24  86.4% | batch:       593 of       686\t|\tloss: 1.15616\n",
      "Training Epoch 24  86.6% | batch:       594 of       686\t|\tloss: 1.06124\n",
      "Training Epoch 24  86.7% | batch:       595 of       686\t|\tloss: 1.0639\n",
      "Training Epoch 24  86.9% | batch:       596 of       686\t|\tloss: 1.02285\n",
      "Training Epoch 24  87.0% | batch:       597 of       686\t|\tloss: 1.69087\n",
      "Training Epoch 24  87.2% | batch:       598 of       686\t|\tloss: 0.961983\n",
      "Training Epoch 24  87.3% | batch:       599 of       686\t|\tloss: 0.861118\n",
      "Training Epoch 24  87.5% | batch:       600 of       686\t|\tloss: 0.896723\n",
      "Training Epoch 24  87.6% | batch:       601 of       686\t|\tloss: 0.974137\n",
      "Training Epoch 24  87.8% | batch:       602 of       686\t|\tloss: 0.888193\n",
      "Training Epoch 24  87.9% | batch:       603 of       686\t|\tloss: 0.963804\n",
      "Training Epoch 24  88.0% | batch:       604 of       686\t|\tloss: 1.02618\n",
      "Training Epoch 24  88.2% | batch:       605 of       686\t|\tloss: 1.17174\n",
      "Training Epoch 24  88.3% | batch:       606 of       686\t|\tloss: 0.83996\n",
      "Training Epoch 24  88.5% | batch:       607 of       686\t|\tloss: 0.868799\n",
      "Training Epoch 24  88.6% | batch:       608 of       686\t|\tloss: 1.00224\n",
      "Training Epoch 24  88.8% | batch:       609 of       686\t|\tloss: 1.22908\n",
      "Training Epoch 24  88.9% | batch:       610 of       686\t|\tloss: 0.962913\n",
      "Training Epoch 24  89.1% | batch:       611 of       686\t|\tloss: 1.11827\n",
      "Training Epoch 24  89.2% | batch:       612 of       686\t|\tloss: 1.21028\n",
      "Training Epoch 24  89.4% | batch:       613 of       686\t|\tloss: 1.27501\n",
      "Training Epoch 24  89.5% | batch:       614 of       686\t|\tloss: 1.00864\n",
      "Training Epoch 24  89.7% | batch:       615 of       686\t|\tloss: 1.10886\n",
      "Training Epoch 24  89.8% | batch:       616 of       686\t|\tloss: 1.17931\n",
      "Training Epoch 24  89.9% | batch:       617 of       686\t|\tloss: 0.96943\n",
      "Training Epoch 24  90.1% | batch:       618 of       686\t|\tloss: 0.995578\n",
      "Training Epoch 24  90.2% | batch:       619 of       686\t|\tloss: 1.11607\n",
      "Training Epoch 24  90.4% | batch:       620 of       686\t|\tloss: 1.02776\n",
      "Training Epoch 24  90.5% | batch:       621 of       686\t|\tloss: 0.895023\n",
      "Training Epoch 24  90.7% | batch:       622 of       686\t|\tloss: 0.696579\n",
      "Training Epoch 24  90.8% | batch:       623 of       686\t|\tloss: 1.21157\n",
      "Training Epoch 24  91.0% | batch:       624 of       686\t|\tloss: 0.867963\n",
      "Training Epoch 24  91.1% | batch:       625 of       686\t|\tloss: 0.896123\n",
      "Training Epoch 24  91.3% | batch:       626 of       686\t|\tloss: 1.06464\n",
      "Training Epoch 24  91.4% | batch:       627 of       686\t|\tloss: 1.14695\n",
      "Training Epoch 24  91.5% | batch:       628 of       686\t|\tloss: 0.746435\n",
      "Training Epoch 24  91.7% | batch:       629 of       686\t|\tloss: 0.938452\n",
      "Training Epoch 24  91.8% | batch:       630 of       686\t|\tloss: 1.27446\n",
      "Training Epoch 24  92.0% | batch:       631 of       686\t|\tloss: 0.931789\n",
      "Training Epoch 24  92.1% | batch:       632 of       686\t|\tloss: 1.37721\n",
      "Training Epoch 24  92.3% | batch:       633 of       686\t|\tloss: 1.05042\n",
      "Training Epoch 24  92.4% | batch:       634 of       686\t|\tloss: 0.826247\n",
      "Training Epoch 24  92.6% | batch:       635 of       686\t|\tloss: 1.08996\n",
      "Training Epoch 24  92.7% | batch:       636 of       686\t|\tloss: 1.12297\n",
      "Training Epoch 24  92.9% | batch:       637 of       686\t|\tloss: 0.892365\n",
      "Training Epoch 24  93.0% | batch:       638 of       686\t|\tloss: 0.76791\n",
      "Training Epoch 24  93.1% | batch:       639 of       686\t|\tloss: 1.03844\n",
      "Training Epoch 24  93.3% | batch:       640 of       686\t|\tloss: 1.12867\n",
      "Training Epoch 24  93.4% | batch:       641 of       686\t|\tloss: 1.1788\n",
      "Training Epoch 24  93.6% | batch:       642 of       686\t|\tloss: 0.980327\n",
      "Training Epoch 24  93.7% | batch:       643 of       686\t|\tloss: 0.875003\n",
      "Training Epoch 24  93.9% | batch:       644 of       686\t|\tloss: 0.943252\n",
      "Training Epoch 24  94.0% | batch:       645 of       686\t|\tloss: 1.0643\n",
      "Training Epoch 24  94.2% | batch:       646 of       686\t|\tloss: 1.04064\n",
      "Training Epoch 24  94.3% | batch:       647 of       686\t|\tloss: 1.10195\n",
      "Training Epoch 24  94.5% | batch:       648 of       686\t|\tloss: 1.00239\n",
      "Training Epoch 24  94.6% | batch:       649 of       686\t|\tloss: 0.813715\n",
      "Training Epoch 24  94.8% | batch:       650 of       686\t|\tloss: 0.777505\n",
      "Training Epoch 24  94.9% | batch:       651 of       686\t|\tloss: 1.21705\n",
      "Training Epoch 24  95.0% | batch:       652 of       686\t|\tloss: 0.987832\n",
      "Training Epoch 24  95.2% | batch:       653 of       686\t|\tloss: 0.920794\n",
      "Training Epoch 24  95.3% | batch:       654 of       686\t|\tloss: 1.18409\n",
      "Training Epoch 24  95.5% | batch:       655 of       686\t|\tloss: 1.07925\n",
      "Training Epoch 24  95.6% | batch:       656 of       686\t|\tloss: 1.02251\n",
      "Training Epoch 24  95.8% | batch:       657 of       686\t|\tloss: 1.04395\n",
      "Training Epoch 24  95.9% | batch:       658 of       686\t|\tloss: 1.04186\n",
      "Training Epoch 24  96.1% | batch:       659 of       686\t|\tloss: 0.827953\n",
      "Training Epoch 24  96.2% | batch:       660 of       686\t|\tloss: 1.20318\n",
      "Training Epoch 24  96.4% | batch:       661 of       686\t|\tloss: 1.08482\n",
      "Training Epoch 24  96.5% | batch:       662 of       686\t|\tloss: 1.00731\n",
      "Training Epoch 24  96.6% | batch:       663 of       686\t|\tloss: 0.977905\n",
      "Training Epoch 24  96.8% | batch:       664 of       686\t|\tloss: 1.0558\n",
      "Training Epoch 24  96.9% | batch:       665 of       686\t|\tloss: 1.22001\n",
      "Training Epoch 24  97.1% | batch:       666 of       686\t|\tloss: 0.851849\n",
      "Training Epoch 24  97.2% | batch:       667 of       686\t|\tloss: 1.05462\n",
      "Training Epoch 24  97.4% | batch:       668 of       686\t|\tloss: 0.884922\n",
      "Training Epoch 24  97.5% | batch:       669 of       686\t|\tloss: 0.973849\n",
      "Training Epoch 24  97.7% | batch:       670 of       686\t|\tloss: 1.0178\n",
      "Training Epoch 24  97.8% | batch:       671 of       686\t|\tloss: 0.907959\n",
      "Training Epoch 24  98.0% | batch:       672 of       686\t|\tloss: 0.89807\n",
      "Training Epoch 24  98.1% | batch:       673 of       686\t|\tloss: 1.03164\n",
      "Training Epoch 24  98.3% | batch:       674 of       686\t|\tloss: 0.844338\n",
      "Training Epoch 24  98.4% | batch:       675 of       686\t|\tloss: 0.76326\n",
      "Training Epoch 24  98.5% | batch:       676 of       686\t|\tloss: 1.03112\n",
      "Training Epoch 24  98.7% | batch:       677 of       686\t|\tloss: 1.00311\n",
      "Training Epoch 24  98.8% | batch:       678 of       686\t|\tloss: 0.912628\n",
      "Training Epoch 24  99.0% | batch:       679 of       686\t|\tloss: 1.21376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:11:03,366 | INFO : Epoch 24 Training Summary: epoch: 24.000000 | loss: 1.053154 | \n",
      "2023-05-25 22:11:03,367 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 25.780949354171753 seconds\n",
      "\n",
      "2023-05-25 22:11:03,368 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 24.050995359818142 seconds\n",
      "2023-05-25 22:11:03,368 | INFO : Avg batch train. time: 0.0350597599997349 seconds\n",
      "2023-05-25 22:11:03,369 | INFO : Avg sample train. time: 0.0002742573163785637 seconds\n",
      "2023-05-25 22:11:03,369 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 24  99.1% | batch:       680 of       686\t|\tloss: 1.08346\n",
      "Training Epoch 24  99.3% | batch:       681 of       686\t|\tloss: 0.864286\n",
      "Training Epoch 24  99.4% | batch:       682 of       686\t|\tloss: 1.38629\n",
      "Training Epoch 24  99.6% | batch:       683 of       686\t|\tloss: 1.1843\n",
      "Training Epoch 24  99.7% | batch:       684 of       686\t|\tloss: 0.864823\n",
      "Training Epoch 24  99.9% | batch:       685 of       686\t|\tloss: 0.831491\n",
      "\n",
      "Evaluating Epoch 24   0.0% | batch:         0 of       172\t|\tloss: 1.35679\n",
      "Evaluating Epoch 24   0.6% | batch:         1 of       172\t|\tloss: 1.46077\n",
      "Evaluating Epoch 24   1.2% | batch:         2 of       172\t|\tloss: 0.675873\n",
      "Evaluating Epoch 24   1.7% | batch:         3 of       172\t|\tloss: 2.58809\n",
      "Evaluating Epoch 24   2.3% | batch:         4 of       172\t|\tloss: 1.16035\n",
      "Evaluating Epoch 24   2.9% | batch:         5 of       172\t|\tloss: 1.28602\n",
      "Evaluating Epoch 24   3.5% | batch:         6 of       172\t|\tloss: 1.16166\n",
      "Evaluating Epoch 24   4.1% | batch:         7 of       172\t|\tloss: 3.08278\n",
      "Evaluating Epoch 24   4.7% | batch:         8 of       172\t|\tloss: 0.606539\n",
      "Evaluating Epoch 24   5.2% | batch:         9 of       172\t|\tloss: 1.76562\n",
      "Evaluating Epoch 24   5.8% | batch:        10 of       172\t|\tloss: 1.31313\n",
      "Evaluating Epoch 24   6.4% | batch:        11 of       172\t|\tloss: 1.21645\n",
      "Evaluating Epoch 24   7.0% | batch:        12 of       172\t|\tloss: 1.39679\n",
      "Evaluating Epoch 24   7.6% | batch:        13 of       172\t|\tloss: 1.26579\n",
      "Evaluating Epoch 24   8.1% | batch:        14 of       172\t|\tloss: 1.70354\n",
      "Evaluating Epoch 24   8.7% | batch:        15 of       172\t|\tloss: 1.34038\n",
      "Evaluating Epoch 24   9.3% | batch:        16 of       172\t|\tloss: 2.21873\n",
      "Evaluating Epoch 24   9.9% | batch:        17 of       172\t|\tloss: 0.787763\n",
      "Evaluating Epoch 24  10.5% | batch:        18 of       172\t|\tloss: 18.3524\n",
      "Evaluating Epoch 24  11.0% | batch:        19 of       172\t|\tloss: 1.41944\n",
      "Evaluating Epoch 24  11.6% | batch:        20 of       172\t|\tloss: 2.89365\n",
      "Evaluating Epoch 24  12.2% | batch:        21 of       172\t|\tloss: 0.846619\n",
      "Evaluating Epoch 24  12.8% | batch:        22 of       172\t|\tloss: 4.49574\n",
      "Evaluating Epoch 24  13.4% | batch:        23 of       172\t|\tloss: 2.90267\n",
      "Evaluating Epoch 24  14.0% | batch:        24 of       172\t|\tloss: 1.32279\n",
      "Evaluating Epoch 24  14.5% | batch:        25 of       172\t|\tloss: 2.92362\n",
      "Evaluating Epoch 24  15.1% | batch:        26 of       172\t|\tloss: 7.87628\n",
      "Evaluating Epoch 24  15.7% | batch:        27 of       172\t|\tloss: 16.5277\n",
      "Evaluating Epoch 24  16.3% | batch:        28 of       172\t|\tloss: 0.280184\n",
      "Evaluating Epoch 24  16.9% | batch:        29 of       172\t|\tloss: 2.48141\n",
      "Evaluating Epoch 24  17.4% | batch:        30 of       172\t|\tloss: 1.21074\n",
      "Evaluating Epoch 24  18.0% | batch:        31 of       172\t|\tloss: 0.30298\n",
      "Evaluating Epoch 24  18.6% | batch:        32 of       172\t|\tloss: 0.374996\n",
      "Evaluating Epoch 24  19.2% | batch:        33 of       172\t|\tloss: 0.504057\n",
      "Evaluating Epoch 24  19.8% | batch:        34 of       172\t|\tloss: 0.298608\n",
      "Evaluating Epoch 24  20.3% | batch:        35 of       172\t|\tloss: 0.525165\n",
      "Evaluating Epoch 24  20.9% | batch:        36 of       172\t|\tloss: 2.54749\n",
      "Evaluating Epoch 24  21.5% | batch:        37 of       172\t|\tloss: 3.11178\n",
      "Evaluating Epoch 24  22.1% | batch:        38 of       172\t|\tloss: 3.52404\n",
      "Evaluating Epoch 24  22.7% | batch:        39 of       172\t|\tloss: 8.00636\n",
      "Evaluating Epoch 24  23.3% | batch:        40 of       172\t|\tloss: 0.423305\n",
      "Evaluating Epoch 24  23.8% | batch:        41 of       172\t|\tloss: 1.5818\n",
      "Evaluating Epoch 24  24.4% | batch:        42 of       172\t|\tloss: 0.380659\n",
      "Evaluating Epoch 24  25.0% | batch:        43 of       172\t|\tloss: 19.9306\n",
      "Evaluating Epoch 24  25.6% | batch:        44 of       172\t|\tloss: 1.31628\n",
      "Evaluating Epoch 24  26.2% | batch:        45 of       172\t|\tloss: 1.54138\n",
      "Evaluating Epoch 24  26.7% | batch:        46 of       172\t|\tloss: 0.463244\n",
      "Evaluating Epoch 24  27.3% | batch:        47 of       172\t|\tloss: 1.01993\n",
      "Evaluating Epoch 24  27.9% | batch:        48 of       172\t|\tloss: 0.187348\n",
      "Evaluating Epoch 24  28.5% | batch:        49 of       172\t|\tloss: 0.733049\n",
      "Evaluating Epoch 24  29.1% | batch:        50 of       172\t|\tloss: 0.242768\n",
      "Evaluating Epoch 24  29.7% | batch:        51 of       172\t|\tloss: 0.826922\n",
      "Evaluating Epoch 24  30.2% | batch:        52 of       172\t|\tloss: 0.358565\n",
      "Evaluating Epoch 24  30.8% | batch:        53 of       172\t|\tloss: 2.65569\n",
      "Evaluating Epoch 24  31.4% | batch:        54 of       172\t|\tloss: 0.93198\n",
      "Evaluating Epoch 24  32.0% | batch:        55 of       172\t|\tloss: 0.396047\n",
      "Evaluating Epoch 24  32.6% | batch:        56 of       172\t|\tloss: 3.0673\n",
      "Evaluating Epoch 24  33.1% | batch:        57 of       172\t|\tloss: 0.540071\n",
      "Evaluating Epoch 24  33.7% | batch:        58 of       172\t|\tloss: 2.05164\n",
      "Evaluating Epoch 24  34.3% | batch:        59 of       172\t|\tloss: 1.1255\n",
      "Evaluating Epoch 24  34.9% | batch:        60 of       172\t|\tloss: 0.974406\n",
      "Evaluating Epoch 24  35.5% | batch:        61 of       172\t|\tloss: 1.83814\n",
      "Evaluating Epoch 24  36.0% | batch:        62 of       172\t|\tloss: 0.82686\n",
      "Evaluating Epoch 24  36.6% | batch:        63 of       172\t|\tloss: 2.83457\n",
      "Evaluating Epoch 24  37.2% | batch:        64 of       172\t|\tloss: 0.474612\n",
      "Evaluating Epoch 24  37.8% | batch:        65 of       172\t|\tloss: 2.14957\n",
      "Evaluating Epoch 24  38.4% | batch:        66 of       172\t|\tloss: 1.63171\n",
      "Evaluating Epoch 24  39.0% | batch:        67 of       172\t|\tloss: 0.246174\n",
      "Evaluating Epoch 24  39.5% | batch:        68 of       172\t|\tloss: 2.23991\n",
      "Evaluating Epoch 24  40.1% | batch:        69 of       172\t|\tloss: 0.912396\n",
      "Evaluating Epoch 24  40.7% | batch:        70 of       172\t|\tloss: 1.59267\n",
      "Evaluating Epoch 24  41.3% | batch:        71 of       172\t|\tloss: 1.45855\n",
      "Evaluating Epoch 24  41.9% | batch:        72 of       172\t|\tloss: 0.481419\n",
      "Evaluating Epoch 24  42.4% | batch:        73 of       172\t|\tloss: 2.34868\n",
      "Evaluating Epoch 24  43.0% | batch:        74 of       172\t|\tloss: 0.224478\n",
      "Evaluating Epoch 24  43.6% | batch:        75 of       172\t|\tloss: 0.200858\n",
      "Evaluating Epoch 24  44.2% | batch:        76 of       172\t|\tloss: 0.29108\n",
      "Evaluating Epoch 24  44.8% | batch:        77 of       172\t|\tloss: 0.267671\n",
      "Evaluating Epoch 24  45.3% | batch:        78 of       172\t|\tloss: 0.28212\n",
      "Evaluating Epoch 24  45.9% | batch:        79 of       172\t|\tloss: 0.282337\n",
      "Evaluating Epoch 24  46.5% | batch:        80 of       172\t|\tloss: 0.218105\n",
      "Evaluating Epoch 24  47.1% | batch:        81 of       172\t|\tloss: 0.296684\n",
      "Evaluating Epoch 24  47.7% | batch:        82 of       172\t|\tloss: 0.206983\n",
      "Evaluating Epoch 24  48.3% | batch:        83 of       172\t|\tloss: 0.364886\n",
      "Evaluating Epoch 24  48.8% | batch:        84 of       172\t|\tloss: 0.539772\n",
      "Evaluating Epoch 24  49.4% | batch:        85 of       172\t|\tloss: 0.398224\n",
      "Evaluating Epoch 24  50.0% | batch:        86 of       172\t|\tloss: 0.391573\n",
      "Evaluating Epoch 24  50.6% | batch:        87 of       172\t|\tloss: 0.563593\n",
      "Evaluating Epoch 24  51.2% | batch:        88 of       172\t|\tloss: 0.299971\n",
      "Evaluating Epoch 24  51.7% | batch:        89 of       172\t|\tloss: 0.391285\n",
      "Evaluating Epoch 24  52.3% | batch:        90 of       172\t|\tloss: 0.536908\n",
      "Evaluating Epoch 24  52.9% | batch:        91 of       172\t|\tloss: 0.223885\n",
      "Evaluating Epoch 24  53.5% | batch:        92 of       172\t|\tloss: 0.253034\n",
      "Evaluating Epoch 24  54.1% | batch:        93 of       172\t|\tloss: 0.658637\n",
      "Evaluating Epoch 24  54.7% | batch:        94 of       172\t|\tloss: 0.41449\n",
      "Evaluating Epoch 24  55.2% | batch:        95 of       172\t|\tloss: 0.222363\n",
      "Evaluating Epoch 24  55.8% | batch:        96 of       172\t|\tloss: 0.475307\n",
      "Evaluating Epoch 24  56.4% | batch:        97 of       172\t|\tloss: 0.474751\n",
      "Evaluating Epoch 24  57.0% | batch:        98 of       172\t|\tloss: 0.377966\n",
      "Evaluating Epoch 24  57.6% | batch:        99 of       172\t|\tloss: 0.283563\n",
      "Evaluating Epoch 24  58.1% | batch:       100 of       172\t|\tloss: 0.378953\n",
      "Evaluating Epoch 24  58.7% | batch:       101 of       172\t|\tloss: 0.408035\n",
      "Evaluating Epoch 24  59.3% | batch:       102 of       172\t|\tloss: 0.301606\n",
      "Evaluating Epoch 24  59.9% | batch:       103 of       172\t|\tloss: 0.708016\n",
      "Evaluating Epoch 24  60.5% | batch:       104 of       172\t|\tloss: 0.416425\n",
      "Evaluating Epoch 24  61.0% | batch:       105 of       172\t|\tloss: 0.210536\n",
      "Evaluating Epoch 24  61.6% | batch:       106 of       172\t|\tloss: 0.30452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 24  62.2% | batch:       107 of       172\t|\tloss: 0.789776\n",
      "Evaluating Epoch 24  62.8% | batch:       108 of       172\t|\tloss: 0.330187\n",
      "Evaluating Epoch 24  63.4% | batch:       109 of       172\t|\tloss: 0.270097\n",
      "Evaluating Epoch 24  64.0% | batch:       110 of       172\t|\tloss: 0.603147\n",
      "Evaluating Epoch 24  64.5% | batch:       111 of       172\t|\tloss: 0.485069\n",
      "Evaluating Epoch 24  65.1% | batch:       112 of       172\t|\tloss: 0.539496\n",
      "Evaluating Epoch 24  65.7% | batch:       113 of       172\t|\tloss: 0.580212\n",
      "Evaluating Epoch 24  66.3% | batch:       114 of       172\t|\tloss: 0.484221\n",
      "Evaluating Epoch 24  66.9% | batch:       115 of       172\t|\tloss: 0.515294\n",
      "Evaluating Epoch 24  67.4% | batch:       116 of       172\t|\tloss: 0.241414\n",
      "Evaluating Epoch 24  68.0% | batch:       117 of       172\t|\tloss: 0.371559\n",
      "Evaluating Epoch 24  68.6% | batch:       118 of       172\t|\tloss: 0.326127\n",
      "Evaluating Epoch 24  69.2% | batch:       119 of       172\t|\tloss: 0.225629\n",
      "Evaluating Epoch 24  69.8% | batch:       120 of       172\t|\tloss: 0.395317\n",
      "Evaluating Epoch 24  70.3% | batch:       121 of       172\t|\tloss: 0.557309\n",
      "Evaluating Epoch 24  70.9% | batch:       122 of       172\t|\tloss: 0.42803\n",
      "Evaluating Epoch 24  71.5% | batch:       123 of       172\t|\tloss: 0.504658\n",
      "Evaluating Epoch 24  72.1% | batch:       124 of       172\t|\tloss: 0.515488\n",
      "Evaluating Epoch 24  72.7% | batch:       125 of       172\t|\tloss: 0.379721\n",
      "Evaluating Epoch 24  73.3% | batch:       126 of       172\t|\tloss: 0.243923\n",
      "Evaluating Epoch 24  73.8% | batch:       127 of       172\t|\tloss: 0.464561\n",
      "Evaluating Epoch 24  74.4% | batch:       128 of       172\t|\tloss: 0.513231\n",
      "Evaluating Epoch 24  75.0% | batch:       129 of       172\t|\tloss: 0.385412\n",
      "Evaluating Epoch 24  75.6% | batch:       130 of       172\t|\tloss: 0.511281\n",
      "Evaluating Epoch 24  76.2% | batch:       131 of       172\t|\tloss: 0.508876\n",
      "Evaluating Epoch 24  76.7% | batch:       132 of       172\t|\tloss: 0.325559\n",
      "Evaluating Epoch 24  77.3% | batch:       133 of       172\t|\tloss: 0.527258\n",
      "Evaluating Epoch 24  77.9% | batch:       134 of       172\t|\tloss: 0.362882\n",
      "Evaluating Epoch 24  78.5% | batch:       135 of       172\t|\tloss: 0.388119\n",
      "Evaluating Epoch 24  79.1% | batch:       136 of       172\t|\tloss: 0.350943\n",
      "Evaluating Epoch 24  79.7% | batch:       137 of       172\t|\tloss: 0.305168\n",
      "Evaluating Epoch 24  80.2% | batch:       138 of       172\t|\tloss: 0.358812\n",
      "Evaluating Epoch 24  80.8% | batch:       139 of       172\t|\tloss: 0.550931\n",
      "Evaluating Epoch 24  81.4% | batch:       140 of       172\t|\tloss: 0.327495\n",
      "Evaluating Epoch 24  82.0% | batch:       141 of       172\t|\tloss: 0.2226\n",
      "Evaluating Epoch 24  82.6% | batch:       142 of       172\t|\tloss: 0.276007\n",
      "Evaluating Epoch 24  83.1% | batch:       143 of       172\t|\tloss: 0.356203\n",
      "Evaluating Epoch 24  83.7% | batch:       144 of       172\t|\tloss: 0.333399\n",
      "Evaluating Epoch 24  84.3% | batch:       145 of       172\t|\tloss: 0.472075\n",
      "Evaluating Epoch 24  84.9% | batch:       146 of       172\t|\tloss: 0.385374\n",
      "Evaluating Epoch 24  85.5% | batch:       147 of       172\t|\tloss: 0.420451\n",
      "Evaluating Epoch 24  86.0% | batch:       148 of       172\t|\tloss: 0.3228\n",
      "Evaluating Epoch 24  86.6% | batch:       149 of       172\t|\tloss: 0.392483\n",
      "Evaluating Epoch 24  87.2% | batch:       150 of       172\t|\tloss: 0.209093\n",
      "Evaluating Epoch 24  87.8% | batch:       151 of       172\t|\tloss: 0.189627\n",
      "Evaluating Epoch 24  88.4% | batch:       152 of       172\t|\tloss: 0.213194\n",
      "Evaluating Epoch 24  89.0% | batch:       153 of       172\t|\tloss: 0.179116\n",
      "Evaluating Epoch 24  89.5% | batch:       154 of       172\t|\tloss: 0.116756\n",
      "Evaluating Epoch 24  90.1% | batch:       155 of       172\t|\tloss: 0.271623\n",
      "Evaluating Epoch 24  90.7% | batch:       156 of       172\t|\tloss: 0.229956\n",
      "Evaluating Epoch 24  91.3% | batch:       157 of       172\t|\tloss: 0.277654\n",
      "Evaluating Epoch 24  91.9% | batch:       158 of       172\t|\tloss: 0.266082\n",
      "Evaluating Epoch 24  92.4% | batch:       159 of       172\t|\tloss: 0.257078\n",
      "Evaluating Epoch 24  93.0% | batch:       160 of       172\t|\tloss: 0.634307\n",
      "Evaluating Epoch 24  93.6% | batch:       161 of       172\t|\tloss: 0.433878\n",
      "Evaluating Epoch 24  94.2% | batch:       162 of       172\t|\tloss: 0.174218\n",
      "Evaluating Epoch 24  94.8% | batch:       163 of       172\t|\tloss: 0.199551\n",
      "Evaluating Epoch 24  95.3% | batch:       164 of       172\t|\tloss: 0.233427\n",
      "Evaluating Epoch 24  95.9% | batch:       165 of       172\t|\tloss: 0.194134\n",
      "Evaluating Epoch 24  96.5% | batch:       166 of       172\t|\tloss: 0.180508\n",
      "Evaluating Epoch 24  97.1% | batch:       167 of       172\t|\tloss: 0.199588\n",
      "Evaluating Epoch 24  97.7% | batch:       168 of       172\t|\tloss: 0.1444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:11:07,603 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 4.233959436416626 seconds\n",
      "\n",
      "2023-05-25 22:11:07,604 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.058123064041138 seconds\n",
      "2023-05-25 22:11:07,605 | INFO : Avg batch val. time: 0.02359373874442522 seconds\n",
      "2023-05-25 22:11:07,605 | INFO : Avg sample val. time: 0.0001848213810648603 seconds\n",
      "2023-05-25 22:11:07,606 | INFO : Epoch 24 Validation Summary: epoch: 24.000000 | loss: 1.187186 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 24  98.3% | batch:       169 of       172\t|\tloss: 0.200987\n",
      "Evaluating Epoch 24  98.8% | batch:       170 of       172\t|\tloss: 0.167469\n",
      "Evaluating Epoch 24  99.4% | batch:       171 of       172\t|\tloss: 0.110046\n",
      "\n",
      "Training Epoch 25   0.0% | batch:         0 of       686\t|\tloss: 0.804848\n",
      "Training Epoch 25   0.1% | batch:         1 of       686\t|\tloss: 1.17917\n",
      "Training Epoch 25   0.3% | batch:         2 of       686\t|\tloss: 0.786424\n",
      "Training Epoch 25   0.4% | batch:         3 of       686\t|\tloss: 0.984223\n",
      "Training Epoch 25   0.6% | batch:         4 of       686\t|\tloss: 0.835134\n",
      "Training Epoch 25   0.7% | batch:         5 of       686\t|\tloss: 1.01309\n",
      "Training Epoch 25   0.9% | batch:         6 of       686\t|\tloss: 0.955855\n",
      "Training Epoch 25   1.0% | batch:         7 of       686\t|\tloss: 1.07194\n",
      "Training Epoch 25   1.2% | batch:         8 of       686\t|\tloss: 0.899818\n",
      "Training Epoch 25   1.3% | batch:         9 of       686\t|\tloss: 1.06902\n",
      "Training Epoch 25   1.5% | batch:        10 of       686\t|\tloss: 0.977432\n",
      "Training Epoch 25   1.6% | batch:        11 of       686\t|\tloss: 0.987897\n",
      "Training Epoch 25   1.7% | batch:        12 of       686\t|\tloss: 1.22467\n",
      "Training Epoch 25   1.9% | batch:        13 of       686\t|\tloss: 1.2455\n",
      "Training Epoch 25   2.0% | batch:        14 of       686\t|\tloss: 1.04292\n",
      "Training Epoch 25   2.2% | batch:        15 of       686\t|\tloss: 1.23074\n",
      "Training Epoch 25   2.3% | batch:        16 of       686\t|\tloss: 1.10102\n",
      "Training Epoch 25   2.5% | batch:        17 of       686\t|\tloss: 0.945998\n",
      "Training Epoch 25   2.6% | batch:        18 of       686\t|\tloss: 0.859132\n",
      "Training Epoch 25   2.8% | batch:        19 of       686\t|\tloss: 0.997476\n",
      "Training Epoch 25   2.9% | batch:        20 of       686\t|\tloss: 0.867899\n",
      "Training Epoch 25   3.1% | batch:        21 of       686\t|\tloss: 0.642677\n",
      "Training Epoch 25   3.2% | batch:        22 of       686\t|\tloss: 0.907516\n",
      "Training Epoch 25   3.4% | batch:        23 of       686\t|\tloss: 1.21994\n",
      "Training Epoch 25   3.5% | batch:        24 of       686\t|\tloss: 1.03613\n",
      "Training Epoch 25   3.6% | batch:        25 of       686\t|\tloss: 0.997801\n",
      "Training Epoch 25   3.8% | batch:        26 of       686\t|\tloss: 0.948314\n",
      "Training Epoch 25   3.9% | batch:        27 of       686\t|\tloss: 1.13033\n",
      "Training Epoch 25   4.1% | batch:        28 of       686\t|\tloss: 0.880914\n",
      "Training Epoch 25   4.2% | batch:        29 of       686\t|\tloss: 1.02792\n",
      "Training Epoch 25   4.4% | batch:        30 of       686\t|\tloss: 1.00421\n",
      "Training Epoch 25   4.5% | batch:        31 of       686\t|\tloss: 0.924959\n",
      "Training Epoch 25   4.7% | batch:        32 of       686\t|\tloss: 1.29708\n",
      "Training Epoch 25   4.8% | batch:        33 of       686\t|\tloss: 0.87752\n",
      "Training Epoch 25   5.0% | batch:        34 of       686\t|\tloss: 1.17581\n",
      "Training Epoch 25   5.1% | batch:        35 of       686\t|\tloss: 1.16258\n",
      "Training Epoch 25   5.2% | batch:        36 of       686\t|\tloss: 1.04482\n",
      "Training Epoch 25   5.4% | batch:        37 of       686\t|\tloss: 1.08759\n",
      "Training Epoch 25   5.5% | batch:        38 of       686\t|\tloss: 0.944237\n",
      "Training Epoch 25   5.7% | batch:        39 of       686\t|\tloss: 0.891815\n",
      "Training Epoch 25   5.8% | batch:        40 of       686\t|\tloss: 1.01728\n",
      "Training Epoch 25   6.0% | batch:        41 of       686\t|\tloss: 0.884711\n",
      "Training Epoch 25   6.1% | batch:        42 of       686\t|\tloss: 1.10059\n",
      "Training Epoch 25   6.3% | batch:        43 of       686\t|\tloss: 1.10457\n",
      "Training Epoch 25   6.4% | batch:        44 of       686\t|\tloss: 0.984279\n",
      "Training Epoch 25   6.6% | batch:        45 of       686\t|\tloss: 0.906626\n",
      "Training Epoch 25   6.7% | batch:        46 of       686\t|\tloss: 0.991912\n",
      "Training Epoch 25   6.9% | batch:        47 of       686\t|\tloss: 1.28607\n",
      "Training Epoch 25   7.0% | batch:        48 of       686\t|\tloss: 1.42337\n",
      "Training Epoch 25   7.1% | batch:        49 of       686\t|\tloss: 0.91703\n",
      "Training Epoch 25   7.3% | batch:        50 of       686\t|\tloss: 1.04997\n",
      "Training Epoch 25   7.4% | batch:        51 of       686\t|\tloss: 1.33044\n",
      "Training Epoch 25   7.6% | batch:        52 of       686\t|\tloss: 0.945389\n",
      "Training Epoch 25   7.7% | batch:        53 of       686\t|\tloss: 0.953122\n",
      "Training Epoch 25   7.9% | batch:        54 of       686\t|\tloss: 0.804196\n",
      "Training Epoch 25   8.0% | batch:        55 of       686\t|\tloss: 0.803103\n",
      "Training Epoch 25   8.2% | batch:        56 of       686\t|\tloss: 1.17541\n",
      "Training Epoch 25   8.3% | batch:        57 of       686\t|\tloss: 0.715578\n",
      "Training Epoch 25   8.5% | batch:        58 of       686\t|\tloss: 1.41111\n",
      "Training Epoch 25   8.6% | batch:        59 of       686\t|\tloss: 0.772726\n",
      "Training Epoch 25   8.7% | batch:        60 of       686\t|\tloss: 0.865606\n",
      "Training Epoch 25   8.9% | batch:        61 of       686\t|\tloss: 0.911803\n",
      "Training Epoch 25   9.0% | batch:        62 of       686\t|\tloss: 0.943414\n",
      "Training Epoch 25   9.2% | batch:        63 of       686\t|\tloss: 1.13892\n",
      "Training Epoch 25   9.3% | batch:        64 of       686\t|\tloss: 1.01176\n",
      "Training Epoch 25   9.5% | batch:        65 of       686\t|\tloss: 0.847272\n",
      "Training Epoch 25   9.6% | batch:        66 of       686\t|\tloss: 1.07442\n",
      "Training Epoch 25   9.8% | batch:        67 of       686\t|\tloss: 1.10187\n",
      "Training Epoch 25   9.9% | batch:        68 of       686\t|\tloss: 1.28588\n",
      "Training Epoch 25  10.1% | batch:        69 of       686\t|\tloss: 0.92693\n",
      "Training Epoch 25  10.2% | batch:        70 of       686\t|\tloss: 1.15411\n",
      "Training Epoch 25  10.3% | batch:        71 of       686\t|\tloss: 0.954533\n",
      "Training Epoch 25  10.5% | batch:        72 of       686\t|\tloss: 0.889196\n",
      "Training Epoch 25  10.6% | batch:        73 of       686\t|\tloss: 1.08102\n",
      "Training Epoch 25  10.8% | batch:        74 of       686\t|\tloss: 0.772041\n",
      "Training Epoch 25  10.9% | batch:        75 of       686\t|\tloss: 1.02804\n",
      "Training Epoch 25  11.1% | batch:        76 of       686\t|\tloss: 1.12338\n",
      "Training Epoch 25  11.2% | batch:        77 of       686\t|\tloss: 0.814196\n",
      "Training Epoch 25  11.4% | batch:        78 of       686\t|\tloss: 1.17729\n",
      "Training Epoch 25  11.5% | batch:        79 of       686\t|\tloss: 0.914856\n",
      "Training Epoch 25  11.7% | batch:        80 of       686\t|\tloss: 1.07063\n",
      "Training Epoch 25  11.8% | batch:        81 of       686\t|\tloss: 0.903447\n",
      "Training Epoch 25  12.0% | batch:        82 of       686\t|\tloss: 0.848224\n",
      "Training Epoch 25  12.1% | batch:        83 of       686\t|\tloss: 1.01181\n",
      "Training Epoch 25  12.2% | batch:        84 of       686\t|\tloss: 1.02974\n",
      "Training Epoch 25  12.4% | batch:        85 of       686\t|\tloss: 1.19334\n",
      "Training Epoch 25  12.5% | batch:        86 of       686\t|\tloss: 0.878107\n",
      "Training Epoch 25  12.7% | batch:        87 of       686\t|\tloss: 0.770091\n",
      "Training Epoch 25  12.8% | batch:        88 of       686\t|\tloss: 1.09675\n",
      "Training Epoch 25  13.0% | batch:        89 of       686\t|\tloss: 1.0326\n",
      "Training Epoch 25  13.1% | batch:        90 of       686\t|\tloss: 0.965034\n",
      "Training Epoch 25  13.3% | batch:        91 of       686\t|\tloss: 1.0709\n",
      "Training Epoch 25  13.4% | batch:        92 of       686\t|\tloss: 1.04032\n",
      "Training Epoch 25  13.6% | batch:        93 of       686\t|\tloss: 0.80206\n",
      "Training Epoch 25  13.7% | batch:        94 of       686\t|\tloss: 0.792981\n",
      "Training Epoch 25  13.8% | batch:        95 of       686\t|\tloss: 0.9541\n",
      "Training Epoch 25  14.0% | batch:        96 of       686\t|\tloss: 1.26275\n",
      "Training Epoch 25  14.1% | batch:        97 of       686\t|\tloss: 0.871774\n",
      "Training Epoch 25  14.3% | batch:        98 of       686\t|\tloss: 1.02926\n",
      "Training Epoch 25  14.4% | batch:        99 of       686\t|\tloss: 0.961354\n",
      "Training Epoch 25  14.6% | batch:       100 of       686\t|\tloss: 1.07806\n",
      "Training Epoch 25  14.7% | batch:       101 of       686\t|\tloss: 0.982576\n",
      "Training Epoch 25  14.9% | batch:       102 of       686\t|\tloss: 0.775074\n",
      "Training Epoch 25  15.0% | batch:       103 of       686\t|\tloss: 1.01334\n",
      "Training Epoch 25  15.2% | batch:       104 of       686\t|\tloss: 0.925553\n",
      "Training Epoch 25  15.3% | batch:       105 of       686\t|\tloss: 0.965595\n",
      "Training Epoch 25  15.5% | batch:       106 of       686\t|\tloss: 0.844678\n",
      "Training Epoch 25  15.6% | batch:       107 of       686\t|\tloss: 1.12719\n",
      "Training Epoch 25  15.7% | batch:       108 of       686\t|\tloss: 0.703532\n",
      "Training Epoch 25  15.9% | batch:       109 of       686\t|\tloss: 0.876581\n",
      "Training Epoch 25  16.0% | batch:       110 of       686\t|\tloss: 1.21086\n",
      "Training Epoch 25  16.2% | batch:       111 of       686\t|\tloss: 1.05946\n",
      "Training Epoch 25  16.3% | batch:       112 of       686\t|\tloss: 0.887075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 25  16.5% | batch:       113 of       686\t|\tloss: 1.29335\n",
      "Training Epoch 25  16.6% | batch:       114 of       686\t|\tloss: 1.02428\n",
      "Training Epoch 25  16.8% | batch:       115 of       686\t|\tloss: 1.2577\n",
      "Training Epoch 25  16.9% | batch:       116 of       686\t|\tloss: 0.937346\n",
      "Training Epoch 25  17.1% | batch:       117 of       686\t|\tloss: 1.34782\n",
      "Training Epoch 25  17.2% | batch:       118 of       686\t|\tloss: 0.955096\n",
      "Training Epoch 25  17.3% | batch:       119 of       686\t|\tloss: 1.18856\n",
      "Training Epoch 25  17.5% | batch:       120 of       686\t|\tloss: 1.22676\n",
      "Training Epoch 25  17.6% | batch:       121 of       686\t|\tloss: 1.09026\n",
      "Training Epoch 25  17.8% | batch:       122 of       686\t|\tloss: 1.08834\n",
      "Training Epoch 25  17.9% | batch:       123 of       686\t|\tloss: 1.19189\n",
      "Training Epoch 25  18.1% | batch:       124 of       686\t|\tloss: 1.18679\n",
      "Training Epoch 25  18.2% | batch:       125 of       686\t|\tloss: 0.880367\n",
      "Training Epoch 25  18.4% | batch:       126 of       686\t|\tloss: 0.764982\n",
      "Training Epoch 25  18.5% | batch:       127 of       686\t|\tloss: 1.03852\n",
      "Training Epoch 25  18.7% | batch:       128 of       686\t|\tloss: 0.949714\n",
      "Training Epoch 25  18.8% | batch:       129 of       686\t|\tloss: 0.885469\n",
      "Training Epoch 25  19.0% | batch:       130 of       686\t|\tloss: 0.909891\n",
      "Training Epoch 25  19.1% | batch:       131 of       686\t|\tloss: 0.877791\n",
      "Training Epoch 25  19.2% | batch:       132 of       686\t|\tloss: 0.846691\n",
      "Training Epoch 25  19.4% | batch:       133 of       686\t|\tloss: 1.10853\n",
      "Training Epoch 25  19.5% | batch:       134 of       686\t|\tloss: 1.08478\n",
      "Training Epoch 25  19.7% | batch:       135 of       686\t|\tloss: 1.09865\n",
      "Training Epoch 25  19.8% | batch:       136 of       686\t|\tloss: 0.705271\n",
      "Training Epoch 25  20.0% | batch:       137 of       686\t|\tloss: 0.844172\n",
      "Training Epoch 25  20.1% | batch:       138 of       686\t|\tloss: 1.01893\n",
      "Training Epoch 25  20.3% | batch:       139 of       686\t|\tloss: 0.988941\n",
      "Training Epoch 25  20.4% | batch:       140 of       686\t|\tloss: 0.932111\n",
      "Training Epoch 25  20.6% | batch:       141 of       686\t|\tloss: 0.875756\n",
      "Training Epoch 25  20.7% | batch:       142 of       686\t|\tloss: 1.15599\n",
      "Training Epoch 25  20.8% | batch:       143 of       686\t|\tloss: 0.926668\n",
      "Training Epoch 25  21.0% | batch:       144 of       686\t|\tloss: 0.806077\n",
      "Training Epoch 25  21.1% | batch:       145 of       686\t|\tloss: 1.05711\n",
      "Training Epoch 25  21.3% | batch:       146 of       686\t|\tloss: 1.06029\n",
      "Training Epoch 25  21.4% | batch:       147 of       686\t|\tloss: 0.773534\n",
      "Training Epoch 25  21.6% | batch:       148 of       686\t|\tloss: 1.11597\n",
      "Training Epoch 25  21.7% | batch:       149 of       686\t|\tloss: 1.06656\n",
      "Training Epoch 25  21.9% | batch:       150 of       686\t|\tloss: 1.06949\n",
      "Training Epoch 25  22.0% | batch:       151 of       686\t|\tloss: 1.08002\n",
      "Training Epoch 25  22.2% | batch:       152 of       686\t|\tloss: 0.905027\n",
      "Training Epoch 25  22.3% | batch:       153 of       686\t|\tloss: 1.12765\n",
      "Training Epoch 25  22.4% | batch:       154 of       686\t|\tloss: 1.17127\n",
      "Training Epoch 25  22.6% | batch:       155 of       686\t|\tloss: 0.945508\n",
      "Training Epoch 25  22.7% | batch:       156 of       686\t|\tloss: 0.925185\n",
      "Training Epoch 25  22.9% | batch:       157 of       686\t|\tloss: 0.980828\n",
      "Training Epoch 25  23.0% | batch:       158 of       686\t|\tloss: 1.13051\n",
      "Training Epoch 25  23.2% | batch:       159 of       686\t|\tloss: 1.15612\n",
      "Training Epoch 25  23.3% | batch:       160 of       686\t|\tloss: 1.06445\n",
      "Training Epoch 25  23.5% | batch:       161 of       686\t|\tloss: 0.846517\n",
      "Training Epoch 25  23.6% | batch:       162 of       686\t|\tloss: 0.917377\n",
      "Training Epoch 25  23.8% | batch:       163 of       686\t|\tloss: 1.58647\n",
      "Training Epoch 25  23.9% | batch:       164 of       686\t|\tloss: 0.975298\n",
      "Training Epoch 25  24.1% | batch:       165 of       686\t|\tloss: 0.84567\n",
      "Training Epoch 25  24.2% | batch:       166 of       686\t|\tloss: 1.22615\n",
      "Training Epoch 25  24.3% | batch:       167 of       686\t|\tloss: 1.24699\n",
      "Training Epoch 25  24.5% | batch:       168 of       686\t|\tloss: 1.27702\n",
      "Training Epoch 25  24.6% | batch:       169 of       686\t|\tloss: 1.0266\n",
      "Training Epoch 25  24.8% | batch:       170 of       686\t|\tloss: 0.995628\n",
      "Training Epoch 25  24.9% | batch:       171 of       686\t|\tloss: 1.21475\n",
      "Training Epoch 25  25.1% | batch:       172 of       686\t|\tloss: 1.095\n",
      "Training Epoch 25  25.2% | batch:       173 of       686\t|\tloss: 0.818381\n",
      "Training Epoch 25  25.4% | batch:       174 of       686\t|\tloss: 0.867999\n",
      "Training Epoch 25  25.5% | batch:       175 of       686\t|\tloss: 0.955443\n",
      "Training Epoch 25  25.7% | batch:       176 of       686\t|\tloss: 0.970627\n",
      "Training Epoch 25  25.8% | batch:       177 of       686\t|\tloss: 1.11128\n",
      "Training Epoch 25  25.9% | batch:       178 of       686\t|\tloss: 0.774432\n",
      "Training Epoch 25  26.1% | batch:       179 of       686\t|\tloss: 0.836132\n",
      "Training Epoch 25  26.2% | batch:       180 of       686\t|\tloss: 1.13751\n",
      "Training Epoch 25  26.4% | batch:       181 of       686\t|\tloss: 0.852921\n",
      "Training Epoch 25  26.5% | batch:       182 of       686\t|\tloss: 0.730612\n",
      "Training Epoch 25  26.7% | batch:       183 of       686\t|\tloss: 1.58064\n",
      "Training Epoch 25  26.8% | batch:       184 of       686\t|\tloss: 1.01493\n",
      "Training Epoch 25  27.0% | batch:       185 of       686\t|\tloss: 1.00104\n",
      "Training Epoch 25  27.1% | batch:       186 of       686\t|\tloss: 1.17017\n",
      "Training Epoch 25  27.3% | batch:       187 of       686\t|\tloss: 0.983075\n",
      "Training Epoch 25  27.4% | batch:       188 of       686\t|\tloss: 1.08403\n",
      "Training Epoch 25  27.6% | batch:       189 of       686\t|\tloss: 0.986761\n",
      "Training Epoch 25  27.7% | batch:       190 of       686\t|\tloss: 1.01665\n",
      "Training Epoch 25  27.8% | batch:       191 of       686\t|\tloss: 1.03335\n",
      "Training Epoch 25  28.0% | batch:       192 of       686\t|\tloss: 0.872526\n",
      "Training Epoch 25  28.1% | batch:       193 of       686\t|\tloss: 0.980038\n",
      "Training Epoch 25  28.3% | batch:       194 of       686\t|\tloss: 0.907012\n",
      "Training Epoch 25  28.4% | batch:       195 of       686\t|\tloss: 0.920588\n",
      "Training Epoch 25  28.6% | batch:       196 of       686\t|\tloss: 0.801062\n",
      "Training Epoch 25  28.7% | batch:       197 of       686\t|\tloss: 1.05411\n",
      "Training Epoch 25  28.9% | batch:       198 of       686\t|\tloss: 1.01464\n",
      "Training Epoch 25  29.0% | batch:       199 of       686\t|\tloss: 0.928184\n",
      "Training Epoch 25  29.2% | batch:       200 of       686\t|\tloss: 0.907873\n",
      "Training Epoch 25  29.3% | batch:       201 of       686\t|\tloss: 0.837945\n",
      "Training Epoch 25  29.4% | batch:       202 of       686\t|\tloss: 0.809362\n",
      "Training Epoch 25  29.6% | batch:       203 of       686\t|\tloss: 0.909382\n",
      "Training Epoch 25  29.7% | batch:       204 of       686\t|\tloss: 0.950943\n",
      "Training Epoch 25  29.9% | batch:       205 of       686\t|\tloss: 0.786158\n",
      "Training Epoch 25  30.0% | batch:       206 of       686\t|\tloss: 1.02053\n",
      "Training Epoch 25  30.2% | batch:       207 of       686\t|\tloss: 0.860178\n",
      "Training Epoch 25  30.3% | batch:       208 of       686\t|\tloss: 1.15949\n",
      "Training Epoch 25  30.5% | batch:       209 of       686\t|\tloss: 0.857322\n",
      "Training Epoch 25  30.6% | batch:       210 of       686\t|\tloss: 1.00563\n",
      "Training Epoch 25  30.8% | batch:       211 of       686\t|\tloss: 0.752728\n",
      "Training Epoch 25  30.9% | batch:       212 of       686\t|\tloss: 1.436\n",
      "Training Epoch 25  31.0% | batch:       213 of       686\t|\tloss: 1.28769\n",
      "Training Epoch 25  31.2% | batch:       214 of       686\t|\tloss: 1.11986\n",
      "Training Epoch 25  31.3% | batch:       215 of       686\t|\tloss: 0.828046\n",
      "Training Epoch 25  31.5% | batch:       216 of       686\t|\tloss: 0.818414\n",
      "Training Epoch 25  31.6% | batch:       217 of       686\t|\tloss: 1.23825\n",
      "Training Epoch 25  31.8% | batch:       218 of       686\t|\tloss: 1.12184\n",
      "Training Epoch 25  31.9% | batch:       219 of       686\t|\tloss: 1.06964\n",
      "Training Epoch 25  32.1% | batch:       220 of       686\t|\tloss: 1.07304\n",
      "Training Epoch 25  32.2% | batch:       221 of       686\t|\tloss: 0.738332\n",
      "Training Epoch 25  32.4% | batch:       222 of       686\t|\tloss: 1.08398\n",
      "Training Epoch 25  32.5% | batch:       223 of       686\t|\tloss: 0.988347\n",
      "Training Epoch 25  32.7% | batch:       224 of       686\t|\tloss: 0.847361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 25  32.8% | batch:       225 of       686\t|\tloss: 1.17014\n",
      "Training Epoch 25  32.9% | batch:       226 of       686\t|\tloss: 0.878604\n",
      "Training Epoch 25  33.1% | batch:       227 of       686\t|\tloss: 1.21716\n",
      "Training Epoch 25  33.2% | batch:       228 of       686\t|\tloss: 0.93504\n",
      "Training Epoch 25  33.4% | batch:       229 of       686\t|\tloss: 0.959618\n",
      "Training Epoch 25  33.5% | batch:       230 of       686\t|\tloss: 0.95989\n",
      "Training Epoch 25  33.7% | batch:       231 of       686\t|\tloss: 0.834512\n",
      "Training Epoch 25  33.8% | batch:       232 of       686\t|\tloss: 0.960653\n",
      "Training Epoch 25  34.0% | batch:       233 of       686\t|\tloss: 0.970773\n",
      "Training Epoch 25  34.1% | batch:       234 of       686\t|\tloss: 0.787585\n",
      "Training Epoch 25  34.3% | batch:       235 of       686\t|\tloss: 1.35617\n",
      "Training Epoch 25  34.4% | batch:       236 of       686\t|\tloss: 1.05843\n",
      "Training Epoch 25  34.5% | batch:       237 of       686\t|\tloss: 1.08911\n",
      "Training Epoch 25  34.7% | batch:       238 of       686\t|\tloss: 1.1764\n",
      "Training Epoch 25  34.8% | batch:       239 of       686\t|\tloss: 0.962098\n",
      "Training Epoch 25  35.0% | batch:       240 of       686\t|\tloss: 0.973281\n",
      "Training Epoch 25  35.1% | batch:       241 of       686\t|\tloss: 0.732942\n",
      "Training Epoch 25  35.3% | batch:       242 of       686\t|\tloss: 0.848017\n",
      "Training Epoch 25  35.4% | batch:       243 of       686\t|\tloss: 0.851114\n",
      "Training Epoch 25  35.6% | batch:       244 of       686\t|\tloss: 0.960555\n",
      "Training Epoch 25  35.7% | batch:       245 of       686\t|\tloss: 1.09048\n",
      "Training Epoch 25  35.9% | batch:       246 of       686\t|\tloss: 0.939248\n",
      "Training Epoch 25  36.0% | batch:       247 of       686\t|\tloss: 0.820135\n",
      "Training Epoch 25  36.2% | batch:       248 of       686\t|\tloss: 1.12125\n",
      "Training Epoch 25  36.3% | batch:       249 of       686\t|\tloss: 0.869277\n",
      "Training Epoch 25  36.4% | batch:       250 of       686\t|\tloss: 1.08412\n",
      "Training Epoch 25  36.6% | batch:       251 of       686\t|\tloss: 1.04686\n",
      "Training Epoch 25  36.7% | batch:       252 of       686\t|\tloss: 0.68886\n",
      "Training Epoch 25  36.9% | batch:       253 of       686\t|\tloss: 0.861915\n",
      "Training Epoch 25  37.0% | batch:       254 of       686\t|\tloss: 1.11662\n",
      "Training Epoch 25  37.2% | batch:       255 of       686\t|\tloss: 0.820967\n",
      "Training Epoch 25  37.3% | batch:       256 of       686\t|\tloss: 0.808463\n",
      "Training Epoch 25  37.5% | batch:       257 of       686\t|\tloss: 0.988568\n",
      "Training Epoch 25  37.6% | batch:       258 of       686\t|\tloss: 0.975488\n",
      "Training Epoch 25  37.8% | batch:       259 of       686\t|\tloss: 0.983944\n",
      "Training Epoch 25  37.9% | batch:       260 of       686\t|\tloss: 1.15191\n",
      "Training Epoch 25  38.0% | batch:       261 of       686\t|\tloss: 1.0802\n",
      "Training Epoch 25  38.2% | batch:       262 of       686\t|\tloss: 1.26475\n",
      "Training Epoch 25  38.3% | batch:       263 of       686\t|\tloss: 0.8892\n",
      "Training Epoch 25  38.5% | batch:       264 of       686\t|\tloss: 0.732604\n",
      "Training Epoch 25  38.6% | batch:       265 of       686\t|\tloss: 0.999221\n",
      "Training Epoch 25  38.8% | batch:       266 of       686\t|\tloss: 0.943369\n",
      "Training Epoch 25  38.9% | batch:       267 of       686\t|\tloss: 0.838817\n",
      "Training Epoch 25  39.1% | batch:       268 of       686\t|\tloss: 1.24258\n",
      "Training Epoch 25  39.2% | batch:       269 of       686\t|\tloss: 0.842782\n",
      "Training Epoch 25  39.4% | batch:       270 of       686\t|\tloss: 0.915459\n",
      "Training Epoch 25  39.5% | batch:       271 of       686\t|\tloss: 1.08266\n",
      "Training Epoch 25  39.7% | batch:       272 of       686\t|\tloss: 0.979343\n",
      "Training Epoch 25  39.8% | batch:       273 of       686\t|\tloss: 0.962641\n",
      "Training Epoch 25  39.9% | batch:       274 of       686\t|\tloss: 1.11615\n",
      "Training Epoch 25  40.1% | batch:       275 of       686\t|\tloss: 0.764007\n",
      "Training Epoch 25  40.2% | batch:       276 of       686\t|\tloss: 1.05724\n",
      "Training Epoch 25  40.4% | batch:       277 of       686\t|\tloss: 1.06126\n",
      "Training Epoch 25  40.5% | batch:       278 of       686\t|\tloss: 0.898123\n",
      "Training Epoch 25  40.7% | batch:       279 of       686\t|\tloss: 0.856745\n",
      "Training Epoch 25  40.8% | batch:       280 of       686\t|\tloss: 1.11691\n",
      "Training Epoch 25  41.0% | batch:       281 of       686\t|\tloss: 1.15509\n",
      "Training Epoch 25  41.1% | batch:       282 of       686\t|\tloss: 0.813499\n",
      "Training Epoch 25  41.3% | batch:       283 of       686\t|\tloss: 0.941501\n",
      "Training Epoch 25  41.4% | batch:       284 of       686\t|\tloss: 1.06937\n",
      "Training Epoch 25  41.5% | batch:       285 of       686\t|\tloss: 0.924436\n",
      "Training Epoch 25  41.7% | batch:       286 of       686\t|\tloss: 1.05388\n",
      "Training Epoch 25  41.8% | batch:       287 of       686\t|\tloss: 1.16499\n",
      "Training Epoch 25  42.0% | batch:       288 of       686\t|\tloss: 1.13556\n",
      "Training Epoch 25  42.1% | batch:       289 of       686\t|\tloss: 0.853552\n",
      "Training Epoch 25  42.3% | batch:       290 of       686\t|\tloss: 0.799558\n",
      "Training Epoch 25  42.4% | batch:       291 of       686\t|\tloss: 1.1564\n",
      "Training Epoch 25  42.6% | batch:       292 of       686\t|\tloss: 0.796705\n",
      "Training Epoch 25  42.7% | batch:       293 of       686\t|\tloss: 0.870011\n",
      "Training Epoch 25  42.9% | batch:       294 of       686\t|\tloss: 0.855379\n",
      "Training Epoch 25  43.0% | batch:       295 of       686\t|\tloss: 1.01262\n",
      "Training Epoch 25  43.1% | batch:       296 of       686\t|\tloss: 1.14114\n",
      "Training Epoch 25  43.3% | batch:       297 of       686\t|\tloss: 0.968854\n",
      "Training Epoch 25  43.4% | batch:       298 of       686\t|\tloss: 0.834634\n",
      "Training Epoch 25  43.6% | batch:       299 of       686\t|\tloss: 0.855619\n",
      "Training Epoch 25  43.7% | batch:       300 of       686\t|\tloss: 0.988359\n",
      "Training Epoch 25  43.9% | batch:       301 of       686\t|\tloss: 1.01397\n",
      "Training Epoch 25  44.0% | batch:       302 of       686\t|\tloss: 0.648734\n",
      "Training Epoch 25  44.2% | batch:       303 of       686\t|\tloss: 1.07465\n",
      "Training Epoch 25  44.3% | batch:       304 of       686\t|\tloss: 1.22757\n",
      "Training Epoch 25  44.5% | batch:       305 of       686\t|\tloss: 0.88067\n",
      "Training Epoch 25  44.6% | batch:       306 of       686\t|\tloss: 0.828165\n",
      "Training Epoch 25  44.8% | batch:       307 of       686\t|\tloss: 1.28962\n",
      "Training Epoch 25  44.9% | batch:       308 of       686\t|\tloss: 0.969856\n",
      "Training Epoch 25  45.0% | batch:       309 of       686\t|\tloss: 0.775575\n",
      "Training Epoch 25  45.2% | batch:       310 of       686\t|\tloss: 0.874887\n",
      "Training Epoch 25  45.3% | batch:       311 of       686\t|\tloss: 0.851298\n",
      "Training Epoch 25  45.5% | batch:       312 of       686\t|\tloss: 1.07802\n",
      "Training Epoch 25  45.6% | batch:       313 of       686\t|\tloss: 1.15663\n",
      "Training Epoch 25  45.8% | batch:       314 of       686\t|\tloss: 1.05393\n",
      "Training Epoch 25  45.9% | batch:       315 of       686\t|\tloss: 1.41267\n",
      "Training Epoch 25  46.1% | batch:       316 of       686\t|\tloss: 0.872898\n",
      "Training Epoch 25  46.2% | batch:       317 of       686\t|\tloss: 0.744809\n",
      "Training Epoch 25  46.4% | batch:       318 of       686\t|\tloss: 0.964617\n",
      "Training Epoch 25  46.5% | batch:       319 of       686\t|\tloss: 0.852193\n",
      "Training Epoch 25  46.6% | batch:       320 of       686\t|\tloss: 1.05582\n",
      "Training Epoch 25  46.8% | batch:       321 of       686\t|\tloss: 0.989893\n",
      "Training Epoch 25  46.9% | batch:       322 of       686\t|\tloss: 1.02402\n",
      "Training Epoch 25  47.1% | batch:       323 of       686\t|\tloss: 0.994061\n",
      "Training Epoch 25  47.2% | batch:       324 of       686\t|\tloss: 0.929818\n",
      "Training Epoch 25  47.4% | batch:       325 of       686\t|\tloss: 1.03278\n",
      "Training Epoch 25  47.5% | batch:       326 of       686\t|\tloss: 0.846709\n",
      "Training Epoch 25  47.7% | batch:       327 of       686\t|\tloss: 0.885641\n",
      "Training Epoch 25  47.8% | batch:       328 of       686\t|\tloss: 1.03787\n",
      "Training Epoch 25  48.0% | batch:       329 of       686\t|\tloss: 0.7636\n",
      "Training Epoch 25  48.1% | batch:       330 of       686\t|\tloss: 0.814078\n",
      "Training Epoch 25  48.3% | batch:       331 of       686\t|\tloss: 1.0102\n",
      "Training Epoch 25  48.4% | batch:       332 of       686\t|\tloss: 0.846515\n",
      "Training Epoch 25  48.5% | batch:       333 of       686\t|\tloss: 1.0396\n",
      "Training Epoch 25  48.7% | batch:       334 of       686\t|\tloss: 0.918816\n",
      "Training Epoch 25  48.8% | batch:       335 of       686\t|\tloss: 1.33856\n",
      "Training Epoch 25  49.0% | batch:       336 of       686\t|\tloss: 0.751868\n",
      "Training Epoch 25  49.1% | batch:       337 of       686\t|\tloss: 0.890842\n",
      "Training Epoch 25  49.3% | batch:       338 of       686\t|\tloss: 0.746215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 25  49.4% | batch:       339 of       686\t|\tloss: 0.77343\n",
      "Training Epoch 25  49.6% | batch:       340 of       686\t|\tloss: 0.584423\n",
      "Training Epoch 25  49.7% | batch:       341 of       686\t|\tloss: 0.771707\n",
      "Training Epoch 25  49.9% | batch:       342 of       686\t|\tloss: 0.772276\n",
      "Training Epoch 25  50.0% | batch:       343 of       686\t|\tloss: 1.14269\n",
      "Training Epoch 25  50.1% | batch:       344 of       686\t|\tloss: 0.849768\n",
      "Training Epoch 25  50.3% | batch:       345 of       686\t|\tloss: 0.789078\n",
      "Training Epoch 25  50.4% | batch:       346 of       686\t|\tloss: 0.80725\n",
      "Training Epoch 25  50.6% | batch:       347 of       686\t|\tloss: 0.723311\n",
      "Training Epoch 25  50.7% | batch:       348 of       686\t|\tloss: 0.906617\n",
      "Training Epoch 25  50.9% | batch:       349 of       686\t|\tloss: 0.941307\n",
      "Training Epoch 25  51.0% | batch:       350 of       686\t|\tloss: 0.801645\n",
      "Training Epoch 25  51.2% | batch:       351 of       686\t|\tloss: 0.887745\n",
      "Training Epoch 25  51.3% | batch:       352 of       686\t|\tloss: 0.95865\n",
      "Training Epoch 25  51.5% | batch:       353 of       686\t|\tloss: 0.883364\n",
      "Training Epoch 25  51.6% | batch:       354 of       686\t|\tloss: 0.755836\n",
      "Training Epoch 25  51.7% | batch:       355 of       686\t|\tloss: 0.738758\n",
      "Training Epoch 25  51.9% | batch:       356 of       686\t|\tloss: 0.983011\n",
      "Training Epoch 25  52.0% | batch:       357 of       686\t|\tloss: 0.820465\n",
      "Training Epoch 25  52.2% | batch:       358 of       686\t|\tloss: 0.890435\n",
      "Training Epoch 25  52.3% | batch:       359 of       686\t|\tloss: 0.795765\n",
      "Training Epoch 25  52.5% | batch:       360 of       686\t|\tloss: 0.813408\n",
      "Training Epoch 25  52.6% | batch:       361 of       686\t|\tloss: 1.03523\n",
      "Training Epoch 25  52.8% | batch:       362 of       686\t|\tloss: 1.07287\n",
      "Training Epoch 25  52.9% | batch:       363 of       686\t|\tloss: 0.91768\n",
      "Training Epoch 25  53.1% | batch:       364 of       686\t|\tloss: 1.17651\n",
      "Training Epoch 25  53.2% | batch:       365 of       686\t|\tloss: 0.868156\n",
      "Training Epoch 25  53.4% | batch:       366 of       686\t|\tloss: 1.09877\n",
      "Training Epoch 25  53.5% | batch:       367 of       686\t|\tloss: 1.03716\n",
      "Training Epoch 25  53.6% | batch:       368 of       686\t|\tloss: 0.747911\n",
      "Training Epoch 25  53.8% | batch:       369 of       686\t|\tloss: 1.54167\n",
      "Training Epoch 25  53.9% | batch:       370 of       686\t|\tloss: 0.905632\n",
      "Training Epoch 25  54.1% | batch:       371 of       686\t|\tloss: 0.995138\n",
      "Training Epoch 25  54.2% | batch:       372 of       686\t|\tloss: 1.07324\n",
      "Training Epoch 25  54.4% | batch:       373 of       686\t|\tloss: 1.02285\n",
      "Training Epoch 25  54.5% | batch:       374 of       686\t|\tloss: 0.760177\n",
      "Training Epoch 25  54.7% | batch:       375 of       686\t|\tloss: 0.923953\n",
      "Training Epoch 25  54.8% | batch:       376 of       686\t|\tloss: 1.05253\n",
      "Training Epoch 25  55.0% | batch:       377 of       686\t|\tloss: 0.985691\n",
      "Training Epoch 25  55.1% | batch:       378 of       686\t|\tloss: 1.20856\n",
      "Training Epoch 25  55.2% | batch:       379 of       686\t|\tloss: 1.02532\n",
      "Training Epoch 25  55.4% | batch:       380 of       686\t|\tloss: 0.876964\n",
      "Training Epoch 25  55.5% | batch:       381 of       686\t|\tloss: 0.909643\n",
      "Training Epoch 25  55.7% | batch:       382 of       686\t|\tloss: 1.10784\n",
      "Training Epoch 25  55.8% | batch:       383 of       686\t|\tloss: 1.20407\n",
      "Training Epoch 25  56.0% | batch:       384 of       686\t|\tloss: 1.1989\n",
      "Training Epoch 25  56.1% | batch:       385 of       686\t|\tloss: 0.874368\n",
      "Training Epoch 25  56.3% | batch:       386 of       686\t|\tloss: 0.995376\n",
      "Training Epoch 25  56.4% | batch:       387 of       686\t|\tloss: 0.884634\n",
      "Training Epoch 25  56.6% | batch:       388 of       686\t|\tloss: 0.924945\n",
      "Training Epoch 25  56.7% | batch:       389 of       686\t|\tloss: 1.05203\n",
      "Training Epoch 25  56.9% | batch:       390 of       686\t|\tloss: 0.966268\n",
      "Training Epoch 25  57.0% | batch:       391 of       686\t|\tloss: 0.941431\n",
      "Training Epoch 25  57.1% | batch:       392 of       686\t|\tloss: 0.896064\n",
      "Training Epoch 25  57.3% | batch:       393 of       686\t|\tloss: 1.04722\n",
      "Training Epoch 25  57.4% | batch:       394 of       686\t|\tloss: 0.919126\n",
      "Training Epoch 25  57.6% | batch:       395 of       686\t|\tloss: 1.07378\n",
      "Training Epoch 25  57.7% | batch:       396 of       686\t|\tloss: 0.730195\n",
      "Training Epoch 25  57.9% | batch:       397 of       686\t|\tloss: 0.887343\n",
      "Training Epoch 25  58.0% | batch:       398 of       686\t|\tloss: 0.839519\n",
      "Training Epoch 25  58.2% | batch:       399 of       686\t|\tloss: 0.614698\n",
      "Training Epoch 25  58.3% | batch:       400 of       686\t|\tloss: 1.02374\n",
      "Training Epoch 25  58.5% | batch:       401 of       686\t|\tloss: 0.912903\n",
      "Training Epoch 25  58.6% | batch:       402 of       686\t|\tloss: 0.933149\n",
      "Training Epoch 25  58.7% | batch:       403 of       686\t|\tloss: 0.92278\n",
      "Training Epoch 25  58.9% | batch:       404 of       686\t|\tloss: 1.11232\n",
      "Training Epoch 25  59.0% | batch:       405 of       686\t|\tloss: 1.06587\n",
      "Training Epoch 25  59.2% | batch:       406 of       686\t|\tloss: 0.691064\n",
      "Training Epoch 25  59.3% | batch:       407 of       686\t|\tloss: 1.61342\n",
      "Training Epoch 25  59.5% | batch:       408 of       686\t|\tloss: 0.860896\n",
      "Training Epoch 25  59.6% | batch:       409 of       686\t|\tloss: 0.862348\n",
      "Training Epoch 25  59.8% | batch:       410 of       686\t|\tloss: 1.02091\n",
      "Training Epoch 25  59.9% | batch:       411 of       686\t|\tloss: 1.10842\n",
      "Training Epoch 25  60.1% | batch:       412 of       686\t|\tloss: 1.17508\n",
      "Training Epoch 25  60.2% | batch:       413 of       686\t|\tloss: 0.948135\n",
      "Training Epoch 25  60.3% | batch:       414 of       686\t|\tloss: 0.706193\n",
      "Training Epoch 25  60.5% | batch:       415 of       686\t|\tloss: 0.956781\n",
      "Training Epoch 25  60.6% | batch:       416 of       686\t|\tloss: 0.841987\n",
      "Training Epoch 25  60.8% | batch:       417 of       686\t|\tloss: 1.00161\n",
      "Training Epoch 25  60.9% | batch:       418 of       686\t|\tloss: 0.743628\n",
      "Training Epoch 25  61.1% | batch:       419 of       686\t|\tloss: 0.952379\n",
      "Training Epoch 25  61.2% | batch:       420 of       686\t|\tloss: 0.928973\n",
      "Training Epoch 25  61.4% | batch:       421 of       686\t|\tloss: 1.10595\n",
      "Training Epoch 25  61.5% | batch:       422 of       686\t|\tloss: 0.981186\n",
      "Training Epoch 25  61.7% | batch:       423 of       686\t|\tloss: 0.962709\n",
      "Training Epoch 25  61.8% | batch:       424 of       686\t|\tloss: 0.786665\n",
      "Training Epoch 25  62.0% | batch:       425 of       686\t|\tloss: 0.883896\n",
      "Training Epoch 25  62.1% | batch:       426 of       686\t|\tloss: 0.89819\n",
      "Training Epoch 25  62.2% | batch:       427 of       686\t|\tloss: 0.936594\n",
      "Training Epoch 25  62.4% | batch:       428 of       686\t|\tloss: 1.02382\n",
      "Training Epoch 25  62.5% | batch:       429 of       686\t|\tloss: 0.834559\n",
      "Training Epoch 25  62.7% | batch:       430 of       686\t|\tloss: 0.904358\n",
      "Training Epoch 25  62.8% | batch:       431 of       686\t|\tloss: 0.944134\n",
      "Training Epoch 25  63.0% | batch:       432 of       686\t|\tloss: 0.830819\n",
      "Training Epoch 25  63.1% | batch:       433 of       686\t|\tloss: 0.952145\n",
      "Training Epoch 25  63.3% | batch:       434 of       686\t|\tloss: 1.11767\n",
      "Training Epoch 25  63.4% | batch:       435 of       686\t|\tloss: 0.974859\n",
      "Training Epoch 25  63.6% | batch:       436 of       686\t|\tloss: 0.857459\n",
      "Training Epoch 25  63.7% | batch:       437 of       686\t|\tloss: 0.977753\n",
      "Training Epoch 25  63.8% | batch:       438 of       686\t|\tloss: 0.947032\n",
      "Training Epoch 25  64.0% | batch:       439 of       686\t|\tloss: 1.36057\n",
      "Training Epoch 25  64.1% | batch:       440 of       686\t|\tloss: 0.801073\n",
      "Training Epoch 25  64.3% | batch:       441 of       686\t|\tloss: 0.911535\n",
      "Training Epoch 25  64.4% | batch:       442 of       686\t|\tloss: 1.13224\n",
      "Training Epoch 25  64.6% | batch:       443 of       686\t|\tloss: 0.768907\n",
      "Training Epoch 25  64.7% | batch:       444 of       686\t|\tloss: 1.04375\n",
      "Training Epoch 25  64.9% | batch:       445 of       686\t|\tloss: 1.02041\n",
      "Training Epoch 25  65.0% | batch:       446 of       686\t|\tloss: 0.879187\n",
      "Training Epoch 25  65.2% | batch:       447 of       686\t|\tloss: 0.741632\n",
      "Training Epoch 25  65.3% | batch:       448 of       686\t|\tloss: 0.791901\n",
      "Training Epoch 25  65.5% | batch:       449 of       686\t|\tloss: 0.889605\n",
      "Training Epoch 25  65.6% | batch:       450 of       686\t|\tloss: 0.803207\n",
      "Training Epoch 25  65.7% | batch:       451 of       686\t|\tloss: 0.977028\n",
      "Training Epoch 25  65.9% | batch:       452 of       686\t|\tloss: 0.814002\n",
      "Training Epoch 25  66.0% | batch:       453 of       686\t|\tloss: 1.01521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 25  66.2% | batch:       454 of       686\t|\tloss: 1.05959\n",
      "Training Epoch 25  66.3% | batch:       455 of       686\t|\tloss: 1.09928\n",
      "Training Epoch 25  66.5% | batch:       456 of       686\t|\tloss: 1.03267\n",
      "Training Epoch 25  66.6% | batch:       457 of       686\t|\tloss: 0.672402\n",
      "Training Epoch 25  66.8% | batch:       458 of       686\t|\tloss: 0.933623\n",
      "Training Epoch 25  66.9% | batch:       459 of       686\t|\tloss: 0.908988\n",
      "Training Epoch 25  67.1% | batch:       460 of       686\t|\tloss: 0.746411\n",
      "Training Epoch 25  67.2% | batch:       461 of       686\t|\tloss: 1.23106\n",
      "Training Epoch 25  67.3% | batch:       462 of       686\t|\tloss: 0.922417\n",
      "Training Epoch 25  67.5% | batch:       463 of       686\t|\tloss: 1.00937\n",
      "Training Epoch 25  67.6% | batch:       464 of       686\t|\tloss: 1.0843\n",
      "Training Epoch 25  67.8% | batch:       465 of       686\t|\tloss: 1.03158\n",
      "Training Epoch 25  67.9% | batch:       466 of       686\t|\tloss: 0.873913\n",
      "Training Epoch 25  68.1% | batch:       467 of       686\t|\tloss: 0.882211\n",
      "Training Epoch 25  68.2% | batch:       468 of       686\t|\tloss: 0.775944\n",
      "Training Epoch 25  68.4% | batch:       469 of       686\t|\tloss: 0.848598\n",
      "Training Epoch 25  68.5% | batch:       470 of       686\t|\tloss: 0.684821\n",
      "Training Epoch 25  68.7% | batch:       471 of       686\t|\tloss: 0.644084\n",
      "Training Epoch 25  68.8% | batch:       472 of       686\t|\tloss: 0.825634\n",
      "Training Epoch 25  69.0% | batch:       473 of       686\t|\tloss: 0.807782\n",
      "Training Epoch 25  69.1% | batch:       474 of       686\t|\tloss: 0.900312\n",
      "Training Epoch 25  69.2% | batch:       475 of       686\t|\tloss: 0.831135\n",
      "Training Epoch 25  69.4% | batch:       476 of       686\t|\tloss: 1.16291\n",
      "Training Epoch 25  69.5% | batch:       477 of       686\t|\tloss: 0.857167\n",
      "Training Epoch 25  69.7% | batch:       478 of       686\t|\tloss: 1.09567\n",
      "Training Epoch 25  69.8% | batch:       479 of       686\t|\tloss: 0.871765\n",
      "Training Epoch 25  70.0% | batch:       480 of       686\t|\tloss: 0.892297\n",
      "Training Epoch 25  70.1% | batch:       481 of       686\t|\tloss: 0.883195\n",
      "Training Epoch 25  70.3% | batch:       482 of       686\t|\tloss: 0.889992\n",
      "Training Epoch 25  70.4% | batch:       483 of       686\t|\tloss: 0.861584\n",
      "Training Epoch 25  70.6% | batch:       484 of       686\t|\tloss: 0.777804\n",
      "Training Epoch 25  70.7% | batch:       485 of       686\t|\tloss: 0.738714\n",
      "Training Epoch 25  70.8% | batch:       486 of       686\t|\tloss: 0.802296\n",
      "Training Epoch 25  71.0% | batch:       487 of       686\t|\tloss: 0.725144\n",
      "Training Epoch 25  71.1% | batch:       488 of       686\t|\tloss: 0.989341\n",
      "Training Epoch 25  71.3% | batch:       489 of       686\t|\tloss: 1.12646\n",
      "Training Epoch 25  71.4% | batch:       490 of       686\t|\tloss: 0.848207\n",
      "Training Epoch 25  71.6% | batch:       491 of       686\t|\tloss: 0.856142\n",
      "Training Epoch 25  71.7% | batch:       492 of       686\t|\tloss: 0.824462\n",
      "Training Epoch 25  71.9% | batch:       493 of       686\t|\tloss: 0.889474\n",
      "Training Epoch 25  72.0% | batch:       494 of       686\t|\tloss: 1.08712\n",
      "Training Epoch 25  72.2% | batch:       495 of       686\t|\tloss: 1.08964\n",
      "Training Epoch 25  72.3% | batch:       496 of       686\t|\tloss: 0.905695\n",
      "Training Epoch 25  72.4% | batch:       497 of       686\t|\tloss: 1.18657\n",
      "Training Epoch 25  72.6% | batch:       498 of       686\t|\tloss: 0.996674\n",
      "Training Epoch 25  72.7% | batch:       499 of       686\t|\tloss: 0.924287\n",
      "Training Epoch 25  72.9% | batch:       500 of       686\t|\tloss: 0.749079\n",
      "Training Epoch 25  73.0% | batch:       501 of       686\t|\tloss: 1.08235\n",
      "Training Epoch 25  73.2% | batch:       502 of       686\t|\tloss: 1.00042\n",
      "Training Epoch 25  73.3% | batch:       503 of       686\t|\tloss: 1.02693\n",
      "Training Epoch 25  73.5% | batch:       504 of       686\t|\tloss: 0.999932\n",
      "Training Epoch 25  73.6% | batch:       505 of       686\t|\tloss: 0.824341\n",
      "Training Epoch 25  73.8% | batch:       506 of       686\t|\tloss: 0.663013\n",
      "Training Epoch 25  73.9% | batch:       507 of       686\t|\tloss: 0.95676\n",
      "Training Epoch 25  74.1% | batch:       508 of       686\t|\tloss: 0.959329\n",
      "Training Epoch 25  74.2% | batch:       509 of       686\t|\tloss: 0.910731\n",
      "Training Epoch 25  74.3% | batch:       510 of       686\t|\tloss: 0.988016\n",
      "Training Epoch 25  74.5% | batch:       511 of       686\t|\tloss: 1.12265\n",
      "Training Epoch 25  74.6% | batch:       512 of       686\t|\tloss: 0.977136\n",
      "Training Epoch 25  74.8% | batch:       513 of       686\t|\tloss: 0.886767\n",
      "Training Epoch 25  74.9% | batch:       514 of       686\t|\tloss: 0.866351\n",
      "Training Epoch 25  75.1% | batch:       515 of       686\t|\tloss: 0.854971\n",
      "Training Epoch 25  75.2% | batch:       516 of       686\t|\tloss: 0.73255\n",
      "Training Epoch 25  75.4% | batch:       517 of       686\t|\tloss: 0.738739\n",
      "Training Epoch 25  75.5% | batch:       518 of       686\t|\tloss: 0.963815\n",
      "Training Epoch 25  75.7% | batch:       519 of       686\t|\tloss: 0.754459\n",
      "Training Epoch 25  75.8% | batch:       520 of       686\t|\tloss: 0.895491\n",
      "Training Epoch 25  75.9% | batch:       521 of       686\t|\tloss: 0.866483\n",
      "Training Epoch 25  76.1% | batch:       522 of       686\t|\tloss: 0.805325\n",
      "Training Epoch 25  76.2% | batch:       523 of       686\t|\tloss: 0.714688\n",
      "Training Epoch 25  76.4% | batch:       524 of       686\t|\tloss: 0.86772\n",
      "Training Epoch 25  76.5% | batch:       525 of       686\t|\tloss: 0.932698\n",
      "Training Epoch 25  76.7% | batch:       526 of       686\t|\tloss: 1.21143\n",
      "Training Epoch 25  76.8% | batch:       527 of       686\t|\tloss: 0.794699\n",
      "Training Epoch 25  77.0% | batch:       528 of       686\t|\tloss: 0.757166\n",
      "Training Epoch 25  77.1% | batch:       529 of       686\t|\tloss: 1.23289\n",
      "Training Epoch 25  77.3% | batch:       530 of       686\t|\tloss: 0.992716\n",
      "Training Epoch 25  77.4% | batch:       531 of       686\t|\tloss: 0.950354\n",
      "Training Epoch 25  77.6% | batch:       532 of       686\t|\tloss: 0.92643\n",
      "Training Epoch 25  77.7% | batch:       533 of       686\t|\tloss: 0.833108\n",
      "Training Epoch 25  77.8% | batch:       534 of       686\t|\tloss: 1.17323\n",
      "Training Epoch 25  78.0% | batch:       535 of       686\t|\tloss: 1.2283\n",
      "Training Epoch 25  78.1% | batch:       536 of       686\t|\tloss: 0.833806\n",
      "Training Epoch 25  78.3% | batch:       537 of       686\t|\tloss: 0.734835\n",
      "Training Epoch 25  78.4% | batch:       538 of       686\t|\tloss: 0.849915\n",
      "Training Epoch 25  78.6% | batch:       539 of       686\t|\tloss: 0.977479\n",
      "Training Epoch 25  78.7% | batch:       540 of       686\t|\tloss: 0.766966\n",
      "Training Epoch 25  78.9% | batch:       541 of       686\t|\tloss: 0.976987\n",
      "Training Epoch 25  79.0% | batch:       542 of       686\t|\tloss: 0.98652\n",
      "Training Epoch 25  79.2% | batch:       543 of       686\t|\tloss: 0.83866\n",
      "Training Epoch 25  79.3% | batch:       544 of       686\t|\tloss: 1.05529\n",
      "Training Epoch 25  79.4% | batch:       545 of       686\t|\tloss: 0.807101\n",
      "Training Epoch 25  79.6% | batch:       546 of       686\t|\tloss: 0.941387\n",
      "Training Epoch 25  79.7% | batch:       547 of       686\t|\tloss: 0.739435\n",
      "Training Epoch 25  79.9% | batch:       548 of       686\t|\tloss: 0.798558\n",
      "Training Epoch 25  80.0% | batch:       549 of       686\t|\tloss: 1.01239\n",
      "Training Epoch 25  80.2% | batch:       550 of       686\t|\tloss: 0.856822\n",
      "Training Epoch 25  80.3% | batch:       551 of       686\t|\tloss: 0.873442\n",
      "Training Epoch 25  80.5% | batch:       552 of       686\t|\tloss: 0.960406\n",
      "Training Epoch 25  80.6% | batch:       553 of       686\t|\tloss: 0.962995\n",
      "Training Epoch 25  80.8% | batch:       554 of       686\t|\tloss: 1.03228\n",
      "Training Epoch 25  80.9% | batch:       555 of       686\t|\tloss: 0.985917\n",
      "Training Epoch 25  81.0% | batch:       556 of       686\t|\tloss: 1.0984\n",
      "Training Epoch 25  81.2% | batch:       557 of       686\t|\tloss: 0.862742\n",
      "Training Epoch 25  81.3% | batch:       558 of       686\t|\tloss: 0.736948\n",
      "Training Epoch 25  81.5% | batch:       559 of       686\t|\tloss: 0.771498\n",
      "Training Epoch 25  81.6% | batch:       560 of       686\t|\tloss: 0.917718\n",
      "Training Epoch 25  81.8% | batch:       561 of       686\t|\tloss: 0.893134\n",
      "Training Epoch 25  81.9% | batch:       562 of       686\t|\tloss: 0.828686\n",
      "Training Epoch 25  82.1% | batch:       563 of       686\t|\tloss: 1.14295\n",
      "Training Epoch 25  82.2% | batch:       564 of       686\t|\tloss: 0.963609\n",
      "Training Epoch 25  82.4% | batch:       565 of       686\t|\tloss: 0.98635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 25  82.5% | batch:       566 of       686\t|\tloss: 0.657457\n",
      "Training Epoch 25  82.7% | batch:       567 of       686\t|\tloss: 0.694358\n",
      "Training Epoch 25  82.8% | batch:       568 of       686\t|\tloss: 0.823961\n",
      "Training Epoch 25  82.9% | batch:       569 of       686\t|\tloss: 0.866808\n",
      "Training Epoch 25  83.1% | batch:       570 of       686\t|\tloss: 1.0322\n",
      "Training Epoch 25  83.2% | batch:       571 of       686\t|\tloss: 1.05517\n",
      "Training Epoch 25  83.4% | batch:       572 of       686\t|\tloss: 1.0779\n",
      "Training Epoch 25  83.5% | batch:       573 of       686\t|\tloss: 0.790329\n",
      "Training Epoch 25  83.7% | batch:       574 of       686\t|\tloss: 0.850133\n",
      "Training Epoch 25  83.8% | batch:       575 of       686\t|\tloss: 0.888599\n",
      "Training Epoch 25  84.0% | batch:       576 of       686\t|\tloss: 0.847187\n",
      "Training Epoch 25  84.1% | batch:       577 of       686\t|\tloss: 1.03828\n",
      "Training Epoch 25  84.3% | batch:       578 of       686\t|\tloss: 0.838722\n",
      "Training Epoch 25  84.4% | batch:       579 of       686\t|\tloss: 0.822127\n",
      "Training Epoch 25  84.5% | batch:       580 of       686\t|\tloss: 0.877979\n",
      "Training Epoch 25  84.7% | batch:       581 of       686\t|\tloss: 0.814513\n",
      "Training Epoch 25  84.8% | batch:       582 of       686\t|\tloss: 0.775517\n",
      "Training Epoch 25  85.0% | batch:       583 of       686\t|\tloss: 1.07106\n",
      "Training Epoch 25  85.1% | batch:       584 of       686\t|\tloss: 0.994143\n",
      "Training Epoch 25  85.3% | batch:       585 of       686\t|\tloss: 1.00802\n",
      "Training Epoch 25  85.4% | batch:       586 of       686\t|\tloss: 0.706529\n",
      "Training Epoch 25  85.6% | batch:       587 of       686\t|\tloss: 0.823829\n",
      "Training Epoch 25  85.7% | batch:       588 of       686\t|\tloss: 0.876632\n",
      "Training Epoch 25  85.9% | batch:       589 of       686\t|\tloss: 0.702176\n",
      "Training Epoch 25  86.0% | batch:       590 of       686\t|\tloss: 0.755317\n",
      "Training Epoch 25  86.2% | batch:       591 of       686\t|\tloss: 0.646045\n",
      "Training Epoch 25  86.3% | batch:       592 of       686\t|\tloss: 1.11828\n",
      "Training Epoch 25  86.4% | batch:       593 of       686\t|\tloss: 0.826488\n",
      "Training Epoch 25  86.6% | batch:       594 of       686\t|\tloss: 0.797852\n",
      "Training Epoch 25  86.7% | batch:       595 of       686\t|\tloss: 0.898745\n",
      "Training Epoch 25  86.9% | batch:       596 of       686\t|\tloss: 0.905028\n",
      "Training Epoch 25  87.0% | batch:       597 of       686\t|\tloss: 0.839174\n",
      "Training Epoch 25  87.2% | batch:       598 of       686\t|\tloss: 1.02083\n",
      "Training Epoch 25  87.3% | batch:       599 of       686\t|\tloss: 0.804761\n",
      "Training Epoch 25  87.5% | batch:       600 of       686\t|\tloss: 0.895324\n",
      "Training Epoch 25  87.6% | batch:       601 of       686\t|\tloss: 0.874411\n",
      "Training Epoch 25  87.8% | batch:       602 of       686\t|\tloss: 0.894667\n",
      "Training Epoch 25  87.9% | batch:       603 of       686\t|\tloss: 0.850334\n",
      "Training Epoch 25  88.0% | batch:       604 of       686\t|\tloss: 0.81155\n",
      "Training Epoch 25  88.2% | batch:       605 of       686\t|\tloss: 1.09831\n",
      "Training Epoch 25  88.3% | batch:       606 of       686\t|\tloss: 0.71065\n",
      "Training Epoch 25  88.5% | batch:       607 of       686\t|\tloss: 0.948806\n",
      "Training Epoch 25  88.6% | batch:       608 of       686\t|\tloss: 0.962458\n",
      "Training Epoch 25  88.8% | batch:       609 of       686\t|\tloss: 0.851844\n",
      "Training Epoch 25  88.9% | batch:       610 of       686\t|\tloss: 0.839584\n",
      "Training Epoch 25  89.1% | batch:       611 of       686\t|\tloss: 1.04085\n",
      "Training Epoch 25  89.2% | batch:       612 of       686\t|\tloss: 0.724886\n",
      "Training Epoch 25  89.4% | batch:       613 of       686\t|\tloss: 0.882877\n",
      "Training Epoch 25  89.5% | batch:       614 of       686\t|\tloss: 0.79735\n",
      "Training Epoch 25  89.7% | batch:       615 of       686\t|\tloss: 1.02823\n",
      "Training Epoch 25  89.8% | batch:       616 of       686\t|\tloss: 0.992044\n",
      "Training Epoch 25  89.9% | batch:       617 of       686\t|\tloss: 0.889449\n",
      "Training Epoch 25  90.1% | batch:       618 of       686\t|\tloss: 0.752323\n",
      "Training Epoch 25  90.2% | batch:       619 of       686\t|\tloss: 1.16886\n",
      "Training Epoch 25  90.4% | batch:       620 of       686\t|\tloss: 1.0255\n",
      "Training Epoch 25  90.5% | batch:       621 of       686\t|\tloss: 0.953384\n",
      "Training Epoch 25  90.7% | batch:       622 of       686\t|\tloss: 0.855638\n",
      "Training Epoch 25  90.8% | batch:       623 of       686\t|\tloss: 1.09412\n",
      "Training Epoch 25  91.0% | batch:       624 of       686\t|\tloss: 0.770429\n",
      "Training Epoch 25  91.1% | batch:       625 of       686\t|\tloss: 0.918739\n",
      "Training Epoch 25  91.3% | batch:       626 of       686\t|\tloss: 1.14841\n",
      "Training Epoch 25  91.4% | batch:       627 of       686\t|\tloss: 0.904127\n",
      "Training Epoch 25  91.5% | batch:       628 of       686\t|\tloss: 0.779776\n",
      "Training Epoch 25  91.7% | batch:       629 of       686\t|\tloss: 1.11862\n",
      "Training Epoch 25  91.8% | batch:       630 of       686\t|\tloss: 0.954692\n",
      "Training Epoch 25  92.0% | batch:       631 of       686\t|\tloss: 0.949006\n",
      "Training Epoch 25  92.1% | batch:       632 of       686\t|\tloss: 0.845632\n",
      "Training Epoch 25  92.3% | batch:       633 of       686\t|\tloss: 0.898905\n",
      "Training Epoch 25  92.4% | batch:       634 of       686\t|\tloss: 0.837486\n",
      "Training Epoch 25  92.6% | batch:       635 of       686\t|\tloss: 1.00533\n",
      "Training Epoch 25  92.7% | batch:       636 of       686\t|\tloss: 0.871636\n",
      "Training Epoch 25  92.9% | batch:       637 of       686\t|\tloss: 0.649761\n",
      "Training Epoch 25  93.0% | batch:       638 of       686\t|\tloss: 0.853805\n",
      "Training Epoch 25  93.1% | batch:       639 of       686\t|\tloss: 0.737107\n",
      "Training Epoch 25  93.3% | batch:       640 of       686\t|\tloss: 0.969803\n",
      "Training Epoch 25  93.4% | batch:       641 of       686\t|\tloss: 0.765727\n",
      "Training Epoch 25  93.6% | batch:       642 of       686\t|\tloss: 0.840917\n",
      "Training Epoch 25  93.7% | batch:       643 of       686\t|\tloss: 0.908059\n",
      "Training Epoch 25  93.9% | batch:       644 of       686\t|\tloss: 0.715294\n",
      "Training Epoch 25  94.0% | batch:       645 of       686\t|\tloss: 0.995921\n",
      "Training Epoch 25  94.2% | batch:       646 of       686\t|\tloss: 0.877113\n",
      "Training Epoch 25  94.3% | batch:       647 of       686\t|\tloss: 1.00061\n",
      "Training Epoch 25  94.5% | batch:       648 of       686\t|\tloss: 0.924081\n",
      "Training Epoch 25  94.6% | batch:       649 of       686\t|\tloss: 1.10421\n",
      "Training Epoch 25  94.8% | batch:       650 of       686\t|\tloss: 0.804783\n",
      "Training Epoch 25  94.9% | batch:       651 of       686\t|\tloss: 0.887337\n",
      "Training Epoch 25  95.0% | batch:       652 of       686\t|\tloss: 0.853497\n",
      "Training Epoch 25  95.2% | batch:       653 of       686\t|\tloss: 0.873311\n",
      "Training Epoch 25  95.3% | batch:       654 of       686\t|\tloss: 0.813237\n",
      "Training Epoch 25  95.5% | batch:       655 of       686\t|\tloss: 0.89431\n",
      "Training Epoch 25  95.6% | batch:       656 of       686\t|\tloss: 0.710609\n",
      "Training Epoch 25  95.8% | batch:       657 of       686\t|\tloss: 0.995446\n",
      "Training Epoch 25  95.9% | batch:       658 of       686\t|\tloss: 0.820537\n",
      "Training Epoch 25  96.1% | batch:       659 of       686\t|\tloss: 0.903746\n",
      "Training Epoch 25  96.2% | batch:       660 of       686\t|\tloss: 0.832677\n",
      "Training Epoch 25  96.4% | batch:       661 of       686\t|\tloss: 0.924405\n",
      "Training Epoch 25  96.5% | batch:       662 of       686\t|\tloss: 0.90354\n",
      "Training Epoch 25  96.6% | batch:       663 of       686\t|\tloss: 0.959807\n",
      "Training Epoch 25  96.8% | batch:       664 of       686\t|\tloss: 0.782164\n",
      "Training Epoch 25  96.9% | batch:       665 of       686\t|\tloss: 0.803894\n",
      "Training Epoch 25  97.1% | batch:       666 of       686\t|\tloss: 0.972486\n",
      "Training Epoch 25  97.2% | batch:       667 of       686\t|\tloss: 0.913781\n",
      "Training Epoch 25  97.4% | batch:       668 of       686\t|\tloss: 0.892164\n",
      "Training Epoch 25  97.5% | batch:       669 of       686\t|\tloss: 1.05417\n",
      "Training Epoch 25  97.7% | batch:       670 of       686\t|\tloss: 0.794466\n",
      "Training Epoch 25  97.8% | batch:       671 of       686\t|\tloss: 0.702671\n",
      "Training Epoch 25  98.0% | batch:       672 of       686\t|\tloss: 1.12894\n",
      "Training Epoch 25  98.1% | batch:       673 of       686\t|\tloss: 0.612873\n",
      "Training Epoch 25  98.3% | batch:       674 of       686\t|\tloss: 1.01693\n",
      "Training Epoch 25  98.4% | batch:       675 of       686\t|\tloss: 0.879485\n",
      "Training Epoch 25  98.5% | batch:       676 of       686\t|\tloss: 0.772531\n",
      "Training Epoch 25  98.7% | batch:       677 of       686\t|\tloss: 0.939037\n",
      "Training Epoch 25  98.8% | batch:       678 of       686\t|\tloss: 0.792518\n",
      "Training Epoch 25  99.0% | batch:       679 of       686\t|\tloss: 0.88847\n",
      "Training Epoch 25  99.1% | batch:       680 of       686\t|\tloss: 0.928173\n",
      "Training Epoch 25  99.3% | batch:       681 of       686\t|\tloss: 1.01589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:11:29,588 | INFO : Epoch 25 Training Summary: epoch: 25.000000 | loss: 0.953444 | \n",
      "2023-05-25 22:11:29,589 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 21.890342235565186 seconds\n",
      "\n",
      "2023-05-25 22:11:29,590 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.96456923484802 seconds\n",
      "2023-05-25 22:11:29,590 | INFO : Avg batch train. time: 0.03493377439482219 seconds\n",
      "2023-05-25 22:11:29,591 | INFO : Avg sample train. time: 0.00027327178556186806 seconds\n",
      "2023-05-25 22:11:29,591 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 25  99.4% | batch:       682 of       686\t|\tloss: 0.737803\n",
      "Training Epoch 25  99.6% | batch:       683 of       686\t|\tloss: 0.854182\n",
      "Training Epoch 25  99.7% | batch:       684 of       686\t|\tloss: 1.31233\n",
      "Training Epoch 25  99.9% | batch:       685 of       686\t|\tloss: 0.739755\n",
      "\n",
      "Evaluating Epoch 25   0.0% | batch:         0 of       172\t|\tloss: 1.33526\n",
      "Evaluating Epoch 25   0.6% | batch:         1 of       172\t|\tloss: 1.41234\n",
      "Evaluating Epoch 25   1.2% | batch:         2 of       172\t|\tloss: 0.641634\n",
      "Evaluating Epoch 25   1.7% | batch:         3 of       172\t|\tloss: 2.36141\n",
      "Evaluating Epoch 25   2.3% | batch:         4 of       172\t|\tloss: 1.13028\n",
      "Evaluating Epoch 25   2.9% | batch:         5 of       172\t|\tloss: 1.22747\n",
      "Evaluating Epoch 25   3.5% | batch:         6 of       172\t|\tloss: 1.13802\n",
      "Evaluating Epoch 25   4.1% | batch:         7 of       172\t|\tloss: 2.88823\n",
      "Evaluating Epoch 25   4.7% | batch:         8 of       172\t|\tloss: 0.583355\n",
      "Evaluating Epoch 25   5.2% | batch:         9 of       172\t|\tloss: 1.66856\n",
      "Evaluating Epoch 25   5.8% | batch:        10 of       172\t|\tloss: 1.22801\n",
      "Evaluating Epoch 25   6.4% | batch:        11 of       172\t|\tloss: 1.17707\n",
      "Evaluating Epoch 25   7.0% | batch:        12 of       172\t|\tloss: 1.36068\n",
      "Evaluating Epoch 25   7.6% | batch:        13 of       172\t|\tloss: 1.17656\n",
      "Evaluating Epoch 25   8.1% | batch:        14 of       172\t|\tloss: 1.57061\n",
      "Evaluating Epoch 25   8.7% | batch:        15 of       172\t|\tloss: 1.28382\n",
      "Evaluating Epoch 25   9.3% | batch:        16 of       172\t|\tloss: 2.13959\n",
      "Evaluating Epoch 25   9.9% | batch:        17 of       172\t|\tloss: 0.786505\n",
      "Evaluating Epoch 25  10.5% | batch:        18 of       172\t|\tloss: 22.381\n",
      "Evaluating Epoch 25  11.0% | batch:        19 of       172\t|\tloss: 1.45812\n",
      "Evaluating Epoch 25  11.6% | batch:        20 of       172\t|\tloss: 2.24655\n",
      "Evaluating Epoch 25  12.2% | batch:        21 of       172\t|\tloss: 0.838896\n",
      "Evaluating Epoch 25  12.8% | batch:        22 of       172\t|\tloss: 6.47296\n",
      "Evaluating Epoch 25  13.4% | batch:        23 of       172\t|\tloss: 3.51325\n",
      "Evaluating Epoch 25  14.0% | batch:        24 of       172\t|\tloss: 1.10323\n",
      "Evaluating Epoch 25  14.5% | batch:        25 of       172\t|\tloss: 2.17025\n",
      "Evaluating Epoch 25  15.1% | batch:        26 of       172\t|\tloss: 9.37381\n",
      "Evaluating Epoch 25  15.7% | batch:        27 of       172\t|\tloss: 19.2163\n",
      "Evaluating Epoch 25  16.3% | batch:        28 of       172\t|\tloss: 0.289798\n",
      "Evaluating Epoch 25  16.9% | batch:        29 of       172\t|\tloss: 1.79581\n",
      "Evaluating Epoch 25  17.4% | batch:        30 of       172\t|\tloss: 1.26142\n",
      "Evaluating Epoch 25  18.0% | batch:        31 of       172\t|\tloss: 0.879045\n",
      "Evaluating Epoch 25  18.6% | batch:        32 of       172\t|\tloss: 0.297268\n",
      "Evaluating Epoch 25  19.2% | batch:        33 of       172\t|\tloss: 0.432584\n",
      "Evaluating Epoch 25  19.8% | batch:        34 of       172\t|\tloss: 0.314013\n",
      "Evaluating Epoch 25  20.3% | batch:        35 of       172\t|\tloss: 0.790374\n",
      "Evaluating Epoch 25  20.9% | batch:        36 of       172\t|\tloss: 2.67202\n",
      "Evaluating Epoch 25  21.5% | batch:        37 of       172\t|\tloss: 3.95247\n",
      "Evaluating Epoch 25  22.1% | batch:        38 of       172\t|\tloss: 4.54689\n",
      "Evaluating Epoch 25  22.7% | batch:        39 of       172\t|\tloss: 10.1946\n",
      "Evaluating Epoch 25  23.3% | batch:        40 of       172\t|\tloss: 0.367858\n",
      "Evaluating Epoch 25  23.8% | batch:        41 of       172\t|\tloss: 0.983476\n",
      "Evaluating Epoch 25  24.4% | batch:        42 of       172\t|\tloss: 0.45044\n",
      "Evaluating Epoch 25  25.0% | batch:        43 of       172\t|\tloss: 24.0786\n",
      "Evaluating Epoch 25  25.6% | batch:        44 of       172\t|\tloss: 1.29141\n",
      "Evaluating Epoch 25  26.2% | batch:        45 of       172\t|\tloss: 1.08631\n",
      "Evaluating Epoch 25  26.7% | batch:        46 of       172\t|\tloss: 0.51738\n",
      "Evaluating Epoch 25  27.3% | batch:        47 of       172\t|\tloss: 1.46013\n",
      "Evaluating Epoch 25  27.9% | batch:        48 of       172\t|\tloss: 0.394568\n",
      "Evaluating Epoch 25  28.5% | batch:        49 of       172\t|\tloss: 0.852577\n",
      "Evaluating Epoch 25  29.1% | batch:        50 of       172\t|\tloss: 0.356335\n",
      "Evaluating Epoch 25  29.7% | batch:        51 of       172\t|\tloss: 0.903707\n",
      "Evaluating Epoch 25  30.2% | batch:        52 of       172\t|\tloss: 1.10525\n",
      "Evaluating Epoch 25  30.8% | batch:        53 of       172\t|\tloss: 2.8622\n",
      "Evaluating Epoch 25  31.4% | batch:        54 of       172\t|\tloss: 0.834037\n",
      "Evaluating Epoch 25  32.0% | batch:        55 of       172\t|\tloss: 0.536516\n",
      "Evaluating Epoch 25  32.6% | batch:        56 of       172\t|\tloss: 3.1887\n",
      "Evaluating Epoch 25  33.1% | batch:        57 of       172\t|\tloss: 0.388924\n",
      "Evaluating Epoch 25  33.7% | batch:        58 of       172\t|\tloss: 2.2235\n",
      "Evaluating Epoch 25  34.3% | batch:        59 of       172\t|\tloss: 1.35352\n",
      "Evaluating Epoch 25  34.9% | batch:        60 of       172\t|\tloss: 1.225\n",
      "Evaluating Epoch 25  35.5% | batch:        61 of       172\t|\tloss: 1.51387\n",
      "Evaluating Epoch 25  36.0% | batch:        62 of       172\t|\tloss: 0.853972\n",
      "Evaluating Epoch 25  36.6% | batch:        63 of       172\t|\tloss: 3.0166\n",
      "Evaluating Epoch 25  37.2% | batch:        64 of       172\t|\tloss: 0.995621\n",
      "Evaluating Epoch 25  37.8% | batch:        65 of       172\t|\tloss: 2.32936\n",
      "Evaluating Epoch 25  38.4% | batch:        66 of       172\t|\tloss: 1.13181\n",
      "Evaluating Epoch 25  39.0% | batch:        67 of       172\t|\tloss: 0.630389\n",
      "Evaluating Epoch 25  39.5% | batch:        68 of       172\t|\tloss: 2.34224\n",
      "Evaluating Epoch 25  40.1% | batch:        69 of       172\t|\tloss: 0.702647\n",
      "Evaluating Epoch 25  40.7% | batch:        70 of       172\t|\tloss: 1.76276\n",
      "Evaluating Epoch 25  41.3% | batch:        71 of       172\t|\tloss: 1.60763\n",
      "Evaluating Epoch 25  41.9% | batch:        72 of       172\t|\tloss: 0.860971\n",
      "Evaluating Epoch 25  42.4% | batch:        73 of       172\t|\tloss: 2.60838\n",
      "Evaluating Epoch 25  43.0% | batch:        74 of       172\t|\tloss: 0.473148\n",
      "Evaluating Epoch 25  43.6% | batch:        75 of       172\t|\tloss: 0.417174\n",
      "Evaluating Epoch 25  44.2% | batch:        76 of       172\t|\tloss: 0.504056\n",
      "Evaluating Epoch 25  44.8% | batch:        77 of       172\t|\tloss: 0.491576\n",
      "Evaluating Epoch 25  45.3% | batch:        78 of       172\t|\tloss: 0.477866\n",
      "Evaluating Epoch 25  45.9% | batch:        79 of       172\t|\tloss: 0.351776\n",
      "Evaluating Epoch 25  46.5% | batch:        80 of       172\t|\tloss: 0.390812\n",
      "Evaluating Epoch 25  47.1% | batch:        81 of       172\t|\tloss: 0.50559\n",
      "Evaluating Epoch 25  47.7% | batch:        82 of       172\t|\tloss: 0.437683\n",
      "Evaluating Epoch 25  48.3% | batch:        83 of       172\t|\tloss: 0.536722\n",
      "Evaluating Epoch 25  48.8% | batch:        84 of       172\t|\tloss: 0.646581\n",
      "Evaluating Epoch 25  49.4% | batch:        85 of       172\t|\tloss: 0.755957\n",
      "Evaluating Epoch 25  50.0% | batch:        86 of       172\t|\tloss: 0.519622\n",
      "Evaluating Epoch 25  50.6% | batch:        87 of       172\t|\tloss: 0.871768\n",
      "Evaluating Epoch 25  51.2% | batch:        88 of       172\t|\tloss: 0.549329\n",
      "Evaluating Epoch 25  51.7% | batch:        89 of       172\t|\tloss: 0.613981\n",
      "Evaluating Epoch 25  52.3% | batch:        90 of       172\t|\tloss: 0.665732\n",
      "Evaluating Epoch 25  52.9% | batch:        91 of       172\t|\tloss: 0.330819\n",
      "Evaluating Epoch 25  53.5% | batch:        92 of       172\t|\tloss: 0.628252\n",
      "Evaluating Epoch 25  54.1% | batch:        93 of       172\t|\tloss: 0.694037\n",
      "Evaluating Epoch 25  54.7% | batch:        94 of       172\t|\tloss: 0.554961\n",
      "Evaluating Epoch 25  55.2% | batch:        95 of       172\t|\tloss: 0.575762\n",
      "Evaluating Epoch 25  55.8% | batch:        96 of       172\t|\tloss: 0.524973\n",
      "Evaluating Epoch 25  56.4% | batch:        97 of       172\t|\tloss: 0.747571\n",
      "Evaluating Epoch 25  57.0% | batch:        98 of       172\t|\tloss: 0.480329\n",
      "Evaluating Epoch 25  57.6% | batch:        99 of       172\t|\tloss: 0.632551\n",
      "Evaluating Epoch 25  58.1% | batch:       100 of       172\t|\tloss: 0.610239\n",
      "Evaluating Epoch 25  58.7% | batch:       101 of       172\t|\tloss: 0.475095\n",
      "Evaluating Epoch 25  59.3% | batch:       102 of       172\t|\tloss: 0.654188\n",
      "Evaluating Epoch 25  59.9% | batch:       103 of       172\t|\tloss: 0.670435\n",
      "Evaluating Epoch 25  60.5% | batch:       104 of       172\t|\tloss: 0.730242\n",
      "Evaluating Epoch 25  61.0% | batch:       105 of       172\t|\tloss: 0.240997\n",
      "Evaluating Epoch 25  61.6% | batch:       106 of       172\t|\tloss: 0.640291\n",
      "Evaluating Epoch 25  62.2% | batch:       107 of       172\t|\tloss: 0.918901\n",
      "Evaluating Epoch 25  62.8% | batch:       108 of       172\t|\tloss: 0.441189\n",
      "Evaluating Epoch 25  63.4% | batch:       109 of       172\t|\tloss: 0.639884\n",
      "Evaluating Epoch 25  64.0% | batch:       110 of       172\t|\tloss: 0.59615\n",
      "Evaluating Epoch 25  64.5% | batch:       111 of       172\t|\tloss: 0.74539\n",
      "Evaluating Epoch 25  65.1% | batch:       112 of       172\t|\tloss: 0.686024\n",
      "Evaluating Epoch 25  65.7% | batch:       113 of       172\t|\tloss: 0.863232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 25  66.3% | batch:       114 of       172\t|\tloss: 0.443058\n",
      "Evaluating Epoch 25  66.9% | batch:       115 of       172\t|\tloss: 0.404111\n",
      "Evaluating Epoch 25  67.4% | batch:       116 of       172\t|\tloss: 0.356018\n",
      "Evaluating Epoch 25  68.0% | batch:       117 of       172\t|\tloss: 0.232521\n",
      "Evaluating Epoch 25  68.6% | batch:       118 of       172\t|\tloss: 0.45575\n",
      "Evaluating Epoch 25  69.2% | batch:       119 of       172\t|\tloss: 0.196444\n",
      "Evaluating Epoch 25  69.8% | batch:       120 of       172\t|\tloss: 0.461296\n",
      "Evaluating Epoch 25  70.3% | batch:       121 of       172\t|\tloss: 0.530183\n",
      "Evaluating Epoch 25  70.9% | batch:       122 of       172\t|\tloss: 0.44126\n",
      "Evaluating Epoch 25  71.5% | batch:       123 of       172\t|\tloss: 0.574225\n",
      "Evaluating Epoch 25  72.1% | batch:       124 of       172\t|\tloss: 0.396333\n",
      "Evaluating Epoch 25  72.7% | batch:       125 of       172\t|\tloss: 0.30584\n",
      "Evaluating Epoch 25  73.3% | batch:       126 of       172\t|\tloss: 0.233111\n",
      "Evaluating Epoch 25  73.8% | batch:       127 of       172\t|\tloss: 0.536191\n",
      "Evaluating Epoch 25  74.4% | batch:       128 of       172\t|\tloss: 0.616253\n",
      "Evaluating Epoch 25  75.0% | batch:       129 of       172\t|\tloss: 0.378282\n",
      "Evaluating Epoch 25  75.6% | batch:       130 of       172\t|\tloss: 0.613619\n",
      "Evaluating Epoch 25  76.2% | batch:       131 of       172\t|\tloss: 0.476984\n",
      "Evaluating Epoch 25  76.7% | batch:       132 of       172\t|\tloss: 0.357702\n",
      "Evaluating Epoch 25  77.3% | batch:       133 of       172\t|\tloss: 0.64725\n",
      "Evaluating Epoch 25  77.9% | batch:       134 of       172\t|\tloss: 0.467277\n",
      "Evaluating Epoch 25  78.5% | batch:       135 of       172\t|\tloss: 0.45754\n",
      "Evaluating Epoch 25  79.1% | batch:       136 of       172\t|\tloss: 0.442749\n",
      "Evaluating Epoch 25  79.7% | batch:       137 of       172\t|\tloss: 0.394379\n",
      "Evaluating Epoch 25  80.2% | batch:       138 of       172\t|\tloss: 0.400712\n",
      "Evaluating Epoch 25  80.8% | batch:       139 of       172\t|\tloss: 0.627348\n",
      "Evaluating Epoch 25  81.4% | batch:       140 of       172\t|\tloss: 0.388503\n",
      "Evaluating Epoch 25  82.0% | batch:       141 of       172\t|\tloss: 0.272812\n",
      "Evaluating Epoch 25  82.6% | batch:       142 of       172\t|\tloss: 0.301893\n",
      "Evaluating Epoch 25  83.1% | batch:       143 of       172\t|\tloss: 0.436426\n",
      "Evaluating Epoch 25  83.7% | batch:       144 of       172\t|\tloss: 0.405208\n",
      "Evaluating Epoch 25  84.3% | batch:       145 of       172\t|\tloss: 0.557734\n",
      "Evaluating Epoch 25  84.9% | batch:       146 of       172\t|\tloss: 0.443489\n",
      "Evaluating Epoch 25  85.5% | batch:       147 of       172\t|\tloss: 0.506693\n",
      "Evaluating Epoch 25  86.0% | batch:       148 of       172\t|\tloss: 0.364384\n",
      "Evaluating Epoch 25  86.6% | batch:       149 of       172\t|\tloss: 0.478626\n",
      "Evaluating Epoch 25  87.2% | batch:       150 of       172\t|\tloss: 0.173405\n",
      "Evaluating Epoch 25  87.8% | batch:       151 of       172\t|\tloss: 0.18997\n",
      "Evaluating Epoch 25  88.4% | batch:       152 of       172\t|\tloss: 0.316552\n",
      "Evaluating Epoch 25  89.0% | batch:       153 of       172\t|\tloss: 0.1499\n",
      "Evaluating Epoch 25  89.5% | batch:       154 of       172\t|\tloss: 0.176047\n",
      "Evaluating Epoch 25  90.1% | batch:       155 of       172\t|\tloss: 0.339011\n",
      "Evaluating Epoch 25  90.7% | batch:       156 of       172\t|\tloss: 0.193941\n",
      "Evaluating Epoch 25  91.3% | batch:       157 of       172\t|\tloss: 0.316518\n",
      "Evaluating Epoch 25  91.9% | batch:       158 of       172\t|\tloss: 0.243469\n",
      "Evaluating Epoch 25  92.4% | batch:       159 of       172\t|\tloss: 0.209308\n",
      "Evaluating Epoch 25  93.0% | batch:       160 of       172\t|\tloss: 0.812907\n",
      "Evaluating Epoch 25  93.6% | batch:       161 of       172\t|\tloss: 0.248668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:11:32,908 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.3161509037017822 seconds\n",
      "\n",
      "2023-05-25 22:11:32,908 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.029585673258855 seconds\n",
      "2023-05-25 22:11:32,909 | INFO : Avg batch val. time: 0.023427823681737528 seconds\n",
      "2023-05-25 22:11:32,909 | INFO : Avg sample val. time: 0.00018352168662653617 seconds\n",
      "2023-05-25 22:11:32,910 | INFO : Epoch 25 Validation Summary: epoch: 25.000000 | loss: 1.354499 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 25  94.2% | batch:       162 of       172\t|\tloss: 0.230917\n",
      "Evaluating Epoch 25  94.8% | batch:       163 of       172\t|\tloss: 0.304535\n",
      "Evaluating Epoch 25  95.3% | batch:       164 of       172\t|\tloss: 0.190199\n",
      "Evaluating Epoch 25  95.9% | batch:       165 of       172\t|\tloss: 0.250048\n",
      "Evaluating Epoch 25  96.5% | batch:       166 of       172\t|\tloss: 0.176983\n",
      "Evaluating Epoch 25  97.1% | batch:       167 of       172\t|\tloss: 0.203634\n",
      "Evaluating Epoch 25  97.7% | batch:       168 of       172\t|\tloss: 0.240753\n",
      "Evaluating Epoch 25  98.3% | batch:       169 of       172\t|\tloss: 0.132018\n",
      "Evaluating Epoch 25  98.8% | batch:       170 of       172\t|\tloss: 0.144891\n",
      "Evaluating Epoch 25  99.4% | batch:       171 of       172\t|\tloss: 0.159381\n",
      "\n",
      "Training Epoch 26   0.0% | batch:         0 of       686\t|\tloss: 1.06634\n",
      "Training Epoch 26   0.1% | batch:         1 of       686\t|\tloss: 0.750585\n",
      "Training Epoch 26   0.3% | batch:         2 of       686\t|\tloss: 0.820183\n",
      "Training Epoch 26   0.4% | batch:         3 of       686\t|\tloss: 0.702661\n",
      "Training Epoch 26   0.6% | batch:         4 of       686\t|\tloss: 0.845985\n",
      "Training Epoch 26   0.7% | batch:         5 of       686\t|\tloss: 0.954472\n",
      "Training Epoch 26   0.9% | batch:         6 of       686\t|\tloss: 1.13084\n",
      "Training Epoch 26   1.0% | batch:         7 of       686\t|\tloss: 0.8146\n",
      "Training Epoch 26   1.2% | batch:         8 of       686\t|\tloss: 0.788972\n",
      "Training Epoch 26   1.3% | batch:         9 of       686\t|\tloss: 1.28592\n",
      "Training Epoch 26   1.5% | batch:        10 of       686\t|\tloss: 0.877341\n",
      "Training Epoch 26   1.6% | batch:        11 of       686\t|\tloss: 1.21618\n",
      "Training Epoch 26   1.7% | batch:        12 of       686\t|\tloss: 1.0913\n",
      "Training Epoch 26   1.9% | batch:        13 of       686\t|\tloss: 0.834716\n",
      "Training Epoch 26   2.0% | batch:        14 of       686\t|\tloss: 0.785324\n",
      "Training Epoch 26   2.2% | batch:        15 of       686\t|\tloss: 0.85932\n",
      "Training Epoch 26   2.3% | batch:        16 of       686\t|\tloss: 1.36908\n",
      "Training Epoch 26   2.5% | batch:        17 of       686\t|\tloss: 1.07751\n",
      "Training Epoch 26   2.6% | batch:        18 of       686\t|\tloss: 1.09143\n",
      "Training Epoch 26   2.8% | batch:        19 of       686\t|\tloss: 0.86083\n",
      "Training Epoch 26   2.9% | batch:        20 of       686\t|\tloss: 1.00648\n",
      "Training Epoch 26   3.1% | batch:        21 of       686\t|\tloss: 0.806849\n",
      "Training Epoch 26   3.2% | batch:        22 of       686\t|\tloss: 0.775815\n",
      "Training Epoch 26   3.4% | batch:        23 of       686\t|\tloss: 0.801334\n",
      "Training Epoch 26   3.5% | batch:        24 of       686\t|\tloss: 0.767847\n",
      "Training Epoch 26   3.6% | batch:        25 of       686\t|\tloss: 0.902163\n",
      "Training Epoch 26   3.8% | batch:        26 of       686\t|\tloss: 0.822163\n",
      "Training Epoch 26   3.9% | batch:        27 of       686\t|\tloss: 0.927849\n",
      "Training Epoch 26   4.1% | batch:        28 of       686\t|\tloss: 0.823777\n",
      "Training Epoch 26   4.2% | batch:        29 of       686\t|\tloss: 0.918163\n",
      "Training Epoch 26   4.4% | batch:        30 of       686\t|\tloss: 0.956917\n",
      "Training Epoch 26   4.5% | batch:        31 of       686\t|\tloss: 0.987818\n",
      "Training Epoch 26   4.7% | batch:        32 of       686\t|\tloss: 0.860867\n",
      "Training Epoch 26   4.8% | batch:        33 of       686\t|\tloss: 0.928778\n",
      "Training Epoch 26   5.0% | batch:        34 of       686\t|\tloss: 1.04148\n",
      "Training Epoch 26   5.1% | batch:        35 of       686\t|\tloss: 0.900318\n",
      "Training Epoch 26   5.2% | batch:        36 of       686\t|\tloss: 0.783899\n",
      "Training Epoch 26   5.4% | batch:        37 of       686\t|\tloss: 1.15331\n",
      "Training Epoch 26   5.5% | batch:        38 of       686\t|\tloss: 1.40627\n",
      "Training Epoch 26   5.7% | batch:        39 of       686\t|\tloss: 1.05453\n",
      "Training Epoch 26   5.8% | batch:        40 of       686\t|\tloss: 0.989324\n",
      "Training Epoch 26   6.0% | batch:        41 of       686\t|\tloss: 0.915812\n",
      "Training Epoch 26   6.1% | batch:        42 of       686\t|\tloss: 0.856918\n",
      "Training Epoch 26   6.3% | batch:        43 of       686\t|\tloss: 0.896334\n",
      "Training Epoch 26   6.4% | batch:        44 of       686\t|\tloss: 0.779653\n",
      "Training Epoch 26   6.6% | batch:        45 of       686\t|\tloss: 0.823532\n",
      "Training Epoch 26   6.7% | batch:        46 of       686\t|\tloss: 1.13466\n",
      "Training Epoch 26   6.9% | batch:        47 of       686\t|\tloss: 0.987442\n",
      "Training Epoch 26   7.0% | batch:        48 of       686\t|\tloss: 0.919401\n",
      "Training Epoch 26   7.1% | batch:        49 of       686\t|\tloss: 0.859909\n",
      "Training Epoch 26   7.3% | batch:        50 of       686\t|\tloss: 1.10945\n",
      "Training Epoch 26   7.4% | batch:        51 of       686\t|\tloss: 0.730034\n",
      "Training Epoch 26   7.6% | batch:        52 of       686\t|\tloss: 0.965426\n",
      "Training Epoch 26   7.7% | batch:        53 of       686\t|\tloss: 0.861353\n",
      "Training Epoch 26   7.9% | batch:        54 of       686\t|\tloss: 0.877722\n",
      "Training Epoch 26   8.0% | batch:        55 of       686\t|\tloss: 0.927139\n",
      "Training Epoch 26   8.2% | batch:        56 of       686\t|\tloss: 0.769729\n",
      "Training Epoch 26   8.3% | batch:        57 of       686\t|\tloss: 0.904038\n",
      "Training Epoch 26   8.5% | batch:        58 of       686\t|\tloss: 1.02838\n",
      "Training Epoch 26   8.6% | batch:        59 of       686\t|\tloss: 0.853229\n",
      "Training Epoch 26   8.7% | batch:        60 of       686\t|\tloss: 0.77501\n",
      "Training Epoch 26   8.9% | batch:        61 of       686\t|\tloss: 0.918059\n",
      "Training Epoch 26   9.0% | batch:        62 of       686\t|\tloss: 1.16974\n",
      "Training Epoch 26   9.2% | batch:        63 of       686\t|\tloss: 0.767084\n",
      "Training Epoch 26   9.3% | batch:        64 of       686\t|\tloss: 0.794833\n",
      "Training Epoch 26   9.5% | batch:        65 of       686\t|\tloss: 0.792828\n",
      "Training Epoch 26   9.6% | batch:        66 of       686\t|\tloss: 0.820602\n",
      "Training Epoch 26   9.8% | batch:        67 of       686\t|\tloss: 0.911411\n",
      "Training Epoch 26   9.9% | batch:        68 of       686\t|\tloss: 1.01592\n",
      "Training Epoch 26  10.1% | batch:        69 of       686\t|\tloss: 0.891632\n",
      "Training Epoch 26  10.2% | batch:        70 of       686\t|\tloss: 0.839356\n",
      "Training Epoch 26  10.3% | batch:        71 of       686\t|\tloss: 0.977822\n",
      "Training Epoch 26  10.5% | batch:        72 of       686\t|\tloss: 1.01495\n",
      "Training Epoch 26  10.6% | batch:        73 of       686\t|\tloss: 0.835744\n",
      "Training Epoch 26  10.8% | batch:        74 of       686\t|\tloss: 1.03978\n",
      "Training Epoch 26  10.9% | batch:        75 of       686\t|\tloss: 0.969186\n",
      "Training Epoch 26  11.1% | batch:        76 of       686\t|\tloss: 0.807593\n",
      "Training Epoch 26  11.2% | batch:        77 of       686\t|\tloss: 1.04182\n",
      "Training Epoch 26  11.4% | batch:        78 of       686\t|\tloss: 0.76611\n",
      "Training Epoch 26  11.5% | batch:        79 of       686\t|\tloss: 0.881269\n",
      "Training Epoch 26  11.7% | batch:        80 of       686\t|\tloss: 1.16903\n",
      "Training Epoch 26  11.8% | batch:        81 of       686\t|\tloss: 0.940663\n",
      "Training Epoch 26  12.0% | batch:        82 of       686\t|\tloss: 1.03632\n",
      "Training Epoch 26  12.1% | batch:        83 of       686\t|\tloss: 1.39436\n",
      "Training Epoch 26  12.2% | batch:        84 of       686\t|\tloss: 1.07063\n",
      "Training Epoch 26  12.4% | batch:        85 of       686\t|\tloss: 1.02004\n",
      "Training Epoch 26  12.5% | batch:        86 of       686\t|\tloss: 0.886648\n",
      "Training Epoch 26  12.7% | batch:        87 of       686\t|\tloss: 0.99653\n",
      "Training Epoch 26  12.8% | batch:        88 of       686\t|\tloss: 1.0999\n",
      "Training Epoch 26  13.0% | batch:        89 of       686\t|\tloss: 0.851439\n",
      "Training Epoch 26  13.1% | batch:        90 of       686\t|\tloss: 0.84621\n",
      "Training Epoch 26  13.3% | batch:        91 of       686\t|\tloss: 0.945519\n",
      "Training Epoch 26  13.4% | batch:        92 of       686\t|\tloss: 0.96354\n",
      "Training Epoch 26  13.6% | batch:        93 of       686\t|\tloss: 0.800575\n",
      "Training Epoch 26  13.7% | batch:        94 of       686\t|\tloss: 1.017\n",
      "Training Epoch 26  13.8% | batch:        95 of       686\t|\tloss: 0.960782\n",
      "Training Epoch 26  14.0% | batch:        96 of       686\t|\tloss: 0.825461\n",
      "Training Epoch 26  14.1% | batch:        97 of       686\t|\tloss: 0.771564\n",
      "Training Epoch 26  14.3% | batch:        98 of       686\t|\tloss: 0.733986\n",
      "Training Epoch 26  14.4% | batch:        99 of       686\t|\tloss: 1.0806\n",
      "Training Epoch 26  14.6% | batch:       100 of       686\t|\tloss: 0.979748\n",
      "Training Epoch 26  14.7% | batch:       101 of       686\t|\tloss: 0.928881\n",
      "Training Epoch 26  14.9% | batch:       102 of       686\t|\tloss: 0.780741\n",
      "Training Epoch 26  15.0% | batch:       103 of       686\t|\tloss: 1.04933\n",
      "Training Epoch 26  15.2% | batch:       104 of       686\t|\tloss: 0.654086\n",
      "Training Epoch 26  15.3% | batch:       105 of       686\t|\tloss: 0.771842\n",
      "Training Epoch 26  15.5% | batch:       106 of       686\t|\tloss: 0.720298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 26  15.6% | batch:       107 of       686\t|\tloss: 0.68109\n",
      "Training Epoch 26  15.7% | batch:       108 of       686\t|\tloss: 0.891433\n",
      "Training Epoch 26  15.9% | batch:       109 of       686\t|\tloss: 0.803199\n",
      "Training Epoch 26  16.0% | batch:       110 of       686\t|\tloss: 0.925311\n",
      "Training Epoch 26  16.2% | batch:       111 of       686\t|\tloss: 0.922739\n",
      "Training Epoch 26  16.3% | batch:       112 of       686\t|\tloss: 0.858994\n",
      "Training Epoch 26  16.5% | batch:       113 of       686\t|\tloss: 1.04044\n",
      "Training Epoch 26  16.6% | batch:       114 of       686\t|\tloss: 0.723357\n",
      "Training Epoch 26  16.8% | batch:       115 of       686\t|\tloss: 0.740765\n",
      "Training Epoch 26  16.9% | batch:       116 of       686\t|\tloss: 0.883264\n",
      "Training Epoch 26  17.1% | batch:       117 of       686\t|\tloss: 0.936855\n",
      "Training Epoch 26  17.2% | batch:       118 of       686\t|\tloss: 1.00952\n",
      "Training Epoch 26  17.3% | batch:       119 of       686\t|\tloss: 1.00961\n",
      "Training Epoch 26  17.5% | batch:       120 of       686\t|\tloss: 0.800882\n",
      "Training Epoch 26  17.6% | batch:       121 of       686\t|\tloss: 0.868179\n",
      "Training Epoch 26  17.8% | batch:       122 of       686\t|\tloss: 0.775339\n",
      "Training Epoch 26  17.9% | batch:       123 of       686\t|\tloss: 0.860725\n",
      "Training Epoch 26  18.1% | batch:       124 of       686\t|\tloss: 0.794053\n",
      "Training Epoch 26  18.2% | batch:       125 of       686\t|\tloss: 0.700923\n",
      "Training Epoch 26  18.4% | batch:       126 of       686\t|\tloss: 0.66792\n",
      "Training Epoch 26  18.5% | batch:       127 of       686\t|\tloss: 0.699315\n",
      "Training Epoch 26  18.7% | batch:       128 of       686\t|\tloss: 1.03431\n",
      "Training Epoch 26  18.8% | batch:       129 of       686\t|\tloss: 0.842127\n",
      "Training Epoch 26  19.0% | batch:       130 of       686\t|\tloss: 0.940782\n",
      "Training Epoch 26  19.1% | batch:       131 of       686\t|\tloss: 1.04654\n",
      "Training Epoch 26  19.2% | batch:       132 of       686\t|\tloss: 1.0174\n",
      "Training Epoch 26  19.4% | batch:       133 of       686\t|\tloss: 0.994491\n",
      "Training Epoch 26  19.5% | batch:       134 of       686\t|\tloss: 1.33431\n",
      "Training Epoch 26  19.7% | batch:       135 of       686\t|\tloss: 0.921788\n",
      "Training Epoch 26  19.8% | batch:       136 of       686\t|\tloss: 0.896342\n",
      "Training Epoch 26  20.0% | batch:       137 of       686\t|\tloss: 0.926053\n",
      "Training Epoch 26  20.1% | batch:       138 of       686\t|\tloss: 1.01052\n",
      "Training Epoch 26  20.3% | batch:       139 of       686\t|\tloss: 0.991114\n",
      "Training Epoch 26  20.4% | batch:       140 of       686\t|\tloss: 0.993984\n",
      "Training Epoch 26  20.6% | batch:       141 of       686\t|\tloss: 1.29145\n",
      "Training Epoch 26  20.7% | batch:       142 of       686\t|\tloss: 0.8481\n",
      "Training Epoch 26  20.8% | batch:       143 of       686\t|\tloss: 1.01348\n",
      "Training Epoch 26  21.0% | batch:       144 of       686\t|\tloss: 0.791095\n",
      "Training Epoch 26  21.1% | batch:       145 of       686\t|\tloss: 1.0314\n",
      "Training Epoch 26  21.3% | batch:       146 of       686\t|\tloss: 1.00166\n",
      "Training Epoch 26  21.4% | batch:       147 of       686\t|\tloss: 0.885437\n",
      "Training Epoch 26  21.6% | batch:       148 of       686\t|\tloss: 0.84052\n",
      "Training Epoch 26  21.7% | batch:       149 of       686\t|\tloss: 0.949083\n",
      "Training Epoch 26  21.9% | batch:       150 of       686\t|\tloss: 0.799436\n",
      "Training Epoch 26  22.0% | batch:       151 of       686\t|\tloss: 0.690174\n",
      "Training Epoch 26  22.2% | batch:       152 of       686\t|\tloss: 1.00854\n",
      "Training Epoch 26  22.3% | batch:       153 of       686\t|\tloss: 0.789267\n",
      "Training Epoch 26  22.4% | batch:       154 of       686\t|\tloss: 0.967546\n",
      "Training Epoch 26  22.6% | batch:       155 of       686\t|\tloss: 0.813662\n",
      "Training Epoch 26  22.7% | batch:       156 of       686\t|\tloss: 0.895012\n",
      "Training Epoch 26  22.9% | batch:       157 of       686\t|\tloss: 1.14604\n",
      "Training Epoch 26  23.0% | batch:       158 of       686\t|\tloss: 0.723862\n",
      "Training Epoch 26  23.2% | batch:       159 of       686\t|\tloss: 0.766461\n",
      "Training Epoch 26  23.3% | batch:       160 of       686\t|\tloss: 0.923495\n",
      "Training Epoch 26  23.5% | batch:       161 of       686\t|\tloss: 0.682298\n",
      "Training Epoch 26  23.6% | batch:       162 of       686\t|\tloss: 0.788614\n",
      "Training Epoch 26  23.8% | batch:       163 of       686\t|\tloss: 0.795179\n",
      "Training Epoch 26  23.9% | batch:       164 of       686\t|\tloss: 0.92726\n",
      "Training Epoch 26  24.1% | batch:       165 of       686\t|\tloss: 0.824339\n",
      "Training Epoch 26  24.2% | batch:       166 of       686\t|\tloss: 0.793894\n",
      "Training Epoch 26  24.3% | batch:       167 of       686\t|\tloss: 0.834442\n",
      "Training Epoch 26  24.5% | batch:       168 of       686\t|\tloss: 0.886542\n",
      "Training Epoch 26  24.6% | batch:       169 of       686\t|\tloss: 1.19861\n",
      "Training Epoch 26  24.8% | batch:       170 of       686\t|\tloss: 1.02408\n",
      "Training Epoch 26  24.9% | batch:       171 of       686\t|\tloss: 0.921811\n",
      "Training Epoch 26  25.1% | batch:       172 of       686\t|\tloss: 0.780732\n",
      "Training Epoch 26  25.2% | batch:       173 of       686\t|\tloss: 0.917724\n",
      "Training Epoch 26  25.4% | batch:       174 of       686\t|\tloss: 0.686948\n",
      "Training Epoch 26  25.5% | batch:       175 of       686\t|\tloss: 0.745237\n",
      "Training Epoch 26  25.7% | batch:       176 of       686\t|\tloss: 0.865841\n",
      "Training Epoch 26  25.8% | batch:       177 of       686\t|\tloss: 0.757455\n",
      "Training Epoch 26  25.9% | batch:       178 of       686\t|\tloss: 1.00119\n",
      "Training Epoch 26  26.1% | batch:       179 of       686\t|\tloss: 1.02483\n",
      "Training Epoch 26  26.2% | batch:       180 of       686\t|\tloss: 1.02952\n",
      "Training Epoch 26  26.4% | batch:       181 of       686\t|\tloss: 0.94308\n",
      "Training Epoch 26  26.5% | batch:       182 of       686\t|\tloss: 0.99779\n",
      "Training Epoch 26  26.7% | batch:       183 of       686\t|\tloss: 1.10341\n",
      "Training Epoch 26  26.8% | batch:       184 of       686\t|\tloss: 0.883117\n",
      "Training Epoch 26  27.0% | batch:       185 of       686\t|\tloss: 0.948565\n",
      "Training Epoch 26  27.1% | batch:       186 of       686\t|\tloss: 0.782333\n",
      "Training Epoch 26  27.3% | batch:       187 of       686\t|\tloss: 1.06042\n",
      "Training Epoch 26  27.4% | batch:       188 of       686\t|\tloss: 0.933844\n",
      "Training Epoch 26  27.6% | batch:       189 of       686\t|\tloss: 0.742783\n",
      "Training Epoch 26  27.7% | batch:       190 of       686\t|\tloss: 0.840736\n",
      "Training Epoch 26  27.8% | batch:       191 of       686\t|\tloss: 0.737398\n",
      "Training Epoch 26  28.0% | batch:       192 of       686\t|\tloss: 1.01368\n",
      "Training Epoch 26  28.1% | batch:       193 of       686\t|\tloss: 1.02501\n",
      "Training Epoch 26  28.3% | batch:       194 of       686\t|\tloss: 0.838436\n",
      "Training Epoch 26  28.4% | batch:       195 of       686\t|\tloss: 0.941759\n",
      "Training Epoch 26  28.6% | batch:       196 of       686\t|\tloss: 0.645089\n",
      "Training Epoch 26  28.7% | batch:       197 of       686\t|\tloss: 0.902123\n",
      "Training Epoch 26  28.9% | batch:       198 of       686\t|\tloss: 0.956498\n",
      "Training Epoch 26  29.0% | batch:       199 of       686\t|\tloss: 0.829376\n",
      "Training Epoch 26  29.2% | batch:       200 of       686\t|\tloss: 0.754487\n",
      "Training Epoch 26  29.3% | batch:       201 of       686\t|\tloss: 0.7601\n",
      "Training Epoch 26  29.4% | batch:       202 of       686\t|\tloss: 0.847741\n",
      "Training Epoch 26  29.6% | batch:       203 of       686\t|\tloss: 1.07319\n",
      "Training Epoch 26  29.7% | batch:       204 of       686\t|\tloss: 1.18032\n",
      "Training Epoch 26  29.9% | batch:       205 of       686\t|\tloss: 0.821669\n",
      "Training Epoch 26  30.0% | batch:       206 of       686\t|\tloss: 0.840801\n",
      "Training Epoch 26  30.2% | batch:       207 of       686\t|\tloss: 1.04728\n",
      "Training Epoch 26  30.3% | batch:       208 of       686\t|\tloss: 1.06951\n",
      "Training Epoch 26  30.5% | batch:       209 of       686\t|\tloss: 0.840574\n",
      "Training Epoch 26  30.6% | batch:       210 of       686\t|\tloss: 0.879872\n",
      "Training Epoch 26  30.8% | batch:       211 of       686\t|\tloss: 0.824609\n",
      "Training Epoch 26  30.9% | batch:       212 of       686\t|\tloss: 0.792439\n",
      "Training Epoch 26  31.0% | batch:       213 of       686\t|\tloss: 0.930563\n",
      "Training Epoch 26  31.2% | batch:       214 of       686\t|\tloss: 0.832294\n",
      "Training Epoch 26  31.3% | batch:       215 of       686\t|\tloss: 0.997337\n",
      "Training Epoch 26  31.5% | batch:       216 of       686\t|\tloss: 0.797229\n",
      "Training Epoch 26  31.6% | batch:       217 of       686\t|\tloss: 0.875086\n",
      "Training Epoch 26  31.8% | batch:       218 of       686\t|\tloss: 0.818291\n",
      "Training Epoch 26  31.9% | batch:       219 of       686\t|\tloss: 0.688975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 26  32.1% | batch:       220 of       686\t|\tloss: 0.85255\n",
      "Training Epoch 26  32.2% | batch:       221 of       686\t|\tloss: 0.842549\n",
      "Training Epoch 26  32.4% | batch:       222 of       686\t|\tloss: 0.859789\n",
      "Training Epoch 26  32.5% | batch:       223 of       686\t|\tloss: 0.846578\n",
      "Training Epoch 26  32.7% | batch:       224 of       686\t|\tloss: 1.00937\n",
      "Training Epoch 26  32.8% | batch:       225 of       686\t|\tloss: 0.776397\n",
      "Training Epoch 26  32.9% | batch:       226 of       686\t|\tloss: 0.774911\n",
      "Training Epoch 26  33.1% | batch:       227 of       686\t|\tloss: 0.842715\n",
      "Training Epoch 26  33.2% | batch:       228 of       686\t|\tloss: 0.699386\n",
      "Training Epoch 26  33.4% | batch:       229 of       686\t|\tloss: 0.821051\n",
      "Training Epoch 26  33.5% | batch:       230 of       686\t|\tloss: 0.828825\n",
      "Training Epoch 26  33.7% | batch:       231 of       686\t|\tloss: 0.84512\n",
      "Training Epoch 26  33.8% | batch:       232 of       686\t|\tloss: 0.930205\n",
      "Training Epoch 26  34.0% | batch:       233 of       686\t|\tloss: 0.76818\n",
      "Training Epoch 26  34.1% | batch:       234 of       686\t|\tloss: 0.839171\n",
      "Training Epoch 26  34.3% | batch:       235 of       686\t|\tloss: 0.940161\n",
      "Training Epoch 26  34.4% | batch:       236 of       686\t|\tloss: 0.94704\n",
      "Training Epoch 26  34.5% | batch:       237 of       686\t|\tloss: 1.10944\n",
      "Training Epoch 26  34.7% | batch:       238 of       686\t|\tloss: 0.73689\n",
      "Training Epoch 26  34.8% | batch:       239 of       686\t|\tloss: 0.878017\n",
      "Training Epoch 26  35.0% | batch:       240 of       686\t|\tloss: 0.751812\n",
      "Training Epoch 26  35.1% | batch:       241 of       686\t|\tloss: 0.859779\n",
      "Training Epoch 26  35.3% | batch:       242 of       686\t|\tloss: 0.991833\n",
      "Training Epoch 26  35.4% | batch:       243 of       686\t|\tloss: 0.920125\n",
      "Training Epoch 26  35.6% | batch:       244 of       686\t|\tloss: 0.577373\n",
      "Training Epoch 26  35.7% | batch:       245 of       686\t|\tloss: 0.980619\n",
      "Training Epoch 26  35.9% | batch:       246 of       686\t|\tloss: 0.811439\n",
      "Training Epoch 26  36.0% | batch:       247 of       686\t|\tloss: 0.852036\n",
      "Training Epoch 26  36.2% | batch:       248 of       686\t|\tloss: 1.08198\n",
      "Training Epoch 26  36.3% | batch:       249 of       686\t|\tloss: 0.815766\n",
      "Training Epoch 26  36.4% | batch:       250 of       686\t|\tloss: 0.743544\n",
      "Training Epoch 26  36.6% | batch:       251 of       686\t|\tloss: 0.85922\n",
      "Training Epoch 26  36.7% | batch:       252 of       686\t|\tloss: 0.78012\n",
      "Training Epoch 26  36.9% | batch:       253 of       686\t|\tloss: 0.80983\n",
      "Training Epoch 26  37.0% | batch:       254 of       686\t|\tloss: 0.813343\n",
      "Training Epoch 26  37.2% | batch:       255 of       686\t|\tloss: 0.714661\n",
      "Training Epoch 26  37.3% | batch:       256 of       686\t|\tloss: 1.06432\n",
      "Training Epoch 26  37.5% | batch:       257 of       686\t|\tloss: 0.835203\n",
      "Training Epoch 26  37.6% | batch:       258 of       686\t|\tloss: 0.844439\n",
      "Training Epoch 26  37.8% | batch:       259 of       686\t|\tloss: 0.902557\n",
      "Training Epoch 26  37.9% | batch:       260 of       686\t|\tloss: 0.70024\n",
      "Training Epoch 26  38.0% | batch:       261 of       686\t|\tloss: 0.76204\n",
      "Training Epoch 26  38.2% | batch:       262 of       686\t|\tloss: 0.840047\n",
      "Training Epoch 26  38.3% | batch:       263 of       686\t|\tloss: 0.757627\n",
      "Training Epoch 26  38.5% | batch:       264 of       686\t|\tloss: 1.27806\n",
      "Training Epoch 26  38.6% | batch:       265 of       686\t|\tloss: 1.09871\n",
      "Training Epoch 26  38.8% | batch:       266 of       686\t|\tloss: 0.873163\n",
      "Training Epoch 26  38.9% | batch:       267 of       686\t|\tloss: 0.803812\n",
      "Training Epoch 26  39.1% | batch:       268 of       686\t|\tloss: 0.789934\n",
      "Training Epoch 26  39.2% | batch:       269 of       686\t|\tloss: 1.1176\n",
      "Training Epoch 26  39.4% | batch:       270 of       686\t|\tloss: 0.883998\n",
      "Training Epoch 26  39.5% | batch:       271 of       686\t|\tloss: 0.909222\n",
      "Training Epoch 26  39.7% | batch:       272 of       686\t|\tloss: 0.798394\n",
      "Training Epoch 26  39.8% | batch:       273 of       686\t|\tloss: 0.90339\n",
      "Training Epoch 26  39.9% | batch:       274 of       686\t|\tloss: 0.901916\n",
      "Training Epoch 26  40.1% | batch:       275 of       686\t|\tloss: 1.03758\n",
      "Training Epoch 26  40.2% | batch:       276 of       686\t|\tloss: 0.867959\n",
      "Training Epoch 26  40.4% | batch:       277 of       686\t|\tloss: 0.87764\n",
      "Training Epoch 26  40.5% | batch:       278 of       686\t|\tloss: 1.04371\n",
      "Training Epoch 26  40.7% | batch:       279 of       686\t|\tloss: 0.776102\n",
      "Training Epoch 26  40.8% | batch:       280 of       686\t|\tloss: 0.942208\n",
      "Training Epoch 26  41.0% | batch:       281 of       686\t|\tloss: 0.822519\n",
      "Training Epoch 26  41.1% | batch:       282 of       686\t|\tloss: 0.800521\n",
      "Training Epoch 26  41.3% | batch:       283 of       686\t|\tloss: 0.752603\n",
      "Training Epoch 26  41.4% | batch:       284 of       686\t|\tloss: 0.921767\n",
      "Training Epoch 26  41.5% | batch:       285 of       686\t|\tloss: 0.96438\n",
      "Training Epoch 26  41.7% | batch:       286 of       686\t|\tloss: 0.690028\n",
      "Training Epoch 26  41.8% | batch:       287 of       686\t|\tloss: 0.732169\n",
      "Training Epoch 26  42.0% | batch:       288 of       686\t|\tloss: 0.872003\n",
      "Training Epoch 26  42.1% | batch:       289 of       686\t|\tloss: 0.919585\n",
      "Training Epoch 26  42.3% | batch:       290 of       686\t|\tloss: 0.657483\n",
      "Training Epoch 26  42.4% | batch:       291 of       686\t|\tloss: 0.980971\n",
      "Training Epoch 26  42.6% | batch:       292 of       686\t|\tloss: 0.915058\n",
      "Training Epoch 26  42.7% | batch:       293 of       686\t|\tloss: 0.841857\n",
      "Training Epoch 26  42.9% | batch:       294 of       686\t|\tloss: 0.770214\n",
      "Training Epoch 26  43.0% | batch:       295 of       686\t|\tloss: 0.622723\n",
      "Training Epoch 26  43.1% | batch:       296 of       686\t|\tloss: 0.93567\n",
      "Training Epoch 26  43.3% | batch:       297 of       686\t|\tloss: 0.943513\n",
      "Training Epoch 26  43.4% | batch:       298 of       686\t|\tloss: 0.791548\n",
      "Training Epoch 26  43.6% | batch:       299 of       686\t|\tloss: 1.20471\n",
      "Training Epoch 26  43.7% | batch:       300 of       686\t|\tloss: 1.14681\n",
      "Training Epoch 26  43.9% | batch:       301 of       686\t|\tloss: 1.02627\n",
      "Training Epoch 26  44.0% | batch:       302 of       686\t|\tloss: 0.741017\n",
      "Training Epoch 26  44.2% | batch:       303 of       686\t|\tloss: 0.933152\n",
      "Training Epoch 26  44.3% | batch:       304 of       686\t|\tloss: 0.792194\n",
      "Training Epoch 26  44.5% | batch:       305 of       686\t|\tloss: 1.06873\n",
      "Training Epoch 26  44.6% | batch:       306 of       686\t|\tloss: 0.990909\n",
      "Training Epoch 26  44.8% | batch:       307 of       686\t|\tloss: 0.844647\n",
      "Training Epoch 26  44.9% | batch:       308 of       686\t|\tloss: 0.723243\n",
      "Training Epoch 26  45.0% | batch:       309 of       686\t|\tloss: 0.860027\n",
      "Training Epoch 26  45.2% | batch:       310 of       686\t|\tloss: 1.02672\n",
      "Training Epoch 26  45.3% | batch:       311 of       686\t|\tloss: 0.753761\n",
      "Training Epoch 26  45.5% | batch:       312 of       686\t|\tloss: 1.1198\n",
      "Training Epoch 26  45.6% | batch:       313 of       686\t|\tloss: 0.854396\n",
      "Training Epoch 26  45.8% | batch:       314 of       686\t|\tloss: 0.778057\n",
      "Training Epoch 26  45.9% | batch:       315 of       686\t|\tloss: 0.924259\n",
      "Training Epoch 26  46.1% | batch:       316 of       686\t|\tloss: 0.950204\n",
      "Training Epoch 26  46.2% | batch:       317 of       686\t|\tloss: 0.745464\n",
      "Training Epoch 26  46.4% | batch:       318 of       686\t|\tloss: 0.810088\n",
      "Training Epoch 26  46.5% | batch:       319 of       686\t|\tloss: 0.785507\n",
      "Training Epoch 26  46.6% | batch:       320 of       686\t|\tloss: 0.839907\n",
      "Training Epoch 26  46.8% | batch:       321 of       686\t|\tloss: 0.946136\n",
      "Training Epoch 26  46.9% | batch:       322 of       686\t|\tloss: 0.663964\n",
      "Training Epoch 26  47.1% | batch:       323 of       686\t|\tloss: 0.799074\n",
      "Training Epoch 26  47.2% | batch:       324 of       686\t|\tloss: 0.870108\n",
      "Training Epoch 26  47.4% | batch:       325 of       686\t|\tloss: 0.751104\n",
      "Training Epoch 26  47.5% | batch:       326 of       686\t|\tloss: 0.809922\n",
      "Training Epoch 26  47.7% | batch:       327 of       686\t|\tloss: 0.741811\n",
      "Training Epoch 26  47.8% | batch:       328 of       686\t|\tloss: 0.667271\n",
      "Training Epoch 26  48.0% | batch:       329 of       686\t|\tloss: 0.816335\n",
      "Training Epoch 26  48.1% | batch:       330 of       686\t|\tloss: 1.11397\n",
      "Training Epoch 26  48.3% | batch:       331 of       686\t|\tloss: 0.802964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 26  48.4% | batch:       332 of       686\t|\tloss: 0.800599\n",
      "Training Epoch 26  48.5% | batch:       333 of       686\t|\tloss: 0.862758\n",
      "Training Epoch 26  48.7% | batch:       334 of       686\t|\tloss: 0.808735\n",
      "Training Epoch 26  48.8% | batch:       335 of       686\t|\tloss: 1.00815\n",
      "Training Epoch 26  49.0% | batch:       336 of       686\t|\tloss: 0.954315\n",
      "Training Epoch 26  49.1% | batch:       337 of       686\t|\tloss: 0.80976\n",
      "Training Epoch 26  49.3% | batch:       338 of       686\t|\tloss: 0.933046\n",
      "Training Epoch 26  49.4% | batch:       339 of       686\t|\tloss: 0.63914\n",
      "Training Epoch 26  49.6% | batch:       340 of       686\t|\tloss: 0.75971\n",
      "Training Epoch 26  49.7% | batch:       341 of       686\t|\tloss: 0.98185\n",
      "Training Epoch 26  49.9% | batch:       342 of       686\t|\tloss: 0.860827\n",
      "Training Epoch 26  50.0% | batch:       343 of       686\t|\tloss: 0.732425\n",
      "Training Epoch 26  50.1% | batch:       344 of       686\t|\tloss: 0.983919\n",
      "Training Epoch 26  50.3% | batch:       345 of       686\t|\tloss: 0.989334\n",
      "Training Epoch 26  50.4% | batch:       346 of       686\t|\tloss: 0.799634\n",
      "Training Epoch 26  50.6% | batch:       347 of       686\t|\tloss: 0.916008\n",
      "Training Epoch 26  50.7% | batch:       348 of       686\t|\tloss: 0.912254\n",
      "Training Epoch 26  50.9% | batch:       349 of       686\t|\tloss: 1.09783\n",
      "Training Epoch 26  51.0% | batch:       350 of       686\t|\tloss: 0.846881\n",
      "Training Epoch 26  51.2% | batch:       351 of       686\t|\tloss: 0.828834\n",
      "Training Epoch 26  51.3% | batch:       352 of       686\t|\tloss: 1.15382\n",
      "Training Epoch 26  51.5% | batch:       353 of       686\t|\tloss: 0.760077\n",
      "Training Epoch 26  51.6% | batch:       354 of       686\t|\tloss: 0.921494\n",
      "Training Epoch 26  51.7% | batch:       355 of       686\t|\tloss: 0.757201\n",
      "Training Epoch 26  51.9% | batch:       356 of       686\t|\tloss: 0.779051\n",
      "Training Epoch 26  52.0% | batch:       357 of       686\t|\tloss: 0.68775\n",
      "Training Epoch 26  52.2% | batch:       358 of       686\t|\tloss: 0.754047\n",
      "Training Epoch 26  52.3% | batch:       359 of       686\t|\tloss: 0.994864\n",
      "Training Epoch 26  52.5% | batch:       360 of       686\t|\tloss: 0.753091\n",
      "Training Epoch 26  52.6% | batch:       361 of       686\t|\tloss: 0.850526\n",
      "Training Epoch 26  52.8% | batch:       362 of       686\t|\tloss: 0.734174\n",
      "Training Epoch 26  52.9% | batch:       363 of       686\t|\tloss: 0.77166\n",
      "Training Epoch 26  53.1% | batch:       364 of       686\t|\tloss: 1.02424\n",
      "Training Epoch 26  53.2% | batch:       365 of       686\t|\tloss: 1.03106\n",
      "Training Epoch 26  53.4% | batch:       366 of       686\t|\tloss: 0.800601\n",
      "Training Epoch 26  53.5% | batch:       367 of       686\t|\tloss: 1.14549\n",
      "Training Epoch 26  53.6% | batch:       368 of       686\t|\tloss: 0.744425\n",
      "Training Epoch 26  53.8% | batch:       369 of       686\t|\tloss: 0.775059\n",
      "Training Epoch 26  53.9% | batch:       370 of       686\t|\tloss: 0.853391\n",
      "Training Epoch 26  54.1% | batch:       371 of       686\t|\tloss: 1.04941\n",
      "Training Epoch 26  54.2% | batch:       372 of       686\t|\tloss: 0.674008\n",
      "Training Epoch 26  54.4% | batch:       373 of       686\t|\tloss: 1.19738\n",
      "Training Epoch 26  54.5% | batch:       374 of       686\t|\tloss: 0.87446\n",
      "Training Epoch 26  54.7% | batch:       375 of       686\t|\tloss: 0.894298\n",
      "Training Epoch 26  54.8% | batch:       376 of       686\t|\tloss: 0.779961\n",
      "Training Epoch 26  55.0% | batch:       377 of       686\t|\tloss: 0.795439\n",
      "Training Epoch 26  55.1% | batch:       378 of       686\t|\tloss: 1.037\n",
      "Training Epoch 26  55.2% | batch:       379 of       686\t|\tloss: 0.989573\n",
      "Training Epoch 26  55.4% | batch:       380 of       686\t|\tloss: 0.714908\n",
      "Training Epoch 26  55.5% | batch:       381 of       686\t|\tloss: 0.857753\n",
      "Training Epoch 26  55.7% | batch:       382 of       686\t|\tloss: 0.91052\n",
      "Training Epoch 26  55.8% | batch:       383 of       686\t|\tloss: 0.947156\n",
      "Training Epoch 26  56.0% | batch:       384 of       686\t|\tloss: 0.645415\n",
      "Training Epoch 26  56.1% | batch:       385 of       686\t|\tloss: 0.858881\n",
      "Training Epoch 26  56.3% | batch:       386 of       686\t|\tloss: 1.00587\n",
      "Training Epoch 26  56.4% | batch:       387 of       686\t|\tloss: 0.920711\n",
      "Training Epoch 26  56.6% | batch:       388 of       686\t|\tloss: 0.774637\n",
      "Training Epoch 26  56.7% | batch:       389 of       686\t|\tloss: 0.766353\n",
      "Training Epoch 26  56.9% | batch:       390 of       686\t|\tloss: 1.02284\n",
      "Training Epoch 26  57.0% | batch:       391 of       686\t|\tloss: 0.988474\n",
      "Training Epoch 26  57.1% | batch:       392 of       686\t|\tloss: 0.91672\n",
      "Training Epoch 26  57.3% | batch:       393 of       686\t|\tloss: 0.586547\n",
      "Training Epoch 26  57.4% | batch:       394 of       686\t|\tloss: 0.655885\n",
      "Training Epoch 26  57.6% | batch:       395 of       686\t|\tloss: 0.649457\n",
      "Training Epoch 26  57.7% | batch:       396 of       686\t|\tloss: 1.00411\n",
      "Training Epoch 26  57.9% | batch:       397 of       686\t|\tloss: 0.718902\n",
      "Training Epoch 26  58.0% | batch:       398 of       686\t|\tloss: 0.843846\n",
      "Training Epoch 26  58.2% | batch:       399 of       686\t|\tloss: 0.708216\n",
      "Training Epoch 26  58.3% | batch:       400 of       686\t|\tloss: 0.935358\n",
      "Training Epoch 26  58.5% | batch:       401 of       686\t|\tloss: 0.767023\n",
      "Training Epoch 26  58.6% | batch:       402 of       686\t|\tloss: 0.980986\n",
      "Training Epoch 26  58.7% | batch:       403 of       686\t|\tloss: 1.10531\n",
      "Training Epoch 26  58.9% | batch:       404 of       686\t|\tloss: 1.00175\n",
      "Training Epoch 26  59.0% | batch:       405 of       686\t|\tloss: 0.745586\n",
      "Training Epoch 26  59.2% | batch:       406 of       686\t|\tloss: 0.822222\n",
      "Training Epoch 26  59.3% | batch:       407 of       686\t|\tloss: 0.885913\n",
      "Training Epoch 26  59.5% | batch:       408 of       686\t|\tloss: 0.856817\n",
      "Training Epoch 26  59.6% | batch:       409 of       686\t|\tloss: 0.812499\n",
      "Training Epoch 26  59.8% | batch:       410 of       686\t|\tloss: 0.687152\n",
      "Training Epoch 26  59.9% | batch:       411 of       686\t|\tloss: 0.826897\n",
      "Training Epoch 26  60.1% | batch:       412 of       686\t|\tloss: 0.902823\n",
      "Training Epoch 26  60.2% | batch:       413 of       686\t|\tloss: 0.684589\n",
      "Training Epoch 26  60.3% | batch:       414 of       686\t|\tloss: 0.799593\n",
      "Training Epoch 26  60.5% | batch:       415 of       686\t|\tloss: 0.922839\n",
      "Training Epoch 26  60.6% | batch:       416 of       686\t|\tloss: 0.640285\n",
      "Training Epoch 26  60.8% | batch:       417 of       686\t|\tloss: 0.858813\n",
      "Training Epoch 26  60.9% | batch:       418 of       686\t|\tloss: 1.12206\n",
      "Training Epoch 26  61.1% | batch:       419 of       686\t|\tloss: 0.925702\n",
      "Training Epoch 26  61.2% | batch:       420 of       686\t|\tloss: 0.897366\n",
      "Training Epoch 26  61.4% | batch:       421 of       686\t|\tloss: 1.01845\n",
      "Training Epoch 26  61.5% | batch:       422 of       686\t|\tloss: 1.05326\n",
      "Training Epoch 26  61.7% | batch:       423 of       686\t|\tloss: 1.10906\n",
      "Training Epoch 26  61.8% | batch:       424 of       686\t|\tloss: 0.776157\n",
      "Training Epoch 26  62.0% | batch:       425 of       686\t|\tloss: 0.951792\n",
      "Training Epoch 26  62.1% | batch:       426 of       686\t|\tloss: 0.89382\n",
      "Training Epoch 26  62.2% | batch:       427 of       686\t|\tloss: 0.804528\n",
      "Training Epoch 26  62.4% | batch:       428 of       686\t|\tloss: 0.727697\n",
      "Training Epoch 26  62.5% | batch:       429 of       686\t|\tloss: 0.790474\n",
      "Training Epoch 26  62.7% | batch:       430 of       686\t|\tloss: 0.957977\n",
      "Training Epoch 26  62.8% | batch:       431 of       686\t|\tloss: 0.953181\n",
      "Training Epoch 26  63.0% | batch:       432 of       686\t|\tloss: 0.698025\n",
      "Training Epoch 26  63.1% | batch:       433 of       686\t|\tloss: 0.76986\n",
      "Training Epoch 26  63.3% | batch:       434 of       686\t|\tloss: 1.11577\n",
      "Training Epoch 26  63.4% | batch:       435 of       686\t|\tloss: 0.834161\n",
      "Training Epoch 26  63.6% | batch:       436 of       686\t|\tloss: 0.876529\n",
      "Training Epoch 26  63.7% | batch:       437 of       686\t|\tloss: 0.907066\n",
      "Training Epoch 26  63.8% | batch:       438 of       686\t|\tloss: 0.704592\n",
      "Training Epoch 26  64.0% | batch:       439 of       686\t|\tloss: 1.19897\n",
      "Training Epoch 26  64.1% | batch:       440 of       686\t|\tloss: 1.14342\n",
      "Training Epoch 26  64.3% | batch:       441 of       686\t|\tloss: 0.821148\n",
      "Training Epoch 26  64.4% | batch:       442 of       686\t|\tloss: 0.703609\n",
      "Training Epoch 26  64.6% | batch:       443 of       686\t|\tloss: 0.710274\n",
      "Training Epoch 26  64.7% | batch:       444 of       686\t|\tloss: 0.734777\n",
      "Training Epoch 26  64.9% | batch:       445 of       686\t|\tloss: 0.582226\n",
      "Training Epoch 26  65.0% | batch:       446 of       686\t|\tloss: 0.868556\n",
      "Training Epoch 26  65.2% | batch:       447 of       686\t|\tloss: 0.844853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 26  65.3% | batch:       448 of       686\t|\tloss: 0.879463\n",
      "Training Epoch 26  65.5% | batch:       449 of       686\t|\tloss: 0.975754\n",
      "Training Epoch 26  65.6% | batch:       450 of       686\t|\tloss: 0.824782\n",
      "Training Epoch 26  65.7% | batch:       451 of       686\t|\tloss: 0.803507\n",
      "Training Epoch 26  65.9% | batch:       452 of       686\t|\tloss: 0.862684\n",
      "Training Epoch 26  66.0% | batch:       453 of       686\t|\tloss: 0.916474\n",
      "Training Epoch 26  66.2% | batch:       454 of       686\t|\tloss: 0.792117\n",
      "Training Epoch 26  66.3% | batch:       455 of       686\t|\tloss: 0.838867\n",
      "Training Epoch 26  66.5% | batch:       456 of       686\t|\tloss: 0.83052\n",
      "Training Epoch 26  66.6% | batch:       457 of       686\t|\tloss: 0.91872\n",
      "Training Epoch 26  66.8% | batch:       458 of       686\t|\tloss: 0.794598\n",
      "Training Epoch 26  66.9% | batch:       459 of       686\t|\tloss: 0.968701\n",
      "Training Epoch 26  67.1% | batch:       460 of       686\t|\tloss: 0.84058\n",
      "Training Epoch 26  67.2% | batch:       461 of       686\t|\tloss: 0.784397\n",
      "Training Epoch 26  67.3% | batch:       462 of       686\t|\tloss: 0.846558\n",
      "Training Epoch 26  67.5% | batch:       463 of       686\t|\tloss: 0.752964\n",
      "Training Epoch 26  67.6% | batch:       464 of       686\t|\tloss: 0.847914\n",
      "Training Epoch 26  67.8% | batch:       465 of       686\t|\tloss: 0.93083\n",
      "Training Epoch 26  67.9% | batch:       466 of       686\t|\tloss: 0.914575\n",
      "Training Epoch 26  68.1% | batch:       467 of       686\t|\tloss: 0.789182\n",
      "Training Epoch 26  68.2% | batch:       468 of       686\t|\tloss: 0.857316\n",
      "Training Epoch 26  68.4% | batch:       469 of       686\t|\tloss: 0.843173\n",
      "Training Epoch 26  68.5% | batch:       470 of       686\t|\tloss: 0.699678\n",
      "Training Epoch 26  68.7% | batch:       471 of       686\t|\tloss: 0.83081\n",
      "Training Epoch 26  68.8% | batch:       472 of       686\t|\tloss: 0.96353\n",
      "Training Epoch 26  69.0% | batch:       473 of       686\t|\tloss: 0.80458\n",
      "Training Epoch 26  69.1% | batch:       474 of       686\t|\tloss: 0.771441\n",
      "Training Epoch 26  69.2% | batch:       475 of       686\t|\tloss: 0.762086\n",
      "Training Epoch 26  69.4% | batch:       476 of       686\t|\tloss: 0.843883\n",
      "Training Epoch 26  69.5% | batch:       477 of       686\t|\tloss: 0.683184\n",
      "Training Epoch 26  69.7% | batch:       478 of       686\t|\tloss: 0.71352\n",
      "Training Epoch 26  69.8% | batch:       479 of       686\t|\tloss: 0.839494\n",
      "Training Epoch 26  70.0% | batch:       480 of       686\t|\tloss: 0.754187\n",
      "Training Epoch 26  70.1% | batch:       481 of       686\t|\tloss: 0.697116\n",
      "Training Epoch 26  70.3% | batch:       482 of       686\t|\tloss: 1.06224\n",
      "Training Epoch 26  70.4% | batch:       483 of       686\t|\tloss: 0.817649\n",
      "Training Epoch 26  70.6% | batch:       484 of       686\t|\tloss: 0.761903\n",
      "Training Epoch 26  70.7% | batch:       485 of       686\t|\tloss: 0.676756\n",
      "Training Epoch 26  70.8% | batch:       486 of       686\t|\tloss: 0.608806\n",
      "Training Epoch 26  71.0% | batch:       487 of       686\t|\tloss: 0.624406\n",
      "Training Epoch 26  71.1% | batch:       488 of       686\t|\tloss: 0.821349\n",
      "Training Epoch 26  71.3% | batch:       489 of       686\t|\tloss: 0.880723\n",
      "Training Epoch 26  71.4% | batch:       490 of       686\t|\tloss: 0.781186\n",
      "Training Epoch 26  71.6% | batch:       491 of       686\t|\tloss: 0.898237\n",
      "Training Epoch 26  71.7% | batch:       492 of       686\t|\tloss: 0.859697\n",
      "Training Epoch 26  71.9% | batch:       493 of       686\t|\tloss: 0.809877\n",
      "Training Epoch 26  72.0% | batch:       494 of       686\t|\tloss: 0.809633\n",
      "Training Epoch 26  72.2% | batch:       495 of       686\t|\tloss: 0.911156\n",
      "Training Epoch 26  72.3% | batch:       496 of       686\t|\tloss: 1.11024\n",
      "Training Epoch 26  72.4% | batch:       497 of       686\t|\tloss: 0.799847\n",
      "Training Epoch 26  72.6% | batch:       498 of       686\t|\tloss: 1.07729\n",
      "Training Epoch 26  72.7% | batch:       499 of       686\t|\tloss: 0.815886\n",
      "Training Epoch 26  72.9% | batch:       500 of       686\t|\tloss: 0.857353\n",
      "Training Epoch 26  73.0% | batch:       501 of       686\t|\tloss: 0.687616\n",
      "Training Epoch 26  73.2% | batch:       502 of       686\t|\tloss: 0.81803\n",
      "Training Epoch 26  73.3% | batch:       503 of       686\t|\tloss: 0.81952\n",
      "Training Epoch 26  73.5% | batch:       504 of       686\t|\tloss: 0.937692\n",
      "Training Epoch 26  73.6% | batch:       505 of       686\t|\tloss: 0.995848\n",
      "Training Epoch 26  73.8% | batch:       506 of       686\t|\tloss: 0.80374\n",
      "Training Epoch 26  73.9% | batch:       507 of       686\t|\tloss: 0.626627\n",
      "Training Epoch 26  74.1% | batch:       508 of       686\t|\tloss: 0.737764\n",
      "Training Epoch 26  74.2% | batch:       509 of       686\t|\tloss: 0.971238\n",
      "Training Epoch 26  74.3% | batch:       510 of       686\t|\tloss: 1.06856\n",
      "Training Epoch 26  74.5% | batch:       511 of       686\t|\tloss: 0.809601\n",
      "Training Epoch 26  74.6% | batch:       512 of       686\t|\tloss: 0.869641\n",
      "Training Epoch 26  74.8% | batch:       513 of       686\t|\tloss: 1.05583\n",
      "Training Epoch 26  74.9% | batch:       514 of       686\t|\tloss: 0.849956\n",
      "Training Epoch 26  75.1% | batch:       515 of       686\t|\tloss: 0.714029\n",
      "Training Epoch 26  75.2% | batch:       516 of       686\t|\tloss: 0.882831\n",
      "Training Epoch 26  75.4% | batch:       517 of       686\t|\tloss: 1.0206\n",
      "Training Epoch 26  75.5% | batch:       518 of       686\t|\tloss: 0.843142\n",
      "Training Epoch 26  75.7% | batch:       519 of       686\t|\tloss: 0.750496\n",
      "Training Epoch 26  75.8% | batch:       520 of       686\t|\tloss: 0.96146\n",
      "Training Epoch 26  75.9% | batch:       521 of       686\t|\tloss: 0.865005\n",
      "Training Epoch 26  76.1% | batch:       522 of       686\t|\tloss: 0.791885\n",
      "Training Epoch 26  76.2% | batch:       523 of       686\t|\tloss: 0.911863\n",
      "Training Epoch 26  76.4% | batch:       524 of       686\t|\tloss: 0.895499\n",
      "Training Epoch 26  76.5% | batch:       525 of       686\t|\tloss: 0.955909\n",
      "Training Epoch 26  76.7% | batch:       526 of       686\t|\tloss: 1.10238\n",
      "Training Epoch 26  76.8% | batch:       527 of       686\t|\tloss: 0.815056\n",
      "Training Epoch 26  77.0% | batch:       528 of       686\t|\tloss: 0.707643\n",
      "Training Epoch 26  77.1% | batch:       529 of       686\t|\tloss: 0.694497\n",
      "Training Epoch 26  77.3% | batch:       530 of       686\t|\tloss: 0.789908\n",
      "Training Epoch 26  77.4% | batch:       531 of       686\t|\tloss: 0.99326\n",
      "Training Epoch 26  77.6% | batch:       532 of       686\t|\tloss: 0.868225\n",
      "Training Epoch 26  77.7% | batch:       533 of       686\t|\tloss: 0.715142\n",
      "Training Epoch 26  77.8% | batch:       534 of       686\t|\tloss: 0.632275\n",
      "Training Epoch 26  78.0% | batch:       535 of       686\t|\tloss: 0.743637\n",
      "Training Epoch 26  78.1% | batch:       536 of       686\t|\tloss: 0.802952\n",
      "Training Epoch 26  78.3% | batch:       537 of       686\t|\tloss: 0.905963\n",
      "Training Epoch 26  78.4% | batch:       538 of       686\t|\tloss: 0.844651\n",
      "Training Epoch 26  78.6% | batch:       539 of       686\t|\tloss: 1.0792\n",
      "Training Epoch 26  78.7% | batch:       540 of       686\t|\tloss: 0.859898\n",
      "Training Epoch 26  78.9% | batch:       541 of       686\t|\tloss: 0.707739\n",
      "Training Epoch 26  79.0% | batch:       542 of       686\t|\tloss: 0.850431\n",
      "Training Epoch 26  79.2% | batch:       543 of       686\t|\tloss: 0.971178\n",
      "Training Epoch 26  79.3% | batch:       544 of       686\t|\tloss: 0.807336\n",
      "Training Epoch 26  79.4% | batch:       545 of       686\t|\tloss: 1.03579\n",
      "Training Epoch 26  79.6% | batch:       546 of       686\t|\tloss: 0.758431\n",
      "Training Epoch 26  79.7% | batch:       547 of       686\t|\tloss: 1.02997\n",
      "Training Epoch 26  79.9% | batch:       548 of       686\t|\tloss: 0.802981\n",
      "Training Epoch 26  80.0% | batch:       549 of       686\t|\tloss: 0.751204\n",
      "Training Epoch 26  80.2% | batch:       550 of       686\t|\tloss: 0.683729\n",
      "Training Epoch 26  80.3% | batch:       551 of       686\t|\tloss: 0.848364\n",
      "Training Epoch 26  80.5% | batch:       552 of       686\t|\tloss: 0.892485\n",
      "Training Epoch 26  80.6% | batch:       553 of       686\t|\tloss: 0.827707\n",
      "Training Epoch 26  80.8% | batch:       554 of       686\t|\tloss: 0.753477\n",
      "Training Epoch 26  80.9% | batch:       555 of       686\t|\tloss: 0.67047\n",
      "Training Epoch 26  81.0% | batch:       556 of       686\t|\tloss: 0.820523\n",
      "Training Epoch 26  81.2% | batch:       557 of       686\t|\tloss: 0.697912\n",
      "Training Epoch 26  81.3% | batch:       558 of       686\t|\tloss: 1.14297\n",
      "Training Epoch 26  81.5% | batch:       559 of       686\t|\tloss: 0.725114\n",
      "Training Epoch 26  81.6% | batch:       560 of       686\t|\tloss: 0.910259\n",
      "Training Epoch 26  81.8% | batch:       561 of       686\t|\tloss: 0.875788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 26  81.9% | batch:       562 of       686\t|\tloss: 1.05714\n",
      "Training Epoch 26  82.1% | batch:       563 of       686\t|\tloss: 0.73933\n",
      "Training Epoch 26  82.2% | batch:       564 of       686\t|\tloss: 1.01763\n",
      "Training Epoch 26  82.4% | batch:       565 of       686\t|\tloss: 0.739012\n",
      "Training Epoch 26  82.5% | batch:       566 of       686\t|\tloss: 0.839865\n",
      "Training Epoch 26  82.7% | batch:       567 of       686\t|\tloss: 0.705423\n",
      "Training Epoch 26  82.8% | batch:       568 of       686\t|\tloss: 0.905492\n",
      "Training Epoch 26  82.9% | batch:       569 of       686\t|\tloss: 0.653654\n",
      "Training Epoch 26  83.1% | batch:       570 of       686\t|\tloss: 0.845837\n",
      "Training Epoch 26  83.2% | batch:       571 of       686\t|\tloss: 0.837456\n",
      "Training Epoch 26  83.4% | batch:       572 of       686\t|\tloss: 1.18586\n",
      "Training Epoch 26  83.5% | batch:       573 of       686\t|\tloss: 0.883918\n",
      "Training Epoch 26  83.7% | batch:       574 of       686\t|\tloss: 0.736998\n",
      "Training Epoch 26  83.8% | batch:       575 of       686\t|\tloss: 0.88555\n",
      "Training Epoch 26  84.0% | batch:       576 of       686\t|\tloss: 0.666541\n",
      "Training Epoch 26  84.1% | batch:       577 of       686\t|\tloss: 1.02594\n",
      "Training Epoch 26  84.3% | batch:       578 of       686\t|\tloss: 1.24212\n",
      "Training Epoch 26  84.4% | batch:       579 of       686\t|\tloss: 0.675362\n",
      "Training Epoch 26  84.5% | batch:       580 of       686\t|\tloss: 0.857401\n",
      "Training Epoch 26  84.7% | batch:       581 of       686\t|\tloss: 0.789339\n",
      "Training Epoch 26  84.8% | batch:       582 of       686\t|\tloss: 0.841639\n",
      "Training Epoch 26  85.0% | batch:       583 of       686\t|\tloss: 0.697958\n",
      "Training Epoch 26  85.1% | batch:       584 of       686\t|\tloss: 0.863706\n",
      "Training Epoch 26  85.3% | batch:       585 of       686\t|\tloss: 0.782548\n",
      "Training Epoch 26  85.4% | batch:       586 of       686\t|\tloss: 0.748169\n",
      "Training Epoch 26  85.6% | batch:       587 of       686\t|\tloss: 0.900685\n",
      "Training Epoch 26  85.7% | batch:       588 of       686\t|\tloss: 0.737454\n",
      "Training Epoch 26  85.9% | batch:       589 of       686\t|\tloss: 0.88225\n",
      "Training Epoch 26  86.0% | batch:       590 of       686\t|\tloss: 0.740072\n",
      "Training Epoch 26  86.2% | batch:       591 of       686\t|\tloss: 0.840676\n",
      "Training Epoch 26  86.3% | batch:       592 of       686\t|\tloss: 0.938772\n",
      "Training Epoch 26  86.4% | batch:       593 of       686\t|\tloss: 0.760753\n",
      "Training Epoch 26  86.6% | batch:       594 of       686\t|\tloss: 0.805078\n",
      "Training Epoch 26  86.7% | batch:       595 of       686\t|\tloss: 0.802548\n",
      "Training Epoch 26  86.9% | batch:       596 of       686\t|\tloss: 0.999738\n",
      "Training Epoch 26  87.0% | batch:       597 of       686\t|\tloss: 0.881234\n",
      "Training Epoch 26  87.2% | batch:       598 of       686\t|\tloss: 0.765702\n",
      "Training Epoch 26  87.3% | batch:       599 of       686\t|\tloss: 0.929851\n",
      "Training Epoch 26  87.5% | batch:       600 of       686\t|\tloss: 0.831345\n",
      "Training Epoch 26  87.6% | batch:       601 of       686\t|\tloss: 1.11987\n",
      "Training Epoch 26  87.8% | batch:       602 of       686\t|\tloss: 0.851687\n",
      "Training Epoch 26  87.9% | batch:       603 of       686\t|\tloss: 0.658952\n",
      "Training Epoch 26  88.0% | batch:       604 of       686\t|\tloss: 0.882935\n",
      "Training Epoch 26  88.2% | batch:       605 of       686\t|\tloss: 0.794825\n",
      "Training Epoch 26  88.3% | batch:       606 of       686\t|\tloss: 0.986467\n",
      "Training Epoch 26  88.5% | batch:       607 of       686\t|\tloss: 0.775371\n",
      "Training Epoch 26  88.6% | batch:       608 of       686\t|\tloss: 0.763296\n",
      "Training Epoch 26  88.8% | batch:       609 of       686\t|\tloss: 0.760524\n",
      "Training Epoch 26  88.9% | batch:       610 of       686\t|\tloss: 0.866135\n",
      "Training Epoch 26  89.1% | batch:       611 of       686\t|\tloss: 0.726405\n",
      "Training Epoch 26  89.2% | batch:       612 of       686\t|\tloss: 0.783156\n",
      "Training Epoch 26  89.4% | batch:       613 of       686\t|\tloss: 0.769152\n",
      "Training Epoch 26  89.5% | batch:       614 of       686\t|\tloss: 1.11026\n",
      "Training Epoch 26  89.7% | batch:       615 of       686\t|\tloss: 0.876211\n",
      "Training Epoch 26  89.8% | batch:       616 of       686\t|\tloss: 0.875661\n",
      "Training Epoch 26  89.9% | batch:       617 of       686\t|\tloss: 0.864107\n",
      "Training Epoch 26  90.1% | batch:       618 of       686\t|\tloss: 0.860717\n",
      "Training Epoch 26  90.2% | batch:       619 of       686\t|\tloss: 0.952266\n",
      "Training Epoch 26  90.4% | batch:       620 of       686\t|\tloss: 0.750365\n",
      "Training Epoch 26  90.5% | batch:       621 of       686\t|\tloss: 0.783726\n",
      "Training Epoch 26  90.7% | batch:       622 of       686\t|\tloss: 0.75997\n",
      "Training Epoch 26  90.8% | batch:       623 of       686\t|\tloss: 0.619093\n",
      "Training Epoch 26  91.0% | batch:       624 of       686\t|\tloss: 0.942345\n",
      "Training Epoch 26  91.1% | batch:       625 of       686\t|\tloss: 0.647728\n",
      "Training Epoch 26  91.3% | batch:       626 of       686\t|\tloss: 0.96858\n",
      "Training Epoch 26  91.4% | batch:       627 of       686\t|\tloss: 0.813292\n",
      "Training Epoch 26  91.5% | batch:       628 of       686\t|\tloss: 0.768439\n",
      "Training Epoch 26  91.7% | batch:       629 of       686\t|\tloss: 0.86988\n",
      "Training Epoch 26  91.8% | batch:       630 of       686\t|\tloss: 0.995275\n",
      "Training Epoch 26  92.0% | batch:       631 of       686\t|\tloss: 0.821095\n",
      "Training Epoch 26  92.1% | batch:       632 of       686\t|\tloss: 0.932015\n",
      "Training Epoch 26  92.3% | batch:       633 of       686\t|\tloss: 0.739347\n",
      "Training Epoch 26  92.4% | batch:       634 of       686\t|\tloss: 0.852799\n",
      "Training Epoch 26  92.6% | batch:       635 of       686\t|\tloss: 1.01719\n",
      "Training Epoch 26  92.7% | batch:       636 of       686\t|\tloss: 0.722692\n",
      "Training Epoch 26  92.9% | batch:       637 of       686\t|\tloss: 0.732218\n",
      "Training Epoch 26  93.0% | batch:       638 of       686\t|\tloss: 0.763324\n",
      "Training Epoch 26  93.1% | batch:       639 of       686\t|\tloss: 0.668141\n",
      "Training Epoch 26  93.3% | batch:       640 of       686\t|\tloss: 0.619274\n",
      "Training Epoch 26  93.4% | batch:       641 of       686\t|\tloss: 0.817454\n",
      "Training Epoch 26  93.6% | batch:       642 of       686\t|\tloss: 0.77372\n",
      "Training Epoch 26  93.7% | batch:       643 of       686\t|\tloss: 0.945277\n",
      "Training Epoch 26  93.9% | batch:       644 of       686\t|\tloss: 0.774553\n",
      "Training Epoch 26  94.0% | batch:       645 of       686\t|\tloss: 0.921659\n",
      "Training Epoch 26  94.2% | batch:       646 of       686\t|\tloss: 0.803475\n",
      "Training Epoch 26  94.3% | batch:       647 of       686\t|\tloss: 1.03373\n",
      "Training Epoch 26  94.5% | batch:       648 of       686\t|\tloss: 1.14206\n",
      "Training Epoch 26  94.6% | batch:       649 of       686\t|\tloss: 0.719684\n",
      "Training Epoch 26  94.8% | batch:       650 of       686\t|\tloss: 0.75364\n",
      "Training Epoch 26  94.9% | batch:       651 of       686\t|\tloss: 0.977083\n",
      "Training Epoch 26  95.0% | batch:       652 of       686\t|\tloss: 0.863266\n",
      "Training Epoch 26  95.2% | batch:       653 of       686\t|\tloss: 0.715246\n",
      "Training Epoch 26  95.3% | batch:       654 of       686\t|\tloss: 0.705381\n",
      "Training Epoch 26  95.5% | batch:       655 of       686\t|\tloss: 0.831934\n",
      "Training Epoch 26  95.6% | batch:       656 of       686\t|\tloss: 0.793695\n",
      "Training Epoch 26  95.8% | batch:       657 of       686\t|\tloss: 0.659342\n",
      "Training Epoch 26  95.9% | batch:       658 of       686\t|\tloss: 0.751713\n",
      "Training Epoch 26  96.1% | batch:       659 of       686\t|\tloss: 0.696483\n",
      "Training Epoch 26  96.2% | batch:       660 of       686\t|\tloss: 0.740245\n",
      "Training Epoch 26  96.4% | batch:       661 of       686\t|\tloss: 0.979219\n",
      "Training Epoch 26  96.5% | batch:       662 of       686\t|\tloss: 1.21148\n",
      "Training Epoch 26  96.6% | batch:       663 of       686\t|\tloss: 0.838209\n",
      "Training Epoch 26  96.8% | batch:       664 of       686\t|\tloss: 0.811233\n",
      "Training Epoch 26  96.9% | batch:       665 of       686\t|\tloss: 0.810457\n",
      "Training Epoch 26  97.1% | batch:       666 of       686\t|\tloss: 0.686273\n",
      "Training Epoch 26  97.2% | batch:       667 of       686\t|\tloss: 0.917598\n",
      "Training Epoch 26  97.4% | batch:       668 of       686\t|\tloss: 0.79666\n",
      "Training Epoch 26  97.5% | batch:       669 of       686\t|\tloss: 0.870786\n",
      "Training Epoch 26  97.7% | batch:       670 of       686\t|\tloss: 0.795668\n",
      "Training Epoch 26  97.8% | batch:       671 of       686\t|\tloss: 0.838097\n",
      "Training Epoch 26  98.0% | batch:       672 of       686\t|\tloss: 0.935284\n",
      "Training Epoch 26  98.1% | batch:       673 of       686\t|\tloss: 0.841337\n",
      "Training Epoch 26  98.3% | batch:       674 of       686\t|\tloss: 0.803505\n",
      "Training Epoch 26  98.4% | batch:       675 of       686\t|\tloss: 0.817858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 26  98.5% | batch:       676 of       686\t|\tloss: 1.09252\n",
      "Training Epoch 26  98.7% | batch:       677 of       686\t|\tloss: 0.714886\n",
      "Training Epoch 26  98.8% | batch:       678 of       686\t|\tloss: 0.894637\n",
      "Training Epoch 26  99.0% | batch:       679 of       686\t|\tloss: 0.736617\n",
      "Training Epoch 26  99.1% | batch:       680 of       686\t|\tloss: 1.05326\n",
      "Training Epoch 26  99.3% | batch:       681 of       686\t|\tloss: 0.950223\n",
      "Training Epoch 26  99.4% | batch:       682 of       686\t|\tloss: 0.718346\n",
      "Training Epoch 26  99.6% | batch:       683 of       686\t|\tloss: 0.836257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:11:54,998 | INFO : Epoch 26 Training Summary: epoch: 26.000000 | loss: 0.871468 | \n",
      "2023-05-25 22:11:54,999 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 22.075047254562378 seconds\n",
      "\n",
      "2023-05-25 22:11:55,000 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.891895312529343 seconds\n",
      "2023-05-25 22:11:55,000 | INFO : Avg batch train. time: 0.034827835732550064 seconds\n",
      "2023-05-25 22:11:55,001 | INFO : Avg sample train. time: 0.00027244307329413696 seconds\n",
      "2023-05-25 22:11:55,001 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 26  99.7% | batch:       684 of       686\t|\tloss: 0.9019\n",
      "Training Epoch 26  99.9% | batch:       685 of       686\t|\tloss: 0.760281\n",
      "\n",
      "Evaluating Epoch 26   0.0% | batch:         0 of       172\t|\tloss: 1.45476\n",
      "Evaluating Epoch 26   0.6% | batch:         1 of       172\t|\tloss: 1.45573\n",
      "Evaluating Epoch 26   1.2% | batch:         2 of       172\t|\tloss: 0.63809\n",
      "Evaluating Epoch 26   1.7% | batch:         3 of       172\t|\tloss: 2.48209\n",
      "Evaluating Epoch 26   2.3% | batch:         4 of       172\t|\tloss: 1.10812\n",
      "Evaluating Epoch 26   2.9% | batch:         5 of       172\t|\tloss: 1.20531\n",
      "Evaluating Epoch 26   3.5% | batch:         6 of       172\t|\tloss: 1.14349\n",
      "Evaluating Epoch 26   4.1% | batch:         7 of       172\t|\tloss: 2.91648\n",
      "Evaluating Epoch 26   4.7% | batch:         8 of       172\t|\tloss: 0.583163\n",
      "Evaluating Epoch 26   5.2% | batch:         9 of       172\t|\tloss: 1.78538\n",
      "Evaluating Epoch 26   5.8% | batch:        10 of       172\t|\tloss: 1.28755\n",
      "Evaluating Epoch 26   6.4% | batch:        11 of       172\t|\tloss: 1.30564\n",
      "Evaluating Epoch 26   7.0% | batch:        12 of       172\t|\tloss: 1.19128\n",
      "Evaluating Epoch 26   7.6% | batch:        13 of       172\t|\tloss: 1.24924\n",
      "Evaluating Epoch 26   8.1% | batch:        14 of       172\t|\tloss: 1.47957\n",
      "Evaluating Epoch 26   8.7% | batch:        15 of       172\t|\tloss: 1.41114\n",
      "Evaluating Epoch 26   9.3% | batch:        16 of       172\t|\tloss: 2.15306\n",
      "Evaluating Epoch 26   9.9% | batch:        17 of       172\t|\tloss: 0.777781\n",
      "Evaluating Epoch 26  10.5% | batch:        18 of       172\t|\tloss: 24.1211\n",
      "Evaluating Epoch 26  11.0% | batch:        19 of       172\t|\tloss: 1.54048\n",
      "Evaluating Epoch 26  11.6% | batch:        20 of       172\t|\tloss: 2.08957\n",
      "Evaluating Epoch 26  12.2% | batch:        21 of       172\t|\tloss: 0.851697\n",
      "Evaluating Epoch 26  12.8% | batch:        22 of       172\t|\tloss: 7.36232\n",
      "Evaluating Epoch 26  13.4% | batch:        23 of       172\t|\tloss: 3.81539\n",
      "Evaluating Epoch 26  14.0% | batch:        24 of       172\t|\tloss: 1.01996\n",
      "Evaluating Epoch 26  14.5% | batch:        25 of       172\t|\tloss: 1.89398\n",
      "Evaluating Epoch 26  15.1% | batch:        26 of       172\t|\tloss: 10.3828\n",
      "Evaluating Epoch 26  15.7% | batch:        27 of       172\t|\tloss: 20.1499\n",
      "Evaluating Epoch 26  16.3% | batch:        28 of       172\t|\tloss: 0.287005\n",
      "Evaluating Epoch 26  16.9% | batch:        29 of       172\t|\tloss: 1.57686\n",
      "Evaluating Epoch 26  17.4% | batch:        30 of       172\t|\tloss: 1.25953\n",
      "Evaluating Epoch 26  18.0% | batch:        31 of       172\t|\tloss: 1.07862\n",
      "Evaluating Epoch 26  18.6% | batch:        32 of       172\t|\tloss: 0.305505\n",
      "Evaluating Epoch 26  19.2% | batch:        33 of       172\t|\tloss: 0.454765\n",
      "Evaluating Epoch 26  19.8% | batch:        34 of       172\t|\tloss: 0.312515\n",
      "Evaluating Epoch 26  20.3% | batch:        35 of       172\t|\tloss: 0.933027\n",
      "Evaluating Epoch 26  20.9% | batch:        36 of       172\t|\tloss: 2.7345\n",
      "Evaluating Epoch 26  21.5% | batch:        37 of       172\t|\tloss: 4.30274\n",
      "Evaluating Epoch 26  22.1% | batch:        38 of       172\t|\tloss: 5.14871\n",
      "Evaluating Epoch 26  22.7% | batch:        39 of       172\t|\tloss: 10.9198\n",
      "Evaluating Epoch 26  23.3% | batch:        40 of       172\t|\tloss: 0.331295\n",
      "Evaluating Epoch 26  23.8% | batch:        41 of       172\t|\tloss: 0.7685\n",
      "Evaluating Epoch 26  24.4% | batch:        42 of       172\t|\tloss: 0.52272\n",
      "Evaluating Epoch 26  25.0% | batch:        43 of       172\t|\tloss: 25.7712\n",
      "Evaluating Epoch 26  25.6% | batch:        44 of       172\t|\tloss: 1.40404\n",
      "Evaluating Epoch 26  26.2% | batch:        45 of       172\t|\tloss: 0.93452\n",
      "Evaluating Epoch 26  26.7% | batch:        46 of       172\t|\tloss: 0.53089\n",
      "Evaluating Epoch 26  27.3% | batch:        47 of       172\t|\tloss: 1.51996\n",
      "Evaluating Epoch 26  27.9% | batch:        48 of       172\t|\tloss: 0.510439\n",
      "Evaluating Epoch 26  28.5% | batch:        49 of       172\t|\tloss: 0.913166\n",
      "Evaluating Epoch 26  29.1% | batch:        50 of       172\t|\tloss: 0.399462\n",
      "Evaluating Epoch 26  29.7% | batch:        51 of       172\t|\tloss: 0.851638\n",
      "Evaluating Epoch 26  30.2% | batch:        52 of       172\t|\tloss: 1.59371\n",
      "Evaluating Epoch 26  30.8% | batch:        53 of       172\t|\tloss: 2.69948\n",
      "Evaluating Epoch 26  31.4% | batch:        54 of       172\t|\tloss: 0.906927\n",
      "Evaluating Epoch 26  32.0% | batch:        55 of       172\t|\tloss: 0.583979\n",
      "Evaluating Epoch 26  32.6% | batch:        56 of       172\t|\tloss: 2.946\n",
      "Evaluating Epoch 26  33.1% | batch:        57 of       172\t|\tloss: 0.387138\n",
      "Evaluating Epoch 26  33.7% | batch:        58 of       172\t|\tloss: 1.92994\n",
      "Evaluating Epoch 26  34.3% | batch:        59 of       172\t|\tloss: 1.61076\n",
      "Evaluating Epoch 26  34.9% | batch:        60 of       172\t|\tloss: 1.23938\n",
      "Evaluating Epoch 26  35.5% | batch:        61 of       172\t|\tloss: 1.42016\n",
      "Evaluating Epoch 26  36.0% | batch:        62 of       172\t|\tloss: 0.75737\n",
      "Evaluating Epoch 26  36.6% | batch:        63 of       172\t|\tloss: 2.76108\n",
      "Evaluating Epoch 26  37.2% | batch:        64 of       172\t|\tloss: 1.37472\n",
      "Evaluating Epoch 26  37.8% | batch:        65 of       172\t|\tloss: 2.10723\n",
      "Evaluating Epoch 26  38.4% | batch:        66 of       172\t|\tloss: 1.08076\n",
      "Evaluating Epoch 26  39.0% | batch:        67 of       172\t|\tloss: 0.793314\n",
      "Evaluating Epoch 26  39.5% | batch:        68 of       172\t|\tloss: 2.27578\n",
      "Evaluating Epoch 26  40.1% | batch:        69 of       172\t|\tloss: 1.059\n",
      "Evaluating Epoch 26  40.7% | batch:        70 of       172\t|\tloss: 1.54252\n",
      "Evaluating Epoch 26  41.3% | batch:        71 of       172\t|\tloss: 1.73257\n",
      "Evaluating Epoch 26  41.9% | batch:        72 of       172\t|\tloss: 1.07713\n",
      "Evaluating Epoch 26  42.4% | batch:        73 of       172\t|\tloss: 2.26803\n",
      "Evaluating Epoch 26  43.0% | batch:        74 of       172\t|\tloss: 0.169652\n",
      "Evaluating Epoch 26  43.6% | batch:        75 of       172\t|\tloss: 0.220098\n",
      "Evaluating Epoch 26  44.2% | batch:        76 of       172\t|\tloss: 0.302886\n",
      "Evaluating Epoch 26  44.8% | batch:        77 of       172\t|\tloss: 0.229929\n",
      "Evaluating Epoch 26  45.3% | batch:        78 of       172\t|\tloss: 0.233318\n",
      "Evaluating Epoch 26  45.9% | batch:        79 of       172\t|\tloss: 0.309636\n",
      "Evaluating Epoch 26  46.5% | batch:        80 of       172\t|\tloss: 0.21295\n",
      "Evaluating Epoch 26  47.1% | batch:        81 of       172\t|\tloss: 0.218163\n",
      "Evaluating Epoch 26  47.7% | batch:        82 of       172\t|\tloss: 0.268436\n",
      "Evaluating Epoch 26  48.3% | batch:        83 of       172\t|\tloss: 0.334128\n",
      "Evaluating Epoch 26  48.8% | batch:        84 of       172\t|\tloss: 0.718536\n",
      "Evaluating Epoch 26  49.4% | batch:        85 of       172\t|\tloss: 0.5036\n",
      "Evaluating Epoch 26  50.0% | batch:        86 of       172\t|\tloss: 0.362872\n",
      "Evaluating Epoch 26  50.6% | batch:        87 of       172\t|\tloss: 0.65097\n",
      "Evaluating Epoch 26  51.2% | batch:        88 of       172\t|\tloss: 0.506651\n",
      "Evaluating Epoch 26  51.7% | batch:        89 of       172\t|\tloss: 0.364319\n",
      "Evaluating Epoch 26  52.3% | batch:        90 of       172\t|\tloss: 0.480786\n",
      "Evaluating Epoch 26  52.9% | batch:        91 of       172\t|\tloss: 0.400839\n",
      "Evaluating Epoch 26  53.5% | batch:        92 of       172\t|\tloss: 0.358778\n",
      "Evaluating Epoch 26  54.1% | batch:        93 of       172\t|\tloss: 0.576687\n",
      "Evaluating Epoch 26  54.7% | batch:        94 of       172\t|\tloss: 0.455933\n",
      "Evaluating Epoch 26  55.2% | batch:        95 of       172\t|\tloss: 0.410565\n",
      "Evaluating Epoch 26  55.8% | batch:        96 of       172\t|\tloss: 0.377416\n",
      "Evaluating Epoch 26  56.4% | batch:        97 of       172\t|\tloss: 0.499744\n",
      "Evaluating Epoch 26  57.0% | batch:        98 of       172\t|\tloss: 0.515383\n",
      "Evaluating Epoch 26  57.6% | batch:        99 of       172\t|\tloss: 0.35491\n",
      "Evaluating Epoch 26  58.1% | batch:       100 of       172\t|\tloss: 0.367731\n",
      "Evaluating Epoch 26  58.7% | batch:       101 of       172\t|\tloss: 0.515352\n",
      "Evaluating Epoch 26  59.3% | batch:       102 of       172\t|\tloss: 0.474455\n",
      "Evaluating Epoch 26  59.9% | batch:       103 of       172\t|\tloss: 0.584545\n",
      "Evaluating Epoch 26  60.5% | batch:       104 of       172\t|\tloss: 0.500062\n",
      "Evaluating Epoch 26  61.0% | batch:       105 of       172\t|\tloss: 0.277465\n",
      "Evaluating Epoch 26  61.6% | batch:       106 of       172\t|\tloss: 0.331815\n",
      "Evaluating Epoch 26  62.2% | batch:       107 of       172\t|\tloss: 0.726785\n",
      "Evaluating Epoch 26  62.8% | batch:       108 of       172\t|\tloss: 0.358987\n",
      "Evaluating Epoch 26  63.4% | batch:       109 of       172\t|\tloss: 0.476719\n",
      "Evaluating Epoch 26  64.0% | batch:       110 of       172\t|\tloss: 0.478952\n",
      "Evaluating Epoch 26  64.5% | batch:       111 of       172\t|\tloss: 0.52308\n",
      "Evaluating Epoch 26  65.1% | batch:       112 of       172\t|\tloss: 0.718589\n",
      "Evaluating Epoch 26  65.7% | batch:       113 of       172\t|\tloss: 0.564464\n",
      "Evaluating Epoch 26  66.3% | batch:       114 of       172\t|\tloss: 0.360865\n",
      "Evaluating Epoch 26  66.9% | batch:       115 of       172\t|\tloss: 0.352306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 26  67.4% | batch:       116 of       172\t|\tloss: 0.38178\n",
      "Evaluating Epoch 26  68.0% | batch:       117 of       172\t|\tloss: 0.206571\n",
      "Evaluating Epoch 26  68.6% | batch:       118 of       172\t|\tloss: 0.516602\n",
      "Evaluating Epoch 26  69.2% | batch:       119 of       172\t|\tloss: 0.208124\n",
      "Evaluating Epoch 26  69.8% | batch:       120 of       172\t|\tloss: 0.49433\n",
      "Evaluating Epoch 26  70.3% | batch:       121 of       172\t|\tloss: 0.50792\n",
      "Evaluating Epoch 26  70.9% | batch:       122 of       172\t|\tloss: 0.456034\n",
      "Evaluating Epoch 26  71.5% | batch:       123 of       172\t|\tloss: 0.631349\n",
      "Evaluating Epoch 26  72.1% | batch:       124 of       172\t|\tloss: 0.461207\n",
      "Evaluating Epoch 26  72.7% | batch:       125 of       172\t|\tloss: 0.288866\n",
      "Evaluating Epoch 26  73.3% | batch:       126 of       172\t|\tloss: 0.280175\n",
      "Evaluating Epoch 26  73.8% | batch:       127 of       172\t|\tloss: 0.535896\n",
      "Evaluating Epoch 26  74.4% | batch:       128 of       172\t|\tloss: 0.66086\n",
      "Evaluating Epoch 26  75.0% | batch:       129 of       172\t|\tloss: 0.383635\n",
      "Evaluating Epoch 26  75.6% | batch:       130 of       172\t|\tloss: 0.643554\n",
      "Evaluating Epoch 26  76.2% | batch:       131 of       172\t|\tloss: 0.490674\n",
      "Evaluating Epoch 26  76.7% | batch:       132 of       172\t|\tloss: 0.439242\n",
      "Evaluating Epoch 26  77.3% | batch:       133 of       172\t|\tloss: 0.796772\n",
      "Evaluating Epoch 26  77.9% | batch:       134 of       172\t|\tloss: 0.61575\n",
      "Evaluating Epoch 26  78.5% | batch:       135 of       172\t|\tloss: 0.574849\n",
      "Evaluating Epoch 26  79.1% | batch:       136 of       172\t|\tloss: 0.554389\n",
      "Evaluating Epoch 26  79.7% | batch:       137 of       172\t|\tloss: 0.549228\n",
      "Evaluating Epoch 26  80.2% | batch:       138 of       172\t|\tloss: 0.474423\n",
      "Evaluating Epoch 26  80.8% | batch:       139 of       172\t|\tloss: 0.808356\n",
      "Evaluating Epoch 26  81.4% | batch:       140 of       172\t|\tloss: 0.493939\n",
      "Evaluating Epoch 26  82.0% | batch:       141 of       172\t|\tloss: 0.428712\n",
      "Evaluating Epoch 26  82.6% | batch:       142 of       172\t|\tloss: 0.404365\n",
      "Evaluating Epoch 26  83.1% | batch:       143 of       172\t|\tloss: 0.586171\n",
      "Evaluating Epoch 26  83.7% | batch:       144 of       172\t|\tloss: 0.538923\n",
      "Evaluating Epoch 26  84.3% | batch:       145 of       172\t|\tloss: 0.706838\n",
      "Evaluating Epoch 26  84.9% | batch:       146 of       172\t|\tloss: 0.525896\n",
      "Evaluating Epoch 26  85.5% | batch:       147 of       172\t|\tloss: 0.672971\n",
      "Evaluating Epoch 26  86.0% | batch:       148 of       172\t|\tloss: 0.46703\n",
      "Evaluating Epoch 26  86.6% | batch:       149 of       172\t|\tloss: 0.628144\n",
      "Evaluating Epoch 26  87.2% | batch:       150 of       172\t|\tloss: 0.170238\n",
      "Evaluating Epoch 26  87.8% | batch:       151 of       172\t|\tloss: 0.405939\n",
      "Evaluating Epoch 26  88.4% | batch:       152 of       172\t|\tloss: 0.454445\n",
      "Evaluating Epoch 26  89.0% | batch:       153 of       172\t|\tloss: 0.214993\n",
      "Evaluating Epoch 26  89.5% | batch:       154 of       172\t|\tloss: 0.386978\n",
      "Evaluating Epoch 26  90.1% | batch:       155 of       172\t|\tloss: 0.468967\n",
      "Evaluating Epoch 26  90.7% | batch:       156 of       172\t|\tloss: 0.321429\n",
      "Evaluating Epoch 26  91.3% | batch:       157 of       172\t|\tloss: 0.433638\n",
      "Evaluating Epoch 26  91.9% | batch:       158 of       172\t|\tloss: 0.201693\n",
      "Evaluating Epoch 26  92.4% | batch:       159 of       172\t|\tloss: 0.332554\n",
      "Evaluating Epoch 26  93.0% | batch:       160 of       172\t|\tloss: 0.949588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:11:58,444 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.442622423171997 seconds\n",
      "\n",
      "2023-05-25 22:11:58,445 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.007846293626009 seconds\n",
      "2023-05-25 22:11:58,445 | INFO : Avg batch val. time: 0.023301431939686097 seconds\n",
      "2023-05-25 22:11:58,446 | INFO : Avg sample val. time: 0.00018253159783331095 seconds\n",
      "2023-05-25 22:11:58,446 | INFO : Epoch 26 Validation Summary: epoch: 26.000000 | loss: 1.396918 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 26  93.6% | batch:       161 of       172\t|\tloss: 0.191114\n",
      "Evaluating Epoch 26  94.2% | batch:       162 of       172\t|\tloss: 0.424181\n",
      "Evaluating Epoch 26  94.8% | batch:       163 of       172\t|\tloss: 0.413656\n",
      "Evaluating Epoch 26  95.3% | batch:       164 of       172\t|\tloss: 0.278014\n",
      "Evaluating Epoch 26  95.9% | batch:       165 of       172\t|\tloss: 0.476305\n",
      "Evaluating Epoch 26  96.5% | batch:       166 of       172\t|\tloss: 0.235654\n",
      "Evaluating Epoch 26  97.1% | batch:       167 of       172\t|\tloss: 0.334348\n",
      "Evaluating Epoch 26  97.7% | batch:       168 of       172\t|\tloss: 0.410198\n",
      "Evaluating Epoch 26  98.3% | batch:       169 of       172\t|\tloss: 0.123998\n",
      "Evaluating Epoch 26  98.8% | batch:       170 of       172\t|\tloss: 0.327316\n",
      "Evaluating Epoch 26  99.4% | batch:       171 of       172\t|\tloss: 0.434803\n",
      "\n",
      "Training Epoch 27   0.0% | batch:         0 of       686\t|\tloss: 0.742721\n",
      "Training Epoch 27   0.1% | batch:         1 of       686\t|\tloss: 1.19534\n",
      "Training Epoch 27   0.3% | batch:         2 of       686\t|\tloss: 0.913412\n",
      "Training Epoch 27   0.4% | batch:         3 of       686\t|\tloss: 0.867056\n",
      "Training Epoch 27   0.6% | batch:         4 of       686\t|\tloss: 0.688712\n",
      "Training Epoch 27   0.7% | batch:         5 of       686\t|\tloss: 0.827931\n",
      "Training Epoch 27   0.9% | batch:         6 of       686\t|\tloss: 0.855317\n",
      "Training Epoch 27   1.0% | batch:         7 of       686\t|\tloss: 0.700508\n",
      "Training Epoch 27   1.2% | batch:         8 of       686\t|\tloss: 0.887384\n",
      "Training Epoch 27   1.3% | batch:         9 of       686\t|\tloss: 0.828531\n",
      "Training Epoch 27   1.5% | batch:        10 of       686\t|\tloss: 1.05591\n",
      "Training Epoch 27   1.6% | batch:        11 of       686\t|\tloss: 0.912563\n",
      "Training Epoch 27   1.7% | batch:        12 of       686\t|\tloss: 0.944096\n",
      "Training Epoch 27   1.9% | batch:        13 of       686\t|\tloss: 0.853606\n",
      "Training Epoch 27   2.0% | batch:        14 of       686\t|\tloss: 1.19767\n",
      "Training Epoch 27   2.2% | batch:        15 of       686\t|\tloss: 0.785915\n",
      "Training Epoch 27   2.3% | batch:        16 of       686\t|\tloss: 0.823563\n",
      "Training Epoch 27   2.5% | batch:        17 of       686\t|\tloss: 0.90042\n",
      "Training Epoch 27   2.6% | batch:        18 of       686\t|\tloss: 0.97016\n",
      "Training Epoch 27   2.8% | batch:        19 of       686\t|\tloss: 0.923965\n",
      "Training Epoch 27   2.9% | batch:        20 of       686\t|\tloss: 0.809301\n",
      "Training Epoch 27   3.1% | batch:        21 of       686\t|\tloss: 0.751184\n",
      "Training Epoch 27   3.2% | batch:        22 of       686\t|\tloss: 0.799428\n",
      "Training Epoch 27   3.4% | batch:        23 of       686\t|\tloss: 0.667411\n",
      "Training Epoch 27   3.5% | batch:        24 of       686\t|\tloss: 0.927896\n",
      "Training Epoch 27   3.6% | batch:        25 of       686\t|\tloss: 0.738309\n",
      "Training Epoch 27   3.8% | batch:        26 of       686\t|\tloss: 0.699089\n",
      "Training Epoch 27   3.9% | batch:        27 of       686\t|\tloss: 0.598537\n",
      "Training Epoch 27   4.1% | batch:        28 of       686\t|\tloss: 0.763055\n",
      "Training Epoch 27   4.2% | batch:        29 of       686\t|\tloss: 0.867033\n",
      "Training Epoch 27   4.4% | batch:        30 of       686\t|\tloss: 0.882824\n",
      "Training Epoch 27   4.5% | batch:        31 of       686\t|\tloss: 0.962428\n",
      "Training Epoch 27   4.7% | batch:        32 of       686\t|\tloss: 0.683307\n",
      "Training Epoch 27   4.8% | batch:        33 of       686\t|\tloss: 0.721485\n",
      "Training Epoch 27   5.0% | batch:        34 of       686\t|\tloss: 0.703571\n",
      "Training Epoch 27   5.1% | batch:        35 of       686\t|\tloss: 0.806715\n",
      "Training Epoch 27   5.2% | batch:        36 of       686\t|\tloss: 0.953085\n",
      "Training Epoch 27   5.4% | batch:        37 of       686\t|\tloss: 0.987247\n",
      "Training Epoch 27   5.5% | batch:        38 of       686\t|\tloss: 0.707354\n",
      "Training Epoch 27   5.7% | batch:        39 of       686\t|\tloss: 1.03826\n",
      "Training Epoch 27   5.8% | batch:        40 of       686\t|\tloss: 0.784012\n",
      "Training Epoch 27   6.0% | batch:        41 of       686\t|\tloss: 0.993906\n",
      "Training Epoch 27   6.1% | batch:        42 of       686\t|\tloss: 0.573035\n",
      "Training Epoch 27   6.3% | batch:        43 of       686\t|\tloss: 0.895634\n",
      "Training Epoch 27   6.4% | batch:        44 of       686\t|\tloss: 0.797776\n",
      "Training Epoch 27   6.6% | batch:        45 of       686\t|\tloss: 0.637351\n",
      "Training Epoch 27   6.7% | batch:        46 of       686\t|\tloss: 0.891123\n",
      "Training Epoch 27   6.9% | batch:        47 of       686\t|\tloss: 0.650159\n",
      "Training Epoch 27   7.0% | batch:        48 of       686\t|\tloss: 0.979082\n",
      "Training Epoch 27   7.1% | batch:        49 of       686\t|\tloss: 0.946592\n",
      "Training Epoch 27   7.3% | batch:        50 of       686\t|\tloss: 0.784892\n",
      "Training Epoch 27   7.4% | batch:        51 of       686\t|\tloss: 0.867419\n",
      "Training Epoch 27   7.6% | batch:        52 of       686\t|\tloss: 1.07134\n",
      "Training Epoch 27   7.7% | batch:        53 of       686\t|\tloss: 0.877897\n",
      "Training Epoch 27   7.9% | batch:        54 of       686\t|\tloss: 0.625023\n",
      "Training Epoch 27   8.0% | batch:        55 of       686\t|\tloss: 0.877632\n",
      "Training Epoch 27   8.2% | batch:        56 of       686\t|\tloss: 0.881824\n",
      "Training Epoch 27   8.3% | batch:        57 of       686\t|\tloss: 0.720563\n",
      "Training Epoch 27   8.5% | batch:        58 of       686\t|\tloss: 0.790681\n",
      "Training Epoch 27   8.6% | batch:        59 of       686\t|\tloss: 0.755422\n",
      "Training Epoch 27   8.7% | batch:        60 of       686\t|\tloss: 0.869677\n",
      "Training Epoch 27   8.9% | batch:        61 of       686\t|\tloss: 0.698619\n",
      "Training Epoch 27   9.0% | batch:        62 of       686\t|\tloss: 1.03331\n",
      "Training Epoch 27   9.2% | batch:        63 of       686\t|\tloss: 0.986982\n",
      "Training Epoch 27   9.3% | batch:        64 of       686\t|\tloss: 0.93767\n",
      "Training Epoch 27   9.5% | batch:        65 of       686\t|\tloss: 0.812128\n",
      "Training Epoch 27   9.6% | batch:        66 of       686\t|\tloss: 0.660118\n",
      "Training Epoch 27   9.8% | batch:        67 of       686\t|\tloss: 1.02155\n",
      "Training Epoch 27   9.9% | batch:        68 of       686\t|\tloss: 0.866255\n",
      "Training Epoch 27  10.1% | batch:        69 of       686\t|\tloss: 1.11416\n",
      "Training Epoch 27  10.2% | batch:        70 of       686\t|\tloss: 0.71417\n",
      "Training Epoch 27  10.3% | batch:        71 of       686\t|\tloss: 0.785532\n",
      "Training Epoch 27  10.5% | batch:        72 of       686\t|\tloss: 0.803183\n",
      "Training Epoch 27  10.6% | batch:        73 of       686\t|\tloss: 0.83185\n",
      "Training Epoch 27  10.8% | batch:        74 of       686\t|\tloss: 0.877875\n",
      "Training Epoch 27  10.9% | batch:        75 of       686\t|\tloss: 0.747371\n",
      "Training Epoch 27  11.1% | batch:        76 of       686\t|\tloss: 0.644615\n",
      "Training Epoch 27  11.2% | batch:        77 of       686\t|\tloss: 0.731619\n",
      "Training Epoch 27  11.4% | batch:        78 of       686\t|\tloss: 0.88997\n",
      "Training Epoch 27  11.5% | batch:        79 of       686\t|\tloss: 0.878926\n",
      "Training Epoch 27  11.7% | batch:        80 of       686\t|\tloss: 0.770161\n",
      "Training Epoch 27  11.8% | batch:        81 of       686\t|\tloss: 0.7108\n",
      "Training Epoch 27  12.0% | batch:        82 of       686\t|\tloss: 0.744338\n",
      "Training Epoch 27  12.1% | batch:        83 of       686\t|\tloss: 0.73065\n",
      "Training Epoch 27  12.2% | batch:        84 of       686\t|\tloss: 0.612522\n",
      "Training Epoch 27  12.4% | batch:        85 of       686\t|\tloss: 0.565867\n",
      "Training Epoch 27  12.5% | batch:        86 of       686\t|\tloss: 0.731566\n",
      "Training Epoch 27  12.7% | batch:        87 of       686\t|\tloss: 0.823987\n",
      "Training Epoch 27  12.8% | batch:        88 of       686\t|\tloss: 0.781595\n",
      "Training Epoch 27  13.0% | batch:        89 of       686\t|\tloss: 0.701505\n",
      "Training Epoch 27  13.1% | batch:        90 of       686\t|\tloss: 0.883981\n",
      "Training Epoch 27  13.3% | batch:        91 of       686\t|\tloss: 1.11311\n",
      "Training Epoch 27  13.4% | batch:        92 of       686\t|\tloss: 0.747996\n",
      "Training Epoch 27  13.6% | batch:        93 of       686\t|\tloss: 0.995177\n",
      "Training Epoch 27  13.7% | batch:        94 of       686\t|\tloss: 0.580318\n",
      "Training Epoch 27  13.8% | batch:        95 of       686\t|\tloss: 0.882264\n",
      "Training Epoch 27  14.0% | batch:        96 of       686\t|\tloss: 0.584413\n",
      "Training Epoch 27  14.1% | batch:        97 of       686\t|\tloss: 0.716233\n",
      "Training Epoch 27  14.3% | batch:        98 of       686\t|\tloss: 0.842009\n",
      "Training Epoch 27  14.4% | batch:        99 of       686\t|\tloss: 0.75073\n",
      "Training Epoch 27  14.6% | batch:       100 of       686\t|\tloss: 0.911202\n",
      "Training Epoch 27  14.7% | batch:       101 of       686\t|\tloss: 0.940758\n",
      "Training Epoch 27  14.9% | batch:       102 of       686\t|\tloss: 0.872244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 27  15.0% | batch:       103 of       686\t|\tloss: 0.859212\n",
      "Training Epoch 27  15.2% | batch:       104 of       686\t|\tloss: 0.863236\n",
      "Training Epoch 27  15.3% | batch:       105 of       686\t|\tloss: 0.977309\n",
      "Training Epoch 27  15.5% | batch:       106 of       686\t|\tloss: 0.76957\n",
      "Training Epoch 27  15.6% | batch:       107 of       686\t|\tloss: 0.718813\n",
      "Training Epoch 27  15.7% | batch:       108 of       686\t|\tloss: 1.12046\n",
      "Training Epoch 27  15.9% | batch:       109 of       686\t|\tloss: 0.676293\n",
      "Training Epoch 27  16.0% | batch:       110 of       686\t|\tloss: 0.717442\n",
      "Training Epoch 27  16.2% | batch:       111 of       686\t|\tloss: 0.699593\n",
      "Training Epoch 27  16.3% | batch:       112 of       686\t|\tloss: 0.94327\n",
      "Training Epoch 27  16.5% | batch:       113 of       686\t|\tloss: 0.689302\n",
      "Training Epoch 27  16.6% | batch:       114 of       686\t|\tloss: 0.72094\n",
      "Training Epoch 27  16.8% | batch:       115 of       686\t|\tloss: 0.825684\n",
      "Training Epoch 27  16.9% | batch:       116 of       686\t|\tloss: 0.863178\n",
      "Training Epoch 27  17.1% | batch:       117 of       686\t|\tloss: 0.67264\n",
      "Training Epoch 27  17.2% | batch:       118 of       686\t|\tloss: 1.11339\n",
      "Training Epoch 27  17.3% | batch:       119 of       686\t|\tloss: 1.05374\n",
      "Training Epoch 27  17.5% | batch:       120 of       686\t|\tloss: 0.626208\n",
      "Training Epoch 27  17.6% | batch:       121 of       686\t|\tloss: 0.730291\n",
      "Training Epoch 27  17.8% | batch:       122 of       686\t|\tloss: 0.801232\n",
      "Training Epoch 27  17.9% | batch:       123 of       686\t|\tloss: 0.735632\n",
      "Training Epoch 27  18.1% | batch:       124 of       686\t|\tloss: 0.811271\n",
      "Training Epoch 27  18.2% | batch:       125 of       686\t|\tloss: 0.806494\n",
      "Training Epoch 27  18.4% | batch:       126 of       686\t|\tloss: 0.911413\n",
      "Training Epoch 27  18.5% | batch:       127 of       686\t|\tloss: 1.32953\n",
      "Training Epoch 27  18.7% | batch:       128 of       686\t|\tloss: 0.824887\n",
      "Training Epoch 27  18.8% | batch:       129 of       686\t|\tloss: 0.881497\n",
      "Training Epoch 27  19.0% | batch:       130 of       686\t|\tloss: 0.911143\n",
      "Training Epoch 27  19.1% | batch:       131 of       686\t|\tloss: 0.840201\n",
      "Training Epoch 27  19.2% | batch:       132 of       686\t|\tloss: 0.778702\n",
      "Training Epoch 27  19.4% | batch:       133 of       686\t|\tloss: 0.743921\n",
      "Training Epoch 27  19.5% | batch:       134 of       686\t|\tloss: 0.757835\n",
      "Training Epoch 27  19.7% | batch:       135 of       686\t|\tloss: 0.84029\n",
      "Training Epoch 27  19.8% | batch:       136 of       686\t|\tloss: 0.787627\n",
      "Training Epoch 27  20.0% | batch:       137 of       686\t|\tloss: 0.858773\n",
      "Training Epoch 27  20.1% | batch:       138 of       686\t|\tloss: 0.720149\n",
      "Training Epoch 27  20.3% | batch:       139 of       686\t|\tloss: 0.723508\n",
      "Training Epoch 27  20.4% | batch:       140 of       686\t|\tloss: 0.786204\n",
      "Training Epoch 27  20.6% | batch:       141 of       686\t|\tloss: 0.702078\n",
      "Training Epoch 27  20.7% | batch:       142 of       686\t|\tloss: 1.08366\n",
      "Training Epoch 27  20.8% | batch:       143 of       686\t|\tloss: 1.00928\n",
      "Training Epoch 27  21.0% | batch:       144 of       686\t|\tloss: 1.00019\n",
      "Training Epoch 27  21.1% | batch:       145 of       686\t|\tloss: 0.826519\n",
      "Training Epoch 27  21.3% | batch:       146 of       686\t|\tloss: 0.735786\n",
      "Training Epoch 27  21.4% | batch:       147 of       686\t|\tloss: 0.684595\n",
      "Training Epoch 27  21.6% | batch:       148 of       686\t|\tloss: 0.896422\n",
      "Training Epoch 27  21.7% | batch:       149 of       686\t|\tloss: 0.772351\n",
      "Training Epoch 27  21.9% | batch:       150 of       686\t|\tloss: 0.64121\n",
      "Training Epoch 27  22.0% | batch:       151 of       686\t|\tloss: 0.814838\n",
      "Training Epoch 27  22.2% | batch:       152 of       686\t|\tloss: 0.750798\n",
      "Training Epoch 27  22.3% | batch:       153 of       686\t|\tloss: 0.680737\n",
      "Training Epoch 27  22.4% | batch:       154 of       686\t|\tloss: 0.809976\n",
      "Training Epoch 27  22.6% | batch:       155 of       686\t|\tloss: 0.629074\n",
      "Training Epoch 27  22.7% | batch:       156 of       686\t|\tloss: 0.677489\n",
      "Training Epoch 27  22.9% | batch:       157 of       686\t|\tloss: 0.933643\n",
      "Training Epoch 27  23.0% | batch:       158 of       686\t|\tloss: 0.707194\n",
      "Training Epoch 27  23.2% | batch:       159 of       686\t|\tloss: 0.789977\n",
      "Training Epoch 27  23.3% | batch:       160 of       686\t|\tloss: 0.733657\n",
      "Training Epoch 27  23.5% | batch:       161 of       686\t|\tloss: 0.965469\n",
      "Training Epoch 27  23.6% | batch:       162 of       686\t|\tloss: 0.693147\n",
      "Training Epoch 27  23.8% | batch:       163 of       686\t|\tloss: 0.785335\n",
      "Training Epoch 27  23.9% | batch:       164 of       686\t|\tloss: 1.0177\n",
      "Training Epoch 27  24.1% | batch:       165 of       686\t|\tloss: 0.84401\n",
      "Training Epoch 27  24.2% | batch:       166 of       686\t|\tloss: 0.842423\n",
      "Training Epoch 27  24.3% | batch:       167 of       686\t|\tloss: 0.780842\n",
      "Training Epoch 27  24.5% | batch:       168 of       686\t|\tloss: 0.835668\n",
      "Training Epoch 27  24.6% | batch:       169 of       686\t|\tloss: 0.654171\n",
      "Training Epoch 27  24.8% | batch:       170 of       686\t|\tloss: 0.915176\n",
      "Training Epoch 27  24.9% | batch:       171 of       686\t|\tloss: 0.828813\n",
      "Training Epoch 27  25.1% | batch:       172 of       686\t|\tloss: 0.829764\n",
      "Training Epoch 27  25.2% | batch:       173 of       686\t|\tloss: 0.739137\n",
      "Training Epoch 27  25.4% | batch:       174 of       686\t|\tloss: 0.910515\n",
      "Training Epoch 27  25.5% | batch:       175 of       686\t|\tloss: 1.41812\n",
      "Training Epoch 27  25.7% | batch:       176 of       686\t|\tloss: 0.793691\n",
      "Training Epoch 27  25.8% | batch:       177 of       686\t|\tloss: 0.883719\n",
      "Training Epoch 27  25.9% | batch:       178 of       686\t|\tloss: 0.995231\n",
      "Training Epoch 27  26.1% | batch:       179 of       686\t|\tloss: 1.02819\n",
      "Training Epoch 27  26.2% | batch:       180 of       686\t|\tloss: 0.773866\n",
      "Training Epoch 27  26.4% | batch:       181 of       686\t|\tloss: 0.741763\n",
      "Training Epoch 27  26.5% | batch:       182 of       686\t|\tloss: 0.836193\n",
      "Training Epoch 27  26.7% | batch:       183 of       686\t|\tloss: 0.734657\n",
      "Training Epoch 27  26.8% | batch:       184 of       686\t|\tloss: 0.975763\n",
      "Training Epoch 27  27.0% | batch:       185 of       686\t|\tloss: 0.768695\n",
      "Training Epoch 27  27.1% | batch:       186 of       686\t|\tloss: 0.789274\n",
      "Training Epoch 27  27.3% | batch:       187 of       686\t|\tloss: 0.795674\n",
      "Training Epoch 27  27.4% | batch:       188 of       686\t|\tloss: 0.899988\n",
      "Training Epoch 27  27.6% | batch:       189 of       686\t|\tloss: 0.8483\n",
      "Training Epoch 27  27.7% | batch:       190 of       686\t|\tloss: 0.770909\n",
      "Training Epoch 27  27.8% | batch:       191 of       686\t|\tloss: 0.682812\n",
      "Training Epoch 27  28.0% | batch:       192 of       686\t|\tloss: 0.791583\n",
      "Training Epoch 27  28.1% | batch:       193 of       686\t|\tloss: 0.821335\n",
      "Training Epoch 27  28.3% | batch:       194 of       686\t|\tloss: 0.719851\n",
      "Training Epoch 27  28.4% | batch:       195 of       686\t|\tloss: 0.702956\n",
      "Training Epoch 27  28.6% | batch:       196 of       686\t|\tloss: 0.776446\n",
      "Training Epoch 27  28.7% | batch:       197 of       686\t|\tloss: 0.739453\n",
      "Training Epoch 27  28.9% | batch:       198 of       686\t|\tloss: 0.793124\n",
      "Training Epoch 27  29.0% | batch:       199 of       686\t|\tloss: 0.721824\n",
      "Training Epoch 27  29.2% | batch:       200 of       686\t|\tloss: 0.857535\n",
      "Training Epoch 27  29.3% | batch:       201 of       686\t|\tloss: 0.790847\n",
      "Training Epoch 27  29.4% | batch:       202 of       686\t|\tloss: 0.550995\n",
      "Training Epoch 27  29.6% | batch:       203 of       686\t|\tloss: 0.907705\n",
      "Training Epoch 27  29.7% | batch:       204 of       686\t|\tloss: 0.64843\n",
      "Training Epoch 27  29.9% | batch:       205 of       686\t|\tloss: 0.803337\n",
      "Training Epoch 27  30.0% | batch:       206 of       686\t|\tloss: 0.740097\n",
      "Training Epoch 27  30.2% | batch:       207 of       686\t|\tloss: 0.860675\n",
      "Training Epoch 27  30.3% | batch:       208 of       686\t|\tloss: 0.73342\n",
      "Training Epoch 27  30.5% | batch:       209 of       686\t|\tloss: 0.819905\n",
      "Training Epoch 27  30.6% | batch:       210 of       686\t|\tloss: 0.66309\n",
      "Training Epoch 27  30.8% | batch:       211 of       686\t|\tloss: 0.784938\n",
      "Training Epoch 27  30.9% | batch:       212 of       686\t|\tloss: 0.790842\n",
      "Training Epoch 27  31.0% | batch:       213 of       686\t|\tloss: 0.706632\n",
      "Training Epoch 27  31.2% | batch:       214 of       686\t|\tloss: 0.737755\n",
      "Training Epoch 27  31.3% | batch:       215 of       686\t|\tloss: 0.92759\n",
      "Training Epoch 27  31.5% | batch:       216 of       686\t|\tloss: 0.652796\n",
      "Training Epoch 27  31.6% | batch:       217 of       686\t|\tloss: 1.04519\n",
      "Training Epoch 27  31.8% | batch:       218 of       686\t|\tloss: 0.882317\n",
      "Training Epoch 27  31.9% | batch:       219 of       686\t|\tloss: 1.046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 27  32.1% | batch:       220 of       686\t|\tloss: 0.785274\n",
      "Training Epoch 27  32.2% | batch:       221 of       686\t|\tloss: 0.736562\n",
      "Training Epoch 27  32.4% | batch:       222 of       686\t|\tloss: 0.692167\n",
      "Training Epoch 27  32.5% | batch:       223 of       686\t|\tloss: 0.751454\n",
      "Training Epoch 27  32.7% | batch:       224 of       686\t|\tloss: 0.801989\n",
      "Training Epoch 27  32.8% | batch:       225 of       686\t|\tloss: 0.679543\n",
      "Training Epoch 27  32.9% | batch:       226 of       686\t|\tloss: 0.919474\n",
      "Training Epoch 27  33.1% | batch:       227 of       686\t|\tloss: 0.747819\n",
      "Training Epoch 27  33.2% | batch:       228 of       686\t|\tloss: 0.875251\n",
      "Training Epoch 27  33.4% | batch:       229 of       686\t|\tloss: 0.820661\n",
      "Training Epoch 27  33.5% | batch:       230 of       686\t|\tloss: 0.719868\n",
      "Training Epoch 27  33.7% | batch:       231 of       686\t|\tloss: 0.734038\n",
      "Training Epoch 27  33.8% | batch:       232 of       686\t|\tloss: 0.849954\n",
      "Training Epoch 27  34.0% | batch:       233 of       686\t|\tloss: 0.844845\n",
      "Training Epoch 27  34.1% | batch:       234 of       686\t|\tloss: 0.82331\n",
      "Training Epoch 27  34.3% | batch:       235 of       686\t|\tloss: 0.853489\n",
      "Training Epoch 27  34.4% | batch:       236 of       686\t|\tloss: 0.886899\n",
      "Training Epoch 27  34.5% | batch:       237 of       686\t|\tloss: 0.71624\n",
      "Training Epoch 27  34.7% | batch:       238 of       686\t|\tloss: 0.691024\n",
      "Training Epoch 27  34.8% | batch:       239 of       686\t|\tloss: 0.788817\n",
      "Training Epoch 27  35.0% | batch:       240 of       686\t|\tloss: 0.836388\n",
      "Training Epoch 27  35.1% | batch:       241 of       686\t|\tloss: 0.753303\n",
      "Training Epoch 27  35.3% | batch:       242 of       686\t|\tloss: 0.62073\n",
      "Training Epoch 27  35.4% | batch:       243 of       686\t|\tloss: 0.909022\n",
      "Training Epoch 27  35.6% | batch:       244 of       686\t|\tloss: 0.783536\n",
      "Training Epoch 27  35.7% | batch:       245 of       686\t|\tloss: 0.749096\n",
      "Training Epoch 27  35.9% | batch:       246 of       686\t|\tloss: 0.696058\n",
      "Training Epoch 27  36.0% | batch:       247 of       686\t|\tloss: 1.04031\n",
      "Training Epoch 27  36.2% | batch:       248 of       686\t|\tloss: 0.656848\n",
      "Training Epoch 27  36.3% | batch:       249 of       686\t|\tloss: 1.1114\n",
      "Training Epoch 27  36.4% | batch:       250 of       686\t|\tloss: 0.706011\n",
      "Training Epoch 27  36.6% | batch:       251 of       686\t|\tloss: 0.686735\n",
      "Training Epoch 27  36.7% | batch:       252 of       686\t|\tloss: 0.988679\n",
      "Training Epoch 27  36.9% | batch:       253 of       686\t|\tloss: 0.739767\n",
      "Training Epoch 27  37.0% | batch:       254 of       686\t|\tloss: 0.923262\n",
      "Training Epoch 27  37.2% | batch:       255 of       686\t|\tloss: 0.738711\n",
      "Training Epoch 27  37.3% | batch:       256 of       686\t|\tloss: 0.863601\n",
      "Training Epoch 27  37.5% | batch:       257 of       686\t|\tloss: 0.789384\n",
      "Training Epoch 27  37.6% | batch:       258 of       686\t|\tloss: 0.784583\n",
      "Training Epoch 27  37.8% | batch:       259 of       686\t|\tloss: 0.766881\n",
      "Training Epoch 27  37.9% | batch:       260 of       686\t|\tloss: 0.7351\n",
      "Training Epoch 27  38.0% | batch:       261 of       686\t|\tloss: 0.567328\n",
      "Training Epoch 27  38.2% | batch:       262 of       686\t|\tloss: 0.825247\n",
      "Training Epoch 27  38.3% | batch:       263 of       686\t|\tloss: 0.973961\n",
      "Training Epoch 27  38.5% | batch:       264 of       686\t|\tloss: 1.0205\n",
      "Training Epoch 27  38.6% | batch:       265 of       686\t|\tloss: 0.849708\n",
      "Training Epoch 27  38.8% | batch:       266 of       686\t|\tloss: 0.713544\n",
      "Training Epoch 27  38.9% | batch:       267 of       686\t|\tloss: 0.852929\n",
      "Training Epoch 27  39.1% | batch:       268 of       686\t|\tloss: 0.728656\n",
      "Training Epoch 27  39.2% | batch:       269 of       686\t|\tloss: 0.951229\n",
      "Training Epoch 27  39.4% | batch:       270 of       686\t|\tloss: 1.10887\n",
      "Training Epoch 27  39.5% | batch:       271 of       686\t|\tloss: 0.705449\n",
      "Training Epoch 27  39.7% | batch:       272 of       686\t|\tloss: 0.617391\n",
      "Training Epoch 27  39.8% | batch:       273 of       686\t|\tloss: 0.861433\n",
      "Training Epoch 27  39.9% | batch:       274 of       686\t|\tloss: 0.810861\n",
      "Training Epoch 27  40.1% | batch:       275 of       686\t|\tloss: 0.931454\n",
      "Training Epoch 27  40.2% | batch:       276 of       686\t|\tloss: 0.727216\n",
      "Training Epoch 27  40.4% | batch:       277 of       686\t|\tloss: 0.784739\n",
      "Training Epoch 27  40.5% | batch:       278 of       686\t|\tloss: 0.883891\n",
      "Training Epoch 27  40.7% | batch:       279 of       686\t|\tloss: 0.929622\n",
      "Training Epoch 27  40.8% | batch:       280 of       686\t|\tloss: 0.994323\n",
      "Training Epoch 27  41.0% | batch:       281 of       686\t|\tloss: 0.613371\n",
      "Training Epoch 27  41.1% | batch:       282 of       686\t|\tloss: 0.710685\n",
      "Training Epoch 27  41.3% | batch:       283 of       686\t|\tloss: 0.560674\n",
      "Training Epoch 27  41.4% | batch:       284 of       686\t|\tloss: 0.680654\n",
      "Training Epoch 27  41.5% | batch:       285 of       686\t|\tloss: 0.888096\n",
      "Training Epoch 27  41.7% | batch:       286 of       686\t|\tloss: 0.701668\n",
      "Training Epoch 27  41.8% | batch:       287 of       686\t|\tloss: 0.96273\n",
      "Training Epoch 27  42.0% | batch:       288 of       686\t|\tloss: 0.733352\n",
      "Training Epoch 27  42.1% | batch:       289 of       686\t|\tloss: 0.635657\n",
      "Training Epoch 27  42.3% | batch:       290 of       686\t|\tloss: 0.787089\n",
      "Training Epoch 27  42.4% | batch:       291 of       686\t|\tloss: 0.74216\n",
      "Training Epoch 27  42.6% | batch:       292 of       686\t|\tloss: 0.689265\n",
      "Training Epoch 27  42.7% | batch:       293 of       686\t|\tloss: 0.816048\n",
      "Training Epoch 27  42.9% | batch:       294 of       686\t|\tloss: 0.797247\n",
      "Training Epoch 27  43.0% | batch:       295 of       686\t|\tloss: 1.15909\n",
      "Training Epoch 27  43.1% | batch:       296 of       686\t|\tloss: 1.05158\n",
      "Training Epoch 27  43.3% | batch:       297 of       686\t|\tloss: 1.00831\n",
      "Training Epoch 27  43.4% | batch:       298 of       686\t|\tloss: 0.815835\n",
      "Training Epoch 27  43.6% | batch:       299 of       686\t|\tloss: 0.986926\n",
      "Training Epoch 27  43.7% | batch:       300 of       686\t|\tloss: 0.829906\n",
      "Training Epoch 27  43.9% | batch:       301 of       686\t|\tloss: 0.842597\n",
      "Training Epoch 27  44.0% | batch:       302 of       686\t|\tloss: 0.781749\n",
      "Training Epoch 27  44.2% | batch:       303 of       686\t|\tloss: 0.591468\n",
      "Training Epoch 27  44.3% | batch:       304 of       686\t|\tloss: 0.81237\n",
      "Training Epoch 27  44.5% | batch:       305 of       686\t|\tloss: 0.836198\n",
      "Training Epoch 27  44.6% | batch:       306 of       686\t|\tloss: 0.699519\n",
      "Training Epoch 27  44.8% | batch:       307 of       686\t|\tloss: 0.749178\n",
      "Training Epoch 27  44.9% | batch:       308 of       686\t|\tloss: 0.76058\n",
      "Training Epoch 27  45.0% | batch:       309 of       686\t|\tloss: 0.850689\n",
      "Training Epoch 27  45.2% | batch:       310 of       686\t|\tloss: 1.22125\n",
      "Training Epoch 27  45.3% | batch:       311 of       686\t|\tloss: 0.965412\n",
      "Training Epoch 27  45.5% | batch:       312 of       686\t|\tloss: 0.718572\n",
      "Training Epoch 27  45.6% | batch:       313 of       686\t|\tloss: 0.615139\n",
      "Training Epoch 27  45.8% | batch:       314 of       686\t|\tloss: 0.687455\n",
      "Training Epoch 27  45.9% | batch:       315 of       686\t|\tloss: 0.728558\n",
      "Training Epoch 27  46.1% | batch:       316 of       686\t|\tloss: 0.915486\n",
      "Training Epoch 27  46.2% | batch:       317 of       686\t|\tloss: 0.745177\n",
      "Training Epoch 27  46.4% | batch:       318 of       686\t|\tloss: 0.806518\n",
      "Training Epoch 27  46.5% | batch:       319 of       686\t|\tloss: 1.05738\n",
      "Training Epoch 27  46.6% | batch:       320 of       686\t|\tloss: 0.967816\n",
      "Training Epoch 27  46.8% | batch:       321 of       686\t|\tloss: 0.820848\n",
      "Training Epoch 27  46.9% | batch:       322 of       686\t|\tloss: 0.694097\n",
      "Training Epoch 27  47.1% | batch:       323 of       686\t|\tloss: 0.76\n",
      "Training Epoch 27  47.2% | batch:       324 of       686\t|\tloss: 0.765423\n",
      "Training Epoch 27  47.4% | batch:       325 of       686\t|\tloss: 0.883747\n",
      "Training Epoch 27  47.5% | batch:       326 of       686\t|\tloss: 0.978985\n",
      "Training Epoch 27  47.7% | batch:       327 of       686\t|\tloss: 0.698153\n",
      "Training Epoch 27  47.8% | batch:       328 of       686\t|\tloss: 0.906764\n",
      "Training Epoch 27  48.0% | batch:       329 of       686\t|\tloss: 0.787487\n",
      "Training Epoch 27  48.1% | batch:       330 of       686\t|\tloss: 0.93178\n",
      "Training Epoch 27  48.3% | batch:       331 of       686\t|\tloss: 0.678744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 27  48.4% | batch:       332 of       686\t|\tloss: 0.970396\n",
      "Training Epoch 27  48.5% | batch:       333 of       686\t|\tloss: 0.640934\n",
      "Training Epoch 27  48.7% | batch:       334 of       686\t|\tloss: 0.624324\n",
      "Training Epoch 27  48.8% | batch:       335 of       686\t|\tloss: 0.690651\n",
      "Training Epoch 27  49.0% | batch:       336 of       686\t|\tloss: 0.801907\n",
      "Training Epoch 27  49.1% | batch:       337 of       686\t|\tloss: 1.12168\n",
      "Training Epoch 27  49.3% | batch:       338 of       686\t|\tloss: 0.731731\n",
      "Training Epoch 27  49.4% | batch:       339 of       686\t|\tloss: 0.68567\n",
      "Training Epoch 27  49.6% | batch:       340 of       686\t|\tloss: 0.700559\n",
      "Training Epoch 27  49.7% | batch:       341 of       686\t|\tloss: 0.80491\n",
      "Training Epoch 27  49.9% | batch:       342 of       686\t|\tloss: 0.885846\n",
      "Training Epoch 27  50.0% | batch:       343 of       686\t|\tloss: 0.859585\n",
      "Training Epoch 27  50.1% | batch:       344 of       686\t|\tloss: 0.72558\n",
      "Training Epoch 27  50.3% | batch:       345 of       686\t|\tloss: 0.820071\n",
      "Training Epoch 27  50.4% | batch:       346 of       686\t|\tloss: 0.904746\n",
      "Training Epoch 27  50.6% | batch:       347 of       686\t|\tloss: 0.700019\n",
      "Training Epoch 27  50.7% | batch:       348 of       686\t|\tloss: 0.754476\n",
      "Training Epoch 27  50.9% | batch:       349 of       686\t|\tloss: 0.839209\n",
      "Training Epoch 27  51.0% | batch:       350 of       686\t|\tloss: 0.715984\n",
      "Training Epoch 27  51.2% | batch:       351 of       686\t|\tloss: 0.734615\n",
      "Training Epoch 27  51.3% | batch:       352 of       686\t|\tloss: 0.643318\n",
      "Training Epoch 27  51.5% | batch:       353 of       686\t|\tloss: 0.500658\n",
      "Training Epoch 27  51.6% | batch:       354 of       686\t|\tloss: 0.831534\n",
      "Training Epoch 27  51.7% | batch:       355 of       686\t|\tloss: 0.848263\n",
      "Training Epoch 27  51.9% | batch:       356 of       686\t|\tloss: 0.957425\n",
      "Training Epoch 27  52.0% | batch:       357 of       686\t|\tloss: 0.795218\n",
      "Training Epoch 27  52.2% | batch:       358 of       686\t|\tloss: 0.822624\n",
      "Training Epoch 27  52.3% | batch:       359 of       686\t|\tloss: 0.708983\n",
      "Training Epoch 27  52.5% | batch:       360 of       686\t|\tloss: 0.607499\n",
      "Training Epoch 27  52.6% | batch:       361 of       686\t|\tloss: 0.761154\n",
      "Training Epoch 27  52.8% | batch:       362 of       686\t|\tloss: 0.632302\n",
      "Training Epoch 27  52.9% | batch:       363 of       686\t|\tloss: 0.973069\n",
      "Training Epoch 27  53.1% | batch:       364 of       686\t|\tloss: 0.742813\n",
      "Training Epoch 27  53.2% | batch:       365 of       686\t|\tloss: 0.743341\n",
      "Training Epoch 27  53.4% | batch:       366 of       686\t|\tloss: 0.655788\n",
      "Training Epoch 27  53.5% | batch:       367 of       686\t|\tloss: 0.809209\n",
      "Training Epoch 27  53.6% | batch:       368 of       686\t|\tloss: 0.791997\n",
      "Training Epoch 27  53.8% | batch:       369 of       686\t|\tloss: 0.598275\n",
      "Training Epoch 27  53.9% | batch:       370 of       686\t|\tloss: 0.800364\n",
      "Training Epoch 27  54.1% | batch:       371 of       686\t|\tloss: 0.662172\n",
      "Training Epoch 27  54.2% | batch:       372 of       686\t|\tloss: 0.509583\n",
      "Training Epoch 27  54.4% | batch:       373 of       686\t|\tloss: 0.81216\n",
      "Training Epoch 27  54.5% | batch:       374 of       686\t|\tloss: 0.638809\n",
      "Training Epoch 27  54.7% | batch:       375 of       686\t|\tloss: 0.871185\n",
      "Training Epoch 27  54.8% | batch:       376 of       686\t|\tloss: 0.710048\n",
      "Training Epoch 27  55.0% | batch:       377 of       686\t|\tloss: 0.852514\n",
      "Training Epoch 27  55.1% | batch:       378 of       686\t|\tloss: 0.967624\n",
      "Training Epoch 27  55.2% | batch:       379 of       686\t|\tloss: 0.672881\n",
      "Training Epoch 27  55.4% | batch:       380 of       686\t|\tloss: 0.994758\n",
      "Training Epoch 27  55.5% | batch:       381 of       686\t|\tloss: 0.667373\n",
      "Training Epoch 27  55.7% | batch:       382 of       686\t|\tloss: 0.562711\n",
      "Training Epoch 27  55.8% | batch:       383 of       686\t|\tloss: 0.763079\n",
      "Training Epoch 27  56.0% | batch:       384 of       686\t|\tloss: 0.745739\n",
      "Training Epoch 27  56.1% | batch:       385 of       686\t|\tloss: 0.675519\n",
      "Training Epoch 27  56.3% | batch:       386 of       686\t|\tloss: 0.649368\n",
      "Training Epoch 27  56.4% | batch:       387 of       686\t|\tloss: 0.879717\n",
      "Training Epoch 27  56.6% | batch:       388 of       686\t|\tloss: 0.696446\n",
      "Training Epoch 27  56.7% | batch:       389 of       686\t|\tloss: 0.845319\n",
      "Training Epoch 27  56.9% | batch:       390 of       686\t|\tloss: 0.704304\n",
      "Training Epoch 27  57.0% | batch:       391 of       686\t|\tloss: 0.779912\n",
      "Training Epoch 27  57.1% | batch:       392 of       686\t|\tloss: 0.79276\n",
      "Training Epoch 27  57.3% | batch:       393 of       686\t|\tloss: 0.770002\n",
      "Training Epoch 27  57.4% | batch:       394 of       686\t|\tloss: 0.779907\n",
      "Training Epoch 27  57.6% | batch:       395 of       686\t|\tloss: 0.767538\n",
      "Training Epoch 27  57.7% | batch:       396 of       686\t|\tloss: 0.669933\n",
      "Training Epoch 27  57.9% | batch:       397 of       686\t|\tloss: 0.787151\n",
      "Training Epoch 27  58.0% | batch:       398 of       686\t|\tloss: 0.802747\n",
      "Training Epoch 27  58.2% | batch:       399 of       686\t|\tloss: 0.944716\n",
      "Training Epoch 27  58.3% | batch:       400 of       686\t|\tloss: 0.776874\n",
      "Training Epoch 27  58.5% | batch:       401 of       686\t|\tloss: 0.837611\n",
      "Training Epoch 27  58.6% | batch:       402 of       686\t|\tloss: 1.08094\n",
      "Training Epoch 27  58.7% | batch:       403 of       686\t|\tloss: 0.99118\n",
      "Training Epoch 27  58.9% | batch:       404 of       686\t|\tloss: 0.790659\n",
      "Training Epoch 27  59.0% | batch:       405 of       686\t|\tloss: 0.841296\n",
      "Training Epoch 27  59.2% | batch:       406 of       686\t|\tloss: 0.601787\n",
      "Training Epoch 27  59.3% | batch:       407 of       686\t|\tloss: 0.997093\n",
      "Training Epoch 27  59.5% | batch:       408 of       686\t|\tloss: 0.885493\n",
      "Training Epoch 27  59.6% | batch:       409 of       686\t|\tloss: 0.66345\n",
      "Training Epoch 27  59.8% | batch:       410 of       686\t|\tloss: 0.622953\n",
      "Training Epoch 27  59.9% | batch:       411 of       686\t|\tloss: 0.661787\n",
      "Training Epoch 27  60.1% | batch:       412 of       686\t|\tloss: 0.728084\n",
      "Training Epoch 27  60.2% | batch:       413 of       686\t|\tloss: 0.764284\n",
      "Training Epoch 27  60.3% | batch:       414 of       686\t|\tloss: 0.726084\n",
      "Training Epoch 27  60.5% | batch:       415 of       686\t|\tloss: 0.805177\n",
      "Training Epoch 27  60.6% | batch:       416 of       686\t|\tloss: 0.738991\n",
      "Training Epoch 27  60.8% | batch:       417 of       686\t|\tloss: 0.887668\n",
      "Training Epoch 27  60.9% | batch:       418 of       686\t|\tloss: 0.763474\n",
      "Training Epoch 27  61.1% | batch:       419 of       686\t|\tloss: 0.841603\n",
      "Training Epoch 27  61.2% | batch:       420 of       686\t|\tloss: 0.676301\n",
      "Training Epoch 27  61.4% | batch:       421 of       686\t|\tloss: 0.900106\n",
      "Training Epoch 27  61.5% | batch:       422 of       686\t|\tloss: 0.867615\n",
      "Training Epoch 27  61.7% | batch:       423 of       686\t|\tloss: 0.928758\n",
      "Training Epoch 27  61.8% | batch:       424 of       686\t|\tloss: 0.69271\n",
      "Training Epoch 27  62.0% | batch:       425 of       686\t|\tloss: 0.717732\n",
      "Training Epoch 27  62.1% | batch:       426 of       686\t|\tloss: 0.844804\n",
      "Training Epoch 27  62.2% | batch:       427 of       686\t|\tloss: 0.959261\n",
      "Training Epoch 27  62.4% | batch:       428 of       686\t|\tloss: 0.79762\n",
      "Training Epoch 27  62.5% | batch:       429 of       686\t|\tloss: 0.939532\n",
      "Training Epoch 27  62.7% | batch:       430 of       686\t|\tloss: 0.846971\n",
      "Training Epoch 27  62.8% | batch:       431 of       686\t|\tloss: 0.751973\n",
      "Training Epoch 27  63.0% | batch:       432 of       686\t|\tloss: 0.745141\n",
      "Training Epoch 27  63.1% | batch:       433 of       686\t|\tloss: 0.690285\n",
      "Training Epoch 27  63.3% | batch:       434 of       686\t|\tloss: 0.909339\n",
      "Training Epoch 27  63.4% | batch:       435 of       686\t|\tloss: 0.82312\n",
      "Training Epoch 27  63.6% | batch:       436 of       686\t|\tloss: 0.602074\n",
      "Training Epoch 27  63.7% | batch:       437 of       686\t|\tloss: 0.770938\n",
      "Training Epoch 27  63.8% | batch:       438 of       686\t|\tloss: 0.527778\n",
      "Training Epoch 27  64.0% | batch:       439 of       686\t|\tloss: 0.749463\n",
      "Training Epoch 27  64.1% | batch:       440 of       686\t|\tloss: 0.936698\n",
      "Training Epoch 27  64.3% | batch:       441 of       686\t|\tloss: 0.681992\n",
      "Training Epoch 27  64.4% | batch:       442 of       686\t|\tloss: 0.762654\n",
      "Training Epoch 27  64.6% | batch:       443 of       686\t|\tloss: 1.06323\n",
      "Training Epoch 27  64.7% | batch:       444 of       686\t|\tloss: 0.77466\n",
      "Training Epoch 27  64.9% | batch:       445 of       686\t|\tloss: 0.627238\n",
      "Training Epoch 27  65.0% | batch:       446 of       686\t|\tloss: 0.813169\n",
      "Training Epoch 27  65.2% | batch:       447 of       686\t|\tloss: 0.800045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 27  65.3% | batch:       448 of       686\t|\tloss: 0.825461\n",
      "Training Epoch 27  65.5% | batch:       449 of       686\t|\tloss: 0.943516\n",
      "Training Epoch 27  65.6% | batch:       450 of       686\t|\tloss: 0.86112\n",
      "Training Epoch 27  65.7% | batch:       451 of       686\t|\tloss: 1.05495\n",
      "Training Epoch 27  65.9% | batch:       452 of       686\t|\tloss: 0.701175\n",
      "Training Epoch 27  66.0% | batch:       453 of       686\t|\tloss: 0.719752\n",
      "Training Epoch 27  66.2% | batch:       454 of       686\t|\tloss: 0.700754\n",
      "Training Epoch 27  66.3% | batch:       455 of       686\t|\tloss: 0.890098\n",
      "Training Epoch 27  66.5% | batch:       456 of       686\t|\tloss: 0.985454\n",
      "Training Epoch 27  66.6% | batch:       457 of       686\t|\tloss: 0.810768\n",
      "Training Epoch 27  66.8% | batch:       458 of       686\t|\tloss: 0.772236\n",
      "Training Epoch 27  66.9% | batch:       459 of       686\t|\tloss: 0.723155\n",
      "Training Epoch 27  67.1% | batch:       460 of       686\t|\tloss: 0.722475\n",
      "Training Epoch 27  67.2% | batch:       461 of       686\t|\tloss: 0.804323\n",
      "Training Epoch 27  67.3% | batch:       462 of       686\t|\tloss: 0.673448\n",
      "Training Epoch 27  67.5% | batch:       463 of       686\t|\tloss: 0.854482\n",
      "Training Epoch 27  67.6% | batch:       464 of       686\t|\tloss: 1.01876\n",
      "Training Epoch 27  67.8% | batch:       465 of       686\t|\tloss: 1.04679\n",
      "Training Epoch 27  67.9% | batch:       466 of       686\t|\tloss: 0.62592\n",
      "Training Epoch 27  68.1% | batch:       467 of       686\t|\tloss: 0.953317\n",
      "Training Epoch 27  68.2% | batch:       468 of       686\t|\tloss: 0.99656\n",
      "Training Epoch 27  68.4% | batch:       469 of       686\t|\tloss: 0.705288\n",
      "Training Epoch 27  68.5% | batch:       470 of       686\t|\tloss: 0.610604\n",
      "Training Epoch 27  68.7% | batch:       471 of       686\t|\tloss: 0.6872\n",
      "Training Epoch 27  68.8% | batch:       472 of       686\t|\tloss: 0.673058\n",
      "Training Epoch 27  69.0% | batch:       473 of       686\t|\tloss: 0.819676\n",
      "Training Epoch 27  69.1% | batch:       474 of       686\t|\tloss: 0.86508\n",
      "Training Epoch 27  69.2% | batch:       475 of       686\t|\tloss: 0.615355\n",
      "Training Epoch 27  69.4% | batch:       476 of       686\t|\tloss: 0.792525\n",
      "Training Epoch 27  69.5% | batch:       477 of       686\t|\tloss: 0.829468\n",
      "Training Epoch 27  69.7% | batch:       478 of       686\t|\tloss: 0.580461\n",
      "Training Epoch 27  69.8% | batch:       479 of       686\t|\tloss: 0.927028\n",
      "Training Epoch 27  70.0% | batch:       480 of       686\t|\tloss: 0.664687\n",
      "Training Epoch 27  70.1% | batch:       481 of       686\t|\tloss: 0.632215\n",
      "Training Epoch 27  70.3% | batch:       482 of       686\t|\tloss: 0.906664\n",
      "Training Epoch 27  70.4% | batch:       483 of       686\t|\tloss: 0.7837\n",
      "Training Epoch 27  70.6% | batch:       484 of       686\t|\tloss: 0.643028\n",
      "Training Epoch 27  70.7% | batch:       485 of       686\t|\tloss: 0.816758\n",
      "Training Epoch 27  70.8% | batch:       486 of       686\t|\tloss: 0.665408\n",
      "Training Epoch 27  71.0% | batch:       487 of       686\t|\tloss: 0.730266\n",
      "Training Epoch 27  71.1% | batch:       488 of       686\t|\tloss: 0.836536\n",
      "Training Epoch 27  71.3% | batch:       489 of       686\t|\tloss: 0.674858\n",
      "Training Epoch 27  71.4% | batch:       490 of       686\t|\tloss: 0.759832\n",
      "Training Epoch 27  71.6% | batch:       491 of       686\t|\tloss: 0.868597\n",
      "Training Epoch 27  71.7% | batch:       492 of       686\t|\tloss: 0.780151\n",
      "Training Epoch 27  71.9% | batch:       493 of       686\t|\tloss: 0.658357\n",
      "Training Epoch 27  72.0% | batch:       494 of       686\t|\tloss: 0.577421\n",
      "Training Epoch 27  72.2% | batch:       495 of       686\t|\tloss: 1.03154\n",
      "Training Epoch 27  72.3% | batch:       496 of       686\t|\tloss: 0.730389\n",
      "Training Epoch 27  72.4% | batch:       497 of       686\t|\tloss: 0.792512\n",
      "Training Epoch 27  72.6% | batch:       498 of       686\t|\tloss: 0.765712\n",
      "Training Epoch 27  72.7% | batch:       499 of       686\t|\tloss: 0.945605\n",
      "Training Epoch 27  72.9% | batch:       500 of       686\t|\tloss: 0.893317\n",
      "Training Epoch 27  73.0% | batch:       501 of       686\t|\tloss: 0.991616\n",
      "Training Epoch 27  73.2% | batch:       502 of       686\t|\tloss: 0.64524\n",
      "Training Epoch 27  73.3% | batch:       503 of       686\t|\tloss: 0.900923\n",
      "Training Epoch 27  73.5% | batch:       504 of       686\t|\tloss: 0.775504\n",
      "Training Epoch 27  73.6% | batch:       505 of       686\t|\tloss: 0.564273\n",
      "Training Epoch 27  73.8% | batch:       506 of       686\t|\tloss: 0.526628\n",
      "Training Epoch 27  73.9% | batch:       507 of       686\t|\tloss: 0.619299\n",
      "Training Epoch 27  74.1% | batch:       508 of       686\t|\tloss: 0.790289\n",
      "Training Epoch 27  74.2% | batch:       509 of       686\t|\tloss: 0.958536\n",
      "Training Epoch 27  74.3% | batch:       510 of       686\t|\tloss: 0.645137\n",
      "Training Epoch 27  74.5% | batch:       511 of       686\t|\tloss: 0.790654\n",
      "Training Epoch 27  74.6% | batch:       512 of       686\t|\tloss: 0.700105\n",
      "Training Epoch 27  74.8% | batch:       513 of       686\t|\tloss: 0.78997\n",
      "Training Epoch 27  74.9% | batch:       514 of       686\t|\tloss: 0.850753\n",
      "Training Epoch 27  75.1% | batch:       515 of       686\t|\tloss: 0.675641\n",
      "Training Epoch 27  75.2% | batch:       516 of       686\t|\tloss: 0.819216\n",
      "Training Epoch 27  75.4% | batch:       517 of       686\t|\tloss: 0.787472\n",
      "Training Epoch 27  75.5% | batch:       518 of       686\t|\tloss: 0.797678\n",
      "Training Epoch 27  75.7% | batch:       519 of       686\t|\tloss: 0.816667\n",
      "Training Epoch 27  75.8% | batch:       520 of       686\t|\tloss: 0.960612\n",
      "Training Epoch 27  75.9% | batch:       521 of       686\t|\tloss: 0.715362\n",
      "Training Epoch 27  76.1% | batch:       522 of       686\t|\tloss: 0.630349\n",
      "Training Epoch 27  76.2% | batch:       523 of       686\t|\tloss: 0.634666\n",
      "Training Epoch 27  76.4% | batch:       524 of       686\t|\tloss: 0.96108\n",
      "Training Epoch 27  76.5% | batch:       525 of       686\t|\tloss: 0.562351\n",
      "Training Epoch 27  76.7% | batch:       526 of       686\t|\tloss: 0.848428\n",
      "Training Epoch 27  76.8% | batch:       527 of       686\t|\tloss: 0.750246\n",
      "Training Epoch 27  77.0% | batch:       528 of       686\t|\tloss: 0.798005\n",
      "Training Epoch 27  77.1% | batch:       529 of       686\t|\tloss: 0.968855\n",
      "Training Epoch 27  77.3% | batch:       530 of       686\t|\tloss: 0.595539\n",
      "Training Epoch 27  77.4% | batch:       531 of       686\t|\tloss: 0.6867\n",
      "Training Epoch 27  77.6% | batch:       532 of       686\t|\tloss: 0.782034\n",
      "Training Epoch 27  77.7% | batch:       533 of       686\t|\tloss: 0.701347\n",
      "Training Epoch 27  77.8% | batch:       534 of       686\t|\tloss: 0.79848\n",
      "Training Epoch 27  78.0% | batch:       535 of       686\t|\tloss: 0.905402\n",
      "Training Epoch 27  78.1% | batch:       536 of       686\t|\tloss: 0.830334\n",
      "Training Epoch 27  78.3% | batch:       537 of       686\t|\tloss: 0.944038\n",
      "Training Epoch 27  78.4% | batch:       538 of       686\t|\tloss: 0.810306\n",
      "Training Epoch 27  78.6% | batch:       539 of       686\t|\tloss: 0.741211\n",
      "Training Epoch 27  78.7% | batch:       540 of       686\t|\tloss: 0.737908\n",
      "Training Epoch 27  78.9% | batch:       541 of       686\t|\tloss: 0.595211\n",
      "Training Epoch 27  79.0% | batch:       542 of       686\t|\tloss: 0.868947\n",
      "Training Epoch 27  79.2% | batch:       543 of       686\t|\tloss: 0.73381\n",
      "Training Epoch 27  79.3% | batch:       544 of       686\t|\tloss: 0.729553\n",
      "Training Epoch 27  79.4% | batch:       545 of       686\t|\tloss: 0.7253\n",
      "Training Epoch 27  79.6% | batch:       546 of       686\t|\tloss: 0.821916\n",
      "Training Epoch 27  79.7% | batch:       547 of       686\t|\tloss: 0.751668\n",
      "Training Epoch 27  79.9% | batch:       548 of       686\t|\tloss: 0.704364\n",
      "Training Epoch 27  80.0% | batch:       549 of       686\t|\tloss: 0.840423\n",
      "Training Epoch 27  80.2% | batch:       550 of       686\t|\tloss: 0.942384\n",
      "Training Epoch 27  80.3% | batch:       551 of       686\t|\tloss: 0.960726\n",
      "Training Epoch 27  80.5% | batch:       552 of       686\t|\tloss: 0.704642\n",
      "Training Epoch 27  80.6% | batch:       553 of       686\t|\tloss: 0.930649\n",
      "Training Epoch 27  80.8% | batch:       554 of       686\t|\tloss: 0.709952\n",
      "Training Epoch 27  80.9% | batch:       555 of       686\t|\tloss: 1.14528\n",
      "Training Epoch 27  81.0% | batch:       556 of       686\t|\tloss: 0.989038\n",
      "Training Epoch 27  81.2% | batch:       557 of       686\t|\tloss: 0.761665\n",
      "Training Epoch 27  81.3% | batch:       558 of       686\t|\tloss: 0.666723\n",
      "Training Epoch 27  81.5% | batch:       559 of       686\t|\tloss: 0.707854\n",
      "Training Epoch 27  81.6% | batch:       560 of       686\t|\tloss: 0.643808\n",
      "Training Epoch 27  81.8% | batch:       561 of       686\t|\tloss: 0.782832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 27  81.9% | batch:       562 of       686\t|\tloss: 0.806432\n",
      "Training Epoch 27  82.1% | batch:       563 of       686\t|\tloss: 0.879792\n",
      "Training Epoch 27  82.2% | batch:       564 of       686\t|\tloss: 0.670633\n",
      "Training Epoch 27  82.4% | batch:       565 of       686\t|\tloss: 0.648982\n",
      "Training Epoch 27  82.5% | batch:       566 of       686\t|\tloss: 0.832217\n",
      "Training Epoch 27  82.7% | batch:       567 of       686\t|\tloss: 0.779563\n",
      "Training Epoch 27  82.8% | batch:       568 of       686\t|\tloss: 0.93117\n",
      "Training Epoch 27  82.9% | batch:       569 of       686\t|\tloss: 0.622756\n",
      "Training Epoch 27  83.1% | batch:       570 of       686\t|\tloss: 0.738252\n",
      "Training Epoch 27  83.2% | batch:       571 of       686\t|\tloss: 0.933144\n",
      "Training Epoch 27  83.4% | batch:       572 of       686\t|\tloss: 0.641091\n",
      "Training Epoch 27  83.5% | batch:       573 of       686\t|\tloss: 0.874246\n",
      "Training Epoch 27  83.7% | batch:       574 of       686\t|\tloss: 0.90133\n",
      "Training Epoch 27  83.8% | batch:       575 of       686\t|\tloss: 0.889743\n",
      "Training Epoch 27  84.0% | batch:       576 of       686\t|\tloss: 0.790435\n",
      "Training Epoch 27  84.1% | batch:       577 of       686\t|\tloss: 0.60979\n",
      "Training Epoch 27  84.3% | batch:       578 of       686\t|\tloss: 0.767349\n",
      "Training Epoch 27  84.4% | batch:       579 of       686\t|\tloss: 0.925502\n",
      "Training Epoch 27  84.5% | batch:       580 of       686\t|\tloss: 0.927915\n",
      "Training Epoch 27  84.7% | batch:       581 of       686\t|\tloss: 0.761343\n",
      "Training Epoch 27  84.8% | batch:       582 of       686\t|\tloss: 0.863564\n",
      "Training Epoch 27  85.0% | batch:       583 of       686\t|\tloss: 0.933747\n",
      "Training Epoch 27  85.1% | batch:       584 of       686\t|\tloss: 0.655383\n",
      "Training Epoch 27  85.3% | batch:       585 of       686\t|\tloss: 0.776282\n",
      "Training Epoch 27  85.4% | batch:       586 of       686\t|\tloss: 0.851287\n",
      "Training Epoch 27  85.6% | batch:       587 of       686\t|\tloss: 0.776883\n",
      "Training Epoch 27  85.7% | batch:       588 of       686\t|\tloss: 0.894146\n",
      "Training Epoch 27  85.9% | batch:       589 of       686\t|\tloss: 0.832754\n",
      "Training Epoch 27  86.0% | batch:       590 of       686\t|\tloss: 0.734913\n",
      "Training Epoch 27  86.2% | batch:       591 of       686\t|\tloss: 0.742106\n",
      "Training Epoch 27  86.3% | batch:       592 of       686\t|\tloss: 0.824865\n",
      "Training Epoch 27  86.4% | batch:       593 of       686\t|\tloss: 1.19005\n",
      "Training Epoch 27  86.6% | batch:       594 of       686\t|\tloss: 0.768532\n",
      "Training Epoch 27  86.7% | batch:       595 of       686\t|\tloss: 0.853554\n",
      "Training Epoch 27  86.9% | batch:       596 of       686\t|\tloss: 0.752888\n",
      "Training Epoch 27  87.0% | batch:       597 of       686\t|\tloss: 0.671128\n",
      "Training Epoch 27  87.2% | batch:       598 of       686\t|\tloss: 0.750144\n",
      "Training Epoch 27  87.3% | batch:       599 of       686\t|\tloss: 0.818735\n",
      "Training Epoch 27  87.5% | batch:       600 of       686\t|\tloss: 0.811761\n",
      "Training Epoch 27  87.6% | batch:       601 of       686\t|\tloss: 0.670138\n",
      "Training Epoch 27  87.8% | batch:       602 of       686\t|\tloss: 0.802861\n",
      "Training Epoch 27  87.9% | batch:       603 of       686\t|\tloss: 0.788719\n",
      "Training Epoch 27  88.0% | batch:       604 of       686\t|\tloss: 0.742494\n",
      "Training Epoch 27  88.2% | batch:       605 of       686\t|\tloss: 0.647466\n",
      "Training Epoch 27  88.3% | batch:       606 of       686\t|\tloss: 0.627529\n",
      "Training Epoch 27  88.5% | batch:       607 of       686\t|\tloss: 0.816643\n",
      "Training Epoch 27  88.6% | batch:       608 of       686\t|\tloss: 0.624853\n",
      "Training Epoch 27  88.8% | batch:       609 of       686\t|\tloss: 0.678801\n",
      "Training Epoch 27  88.9% | batch:       610 of       686\t|\tloss: 0.76889\n",
      "Training Epoch 27  89.1% | batch:       611 of       686\t|\tloss: 0.763072\n",
      "Training Epoch 27  89.2% | batch:       612 of       686\t|\tloss: 0.643146\n",
      "Training Epoch 27  89.4% | batch:       613 of       686\t|\tloss: 0.609909\n",
      "Training Epoch 27  89.5% | batch:       614 of       686\t|\tloss: 0.816445\n",
      "Training Epoch 27  89.7% | batch:       615 of       686\t|\tloss: 0.682949\n",
      "Training Epoch 27  89.8% | batch:       616 of       686\t|\tloss: 1.04838\n",
      "Training Epoch 27  89.9% | batch:       617 of       686\t|\tloss: 0.634088\n",
      "Training Epoch 27  90.1% | batch:       618 of       686\t|\tloss: 0.770847\n",
      "Training Epoch 27  90.2% | batch:       619 of       686\t|\tloss: 0.948448\n",
      "Training Epoch 27  90.4% | batch:       620 of       686\t|\tloss: 0.89665\n",
      "Training Epoch 27  90.5% | batch:       621 of       686\t|\tloss: 0.71037\n",
      "Training Epoch 27  90.7% | batch:       622 of       686\t|\tloss: 0.766904\n",
      "Training Epoch 27  90.8% | batch:       623 of       686\t|\tloss: 0.853388\n",
      "Training Epoch 27  91.0% | batch:       624 of       686\t|\tloss: 0.862611\n",
      "Training Epoch 27  91.1% | batch:       625 of       686\t|\tloss: 0.691678\n",
      "Training Epoch 27  91.3% | batch:       626 of       686\t|\tloss: 0.833796\n",
      "Training Epoch 27  91.4% | batch:       627 of       686\t|\tloss: 0.726408\n",
      "Training Epoch 27  91.5% | batch:       628 of       686\t|\tloss: 0.575495\n",
      "Training Epoch 27  91.7% | batch:       629 of       686\t|\tloss: 0.790435\n",
      "Training Epoch 27  91.8% | batch:       630 of       686\t|\tloss: 0.620204\n",
      "Training Epoch 27  92.0% | batch:       631 of       686\t|\tloss: 0.734509\n",
      "Training Epoch 27  92.1% | batch:       632 of       686\t|\tloss: 0.856785\n",
      "Training Epoch 27  92.3% | batch:       633 of       686\t|\tloss: 0.751666\n",
      "Training Epoch 27  92.4% | batch:       634 of       686\t|\tloss: 0.694048\n",
      "Training Epoch 27  92.6% | batch:       635 of       686\t|\tloss: 0.651999\n",
      "Training Epoch 27  92.7% | batch:       636 of       686\t|\tloss: 0.637979\n",
      "Training Epoch 27  92.9% | batch:       637 of       686\t|\tloss: 0.666077\n",
      "Training Epoch 27  93.0% | batch:       638 of       686\t|\tloss: 0.653085\n",
      "Training Epoch 27  93.1% | batch:       639 of       686\t|\tloss: 0.867779\n",
      "Training Epoch 27  93.3% | batch:       640 of       686\t|\tloss: 0.675291\n",
      "Training Epoch 27  93.4% | batch:       641 of       686\t|\tloss: 0.928097\n",
      "Training Epoch 27  93.6% | batch:       642 of       686\t|\tloss: 0.760945\n",
      "Training Epoch 27  93.7% | batch:       643 of       686\t|\tloss: 0.571736\n",
      "Training Epoch 27  93.9% | batch:       644 of       686\t|\tloss: 0.822576\n",
      "Training Epoch 27  94.0% | batch:       645 of       686\t|\tloss: 0.723136\n",
      "Training Epoch 27  94.2% | batch:       646 of       686\t|\tloss: 0.79096\n",
      "Training Epoch 27  94.3% | batch:       647 of       686\t|\tloss: 0.66933\n",
      "Training Epoch 27  94.5% | batch:       648 of       686\t|\tloss: 0.912878\n",
      "Training Epoch 27  94.6% | batch:       649 of       686\t|\tloss: 0.550033\n",
      "Training Epoch 27  94.8% | batch:       650 of       686\t|\tloss: 0.682265\n",
      "Training Epoch 27  94.9% | batch:       651 of       686\t|\tloss: 0.939821\n",
      "Training Epoch 27  95.0% | batch:       652 of       686\t|\tloss: 0.79794\n",
      "Training Epoch 27  95.2% | batch:       653 of       686\t|\tloss: 0.733359\n",
      "Training Epoch 27  95.3% | batch:       654 of       686\t|\tloss: 0.711503\n",
      "Training Epoch 27  95.5% | batch:       655 of       686\t|\tloss: 0.72095\n",
      "Training Epoch 27  95.6% | batch:       656 of       686\t|\tloss: 0.657764\n",
      "Training Epoch 27  95.8% | batch:       657 of       686\t|\tloss: 0.583981\n",
      "Training Epoch 27  95.9% | batch:       658 of       686\t|\tloss: 0.713461\n",
      "Training Epoch 27  96.1% | batch:       659 of       686\t|\tloss: 0.703218\n",
      "Training Epoch 27  96.2% | batch:       660 of       686\t|\tloss: 0.902907\n",
      "Training Epoch 27  96.4% | batch:       661 of       686\t|\tloss: 0.968245\n",
      "Training Epoch 27  96.5% | batch:       662 of       686\t|\tloss: 0.804521\n",
      "Training Epoch 27  96.6% | batch:       663 of       686\t|\tloss: 0.56703\n",
      "Training Epoch 27  96.8% | batch:       664 of       686\t|\tloss: 0.798476\n",
      "Training Epoch 27  96.9% | batch:       665 of       686\t|\tloss: 0.689662\n",
      "Training Epoch 27  97.1% | batch:       666 of       686\t|\tloss: 0.808807\n",
      "Training Epoch 27  97.2% | batch:       667 of       686\t|\tloss: 0.873218\n",
      "Training Epoch 27  97.4% | batch:       668 of       686\t|\tloss: 0.684939\n",
      "Training Epoch 27  97.5% | batch:       669 of       686\t|\tloss: 0.880803\n",
      "Training Epoch 27  97.7% | batch:       670 of       686\t|\tloss: 0.955045\n",
      "Training Epoch 27  97.8% | batch:       671 of       686\t|\tloss: 0.924507\n",
      "Training Epoch 27  98.0% | batch:       672 of       686\t|\tloss: 1.01945\n",
      "Training Epoch 27  98.1% | batch:       673 of       686\t|\tloss: 0.698233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 27  98.3% | batch:       674 of       686\t|\tloss: 0.752227\n",
      "Training Epoch 27  98.4% | batch:       675 of       686\t|\tloss: 1.07669\n",
      "Training Epoch 27  98.5% | batch:       676 of       686\t|\tloss: 0.906411\n",
      "Training Epoch 27  98.7% | batch:       677 of       686\t|\tloss: 0.775689\n",
      "Training Epoch 27  98.8% | batch:       678 of       686\t|\tloss: 0.647783\n",
      "Training Epoch 27  99.0% | batch:       679 of       686\t|\tloss: 0.71959\n",
      "Training Epoch 27  99.1% | batch:       680 of       686\t|\tloss: 0.894215\n",
      "Training Epoch 27  99.3% | batch:       681 of       686\t|\tloss: 0.861637\n",
      "Training Epoch 27  99.4% | batch:       682 of       686\t|\tloss: 0.763542\n",
      "Training Epoch 27  99.6% | batch:       683 of       686\t|\tloss: 0.859374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:12:20,610 | INFO : Epoch 27 Training Summary: epoch: 27.000000 | loss: 0.798861 | \n",
      "2023-05-25 22:12:20,611 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 22.12894344329834 seconds\n",
      "\n",
      "2023-05-25 22:12:20,612 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.82660079885412 seconds\n",
      "2023-05-25 22:12:20,613 | INFO : Avg batch train. time: 0.03473265422573487 seconds\n",
      "2023-05-25 22:12:20,614 | INFO : Avg sample train. time: 0.00027169850959409453 seconds\n",
      "2023-05-25 22:12:20,614 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 27  99.7% | batch:       684 of       686\t|\tloss: 0.841195\n",
      "Training Epoch 27  99.9% | batch:       685 of       686\t|\tloss: 1.36382\n",
      "\n",
      "Evaluating Epoch 27   0.0% | batch:         0 of       172\t|\tloss: 1.26388\n",
      "Evaluating Epoch 27   0.6% | batch:         1 of       172\t|\tloss: 1.31847\n",
      "Evaluating Epoch 27   1.2% | batch:         2 of       172\t|\tloss: 0.551739\n",
      "Evaluating Epoch 27   1.7% | batch:         3 of       172\t|\tloss: 2.02588\n",
      "Evaluating Epoch 27   2.3% | batch:         4 of       172\t|\tloss: 1.06002\n",
      "Evaluating Epoch 27   2.9% | batch:         5 of       172\t|\tloss: 1.19148\n",
      "Evaluating Epoch 27   3.5% | batch:         6 of       172\t|\tloss: 1.04479\n",
      "Evaluating Epoch 27   4.1% | batch:         7 of       172\t|\tloss: 2.5266\n",
      "Evaluating Epoch 27   4.7% | batch:         8 of       172\t|\tloss: 0.571051\n",
      "Evaluating Epoch 27   5.2% | batch:         9 of       172\t|\tloss: 1.49754\n",
      "Evaluating Epoch 27   5.8% | batch:        10 of       172\t|\tloss: 1.0956\n",
      "Evaluating Epoch 27   6.4% | batch:        11 of       172\t|\tloss: 1.1217\n",
      "Evaluating Epoch 27   7.0% | batch:        12 of       172\t|\tloss: 1.27211\n",
      "Evaluating Epoch 27   7.6% | batch:        13 of       172\t|\tloss: 1.04315\n",
      "Evaluating Epoch 27   8.1% | batch:        14 of       172\t|\tloss: 1.36423\n",
      "Evaluating Epoch 27   8.7% | batch:        15 of       172\t|\tloss: 1.19643\n",
      "Evaluating Epoch 27   9.3% | batch:        16 of       172\t|\tloss: 1.97815\n",
      "Evaluating Epoch 27   9.9% | batch:        17 of       172\t|\tloss: 0.757729\n",
      "Evaluating Epoch 27  10.5% | batch:        18 of       172\t|\tloss: 20.3566\n",
      "Evaluating Epoch 27  11.0% | batch:        19 of       172\t|\tloss: 1.45684\n",
      "Evaluating Epoch 27  11.6% | batch:        20 of       172\t|\tloss: 2.26963\n",
      "Evaluating Epoch 27  12.2% | batch:        21 of       172\t|\tloss: 0.767926\n",
      "Evaluating Epoch 27  12.8% | batch:        22 of       172\t|\tloss: 5.73477\n",
      "Evaluating Epoch 27  13.4% | batch:        23 of       172\t|\tloss: 3.40849\n",
      "Evaluating Epoch 27  14.0% | batch:        24 of       172\t|\tloss: 1.08535\n",
      "Evaluating Epoch 27  14.5% | batch:        25 of       172\t|\tloss: 1.98106\n",
      "Evaluating Epoch 27  15.1% | batch:        26 of       172\t|\tloss: 8.60881\n",
      "Evaluating Epoch 27  15.7% | batch:        27 of       172\t|\tloss: 17.0669\n",
      "Evaluating Epoch 27  16.3% | batch:        28 of       172\t|\tloss: 0.311518\n",
      "Evaluating Epoch 27  16.9% | batch:        29 of       172\t|\tloss: 1.63049\n",
      "Evaluating Epoch 27  17.4% | batch:        30 of       172\t|\tloss: 1.17228\n",
      "Evaluating Epoch 27  18.0% | batch:        31 of       172\t|\tloss: 0.68677\n",
      "Evaluating Epoch 27  18.6% | batch:        32 of       172\t|\tloss: 0.309038\n",
      "Evaluating Epoch 27  19.2% | batch:        33 of       172\t|\tloss: 0.438112\n",
      "Evaluating Epoch 27  19.8% | batch:        34 of       172\t|\tloss: 0.307396\n",
      "Evaluating Epoch 27  20.3% | batch:        35 of       172\t|\tloss: 0.928146\n",
      "Evaluating Epoch 27  20.9% | batch:        36 of       172\t|\tloss: 2.68611\n",
      "Evaluating Epoch 27  21.5% | batch:        37 of       172\t|\tloss: 4.21495\n",
      "Evaluating Epoch 27  22.1% | batch:        38 of       172\t|\tloss: 4.24492\n",
      "Evaluating Epoch 27  22.7% | batch:        39 of       172\t|\tloss: 9.28514\n",
      "Evaluating Epoch 27  23.3% | batch:        40 of       172\t|\tloss: 0.410247\n",
      "Evaluating Epoch 27  23.8% | batch:        41 of       172\t|\tloss: 0.780439\n",
      "Evaluating Epoch 27  24.4% | batch:        42 of       172\t|\tloss: 0.455477\n",
      "Evaluating Epoch 27  25.0% | batch:        43 of       172\t|\tloss: 21.6008\n",
      "Evaluating Epoch 27  25.6% | batch:        44 of       172\t|\tloss: 1.37188\n",
      "Evaluating Epoch 27  26.2% | batch:        45 of       172\t|\tloss: 1.00149\n",
      "Evaluating Epoch 27  26.7% | batch:        46 of       172\t|\tloss: 0.469198\n",
      "Evaluating Epoch 27  27.3% | batch:        47 of       172\t|\tloss: 1.12115\n",
      "Evaluating Epoch 27  27.9% | batch:        48 of       172\t|\tloss: 0.468637\n",
      "Evaluating Epoch 27  28.5% | batch:        49 of       172\t|\tloss: 0.939857\n",
      "Evaluating Epoch 27  29.1% | batch:        50 of       172\t|\tloss: 0.429716\n",
      "Evaluating Epoch 27  29.7% | batch:        51 of       172\t|\tloss: 1.01598\n",
      "Evaluating Epoch 27  30.2% | batch:        52 of       172\t|\tloss: 0.539531\n",
      "Evaluating Epoch 27  30.8% | batch:        53 of       172\t|\tloss: 3.23876\n",
      "Evaluating Epoch 27  31.4% | batch:        54 of       172\t|\tloss: 0.91661\n",
      "Evaluating Epoch 27  32.0% | batch:        55 of       172\t|\tloss: 0.343293\n",
      "Evaluating Epoch 27  32.6% | batch:        56 of       172\t|\tloss: 3.62422\n",
      "Evaluating Epoch 27  33.1% | batch:        57 of       172\t|\tloss: 0.272946\n",
      "Evaluating Epoch 27  33.7% | batch:        58 of       172\t|\tloss: 2.51503\n",
      "Evaluating Epoch 27  34.3% | batch:        59 of       172\t|\tloss: 1.22291\n",
      "Evaluating Epoch 27  34.9% | batch:        60 of       172\t|\tloss: 1.22396\n",
      "Evaluating Epoch 27  35.5% | batch:        61 of       172\t|\tloss: 1.94076\n",
      "Evaluating Epoch 27  36.0% | batch:        62 of       172\t|\tloss: 0.919974\n",
      "Evaluating Epoch 27  36.6% | batch:        63 of       172\t|\tloss: 3.32605\n",
      "Evaluating Epoch 27  37.2% | batch:        64 of       172\t|\tloss: 0.631883\n",
      "Evaluating Epoch 27  37.8% | batch:        65 of       172\t|\tloss: 2.62013\n",
      "Evaluating Epoch 27  38.4% | batch:        66 of       172\t|\tloss: 1.60945\n",
      "Evaluating Epoch 27  39.0% | batch:        67 of       172\t|\tloss: 0.346855\n",
      "Evaluating Epoch 27  39.5% | batch:        68 of       172\t|\tloss: 2.69922\n",
      "Evaluating Epoch 27  40.1% | batch:        69 of       172\t|\tloss: 0.689852\n",
      "Evaluating Epoch 27  40.7% | batch:        70 of       172\t|\tloss: 2.0465\n",
      "Evaluating Epoch 27  41.3% | batch:        71 of       172\t|\tloss: 1.54453\n",
      "Evaluating Epoch 27  41.9% | batch:        72 of       172\t|\tloss: 0.578613\n",
      "Evaluating Epoch 27  42.4% | batch:        73 of       172\t|\tloss: 2.88459\n",
      "Evaluating Epoch 27  43.0% | batch:        74 of       172\t|\tloss: 0.409894\n",
      "Evaluating Epoch 27  43.6% | batch:        75 of       172\t|\tloss: 0.357679\n",
      "Evaluating Epoch 27  44.2% | batch:        76 of       172\t|\tloss: 0.459997\n",
      "Evaluating Epoch 27  44.8% | batch:        77 of       172\t|\tloss: 0.407371\n",
      "Evaluating Epoch 27  45.3% | batch:        78 of       172\t|\tloss: 0.407809\n",
      "Evaluating Epoch 27  45.9% | batch:        79 of       172\t|\tloss: 0.314041\n",
      "Evaluating Epoch 27  46.5% | batch:        80 of       172\t|\tloss: 0.329988\n",
      "Evaluating Epoch 27  47.1% | batch:        81 of       172\t|\tloss: 0.431866\n",
      "Evaluating Epoch 27  47.7% | batch:        82 of       172\t|\tloss: 0.403569\n",
      "Evaluating Epoch 27  48.3% | batch:        83 of       172\t|\tloss: 0.411336\n",
      "Evaluating Epoch 27  48.8% | batch:        84 of       172\t|\tloss: 0.513327\n",
      "Evaluating Epoch 27  49.4% | batch:        85 of       172\t|\tloss: 0.610858\n",
      "Evaluating Epoch 27  50.0% | batch:        86 of       172\t|\tloss: 0.52238\n",
      "Evaluating Epoch 27  50.6% | batch:        87 of       172\t|\tloss: 0.612128\n",
      "Evaluating Epoch 27  51.2% | batch:        88 of       172\t|\tloss: 0.424645\n",
      "Evaluating Epoch 27  51.7% | batch:        89 of       172\t|\tloss: 0.636239\n",
      "Evaluating Epoch 27  52.3% | batch:        90 of       172\t|\tloss: 0.606098\n",
      "Evaluating Epoch 27  52.9% | batch:        91 of       172\t|\tloss: 0.195407\n",
      "Evaluating Epoch 27  53.5% | batch:        92 of       172\t|\tloss: 0.509093\n",
      "Evaluating Epoch 27  54.1% | batch:        93 of       172\t|\tloss: 0.874503\n",
      "Evaluating Epoch 27  54.7% | batch:        94 of       172\t|\tloss: 0.325544\n",
      "Evaluating Epoch 27  55.2% | batch:        95 of       172\t|\tloss: 0.468945\n",
      "Evaluating Epoch 27  55.8% | batch:        96 of       172\t|\tloss: 0.728987\n",
      "Evaluating Epoch 27  56.4% | batch:        97 of       172\t|\tloss: 0.547702\n",
      "Evaluating Epoch 27  57.0% | batch:        98 of       172\t|\tloss: 0.395155\n",
      "Evaluating Epoch 27  57.6% | batch:        99 of       172\t|\tloss: 0.53012\n",
      "Evaluating Epoch 27  58.1% | batch:       100 of       172\t|\tloss: 0.578701\n",
      "Evaluating Epoch 27  58.7% | batch:       101 of       172\t|\tloss: 0.320137\n",
      "Evaluating Epoch 27  59.3% | batch:       102 of       172\t|\tloss: 0.491295\n",
      "Evaluating Epoch 27  59.9% | batch:       103 of       172\t|\tloss: 0.912889\n",
      "Evaluating Epoch 27  60.5% | batch:       104 of       172\t|\tloss: 0.499844\n",
      "Evaluating Epoch 27  61.0% | batch:       105 of       172\t|\tloss: 0.232626\n",
      "Evaluating Epoch 27  61.6% | batch:       106 of       172\t|\tloss: 0.553224\n",
      "Evaluating Epoch 27  62.2% | batch:       107 of       172\t|\tloss: 1.01432\n",
      "Evaluating Epoch 27  62.8% | batch:       108 of       172\t|\tloss: 0.201855\n",
      "Evaluating Epoch 27  63.4% | batch:       109 of       172\t|\tloss: 0.48072\n",
      "Evaluating Epoch 27  64.0% | batch:       110 of       172\t|\tloss: 0.828005\n",
      "Evaluating Epoch 27  64.5% | batch:       111 of       172\t|\tloss: 0.520943\n",
      "Evaluating Epoch 27  65.1% | batch:       112 of       172\t|\tloss: 0.456849\n",
      "Evaluating Epoch 27  65.7% | batch:       113 of       172\t|\tloss: 0.759376\n",
      "Evaluating Epoch 27  66.3% | batch:       114 of       172\t|\tloss: 0.678333\n",
      "Evaluating Epoch 27  66.9% | batch:       115 of       172\t|\tloss: 0.364172\n",
      "Evaluating Epoch 27  67.4% | batch:       116 of       172\t|\tloss: 0.286626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 27  68.0% | batch:       117 of       172\t|\tloss: 0.220733\n",
      "Evaluating Epoch 27  68.6% | batch:       118 of       172\t|\tloss: 0.30202\n",
      "Evaluating Epoch 27  69.2% | batch:       119 of       172\t|\tloss: 0.190227\n",
      "Evaluating Epoch 27  69.8% | batch:       120 of       172\t|\tloss: 0.35859\n",
      "Evaluating Epoch 27  70.3% | batch:       121 of       172\t|\tloss: 0.352535\n",
      "Evaluating Epoch 27  70.9% | batch:       122 of       172\t|\tloss: 0.291799\n",
      "Evaluating Epoch 27  71.5% | batch:       123 of       172\t|\tloss: 0.470811\n",
      "Evaluating Epoch 27  72.1% | batch:       124 of       172\t|\tloss: 0.325869\n",
      "Evaluating Epoch 27  72.7% | batch:       125 of       172\t|\tloss: 0.286564\n",
      "Evaluating Epoch 27  73.3% | batch:       126 of       172\t|\tloss: 0.20068\n",
      "Evaluating Epoch 27  73.8% | batch:       127 of       172\t|\tloss: 0.442526\n",
      "Evaluating Epoch 27  74.4% | batch:       128 of       172\t|\tloss: 0.505381\n",
      "Evaluating Epoch 27  75.0% | batch:       129 of       172\t|\tloss: 0.307946\n",
      "Evaluating Epoch 27  75.6% | batch:       130 of       172\t|\tloss: 0.42342\n",
      "Evaluating Epoch 27  76.2% | batch:       131 of       172\t|\tloss: 0.452775\n",
      "Evaluating Epoch 27  76.7% | batch:       132 of       172\t|\tloss: 0.367942\n",
      "Evaluating Epoch 27  77.3% | batch:       133 of       172\t|\tloss: 0.695332\n",
      "Evaluating Epoch 27  77.9% | batch:       134 of       172\t|\tloss: 0.437135\n",
      "Evaluating Epoch 27  78.5% | batch:       135 of       172\t|\tloss: 0.502403\n",
      "Evaluating Epoch 27  79.1% | batch:       136 of       172\t|\tloss: 0.423734\n",
      "Evaluating Epoch 27  79.7% | batch:       137 of       172\t|\tloss: 0.446457\n",
      "Evaluating Epoch 27  80.2% | batch:       138 of       172\t|\tloss: 0.377881\n",
      "Evaluating Epoch 27  80.8% | batch:       139 of       172\t|\tloss: 0.68746\n",
      "Evaluating Epoch 27  81.4% | batch:       140 of       172\t|\tloss: 0.359501\n",
      "Evaluating Epoch 27  82.0% | batch:       141 of       172\t|\tloss: 0.285165\n",
      "Evaluating Epoch 27  82.6% | batch:       142 of       172\t|\tloss: 0.323235\n",
      "Evaluating Epoch 27  83.1% | batch:       143 of       172\t|\tloss: 0.450725\n",
      "Evaluating Epoch 27  83.7% | batch:       144 of       172\t|\tloss: 0.400979\n",
      "Evaluating Epoch 27  84.3% | batch:       145 of       172\t|\tloss: 0.601885\n",
      "Evaluating Epoch 27  84.9% | batch:       146 of       172\t|\tloss: 0.428718\n",
      "Evaluating Epoch 27  85.5% | batch:       147 of       172\t|\tloss: 0.563141\n",
      "Evaluating Epoch 27  86.0% | batch:       148 of       172\t|\tloss: 0.359157\n",
      "Evaluating Epoch 27  86.6% | batch:       149 of       172\t|\tloss: 0.489009\n",
      "Evaluating Epoch 27  87.2% | batch:       150 of       172\t|\tloss: 0.152496\n",
      "Evaluating Epoch 27  87.8% | batch:       151 of       172\t|\tloss: 0.307787\n",
      "Evaluating Epoch 27  88.4% | batch:       152 of       172\t|\tloss: 0.395899\n",
      "Evaluating Epoch 27  89.0% | batch:       153 of       172\t|\tloss: 0.185771\n",
      "Evaluating Epoch 27  89.5% | batch:       154 of       172\t|\tloss: 0.299956\n",
      "Evaluating Epoch 27  90.1% | batch:       155 of       172\t|\tloss: 0.401731\n",
      "Evaluating Epoch 27  90.7% | batch:       156 of       172\t|\tloss: 0.277471\n",
      "Evaluating Epoch 27  91.3% | batch:       157 of       172\t|\tloss: 0.37218\n",
      "Evaluating Epoch 27  91.9% | batch:       158 of       172\t|\tloss: 0.196409\n",
      "Evaluating Epoch 27  92.4% | batch:       159 of       172\t|\tloss: 0.260587\n",
      "Evaluating Epoch 27  93.0% | batch:       160 of       172\t|\tloss: 0.927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:12:24,742 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 4.127094268798828 seconds\n",
      "\n",
      "2023-05-25 22:12:24,743 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.0121051498821805 seconds\n",
      "2023-05-25 22:12:24,743 | INFO : Avg batch val. time: 0.02332619273187314 seconds\n",
      "2023-05-25 22:12:24,745 | INFO : Avg sample val. time: 0.00018272556131904087 seconds\n",
      "2023-05-25 22:12:24,746 | INFO : Epoch 27 Validation Summary: epoch: 27.000000 | loss: 1.277966 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 27  93.6% | batch:       161 of       172\t|\tloss: 0.174246\n",
      "Evaluating Epoch 27  94.2% | batch:       162 of       172\t|\tloss: 0.340852\n",
      "Evaluating Epoch 27  94.8% | batch:       163 of       172\t|\tloss: 0.375572\n",
      "Evaluating Epoch 27  95.3% | batch:       164 of       172\t|\tloss: 0.238423\n",
      "Evaluating Epoch 27  95.9% | batch:       165 of       172\t|\tloss: 0.357002\n",
      "Evaluating Epoch 27  96.5% | batch:       166 of       172\t|\tloss: 0.188681\n",
      "Evaluating Epoch 27  97.1% | batch:       167 of       172\t|\tloss: 0.286222\n",
      "Evaluating Epoch 27  97.7% | batch:       168 of       172\t|\tloss: 0.33903\n",
      "Evaluating Epoch 27  98.3% | batch:       169 of       172\t|\tloss: 0.118421\n",
      "Evaluating Epoch 27  98.8% | batch:       170 of       172\t|\tloss: 0.258306\n",
      "Evaluating Epoch 27  99.4% | batch:       171 of       172\t|\tloss: 0.349292\n",
      "\n",
      "Training Epoch 28   0.0% | batch:         0 of       686\t|\tloss: 0.631145\n",
      "Training Epoch 28   0.1% | batch:         1 of       686\t|\tloss: 0.887477\n",
      "Training Epoch 28   0.3% | batch:         2 of       686\t|\tloss: 0.784883\n",
      "Training Epoch 28   0.4% | batch:         3 of       686\t|\tloss: 0.850443\n",
      "Training Epoch 28   0.6% | batch:         4 of       686\t|\tloss: 0.6226\n",
      "Training Epoch 28   0.7% | batch:         5 of       686\t|\tloss: 0.700009\n",
      "Training Epoch 28   0.9% | batch:         6 of       686\t|\tloss: 0.585843\n",
      "Training Epoch 28   1.0% | batch:         7 of       686\t|\tloss: 0.786411\n",
      "Training Epoch 28   1.2% | batch:         8 of       686\t|\tloss: 0.639681\n",
      "Training Epoch 28   1.3% | batch:         9 of       686\t|\tloss: 0.808245\n",
      "Training Epoch 28   1.5% | batch:        10 of       686\t|\tloss: 0.76039\n",
      "Training Epoch 28   1.6% | batch:        11 of       686\t|\tloss: 0.766705\n",
      "Training Epoch 28   1.7% | batch:        12 of       686\t|\tloss: 0.74761\n",
      "Training Epoch 28   1.9% | batch:        13 of       686\t|\tloss: 0.703856\n",
      "Training Epoch 28   2.0% | batch:        14 of       686\t|\tloss: 0.805431\n",
      "Training Epoch 28   2.2% | batch:        15 of       686\t|\tloss: 0.608738\n",
      "Training Epoch 28   2.3% | batch:        16 of       686\t|\tloss: 0.696225\n",
      "Training Epoch 28   2.5% | batch:        17 of       686\t|\tloss: 0.981605\n",
      "Training Epoch 28   2.6% | batch:        18 of       686\t|\tloss: 0.751454\n",
      "Training Epoch 28   2.8% | batch:        19 of       686\t|\tloss: 0.976806\n",
      "Training Epoch 28   2.9% | batch:        20 of       686\t|\tloss: 0.815086\n",
      "Training Epoch 28   3.1% | batch:        21 of       686\t|\tloss: 0.722513\n",
      "Training Epoch 28   3.2% | batch:        22 of       686\t|\tloss: 0.741145\n",
      "Training Epoch 28   3.4% | batch:        23 of       686\t|\tloss: 0.703012\n",
      "Training Epoch 28   3.5% | batch:        24 of       686\t|\tloss: 0.860675\n",
      "Training Epoch 28   3.6% | batch:        25 of       686\t|\tloss: 0.647232\n",
      "Training Epoch 28   3.8% | batch:        26 of       686\t|\tloss: 0.561926\n",
      "Training Epoch 28   3.9% | batch:        27 of       686\t|\tloss: 1.0604\n",
      "Training Epoch 28   4.1% | batch:        28 of       686\t|\tloss: 0.881503\n",
      "Training Epoch 28   4.2% | batch:        29 of       686\t|\tloss: 0.791137\n",
      "Training Epoch 28   4.4% | batch:        30 of       686\t|\tloss: 0.991968\n",
      "Training Epoch 28   4.5% | batch:        31 of       686\t|\tloss: 0.621053\n",
      "Training Epoch 28   4.7% | batch:        32 of       686\t|\tloss: 0.813179\n",
      "Training Epoch 28   4.8% | batch:        33 of       686\t|\tloss: 0.826259\n",
      "Training Epoch 28   5.0% | batch:        34 of       686\t|\tloss: 0.876917\n",
      "Training Epoch 28   5.1% | batch:        35 of       686\t|\tloss: 0.607885\n",
      "Training Epoch 28   5.2% | batch:        36 of       686\t|\tloss: 0.791243\n",
      "Training Epoch 28   5.4% | batch:        37 of       686\t|\tloss: 0.920355\n",
      "Training Epoch 28   5.5% | batch:        38 of       686\t|\tloss: 0.832341\n",
      "Training Epoch 28   5.7% | batch:        39 of       686\t|\tloss: 0.89829\n",
      "Training Epoch 28   5.8% | batch:        40 of       686\t|\tloss: 0.909093\n",
      "Training Epoch 28   6.0% | batch:        41 of       686\t|\tloss: 0.816781\n",
      "Training Epoch 28   6.1% | batch:        42 of       686\t|\tloss: 0.718859\n",
      "Training Epoch 28   6.3% | batch:        43 of       686\t|\tloss: 0.758354\n",
      "Training Epoch 28   6.4% | batch:        44 of       686\t|\tloss: 0.71335\n",
      "Training Epoch 28   6.6% | batch:        45 of       686\t|\tloss: 0.784315\n",
      "Training Epoch 28   6.7% | batch:        46 of       686\t|\tloss: 0.974411\n",
      "Training Epoch 28   6.9% | batch:        47 of       686\t|\tloss: 0.824485\n",
      "Training Epoch 28   7.0% | batch:        48 of       686\t|\tloss: 0.769827\n",
      "Training Epoch 28   7.1% | batch:        49 of       686\t|\tloss: 0.713939\n",
      "Training Epoch 28   7.3% | batch:        50 of       686\t|\tloss: 0.807823\n",
      "Training Epoch 28   7.4% | batch:        51 of       686\t|\tloss: 0.861493\n",
      "Training Epoch 28   7.6% | batch:        52 of       686\t|\tloss: 0.879714\n",
      "Training Epoch 28   7.7% | batch:        53 of       686\t|\tloss: 0.6629\n",
      "Training Epoch 28   7.9% | batch:        54 of       686\t|\tloss: 0.646526\n",
      "Training Epoch 28   8.0% | batch:        55 of       686\t|\tloss: 0.798703\n",
      "Training Epoch 28   8.2% | batch:        56 of       686\t|\tloss: 0.904236\n",
      "Training Epoch 28   8.3% | batch:        57 of       686\t|\tloss: 0.832848\n",
      "Training Epoch 28   8.5% | batch:        58 of       686\t|\tloss: 0.686745\n",
      "Training Epoch 28   8.6% | batch:        59 of       686\t|\tloss: 0.722427\n",
      "Training Epoch 28   8.7% | batch:        60 of       686\t|\tloss: 1.04281\n",
      "Training Epoch 28   8.9% | batch:        61 of       686\t|\tloss: 0.784788\n",
      "Training Epoch 28   9.0% | batch:        62 of       686\t|\tloss: 0.851687\n",
      "Training Epoch 28   9.2% | batch:        63 of       686\t|\tloss: 0.673775\n",
      "Training Epoch 28   9.3% | batch:        64 of       686\t|\tloss: 0.684447\n",
      "Training Epoch 28   9.5% | batch:        65 of       686\t|\tloss: 0.929912\n",
      "Training Epoch 28   9.6% | batch:        66 of       686\t|\tloss: 0.710269\n",
      "Training Epoch 28   9.8% | batch:        67 of       686\t|\tloss: 0.749587\n",
      "Training Epoch 28   9.9% | batch:        68 of       686\t|\tloss: 0.748734\n",
      "Training Epoch 28  10.1% | batch:        69 of       686\t|\tloss: 1.05054\n",
      "Training Epoch 28  10.2% | batch:        70 of       686\t|\tloss: 0.800056\n",
      "Training Epoch 28  10.3% | batch:        71 of       686\t|\tloss: 0.716064\n",
      "Training Epoch 28  10.5% | batch:        72 of       686\t|\tloss: 0.907266\n",
      "Training Epoch 28  10.6% | batch:        73 of       686\t|\tloss: 0.766182\n",
      "Training Epoch 28  10.8% | batch:        74 of       686\t|\tloss: 0.554417\n",
      "Training Epoch 28  10.9% | batch:        75 of       686\t|\tloss: 0.806202\n",
      "Training Epoch 28  11.1% | batch:        76 of       686\t|\tloss: 0.691189\n",
      "Training Epoch 28  11.2% | batch:        77 of       686\t|\tloss: 0.704475\n",
      "Training Epoch 28  11.4% | batch:        78 of       686\t|\tloss: 0.843923\n",
      "Training Epoch 28  11.5% | batch:        79 of       686\t|\tloss: 0.928617\n",
      "Training Epoch 28  11.7% | batch:        80 of       686\t|\tloss: 0.93581\n",
      "Training Epoch 28  11.8% | batch:        81 of       686\t|\tloss: 0.652298\n",
      "Training Epoch 28  12.0% | batch:        82 of       686\t|\tloss: 0.729635\n",
      "Training Epoch 28  12.1% | batch:        83 of       686\t|\tloss: 0.62958\n",
      "Training Epoch 28  12.2% | batch:        84 of       686\t|\tloss: 0.768088\n",
      "Training Epoch 28  12.4% | batch:        85 of       686\t|\tloss: 0.754741\n",
      "Training Epoch 28  12.5% | batch:        86 of       686\t|\tloss: 0.588342\n",
      "Training Epoch 28  12.7% | batch:        87 of       686\t|\tloss: 0.667804\n",
      "Training Epoch 28  12.8% | batch:        88 of       686\t|\tloss: 0.602306\n",
      "Training Epoch 28  13.0% | batch:        89 of       686\t|\tloss: 0.678891\n",
      "Training Epoch 28  13.1% | batch:        90 of       686\t|\tloss: 0.862494\n",
      "Training Epoch 28  13.3% | batch:        91 of       686\t|\tloss: 0.846762\n",
      "Training Epoch 28  13.4% | batch:        92 of       686\t|\tloss: 0.702854\n",
      "Training Epoch 28  13.6% | batch:        93 of       686\t|\tloss: 0.686068\n",
      "Training Epoch 28  13.7% | batch:        94 of       686\t|\tloss: 0.716592\n",
      "Training Epoch 28  13.8% | batch:        95 of       686\t|\tloss: 0.647215\n",
      "Training Epoch 28  14.0% | batch:        96 of       686\t|\tloss: 0.855999\n",
      "Training Epoch 28  14.1% | batch:        97 of       686\t|\tloss: 0.82337\n",
      "Training Epoch 28  14.3% | batch:        98 of       686\t|\tloss: 0.680821\n",
      "Training Epoch 28  14.4% | batch:        99 of       686\t|\tloss: 0.749827\n",
      "Training Epoch 28  14.6% | batch:       100 of       686\t|\tloss: 0.805697\n",
      "Training Epoch 28  14.7% | batch:       101 of       686\t|\tloss: 0.701144\n",
      "Training Epoch 28  14.9% | batch:       102 of       686\t|\tloss: 0.582475\n",
      "Training Epoch 28  15.0% | batch:       103 of       686\t|\tloss: 0.646936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 28  15.2% | batch:       104 of       686\t|\tloss: 0.743222\n",
      "Training Epoch 28  15.3% | batch:       105 of       686\t|\tloss: 0.612109\n",
      "Training Epoch 28  15.5% | batch:       106 of       686\t|\tloss: 0.847787\n",
      "Training Epoch 28  15.6% | batch:       107 of       686\t|\tloss: 0.659771\n",
      "Training Epoch 28  15.7% | batch:       108 of       686\t|\tloss: 1.26214\n",
      "Training Epoch 28  15.9% | batch:       109 of       686\t|\tloss: 1.20618\n",
      "Training Epoch 28  16.0% | batch:       110 of       686\t|\tloss: 0.645484\n",
      "Training Epoch 28  16.2% | batch:       111 of       686\t|\tloss: 0.702744\n",
      "Training Epoch 28  16.3% | batch:       112 of       686\t|\tloss: 0.869208\n",
      "Training Epoch 28  16.5% | batch:       113 of       686\t|\tloss: 0.694711\n",
      "Training Epoch 28  16.6% | batch:       114 of       686\t|\tloss: 0.696392\n",
      "Training Epoch 28  16.8% | batch:       115 of       686\t|\tloss: 0.796857\n",
      "Training Epoch 28  16.9% | batch:       116 of       686\t|\tloss: 0.70365\n",
      "Training Epoch 28  17.1% | batch:       117 of       686\t|\tloss: 0.760554\n",
      "Training Epoch 28  17.2% | batch:       118 of       686\t|\tloss: 0.809273\n",
      "Training Epoch 28  17.3% | batch:       119 of       686\t|\tloss: 0.934697\n",
      "Training Epoch 28  17.5% | batch:       120 of       686\t|\tloss: 1.07615\n",
      "Training Epoch 28  17.6% | batch:       121 of       686\t|\tloss: 0.75682\n",
      "Training Epoch 28  17.8% | batch:       122 of       686\t|\tloss: 0.693696\n",
      "Training Epoch 28  17.9% | batch:       123 of       686\t|\tloss: 0.679523\n",
      "Training Epoch 28  18.1% | batch:       124 of       686\t|\tloss: 0.890248\n",
      "Training Epoch 28  18.2% | batch:       125 of       686\t|\tloss: 0.852014\n",
      "Training Epoch 28  18.4% | batch:       126 of       686\t|\tloss: 0.599265\n",
      "Training Epoch 28  18.5% | batch:       127 of       686\t|\tloss: 0.658391\n",
      "Training Epoch 28  18.7% | batch:       128 of       686\t|\tloss: 0.979733\n",
      "Training Epoch 28  18.8% | batch:       129 of       686\t|\tloss: 0.621843\n",
      "Training Epoch 28  19.0% | batch:       130 of       686\t|\tloss: 0.623823\n",
      "Training Epoch 28  19.1% | batch:       131 of       686\t|\tloss: 0.866546\n",
      "Training Epoch 28  19.2% | batch:       132 of       686\t|\tloss: 0.657004\n",
      "Training Epoch 28  19.4% | batch:       133 of       686\t|\tloss: 0.838852\n",
      "Training Epoch 28  19.5% | batch:       134 of       686\t|\tloss: 0.62404\n",
      "Training Epoch 28  19.7% | batch:       135 of       686\t|\tloss: 0.705083\n",
      "Training Epoch 28  19.8% | batch:       136 of       686\t|\tloss: 0.706914\n",
      "Training Epoch 28  20.0% | batch:       137 of       686\t|\tloss: 0.799036\n",
      "Training Epoch 28  20.1% | batch:       138 of       686\t|\tloss: 0.59244\n",
      "Training Epoch 28  20.3% | batch:       139 of       686\t|\tloss: 0.871346\n",
      "Training Epoch 28  20.4% | batch:       140 of       686\t|\tloss: 0.547812\n",
      "Training Epoch 28  20.6% | batch:       141 of       686\t|\tloss: 0.677732\n",
      "Training Epoch 28  20.7% | batch:       142 of       686\t|\tloss: 0.643575\n",
      "Training Epoch 28  20.8% | batch:       143 of       686\t|\tloss: 0.559191\n",
      "Training Epoch 28  21.0% | batch:       144 of       686\t|\tloss: 0.759257\n",
      "Training Epoch 28  21.1% | batch:       145 of       686\t|\tloss: 0.758391\n",
      "Training Epoch 28  21.3% | batch:       146 of       686\t|\tloss: 0.643946\n",
      "Training Epoch 28  21.4% | batch:       147 of       686\t|\tloss: 0.780201\n",
      "Training Epoch 28  21.6% | batch:       148 of       686\t|\tloss: 0.649551\n",
      "Training Epoch 28  21.7% | batch:       149 of       686\t|\tloss: 0.73886\n",
      "Training Epoch 28  21.9% | batch:       150 of       686\t|\tloss: 1.25295\n",
      "Training Epoch 28  22.0% | batch:       151 of       686\t|\tloss: 0.99583\n",
      "Training Epoch 28  22.2% | batch:       152 of       686\t|\tloss: 0.757196\n",
      "Training Epoch 28  22.3% | batch:       153 of       686\t|\tloss: 0.672163\n",
      "Training Epoch 28  22.4% | batch:       154 of       686\t|\tloss: 0.900721\n",
      "Training Epoch 28  22.6% | batch:       155 of       686\t|\tloss: 0.679845\n",
      "Training Epoch 28  22.7% | batch:       156 of       686\t|\tloss: 0.763144\n",
      "Training Epoch 28  22.9% | batch:       157 of       686\t|\tloss: 0.748423\n",
      "Training Epoch 28  23.0% | batch:       158 of       686\t|\tloss: 0.892375\n",
      "Training Epoch 28  23.2% | batch:       159 of       686\t|\tloss: 0.824827\n",
      "Training Epoch 28  23.3% | batch:       160 of       686\t|\tloss: 0.822669\n",
      "Training Epoch 28  23.5% | batch:       161 of       686\t|\tloss: 0.719567\n",
      "Training Epoch 28  23.6% | batch:       162 of       686\t|\tloss: 0.762532\n",
      "Training Epoch 28  23.8% | batch:       163 of       686\t|\tloss: 0.763284\n",
      "Training Epoch 28  23.9% | batch:       164 of       686\t|\tloss: 0.76084\n",
      "Training Epoch 28  24.1% | batch:       165 of       686\t|\tloss: 0.681365\n",
      "Training Epoch 28  24.2% | batch:       166 of       686\t|\tloss: 0.678761\n",
      "Training Epoch 28  24.3% | batch:       167 of       686\t|\tloss: 0.604558\n",
      "Training Epoch 28  24.5% | batch:       168 of       686\t|\tloss: 0.870345\n",
      "Training Epoch 28  24.6% | batch:       169 of       686\t|\tloss: 0.771833\n",
      "Training Epoch 28  24.8% | batch:       170 of       686\t|\tloss: 0.773092\n",
      "Training Epoch 28  24.9% | batch:       171 of       686\t|\tloss: 0.884969\n",
      "Training Epoch 28  25.1% | batch:       172 of       686\t|\tloss: 0.61088\n",
      "Training Epoch 28  25.2% | batch:       173 of       686\t|\tloss: 0.669525\n",
      "Training Epoch 28  25.4% | batch:       174 of       686\t|\tloss: 0.810258\n",
      "Training Epoch 28  25.5% | batch:       175 of       686\t|\tloss: 0.718252\n",
      "Training Epoch 28  25.7% | batch:       176 of       686\t|\tloss: 0.912684\n",
      "Training Epoch 28  25.8% | batch:       177 of       686\t|\tloss: 1.01841\n",
      "Training Epoch 28  25.9% | batch:       178 of       686\t|\tloss: 0.944957\n",
      "Training Epoch 28  26.1% | batch:       179 of       686\t|\tloss: 0.741824\n",
      "Training Epoch 28  26.2% | batch:       180 of       686\t|\tloss: 0.702\n",
      "Training Epoch 28  26.4% | batch:       181 of       686\t|\tloss: 0.642865\n",
      "Training Epoch 28  26.5% | batch:       182 of       686\t|\tloss: 0.72913\n",
      "Training Epoch 28  26.7% | batch:       183 of       686\t|\tloss: 0.725742\n",
      "Training Epoch 28  26.8% | batch:       184 of       686\t|\tloss: 0.863054\n",
      "Training Epoch 28  27.0% | batch:       185 of       686\t|\tloss: 0.878597\n",
      "Training Epoch 28  27.1% | batch:       186 of       686\t|\tloss: 0.646786\n",
      "Training Epoch 28  27.3% | batch:       187 of       686\t|\tloss: 0.671402\n",
      "Training Epoch 28  27.4% | batch:       188 of       686\t|\tloss: 0.754783\n",
      "Training Epoch 28  27.6% | batch:       189 of       686\t|\tloss: 0.758165\n",
      "Training Epoch 28  27.7% | batch:       190 of       686\t|\tloss: 0.648121\n",
      "Training Epoch 28  27.8% | batch:       191 of       686\t|\tloss: 0.611381\n",
      "Training Epoch 28  28.0% | batch:       192 of       686\t|\tloss: 0.875579\n",
      "Training Epoch 28  28.1% | batch:       193 of       686\t|\tloss: 0.554511\n",
      "Training Epoch 28  28.3% | batch:       194 of       686\t|\tloss: 0.857891\n",
      "Training Epoch 28  28.4% | batch:       195 of       686\t|\tloss: 0.703185\n",
      "Training Epoch 28  28.6% | batch:       196 of       686\t|\tloss: 0.66855\n",
      "Training Epoch 28  28.7% | batch:       197 of       686\t|\tloss: 0.709668\n",
      "Training Epoch 28  28.9% | batch:       198 of       686\t|\tloss: 0.866445\n",
      "Training Epoch 28  29.0% | batch:       199 of       686\t|\tloss: 0.641074\n",
      "Training Epoch 28  29.2% | batch:       200 of       686\t|\tloss: 0.652696\n",
      "Training Epoch 28  29.3% | batch:       201 of       686\t|\tloss: 0.792319\n",
      "Training Epoch 28  29.4% | batch:       202 of       686\t|\tloss: 0.79304\n",
      "Training Epoch 28  29.6% | batch:       203 of       686\t|\tloss: 0.723171\n",
      "Training Epoch 28  29.7% | batch:       204 of       686\t|\tloss: 0.589566\n",
      "Training Epoch 28  29.9% | batch:       205 of       686\t|\tloss: 0.997242\n",
      "Training Epoch 28  30.0% | batch:       206 of       686\t|\tloss: 0.753905\n",
      "Training Epoch 28  30.2% | batch:       207 of       686\t|\tloss: 0.709774\n",
      "Training Epoch 28  30.3% | batch:       208 of       686\t|\tloss: 0.80099\n",
      "Training Epoch 28  30.5% | batch:       209 of       686\t|\tloss: 0.824805\n",
      "Training Epoch 28  30.6% | batch:       210 of       686\t|\tloss: 0.820261\n",
      "Training Epoch 28  30.8% | batch:       211 of       686\t|\tloss: 0.802018\n",
      "Training Epoch 28  30.9% | batch:       212 of       686\t|\tloss: 0.809242\n",
      "Training Epoch 28  31.0% | batch:       213 of       686\t|\tloss: 0.717919\n",
      "Training Epoch 28  31.2% | batch:       214 of       686\t|\tloss: 0.716509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 28  31.3% | batch:       215 of       686\t|\tloss: 0.76384\n",
      "Training Epoch 28  31.5% | batch:       216 of       686\t|\tloss: 0.662519\n",
      "Training Epoch 28  31.6% | batch:       217 of       686\t|\tloss: 0.635292\n",
      "Training Epoch 28  31.8% | batch:       218 of       686\t|\tloss: 0.582417\n",
      "Training Epoch 28  31.9% | batch:       219 of       686\t|\tloss: 0.758957\n",
      "Training Epoch 28  32.1% | batch:       220 of       686\t|\tloss: 0.85484\n",
      "Training Epoch 28  32.2% | batch:       221 of       686\t|\tloss: 0.689055\n",
      "Training Epoch 28  32.4% | batch:       222 of       686\t|\tloss: 0.73124\n",
      "Training Epoch 28  32.5% | batch:       223 of       686\t|\tloss: 0.82681\n",
      "Training Epoch 28  32.7% | batch:       224 of       686\t|\tloss: 0.691092\n",
      "Training Epoch 28  32.8% | batch:       225 of       686\t|\tloss: 0.803202\n",
      "Training Epoch 28  32.9% | batch:       226 of       686\t|\tloss: 0.724408\n",
      "Training Epoch 28  33.1% | batch:       227 of       686\t|\tloss: 0.684708\n",
      "Training Epoch 28  33.2% | batch:       228 of       686\t|\tloss: 0.799207\n",
      "Training Epoch 28  33.4% | batch:       229 of       686\t|\tloss: 0.893603\n",
      "Training Epoch 28  33.5% | batch:       230 of       686\t|\tloss: 0.692871\n",
      "Training Epoch 28  33.7% | batch:       231 of       686\t|\tloss: 0.671269\n",
      "Training Epoch 28  33.8% | batch:       232 of       686\t|\tloss: 0.875881\n",
      "Training Epoch 28  34.0% | batch:       233 of       686\t|\tloss: 0.74031\n",
      "Training Epoch 28  34.1% | batch:       234 of       686\t|\tloss: 0.682647\n",
      "Training Epoch 28  34.3% | batch:       235 of       686\t|\tloss: 0.710939\n",
      "Training Epoch 28  34.4% | batch:       236 of       686\t|\tloss: 0.70151\n",
      "Training Epoch 28  34.5% | batch:       237 of       686\t|\tloss: 0.722044\n",
      "Training Epoch 28  34.7% | batch:       238 of       686\t|\tloss: 0.751503\n",
      "Training Epoch 28  34.8% | batch:       239 of       686\t|\tloss: 0.888223\n",
      "Training Epoch 28  35.0% | batch:       240 of       686\t|\tloss: 0.836298\n",
      "Training Epoch 28  35.1% | batch:       241 of       686\t|\tloss: 0.601841\n",
      "Training Epoch 28  35.3% | batch:       242 of       686\t|\tloss: 0.808899\n",
      "Training Epoch 28  35.4% | batch:       243 of       686\t|\tloss: 0.746855\n",
      "Training Epoch 28  35.6% | batch:       244 of       686\t|\tloss: 0.75311\n",
      "Training Epoch 28  35.7% | batch:       245 of       686\t|\tloss: 0.765481\n",
      "Training Epoch 28  35.9% | batch:       246 of       686\t|\tloss: 0.651517\n",
      "Training Epoch 28  36.0% | batch:       247 of       686\t|\tloss: 0.85545\n",
      "Training Epoch 28  36.2% | batch:       248 of       686\t|\tloss: 0.786715\n",
      "Training Epoch 28  36.3% | batch:       249 of       686\t|\tloss: 0.697464\n",
      "Training Epoch 28  36.4% | batch:       250 of       686\t|\tloss: 0.807016\n",
      "Training Epoch 28  36.6% | batch:       251 of       686\t|\tloss: 0.768456\n",
      "Training Epoch 28  36.7% | batch:       252 of       686\t|\tloss: 0.734556\n",
      "Training Epoch 28  36.9% | batch:       253 of       686\t|\tloss: 0.640151\n",
      "Training Epoch 28  37.0% | batch:       254 of       686\t|\tloss: 0.695581\n",
      "Training Epoch 28  37.2% | batch:       255 of       686\t|\tloss: 0.726108\n",
      "Training Epoch 28  37.3% | batch:       256 of       686\t|\tloss: 0.664195\n",
      "Training Epoch 28  37.5% | batch:       257 of       686\t|\tloss: 0.74116\n",
      "Training Epoch 28  37.6% | batch:       258 of       686\t|\tloss: 0.720418\n",
      "Training Epoch 28  37.8% | batch:       259 of       686\t|\tloss: 0.674116\n",
      "Training Epoch 28  37.9% | batch:       260 of       686\t|\tloss: 0.670712\n",
      "Training Epoch 28  38.0% | batch:       261 of       686\t|\tloss: 0.744915\n",
      "Training Epoch 28  38.2% | batch:       262 of       686\t|\tloss: 0.660644\n",
      "Training Epoch 28  38.3% | batch:       263 of       686\t|\tloss: 0.649312\n",
      "Training Epoch 28  38.5% | batch:       264 of       686\t|\tloss: 0.785825\n",
      "Training Epoch 28  38.6% | batch:       265 of       686\t|\tloss: 0.863912\n",
      "Training Epoch 28  38.8% | batch:       266 of       686\t|\tloss: 0.638211\n",
      "Training Epoch 28  38.9% | batch:       267 of       686\t|\tloss: 0.794098\n",
      "Training Epoch 28  39.1% | batch:       268 of       686\t|\tloss: 1.43415\n",
      "Training Epoch 28  39.2% | batch:       269 of       686\t|\tloss: 0.566531\n",
      "Training Epoch 28  39.4% | batch:       270 of       686\t|\tloss: 0.682329\n",
      "Training Epoch 28  39.5% | batch:       271 of       686\t|\tloss: 0.997202\n",
      "Training Epoch 28  39.7% | batch:       272 of       686\t|\tloss: 1.0846\n",
      "Training Epoch 28  39.8% | batch:       273 of       686\t|\tloss: 0.664503\n",
      "Training Epoch 28  39.9% | batch:       274 of       686\t|\tloss: 0.505302\n",
      "Training Epoch 28  40.1% | batch:       275 of       686\t|\tloss: 0.680757\n",
      "Training Epoch 28  40.2% | batch:       276 of       686\t|\tloss: 0.690474\n",
      "Training Epoch 28  40.4% | batch:       277 of       686\t|\tloss: 0.550072\n",
      "Training Epoch 28  40.5% | batch:       278 of       686\t|\tloss: 0.711399\n",
      "Training Epoch 28  40.7% | batch:       279 of       686\t|\tloss: 0.84888\n",
      "Training Epoch 28  40.8% | batch:       280 of       686\t|\tloss: 0.846068\n",
      "Training Epoch 28  41.0% | batch:       281 of       686\t|\tloss: 1.02842\n",
      "Training Epoch 28  41.1% | batch:       282 of       686\t|\tloss: 0.671258\n",
      "Training Epoch 28  41.3% | batch:       283 of       686\t|\tloss: 0.72277\n",
      "Training Epoch 28  41.4% | batch:       284 of       686\t|\tloss: 0.904563\n",
      "Training Epoch 28  41.5% | batch:       285 of       686\t|\tloss: 0.844176\n",
      "Training Epoch 28  41.7% | batch:       286 of       686\t|\tloss: 0.886854\n",
      "Training Epoch 28  41.8% | batch:       287 of       686\t|\tloss: 0.738722\n",
      "Training Epoch 28  42.0% | batch:       288 of       686\t|\tloss: 0.569777\n",
      "Training Epoch 28  42.1% | batch:       289 of       686\t|\tloss: 0.693323\n",
      "Training Epoch 28  42.3% | batch:       290 of       686\t|\tloss: 0.683725\n",
      "Training Epoch 28  42.4% | batch:       291 of       686\t|\tloss: 0.69305\n",
      "Training Epoch 28  42.6% | batch:       292 of       686\t|\tloss: 0.855071\n",
      "Training Epoch 28  42.7% | batch:       293 of       686\t|\tloss: 0.598033\n",
      "Training Epoch 28  42.9% | batch:       294 of       686\t|\tloss: 0.757889\n",
      "Training Epoch 28  43.0% | batch:       295 of       686\t|\tloss: 0.787843\n",
      "Training Epoch 28  43.1% | batch:       296 of       686\t|\tloss: 0.840481\n",
      "Training Epoch 28  43.3% | batch:       297 of       686\t|\tloss: 0.724437\n",
      "Training Epoch 28  43.4% | batch:       298 of       686\t|\tloss: 0.800464\n",
      "Training Epoch 28  43.6% | batch:       299 of       686\t|\tloss: 0.714679\n",
      "Training Epoch 28  43.7% | batch:       300 of       686\t|\tloss: 0.697391\n",
      "Training Epoch 28  43.9% | batch:       301 of       686\t|\tloss: 0.615603\n",
      "Training Epoch 28  44.0% | batch:       302 of       686\t|\tloss: 0.77496\n",
      "Training Epoch 28  44.2% | batch:       303 of       686\t|\tloss: 0.875812\n",
      "Training Epoch 28  44.3% | batch:       304 of       686\t|\tloss: 0.795272\n",
      "Training Epoch 28  44.5% | batch:       305 of       686\t|\tloss: 0.784154\n",
      "Training Epoch 28  44.6% | batch:       306 of       686\t|\tloss: 0.635088\n",
      "Training Epoch 28  44.8% | batch:       307 of       686\t|\tloss: 0.707984\n",
      "Training Epoch 28  44.9% | batch:       308 of       686\t|\tloss: 0.609842\n",
      "Training Epoch 28  45.0% | batch:       309 of       686\t|\tloss: 1.03989\n",
      "Training Epoch 28  45.2% | batch:       310 of       686\t|\tloss: 0.898018\n",
      "Training Epoch 28  45.3% | batch:       311 of       686\t|\tloss: 0.775399\n",
      "Training Epoch 28  45.5% | batch:       312 of       686\t|\tloss: 0.790869\n",
      "Training Epoch 28  45.6% | batch:       313 of       686\t|\tloss: 0.632436\n",
      "Training Epoch 28  45.8% | batch:       314 of       686\t|\tloss: 0.641474\n",
      "Training Epoch 28  45.9% | batch:       315 of       686\t|\tloss: 0.718443\n",
      "Training Epoch 28  46.1% | batch:       316 of       686\t|\tloss: 0.691356\n",
      "Training Epoch 28  46.2% | batch:       317 of       686\t|\tloss: 0.947198\n",
      "Training Epoch 28  46.4% | batch:       318 of       686\t|\tloss: 0.929019\n",
      "Training Epoch 28  46.5% | batch:       319 of       686\t|\tloss: 0.696266\n",
      "Training Epoch 28  46.6% | batch:       320 of       686\t|\tloss: 0.940166\n",
      "Training Epoch 28  46.8% | batch:       321 of       686\t|\tloss: 0.704051\n",
      "Training Epoch 28  46.9% | batch:       322 of       686\t|\tloss: 0.919358\n",
      "Training Epoch 28  47.1% | batch:       323 of       686\t|\tloss: 0.645191\n",
      "Training Epoch 28  47.2% | batch:       324 of       686\t|\tloss: 0.662166\n",
      "Training Epoch 28  47.4% | batch:       325 of       686\t|\tloss: 0.656923\n",
      "Training Epoch 28  47.5% | batch:       326 of       686\t|\tloss: 0.905776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 28  47.7% | batch:       327 of       686\t|\tloss: 1.04203\n",
      "Training Epoch 28  47.8% | batch:       328 of       686\t|\tloss: 0.650561\n",
      "Training Epoch 28  48.0% | batch:       329 of       686\t|\tloss: 0.661107\n",
      "Training Epoch 28  48.1% | batch:       330 of       686\t|\tloss: 0.717804\n",
      "Training Epoch 28  48.3% | batch:       331 of       686\t|\tloss: 0.982313\n",
      "Training Epoch 28  48.4% | batch:       332 of       686\t|\tloss: 0.543413\n",
      "Training Epoch 28  48.5% | batch:       333 of       686\t|\tloss: 0.67486\n",
      "Training Epoch 28  48.7% | batch:       334 of       686\t|\tloss: 0.706428\n",
      "Training Epoch 28  48.8% | batch:       335 of       686\t|\tloss: 0.725316\n",
      "Training Epoch 28  49.0% | batch:       336 of       686\t|\tloss: 0.786419\n",
      "Training Epoch 28  49.1% | batch:       337 of       686\t|\tloss: 0.626442\n",
      "Training Epoch 28  49.3% | batch:       338 of       686\t|\tloss: 0.647926\n",
      "Training Epoch 28  49.4% | batch:       339 of       686\t|\tloss: 0.718004\n",
      "Training Epoch 28  49.6% | batch:       340 of       686\t|\tloss: 0.747099\n",
      "Training Epoch 28  49.7% | batch:       341 of       686\t|\tloss: 0.648899\n",
      "Training Epoch 28  49.9% | batch:       342 of       686\t|\tloss: 0.789373\n",
      "Training Epoch 28  50.0% | batch:       343 of       686\t|\tloss: 0.670905\n",
      "Training Epoch 28  50.1% | batch:       344 of       686\t|\tloss: 0.75523\n",
      "Training Epoch 28  50.3% | batch:       345 of       686\t|\tloss: 0.807389\n",
      "Training Epoch 28  50.4% | batch:       346 of       686\t|\tloss: 0.750526\n",
      "Training Epoch 28  50.6% | batch:       347 of       686\t|\tloss: 0.935524\n",
      "Training Epoch 28  50.7% | batch:       348 of       686\t|\tloss: 0.69361\n",
      "Training Epoch 28  50.9% | batch:       349 of       686\t|\tloss: 1.18097\n",
      "Training Epoch 28  51.0% | batch:       350 of       686\t|\tloss: 0.69985\n",
      "Training Epoch 28  51.2% | batch:       351 of       686\t|\tloss: 0.688709\n",
      "Training Epoch 28  51.3% | batch:       352 of       686\t|\tloss: 0.762733\n",
      "Training Epoch 28  51.5% | batch:       353 of       686\t|\tloss: 0.756603\n",
      "Training Epoch 28  51.6% | batch:       354 of       686\t|\tloss: 0.858848\n",
      "Training Epoch 28  51.7% | batch:       355 of       686\t|\tloss: 0.691404\n",
      "Training Epoch 28  51.9% | batch:       356 of       686\t|\tloss: 0.717682\n",
      "Training Epoch 28  52.0% | batch:       357 of       686\t|\tloss: 0.786384\n",
      "Training Epoch 28  52.2% | batch:       358 of       686\t|\tloss: 1.11994\n",
      "Training Epoch 28  52.3% | batch:       359 of       686\t|\tloss: 0.705019\n",
      "Training Epoch 28  52.5% | batch:       360 of       686\t|\tloss: 0.685439\n",
      "Training Epoch 28  52.6% | batch:       361 of       686\t|\tloss: 0.883908\n",
      "Training Epoch 28  52.8% | batch:       362 of       686\t|\tloss: 0.555671\n",
      "Training Epoch 28  52.9% | batch:       363 of       686\t|\tloss: 0.594621\n",
      "Training Epoch 28  53.1% | batch:       364 of       686\t|\tloss: 0.660595\n",
      "Training Epoch 28  53.2% | batch:       365 of       686\t|\tloss: 0.736139\n",
      "Training Epoch 28  53.4% | batch:       366 of       686\t|\tloss: 0.682577\n",
      "Training Epoch 28  53.5% | batch:       367 of       686\t|\tloss: 0.724853\n",
      "Training Epoch 28  53.6% | batch:       368 of       686\t|\tloss: 0.634587\n",
      "Training Epoch 28  53.8% | batch:       369 of       686\t|\tloss: 0.813309\n",
      "Training Epoch 28  53.9% | batch:       370 of       686\t|\tloss: 0.751139\n",
      "Training Epoch 28  54.1% | batch:       371 of       686\t|\tloss: 0.685749\n",
      "Training Epoch 28  54.2% | batch:       372 of       686\t|\tloss: 0.601519\n",
      "Training Epoch 28  54.4% | batch:       373 of       686\t|\tloss: 0.739885\n",
      "Training Epoch 28  54.5% | batch:       374 of       686\t|\tloss: 0.697615\n",
      "Training Epoch 28  54.7% | batch:       375 of       686\t|\tloss: 0.79974\n",
      "Training Epoch 28  54.8% | batch:       376 of       686\t|\tloss: 0.985086\n",
      "Training Epoch 28  55.0% | batch:       377 of       686\t|\tloss: 0.72087\n",
      "Training Epoch 28  55.1% | batch:       378 of       686\t|\tloss: 0.787755\n",
      "Training Epoch 28  55.2% | batch:       379 of       686\t|\tloss: 0.636782\n",
      "Training Epoch 28  55.4% | batch:       380 of       686\t|\tloss: 0.729557\n",
      "Training Epoch 28  55.5% | batch:       381 of       686\t|\tloss: 0.63358\n",
      "Training Epoch 28  55.7% | batch:       382 of       686\t|\tloss: 0.764712\n",
      "Training Epoch 28  55.8% | batch:       383 of       686\t|\tloss: 0.774008\n",
      "Training Epoch 28  56.0% | batch:       384 of       686\t|\tloss: 0.912724\n",
      "Training Epoch 28  56.1% | batch:       385 of       686\t|\tloss: 0.785196\n",
      "Training Epoch 28  56.3% | batch:       386 of       686\t|\tloss: 0.628782\n",
      "Training Epoch 28  56.4% | batch:       387 of       686\t|\tloss: 0.771509\n",
      "Training Epoch 28  56.6% | batch:       388 of       686\t|\tloss: 0.684982\n",
      "Training Epoch 28  56.7% | batch:       389 of       686\t|\tloss: 0.647336\n",
      "Training Epoch 28  56.9% | batch:       390 of       686\t|\tloss: 0.604176\n",
      "Training Epoch 28  57.0% | batch:       391 of       686\t|\tloss: 0.734228\n",
      "Training Epoch 28  57.1% | batch:       392 of       686\t|\tloss: 0.727441\n",
      "Training Epoch 28  57.3% | batch:       393 of       686\t|\tloss: 0.590706\n",
      "Training Epoch 28  57.4% | batch:       394 of       686\t|\tloss: 0.648348\n",
      "Training Epoch 28  57.6% | batch:       395 of       686\t|\tloss: 0.795217\n",
      "Training Epoch 28  57.7% | batch:       396 of       686\t|\tloss: 0.697508\n",
      "Training Epoch 28  57.9% | batch:       397 of       686\t|\tloss: 0.730582\n",
      "Training Epoch 28  58.0% | batch:       398 of       686\t|\tloss: 0.699807\n",
      "Training Epoch 28  58.2% | batch:       399 of       686\t|\tloss: 0.565502\n",
      "Training Epoch 28  58.3% | batch:       400 of       686\t|\tloss: 1.00574\n",
      "Training Epoch 28  58.5% | batch:       401 of       686\t|\tloss: 0.70825\n",
      "Training Epoch 28  58.6% | batch:       402 of       686\t|\tloss: 0.710592\n",
      "Training Epoch 28  58.7% | batch:       403 of       686\t|\tloss: 0.636411\n",
      "Training Epoch 28  58.9% | batch:       404 of       686\t|\tloss: 0.836381\n",
      "Training Epoch 28  59.0% | batch:       405 of       686\t|\tloss: 0.992593\n",
      "Training Epoch 28  59.2% | batch:       406 of       686\t|\tloss: 0.671953\n",
      "Training Epoch 28  59.3% | batch:       407 of       686\t|\tloss: 0.767146\n",
      "Training Epoch 28  59.5% | batch:       408 of       686\t|\tloss: 0.641396\n",
      "Training Epoch 28  59.6% | batch:       409 of       686\t|\tloss: 0.69558\n",
      "Training Epoch 28  59.8% | batch:       410 of       686\t|\tloss: 0.603305\n",
      "Training Epoch 28  59.9% | batch:       411 of       686\t|\tloss: 0.754236\n",
      "Training Epoch 28  60.1% | batch:       412 of       686\t|\tloss: 0.891566\n",
      "Training Epoch 28  60.2% | batch:       413 of       686\t|\tloss: 0.692883\n",
      "Training Epoch 28  60.3% | batch:       414 of       686\t|\tloss: 0.600308\n",
      "Training Epoch 28  60.5% | batch:       415 of       686\t|\tloss: 0.772025\n",
      "Training Epoch 28  60.6% | batch:       416 of       686\t|\tloss: 0.664035\n",
      "Training Epoch 28  60.8% | batch:       417 of       686\t|\tloss: 0.651232\n",
      "Training Epoch 28  60.9% | batch:       418 of       686\t|\tloss: 0.757523\n",
      "Training Epoch 28  61.1% | batch:       419 of       686\t|\tloss: 0.909651\n",
      "Training Epoch 28  61.2% | batch:       420 of       686\t|\tloss: 0.694906\n",
      "Training Epoch 28  61.4% | batch:       421 of       686\t|\tloss: 0.88191\n",
      "Training Epoch 28  61.5% | batch:       422 of       686\t|\tloss: 0.731284\n",
      "Training Epoch 28  61.7% | batch:       423 of       686\t|\tloss: 0.82931\n",
      "Training Epoch 28  61.8% | batch:       424 of       686\t|\tloss: 0.626557\n",
      "Training Epoch 28  62.0% | batch:       425 of       686\t|\tloss: 0.65033\n",
      "Training Epoch 28  62.1% | batch:       426 of       686\t|\tloss: 0.553612\n",
      "Training Epoch 28  62.2% | batch:       427 of       686\t|\tloss: 0.683928\n",
      "Training Epoch 28  62.4% | batch:       428 of       686\t|\tloss: 0.612473\n",
      "Training Epoch 28  62.5% | batch:       429 of       686\t|\tloss: 0.649703\n",
      "Training Epoch 28  62.7% | batch:       430 of       686\t|\tloss: 0.758773\n",
      "Training Epoch 28  62.8% | batch:       431 of       686\t|\tloss: 0.919254\n",
      "Training Epoch 28  63.0% | batch:       432 of       686\t|\tloss: 0.85143\n",
      "Training Epoch 28  63.1% | batch:       433 of       686\t|\tloss: 0.867307\n",
      "Training Epoch 28  63.3% | batch:       434 of       686\t|\tloss: 0.691746\n",
      "Training Epoch 28  63.4% | batch:       435 of       686\t|\tloss: 0.540522\n",
      "Training Epoch 28  63.6% | batch:       436 of       686\t|\tloss: 0.741502\n",
      "Training Epoch 28  63.7% | batch:       437 of       686\t|\tloss: 0.839334\n",
      "Training Epoch 28  63.8% | batch:       438 of       686\t|\tloss: 0.617904\n",
      "Training Epoch 28  64.0% | batch:       439 of       686\t|\tloss: 0.790716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 28  64.1% | batch:       440 of       686\t|\tloss: 0.736265\n",
      "Training Epoch 28  64.3% | batch:       441 of       686\t|\tloss: 0.622283\n",
      "Training Epoch 28  64.4% | batch:       442 of       686\t|\tloss: 0.778548\n",
      "Training Epoch 28  64.6% | batch:       443 of       686\t|\tloss: 0.825742\n",
      "Training Epoch 28  64.7% | batch:       444 of       686\t|\tloss: 0.823007\n",
      "Training Epoch 28  64.9% | batch:       445 of       686\t|\tloss: 0.773358\n",
      "Training Epoch 28  65.0% | batch:       446 of       686\t|\tloss: 0.756261\n",
      "Training Epoch 28  65.2% | batch:       447 of       686\t|\tloss: 0.5634\n",
      "Training Epoch 28  65.3% | batch:       448 of       686\t|\tloss: 0.678483\n",
      "Training Epoch 28  65.5% | batch:       449 of       686\t|\tloss: 0.7435\n",
      "Training Epoch 28  65.6% | batch:       450 of       686\t|\tloss: 0.708146\n",
      "Training Epoch 28  65.7% | batch:       451 of       686\t|\tloss: 0.719056\n",
      "Training Epoch 28  65.9% | batch:       452 of       686\t|\tloss: 1.01045\n",
      "Training Epoch 28  66.0% | batch:       453 of       686\t|\tloss: 0.774219\n",
      "Training Epoch 28  66.2% | batch:       454 of       686\t|\tloss: 0.671807\n",
      "Training Epoch 28  66.3% | batch:       455 of       686\t|\tloss: 0.664781\n",
      "Training Epoch 28  66.5% | batch:       456 of       686\t|\tloss: 0.6794\n",
      "Training Epoch 28  66.6% | batch:       457 of       686\t|\tloss: 0.758967\n",
      "Training Epoch 28  66.8% | batch:       458 of       686\t|\tloss: 0.637291\n",
      "Training Epoch 28  66.9% | batch:       459 of       686\t|\tloss: 0.870734\n",
      "Training Epoch 28  67.1% | batch:       460 of       686\t|\tloss: 0.731857\n",
      "Training Epoch 28  67.2% | batch:       461 of       686\t|\tloss: 0.582684\n",
      "Training Epoch 28  67.3% | batch:       462 of       686\t|\tloss: 0.869762\n",
      "Training Epoch 28  67.5% | batch:       463 of       686\t|\tloss: 0.831461\n",
      "Training Epoch 28  67.6% | batch:       464 of       686\t|\tloss: 0.671717\n",
      "Training Epoch 28  67.8% | batch:       465 of       686\t|\tloss: 0.698386\n",
      "Training Epoch 28  67.9% | batch:       466 of       686\t|\tloss: 0.664407\n",
      "Training Epoch 28  68.1% | batch:       467 of       686\t|\tloss: 0.847805\n",
      "Training Epoch 28  68.2% | batch:       468 of       686\t|\tloss: 0.64071\n",
      "Training Epoch 28  68.4% | batch:       469 of       686\t|\tloss: 0.636781\n",
      "Training Epoch 28  68.5% | batch:       470 of       686\t|\tloss: 0.631617\n",
      "Training Epoch 28  68.7% | batch:       471 of       686\t|\tloss: 0.669784\n",
      "Training Epoch 28  68.8% | batch:       472 of       686\t|\tloss: 0.67048\n",
      "Training Epoch 28  69.0% | batch:       473 of       686\t|\tloss: 0.608901\n",
      "Training Epoch 28  69.1% | batch:       474 of       686\t|\tloss: 1.3169\n",
      "Training Epoch 28  69.2% | batch:       475 of       686\t|\tloss: 0.794894\n",
      "Training Epoch 28  69.4% | batch:       476 of       686\t|\tloss: 0.749575\n",
      "Training Epoch 28  69.5% | batch:       477 of       686\t|\tloss: 0.707929\n",
      "Training Epoch 28  69.7% | batch:       478 of       686\t|\tloss: 0.797249\n",
      "Training Epoch 28  69.8% | batch:       479 of       686\t|\tloss: 0.744706\n",
      "Training Epoch 28  70.0% | batch:       480 of       686\t|\tloss: 0.723074\n",
      "Training Epoch 28  70.1% | batch:       481 of       686\t|\tloss: 0.877578\n",
      "Training Epoch 28  70.3% | batch:       482 of       686\t|\tloss: 0.573475\n",
      "Training Epoch 28  70.4% | batch:       483 of       686\t|\tloss: 0.830891\n",
      "Training Epoch 28  70.6% | batch:       484 of       686\t|\tloss: 0.819222\n",
      "Training Epoch 28  70.7% | batch:       485 of       686\t|\tloss: 0.768072\n",
      "Training Epoch 28  70.8% | batch:       486 of       686\t|\tloss: 0.671379\n",
      "Training Epoch 28  71.0% | batch:       487 of       686\t|\tloss: 0.739303\n",
      "Training Epoch 28  71.1% | batch:       488 of       686\t|\tloss: 0.587815\n",
      "Training Epoch 28  71.3% | batch:       489 of       686\t|\tloss: 0.810927\n",
      "Training Epoch 28  71.4% | batch:       490 of       686\t|\tloss: 0.671957\n",
      "Training Epoch 28  71.6% | batch:       491 of       686\t|\tloss: 0.782252\n",
      "Training Epoch 28  71.7% | batch:       492 of       686\t|\tloss: 0.790811\n",
      "Training Epoch 28  71.9% | batch:       493 of       686\t|\tloss: 0.593617\n",
      "Training Epoch 28  72.0% | batch:       494 of       686\t|\tloss: 0.756119\n",
      "Training Epoch 28  72.2% | batch:       495 of       686\t|\tloss: 0.698293\n",
      "Training Epoch 28  72.3% | batch:       496 of       686\t|\tloss: 0.475454\n",
      "Training Epoch 28  72.4% | batch:       497 of       686\t|\tloss: 0.685904\n",
      "Training Epoch 28  72.6% | batch:       498 of       686\t|\tloss: 0.787886\n",
      "Training Epoch 28  72.7% | batch:       499 of       686\t|\tloss: 0.652797\n",
      "Training Epoch 28  72.9% | batch:       500 of       686\t|\tloss: 0.887032\n",
      "Training Epoch 28  73.0% | batch:       501 of       686\t|\tloss: 0.695542\n",
      "Training Epoch 28  73.2% | batch:       502 of       686\t|\tloss: 0.820414\n",
      "Training Epoch 28  73.3% | batch:       503 of       686\t|\tloss: 0.842198\n",
      "Training Epoch 28  73.5% | batch:       504 of       686\t|\tloss: 0.665222\n",
      "Training Epoch 28  73.6% | batch:       505 of       686\t|\tloss: 0.645428\n",
      "Training Epoch 28  73.8% | batch:       506 of       686\t|\tloss: 0.767767\n",
      "Training Epoch 28  73.9% | batch:       507 of       686\t|\tloss: 0.833238\n",
      "Training Epoch 28  74.1% | batch:       508 of       686\t|\tloss: 0.751239\n",
      "Training Epoch 28  74.2% | batch:       509 of       686\t|\tloss: 0.805279\n",
      "Training Epoch 28  74.3% | batch:       510 of       686\t|\tloss: 1.02458\n",
      "Training Epoch 28  74.5% | batch:       511 of       686\t|\tloss: 0.739941\n",
      "Training Epoch 28  74.6% | batch:       512 of       686\t|\tloss: 0.75529\n",
      "Training Epoch 28  74.8% | batch:       513 of       686\t|\tloss: 0.770378\n",
      "Training Epoch 28  74.9% | batch:       514 of       686\t|\tloss: 0.789159\n",
      "Training Epoch 28  75.1% | batch:       515 of       686\t|\tloss: 0.780659\n",
      "Training Epoch 28  75.2% | batch:       516 of       686\t|\tloss: 0.563965\n",
      "Training Epoch 28  75.4% | batch:       517 of       686\t|\tloss: 0.993497\n",
      "Training Epoch 28  75.5% | batch:       518 of       686\t|\tloss: 0.809862\n",
      "Training Epoch 28  75.7% | batch:       519 of       686\t|\tloss: 0.745802\n",
      "Training Epoch 28  75.8% | batch:       520 of       686\t|\tloss: 0.681453\n",
      "Training Epoch 28  75.9% | batch:       521 of       686\t|\tloss: 0.718469\n",
      "Training Epoch 28  76.1% | batch:       522 of       686\t|\tloss: 0.654241\n",
      "Training Epoch 28  76.2% | batch:       523 of       686\t|\tloss: 0.665554\n",
      "Training Epoch 28  76.4% | batch:       524 of       686\t|\tloss: 0.593322\n",
      "Training Epoch 28  76.5% | batch:       525 of       686\t|\tloss: 0.820605\n",
      "Training Epoch 28  76.7% | batch:       526 of       686\t|\tloss: 0.763781\n",
      "Training Epoch 28  76.8% | batch:       527 of       686\t|\tloss: 0.684707\n",
      "Training Epoch 28  77.0% | batch:       528 of       686\t|\tloss: 0.872723\n",
      "Training Epoch 28  77.1% | batch:       529 of       686\t|\tloss: 0.699406\n",
      "Training Epoch 28  77.3% | batch:       530 of       686\t|\tloss: 0.766216\n",
      "Training Epoch 28  77.4% | batch:       531 of       686\t|\tloss: 0.621308\n",
      "Training Epoch 28  77.6% | batch:       532 of       686\t|\tloss: 0.669268\n",
      "Training Epoch 28  77.7% | batch:       533 of       686\t|\tloss: 0.728237\n",
      "Training Epoch 28  77.8% | batch:       534 of       686\t|\tloss: 0.695359\n",
      "Training Epoch 28  78.0% | batch:       535 of       686\t|\tloss: 0.689766\n",
      "Training Epoch 28  78.1% | batch:       536 of       686\t|\tloss: 0.710322\n",
      "Training Epoch 28  78.3% | batch:       537 of       686\t|\tloss: 0.725366\n",
      "Training Epoch 28  78.4% | batch:       538 of       686\t|\tloss: 0.768728\n",
      "Training Epoch 28  78.6% | batch:       539 of       686\t|\tloss: 0.61646\n",
      "Training Epoch 28  78.7% | batch:       540 of       686\t|\tloss: 0.81974\n",
      "Training Epoch 28  78.9% | batch:       541 of       686\t|\tloss: 0.672936\n",
      "Training Epoch 28  79.0% | batch:       542 of       686\t|\tloss: 0.751202\n",
      "Training Epoch 28  79.2% | batch:       543 of       686\t|\tloss: 0.650005\n",
      "Training Epoch 28  79.3% | batch:       544 of       686\t|\tloss: 0.746721\n",
      "Training Epoch 28  79.4% | batch:       545 of       686\t|\tloss: 1.01956\n",
      "Training Epoch 28  79.6% | batch:       546 of       686\t|\tloss: 0.591724\n",
      "Training Epoch 28  79.7% | batch:       547 of       686\t|\tloss: 0.672749\n",
      "Training Epoch 28  79.9% | batch:       548 of       686\t|\tloss: 0.700276\n",
      "Training Epoch 28  80.0% | batch:       549 of       686\t|\tloss: 0.851017\n",
      "Training Epoch 28  80.2% | batch:       550 of       686\t|\tloss: 0.742893\n",
      "Training Epoch 28  80.3% | batch:       551 of       686\t|\tloss: 0.721012\n",
      "Training Epoch 28  80.5% | batch:       552 of       686\t|\tloss: 0.688154\n",
      "Training Epoch 28  80.6% | batch:       553 of       686\t|\tloss: 0.810194\n",
      "Training Epoch 28  80.8% | batch:       554 of       686\t|\tloss: 0.916591\n",
      "Training Epoch 28  80.9% | batch:       555 of       686\t|\tloss: 0.639868\n",
      "Training Epoch 28  81.0% | batch:       556 of       686\t|\tloss: 0.812272\n",
      "Training Epoch 28  81.2% | batch:       557 of       686\t|\tloss: 0.684865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 28  81.3% | batch:       558 of       686\t|\tloss: 0.992929\n",
      "Training Epoch 28  81.5% | batch:       559 of       686\t|\tloss: 0.667762\n",
      "Training Epoch 28  81.6% | batch:       560 of       686\t|\tloss: 0.93884\n",
      "Training Epoch 28  81.8% | batch:       561 of       686\t|\tloss: 0.796138\n",
      "Training Epoch 28  81.9% | batch:       562 of       686\t|\tloss: 0.743579\n",
      "Training Epoch 28  82.1% | batch:       563 of       686\t|\tloss: 0.616456\n",
      "Training Epoch 28  82.2% | batch:       564 of       686\t|\tloss: 0.67581\n",
      "Training Epoch 28  82.4% | batch:       565 of       686\t|\tloss: 0.81683\n",
      "Training Epoch 28  82.5% | batch:       566 of       686\t|\tloss: 0.49485\n",
      "Training Epoch 28  82.7% | batch:       567 of       686\t|\tloss: 0.763627\n",
      "Training Epoch 28  82.8% | batch:       568 of       686\t|\tloss: 0.748274\n",
      "Training Epoch 28  82.9% | batch:       569 of       686\t|\tloss: 0.894562\n",
      "Training Epoch 28  83.1% | batch:       570 of       686\t|\tloss: 0.894246\n",
      "Training Epoch 28  83.2% | batch:       571 of       686\t|\tloss: 0.662138\n",
      "Training Epoch 28  83.4% | batch:       572 of       686\t|\tloss: 0.695628\n",
      "Training Epoch 28  83.5% | batch:       573 of       686\t|\tloss: 0.747169\n",
      "Training Epoch 28  83.7% | batch:       574 of       686\t|\tloss: 0.746298\n",
      "Training Epoch 28  83.8% | batch:       575 of       686\t|\tloss: 0.713249\n",
      "Training Epoch 28  84.0% | batch:       576 of       686\t|\tloss: 0.722915\n",
      "Training Epoch 28  84.1% | batch:       577 of       686\t|\tloss: 0.797657\n",
      "Training Epoch 28  84.3% | batch:       578 of       686\t|\tloss: 0.722476\n",
      "Training Epoch 28  84.4% | batch:       579 of       686\t|\tloss: 0.617945\n",
      "Training Epoch 28  84.5% | batch:       580 of       686\t|\tloss: 0.742197\n",
      "Training Epoch 28  84.7% | batch:       581 of       686\t|\tloss: 0.884766\n",
      "Training Epoch 28  84.8% | batch:       582 of       686\t|\tloss: 0.676752\n",
      "Training Epoch 28  85.0% | batch:       583 of       686\t|\tloss: 0.86091\n",
      "Training Epoch 28  85.1% | batch:       584 of       686\t|\tloss: 0.661093\n",
      "Training Epoch 28  85.3% | batch:       585 of       686\t|\tloss: 0.636066\n",
      "Training Epoch 28  85.4% | batch:       586 of       686\t|\tloss: 0.768008\n",
      "Training Epoch 28  85.6% | batch:       587 of       686\t|\tloss: 0.72018\n",
      "Training Epoch 28  85.7% | batch:       588 of       686\t|\tloss: 0.627235\n",
      "Training Epoch 28  85.9% | batch:       589 of       686\t|\tloss: 0.751526\n",
      "Training Epoch 28  86.0% | batch:       590 of       686\t|\tloss: 0.656952\n",
      "Training Epoch 28  86.2% | batch:       591 of       686\t|\tloss: 0.708772\n",
      "Training Epoch 28  86.3% | batch:       592 of       686\t|\tloss: 0.596005\n",
      "Training Epoch 28  86.4% | batch:       593 of       686\t|\tloss: 0.885412\n",
      "Training Epoch 28  86.6% | batch:       594 of       686\t|\tloss: 0.818035\n",
      "Training Epoch 28  86.7% | batch:       595 of       686\t|\tloss: 1.07822\n",
      "Training Epoch 28  86.9% | batch:       596 of       686\t|\tloss: 0.802648\n",
      "Training Epoch 28  87.0% | batch:       597 of       686\t|\tloss: 0.708957\n",
      "Training Epoch 28  87.2% | batch:       598 of       686\t|\tloss: 0.794385\n",
      "Training Epoch 28  87.3% | batch:       599 of       686\t|\tloss: 0.519896\n",
      "Training Epoch 28  87.5% | batch:       600 of       686\t|\tloss: 0.717906\n",
      "Training Epoch 28  87.6% | batch:       601 of       686\t|\tloss: 0.80247\n",
      "Training Epoch 28  87.8% | batch:       602 of       686\t|\tloss: 0.7738\n",
      "Training Epoch 28  87.9% | batch:       603 of       686\t|\tloss: 0.700465\n",
      "Training Epoch 28  88.0% | batch:       604 of       686\t|\tloss: 0.578998\n",
      "Training Epoch 28  88.2% | batch:       605 of       686\t|\tloss: 0.818683\n",
      "Training Epoch 28  88.3% | batch:       606 of       686\t|\tloss: 0.602641\n",
      "Training Epoch 28  88.5% | batch:       607 of       686\t|\tloss: 0.654023\n",
      "Training Epoch 28  88.6% | batch:       608 of       686\t|\tloss: 0.752221\n",
      "Training Epoch 28  88.8% | batch:       609 of       686\t|\tloss: 0.752455\n",
      "Training Epoch 28  88.9% | batch:       610 of       686\t|\tloss: 0.757967\n",
      "Training Epoch 28  89.1% | batch:       611 of       686\t|\tloss: 0.772204\n",
      "Training Epoch 28  89.2% | batch:       612 of       686\t|\tloss: 0.751707\n",
      "Training Epoch 28  89.4% | batch:       613 of       686\t|\tloss: 0.659912\n",
      "Training Epoch 28  89.5% | batch:       614 of       686\t|\tloss: 0.70891\n",
      "Training Epoch 28  89.7% | batch:       615 of       686\t|\tloss: 0.65446\n",
      "Training Epoch 28  89.8% | batch:       616 of       686\t|\tloss: 0.797643\n",
      "Training Epoch 28  89.9% | batch:       617 of       686\t|\tloss: 0.717523\n",
      "Training Epoch 28  90.1% | batch:       618 of       686\t|\tloss: 0.803016\n",
      "Training Epoch 28  90.2% | batch:       619 of       686\t|\tloss: 0.679599\n",
      "Training Epoch 28  90.4% | batch:       620 of       686\t|\tloss: 0.885112\n",
      "Training Epoch 28  90.5% | batch:       621 of       686\t|\tloss: 0.988498\n",
      "Training Epoch 28  90.7% | batch:       622 of       686\t|\tloss: 0.726369\n",
      "Training Epoch 28  90.8% | batch:       623 of       686\t|\tloss: 0.731901\n",
      "Training Epoch 28  91.0% | batch:       624 of       686\t|\tloss: 0.592977\n",
      "Training Epoch 28  91.1% | batch:       625 of       686\t|\tloss: 0.810339\n",
      "Training Epoch 28  91.3% | batch:       626 of       686\t|\tloss: 0.748894\n",
      "Training Epoch 28  91.4% | batch:       627 of       686\t|\tloss: 0.76786\n",
      "Training Epoch 28  91.5% | batch:       628 of       686\t|\tloss: 0.758656\n",
      "Training Epoch 28  91.7% | batch:       629 of       686\t|\tloss: 0.650907\n",
      "Training Epoch 28  91.8% | batch:       630 of       686\t|\tloss: 0.884539\n",
      "Training Epoch 28  92.0% | batch:       631 of       686\t|\tloss: 0.730001\n",
      "Training Epoch 28  92.1% | batch:       632 of       686\t|\tloss: 0.760978\n",
      "Training Epoch 28  92.3% | batch:       633 of       686\t|\tloss: 0.63595\n",
      "Training Epoch 28  92.4% | batch:       634 of       686\t|\tloss: 0.746558\n",
      "Training Epoch 28  92.6% | batch:       635 of       686\t|\tloss: 0.595667\n",
      "Training Epoch 28  92.7% | batch:       636 of       686\t|\tloss: 0.817666\n",
      "Training Epoch 28  92.9% | batch:       637 of       686\t|\tloss: 0.84065\n",
      "Training Epoch 28  93.0% | batch:       638 of       686\t|\tloss: 0.619272\n",
      "Training Epoch 28  93.1% | batch:       639 of       686\t|\tloss: 0.687468\n",
      "Training Epoch 28  93.3% | batch:       640 of       686\t|\tloss: 0.680458\n",
      "Training Epoch 28  93.4% | batch:       641 of       686\t|\tloss: 0.713819\n",
      "Training Epoch 28  93.6% | batch:       642 of       686\t|\tloss: 0.756859\n",
      "Training Epoch 28  93.7% | batch:       643 of       686\t|\tloss: 0.755111\n",
      "Training Epoch 28  93.9% | batch:       644 of       686\t|\tloss: 0.821794\n",
      "Training Epoch 28  94.0% | batch:       645 of       686\t|\tloss: 0.930365\n",
      "Training Epoch 28  94.2% | batch:       646 of       686\t|\tloss: 1.05515\n",
      "Training Epoch 28  94.3% | batch:       647 of       686\t|\tloss: 0.817527\n",
      "Training Epoch 28  94.5% | batch:       648 of       686\t|\tloss: 0.650829\n",
      "Training Epoch 28  94.6% | batch:       649 of       686\t|\tloss: 0.911442\n",
      "Training Epoch 28  94.8% | batch:       650 of       686\t|\tloss: 0.806229\n",
      "Training Epoch 28  94.9% | batch:       651 of       686\t|\tloss: 1.00984\n",
      "Training Epoch 28  95.0% | batch:       652 of       686\t|\tloss: 0.599008\n",
      "Training Epoch 28  95.2% | batch:       653 of       686\t|\tloss: 0.75428\n",
      "Training Epoch 28  95.3% | batch:       654 of       686\t|\tloss: 0.69309\n",
      "Training Epoch 28  95.5% | batch:       655 of       686\t|\tloss: 0.816743\n",
      "Training Epoch 28  95.6% | batch:       656 of       686\t|\tloss: 0.749396\n",
      "Training Epoch 28  95.8% | batch:       657 of       686\t|\tloss: 0.643641\n",
      "Training Epoch 28  95.9% | batch:       658 of       686\t|\tloss: 0.73785\n",
      "Training Epoch 28  96.1% | batch:       659 of       686\t|\tloss: 0.664447\n",
      "Training Epoch 28  96.2% | batch:       660 of       686\t|\tloss: 0.73194\n",
      "Training Epoch 28  96.4% | batch:       661 of       686\t|\tloss: 0.61368\n",
      "Training Epoch 28  96.5% | batch:       662 of       686\t|\tloss: 0.805501\n",
      "Training Epoch 28  96.6% | batch:       663 of       686\t|\tloss: 0.737475\n",
      "Training Epoch 28  96.8% | batch:       664 of       686\t|\tloss: 0.751543\n",
      "Training Epoch 28  96.9% | batch:       665 of       686\t|\tloss: 0.807379\n",
      "Training Epoch 28  97.1% | batch:       666 of       686\t|\tloss: 0.775147\n",
      "Training Epoch 28  97.2% | batch:       667 of       686\t|\tloss: 0.625905\n",
      "Training Epoch 28  97.4% | batch:       668 of       686\t|\tloss: 0.759366\n",
      "Training Epoch 28  97.5% | batch:       669 of       686\t|\tloss: 0.622451\n",
      "Training Epoch 28  97.7% | batch:       670 of       686\t|\tloss: 0.756252\n",
      "Training Epoch 28  97.8% | batch:       671 of       686\t|\tloss: 0.716164\n",
      "Training Epoch 28  98.0% | batch:       672 of       686\t|\tloss: 0.590076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 28  98.1% | batch:       673 of       686\t|\tloss: 0.617165\n",
      "Training Epoch 28  98.3% | batch:       674 of       686\t|\tloss: 0.657748\n",
      "Training Epoch 28  98.4% | batch:       675 of       686\t|\tloss: 0.923382\n",
      "Training Epoch 28  98.5% | batch:       676 of       686\t|\tloss: 0.621222\n",
      "Training Epoch 28  98.7% | batch:       677 of       686\t|\tloss: 0.64746\n",
      "Training Epoch 28  98.8% | batch:       678 of       686\t|\tloss: 0.767391\n",
      "Training Epoch 28  99.0% | batch:       679 of       686\t|\tloss: 0.557326\n",
      "Training Epoch 28  99.1% | batch:       680 of       686\t|\tloss: 0.922604\n",
      "Training Epoch 28  99.3% | batch:       681 of       686\t|\tloss: 0.680571\n",
      "Training Epoch 28  99.4% | batch:       682 of       686\t|\tloss: 0.562384\n",
      "Training Epoch 28  99.6% | batch:       683 of       686\t|\tloss: 0.734313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:12:47,676 | INFO : Epoch 28 Training Summary: epoch: 28.000000 | loss: 0.750357 | \n",
      "2023-05-25 22:12:47,680 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 22.912120819091797 seconds\n",
      "\n",
      "2023-05-25 22:12:47,681 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.793940799576895 seconds\n",
      "2023-05-25 22:12:47,681 | INFO : Avg batch train. time: 0.034685044897342414 seconds\n",
      "2023-05-25 22:12:47,682 | INFO : Avg sample train. time: 0.00027132608244001246 seconds\n",
      "2023-05-25 22:12:47,682 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 28  99.7% | batch:       684 of       686\t|\tloss: 0.71284\n",
      "Training Epoch 28  99.9% | batch:       685 of       686\t|\tloss: 0.58222\n",
      "\n",
      "Evaluating Epoch 28   0.0% | batch:         0 of       172\t|\tloss: 1.0948\n",
      "Evaluating Epoch 28   0.6% | batch:         1 of       172\t|\tloss: 1.19374\n",
      "Evaluating Epoch 28   1.2% | batch:         2 of       172\t|\tloss: 0.512605\n",
      "Evaluating Epoch 28   1.7% | batch:         3 of       172\t|\tloss: 1.79455\n",
      "Evaluating Epoch 28   2.3% | batch:         4 of       172\t|\tloss: 0.99128\n",
      "Evaluating Epoch 28   2.9% | batch:         5 of       172\t|\tloss: 1.07474\n",
      "Evaluating Epoch 28   3.5% | batch:         6 of       172\t|\tloss: 0.996646\n",
      "Evaluating Epoch 28   4.1% | batch:         7 of       172\t|\tloss: 2.22051\n",
      "Evaluating Epoch 28   4.7% | batch:         8 of       172\t|\tloss: 0.537121\n",
      "Evaluating Epoch 28   5.2% | batch:         9 of       172\t|\tloss: 1.29479\n",
      "Evaluating Epoch 28   5.8% | batch:        10 of       172\t|\tloss: 0.969367\n",
      "Evaluating Epoch 28   6.4% | batch:        11 of       172\t|\tloss: 1.07342\n",
      "Evaluating Epoch 28   7.0% | batch:        12 of       172\t|\tloss: 1.14155\n",
      "Evaluating Epoch 28   7.6% | batch:        13 of       172\t|\tloss: 1.04939\n",
      "Evaluating Epoch 28   8.1% | batch:        14 of       172\t|\tloss: 1.25379\n",
      "Evaluating Epoch 28   8.7% | batch:        15 of       172\t|\tloss: 1.14409\n",
      "Evaluating Epoch 28   9.3% | batch:        16 of       172\t|\tloss: 1.76193\n",
      "Evaluating Epoch 28   9.9% | batch:        17 of       172\t|\tloss: 0.707165\n",
      "Evaluating Epoch 28  10.5% | batch:        18 of       172\t|\tloss: 18.6648\n",
      "Evaluating Epoch 28  11.0% | batch:        19 of       172\t|\tloss: 1.26339\n",
      "Evaluating Epoch 28  11.6% | batch:        20 of       172\t|\tloss: 2.49491\n",
      "Evaluating Epoch 28  12.2% | batch:        21 of       172\t|\tloss: 0.72671\n",
      "Evaluating Epoch 28  12.8% | batch:        22 of       172\t|\tloss: 4.73417\n",
      "Evaluating Epoch 28  13.4% | batch:        23 of       172\t|\tloss: 3.00439\n",
      "Evaluating Epoch 28  14.0% | batch:        24 of       172\t|\tloss: 1.38868\n",
      "Evaluating Epoch 28  14.5% | batch:        25 of       172\t|\tloss: 2.50218\n",
      "Evaluating Epoch 28  15.1% | batch:        26 of       172\t|\tloss: 7.7893\n",
      "Evaluating Epoch 28  15.7% | batch:        27 of       172\t|\tloss: 15.6334\n",
      "Evaluating Epoch 28  16.3% | batch:        28 of       172\t|\tloss: 0.486952\n",
      "Evaluating Epoch 28  16.9% | batch:        29 of       172\t|\tloss: 1.86004\n",
      "Evaluating Epoch 28  17.4% | batch:        30 of       172\t|\tloss: 0.927845\n",
      "Evaluating Epoch 28  18.0% | batch:        31 of       172\t|\tloss: 0.417767\n",
      "Evaluating Epoch 28  18.6% | batch:        32 of       172\t|\tloss: 0.390929\n",
      "Evaluating Epoch 28  19.2% | batch:        33 of       172\t|\tloss: 0.448761\n",
      "Evaluating Epoch 28  19.8% | batch:        34 of       172\t|\tloss: 0.320925\n",
      "Evaluating Epoch 28  20.3% | batch:        35 of       172\t|\tloss: 0.765117\n",
      "Evaluating Epoch 28  20.9% | batch:        36 of       172\t|\tloss: 2.57107\n",
      "Evaluating Epoch 28  21.5% | batch:        37 of       172\t|\tloss: 4.13386\n",
      "Evaluating Epoch 28  22.1% | batch:        38 of       172\t|\tloss: 3.70513\n",
      "Evaluating Epoch 28  22.7% | batch:        39 of       172\t|\tloss: 8.28635\n",
      "Evaluating Epoch 28  23.3% | batch:        40 of       172\t|\tloss: 0.634002\n",
      "Evaluating Epoch 28  23.8% | batch:        41 of       172\t|\tloss: 0.971702\n",
      "Evaluating Epoch 28  24.4% | batch:        42 of       172\t|\tloss: 0.277129\n",
      "Evaluating Epoch 28  25.0% | batch:        43 of       172\t|\tloss: 19.7127\n",
      "Evaluating Epoch 28  25.6% | batch:        44 of       172\t|\tloss: 1.34371\n",
      "Evaluating Epoch 28  26.2% | batch:        45 of       172\t|\tloss: 1.22872\n",
      "Evaluating Epoch 28  26.7% | batch:        46 of       172\t|\tloss: 0.359219\n",
      "Evaluating Epoch 28  27.3% | batch:        47 of       172\t|\tloss: 0.941665\n",
      "Evaluating Epoch 28  27.9% | batch:        48 of       172\t|\tloss: 0.312977\n",
      "Evaluating Epoch 28  28.5% | batch:        49 of       172\t|\tloss: 0.936084\n",
      "Evaluating Epoch 28  29.1% | batch:        50 of       172\t|\tloss: 0.435682\n",
      "Evaluating Epoch 28  29.7% | batch:        51 of       172\t|\tloss: 0.856301\n",
      "Evaluating Epoch 28  30.2% | batch:        52 of       172\t|\tloss: 0.710475\n",
      "Evaluating Epoch 28  30.8% | batch:        53 of       172\t|\tloss: 3.05627\n",
      "Evaluating Epoch 28  31.4% | batch:        54 of       172\t|\tloss: 0.854472\n",
      "Evaluating Epoch 28  32.0% | batch:        55 of       172\t|\tloss: 0.379343\n",
      "Evaluating Epoch 28  32.6% | batch:        56 of       172\t|\tloss: 3.31135\n",
      "Evaluating Epoch 28  33.1% | batch:        57 of       172\t|\tloss: 0.215431\n",
      "Evaluating Epoch 28  33.7% | batch:        58 of       172\t|\tloss: 2.26592\n",
      "Evaluating Epoch 28  34.3% | batch:        59 of       172\t|\tloss: 1.18336\n",
      "Evaluating Epoch 28  34.9% | batch:        60 of       172\t|\tloss: 1.27871\n",
      "Evaluating Epoch 28  35.5% | batch:        61 of       172\t|\tloss: 1.66223\n",
      "Evaluating Epoch 28  36.0% | batch:        62 of       172\t|\tloss: 0.812944\n",
      "Evaluating Epoch 28  36.6% | batch:        63 of       172\t|\tloss: 3.13995\n",
      "Evaluating Epoch 28  37.2% | batch:        64 of       172\t|\tloss: 0.829534\n",
      "Evaluating Epoch 28  37.8% | batch:        65 of       172\t|\tloss: 2.39883\n",
      "Evaluating Epoch 28  38.4% | batch:        66 of       172\t|\tloss: 1.33905\n",
      "Evaluating Epoch 28  39.0% | batch:        67 of       172\t|\tloss: 0.547112\n",
      "Evaluating Epoch 28  39.5% | batch:        68 of       172\t|\tloss: 2.52545\n",
      "Evaluating Epoch 28  40.1% | batch:        69 of       172\t|\tloss: 0.725596\n",
      "Evaluating Epoch 28  40.7% | batch:        70 of       172\t|\tloss: 1.79821\n",
      "Evaluating Epoch 28  41.3% | batch:        71 of       172\t|\tloss: 1.49763\n",
      "Evaluating Epoch 28  41.9% | batch:        72 of       172\t|\tloss: 0.775117\n",
      "Evaluating Epoch 28  42.4% | batch:        73 of       172\t|\tloss: 2.64122\n",
      "Evaluating Epoch 28  43.0% | batch:        74 of       172\t|\tloss: 0.357206\n",
      "Evaluating Epoch 28  43.6% | batch:        75 of       172\t|\tloss: 0.283581\n",
      "Evaluating Epoch 28  44.2% | batch:        76 of       172\t|\tloss: 0.406324\n",
      "Evaluating Epoch 28  44.8% | batch:        77 of       172\t|\tloss: 0.332086\n",
      "Evaluating Epoch 28  45.3% | batch:        78 of       172\t|\tloss: 0.318014\n",
      "Evaluating Epoch 28  45.9% | batch:        79 of       172\t|\tloss: 0.275019\n",
      "Evaluating Epoch 28  46.5% | batch:        80 of       172\t|\tloss: 0.282452\n",
      "Evaluating Epoch 28  47.1% | batch:        81 of       172\t|\tloss: 0.394852\n",
      "Evaluating Epoch 28  47.7% | batch:        82 of       172\t|\tloss: 0.366786\n",
      "Evaluating Epoch 28  48.3% | batch:        83 of       172\t|\tloss: 0.262919\n",
      "Evaluating Epoch 28  48.8% | batch:        84 of       172\t|\tloss: 0.582298\n",
      "Evaluating Epoch 28  49.4% | batch:        85 of       172\t|\tloss: 0.480553\n",
      "Evaluating Epoch 28  50.0% | batch:        86 of       172\t|\tloss: 0.487651\n",
      "Evaluating Epoch 28  50.6% | batch:        87 of       172\t|\tloss: 0.537972\n",
      "Evaluating Epoch 28  51.2% | batch:        88 of       172\t|\tloss: 0.442606\n",
      "Evaluating Epoch 28  51.7% | batch:        89 of       172\t|\tloss: 0.479854\n",
      "Evaluating Epoch 28  52.3% | batch:        90 of       172\t|\tloss: 0.442588\n",
      "Evaluating Epoch 28  52.9% | batch:        91 of       172\t|\tloss: 0.304014\n",
      "Evaluating Epoch 28  53.5% | batch:        92 of       172\t|\tloss: 0.354356\n",
      "Evaluating Epoch 28  54.1% | batch:        93 of       172\t|\tloss: 0.745411\n",
      "Evaluating Epoch 28  54.7% | batch:        94 of       172\t|\tloss: 0.285333\n",
      "Evaluating Epoch 28  55.2% | batch:        95 of       172\t|\tloss: 0.344638\n",
      "Evaluating Epoch 28  55.8% | batch:        96 of       172\t|\tloss: 0.617053\n",
      "Evaluating Epoch 28  56.4% | batch:        97 of       172\t|\tloss: 0.421914\n",
      "Evaluating Epoch 28  57.0% | batch:        98 of       172\t|\tloss: 0.431778\n",
      "Evaluating Epoch 28  57.6% | batch:        99 of       172\t|\tloss: 0.376432\n",
      "Evaluating Epoch 28  58.1% | batch:       100 of       172\t|\tloss: 0.465392\n",
      "Evaluating Epoch 28  58.7% | batch:       101 of       172\t|\tloss: 0.368047\n",
      "Evaluating Epoch 28  59.3% | batch:       102 of       172\t|\tloss: 0.376146\n",
      "Evaluating Epoch 28  59.9% | batch:       103 of       172\t|\tloss: 0.814485\n",
      "Evaluating Epoch 28  60.5% | batch:       104 of       172\t|\tloss: 0.392586\n",
      "Evaluating Epoch 28  61.0% | batch:       105 of       172\t|\tloss: 0.23366\n",
      "Evaluating Epoch 28  61.6% | batch:       106 of       172\t|\tloss: 0.360981\n",
      "Evaluating Epoch 28  62.2% | batch:       107 of       172\t|\tloss: 0.850439\n",
      "Evaluating Epoch 28  62.8% | batch:       108 of       172\t|\tloss: 0.209659\n",
      "Evaluating Epoch 28  63.4% | batch:       109 of       172\t|\tloss: 0.396859\n",
      "Evaluating Epoch 28  64.0% | batch:       110 of       172\t|\tloss: 0.735284\n",
      "Evaluating Epoch 28  64.5% | batch:       111 of       172\t|\tloss: 0.401984\n",
      "Evaluating Epoch 28  65.1% | batch:       112 of       172\t|\tloss: 0.532494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 28  65.7% | batch:       113 of       172\t|\tloss: 0.523207\n",
      "Evaluating Epoch 28  66.3% | batch:       114 of       172\t|\tloss: 0.740891\n",
      "Evaluating Epoch 28  66.9% | batch:       115 of       172\t|\tloss: 0.565412\n",
      "Evaluating Epoch 28  67.4% | batch:       116 of       172\t|\tloss: 0.324583\n",
      "Evaluating Epoch 28  68.0% | batch:       117 of       172\t|\tloss: 0.525598\n",
      "Evaluating Epoch 28  68.6% | batch:       118 of       172\t|\tloss: 0.145933\n",
      "Evaluating Epoch 28  69.2% | batch:       119 of       172\t|\tloss: 0.357327\n",
      "Evaluating Epoch 28  69.8% | batch:       120 of       172\t|\tloss: 0.213575\n",
      "Evaluating Epoch 28  70.3% | batch:       121 of       172\t|\tloss: 0.199437\n",
      "Evaluating Epoch 28  70.9% | batch:       122 of       172\t|\tloss: 0.218573\n",
      "Evaluating Epoch 28  71.5% | batch:       123 of       172\t|\tloss: 0.263527\n",
      "Evaluating Epoch 28  72.1% | batch:       124 of       172\t|\tloss: 0.267589\n",
      "Evaluating Epoch 28  72.7% | batch:       125 of       172\t|\tloss: 0.440443\n",
      "Evaluating Epoch 28  73.3% | batch:       126 of       172\t|\tloss: 0.356543\n",
      "Evaluating Epoch 28  73.8% | batch:       127 of       172\t|\tloss: 0.412454\n",
      "Evaluating Epoch 28  74.4% | batch:       128 of       172\t|\tloss: 0.317439\n",
      "Evaluating Epoch 28  75.0% | batch:       129 of       172\t|\tloss: 0.402656\n",
      "Evaluating Epoch 28  75.6% | batch:       130 of       172\t|\tloss: 0.169503\n",
      "Evaluating Epoch 28  76.2% | batch:       131 of       172\t|\tloss: 0.487349\n",
      "Evaluating Epoch 28  76.7% | batch:       132 of       172\t|\tloss: 0.404301\n",
      "Evaluating Epoch 28  77.3% | batch:       133 of       172\t|\tloss: 0.556949\n",
      "Evaluating Epoch 28  77.9% | batch:       134 of       172\t|\tloss: 0.417555\n",
      "Evaluating Epoch 28  78.5% | batch:       135 of       172\t|\tloss: 0.432235\n",
      "Evaluating Epoch 28  79.1% | batch:       136 of       172\t|\tloss: 0.353793\n",
      "Evaluating Epoch 28  79.7% | batch:       137 of       172\t|\tloss: 0.336558\n",
      "Evaluating Epoch 28  80.2% | batch:       138 of       172\t|\tloss: 0.336433\n",
      "Evaluating Epoch 28  80.8% | batch:       139 of       172\t|\tloss: 0.59403\n",
      "Evaluating Epoch 28  81.4% | batch:       140 of       172\t|\tloss: 0.32764\n",
      "Evaluating Epoch 28  82.0% | batch:       141 of       172\t|\tloss: 0.251663\n",
      "Evaluating Epoch 28  82.6% | batch:       142 of       172\t|\tloss: 0.315524\n",
      "Evaluating Epoch 28  83.1% | batch:       143 of       172\t|\tloss: 0.336779\n",
      "Evaluating Epoch 28  83.7% | batch:       144 of       172\t|\tloss: 0.409617\n",
      "Evaluating Epoch 28  84.3% | batch:       145 of       172\t|\tloss: 0.49177\n",
      "Evaluating Epoch 28  84.9% | batch:       146 of       172\t|\tloss: 0.386822\n",
      "Evaluating Epoch 28  85.5% | batch:       147 of       172\t|\tloss: 0.454536\n",
      "Evaluating Epoch 28  86.0% | batch:       148 of       172\t|\tloss: 0.331445\n",
      "Evaluating Epoch 28  86.6% | batch:       149 of       172\t|\tloss: 0.392935\n",
      "Evaluating Epoch 28  87.2% | batch:       150 of       172\t|\tloss: 0.11368\n",
      "Evaluating Epoch 28  87.8% | batch:       151 of       172\t|\tloss: 0.168596\n",
      "Evaluating Epoch 28  88.4% | batch:       152 of       172\t|\tloss: 0.255908\n",
      "Evaluating Epoch 28  89.0% | batch:       153 of       172\t|\tloss: 0.134371\n",
      "Evaluating Epoch 28  89.5% | batch:       154 of       172\t|\tloss: 0.123122\n",
      "Evaluating Epoch 28  90.1% | batch:       155 of       172\t|\tloss: 0.319953\n",
      "Evaluating Epoch 28  90.7% | batch:       156 of       172\t|\tloss: 0.163092\n",
      "Evaluating Epoch 28  91.3% | batch:       157 of       172\t|\tloss: 0.231397\n",
      "Evaluating Epoch 28  91.9% | batch:       158 of       172\t|\tloss: 0.151859\n",
      "Evaluating Epoch 28  92.4% | batch:       159 of       172\t|\tloss: 0.17516\n",
      "Evaluating Epoch 28  93.0% | batch:       160 of       172\t|\tloss: 0.828214\n",
      "Evaluating Epoch 28  93.6% | batch:       161 of       172\t|\tloss: 0.152575\n",
      "Evaluating Epoch 28  94.2% | batch:       162 of       172\t|\tloss: 0.176622\n",
      "Evaluating Epoch 28  94.8% | batch:       163 of       172\t|\tloss: 0.279565\n",
      "Evaluating Epoch 28  95.3% | batch:       164 of       172\t|\tloss: 0.134669\n",
      "Evaluating Epoch 28  95.9% | batch:       165 of       172\t|\tloss: 0.205432\n",
      "Evaluating Epoch 28  96.5% | batch:       166 of       172\t|\tloss: 0.145731\n",
      "Evaluating Epoch 28  97.1% | batch:       167 of       172\t|\tloss: 0.166929\n",
      "Evaluating Epoch 28  97.7% | batch:       168 of       172\t|\tloss: 0.156538\n",
      "Evaluating Epoch 28  98.3% | batch:       169 of       172\t|\tloss: 0.10181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:12:53,015 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 5.331914901733398 seconds\n",
      "\n",
      "2023-05-25 22:12:53,019 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.057615830980498 seconds\n",
      "2023-05-25 22:12:53,021 | INFO : Avg batch val. time: 0.023590789715002895 seconds\n",
      "2023-05-25 22:12:53,021 | INFO : Avg sample val. time: 0.00018479827986430286 seconds\n",
      "2023-05-25 22:12:53,023 | INFO : Epoch 28 Validation Summary: epoch: 28.000000 | loss: 1.169010 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 28  98.8% | batch:       170 of       172\t|\tloss: 0.132971\n",
      "Evaluating Epoch 28  99.4% | batch:       171 of       172\t|\tloss: 0.207792\n",
      "\n",
      "Training Epoch 29   0.0% | batch:         0 of       686\t|\tloss: 0.821384\n",
      "Training Epoch 29   0.1% | batch:         1 of       686\t|\tloss: 0.880532\n",
      "Training Epoch 29   0.3% | batch:         2 of       686\t|\tloss: 0.639673\n",
      "Training Epoch 29   0.4% | batch:         3 of       686\t|\tloss: 0.688607\n",
      "Training Epoch 29   0.6% | batch:         4 of       686\t|\tloss: 0.702738\n",
      "Training Epoch 29   0.7% | batch:         5 of       686\t|\tloss: 0.878545\n",
      "Training Epoch 29   0.9% | batch:         6 of       686\t|\tloss: 0.815701\n",
      "Training Epoch 29   1.0% | batch:         7 of       686\t|\tloss: 0.543586\n",
      "Training Epoch 29   1.2% | batch:         8 of       686\t|\tloss: 1.00136\n",
      "Training Epoch 29   1.3% | batch:         9 of       686\t|\tloss: 0.787835\n",
      "Training Epoch 29   1.5% | batch:        10 of       686\t|\tloss: 0.910236\n",
      "Training Epoch 29   1.6% | batch:        11 of       686\t|\tloss: 0.727144\n",
      "Training Epoch 29   1.7% | batch:        12 of       686\t|\tloss: 0.903681\n",
      "Training Epoch 29   1.9% | batch:        13 of       686\t|\tloss: 0.894962\n",
      "Training Epoch 29   2.0% | batch:        14 of       686\t|\tloss: 0.833094\n",
      "Training Epoch 29   2.2% | batch:        15 of       686\t|\tloss: 0.587004\n",
      "Training Epoch 29   2.3% | batch:        16 of       686\t|\tloss: 0.829098\n",
      "Training Epoch 29   2.5% | batch:        17 of       686\t|\tloss: 0.55342\n",
      "Training Epoch 29   2.6% | batch:        18 of       686\t|\tloss: 0.837912\n",
      "Training Epoch 29   2.8% | batch:        19 of       686\t|\tloss: 0.712543\n",
      "Training Epoch 29   2.9% | batch:        20 of       686\t|\tloss: 0.759984\n",
      "Training Epoch 29   3.1% | batch:        21 of       686\t|\tloss: 0.683738\n",
      "Training Epoch 29   3.2% | batch:        22 of       686\t|\tloss: 0.722452\n",
      "Training Epoch 29   3.4% | batch:        23 of       686\t|\tloss: 0.894828\n",
      "Training Epoch 29   3.5% | batch:        24 of       686\t|\tloss: 0.904599\n",
      "Training Epoch 29   3.6% | batch:        25 of       686\t|\tloss: 0.621932\n",
      "Training Epoch 29   3.8% | batch:        26 of       686\t|\tloss: 0.677857\n",
      "Training Epoch 29   3.9% | batch:        27 of       686\t|\tloss: 0.658234\n",
      "Training Epoch 29   4.1% | batch:        28 of       686\t|\tloss: 0.663144\n",
      "Training Epoch 29   4.2% | batch:        29 of       686\t|\tloss: 0.705834\n",
      "Training Epoch 29   4.4% | batch:        30 of       686\t|\tloss: 0.780593\n",
      "Training Epoch 29   4.5% | batch:        31 of       686\t|\tloss: 0.9132\n",
      "Training Epoch 29   4.7% | batch:        32 of       686\t|\tloss: 0.730335\n",
      "Training Epoch 29   4.8% | batch:        33 of       686\t|\tloss: 0.833948\n",
      "Training Epoch 29   5.0% | batch:        34 of       686\t|\tloss: 0.70605\n",
      "Training Epoch 29   5.1% | batch:        35 of       686\t|\tloss: 0.905366\n",
      "Training Epoch 29   5.2% | batch:        36 of       686\t|\tloss: 0.700067\n",
      "Training Epoch 29   5.4% | batch:        37 of       686\t|\tloss: 0.589612\n",
      "Training Epoch 29   5.5% | batch:        38 of       686\t|\tloss: 0.656206\n",
      "Training Epoch 29   5.7% | batch:        39 of       686\t|\tloss: 0.7124\n",
      "Training Epoch 29   5.8% | batch:        40 of       686\t|\tloss: 0.571544\n",
      "Training Epoch 29   6.0% | batch:        41 of       686\t|\tloss: 0.734336\n",
      "Training Epoch 29   6.1% | batch:        42 of       686\t|\tloss: 0.791299\n",
      "Training Epoch 29   6.3% | batch:        43 of       686\t|\tloss: 0.806048\n",
      "Training Epoch 29   6.4% | batch:        44 of       686\t|\tloss: 0.656088\n",
      "Training Epoch 29   6.6% | batch:        45 of       686\t|\tloss: 0.516129\n",
      "Training Epoch 29   6.7% | batch:        46 of       686\t|\tloss: 1.26868\n",
      "Training Epoch 29   6.9% | batch:        47 of       686\t|\tloss: 0.613896\n",
      "Training Epoch 29   7.0% | batch:        48 of       686\t|\tloss: 0.745286\n",
      "Training Epoch 29   7.1% | batch:        49 of       686\t|\tloss: 0.746038\n",
      "Training Epoch 29   7.3% | batch:        50 of       686\t|\tloss: 0.64882\n",
      "Training Epoch 29   7.4% | batch:        51 of       686\t|\tloss: 0.838603\n",
      "Training Epoch 29   7.6% | batch:        52 of       686\t|\tloss: 0.530663\n",
      "Training Epoch 29   7.7% | batch:        53 of       686\t|\tloss: 0.744993\n",
      "Training Epoch 29   7.9% | batch:        54 of       686\t|\tloss: 0.630909\n",
      "Training Epoch 29   8.0% | batch:        55 of       686\t|\tloss: 0.92547\n",
      "Training Epoch 29   8.2% | batch:        56 of       686\t|\tloss: 0.630606\n",
      "Training Epoch 29   8.3% | batch:        57 of       686\t|\tloss: 0.695697\n",
      "Training Epoch 29   8.5% | batch:        58 of       686\t|\tloss: 0.817894\n",
      "Training Epoch 29   8.6% | batch:        59 of       686\t|\tloss: 0.600181\n",
      "Training Epoch 29   8.7% | batch:        60 of       686\t|\tloss: 0.906283\n",
      "Training Epoch 29   8.9% | batch:        61 of       686\t|\tloss: 0.696645\n",
      "Training Epoch 29   9.0% | batch:        62 of       686\t|\tloss: 0.66255\n",
      "Training Epoch 29   9.2% | batch:        63 of       686\t|\tloss: 0.716342\n",
      "Training Epoch 29   9.3% | batch:        64 of       686\t|\tloss: 0.578131\n",
      "Training Epoch 29   9.5% | batch:        65 of       686\t|\tloss: 0.880203\n",
      "Training Epoch 29   9.6% | batch:        66 of       686\t|\tloss: 0.767766\n",
      "Training Epoch 29   9.8% | batch:        67 of       686\t|\tloss: 0.648497\n",
      "Training Epoch 29   9.9% | batch:        68 of       686\t|\tloss: 0.694603\n",
      "Training Epoch 29  10.1% | batch:        69 of       686\t|\tloss: 0.660722\n",
      "Training Epoch 29  10.2% | batch:        70 of       686\t|\tloss: 0.54112\n",
      "Training Epoch 29  10.3% | batch:        71 of       686\t|\tloss: 0.834843\n",
      "Training Epoch 29  10.5% | batch:        72 of       686\t|\tloss: 0.639452\n",
      "Training Epoch 29  10.6% | batch:        73 of       686\t|\tloss: 0.777993\n",
      "Training Epoch 29  10.8% | batch:        74 of       686\t|\tloss: 0.708112\n",
      "Training Epoch 29  10.9% | batch:        75 of       686\t|\tloss: 0.648151\n",
      "Training Epoch 29  11.1% | batch:        76 of       686\t|\tloss: 0.85446\n",
      "Training Epoch 29  11.2% | batch:        77 of       686\t|\tloss: 0.646592\n",
      "Training Epoch 29  11.4% | batch:        78 of       686\t|\tloss: 0.733536\n",
      "Training Epoch 29  11.5% | batch:        79 of       686\t|\tloss: 0.788408\n",
      "Training Epoch 29  11.7% | batch:        80 of       686\t|\tloss: 0.673092\n",
      "Training Epoch 29  11.8% | batch:        81 of       686\t|\tloss: 0.670596\n",
      "Training Epoch 29  12.0% | batch:        82 of       686\t|\tloss: 0.600993\n",
      "Training Epoch 29  12.1% | batch:        83 of       686\t|\tloss: 0.698638\n",
      "Training Epoch 29  12.2% | batch:        84 of       686\t|\tloss: 0.75502\n",
      "Training Epoch 29  12.4% | batch:        85 of       686\t|\tloss: 0.63829\n",
      "Training Epoch 29  12.5% | batch:        86 of       686\t|\tloss: 0.754946\n",
      "Training Epoch 29  12.7% | batch:        87 of       686\t|\tloss: 0.526172\n",
      "Training Epoch 29  12.8% | batch:        88 of       686\t|\tloss: 0.674118\n",
      "Training Epoch 29  13.0% | batch:        89 of       686\t|\tloss: 0.637561\n",
      "Training Epoch 29  13.1% | batch:        90 of       686\t|\tloss: 0.750077\n",
      "Training Epoch 29  13.3% | batch:        91 of       686\t|\tloss: 0.923188\n",
      "Training Epoch 29  13.4% | batch:        92 of       686\t|\tloss: 0.778826\n",
      "Training Epoch 29  13.6% | batch:        93 of       686\t|\tloss: 0.494358\n",
      "Training Epoch 29  13.7% | batch:        94 of       686\t|\tloss: 1.02238\n",
      "Training Epoch 29  13.8% | batch:        95 of       686\t|\tloss: 0.735241\n",
      "Training Epoch 29  14.0% | batch:        96 of       686\t|\tloss: 0.66035\n",
      "Training Epoch 29  14.1% | batch:        97 of       686\t|\tloss: 0.667242\n",
      "Training Epoch 29  14.3% | batch:        98 of       686\t|\tloss: 0.688352\n",
      "Training Epoch 29  14.4% | batch:        99 of       686\t|\tloss: 0.677305\n",
      "Training Epoch 29  14.6% | batch:       100 of       686\t|\tloss: 0.676946\n",
      "Training Epoch 29  14.7% | batch:       101 of       686\t|\tloss: 0.558544\n",
      "Training Epoch 29  14.9% | batch:       102 of       686\t|\tloss: 1.02352\n",
      "Training Epoch 29  15.0% | batch:       103 of       686\t|\tloss: 0.821009\n",
      "Training Epoch 29  15.2% | batch:       104 of       686\t|\tloss: 0.732155\n",
      "Training Epoch 29  15.3% | batch:       105 of       686\t|\tloss: 0.822436\n",
      "Training Epoch 29  15.5% | batch:       106 of       686\t|\tloss: 0.722795\n",
      "Training Epoch 29  15.6% | batch:       107 of       686\t|\tloss: 0.699832\n",
      "Training Epoch 29  15.7% | batch:       108 of       686\t|\tloss: 0.666987\n",
      "Training Epoch 29  15.9% | batch:       109 of       686\t|\tloss: 0.776587\n",
      "Training Epoch 29  16.0% | batch:       110 of       686\t|\tloss: 0.737737\n",
      "Training Epoch 29  16.2% | batch:       111 of       686\t|\tloss: 0.605446\n",
      "Training Epoch 29  16.3% | batch:       112 of       686\t|\tloss: 0.643416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 29  16.5% | batch:       113 of       686\t|\tloss: 0.764164\n",
      "Training Epoch 29  16.6% | batch:       114 of       686\t|\tloss: 0.847433\n",
      "Training Epoch 29  16.8% | batch:       115 of       686\t|\tloss: 0.647015\n",
      "Training Epoch 29  16.9% | batch:       116 of       686\t|\tloss: 0.595943\n",
      "Training Epoch 29  17.1% | batch:       117 of       686\t|\tloss: 0.754751\n",
      "Training Epoch 29  17.2% | batch:       118 of       686\t|\tloss: 0.72717\n",
      "Training Epoch 29  17.3% | batch:       119 of       686\t|\tloss: 0.580011\n",
      "Training Epoch 29  17.5% | batch:       120 of       686\t|\tloss: 0.659127\n",
      "Training Epoch 29  17.6% | batch:       121 of       686\t|\tloss: 0.801382\n",
      "Training Epoch 29  17.8% | batch:       122 of       686\t|\tloss: 0.734096\n",
      "Training Epoch 29  17.9% | batch:       123 of       686\t|\tloss: 0.905734\n",
      "Training Epoch 29  18.1% | batch:       124 of       686\t|\tloss: 0.62739\n",
      "Training Epoch 29  18.2% | batch:       125 of       686\t|\tloss: 0.706057\n",
      "Training Epoch 29  18.4% | batch:       126 of       686\t|\tloss: 0.579036\n",
      "Training Epoch 29  18.5% | batch:       127 of       686\t|\tloss: 1.07391\n",
      "Training Epoch 29  18.7% | batch:       128 of       686\t|\tloss: 0.667957\n",
      "Training Epoch 29  18.8% | batch:       129 of       686\t|\tloss: 0.659651\n",
      "Training Epoch 29  19.0% | batch:       130 of       686\t|\tloss: 0.717123\n",
      "Training Epoch 29  19.1% | batch:       131 of       686\t|\tloss: 0.574562\n",
      "Training Epoch 29  19.2% | batch:       132 of       686\t|\tloss: 0.613375\n",
      "Training Epoch 29  19.4% | batch:       133 of       686\t|\tloss: 0.679392\n",
      "Training Epoch 29  19.5% | batch:       134 of       686\t|\tloss: 0.884818\n",
      "Training Epoch 29  19.7% | batch:       135 of       686\t|\tloss: 0.691904\n",
      "Training Epoch 29  19.8% | batch:       136 of       686\t|\tloss: 0.810593\n",
      "Training Epoch 29  20.0% | batch:       137 of       686\t|\tloss: 0.899822\n",
      "Training Epoch 29  20.1% | batch:       138 of       686\t|\tloss: 0.699686\n",
      "Training Epoch 29  20.3% | batch:       139 of       686\t|\tloss: 0.677619\n",
      "Training Epoch 29  20.4% | batch:       140 of       686\t|\tloss: 0.657615\n",
      "Training Epoch 29  20.6% | batch:       141 of       686\t|\tloss: 0.596992\n",
      "Training Epoch 29  20.7% | batch:       142 of       686\t|\tloss: 0.574973\n",
      "Training Epoch 29  20.8% | batch:       143 of       686\t|\tloss: 0.741604\n",
      "Training Epoch 29  21.0% | batch:       144 of       686\t|\tloss: 0.600853\n",
      "Training Epoch 29  21.1% | batch:       145 of       686\t|\tloss: 0.616423\n",
      "Training Epoch 29  21.3% | batch:       146 of       686\t|\tloss: 0.708686\n",
      "Training Epoch 29  21.4% | batch:       147 of       686\t|\tloss: 0.818914\n",
      "Training Epoch 29  21.6% | batch:       148 of       686\t|\tloss: 1.00816\n",
      "Training Epoch 29  21.7% | batch:       149 of       686\t|\tloss: 0.815936\n",
      "Training Epoch 29  21.9% | batch:       150 of       686\t|\tloss: 0.957594\n",
      "Training Epoch 29  22.0% | batch:       151 of       686\t|\tloss: 0.647175\n",
      "Training Epoch 29  22.2% | batch:       152 of       686\t|\tloss: 0.735752\n",
      "Training Epoch 29  22.3% | batch:       153 of       686\t|\tloss: 0.576801\n",
      "Training Epoch 29  22.4% | batch:       154 of       686\t|\tloss: 0.814715\n",
      "Training Epoch 29  22.6% | batch:       155 of       686\t|\tloss: 0.630609\n",
      "Training Epoch 29  22.7% | batch:       156 of       686\t|\tloss: 0.775903\n",
      "Training Epoch 29  22.9% | batch:       157 of       686\t|\tloss: 0.693119\n",
      "Training Epoch 29  23.0% | batch:       158 of       686\t|\tloss: 0.639945\n",
      "Training Epoch 29  23.2% | batch:       159 of       686\t|\tloss: 0.577418\n",
      "Training Epoch 29  23.3% | batch:       160 of       686\t|\tloss: 0.70207\n",
      "Training Epoch 29  23.5% | batch:       161 of       686\t|\tloss: 0.767591\n",
      "Training Epoch 29  23.6% | batch:       162 of       686\t|\tloss: 0.592531\n",
      "Training Epoch 29  23.8% | batch:       163 of       686\t|\tloss: 0.721598\n",
      "Training Epoch 29  23.9% | batch:       164 of       686\t|\tloss: 0.764752\n",
      "Training Epoch 29  24.1% | batch:       165 of       686\t|\tloss: 0.715612\n",
      "Training Epoch 29  24.2% | batch:       166 of       686\t|\tloss: 0.72409\n",
      "Training Epoch 29  24.3% | batch:       167 of       686\t|\tloss: 0.713763\n",
      "Training Epoch 29  24.5% | batch:       168 of       686\t|\tloss: 0.958173\n",
      "Training Epoch 29  24.6% | batch:       169 of       686\t|\tloss: 0.668342\n",
      "Training Epoch 29  24.8% | batch:       170 of       686\t|\tloss: 0.802032\n",
      "Training Epoch 29  24.9% | batch:       171 of       686\t|\tloss: 0.670284\n",
      "Training Epoch 29  25.1% | batch:       172 of       686\t|\tloss: 0.565152\n",
      "Training Epoch 29  25.2% | batch:       173 of       686\t|\tloss: 0.72947\n",
      "Training Epoch 29  25.4% | batch:       174 of       686\t|\tloss: 0.484463\n",
      "Training Epoch 29  25.5% | batch:       175 of       686\t|\tloss: 0.671962\n",
      "Training Epoch 29  25.7% | batch:       176 of       686\t|\tloss: 0.526211\n",
      "Training Epoch 29  25.8% | batch:       177 of       686\t|\tloss: 0.579365\n",
      "Training Epoch 29  25.9% | batch:       178 of       686\t|\tloss: 0.540262\n",
      "Training Epoch 29  26.1% | batch:       179 of       686\t|\tloss: 0.583015\n",
      "Training Epoch 29  26.2% | batch:       180 of       686\t|\tloss: 0.696508\n",
      "Training Epoch 29  26.4% | batch:       181 of       686\t|\tloss: 0.617391\n",
      "Training Epoch 29  26.5% | batch:       182 of       686\t|\tloss: 0.630619\n",
      "Training Epoch 29  26.7% | batch:       183 of       686\t|\tloss: 0.59456\n",
      "Training Epoch 29  26.8% | batch:       184 of       686\t|\tloss: 0.698168\n",
      "Training Epoch 29  27.0% | batch:       185 of       686\t|\tloss: 0.841121\n",
      "Training Epoch 29  27.1% | batch:       186 of       686\t|\tloss: 0.561386\n",
      "Training Epoch 29  27.3% | batch:       187 of       686\t|\tloss: 0.760931\n",
      "Training Epoch 29  27.4% | batch:       188 of       686\t|\tloss: 0.869677\n",
      "Training Epoch 29  27.6% | batch:       189 of       686\t|\tloss: 0.712709\n",
      "Training Epoch 29  27.7% | batch:       190 of       686\t|\tloss: 0.863155\n",
      "Training Epoch 29  27.8% | batch:       191 of       686\t|\tloss: 0.809234\n",
      "Training Epoch 29  28.0% | batch:       192 of       686\t|\tloss: 0.67127\n",
      "Training Epoch 29  28.1% | batch:       193 of       686\t|\tloss: 0.916203\n",
      "Training Epoch 29  28.3% | batch:       194 of       686\t|\tloss: 0.668809\n",
      "Training Epoch 29  28.4% | batch:       195 of       686\t|\tloss: 0.660471\n",
      "Training Epoch 29  28.6% | batch:       196 of       686\t|\tloss: 0.646679\n",
      "Training Epoch 29  28.7% | batch:       197 of       686\t|\tloss: 0.636734\n",
      "Training Epoch 29  28.9% | batch:       198 of       686\t|\tloss: 0.72427\n",
      "Training Epoch 29  29.0% | batch:       199 of       686\t|\tloss: 0.642711\n",
      "Training Epoch 29  29.2% | batch:       200 of       686\t|\tloss: 0.644893\n",
      "Training Epoch 29  29.3% | batch:       201 of       686\t|\tloss: 0.824117\n",
      "Training Epoch 29  29.4% | batch:       202 of       686\t|\tloss: 0.723694\n",
      "Training Epoch 29  29.6% | batch:       203 of       686\t|\tloss: 1.02812\n",
      "Training Epoch 29  29.7% | batch:       204 of       686\t|\tloss: 0.917835\n",
      "Training Epoch 29  29.9% | batch:       205 of       686\t|\tloss: 0.721431\n",
      "Training Epoch 29  30.0% | batch:       206 of       686\t|\tloss: 0.881099\n",
      "Training Epoch 29  30.2% | batch:       207 of       686\t|\tloss: 0.597388\n",
      "Training Epoch 29  30.3% | batch:       208 of       686\t|\tloss: 0.540707\n",
      "Training Epoch 29  30.5% | batch:       209 of       686\t|\tloss: 0.619832\n",
      "Training Epoch 29  30.6% | batch:       210 of       686\t|\tloss: 0.664627\n",
      "Training Epoch 29  30.8% | batch:       211 of       686\t|\tloss: 0.76372\n",
      "Training Epoch 29  30.9% | batch:       212 of       686\t|\tloss: 0.665981\n",
      "Training Epoch 29  31.0% | batch:       213 of       686\t|\tloss: 0.880494\n",
      "Training Epoch 29  31.2% | batch:       214 of       686\t|\tloss: 0.797533\n",
      "Training Epoch 29  31.3% | batch:       215 of       686\t|\tloss: 0.649208\n",
      "Training Epoch 29  31.5% | batch:       216 of       686\t|\tloss: 0.867585\n",
      "Training Epoch 29  31.6% | batch:       217 of       686\t|\tloss: 0.786783\n",
      "Training Epoch 29  31.8% | batch:       218 of       686\t|\tloss: 0.583031\n",
      "Training Epoch 29  31.9% | batch:       219 of       686\t|\tloss: 0.672982\n",
      "Training Epoch 29  32.1% | batch:       220 of       686\t|\tloss: 0.712244\n",
      "Training Epoch 29  32.2% | batch:       221 of       686\t|\tloss: 0.77888\n",
      "Training Epoch 29  32.4% | batch:       222 of       686\t|\tloss: 0.706587\n",
      "Training Epoch 29  32.5% | batch:       223 of       686\t|\tloss: 0.787755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 29  32.7% | batch:       224 of       686\t|\tloss: 0.690794\n",
      "Training Epoch 29  32.8% | batch:       225 of       686\t|\tloss: 1.01626\n",
      "Training Epoch 29  32.9% | batch:       226 of       686\t|\tloss: 0.781916\n",
      "Training Epoch 29  33.1% | batch:       227 of       686\t|\tloss: 0.771908\n",
      "Training Epoch 29  33.2% | batch:       228 of       686\t|\tloss: 0.705804\n",
      "Training Epoch 29  33.4% | batch:       229 of       686\t|\tloss: 0.869573\n",
      "Training Epoch 29  33.5% | batch:       230 of       686\t|\tloss: 0.710874\n",
      "Training Epoch 29  33.7% | batch:       231 of       686\t|\tloss: 0.626408\n",
      "Training Epoch 29  33.8% | batch:       232 of       686\t|\tloss: 0.64776\n",
      "Training Epoch 29  34.0% | batch:       233 of       686\t|\tloss: 0.531487\n",
      "Training Epoch 29  34.1% | batch:       234 of       686\t|\tloss: 0.787276\n",
      "Training Epoch 29  34.3% | batch:       235 of       686\t|\tloss: 0.611316\n",
      "Training Epoch 29  34.4% | batch:       236 of       686\t|\tloss: 0.703081\n",
      "Training Epoch 29  34.5% | batch:       237 of       686\t|\tloss: 0.654074\n",
      "Training Epoch 29  34.7% | batch:       238 of       686\t|\tloss: 0.724279\n",
      "Training Epoch 29  34.8% | batch:       239 of       686\t|\tloss: 0.667732\n",
      "Training Epoch 29  35.0% | batch:       240 of       686\t|\tloss: 0.749331\n",
      "Training Epoch 29  35.1% | batch:       241 of       686\t|\tloss: 0.668715\n",
      "Training Epoch 29  35.3% | batch:       242 of       686\t|\tloss: 0.691968\n",
      "Training Epoch 29  35.4% | batch:       243 of       686\t|\tloss: 0.687198\n",
      "Training Epoch 29  35.6% | batch:       244 of       686\t|\tloss: 0.819184\n",
      "Training Epoch 29  35.7% | batch:       245 of       686\t|\tloss: 0.59353\n",
      "Training Epoch 29  35.9% | batch:       246 of       686\t|\tloss: 0.506166\n",
      "Training Epoch 29  36.0% | batch:       247 of       686\t|\tloss: 0.752499\n",
      "Training Epoch 29  36.2% | batch:       248 of       686\t|\tloss: 0.653823\n",
      "Training Epoch 29  36.3% | batch:       249 of       686\t|\tloss: 0.630403\n",
      "Training Epoch 29  36.4% | batch:       250 of       686\t|\tloss: 0.6103\n",
      "Training Epoch 29  36.6% | batch:       251 of       686\t|\tloss: 0.858638\n",
      "Training Epoch 29  36.7% | batch:       252 of       686\t|\tloss: 0.647289\n",
      "Training Epoch 29  36.9% | batch:       253 of       686\t|\tloss: 0.661136\n",
      "Training Epoch 29  37.0% | batch:       254 of       686\t|\tloss: 0.794449\n",
      "Training Epoch 29  37.2% | batch:       255 of       686\t|\tloss: 0.627811\n",
      "Training Epoch 29  37.3% | batch:       256 of       686\t|\tloss: 0.699007\n",
      "Training Epoch 29  37.5% | batch:       257 of       686\t|\tloss: 0.756239\n",
      "Training Epoch 29  37.6% | batch:       258 of       686\t|\tloss: 0.604734\n",
      "Training Epoch 29  37.8% | batch:       259 of       686\t|\tloss: 0.738322\n",
      "Training Epoch 29  37.9% | batch:       260 of       686\t|\tloss: 0.743698\n",
      "Training Epoch 29  38.0% | batch:       261 of       686\t|\tloss: 0.496891\n",
      "Training Epoch 29  38.2% | batch:       262 of       686\t|\tloss: 0.586464\n",
      "Training Epoch 29  38.3% | batch:       263 of       686\t|\tloss: 0.813456\n",
      "Training Epoch 29  38.5% | batch:       264 of       686\t|\tloss: 0.814383\n",
      "Training Epoch 29  38.6% | batch:       265 of       686\t|\tloss: 0.658022\n",
      "Training Epoch 29  38.8% | batch:       266 of       686\t|\tloss: 0.729667\n",
      "Training Epoch 29  38.9% | batch:       267 of       686\t|\tloss: 0.705012\n",
      "Training Epoch 29  39.1% | batch:       268 of       686\t|\tloss: 0.675045\n",
      "Training Epoch 29  39.2% | batch:       269 of       686\t|\tloss: 0.617642\n",
      "Training Epoch 29  39.4% | batch:       270 of       686\t|\tloss: 0.688557\n",
      "Training Epoch 29  39.5% | batch:       271 of       686\t|\tloss: 0.584018\n",
      "Training Epoch 29  39.7% | batch:       272 of       686\t|\tloss: 0.567832\n",
      "Training Epoch 29  39.8% | batch:       273 of       686\t|\tloss: 0.763366\n",
      "Training Epoch 29  39.9% | batch:       274 of       686\t|\tloss: 0.695969\n",
      "Training Epoch 29  40.1% | batch:       275 of       686\t|\tloss: 0.853667\n",
      "Training Epoch 29  40.2% | batch:       276 of       686\t|\tloss: 0.724111\n",
      "Training Epoch 29  40.4% | batch:       277 of       686\t|\tloss: 0.700778\n",
      "Training Epoch 29  40.5% | batch:       278 of       686\t|\tloss: 0.719638\n",
      "Training Epoch 29  40.7% | batch:       279 of       686\t|\tloss: 0.695169\n",
      "Training Epoch 29  40.8% | batch:       280 of       686\t|\tloss: 0.549955\n",
      "Training Epoch 29  41.0% | batch:       281 of       686\t|\tloss: 0.855174\n",
      "Training Epoch 29  41.1% | batch:       282 of       686\t|\tloss: 0.631099\n",
      "Training Epoch 29  41.3% | batch:       283 of       686\t|\tloss: 0.654485\n",
      "Training Epoch 29  41.4% | batch:       284 of       686\t|\tloss: 0.860656\n",
      "Training Epoch 29  41.5% | batch:       285 of       686\t|\tloss: 0.693904\n",
      "Training Epoch 29  41.7% | batch:       286 of       686\t|\tloss: 0.621913\n",
      "Training Epoch 29  41.8% | batch:       287 of       686\t|\tloss: 0.749573\n",
      "Training Epoch 29  42.0% | batch:       288 of       686\t|\tloss: 0.558004\n",
      "Training Epoch 29  42.1% | batch:       289 of       686\t|\tloss: 0.689382\n",
      "Training Epoch 29  42.3% | batch:       290 of       686\t|\tloss: 0.882356\n",
      "Training Epoch 29  42.4% | batch:       291 of       686\t|\tloss: 0.733362\n",
      "Training Epoch 29  42.6% | batch:       292 of       686\t|\tloss: 0.933722\n",
      "Training Epoch 29  42.7% | batch:       293 of       686\t|\tloss: 0.660672\n",
      "Training Epoch 29  42.9% | batch:       294 of       686\t|\tloss: 0.872836\n",
      "Training Epoch 29  43.0% | batch:       295 of       686\t|\tloss: 0.758841\n",
      "Training Epoch 29  43.1% | batch:       296 of       686\t|\tloss: 0.761043\n",
      "Training Epoch 29  43.3% | batch:       297 of       686\t|\tloss: 0.663841\n",
      "Training Epoch 29  43.4% | batch:       298 of       686\t|\tloss: 0.675022\n",
      "Training Epoch 29  43.6% | batch:       299 of       686\t|\tloss: 0.620285\n",
      "Training Epoch 29  43.7% | batch:       300 of       686\t|\tloss: 0.782213\n",
      "Training Epoch 29  43.9% | batch:       301 of       686\t|\tloss: 0.577538\n",
      "Training Epoch 29  44.0% | batch:       302 of       686\t|\tloss: 0.769609\n",
      "Training Epoch 29  44.2% | batch:       303 of       686\t|\tloss: 0.823189\n",
      "Training Epoch 29  44.3% | batch:       304 of       686\t|\tloss: 0.846801\n",
      "Training Epoch 29  44.5% | batch:       305 of       686\t|\tloss: 0.934171\n",
      "Training Epoch 29  44.6% | batch:       306 of       686\t|\tloss: 0.611474\n",
      "Training Epoch 29  44.8% | batch:       307 of       686\t|\tloss: 0.571706\n",
      "Training Epoch 29  44.9% | batch:       308 of       686\t|\tloss: 0.649175\n",
      "Training Epoch 29  45.0% | batch:       309 of       686\t|\tloss: 0.642131\n",
      "Training Epoch 29  45.2% | batch:       310 of       686\t|\tloss: 0.696296\n",
      "Training Epoch 29  45.3% | batch:       311 of       686\t|\tloss: 0.765857\n",
      "Training Epoch 29  45.5% | batch:       312 of       686\t|\tloss: 0.950528\n",
      "Training Epoch 29  45.6% | batch:       313 of       686\t|\tloss: 0.874353\n",
      "Training Epoch 29  45.8% | batch:       314 of       686\t|\tloss: 0.782375\n",
      "Training Epoch 29  45.9% | batch:       315 of       686\t|\tloss: 0.628045\n",
      "Training Epoch 29  46.1% | batch:       316 of       686\t|\tloss: 0.908492\n",
      "Training Epoch 29  46.2% | batch:       317 of       686\t|\tloss: 0.63864\n",
      "Training Epoch 29  46.4% | batch:       318 of       686\t|\tloss: 0.585858\n",
      "Training Epoch 29  46.5% | batch:       319 of       686\t|\tloss: 0.559136\n",
      "Training Epoch 29  46.6% | batch:       320 of       686\t|\tloss: 0.595668\n",
      "Training Epoch 29  46.8% | batch:       321 of       686\t|\tloss: 0.569666\n",
      "Training Epoch 29  46.9% | batch:       322 of       686\t|\tloss: 0.566612\n",
      "Training Epoch 29  47.1% | batch:       323 of       686\t|\tloss: 0.766817\n",
      "Training Epoch 29  47.2% | batch:       324 of       686\t|\tloss: 0.719687\n",
      "Training Epoch 29  47.4% | batch:       325 of       686\t|\tloss: 0.585967\n",
      "Training Epoch 29  47.5% | batch:       326 of       686\t|\tloss: 0.589133\n",
      "Training Epoch 29  47.7% | batch:       327 of       686\t|\tloss: 0.705691\n",
      "Training Epoch 29  47.8% | batch:       328 of       686\t|\tloss: 0.650142\n",
      "Training Epoch 29  48.0% | batch:       329 of       686\t|\tloss: 0.653747\n",
      "Training Epoch 29  48.1% | batch:       330 of       686\t|\tloss: 0.773808\n",
      "Training Epoch 29  48.3% | batch:       331 of       686\t|\tloss: 0.715939\n",
      "Training Epoch 29  48.4% | batch:       332 of       686\t|\tloss: 0.528902\n",
      "Training Epoch 29  48.5% | batch:       333 of       686\t|\tloss: 0.667468\n",
      "Training Epoch 29  48.7% | batch:       334 of       686\t|\tloss: 0.674629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 29  48.8% | batch:       335 of       686\t|\tloss: 0.594696\n",
      "Training Epoch 29  49.0% | batch:       336 of       686\t|\tloss: 0.945476\n",
      "Training Epoch 29  49.1% | batch:       337 of       686\t|\tloss: 0.74596\n",
      "Training Epoch 29  49.3% | batch:       338 of       686\t|\tloss: 0.618845\n",
      "Training Epoch 29  49.4% | batch:       339 of       686\t|\tloss: 0.772218\n",
      "Training Epoch 29  49.6% | batch:       340 of       686\t|\tloss: 0.751205\n",
      "Training Epoch 29  49.7% | batch:       341 of       686\t|\tloss: 0.720585\n",
      "Training Epoch 29  49.9% | batch:       342 of       686\t|\tloss: 0.716682\n",
      "Training Epoch 29  50.0% | batch:       343 of       686\t|\tloss: 0.664809\n",
      "Training Epoch 29  50.1% | batch:       344 of       686\t|\tloss: 0.661122\n",
      "Training Epoch 29  50.3% | batch:       345 of       686\t|\tloss: 0.722951\n",
      "Training Epoch 29  50.4% | batch:       346 of       686\t|\tloss: 0.674921\n",
      "Training Epoch 29  50.6% | batch:       347 of       686\t|\tloss: 0.920824\n",
      "Training Epoch 29  50.7% | batch:       348 of       686\t|\tloss: 0.620483\n",
      "Training Epoch 29  50.9% | batch:       349 of       686\t|\tloss: 0.805184\n",
      "Training Epoch 29  51.0% | batch:       350 of       686\t|\tloss: 0.599614\n",
      "Training Epoch 29  51.2% | batch:       351 of       686\t|\tloss: 0.687178\n",
      "Training Epoch 29  51.3% | batch:       352 of       686\t|\tloss: 0.652457\n",
      "Training Epoch 29  51.5% | batch:       353 of       686\t|\tloss: 0.649762\n",
      "Training Epoch 29  51.6% | batch:       354 of       686\t|\tloss: 0.653559\n",
      "Training Epoch 29  51.7% | batch:       355 of       686\t|\tloss: 0.826655\n",
      "Training Epoch 29  51.9% | batch:       356 of       686\t|\tloss: 0.552825\n",
      "Training Epoch 29  52.0% | batch:       357 of       686\t|\tloss: 0.782504\n",
      "Training Epoch 29  52.2% | batch:       358 of       686\t|\tloss: 0.664224\n",
      "Training Epoch 29  52.3% | batch:       359 of       686\t|\tloss: 0.776985\n",
      "Training Epoch 29  52.5% | batch:       360 of       686\t|\tloss: 1.05693\n",
      "Training Epoch 29  52.6% | batch:       361 of       686\t|\tloss: 0.733336\n",
      "Training Epoch 29  52.8% | batch:       362 of       686\t|\tloss: 0.821133\n",
      "Training Epoch 29  52.9% | batch:       363 of       686\t|\tloss: 0.703104\n",
      "Training Epoch 29  53.1% | batch:       364 of       686\t|\tloss: 0.731091\n",
      "Training Epoch 29  53.2% | batch:       365 of       686\t|\tloss: 0.948343\n",
      "Training Epoch 29  53.4% | batch:       366 of       686\t|\tloss: 0.637487\n",
      "Training Epoch 29  53.5% | batch:       367 of       686\t|\tloss: 0.784909\n",
      "Training Epoch 29  53.6% | batch:       368 of       686\t|\tloss: 0.771779\n",
      "Training Epoch 29  53.8% | batch:       369 of       686\t|\tloss: 0.640877\n",
      "Training Epoch 29  53.9% | batch:       370 of       686\t|\tloss: 0.787436\n",
      "Training Epoch 29  54.1% | batch:       371 of       686\t|\tloss: 0.727185\n",
      "Training Epoch 29  54.2% | batch:       372 of       686\t|\tloss: 0.694263\n",
      "Training Epoch 29  54.4% | batch:       373 of       686\t|\tloss: 0.802763\n",
      "Training Epoch 29  54.5% | batch:       374 of       686\t|\tloss: 0.867787\n",
      "Training Epoch 29  54.7% | batch:       375 of       686\t|\tloss: 0.714903\n",
      "Training Epoch 29  54.8% | batch:       376 of       686\t|\tloss: 0.683717\n",
      "Training Epoch 29  55.0% | batch:       377 of       686\t|\tloss: 0.830209\n",
      "Training Epoch 29  55.1% | batch:       378 of       686\t|\tloss: 0.704554\n",
      "Training Epoch 29  55.2% | batch:       379 of       686\t|\tloss: 0.795031\n",
      "Training Epoch 29  55.4% | batch:       380 of       686\t|\tloss: 0.710304\n",
      "Training Epoch 29  55.5% | batch:       381 of       686\t|\tloss: 0.614599\n",
      "Training Epoch 29  55.7% | batch:       382 of       686\t|\tloss: 0.658891\n",
      "Training Epoch 29  55.8% | batch:       383 of       686\t|\tloss: 0.814697\n",
      "Training Epoch 29  56.0% | batch:       384 of       686\t|\tloss: 0.670989\n",
      "Training Epoch 29  56.1% | batch:       385 of       686\t|\tloss: 0.708858\n",
      "Training Epoch 29  56.3% | batch:       386 of       686\t|\tloss: 0.696791\n",
      "Training Epoch 29  56.4% | batch:       387 of       686\t|\tloss: 0.591679\n",
      "Training Epoch 29  56.6% | batch:       388 of       686\t|\tloss: 0.575809\n",
      "Training Epoch 29  56.7% | batch:       389 of       686\t|\tloss: 0.708945\n",
      "Training Epoch 29  56.9% | batch:       390 of       686\t|\tloss: 0.622971\n",
      "Training Epoch 29  57.0% | batch:       391 of       686\t|\tloss: 0.533326\n",
      "Training Epoch 29  57.1% | batch:       392 of       686\t|\tloss: 0.534715\n",
      "Training Epoch 29  57.3% | batch:       393 of       686\t|\tloss: 0.949631\n",
      "Training Epoch 29  57.4% | batch:       394 of       686\t|\tloss: 0.619769\n",
      "Training Epoch 29  57.6% | batch:       395 of       686\t|\tloss: 0.724797\n",
      "Training Epoch 29  57.7% | batch:       396 of       686\t|\tloss: 0.66765\n",
      "Training Epoch 29  57.9% | batch:       397 of       686\t|\tloss: 0.829489\n",
      "Training Epoch 29  58.0% | batch:       398 of       686\t|\tloss: 0.664744\n",
      "Training Epoch 29  58.2% | batch:       399 of       686\t|\tloss: 0.61349\n",
      "Training Epoch 29  58.3% | batch:       400 of       686\t|\tloss: 0.811456\n",
      "Training Epoch 29  58.5% | batch:       401 of       686\t|\tloss: 0.789261\n",
      "Training Epoch 29  58.6% | batch:       402 of       686\t|\tloss: 0.804014\n",
      "Training Epoch 29  58.7% | batch:       403 of       686\t|\tloss: 0.515859\n",
      "Training Epoch 29  58.9% | batch:       404 of       686\t|\tloss: 0.758674\n",
      "Training Epoch 29  59.0% | batch:       405 of       686\t|\tloss: 0.838279\n",
      "Training Epoch 29  59.2% | batch:       406 of       686\t|\tloss: 0.711509\n",
      "Training Epoch 29  59.3% | batch:       407 of       686\t|\tloss: 0.877003\n",
      "Training Epoch 29  59.5% | batch:       408 of       686\t|\tloss: 0.891368\n",
      "Training Epoch 29  59.6% | batch:       409 of       686\t|\tloss: 0.712115\n",
      "Training Epoch 29  59.8% | batch:       410 of       686\t|\tloss: 0.922252\n",
      "Training Epoch 29  59.9% | batch:       411 of       686\t|\tloss: 0.809002\n",
      "Training Epoch 29  60.1% | batch:       412 of       686\t|\tloss: 0.7047\n",
      "Training Epoch 29  60.2% | batch:       413 of       686\t|\tloss: 0.763493\n",
      "Training Epoch 29  60.3% | batch:       414 of       686\t|\tloss: 0.813891\n",
      "Training Epoch 29  60.5% | batch:       415 of       686\t|\tloss: 0.721873\n",
      "Training Epoch 29  60.6% | batch:       416 of       686\t|\tloss: 0.932047\n",
      "Training Epoch 29  60.8% | batch:       417 of       686\t|\tloss: 0.86532\n",
      "Training Epoch 29  60.9% | batch:       418 of       686\t|\tloss: 0.871848\n",
      "Training Epoch 29  61.1% | batch:       419 of       686\t|\tloss: 0.844099\n",
      "Training Epoch 29  61.2% | batch:       420 of       686\t|\tloss: 0.67692\n",
      "Training Epoch 29  61.4% | batch:       421 of       686\t|\tloss: 0.796789\n",
      "Training Epoch 29  61.5% | batch:       422 of       686\t|\tloss: 0.740754\n",
      "Training Epoch 29  61.7% | batch:       423 of       686\t|\tloss: 0.926209\n",
      "Training Epoch 29  61.8% | batch:       424 of       686\t|\tloss: 0.622985\n",
      "Training Epoch 29  62.0% | batch:       425 of       686\t|\tloss: 0.836367\n",
      "Training Epoch 29  62.1% | batch:       426 of       686\t|\tloss: 0.733947\n",
      "Training Epoch 29  62.2% | batch:       427 of       686\t|\tloss: 0.63482\n",
      "Training Epoch 29  62.4% | batch:       428 of       686\t|\tloss: 0.709022\n",
      "Training Epoch 29  62.5% | batch:       429 of       686\t|\tloss: 0.620229\n",
      "Training Epoch 29  62.7% | batch:       430 of       686\t|\tloss: 0.703372\n",
      "Training Epoch 29  62.8% | batch:       431 of       686\t|\tloss: 0.595953\n",
      "Training Epoch 29  63.0% | batch:       432 of       686\t|\tloss: 0.700148\n",
      "Training Epoch 29  63.1% | batch:       433 of       686\t|\tloss: 0.667755\n",
      "Training Epoch 29  63.3% | batch:       434 of       686\t|\tloss: 0.737675\n",
      "Training Epoch 29  63.4% | batch:       435 of       686\t|\tloss: 0.783914\n",
      "Training Epoch 29  63.6% | batch:       436 of       686\t|\tloss: 0.622237\n",
      "Training Epoch 29  63.7% | batch:       437 of       686\t|\tloss: 0.683394\n",
      "Training Epoch 29  63.8% | batch:       438 of       686\t|\tloss: 0.598139\n",
      "Training Epoch 29  64.0% | batch:       439 of       686\t|\tloss: 0.696152\n",
      "Training Epoch 29  64.1% | batch:       440 of       686\t|\tloss: 0.672257\n",
      "Training Epoch 29  64.3% | batch:       441 of       686\t|\tloss: 0.875728\n",
      "Training Epoch 29  64.4% | batch:       442 of       686\t|\tloss: 0.577854\n",
      "Training Epoch 29  64.6% | batch:       443 of       686\t|\tloss: 0.627801\n",
      "Training Epoch 29  64.7% | batch:       444 of       686\t|\tloss: 0.692094\n",
      "Training Epoch 29  64.9% | batch:       445 of       686\t|\tloss: 0.661031\n",
      "Training Epoch 29  65.0% | batch:       446 of       686\t|\tloss: 0.839242\n",
      "Training Epoch 29  65.2% | batch:       447 of       686\t|\tloss: 0.859622\n",
      "Training Epoch 29  65.3% | batch:       448 of       686\t|\tloss: 0.645498\n",
      "Training Epoch 29  65.5% | batch:       449 of       686\t|\tloss: 0.550025\n",
      "Training Epoch 29  65.6% | batch:       450 of       686\t|\tloss: 0.635542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 29  65.7% | batch:       451 of       686\t|\tloss: 0.588033\n",
      "Training Epoch 29  65.9% | batch:       452 of       686\t|\tloss: 0.526117\n",
      "Training Epoch 29  66.0% | batch:       453 of       686\t|\tloss: 0.579683\n",
      "Training Epoch 29  66.2% | batch:       454 of       686\t|\tloss: 0.600464\n",
      "Training Epoch 29  66.3% | batch:       455 of       686\t|\tloss: 0.961884\n",
      "Training Epoch 29  66.5% | batch:       456 of       686\t|\tloss: 0.649841\n",
      "Training Epoch 29  66.6% | batch:       457 of       686\t|\tloss: 0.698805\n",
      "Training Epoch 29  66.8% | batch:       458 of       686\t|\tloss: 0.599417\n",
      "Training Epoch 29  66.9% | batch:       459 of       686\t|\tloss: 0.816025\n",
      "Training Epoch 29  67.1% | batch:       460 of       686\t|\tloss: 0.772689\n",
      "Training Epoch 29  67.2% | batch:       461 of       686\t|\tloss: 0.574001\n",
      "Training Epoch 29  67.3% | batch:       462 of       686\t|\tloss: 0.684929\n",
      "Training Epoch 29  67.5% | batch:       463 of       686\t|\tloss: 0.767647\n",
      "Training Epoch 29  67.6% | batch:       464 of       686\t|\tloss: 0.814434\n",
      "Training Epoch 29  67.8% | batch:       465 of       686\t|\tloss: 0.749364\n",
      "Training Epoch 29  67.9% | batch:       466 of       686\t|\tloss: 0.744543\n",
      "Training Epoch 29  68.1% | batch:       467 of       686\t|\tloss: 0.944439\n",
      "Training Epoch 29  68.2% | batch:       468 of       686\t|\tloss: 0.847262\n",
      "Training Epoch 29  68.4% | batch:       469 of       686\t|\tloss: 0.597053\n",
      "Training Epoch 29  68.5% | batch:       470 of       686\t|\tloss: 0.819839\n",
      "Training Epoch 29  68.7% | batch:       471 of       686\t|\tloss: 0.709223\n",
      "Training Epoch 29  68.8% | batch:       472 of       686\t|\tloss: 0.678891\n",
      "Training Epoch 29  69.0% | batch:       473 of       686\t|\tloss: 0.754036\n",
      "Training Epoch 29  69.1% | batch:       474 of       686\t|\tloss: 0.600106\n",
      "Training Epoch 29  69.2% | batch:       475 of       686\t|\tloss: 0.930301\n",
      "Training Epoch 29  69.4% | batch:       476 of       686\t|\tloss: 0.942648\n",
      "Training Epoch 29  69.5% | batch:       477 of       686\t|\tloss: 0.79508\n",
      "Training Epoch 29  69.7% | batch:       478 of       686\t|\tloss: 0.638996\n",
      "Training Epoch 29  69.8% | batch:       479 of       686\t|\tloss: 0.708684\n",
      "Training Epoch 29  70.0% | batch:       480 of       686\t|\tloss: 0.723227\n",
      "Training Epoch 29  70.1% | batch:       481 of       686\t|\tloss: 0.637692\n",
      "Training Epoch 29  70.3% | batch:       482 of       686\t|\tloss: 0.524078\n",
      "Training Epoch 29  70.4% | batch:       483 of       686\t|\tloss: 0.639962\n",
      "Training Epoch 29  70.6% | batch:       484 of       686\t|\tloss: 0.689771\n",
      "Training Epoch 29  70.7% | batch:       485 of       686\t|\tloss: 0.689637\n",
      "Training Epoch 29  70.8% | batch:       486 of       686\t|\tloss: 0.698493\n",
      "Training Epoch 29  71.0% | batch:       487 of       686\t|\tloss: 0.730339\n",
      "Training Epoch 29  71.1% | batch:       488 of       686\t|\tloss: 0.545966\n",
      "Training Epoch 29  71.3% | batch:       489 of       686\t|\tloss: 0.624522\n",
      "Training Epoch 29  71.4% | batch:       490 of       686\t|\tloss: 0.546027\n",
      "Training Epoch 29  71.6% | batch:       491 of       686\t|\tloss: 0.771201\n",
      "Training Epoch 29  71.7% | batch:       492 of       686\t|\tloss: 0.717333\n",
      "Training Epoch 29  71.9% | batch:       493 of       686\t|\tloss: 0.695925\n",
      "Training Epoch 29  72.0% | batch:       494 of       686\t|\tloss: 0.571712\n",
      "Training Epoch 29  72.2% | batch:       495 of       686\t|\tloss: 0.571429\n",
      "Training Epoch 29  72.3% | batch:       496 of       686\t|\tloss: 0.901665\n",
      "Training Epoch 29  72.4% | batch:       497 of       686\t|\tloss: 1.18874\n",
      "Training Epoch 29  72.6% | batch:       498 of       686\t|\tloss: 0.750827\n",
      "Training Epoch 29  72.7% | batch:       499 of       686\t|\tloss: 0.614878\n",
      "Training Epoch 29  72.9% | batch:       500 of       686\t|\tloss: 0.63389\n",
      "Training Epoch 29  73.0% | batch:       501 of       686\t|\tloss: 0.726982\n",
      "Training Epoch 29  73.2% | batch:       502 of       686\t|\tloss: 0.654419\n",
      "Training Epoch 29  73.3% | batch:       503 of       686\t|\tloss: 0.789611\n",
      "Training Epoch 29  73.5% | batch:       504 of       686\t|\tloss: 0.917558\n",
      "Training Epoch 29  73.6% | batch:       505 of       686\t|\tloss: 0.765644\n",
      "Training Epoch 29  73.8% | batch:       506 of       686\t|\tloss: 0.699085\n",
      "Training Epoch 29  73.9% | batch:       507 of       686\t|\tloss: 0.727377\n",
      "Training Epoch 29  74.1% | batch:       508 of       686\t|\tloss: 0.706815\n",
      "Training Epoch 29  74.2% | batch:       509 of       686\t|\tloss: 0.56959\n",
      "Training Epoch 29  74.3% | batch:       510 of       686\t|\tloss: 0.616375\n",
      "Training Epoch 29  74.5% | batch:       511 of       686\t|\tloss: 0.680768\n",
      "Training Epoch 29  74.6% | batch:       512 of       686\t|\tloss: 0.642437\n",
      "Training Epoch 29  74.8% | batch:       513 of       686\t|\tloss: 0.792707\n",
      "Training Epoch 29  74.9% | batch:       514 of       686\t|\tloss: 0.767506\n",
      "Training Epoch 29  75.1% | batch:       515 of       686\t|\tloss: 0.791839\n",
      "Training Epoch 29  75.2% | batch:       516 of       686\t|\tloss: 0.537308\n",
      "Training Epoch 29  75.4% | batch:       517 of       686\t|\tloss: 0.647147\n",
      "Training Epoch 29  75.5% | batch:       518 of       686\t|\tloss: 0.615352\n",
      "Training Epoch 29  75.7% | batch:       519 of       686\t|\tloss: 0.571923\n",
      "Training Epoch 29  75.8% | batch:       520 of       686\t|\tloss: 0.631419\n",
      "Training Epoch 29  75.9% | batch:       521 of       686\t|\tloss: 0.6777\n",
      "Training Epoch 29  76.1% | batch:       522 of       686\t|\tloss: 0.559856\n",
      "Training Epoch 29  76.2% | batch:       523 of       686\t|\tloss: 0.7024\n",
      "Training Epoch 29  76.4% | batch:       524 of       686\t|\tloss: 0.710845\n",
      "Training Epoch 29  76.5% | batch:       525 of       686\t|\tloss: 0.628037\n",
      "Training Epoch 29  76.7% | batch:       526 of       686\t|\tloss: 0.573423\n",
      "Training Epoch 29  76.8% | batch:       527 of       686\t|\tloss: 0.615008\n",
      "Training Epoch 29  77.0% | batch:       528 of       686\t|\tloss: 0.492695\n",
      "Training Epoch 29  77.1% | batch:       529 of       686\t|\tloss: 0.77089\n",
      "Training Epoch 29  77.3% | batch:       530 of       686\t|\tloss: 1.01147\n",
      "Training Epoch 29  77.4% | batch:       531 of       686\t|\tloss: 0.799041\n",
      "Training Epoch 29  77.6% | batch:       532 of       686\t|\tloss: 0.758541\n",
      "Training Epoch 29  77.7% | batch:       533 of       686\t|\tloss: 0.829772\n",
      "Training Epoch 29  77.8% | batch:       534 of       686\t|\tloss: 0.621628\n",
      "Training Epoch 29  78.0% | batch:       535 of       686\t|\tloss: 0.604359\n",
      "Training Epoch 29  78.1% | batch:       536 of       686\t|\tloss: 0.685641\n",
      "Training Epoch 29  78.3% | batch:       537 of       686\t|\tloss: 0.857644\n",
      "Training Epoch 29  78.4% | batch:       538 of       686\t|\tloss: 0.701954\n",
      "Training Epoch 29  78.6% | batch:       539 of       686\t|\tloss: 0.562837\n",
      "Training Epoch 29  78.7% | batch:       540 of       686\t|\tloss: 0.930979\n",
      "Training Epoch 29  78.9% | batch:       541 of       686\t|\tloss: 0.810203\n",
      "Training Epoch 29  79.0% | batch:       542 of       686\t|\tloss: 0.573378\n",
      "Training Epoch 29  79.2% | batch:       543 of       686\t|\tloss: 0.668195\n",
      "Training Epoch 29  79.3% | batch:       544 of       686\t|\tloss: 0.577598\n",
      "Training Epoch 29  79.4% | batch:       545 of       686\t|\tloss: 0.64726\n",
      "Training Epoch 29  79.6% | batch:       546 of       686\t|\tloss: 0.830994\n",
      "Training Epoch 29  79.7% | batch:       547 of       686\t|\tloss: 0.765953\n",
      "Training Epoch 29  79.9% | batch:       548 of       686\t|\tloss: 0.630328\n",
      "Training Epoch 29  80.0% | batch:       549 of       686\t|\tloss: 0.769221\n",
      "Training Epoch 29  80.2% | batch:       550 of       686\t|\tloss: 0.509269\n",
      "Training Epoch 29  80.3% | batch:       551 of       686\t|\tloss: 0.862865\n",
      "Training Epoch 29  80.5% | batch:       552 of       686\t|\tloss: 0.573833\n",
      "Training Epoch 29  80.6% | batch:       553 of       686\t|\tloss: 0.617881\n",
      "Training Epoch 29  80.8% | batch:       554 of       686\t|\tloss: 0.583706\n",
      "Training Epoch 29  80.9% | batch:       555 of       686\t|\tloss: 0.582332\n",
      "Training Epoch 29  81.0% | batch:       556 of       686\t|\tloss: 0.669349\n",
      "Training Epoch 29  81.2% | batch:       557 of       686\t|\tloss: 0.552104\n",
      "Training Epoch 29  81.3% | batch:       558 of       686\t|\tloss: 0.938203\n",
      "Training Epoch 29  81.5% | batch:       559 of       686\t|\tloss: 0.86762\n",
      "Training Epoch 29  81.6% | batch:       560 of       686\t|\tloss: 0.743439\n",
      "Training Epoch 29  81.8% | batch:       561 of       686\t|\tloss: 0.509005\n",
      "Training Epoch 29  81.9% | batch:       562 of       686\t|\tloss: 0.81636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 29  82.1% | batch:       563 of       686\t|\tloss: 0.737765\n",
      "Training Epoch 29  82.2% | batch:       564 of       686\t|\tloss: 0.582681\n",
      "Training Epoch 29  82.4% | batch:       565 of       686\t|\tloss: 0.633428\n",
      "Training Epoch 29  82.5% | batch:       566 of       686\t|\tloss: 0.53972\n",
      "Training Epoch 29  82.7% | batch:       567 of       686\t|\tloss: 0.799988\n",
      "Training Epoch 29  82.8% | batch:       568 of       686\t|\tloss: 0.79937\n",
      "Training Epoch 29  82.9% | batch:       569 of       686\t|\tloss: 0.637488\n",
      "Training Epoch 29  83.1% | batch:       570 of       686\t|\tloss: 0.747762\n",
      "Training Epoch 29  83.2% | batch:       571 of       686\t|\tloss: 0.965985\n",
      "Training Epoch 29  83.4% | batch:       572 of       686\t|\tloss: 0.547532\n",
      "Training Epoch 29  83.5% | batch:       573 of       686\t|\tloss: 0.545236\n",
      "Training Epoch 29  83.7% | batch:       574 of       686\t|\tloss: 0.601985\n",
      "Training Epoch 29  83.8% | batch:       575 of       686\t|\tloss: 0.747955\n",
      "Training Epoch 29  84.0% | batch:       576 of       686\t|\tloss: 0.735307\n",
      "Training Epoch 29  84.1% | batch:       577 of       686\t|\tloss: 0.610137\n",
      "Training Epoch 29  84.3% | batch:       578 of       686\t|\tloss: 0.641156\n",
      "Training Epoch 29  84.4% | batch:       579 of       686\t|\tloss: 0.533423\n",
      "Training Epoch 29  84.5% | batch:       580 of       686\t|\tloss: 0.587122\n",
      "Training Epoch 29  84.7% | batch:       581 of       686\t|\tloss: 0.752148\n",
      "Training Epoch 29  84.8% | batch:       582 of       686\t|\tloss: 0.603577\n",
      "Training Epoch 29  85.0% | batch:       583 of       686\t|\tloss: 0.654135\n",
      "Training Epoch 29  85.1% | batch:       584 of       686\t|\tloss: 0.770815\n",
      "Training Epoch 29  85.3% | batch:       585 of       686\t|\tloss: 0.864413\n",
      "Training Epoch 29  85.4% | batch:       586 of       686\t|\tloss: 0.693314\n",
      "Training Epoch 29  85.6% | batch:       587 of       686\t|\tloss: 0.678965\n",
      "Training Epoch 29  85.7% | batch:       588 of       686\t|\tloss: 0.63218\n",
      "Training Epoch 29  85.9% | batch:       589 of       686\t|\tloss: 0.75428\n",
      "Training Epoch 29  86.0% | batch:       590 of       686\t|\tloss: 0.664881\n",
      "Training Epoch 29  86.2% | batch:       591 of       686\t|\tloss: 0.782646\n",
      "Training Epoch 29  86.3% | batch:       592 of       686\t|\tloss: 0.520333\n",
      "Training Epoch 29  86.4% | batch:       593 of       686\t|\tloss: 0.852176\n",
      "Training Epoch 29  86.6% | batch:       594 of       686\t|\tloss: 0.548202\n",
      "Training Epoch 29  86.7% | batch:       595 of       686\t|\tloss: 0.564187\n",
      "Training Epoch 29  86.9% | batch:       596 of       686\t|\tloss: 0.558071\n",
      "Training Epoch 29  87.0% | batch:       597 of       686\t|\tloss: 0.628351\n",
      "Training Epoch 29  87.2% | batch:       598 of       686\t|\tloss: 0.859771\n",
      "Training Epoch 29  87.3% | batch:       599 of       686\t|\tloss: 0.612585\n",
      "Training Epoch 29  87.5% | batch:       600 of       686\t|\tloss: 0.703184\n",
      "Training Epoch 29  87.6% | batch:       601 of       686\t|\tloss: 0.581376\n",
      "Training Epoch 29  87.8% | batch:       602 of       686\t|\tloss: 0.722799\n",
      "Training Epoch 29  87.9% | batch:       603 of       686\t|\tloss: 0.697854\n",
      "Training Epoch 29  88.0% | batch:       604 of       686\t|\tloss: 0.805375\n",
      "Training Epoch 29  88.2% | batch:       605 of       686\t|\tloss: 0.506854\n",
      "Training Epoch 29  88.3% | batch:       606 of       686\t|\tloss: 0.706423\n",
      "Training Epoch 29  88.5% | batch:       607 of       686\t|\tloss: 0.615254\n",
      "Training Epoch 29  88.6% | batch:       608 of       686\t|\tloss: 0.720448\n",
      "Training Epoch 29  88.8% | batch:       609 of       686\t|\tloss: 0.925411\n",
      "Training Epoch 29  88.9% | batch:       610 of       686\t|\tloss: 0.852069\n",
      "Training Epoch 29  89.1% | batch:       611 of       686\t|\tloss: 0.553848\n",
      "Training Epoch 29  89.2% | batch:       612 of       686\t|\tloss: 0.770304\n",
      "Training Epoch 29  89.4% | batch:       613 of       686\t|\tloss: 0.710733\n",
      "Training Epoch 29  89.5% | batch:       614 of       686\t|\tloss: 0.690881\n",
      "Training Epoch 29  89.7% | batch:       615 of       686\t|\tloss: 0.611741\n",
      "Training Epoch 29  89.8% | batch:       616 of       686\t|\tloss: 0.675136\n",
      "Training Epoch 29  89.9% | batch:       617 of       686\t|\tloss: 0.604361\n",
      "Training Epoch 29  90.1% | batch:       618 of       686\t|\tloss: 0.771171\n",
      "Training Epoch 29  90.2% | batch:       619 of       686\t|\tloss: 0.682951\n",
      "Training Epoch 29  90.4% | batch:       620 of       686\t|\tloss: 0.90644\n",
      "Training Epoch 29  90.5% | batch:       621 of       686\t|\tloss: 0.865307\n",
      "Training Epoch 29  90.7% | batch:       622 of       686\t|\tloss: 0.691568\n",
      "Training Epoch 29  90.8% | batch:       623 of       686\t|\tloss: 0.687954\n",
      "Training Epoch 29  91.0% | batch:       624 of       686\t|\tloss: 0.762938\n",
      "Training Epoch 29  91.1% | batch:       625 of       686\t|\tloss: 0.641118\n",
      "Training Epoch 29  91.3% | batch:       626 of       686\t|\tloss: 0.741925\n",
      "Training Epoch 29  91.4% | batch:       627 of       686\t|\tloss: 0.720913\n",
      "Training Epoch 29  91.5% | batch:       628 of       686\t|\tloss: 0.697045\n",
      "Training Epoch 29  91.7% | batch:       629 of       686\t|\tloss: 0.714833\n",
      "Training Epoch 29  91.8% | batch:       630 of       686\t|\tloss: 0.810223\n",
      "Training Epoch 29  92.0% | batch:       631 of       686\t|\tloss: 0.649891\n",
      "Training Epoch 29  92.1% | batch:       632 of       686\t|\tloss: 0.559145\n",
      "Training Epoch 29  92.3% | batch:       633 of       686\t|\tloss: 0.681162\n",
      "Training Epoch 29  92.4% | batch:       634 of       686\t|\tloss: 0.668419\n",
      "Training Epoch 29  92.6% | batch:       635 of       686\t|\tloss: 0.527946\n",
      "Training Epoch 29  92.7% | batch:       636 of       686\t|\tloss: 0.570843\n",
      "Training Epoch 29  92.9% | batch:       637 of       686\t|\tloss: 0.693938\n",
      "Training Epoch 29  93.0% | batch:       638 of       686\t|\tloss: 0.673561\n",
      "Training Epoch 29  93.1% | batch:       639 of       686\t|\tloss: 0.722988\n",
      "Training Epoch 29  93.3% | batch:       640 of       686\t|\tloss: 0.636586\n",
      "Training Epoch 29  93.4% | batch:       641 of       686\t|\tloss: 0.742546\n",
      "Training Epoch 29  93.6% | batch:       642 of       686\t|\tloss: 0.776548\n",
      "Training Epoch 29  93.7% | batch:       643 of       686\t|\tloss: 0.769619\n",
      "Training Epoch 29  93.9% | batch:       644 of       686\t|\tloss: 0.948342\n",
      "Training Epoch 29  94.0% | batch:       645 of       686\t|\tloss: 0.56413\n",
      "Training Epoch 29  94.2% | batch:       646 of       686\t|\tloss: 0.867716\n",
      "Training Epoch 29  94.3% | batch:       647 of       686\t|\tloss: 0.639918\n",
      "Training Epoch 29  94.5% | batch:       648 of       686\t|\tloss: 0.603856\n",
      "Training Epoch 29  94.6% | batch:       649 of       686\t|\tloss: 0.646501\n",
      "Training Epoch 29  94.8% | batch:       650 of       686\t|\tloss: 0.795202\n",
      "Training Epoch 29  94.9% | batch:       651 of       686\t|\tloss: 0.744936\n",
      "Training Epoch 29  95.0% | batch:       652 of       686\t|\tloss: 0.849985\n",
      "Training Epoch 29  95.2% | batch:       653 of       686\t|\tloss: 0.649064\n",
      "Training Epoch 29  95.3% | batch:       654 of       686\t|\tloss: 0.970288\n",
      "Training Epoch 29  95.5% | batch:       655 of       686\t|\tloss: 0.605024\n",
      "Training Epoch 29  95.6% | batch:       656 of       686\t|\tloss: 0.688918\n",
      "Training Epoch 29  95.8% | batch:       657 of       686\t|\tloss: 0.82159\n",
      "Training Epoch 29  95.9% | batch:       658 of       686\t|\tloss: 0.661255\n",
      "Training Epoch 29  96.1% | batch:       659 of       686\t|\tloss: 0.652778\n",
      "Training Epoch 29  96.2% | batch:       660 of       686\t|\tloss: 0.539457\n",
      "Training Epoch 29  96.4% | batch:       661 of       686\t|\tloss: 0.824573\n",
      "Training Epoch 29  96.5% | batch:       662 of       686\t|\tloss: 0.764243\n",
      "Training Epoch 29  96.6% | batch:       663 of       686\t|\tloss: 0.509608\n",
      "Training Epoch 29  96.8% | batch:       664 of       686\t|\tloss: 0.560353\n",
      "Training Epoch 29  96.9% | batch:       665 of       686\t|\tloss: 0.672871\n",
      "Training Epoch 29  97.1% | batch:       666 of       686\t|\tloss: 0.622494\n",
      "Training Epoch 29  97.2% | batch:       667 of       686\t|\tloss: 0.70493\n",
      "Training Epoch 29  97.4% | batch:       668 of       686\t|\tloss: 0.527056\n",
      "Training Epoch 29  97.5% | batch:       669 of       686\t|\tloss: 0.542334\n",
      "Training Epoch 29  97.7% | batch:       670 of       686\t|\tloss: 0.824352\n",
      "Training Epoch 29  97.8% | batch:       671 of       686\t|\tloss: 0.973597\n",
      "Training Epoch 29  98.0% | batch:       672 of       686\t|\tloss: 0.750444\n",
      "Training Epoch 29  98.1% | batch:       673 of       686\t|\tloss: 0.60053\n",
      "Training Epoch 29  98.3% | batch:       674 of       686\t|\tloss: 0.587171\n",
      "Training Epoch 29  98.4% | batch:       675 of       686\t|\tloss: 0.593653\n",
      "Training Epoch 29  98.5% | batch:       676 of       686\t|\tloss: 0.574468\n",
      "Training Epoch 29  98.7% | batch:       677 of       686\t|\tloss: 0.725805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 29  98.8% | batch:       678 of       686\t|\tloss: 0.656064\n",
      "Training Epoch 29  99.0% | batch:       679 of       686\t|\tloss: 0.672423\n",
      "Training Epoch 29  99.1% | batch:       680 of       686\t|\tloss: 0.679291\n",
      "Training Epoch 29  99.3% | batch:       681 of       686\t|\tloss: 0.64538\n",
      "Training Epoch 29  99.4% | batch:       682 of       686\t|\tloss: 0.505177\n",
      "Training Epoch 29  99.6% | batch:       683 of       686\t|\tloss: 0.5582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:13:18,584 | INFO : Epoch 29 Training Summary: epoch: 29.000000 | loss: 0.710849 | \n",
      "2023-05-25 22:13:18,585 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 25.404454946517944 seconds\n",
      "\n",
      "2023-05-25 22:13:18,585 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.84947577016107 seconds\n",
      "2023-05-25 22:13:18,586 | INFO : Avg batch train. time: 0.03476599966495783 seconds\n",
      "2023-05-25 22:13:18,586 | INFO : Avg sample train. time: 0.0002719593565215927 seconds\n",
      "2023-05-25 22:13:18,586 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 29  99.7% | batch:       684 of       686\t|\tloss: 0.851239\n",
      "Training Epoch 29  99.9% | batch:       685 of       686\t|\tloss: 0.57047\n",
      "\n",
      "Evaluating Epoch 29   0.0% | batch:         0 of       172\t|\tloss: 1.23947\n",
      "Evaluating Epoch 29   0.6% | batch:         1 of       172\t|\tloss: 1.26724\n",
      "Evaluating Epoch 29   1.2% | batch:         2 of       172\t|\tloss: 0.564246\n",
      "Evaluating Epoch 29   1.7% | batch:         3 of       172\t|\tloss: 1.9304\n",
      "Evaluating Epoch 29   2.3% | batch:         4 of       172\t|\tloss: 1.01379\n",
      "Evaluating Epoch 29   2.9% | batch:         5 of       172\t|\tloss: 1.07214\n",
      "Evaluating Epoch 29   3.5% | batch:         6 of       172\t|\tloss: 1.11546\n",
      "Evaluating Epoch 29   4.1% | batch:         7 of       172\t|\tloss: 2.34363\n",
      "Evaluating Epoch 29   4.7% | batch:         8 of       172\t|\tloss: 0.595675\n",
      "Evaluating Epoch 29   5.2% | batch:         9 of       172\t|\tloss: 1.41426\n",
      "Evaluating Epoch 29   5.8% | batch:        10 of       172\t|\tloss: 1.02687\n",
      "Evaluating Epoch 29   6.4% | batch:        11 of       172\t|\tloss: 1.17999\n",
      "Evaluating Epoch 29   7.0% | batch:        12 of       172\t|\tloss: 1.14742\n",
      "Evaluating Epoch 29   7.6% | batch:        13 of       172\t|\tloss: 1.16561\n",
      "Evaluating Epoch 29   8.1% | batch:        14 of       172\t|\tloss: 1.24192\n",
      "Evaluating Epoch 29   8.7% | batch:        15 of       172\t|\tloss: 1.20293\n",
      "Evaluating Epoch 29   9.3% | batch:        16 of       172\t|\tloss: 1.82718\n",
      "Evaluating Epoch 29   9.9% | batch:        17 of       172\t|\tloss: 0.771707\n",
      "Evaluating Epoch 29  10.5% | batch:        18 of       172\t|\tloss: 18.2274\n",
      "Evaluating Epoch 29  11.0% | batch:        19 of       172\t|\tloss: 1.35773\n",
      "Evaluating Epoch 29  11.6% | batch:        20 of       172\t|\tloss: 2.18583\n",
      "Evaluating Epoch 29  12.2% | batch:        21 of       172\t|\tloss: 0.773106\n",
      "Evaluating Epoch 29  12.8% | batch:        22 of       172\t|\tloss: 4.88578\n",
      "Evaluating Epoch 29  13.4% | batch:        23 of       172\t|\tloss: 3.14198\n",
      "Evaluating Epoch 29  14.0% | batch:        24 of       172\t|\tloss: 1.06475\n",
      "Evaluating Epoch 29  14.5% | batch:        25 of       172\t|\tloss: 1.88979\n",
      "Evaluating Epoch 29  15.1% | batch:        26 of       172\t|\tloss: 7.34852\n",
      "Evaluating Epoch 29  15.7% | batch:        27 of       172\t|\tloss: 14.9838\n",
      "Evaluating Epoch 29  16.3% | batch:        28 of       172\t|\tloss: 0.273028\n",
      "Evaluating Epoch 29  16.9% | batch:        29 of       172\t|\tloss: 1.54058\n",
      "Evaluating Epoch 29  17.4% | batch:        30 of       172\t|\tloss: 1.19671\n",
      "Evaluating Epoch 29  18.0% | batch:        31 of       172\t|\tloss: 0.511734\n",
      "Evaluating Epoch 29  18.6% | batch:        32 of       172\t|\tloss: 0.276146\n",
      "Evaluating Epoch 29  19.2% | batch:        33 of       172\t|\tloss: 0.467122\n",
      "Evaluating Epoch 29  19.8% | batch:        34 of       172\t|\tloss: 0.321515\n",
      "Evaluating Epoch 29  20.3% | batch:        35 of       172\t|\tloss: 0.946987\n",
      "Evaluating Epoch 29  20.9% | batch:        36 of       172\t|\tloss: 2.53118\n",
      "Evaluating Epoch 29  21.5% | batch:        37 of       172\t|\tloss: 4.38207\n",
      "Evaluating Epoch 29  22.1% | batch:        38 of       172\t|\tloss: 3.66588\n",
      "Evaluating Epoch 29  22.7% | batch:        39 of       172\t|\tloss: 8.34278\n",
      "Evaluating Epoch 29  23.3% | batch:        40 of       172\t|\tloss: 0.354129\n",
      "Evaluating Epoch 29  23.8% | batch:        41 of       172\t|\tloss: 0.672738\n",
      "Evaluating Epoch 29  24.4% | batch:        42 of       172\t|\tloss: 0.417689\n",
      "Evaluating Epoch 29  25.0% | batch:        43 of       172\t|\tloss: 18.807\n",
      "Evaluating Epoch 29  25.6% | batch:        44 of       172\t|\tloss: 1.33453\n",
      "Evaluating Epoch 29  26.2% | batch:        45 of       172\t|\tloss: 0.945285\n",
      "Evaluating Epoch 29  26.7% | batch:        46 of       172\t|\tloss: 0.478892\n",
      "Evaluating Epoch 29  27.3% | batch:        47 of       172\t|\tloss: 0.95295\n",
      "Evaluating Epoch 29  27.9% | batch:        48 of       172\t|\tloss: 0.469663\n",
      "Evaluating Epoch 29  28.5% | batch:        49 of       172\t|\tloss: 0.964413\n",
      "Evaluating Epoch 29  29.1% | batch:        50 of       172\t|\tloss: 0.515082\n",
      "Evaluating Epoch 29  29.7% | batch:        51 of       172\t|\tloss: 0.988804\n",
      "Evaluating Epoch 29  30.2% | batch:        52 of       172\t|\tloss: 0.49928\n",
      "Evaluating Epoch 29  30.8% | batch:        53 of       172\t|\tloss: 3.146\n",
      "Evaluating Epoch 29  31.4% | batch:        54 of       172\t|\tloss: 0.866879\n",
      "Evaluating Epoch 29  32.0% | batch:        55 of       172\t|\tloss: 0.355697\n",
      "Evaluating Epoch 29  32.6% | batch:        56 of       172\t|\tloss: 3.57861\n",
      "Evaluating Epoch 29  33.1% | batch:        57 of       172\t|\tloss: 0.325936\n",
      "Evaluating Epoch 29  33.7% | batch:        58 of       172\t|\tloss: 2.34369\n",
      "Evaluating Epoch 29  34.3% | batch:        59 of       172\t|\tloss: 1.25705\n",
      "Evaluating Epoch 29  34.9% | batch:        60 of       172\t|\tloss: 1.13247\n",
      "Evaluating Epoch 29  35.5% | batch:        61 of       172\t|\tloss: 2.01189\n",
      "Evaluating Epoch 29  36.0% | batch:        62 of       172\t|\tloss: 0.856007\n",
      "Evaluating Epoch 29  36.6% | batch:        63 of       172\t|\tloss: 3.24019\n",
      "Evaluating Epoch 29  37.2% | batch:        64 of       172\t|\tloss: 0.641706\n",
      "Evaluating Epoch 29  37.8% | batch:        65 of       172\t|\tloss: 2.48631\n",
      "Evaluating Epoch 29  38.4% | batch:        66 of       172\t|\tloss: 1.65705\n",
      "Evaluating Epoch 29  39.0% | batch:        67 of       172\t|\tloss: 0.382596\n",
      "Evaluating Epoch 29  39.5% | batch:        68 of       172\t|\tloss: 2.67073\n",
      "Evaluating Epoch 29  40.1% | batch:        69 of       172\t|\tloss: 0.747644\n",
      "Evaluating Epoch 29  40.7% | batch:        70 of       172\t|\tloss: 1.8504\n",
      "Evaluating Epoch 29  41.3% | batch:        71 of       172\t|\tloss: 1.45766\n",
      "Evaluating Epoch 29  41.9% | batch:        72 of       172\t|\tloss: 0.651179\n",
      "Evaluating Epoch 29  42.4% | batch:        73 of       172\t|\tloss: 2.70843\n",
      "Evaluating Epoch 29  43.0% | batch:        74 of       172\t|\tloss: 0.421579\n",
      "Evaluating Epoch 29  43.6% | batch:        75 of       172\t|\tloss: 0.345724\n",
      "Evaluating Epoch 29  44.2% | batch:        76 of       172\t|\tloss: 0.465462\n",
      "Evaluating Epoch 29  44.8% | batch:        77 of       172\t|\tloss: 0.389408\n",
      "Evaluating Epoch 29  45.3% | batch:        78 of       172\t|\tloss: 0.376836\n",
      "Evaluating Epoch 29  45.9% | batch:        79 of       172\t|\tloss: 0.290097\n",
      "Evaluating Epoch 29  46.5% | batch:        80 of       172\t|\tloss: 0.33388\n",
      "Evaluating Epoch 29  47.1% | batch:        81 of       172\t|\tloss: 0.444828\n",
      "Evaluating Epoch 29  47.7% | batch:        82 of       172\t|\tloss: 0.412295\n",
      "Evaluating Epoch 29  48.3% | batch:        83 of       172\t|\tloss: 0.36338\n",
      "Evaluating Epoch 29  48.8% | batch:        84 of       172\t|\tloss: 0.541072\n",
      "Evaluating Epoch 29  49.4% | batch:        85 of       172\t|\tloss: 0.574501\n",
      "Evaluating Epoch 29  50.0% | batch:        86 of       172\t|\tloss: 0.560783\n",
      "Evaluating Epoch 29  50.6% | batch:        87 of       172\t|\tloss: 0.589937\n",
      "Evaluating Epoch 29  51.2% | batch:        88 of       172\t|\tloss: 0.477919\n",
      "Evaluating Epoch 29  51.7% | batch:        89 of       172\t|\tloss: 0.622177\n",
      "Evaluating Epoch 29  52.3% | batch:        90 of       172\t|\tloss: 0.542834\n",
      "Evaluating Epoch 29  52.9% | batch:        91 of       172\t|\tloss: 0.261523\n",
      "Evaluating Epoch 29  53.5% | batch:        92 of       172\t|\tloss: 0.47545\n",
      "Evaluating Epoch 29  54.1% | batch:        93 of       172\t|\tloss: 0.898614\n",
      "Evaluating Epoch 29  54.7% | batch:        94 of       172\t|\tloss: 0.302281\n",
      "Evaluating Epoch 29  55.2% | batch:        95 of       172\t|\tloss: 0.462354\n",
      "Evaluating Epoch 29  55.8% | batch:        96 of       172\t|\tloss: 0.773734\n",
      "Evaluating Epoch 29  56.4% | batch:        97 of       172\t|\tloss: 0.546961\n",
      "Evaluating Epoch 29  57.0% | batch:        98 of       172\t|\tloss: 0.417403\n",
      "Evaluating Epoch 29  57.6% | batch:        99 of       172\t|\tloss: 0.535737\n",
      "Evaluating Epoch 29  58.1% | batch:       100 of       172\t|\tloss: 0.603294\n",
      "Evaluating Epoch 29  58.7% | batch:       101 of       172\t|\tloss: 0.33562\n",
      "Evaluating Epoch 29  59.3% | batch:       102 of       172\t|\tloss: 0.459234\n",
      "Evaluating Epoch 29  59.9% | batch:       103 of       172\t|\tloss: 0.946999\n",
      "Evaluating Epoch 29  60.5% | batch:       104 of       172\t|\tloss: 0.475722\n",
      "Evaluating Epoch 29  61.0% | batch:       105 of       172\t|\tloss: 0.224638\n",
      "Evaluating Epoch 29  61.6% | batch:       106 of       172\t|\tloss: 0.520851\n",
      "Evaluating Epoch 29  62.2% | batch:       107 of       172\t|\tloss: 1.02787\n",
      "Evaluating Epoch 29  62.8% | batch:       108 of       172\t|\tloss: 0.200943\n",
      "Evaluating Epoch 29  63.4% | batch:       109 of       172\t|\tloss: 0.500442\n",
      "Evaluating Epoch 29  64.0% | batch:       110 of       172\t|\tloss: 0.878484\n",
      "Evaluating Epoch 29  64.5% | batch:       111 of       172\t|\tloss: 0.507661\n",
      "Evaluating Epoch 29  65.1% | batch:       112 of       172\t|\tloss: 0.467858\n",
      "Evaluating Epoch 29  65.7% | batch:       113 of       172\t|\tloss: 0.677921\n",
      "Evaluating Epoch 29  66.3% | batch:       114 of       172\t|\tloss: 0.873681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 29  66.9% | batch:       115 of       172\t|\tloss: 0.457025\n",
      "Evaluating Epoch 29  67.4% | batch:       116 of       172\t|\tloss: 0.286882\n",
      "Evaluating Epoch 29  68.0% | batch:       117 of       172\t|\tloss: 0.447748\n",
      "Evaluating Epoch 29  68.6% | batch:       118 of       172\t|\tloss: 0.168556\n",
      "Evaluating Epoch 29  69.2% | batch:       119 of       172\t|\tloss: 0.297405\n",
      "Evaluating Epoch 29  69.8% | batch:       120 of       172\t|\tloss: 0.185955\n",
      "Evaluating Epoch 29  70.3% | batch:       121 of       172\t|\tloss: 0.118001\n",
      "Evaluating Epoch 29  70.9% | batch:       122 of       172\t|\tloss: 0.110596\n",
      "Evaluating Epoch 29  71.5% | batch:       123 of       172\t|\tloss: 0.283936\n",
      "Evaluating Epoch 29  72.1% | batch:       124 of       172\t|\tloss: 0.291784\n",
      "Evaluating Epoch 29  72.7% | batch:       125 of       172\t|\tloss: 0.381369\n",
      "Evaluating Epoch 29  73.3% | batch:       126 of       172\t|\tloss: 0.395425\n",
      "Evaluating Epoch 29  73.8% | batch:       127 of       172\t|\tloss: 0.378239\n",
      "Evaluating Epoch 29  74.4% | batch:       128 of       172\t|\tloss: 0.234496\n",
      "Evaluating Epoch 29  75.0% | batch:       129 of       172\t|\tloss: 0.283478\n",
      "Evaluating Epoch 29  75.6% | batch:       130 of       172\t|\tloss: 0.155999\n",
      "Evaluating Epoch 29  76.2% | batch:       131 of       172\t|\tloss: 0.424949\n",
      "Evaluating Epoch 29  76.7% | batch:       132 of       172\t|\tloss: 0.363676\n",
      "Evaluating Epoch 29  77.3% | batch:       133 of       172\t|\tloss: 0.552842\n",
      "Evaluating Epoch 29  77.9% | batch:       134 of       172\t|\tloss: 0.365211\n",
      "Evaluating Epoch 29  78.5% | batch:       135 of       172\t|\tloss: 0.453827\n",
      "Evaluating Epoch 29  79.1% | batch:       136 of       172\t|\tloss: 0.331967\n",
      "Evaluating Epoch 29  79.7% | batch:       137 of       172\t|\tloss: 0.368148\n",
      "Evaluating Epoch 29  80.2% | batch:       138 of       172\t|\tloss: 0.32722\n",
      "Evaluating Epoch 29  80.8% | batch:       139 of       172\t|\tloss: 0.621563\n",
      "Evaluating Epoch 29  81.4% | batch:       140 of       172\t|\tloss: 0.30777\n",
      "Evaluating Epoch 29  82.0% | batch:       141 of       172\t|\tloss: 0.250754\n",
      "Evaluating Epoch 29  82.6% | batch:       142 of       172\t|\tloss: 0.310607\n",
      "Evaluating Epoch 29  83.1% | batch:       143 of       172\t|\tloss: 0.354974\n",
      "Evaluating Epoch 29  83.7% | batch:       144 of       172\t|\tloss: 0.376041\n",
      "Evaluating Epoch 29  84.3% | batch:       145 of       172\t|\tloss: 0.519288\n",
      "Evaluating Epoch 29  84.9% | batch:       146 of       172\t|\tloss: 0.37467\n",
      "Evaluating Epoch 29  85.5% | batch:       147 of       172\t|\tloss: 0.481452\n",
      "Evaluating Epoch 29  86.0% | batch:       148 of       172\t|\tloss: 0.317418\n",
      "Evaluating Epoch 29  86.6% | batch:       149 of       172\t|\tloss: 0.399953\n",
      "Evaluating Epoch 29  87.2% | batch:       150 of       172\t|\tloss: 0.137166\n",
      "Evaluating Epoch 29  87.8% | batch:       151 of       172\t|\tloss: 0.19031\n",
      "Evaluating Epoch 29  88.4% | batch:       152 of       172\t|\tloss: 0.303345\n",
      "Evaluating Epoch 29  89.0% | batch:       153 of       172\t|\tloss: 0.152047\n",
      "Evaluating Epoch 29  89.5% | batch:       154 of       172\t|\tloss: 0.152607\n",
      "Evaluating Epoch 29  90.1% | batch:       155 of       172\t|\tloss: 0.354019\n",
      "Evaluating Epoch 29  90.7% | batch:       156 of       172\t|\tloss: 0.188725\n",
      "Evaluating Epoch 29  91.3% | batch:       157 of       172\t|\tloss: 0.242392\n",
      "Evaluating Epoch 29  91.9% | batch:       158 of       172\t|\tloss: 0.161267\n",
      "Evaluating Epoch 29  92.4% | batch:       159 of       172\t|\tloss: 0.175429\n",
      "Evaluating Epoch 29  93.0% | batch:       160 of       172\t|\tloss: 0.936174\n",
      "Evaluating Epoch 29  93.6% | batch:       161 of       172\t|\tloss: 0.164586\n",
      "Evaluating Epoch 29  94.2% | batch:       162 of       172\t|\tloss: 0.197315\n",
      "Evaluating Epoch 29  94.8% | batch:       163 of       172\t|\tloss: 0.306592\n",
      "Evaluating Epoch 29  95.3% | batch:       164 of       172\t|\tloss: 0.15906\n",
      "Evaluating Epoch 29  95.9% | batch:       165 of       172\t|\tloss: 0.223219\n",
      "Evaluating Epoch 29  96.5% | batch:       166 of       172\t|\tloss: 0.153244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:13:22,024 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.436927318572998 seconds\n",
      "\n",
      "2023-05-25 22:13:22,024 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.036926213900248 seconds\n",
      "2023-05-25 22:13:22,025 | INFO : Avg batch val. time: 0.02347050124360609 seconds\n",
      "2023-05-25 22:13:22,025 | INFO : Avg sample val. time: 0.00018385600099741532 seconds\n",
      "2023-05-25 22:13:22,026 | INFO : Epoch 29 Validation Summary: epoch: 29.000000 | loss: 1.179213 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 29  97.1% | batch:       167 of       172\t|\tloss: 0.180146\n",
      "Evaluating Epoch 29  97.7% | batch:       168 of       172\t|\tloss: 0.204697\n",
      "Evaluating Epoch 29  98.3% | batch:       169 of       172\t|\tloss: 0.109498\n",
      "Evaluating Epoch 29  98.8% | batch:       170 of       172\t|\tloss: 0.14389\n",
      "Evaluating Epoch 29  99.4% | batch:       171 of       172\t|\tloss: 0.250836\n",
      "\n",
      "Training Epoch 30   0.0% | batch:         0 of       686\t|\tloss: 0.617196\n",
      "Training Epoch 30   0.1% | batch:         1 of       686\t|\tloss: 0.677308\n",
      "Training Epoch 30   0.3% | batch:         2 of       686\t|\tloss: 0.692342\n",
      "Training Epoch 30   0.4% | batch:         3 of       686\t|\tloss: 0.93833\n",
      "Training Epoch 30   0.6% | batch:         4 of       686\t|\tloss: 0.668126\n",
      "Training Epoch 30   0.7% | batch:         5 of       686\t|\tloss: 0.78862\n",
      "Training Epoch 30   0.9% | batch:         6 of       686\t|\tloss: 0.470005\n",
      "Training Epoch 30   1.0% | batch:         7 of       686\t|\tloss: 0.580277\n",
      "Training Epoch 30   1.2% | batch:         8 of       686\t|\tloss: 0.576909\n",
      "Training Epoch 30   1.3% | batch:         9 of       686\t|\tloss: 0.732536\n",
      "Training Epoch 30   1.5% | batch:        10 of       686\t|\tloss: 0.660895\n",
      "Training Epoch 30   1.6% | batch:        11 of       686\t|\tloss: 0.555223\n",
      "Training Epoch 30   1.7% | batch:        12 of       686\t|\tloss: 0.688716\n",
      "Training Epoch 30   1.9% | batch:        13 of       686\t|\tloss: 0.833386\n",
      "Training Epoch 30   2.0% | batch:        14 of       686\t|\tloss: 0.635776\n",
      "Training Epoch 30   2.2% | batch:        15 of       686\t|\tloss: 0.870633\n",
      "Training Epoch 30   2.3% | batch:        16 of       686\t|\tloss: 0.61056\n",
      "Training Epoch 30   2.5% | batch:        17 of       686\t|\tloss: 0.644647\n",
      "Training Epoch 30   2.6% | batch:        18 of       686\t|\tloss: 0.807796\n",
      "Training Epoch 30   2.8% | batch:        19 of       686\t|\tloss: 0.940392\n",
      "Training Epoch 30   2.9% | batch:        20 of       686\t|\tloss: 0.690546\n",
      "Training Epoch 30   3.1% | batch:        21 of       686\t|\tloss: 0.676315\n",
      "Training Epoch 30   3.2% | batch:        22 of       686\t|\tloss: 0.81083\n",
      "Training Epoch 30   3.4% | batch:        23 of       686\t|\tloss: 0.813146\n",
      "Training Epoch 30   3.5% | batch:        24 of       686\t|\tloss: 0.731959\n",
      "Training Epoch 30   3.6% | batch:        25 of       686\t|\tloss: 0.686303\n",
      "Training Epoch 30   3.8% | batch:        26 of       686\t|\tloss: 0.484902\n",
      "Training Epoch 30   3.9% | batch:        27 of       686\t|\tloss: 0.988157\n",
      "Training Epoch 30   4.1% | batch:        28 of       686\t|\tloss: 0.623552\n",
      "Training Epoch 30   4.2% | batch:        29 of       686\t|\tloss: 0.545322\n",
      "Training Epoch 30   4.4% | batch:        30 of       686\t|\tloss: 0.525377\n",
      "Training Epoch 30   4.5% | batch:        31 of       686\t|\tloss: 0.811779\n",
      "Training Epoch 30   4.7% | batch:        32 of       686\t|\tloss: 0.728439\n",
      "Training Epoch 30   4.8% | batch:        33 of       686\t|\tloss: 0.796593\n",
      "Training Epoch 30   5.0% | batch:        34 of       686\t|\tloss: 0.636641\n",
      "Training Epoch 30   5.1% | batch:        35 of       686\t|\tloss: 0.99152\n",
      "Training Epoch 30   5.2% | batch:        36 of       686\t|\tloss: 0.602363\n",
      "Training Epoch 30   5.4% | batch:        37 of       686\t|\tloss: 0.795637\n",
      "Training Epoch 30   5.5% | batch:        38 of       686\t|\tloss: 0.64571\n",
      "Training Epoch 30   5.7% | batch:        39 of       686\t|\tloss: 0.620391\n",
      "Training Epoch 30   5.8% | batch:        40 of       686\t|\tloss: 0.611211\n",
      "Training Epoch 30   6.0% | batch:        41 of       686\t|\tloss: 0.678566\n",
      "Training Epoch 30   6.1% | batch:        42 of       686\t|\tloss: 1.01472\n",
      "Training Epoch 30   6.3% | batch:        43 of       686\t|\tloss: 0.616299\n",
      "Training Epoch 30   6.4% | batch:        44 of       686\t|\tloss: 0.713157\n",
      "Training Epoch 30   6.6% | batch:        45 of       686\t|\tloss: 1.23608\n",
      "Training Epoch 30   6.7% | batch:        46 of       686\t|\tloss: 0.662962\n",
      "Training Epoch 30   6.9% | batch:        47 of       686\t|\tloss: 0.551773\n",
      "Training Epoch 30   7.0% | batch:        48 of       686\t|\tloss: 0.740295\n",
      "Training Epoch 30   7.1% | batch:        49 of       686\t|\tloss: 0.924694\n",
      "Training Epoch 30   7.3% | batch:        50 of       686\t|\tloss: 0.522635\n",
      "Training Epoch 30   7.4% | batch:        51 of       686\t|\tloss: 0.590365\n",
      "Training Epoch 30   7.6% | batch:        52 of       686\t|\tloss: 0.664086\n",
      "Training Epoch 30   7.7% | batch:        53 of       686\t|\tloss: 0.679237\n",
      "Training Epoch 30   7.9% | batch:        54 of       686\t|\tloss: 0.789837\n",
      "Training Epoch 30   8.0% | batch:        55 of       686\t|\tloss: 0.681225\n",
      "Training Epoch 30   8.2% | batch:        56 of       686\t|\tloss: 0.792915\n",
      "Training Epoch 30   8.3% | batch:        57 of       686\t|\tloss: 0.57309\n",
      "Training Epoch 30   8.5% | batch:        58 of       686\t|\tloss: 0.719014\n",
      "Training Epoch 30   8.6% | batch:        59 of       686\t|\tloss: 0.73103\n",
      "Training Epoch 30   8.7% | batch:        60 of       686\t|\tloss: 0.805976\n",
      "Training Epoch 30   8.9% | batch:        61 of       686\t|\tloss: 0.645336\n",
      "Training Epoch 30   9.0% | batch:        62 of       686\t|\tloss: 0.558458\n",
      "Training Epoch 30   9.2% | batch:        63 of       686\t|\tloss: 0.621615\n",
      "Training Epoch 30   9.3% | batch:        64 of       686\t|\tloss: 0.644063\n",
      "Training Epoch 30   9.5% | batch:        65 of       686\t|\tloss: 0.708805\n",
      "Training Epoch 30   9.6% | batch:        66 of       686\t|\tloss: 1.10863\n",
      "Training Epoch 30   9.8% | batch:        67 of       686\t|\tloss: 0.564855\n",
      "Training Epoch 30   9.9% | batch:        68 of       686\t|\tloss: 0.648811\n",
      "Training Epoch 30  10.1% | batch:        69 of       686\t|\tloss: 0.603285\n",
      "Training Epoch 30  10.2% | batch:        70 of       686\t|\tloss: 0.698921\n",
      "Training Epoch 30  10.3% | batch:        71 of       686\t|\tloss: 0.751884\n",
      "Training Epoch 30  10.5% | batch:        72 of       686\t|\tloss: 0.576865\n",
      "Training Epoch 30  10.6% | batch:        73 of       686\t|\tloss: 0.738025\n",
      "Training Epoch 30  10.8% | batch:        74 of       686\t|\tloss: 0.683487\n",
      "Training Epoch 30  10.9% | batch:        75 of       686\t|\tloss: 0.619493\n",
      "Training Epoch 30  11.1% | batch:        76 of       686\t|\tloss: 0.700236\n",
      "Training Epoch 30  11.2% | batch:        77 of       686\t|\tloss: 0.644701\n",
      "Training Epoch 30  11.4% | batch:        78 of       686\t|\tloss: 0.796003\n",
      "Training Epoch 30  11.5% | batch:        79 of       686\t|\tloss: 0.556372\n",
      "Training Epoch 30  11.7% | batch:        80 of       686\t|\tloss: 0.693155\n",
      "Training Epoch 30  11.8% | batch:        81 of       686\t|\tloss: 0.759473\n",
      "Training Epoch 30  12.0% | batch:        82 of       686\t|\tloss: 0.716516\n",
      "Training Epoch 30  12.1% | batch:        83 of       686\t|\tloss: 0.691275\n",
      "Training Epoch 30  12.2% | batch:        84 of       686\t|\tloss: 0.536122\n",
      "Training Epoch 30  12.4% | batch:        85 of       686\t|\tloss: 0.755943\n",
      "Training Epoch 30  12.5% | batch:        86 of       686\t|\tloss: 0.696514\n",
      "Training Epoch 30  12.7% | batch:        87 of       686\t|\tloss: 0.73592\n",
      "Training Epoch 30  12.8% | batch:        88 of       686\t|\tloss: 0.787572\n",
      "Training Epoch 30  13.0% | batch:        89 of       686\t|\tloss: 0.75982\n",
      "Training Epoch 30  13.1% | batch:        90 of       686\t|\tloss: 0.696231\n",
      "Training Epoch 30  13.3% | batch:        91 of       686\t|\tloss: 0.675152\n",
      "Training Epoch 30  13.4% | batch:        92 of       686\t|\tloss: 0.65221\n",
      "Training Epoch 30  13.6% | batch:        93 of       686\t|\tloss: 0.590088\n",
      "Training Epoch 30  13.7% | batch:        94 of       686\t|\tloss: 0.824115\n",
      "Training Epoch 30  13.8% | batch:        95 of       686\t|\tloss: 0.484382\n",
      "Training Epoch 30  14.0% | batch:        96 of       686\t|\tloss: 0.7815\n",
      "Training Epoch 30  14.1% | batch:        97 of       686\t|\tloss: 0.827192\n",
      "Training Epoch 30  14.3% | batch:        98 of       686\t|\tloss: 0.630683\n",
      "Training Epoch 30  14.4% | batch:        99 of       686\t|\tloss: 0.70633\n",
      "Training Epoch 30  14.6% | batch:       100 of       686\t|\tloss: 0.705863\n",
      "Training Epoch 30  14.7% | batch:       101 of       686\t|\tloss: 0.779007\n",
      "Training Epoch 30  14.9% | batch:       102 of       686\t|\tloss: 0.623814\n",
      "Training Epoch 30  15.0% | batch:       103 of       686\t|\tloss: 1.01935\n",
      "Training Epoch 30  15.2% | batch:       104 of       686\t|\tloss: 0.611388\n",
      "Training Epoch 30  15.3% | batch:       105 of       686\t|\tloss: 0.78847\n",
      "Training Epoch 30  15.5% | batch:       106 of       686\t|\tloss: 0.772579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 30  15.6% | batch:       107 of       686\t|\tloss: 0.731413\n",
      "Training Epoch 30  15.7% | batch:       108 of       686\t|\tloss: 1.00118\n",
      "Training Epoch 30  15.9% | batch:       109 of       686\t|\tloss: 0.643899\n",
      "Training Epoch 30  16.0% | batch:       110 of       686\t|\tloss: 0.584737\n",
      "Training Epoch 30  16.2% | batch:       111 of       686\t|\tloss: 0.629576\n",
      "Training Epoch 30  16.3% | batch:       112 of       686\t|\tloss: 0.690588\n",
      "Training Epoch 30  16.5% | batch:       113 of       686\t|\tloss: 0.54051\n",
      "Training Epoch 30  16.6% | batch:       114 of       686\t|\tloss: 0.687487\n",
      "Training Epoch 30  16.8% | batch:       115 of       686\t|\tloss: 0.718873\n",
      "Training Epoch 30  16.9% | batch:       116 of       686\t|\tloss: 0.717381\n",
      "Training Epoch 30  17.1% | batch:       117 of       686\t|\tloss: 0.494617\n",
      "Training Epoch 30  17.2% | batch:       118 of       686\t|\tloss: 0.738198\n",
      "Training Epoch 30  17.3% | batch:       119 of       686\t|\tloss: 0.87068\n",
      "Training Epoch 30  17.5% | batch:       120 of       686\t|\tloss: 0.566066\n",
      "Training Epoch 30  17.6% | batch:       121 of       686\t|\tloss: 0.956162\n",
      "Training Epoch 30  17.8% | batch:       122 of       686\t|\tloss: 0.584579\n",
      "Training Epoch 30  17.9% | batch:       123 of       686\t|\tloss: 0.628349\n",
      "Training Epoch 30  18.1% | batch:       124 of       686\t|\tloss: 0.823902\n",
      "Training Epoch 30  18.2% | batch:       125 of       686\t|\tloss: 0.90004\n",
      "Training Epoch 30  18.4% | batch:       126 of       686\t|\tloss: 0.752983\n",
      "Training Epoch 30  18.5% | batch:       127 of       686\t|\tloss: 0.74112\n",
      "Training Epoch 30  18.7% | batch:       128 of       686\t|\tloss: 0.886889\n",
      "Training Epoch 30  18.8% | batch:       129 of       686\t|\tloss: 0.692876\n",
      "Training Epoch 30  19.0% | batch:       130 of       686\t|\tloss: 0.707482\n",
      "Training Epoch 30  19.1% | batch:       131 of       686\t|\tloss: 0.655076\n",
      "Training Epoch 30  19.2% | batch:       132 of       686\t|\tloss: 0.54142\n",
      "Training Epoch 30  19.4% | batch:       133 of       686\t|\tloss: 0.536042\n",
      "Training Epoch 30  19.5% | batch:       134 of       686\t|\tloss: 0.684348\n",
      "Training Epoch 30  19.7% | batch:       135 of       686\t|\tloss: 0.707062\n",
      "Training Epoch 30  19.8% | batch:       136 of       686\t|\tloss: 0.93027\n",
      "Training Epoch 30  20.0% | batch:       137 of       686\t|\tloss: 0.573316\n",
      "Training Epoch 30  20.1% | batch:       138 of       686\t|\tloss: 0.558891\n",
      "Training Epoch 30  20.3% | batch:       139 of       686\t|\tloss: 0.503034\n",
      "Training Epoch 30  20.4% | batch:       140 of       686\t|\tloss: 0.568918\n",
      "Training Epoch 30  20.6% | batch:       141 of       686\t|\tloss: 0.804547\n",
      "Training Epoch 30  20.7% | batch:       142 of       686\t|\tloss: 0.662649\n",
      "Training Epoch 30  20.8% | batch:       143 of       686\t|\tloss: 0.56487\n",
      "Training Epoch 30  21.0% | batch:       144 of       686\t|\tloss: 0.621393\n",
      "Training Epoch 30  21.1% | batch:       145 of       686\t|\tloss: 0.686009\n",
      "Training Epoch 30  21.3% | batch:       146 of       686\t|\tloss: 0.595692\n",
      "Training Epoch 30  21.4% | batch:       147 of       686\t|\tloss: 0.786818\n",
      "Training Epoch 30  21.6% | batch:       148 of       686\t|\tloss: 0.672257\n",
      "Training Epoch 30  21.7% | batch:       149 of       686\t|\tloss: 0.825537\n",
      "Training Epoch 30  21.9% | batch:       150 of       686\t|\tloss: 0.765571\n",
      "Training Epoch 30  22.0% | batch:       151 of       686\t|\tloss: 0.583553\n",
      "Training Epoch 30  22.2% | batch:       152 of       686\t|\tloss: 0.834844\n",
      "Training Epoch 30  22.3% | batch:       153 of       686\t|\tloss: 0.48776\n",
      "Training Epoch 30  22.4% | batch:       154 of       686\t|\tloss: 0.679609\n",
      "Training Epoch 30  22.6% | batch:       155 of       686\t|\tloss: 0.674714\n",
      "Training Epoch 30  22.7% | batch:       156 of       686\t|\tloss: 0.821144\n",
      "Training Epoch 30  22.9% | batch:       157 of       686\t|\tloss: 0.60304\n",
      "Training Epoch 30  23.0% | batch:       158 of       686\t|\tloss: 0.576661\n",
      "Training Epoch 30  23.2% | batch:       159 of       686\t|\tloss: 0.623391\n",
      "Training Epoch 30  23.3% | batch:       160 of       686\t|\tloss: 0.605834\n",
      "Training Epoch 30  23.5% | batch:       161 of       686\t|\tloss: 0.676414\n",
      "Training Epoch 30  23.6% | batch:       162 of       686\t|\tloss: 0.571476\n",
      "Training Epoch 30  23.8% | batch:       163 of       686\t|\tloss: 0.898994\n",
      "Training Epoch 30  23.9% | batch:       164 of       686\t|\tloss: 0.465898\n",
      "Training Epoch 30  24.1% | batch:       165 of       686\t|\tloss: 0.636102\n",
      "Training Epoch 30  24.2% | batch:       166 of       686\t|\tloss: 0.583025\n",
      "Training Epoch 30  24.3% | batch:       167 of       686\t|\tloss: 0.682593\n",
      "Training Epoch 30  24.5% | batch:       168 of       686\t|\tloss: 0.808827\n",
      "Training Epoch 30  24.6% | batch:       169 of       686\t|\tloss: 0.691504\n",
      "Training Epoch 30  24.8% | batch:       170 of       686\t|\tloss: 0.573395\n",
      "Training Epoch 30  24.9% | batch:       171 of       686\t|\tloss: 0.974357\n",
      "Training Epoch 30  25.1% | batch:       172 of       686\t|\tloss: 0.681342\n",
      "Training Epoch 30  25.2% | batch:       173 of       686\t|\tloss: 0.516874\n",
      "Training Epoch 30  25.4% | batch:       174 of       686\t|\tloss: 0.945754\n",
      "Training Epoch 30  25.5% | batch:       175 of       686\t|\tloss: 0.735306\n",
      "Training Epoch 30  25.7% | batch:       176 of       686\t|\tloss: 0.742266\n",
      "Training Epoch 30  25.8% | batch:       177 of       686\t|\tloss: 0.687847\n",
      "Training Epoch 30  25.9% | batch:       178 of       686\t|\tloss: 0.877336\n",
      "Training Epoch 30  26.1% | batch:       179 of       686\t|\tloss: 0.733341\n",
      "Training Epoch 30  26.2% | batch:       180 of       686\t|\tloss: 0.592625\n",
      "Training Epoch 30  26.4% | batch:       181 of       686\t|\tloss: 0.528771\n",
      "Training Epoch 30  26.5% | batch:       182 of       686\t|\tloss: 0.485875\n",
      "Training Epoch 30  26.7% | batch:       183 of       686\t|\tloss: 0.774543\n",
      "Training Epoch 30  26.8% | batch:       184 of       686\t|\tloss: 0.689971\n",
      "Training Epoch 30  27.0% | batch:       185 of       686\t|\tloss: 0.677325\n",
      "Training Epoch 30  27.1% | batch:       186 of       686\t|\tloss: 0.669352\n",
      "Training Epoch 30  27.3% | batch:       187 of       686\t|\tloss: 0.608644\n",
      "Training Epoch 30  27.4% | batch:       188 of       686\t|\tloss: 0.660735\n",
      "Training Epoch 30  27.6% | batch:       189 of       686\t|\tloss: 0.502182\n",
      "Training Epoch 30  27.7% | batch:       190 of       686\t|\tloss: 0.704716\n",
      "Training Epoch 30  27.8% | batch:       191 of       686\t|\tloss: 0.58071\n",
      "Training Epoch 30  28.0% | batch:       192 of       686\t|\tloss: 0.925428\n",
      "Training Epoch 30  28.1% | batch:       193 of       686\t|\tloss: 0.946272\n",
      "Training Epoch 30  28.3% | batch:       194 of       686\t|\tloss: 0.720915\n",
      "Training Epoch 30  28.4% | batch:       195 of       686\t|\tloss: 0.834394\n",
      "Training Epoch 30  28.6% | batch:       196 of       686\t|\tloss: 0.628656\n",
      "Training Epoch 30  28.7% | batch:       197 of       686\t|\tloss: 0.569087\n",
      "Training Epoch 30  28.9% | batch:       198 of       686\t|\tloss: 0.558961\n",
      "Training Epoch 30  29.0% | batch:       199 of       686\t|\tloss: 0.5265\n",
      "Training Epoch 30  29.2% | batch:       200 of       686\t|\tloss: 0.68425\n",
      "Training Epoch 30  29.3% | batch:       201 of       686\t|\tloss: 0.651796\n",
      "Training Epoch 30  29.4% | batch:       202 of       686\t|\tloss: 0.960799\n",
      "Training Epoch 30  29.6% | batch:       203 of       686\t|\tloss: 0.598343\n",
      "Training Epoch 30  29.7% | batch:       204 of       686\t|\tloss: 0.52782\n",
      "Training Epoch 30  29.9% | batch:       205 of       686\t|\tloss: 0.653248\n",
      "Training Epoch 30  30.0% | batch:       206 of       686\t|\tloss: 0.698681\n",
      "Training Epoch 30  30.2% | batch:       207 of       686\t|\tloss: 0.923347\n",
      "Training Epoch 30  30.3% | batch:       208 of       686\t|\tloss: 0.814252\n",
      "Training Epoch 30  30.5% | batch:       209 of       686\t|\tloss: 0.770435\n",
      "Training Epoch 30  30.6% | batch:       210 of       686\t|\tloss: 0.585713\n",
      "Training Epoch 30  30.8% | batch:       211 of       686\t|\tloss: 0.869416\n",
      "Training Epoch 30  30.9% | batch:       212 of       686\t|\tloss: 0.600568\n",
      "Training Epoch 30  31.0% | batch:       213 of       686\t|\tloss: 0.494087\n",
      "Training Epoch 30  31.2% | batch:       214 of       686\t|\tloss: 0.755469\n",
      "Training Epoch 30  31.3% | batch:       215 of       686\t|\tloss: 0.608689\n",
      "Training Epoch 30  31.5% | batch:       216 of       686\t|\tloss: 0.593249\n",
      "Training Epoch 30  31.6% | batch:       217 of       686\t|\tloss: 0.725857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 30  31.8% | batch:       218 of       686\t|\tloss: 0.634768\n",
      "Training Epoch 30  31.9% | batch:       219 of       686\t|\tloss: 0.73645\n",
      "Training Epoch 30  32.1% | batch:       220 of       686\t|\tloss: 0.63321\n",
      "Training Epoch 30  32.2% | batch:       221 of       686\t|\tloss: 0.719409\n",
      "Training Epoch 30  32.4% | batch:       222 of       686\t|\tloss: 0.777496\n",
      "Training Epoch 30  32.5% | batch:       223 of       686\t|\tloss: 0.710042\n",
      "Training Epoch 30  32.7% | batch:       224 of       686\t|\tloss: 0.652339\n",
      "Training Epoch 30  32.8% | batch:       225 of       686\t|\tloss: 0.610952\n",
      "Training Epoch 30  32.9% | batch:       226 of       686\t|\tloss: 0.58792\n",
      "Training Epoch 30  33.1% | batch:       227 of       686\t|\tloss: 0.761547\n",
      "Training Epoch 30  33.2% | batch:       228 of       686\t|\tloss: 0.671545\n",
      "Training Epoch 30  33.4% | batch:       229 of       686\t|\tloss: 0.797823\n",
      "Training Epoch 30  33.5% | batch:       230 of       686\t|\tloss: 0.697019\n",
      "Training Epoch 30  33.7% | batch:       231 of       686\t|\tloss: 0.688087\n",
      "Training Epoch 30  33.8% | batch:       232 of       686\t|\tloss: 0.517557\n",
      "Training Epoch 30  34.0% | batch:       233 of       686\t|\tloss: 0.72133\n",
      "Training Epoch 30  34.1% | batch:       234 of       686\t|\tloss: 0.612904\n",
      "Training Epoch 30  34.3% | batch:       235 of       686\t|\tloss: 0.913716\n",
      "Training Epoch 30  34.4% | batch:       236 of       686\t|\tloss: 0.536088\n",
      "Training Epoch 30  34.5% | batch:       237 of       686\t|\tloss: 0.946993\n",
      "Training Epoch 30  34.7% | batch:       238 of       686\t|\tloss: 0.735555\n",
      "Training Epoch 30  34.8% | batch:       239 of       686\t|\tloss: 0.811805\n",
      "Training Epoch 30  35.0% | batch:       240 of       686\t|\tloss: 0.633679\n",
      "Training Epoch 30  35.1% | batch:       241 of       686\t|\tloss: 0.830401\n",
      "Training Epoch 30  35.3% | batch:       242 of       686\t|\tloss: 0.640478\n",
      "Training Epoch 30  35.4% | batch:       243 of       686\t|\tloss: 0.609441\n",
      "Training Epoch 30  35.6% | batch:       244 of       686\t|\tloss: 0.843806\n",
      "Training Epoch 30  35.7% | batch:       245 of       686\t|\tloss: 0.673408\n",
      "Training Epoch 30  35.9% | batch:       246 of       686\t|\tloss: 0.907311\n",
      "Training Epoch 30  36.0% | batch:       247 of       686\t|\tloss: 0.563632\n",
      "Training Epoch 30  36.2% | batch:       248 of       686\t|\tloss: 0.811754\n",
      "Training Epoch 30  36.3% | batch:       249 of       686\t|\tloss: 0.623708\n",
      "Training Epoch 30  36.4% | batch:       250 of       686\t|\tloss: 0.744668\n",
      "Training Epoch 30  36.6% | batch:       251 of       686\t|\tloss: 0.720425\n",
      "Training Epoch 30  36.7% | batch:       252 of       686\t|\tloss: 0.827519\n",
      "Training Epoch 30  36.9% | batch:       253 of       686\t|\tloss: 0.760522\n",
      "Training Epoch 30  37.0% | batch:       254 of       686\t|\tloss: 0.598599\n",
      "Training Epoch 30  37.2% | batch:       255 of       686\t|\tloss: 0.764205\n",
      "Training Epoch 30  37.3% | batch:       256 of       686\t|\tloss: 0.684932\n",
      "Training Epoch 30  37.5% | batch:       257 of       686\t|\tloss: 0.641033\n",
      "Training Epoch 30  37.6% | batch:       258 of       686\t|\tloss: 0.638942\n",
      "Training Epoch 30  37.8% | batch:       259 of       686\t|\tloss: 0.704625\n",
      "Training Epoch 30  37.9% | batch:       260 of       686\t|\tloss: 0.701294\n",
      "Training Epoch 30  38.0% | batch:       261 of       686\t|\tloss: 0.7237\n",
      "Training Epoch 30  38.2% | batch:       262 of       686\t|\tloss: 0.67582\n",
      "Training Epoch 30  38.3% | batch:       263 of       686\t|\tloss: 0.609611\n",
      "Training Epoch 30  38.5% | batch:       264 of       686\t|\tloss: 0.701564\n",
      "Training Epoch 30  38.6% | batch:       265 of       686\t|\tloss: 0.565478\n",
      "Training Epoch 30  38.8% | batch:       266 of       686\t|\tloss: 0.591398\n",
      "Training Epoch 30  38.9% | batch:       267 of       686\t|\tloss: 0.706491\n",
      "Training Epoch 30  39.1% | batch:       268 of       686\t|\tloss: 0.65462\n",
      "Training Epoch 30  39.2% | batch:       269 of       686\t|\tloss: 0.771074\n",
      "Training Epoch 30  39.4% | batch:       270 of       686\t|\tloss: 0.798184\n",
      "Training Epoch 30  39.5% | batch:       271 of       686\t|\tloss: 0.547279\n",
      "Training Epoch 30  39.7% | batch:       272 of       686\t|\tloss: 0.672965\n",
      "Training Epoch 30  39.8% | batch:       273 of       686\t|\tloss: 0.697383\n",
      "Training Epoch 30  39.9% | batch:       274 of       686\t|\tloss: 0.845695\n",
      "Training Epoch 30  40.1% | batch:       275 of       686\t|\tloss: 0.912155\n",
      "Training Epoch 30  40.2% | batch:       276 of       686\t|\tloss: 0.637526\n",
      "Training Epoch 30  40.4% | batch:       277 of       686\t|\tloss: 0.841257\n",
      "Training Epoch 30  40.5% | batch:       278 of       686\t|\tloss: 0.742113\n",
      "Training Epoch 30  40.7% | batch:       279 of       686\t|\tloss: 0.630983\n",
      "Training Epoch 30  40.8% | batch:       280 of       686\t|\tloss: 0.691635\n",
      "Training Epoch 30  41.0% | batch:       281 of       686\t|\tloss: 0.718292\n",
      "Training Epoch 30  41.1% | batch:       282 of       686\t|\tloss: 0.7257\n",
      "Training Epoch 30  41.3% | batch:       283 of       686\t|\tloss: 0.707694\n",
      "Training Epoch 30  41.4% | batch:       284 of       686\t|\tloss: 0.870314\n",
      "Training Epoch 30  41.5% | batch:       285 of       686\t|\tloss: 0.587013\n",
      "Training Epoch 30  41.7% | batch:       286 of       686\t|\tloss: 0.541871\n",
      "Training Epoch 30  41.8% | batch:       287 of       686\t|\tloss: 0.701553\n",
      "Training Epoch 30  42.0% | batch:       288 of       686\t|\tloss: 0.722962\n",
      "Training Epoch 30  42.1% | batch:       289 of       686\t|\tloss: 0.47703\n",
      "Training Epoch 30  42.3% | batch:       290 of       686\t|\tloss: 0.622412\n",
      "Training Epoch 30  42.4% | batch:       291 of       686\t|\tloss: 0.647199\n",
      "Training Epoch 30  42.6% | batch:       292 of       686\t|\tloss: 0.677303\n",
      "Training Epoch 30  42.7% | batch:       293 of       686\t|\tloss: 0.602715\n",
      "Training Epoch 30  42.9% | batch:       294 of       686\t|\tloss: 0.496589\n",
      "Training Epoch 30  43.0% | batch:       295 of       686\t|\tloss: 0.588827\n",
      "Training Epoch 30  43.1% | batch:       296 of       686\t|\tloss: 0.566606\n",
      "Training Epoch 30  43.3% | batch:       297 of       686\t|\tloss: 0.59222\n",
      "Training Epoch 30  43.4% | batch:       298 of       686\t|\tloss: 0.742015\n",
      "Training Epoch 30  43.6% | batch:       299 of       686\t|\tloss: 0.902005\n",
      "Training Epoch 30  43.7% | batch:       300 of       686\t|\tloss: 0.692061\n",
      "Training Epoch 30  43.9% | batch:       301 of       686\t|\tloss: 0.610372\n",
      "Training Epoch 30  44.0% | batch:       302 of       686\t|\tloss: 0.795043\n",
      "Training Epoch 30  44.2% | batch:       303 of       686\t|\tloss: 0.55766\n",
      "Training Epoch 30  44.3% | batch:       304 of       686\t|\tloss: 0.573294\n",
      "Training Epoch 30  44.5% | batch:       305 of       686\t|\tloss: 0.790858\n",
      "Training Epoch 30  44.6% | batch:       306 of       686\t|\tloss: 0.697431\n",
      "Training Epoch 30  44.8% | batch:       307 of       686\t|\tloss: 0.580074\n",
      "Training Epoch 30  44.9% | batch:       308 of       686\t|\tloss: 0.777067\n",
      "Training Epoch 30  45.0% | batch:       309 of       686\t|\tloss: 0.58959\n",
      "Training Epoch 30  45.2% | batch:       310 of       686\t|\tloss: 0.582333\n",
      "Training Epoch 30  45.3% | batch:       311 of       686\t|\tloss: 0.637007\n",
      "Training Epoch 30  45.5% | batch:       312 of       686\t|\tloss: 0.600414\n",
      "Training Epoch 30  45.6% | batch:       313 of       686\t|\tloss: 0.609695\n",
      "Training Epoch 30  45.8% | batch:       314 of       686\t|\tloss: 0.80123\n",
      "Training Epoch 30  45.9% | batch:       315 of       686\t|\tloss: 0.665437\n",
      "Training Epoch 30  46.1% | batch:       316 of       686\t|\tloss: 0.80382\n",
      "Training Epoch 30  46.2% | batch:       317 of       686\t|\tloss: 0.518583\n",
      "Training Epoch 30  46.4% | batch:       318 of       686\t|\tloss: 0.590764\n",
      "Training Epoch 30  46.5% | batch:       319 of       686\t|\tloss: 0.630122\n",
      "Training Epoch 30  46.6% | batch:       320 of       686\t|\tloss: 0.608566\n",
      "Training Epoch 30  46.8% | batch:       321 of       686\t|\tloss: 0.692522\n",
      "Training Epoch 30  46.9% | batch:       322 of       686\t|\tloss: 0.690045\n",
      "Training Epoch 30  47.1% | batch:       323 of       686\t|\tloss: 0.609604\n",
      "Training Epoch 30  47.2% | batch:       324 of       686\t|\tloss: 0.599225\n",
      "Training Epoch 30  47.4% | batch:       325 of       686\t|\tloss: 0.786998\n",
      "Training Epoch 30  47.5% | batch:       326 of       686\t|\tloss: 0.724484\n",
      "Training Epoch 30  47.7% | batch:       327 of       686\t|\tloss: 0.73733\n",
      "Training Epoch 30  47.8% | batch:       328 of       686\t|\tloss: 0.633092\n",
      "Training Epoch 30  48.0% | batch:       329 of       686\t|\tloss: 0.975633\n",
      "Training Epoch 30  48.1% | batch:       330 of       686\t|\tloss: 0.785715\n",
      "Training Epoch 30  48.3% | batch:       331 of       686\t|\tloss: 0.826384\n",
      "Training Epoch 30  48.4% | batch:       332 of       686\t|\tloss: 0.677928\n",
      "Training Epoch 30  48.5% | batch:       333 of       686\t|\tloss: 0.553778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 30  48.7% | batch:       334 of       686\t|\tloss: 0.762554\n",
      "Training Epoch 30  48.8% | batch:       335 of       686\t|\tloss: 0.605064\n",
      "Training Epoch 30  49.0% | batch:       336 of       686\t|\tloss: 0.645566\n",
      "Training Epoch 30  49.1% | batch:       337 of       686\t|\tloss: 0.664455\n",
      "Training Epoch 30  49.3% | batch:       338 of       686\t|\tloss: 0.690534\n",
      "Training Epoch 30  49.4% | batch:       339 of       686\t|\tloss: 0.73013\n",
      "Training Epoch 30  49.6% | batch:       340 of       686\t|\tloss: 0.560741\n",
      "Training Epoch 30  49.7% | batch:       341 of       686\t|\tloss: 0.660113\n",
      "Training Epoch 30  49.9% | batch:       342 of       686\t|\tloss: 0.635874\n",
      "Training Epoch 30  50.0% | batch:       343 of       686\t|\tloss: 0.630182\n",
      "Training Epoch 30  50.1% | batch:       344 of       686\t|\tloss: 0.699655\n",
      "Training Epoch 30  50.3% | batch:       345 of       686\t|\tloss: 0.696146\n",
      "Training Epoch 30  50.4% | batch:       346 of       686\t|\tloss: 0.528491\n",
      "Training Epoch 30  50.6% | batch:       347 of       686\t|\tloss: 0.728058\n",
      "Training Epoch 30  50.7% | batch:       348 of       686\t|\tloss: 0.982359\n",
      "Training Epoch 30  50.9% | batch:       349 of       686\t|\tloss: 0.634659\n",
      "Training Epoch 30  51.0% | batch:       350 of       686\t|\tloss: 0.692214\n",
      "Training Epoch 30  51.2% | batch:       351 of       686\t|\tloss: 0.756893\n",
      "Training Epoch 30  51.3% | batch:       352 of       686\t|\tloss: 0.796201\n",
      "Training Epoch 30  51.5% | batch:       353 of       686\t|\tloss: 0.688009\n",
      "Training Epoch 30  51.6% | batch:       354 of       686\t|\tloss: 0.64155\n",
      "Training Epoch 30  51.7% | batch:       355 of       686\t|\tloss: 0.628059\n",
      "Training Epoch 30  51.9% | batch:       356 of       686\t|\tloss: 0.650442\n",
      "Training Epoch 30  52.0% | batch:       357 of       686\t|\tloss: 0.53737\n",
      "Training Epoch 30  52.2% | batch:       358 of       686\t|\tloss: 0.723719\n",
      "Training Epoch 30  52.3% | batch:       359 of       686\t|\tloss: 0.900012\n",
      "Training Epoch 30  52.5% | batch:       360 of       686\t|\tloss: 0.631654\n",
      "Training Epoch 30  52.6% | batch:       361 of       686\t|\tloss: 0.570618\n",
      "Training Epoch 30  52.8% | batch:       362 of       686\t|\tloss: 0.932346\n",
      "Training Epoch 30  52.9% | batch:       363 of       686\t|\tloss: 0.780961\n",
      "Training Epoch 30  53.1% | batch:       364 of       686\t|\tloss: 0.708696\n",
      "Training Epoch 30  53.2% | batch:       365 of       686\t|\tloss: 0.609457\n",
      "Training Epoch 30  53.4% | batch:       366 of       686\t|\tloss: 0.736782\n",
      "Training Epoch 30  53.5% | batch:       367 of       686\t|\tloss: 0.719928\n",
      "Training Epoch 30  53.6% | batch:       368 of       686\t|\tloss: 0.712396\n",
      "Training Epoch 30  53.8% | batch:       369 of       686\t|\tloss: 0.506654\n",
      "Training Epoch 30  53.9% | batch:       370 of       686\t|\tloss: 0.597659\n",
      "Training Epoch 30  54.1% | batch:       371 of       686\t|\tloss: 0.550265\n",
      "Training Epoch 30  54.2% | batch:       372 of       686\t|\tloss: 0.777671\n",
      "Training Epoch 30  54.4% | batch:       373 of       686\t|\tloss: 0.567861\n",
      "Training Epoch 30  54.5% | batch:       374 of       686\t|\tloss: 0.662385\n",
      "Training Epoch 30  54.7% | batch:       375 of       686\t|\tloss: 0.495579\n",
      "Training Epoch 30  54.8% | batch:       376 of       686\t|\tloss: 0.508349\n",
      "Training Epoch 30  55.0% | batch:       377 of       686\t|\tloss: 0.735023\n",
      "Training Epoch 30  55.1% | batch:       378 of       686\t|\tloss: 0.843855\n",
      "Training Epoch 30  55.2% | batch:       379 of       686\t|\tloss: 0.545507\n",
      "Training Epoch 30  55.4% | batch:       380 of       686\t|\tloss: 0.770158\n",
      "Training Epoch 30  55.5% | batch:       381 of       686\t|\tloss: 0.80373\n",
      "Training Epoch 30  55.7% | batch:       382 of       686\t|\tloss: 0.688749\n",
      "Training Epoch 30  55.8% | batch:       383 of       686\t|\tloss: 0.735212\n",
      "Training Epoch 30  56.0% | batch:       384 of       686\t|\tloss: 0.647153\n",
      "Training Epoch 30  56.1% | batch:       385 of       686\t|\tloss: 0.689245\n",
      "Training Epoch 30  56.3% | batch:       386 of       686\t|\tloss: 0.715575\n",
      "Training Epoch 30  56.4% | batch:       387 of       686\t|\tloss: 0.886525\n",
      "Training Epoch 30  56.6% | batch:       388 of       686\t|\tloss: 0.672116\n",
      "Training Epoch 30  56.7% | batch:       389 of       686\t|\tloss: 0.719528\n",
      "Training Epoch 30  56.9% | batch:       390 of       686\t|\tloss: 0.542067\n",
      "Training Epoch 30  57.0% | batch:       391 of       686\t|\tloss: 0.738155\n",
      "Training Epoch 30  57.1% | batch:       392 of       686\t|\tloss: 0.658439\n",
      "Training Epoch 30  57.3% | batch:       393 of       686\t|\tloss: 0.665163\n",
      "Training Epoch 30  57.4% | batch:       394 of       686\t|\tloss: 0.926333\n",
      "Training Epoch 30  57.6% | batch:       395 of       686\t|\tloss: 0.706242\n",
      "Training Epoch 30  57.7% | batch:       396 of       686\t|\tloss: 0.500617\n",
      "Training Epoch 30  57.9% | batch:       397 of       686\t|\tloss: 0.619043\n",
      "Training Epoch 30  58.0% | batch:       398 of       686\t|\tloss: 0.820132\n",
      "Training Epoch 30  58.2% | batch:       399 of       686\t|\tloss: 0.737408\n",
      "Training Epoch 30  58.3% | batch:       400 of       686\t|\tloss: 0.674517\n",
      "Training Epoch 30  58.5% | batch:       401 of       686\t|\tloss: 0.695967\n",
      "Training Epoch 30  58.6% | batch:       402 of       686\t|\tloss: 0.626855\n",
      "Training Epoch 30  58.7% | batch:       403 of       686\t|\tloss: 0.781549\n",
      "Training Epoch 30  58.9% | batch:       404 of       686\t|\tloss: 0.668117\n",
      "Training Epoch 30  59.0% | batch:       405 of       686\t|\tloss: 0.733761\n",
      "Training Epoch 30  59.2% | batch:       406 of       686\t|\tloss: 0.704576\n",
      "Training Epoch 30  59.3% | batch:       407 of       686\t|\tloss: 0.998928\n",
      "Training Epoch 30  59.5% | batch:       408 of       686\t|\tloss: 0.602253\n",
      "Training Epoch 30  59.6% | batch:       409 of       686\t|\tloss: 0.758362\n",
      "Training Epoch 30  59.8% | batch:       410 of       686\t|\tloss: 0.526472\n",
      "Training Epoch 30  59.9% | batch:       411 of       686\t|\tloss: 0.664775\n",
      "Training Epoch 30  60.1% | batch:       412 of       686\t|\tloss: 0.67282\n",
      "Training Epoch 30  60.2% | batch:       413 of       686\t|\tloss: 0.593596\n",
      "Training Epoch 30  60.3% | batch:       414 of       686\t|\tloss: 0.627956\n",
      "Training Epoch 30  60.5% | batch:       415 of       686\t|\tloss: 0.607643\n",
      "Training Epoch 30  60.6% | batch:       416 of       686\t|\tloss: 0.517094\n",
      "Training Epoch 30  60.8% | batch:       417 of       686\t|\tloss: 0.473845\n",
      "Training Epoch 30  60.9% | batch:       418 of       686\t|\tloss: 0.646469\n",
      "Training Epoch 30  61.1% | batch:       419 of       686\t|\tloss: 0.509535\n",
      "Training Epoch 30  61.2% | batch:       420 of       686\t|\tloss: 0.599342\n",
      "Training Epoch 30  61.4% | batch:       421 of       686\t|\tloss: 0.535285\n",
      "Training Epoch 30  61.5% | batch:       422 of       686\t|\tloss: 0.71312\n",
      "Training Epoch 30  61.7% | batch:       423 of       686\t|\tloss: 0.711519\n",
      "Training Epoch 30  61.8% | batch:       424 of       686\t|\tloss: 0.734288\n",
      "Training Epoch 30  62.0% | batch:       425 of       686\t|\tloss: 0.544134\n",
      "Training Epoch 30  62.1% | batch:       426 of       686\t|\tloss: 0.539664\n",
      "Training Epoch 30  62.2% | batch:       427 of       686\t|\tloss: 0.461148\n",
      "Training Epoch 30  62.4% | batch:       428 of       686\t|\tloss: 0.686156\n",
      "Training Epoch 30  62.5% | batch:       429 of       686\t|\tloss: 0.826679\n",
      "Training Epoch 30  62.7% | batch:       430 of       686\t|\tloss: 0.551995\n",
      "Training Epoch 30  62.8% | batch:       431 of       686\t|\tloss: 0.722799\n",
      "Training Epoch 30  63.0% | batch:       432 of       686\t|\tloss: 0.721234\n",
      "Training Epoch 30  63.1% | batch:       433 of       686\t|\tloss: 0.679919\n",
      "Training Epoch 30  63.3% | batch:       434 of       686\t|\tloss: 0.600658\n",
      "Training Epoch 30  63.4% | batch:       435 of       686\t|\tloss: 0.572065\n",
      "Training Epoch 30  63.6% | batch:       436 of       686\t|\tloss: 0.601248\n",
      "Training Epoch 30  63.7% | batch:       437 of       686\t|\tloss: 0.626834\n",
      "Training Epoch 30  63.8% | batch:       438 of       686\t|\tloss: 0.548149\n",
      "Training Epoch 30  64.0% | batch:       439 of       686\t|\tloss: 0.614305\n",
      "Training Epoch 30  64.1% | batch:       440 of       686\t|\tloss: 0.737446\n",
      "Training Epoch 30  64.3% | batch:       441 of       686\t|\tloss: 0.622387\n",
      "Training Epoch 30  64.4% | batch:       442 of       686\t|\tloss: 0.759942\n",
      "Training Epoch 30  64.6% | batch:       443 of       686\t|\tloss: 0.623118\n",
      "Training Epoch 30  64.7% | batch:       444 of       686\t|\tloss: 0.798741\n",
      "Training Epoch 30  64.9% | batch:       445 of       686\t|\tloss: 0.697125\n",
      "Training Epoch 30  65.0% | batch:       446 of       686\t|\tloss: 0.593901\n",
      "Training Epoch 30  65.2% | batch:       447 of       686\t|\tloss: 0.558551\n",
      "Training Epoch 30  65.3% | batch:       448 of       686\t|\tloss: 0.76438\n",
      "Training Epoch 30  65.5% | batch:       449 of       686\t|\tloss: 0.733209\n",
      "Training Epoch 30  65.6% | batch:       450 of       686\t|\tloss: 0.738553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 30  65.7% | batch:       451 of       686\t|\tloss: 0.669292\n",
      "Training Epoch 30  65.9% | batch:       452 of       686\t|\tloss: 0.563345\n",
      "Training Epoch 30  66.0% | batch:       453 of       686\t|\tloss: 0.79948\n",
      "Training Epoch 30  66.2% | batch:       454 of       686\t|\tloss: 0.663712\n",
      "Training Epoch 30  66.3% | batch:       455 of       686\t|\tloss: 0.847533\n",
      "Training Epoch 30  66.5% | batch:       456 of       686\t|\tloss: 0.653241\n",
      "Training Epoch 30  66.6% | batch:       457 of       686\t|\tloss: 0.5862\n",
      "Training Epoch 30  66.8% | batch:       458 of       686\t|\tloss: 0.60432\n",
      "Training Epoch 30  66.9% | batch:       459 of       686\t|\tloss: 0.685511\n",
      "Training Epoch 30  67.1% | batch:       460 of       686\t|\tloss: 0.590971\n",
      "Training Epoch 30  67.2% | batch:       461 of       686\t|\tloss: 0.895662\n",
      "Training Epoch 30  67.3% | batch:       462 of       686\t|\tloss: 0.692576\n",
      "Training Epoch 30  67.5% | batch:       463 of       686\t|\tloss: 0.52393\n",
      "Training Epoch 30  67.6% | batch:       464 of       686\t|\tloss: 0.689466\n",
      "Training Epoch 30  67.8% | batch:       465 of       686\t|\tloss: 0.707091\n",
      "Training Epoch 30  67.9% | batch:       466 of       686\t|\tloss: 0.771536\n",
      "Training Epoch 30  68.1% | batch:       467 of       686\t|\tloss: 0.634026\n",
      "Training Epoch 30  68.2% | batch:       468 of       686\t|\tloss: 1.05878\n",
      "Training Epoch 30  68.4% | batch:       469 of       686\t|\tloss: 0.65297\n",
      "Training Epoch 30  68.5% | batch:       470 of       686\t|\tloss: 0.773116\n",
      "Training Epoch 30  68.7% | batch:       471 of       686\t|\tloss: 0.794443\n",
      "Training Epoch 30  68.8% | batch:       472 of       686\t|\tloss: 0.811817\n",
      "Training Epoch 30  69.0% | batch:       473 of       686\t|\tloss: 0.680388\n",
      "Training Epoch 30  69.1% | batch:       474 of       686\t|\tloss: 0.595359\n",
      "Training Epoch 30  69.2% | batch:       475 of       686\t|\tloss: 0.63659\n",
      "Training Epoch 30  69.4% | batch:       476 of       686\t|\tloss: 0.652875\n",
      "Training Epoch 30  69.5% | batch:       477 of       686\t|\tloss: 0.708105\n",
      "Training Epoch 30  69.7% | batch:       478 of       686\t|\tloss: 0.665367\n",
      "Training Epoch 30  69.8% | batch:       479 of       686\t|\tloss: 0.71421\n",
      "Training Epoch 30  70.0% | batch:       480 of       686\t|\tloss: 0.740677\n",
      "Training Epoch 30  70.1% | batch:       481 of       686\t|\tloss: 0.732969\n",
      "Training Epoch 30  70.3% | batch:       482 of       686\t|\tloss: 0.595291\n",
      "Training Epoch 30  70.4% | batch:       483 of       686\t|\tloss: 0.689222\n",
      "Training Epoch 30  70.6% | batch:       484 of       686\t|\tloss: 0.669654\n",
      "Training Epoch 30  70.7% | batch:       485 of       686\t|\tloss: 0.620234\n",
      "Training Epoch 30  70.8% | batch:       486 of       686\t|\tloss: 0.661378\n",
      "Training Epoch 30  71.0% | batch:       487 of       686\t|\tloss: 0.870554\n",
      "Training Epoch 30  71.1% | batch:       488 of       686\t|\tloss: 0.852241\n",
      "Training Epoch 30  71.3% | batch:       489 of       686\t|\tloss: 0.518696\n",
      "Training Epoch 30  71.4% | batch:       490 of       686\t|\tloss: 0.762741\n",
      "Training Epoch 30  71.6% | batch:       491 of       686\t|\tloss: 0.754252\n",
      "Training Epoch 30  71.7% | batch:       492 of       686\t|\tloss: 0.766801\n",
      "Training Epoch 30  71.9% | batch:       493 of       686\t|\tloss: 0.671444\n",
      "Training Epoch 30  72.0% | batch:       494 of       686\t|\tloss: 0.539543\n",
      "Training Epoch 30  72.2% | batch:       495 of       686\t|\tloss: 0.570764\n",
      "Training Epoch 30  72.3% | batch:       496 of       686\t|\tloss: 0.628227\n",
      "Training Epoch 30  72.4% | batch:       497 of       686\t|\tloss: 0.825496\n",
      "Training Epoch 30  72.6% | batch:       498 of       686\t|\tloss: 0.660791\n",
      "Training Epoch 30  72.7% | batch:       499 of       686\t|\tloss: 0.620924\n",
      "Training Epoch 30  72.9% | batch:       500 of       686\t|\tloss: 0.713082\n",
      "Training Epoch 30  73.0% | batch:       501 of       686\t|\tloss: 0.556038\n",
      "Training Epoch 30  73.2% | batch:       502 of       686\t|\tloss: 0.632786\n",
      "Training Epoch 30  73.3% | batch:       503 of       686\t|\tloss: 0.479754\n",
      "Training Epoch 30  73.5% | batch:       504 of       686\t|\tloss: 0.582848\n",
      "Training Epoch 30  73.6% | batch:       505 of       686\t|\tloss: 0.477528\n",
      "Training Epoch 30  73.8% | batch:       506 of       686\t|\tloss: 0.676622\n",
      "Training Epoch 30  73.9% | batch:       507 of       686\t|\tloss: 0.566036\n",
      "Training Epoch 30  74.1% | batch:       508 of       686\t|\tloss: 0.543071\n",
      "Training Epoch 30  74.2% | batch:       509 of       686\t|\tloss: 0.63589\n",
      "Training Epoch 30  74.3% | batch:       510 of       686\t|\tloss: 0.800103\n",
      "Training Epoch 30  74.5% | batch:       511 of       686\t|\tloss: 0.515651\n",
      "Training Epoch 30  74.6% | batch:       512 of       686\t|\tloss: 0.701536\n",
      "Training Epoch 30  74.8% | batch:       513 of       686\t|\tloss: 0.782972\n",
      "Training Epoch 30  74.9% | batch:       514 of       686\t|\tloss: 0.644984\n",
      "Training Epoch 30  75.1% | batch:       515 of       686\t|\tloss: 0.516797\n",
      "Training Epoch 30  75.2% | batch:       516 of       686\t|\tloss: 0.755882\n",
      "Training Epoch 30  75.4% | batch:       517 of       686\t|\tloss: 0.475412\n",
      "Training Epoch 30  75.5% | batch:       518 of       686\t|\tloss: 0.743275\n",
      "Training Epoch 30  75.7% | batch:       519 of       686\t|\tloss: 0.716017\n",
      "Training Epoch 30  75.8% | batch:       520 of       686\t|\tloss: 0.885264\n",
      "Training Epoch 30  75.9% | batch:       521 of       686\t|\tloss: 0.646017\n",
      "Training Epoch 30  76.1% | batch:       522 of       686\t|\tloss: 0.572951\n",
      "Training Epoch 30  76.2% | batch:       523 of       686\t|\tloss: 0.707564\n",
      "Training Epoch 30  76.4% | batch:       524 of       686\t|\tloss: 0.62813\n",
      "Training Epoch 30  76.5% | batch:       525 of       686\t|\tloss: 0.68801\n",
      "Training Epoch 30  76.7% | batch:       526 of       686\t|\tloss: 0.631766\n",
      "Training Epoch 30  76.8% | batch:       527 of       686\t|\tloss: 0.640831\n",
      "Training Epoch 30  77.0% | batch:       528 of       686\t|\tloss: 0.817385\n",
      "Training Epoch 30  77.1% | batch:       529 of       686\t|\tloss: 0.518221\n",
      "Training Epoch 30  77.3% | batch:       530 of       686\t|\tloss: 0.671735\n",
      "Training Epoch 30  77.4% | batch:       531 of       686\t|\tloss: 0.610643\n",
      "Training Epoch 30  77.6% | batch:       532 of       686\t|\tloss: 0.758792\n",
      "Training Epoch 30  77.7% | batch:       533 of       686\t|\tloss: 0.649\n",
      "Training Epoch 30  77.8% | batch:       534 of       686\t|\tloss: 0.583648\n",
      "Training Epoch 30  78.0% | batch:       535 of       686\t|\tloss: 0.583396\n",
      "Training Epoch 30  78.1% | batch:       536 of       686\t|\tloss: 0.777103\n",
      "Training Epoch 30  78.3% | batch:       537 of       686\t|\tloss: 0.724755\n",
      "Training Epoch 30  78.4% | batch:       538 of       686\t|\tloss: 0.618047\n",
      "Training Epoch 30  78.6% | batch:       539 of       686\t|\tloss: 0.706276\n",
      "Training Epoch 30  78.7% | batch:       540 of       686\t|\tloss: 0.552658\n",
      "Training Epoch 30  78.9% | batch:       541 of       686\t|\tloss: 0.784428\n",
      "Training Epoch 30  79.0% | batch:       542 of       686\t|\tloss: 0.631972\n",
      "Training Epoch 30  79.2% | batch:       543 of       686\t|\tloss: 0.61811\n",
      "Training Epoch 30  79.3% | batch:       544 of       686\t|\tloss: 0.71191\n",
      "Training Epoch 30  79.4% | batch:       545 of       686\t|\tloss: 0.526087\n",
      "Training Epoch 30  79.6% | batch:       546 of       686\t|\tloss: 0.886667\n",
      "Training Epoch 30  79.7% | batch:       547 of       686\t|\tloss: 0.752448\n",
      "Training Epoch 30  79.9% | batch:       548 of       686\t|\tloss: 0.755521\n",
      "Training Epoch 30  80.0% | batch:       549 of       686\t|\tloss: 0.74079\n",
      "Training Epoch 30  80.2% | batch:       550 of       686\t|\tloss: 0.51611\n",
      "Training Epoch 30  80.3% | batch:       551 of       686\t|\tloss: 0.594967\n",
      "Training Epoch 30  80.5% | batch:       552 of       686\t|\tloss: 0.87424\n",
      "Training Epoch 30  80.6% | batch:       553 of       686\t|\tloss: 0.694603\n",
      "Training Epoch 30  80.8% | batch:       554 of       686\t|\tloss: 0.644278\n",
      "Training Epoch 30  80.9% | batch:       555 of       686\t|\tloss: 0.88912\n",
      "Training Epoch 30  81.0% | batch:       556 of       686\t|\tloss: 0.628266\n",
      "Training Epoch 30  81.2% | batch:       557 of       686\t|\tloss: 0.634572\n",
      "Training Epoch 30  81.3% | batch:       558 of       686\t|\tloss: 0.515862\n",
      "Training Epoch 30  81.5% | batch:       559 of       686\t|\tloss: 0.642746\n",
      "Training Epoch 30  81.6% | batch:       560 of       686\t|\tloss: 0.642283\n",
      "Training Epoch 30  81.8% | batch:       561 of       686\t|\tloss: 0.584651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 30  81.9% | batch:       562 of       686\t|\tloss: 0.65994\n",
      "Training Epoch 30  82.1% | batch:       563 of       686\t|\tloss: 0.653956\n",
      "Training Epoch 30  82.2% | batch:       564 of       686\t|\tloss: 0.608488\n",
      "Training Epoch 30  82.4% | batch:       565 of       686\t|\tloss: 0.569694\n",
      "Training Epoch 30  82.5% | batch:       566 of       686\t|\tloss: 0.625301\n",
      "Training Epoch 30  82.7% | batch:       567 of       686\t|\tloss: 0.691682\n",
      "Training Epoch 30  82.8% | batch:       568 of       686\t|\tloss: 0.868453\n",
      "Training Epoch 30  82.9% | batch:       569 of       686\t|\tloss: 0.578431\n",
      "Training Epoch 30  83.1% | batch:       570 of       686\t|\tloss: 0.674018\n",
      "Training Epoch 30  83.2% | batch:       571 of       686\t|\tloss: 0.650266\n",
      "Training Epoch 30  83.4% | batch:       572 of       686\t|\tloss: 0.572419\n",
      "Training Epoch 30  83.5% | batch:       573 of       686\t|\tloss: 0.606752\n",
      "Training Epoch 30  83.7% | batch:       574 of       686\t|\tloss: 0.606359\n",
      "Training Epoch 30  83.8% | batch:       575 of       686\t|\tloss: 0.701301\n",
      "Training Epoch 30  84.0% | batch:       576 of       686\t|\tloss: 0.777658\n",
      "Training Epoch 30  84.1% | batch:       577 of       686\t|\tloss: 0.663427\n",
      "Training Epoch 30  84.3% | batch:       578 of       686\t|\tloss: 0.6609\n",
      "Training Epoch 30  84.4% | batch:       579 of       686\t|\tloss: 0.726834\n",
      "Training Epoch 30  84.5% | batch:       580 of       686\t|\tloss: 0.738336\n",
      "Training Epoch 30  84.7% | batch:       581 of       686\t|\tloss: 0.728899\n",
      "Training Epoch 30  84.8% | batch:       582 of       686\t|\tloss: 0.8681\n",
      "Training Epoch 30  85.0% | batch:       583 of       686\t|\tloss: 0.702745\n",
      "Training Epoch 30  85.1% | batch:       584 of       686\t|\tloss: 0.877761\n",
      "Training Epoch 30  85.3% | batch:       585 of       686\t|\tloss: 0.560387\n",
      "Training Epoch 30  85.4% | batch:       586 of       686\t|\tloss: 0.628052\n",
      "Training Epoch 30  85.6% | batch:       587 of       686\t|\tloss: 0.642759\n",
      "Training Epoch 30  85.7% | batch:       588 of       686\t|\tloss: 0.657865\n",
      "Training Epoch 30  85.9% | batch:       589 of       686\t|\tloss: 0.685453\n",
      "Training Epoch 30  86.0% | batch:       590 of       686\t|\tloss: 0.825254\n",
      "Training Epoch 30  86.2% | batch:       591 of       686\t|\tloss: 0.596792\n",
      "Training Epoch 30  86.3% | batch:       592 of       686\t|\tloss: 0.68797\n",
      "Training Epoch 30  86.4% | batch:       593 of       686\t|\tloss: 0.622663\n",
      "Training Epoch 30  86.6% | batch:       594 of       686\t|\tloss: 0.764003\n",
      "Training Epoch 30  86.7% | batch:       595 of       686\t|\tloss: 0.745959\n",
      "Training Epoch 30  86.9% | batch:       596 of       686\t|\tloss: 0.931864\n",
      "Training Epoch 30  87.0% | batch:       597 of       686\t|\tloss: 0.636764\n",
      "Training Epoch 30  87.2% | batch:       598 of       686\t|\tloss: 0.817779\n",
      "Training Epoch 30  87.3% | batch:       599 of       686\t|\tloss: 0.527946\n",
      "Training Epoch 30  87.5% | batch:       600 of       686\t|\tloss: 0.645771\n",
      "Training Epoch 30  87.6% | batch:       601 of       686\t|\tloss: 0.642776\n",
      "Training Epoch 30  87.8% | batch:       602 of       686\t|\tloss: 0.647081\n",
      "Training Epoch 30  87.9% | batch:       603 of       686\t|\tloss: 0.426178\n",
      "Training Epoch 30  88.0% | batch:       604 of       686\t|\tloss: 1.16287\n",
      "Training Epoch 30  88.2% | batch:       605 of       686\t|\tloss: 0.632627\n",
      "Training Epoch 30  88.3% | batch:       606 of       686\t|\tloss: 0.690624\n",
      "Training Epoch 30  88.5% | batch:       607 of       686\t|\tloss: 0.520539\n",
      "Training Epoch 30  88.6% | batch:       608 of       686\t|\tloss: 0.537514\n",
      "Training Epoch 30  88.8% | batch:       609 of       686\t|\tloss: 0.543762\n",
      "Training Epoch 30  88.9% | batch:       610 of       686\t|\tloss: 0.679479\n",
      "Training Epoch 30  89.1% | batch:       611 of       686\t|\tloss: 0.609055\n",
      "Training Epoch 30  89.2% | batch:       612 of       686\t|\tloss: 0.678408\n",
      "Training Epoch 30  89.4% | batch:       613 of       686\t|\tloss: 0.73678\n",
      "Training Epoch 30  89.5% | batch:       614 of       686\t|\tloss: 0.720408\n",
      "Training Epoch 30  89.7% | batch:       615 of       686\t|\tloss: 0.695924\n",
      "Training Epoch 30  89.8% | batch:       616 of       686\t|\tloss: 0.680156\n",
      "Training Epoch 30  89.9% | batch:       617 of       686\t|\tloss: 0.602963\n",
      "Training Epoch 30  90.1% | batch:       618 of       686\t|\tloss: 0.666847\n",
      "Training Epoch 30  90.2% | batch:       619 of       686\t|\tloss: 0.635726\n",
      "Training Epoch 30  90.4% | batch:       620 of       686\t|\tloss: 0.70164\n",
      "Training Epoch 30  90.5% | batch:       621 of       686\t|\tloss: 0.630906\n",
      "Training Epoch 30  90.7% | batch:       622 of       686\t|\tloss: 0.65326\n",
      "Training Epoch 30  90.8% | batch:       623 of       686\t|\tloss: 0.646495\n",
      "Training Epoch 30  91.0% | batch:       624 of       686\t|\tloss: 0.52881\n",
      "Training Epoch 30  91.1% | batch:       625 of       686\t|\tloss: 0.641561\n",
      "Training Epoch 30  91.3% | batch:       626 of       686\t|\tloss: 0.768165\n",
      "Training Epoch 30  91.4% | batch:       627 of       686\t|\tloss: 0.817341\n",
      "Training Epoch 30  91.5% | batch:       628 of       686\t|\tloss: 0.804551\n",
      "Training Epoch 30  91.7% | batch:       629 of       686\t|\tloss: 0.73455\n",
      "Training Epoch 30  91.8% | batch:       630 of       686\t|\tloss: 0.695309\n",
      "Training Epoch 30  92.0% | batch:       631 of       686\t|\tloss: 0.573412\n",
      "Training Epoch 30  92.1% | batch:       632 of       686\t|\tloss: 0.484789\n",
      "Training Epoch 30  92.3% | batch:       633 of       686\t|\tloss: 0.673312\n",
      "Training Epoch 30  92.4% | batch:       634 of       686\t|\tloss: 0.649448\n",
      "Training Epoch 30  92.6% | batch:       635 of       686\t|\tloss: 0.636752\n",
      "Training Epoch 30  92.7% | batch:       636 of       686\t|\tloss: 0.750142\n",
      "Training Epoch 30  92.9% | batch:       637 of       686\t|\tloss: 0.748672\n",
      "Training Epoch 30  93.0% | batch:       638 of       686\t|\tloss: 0.532713\n",
      "Training Epoch 30  93.1% | batch:       639 of       686\t|\tloss: 0.539571\n",
      "Training Epoch 30  93.3% | batch:       640 of       686\t|\tloss: 0.785499\n",
      "Training Epoch 30  93.4% | batch:       641 of       686\t|\tloss: 0.774575\n",
      "Training Epoch 30  93.6% | batch:       642 of       686\t|\tloss: 0.83508\n",
      "Training Epoch 30  93.7% | batch:       643 of       686\t|\tloss: 0.549311\n",
      "Training Epoch 30  93.9% | batch:       644 of       686\t|\tloss: 0.576674\n",
      "Training Epoch 30  94.0% | batch:       645 of       686\t|\tloss: 0.633497\n",
      "Training Epoch 30  94.2% | batch:       646 of       686\t|\tloss: 0.680375\n",
      "Training Epoch 30  94.3% | batch:       647 of       686\t|\tloss: 0.604443\n",
      "Training Epoch 30  94.5% | batch:       648 of       686\t|\tloss: 0.847269\n",
      "Training Epoch 30  94.6% | batch:       649 of       686\t|\tloss: 0.72892\n",
      "Training Epoch 30  94.8% | batch:       650 of       686\t|\tloss: 0.586279\n",
      "Training Epoch 30  94.9% | batch:       651 of       686\t|\tloss: 0.715364\n",
      "Training Epoch 30  95.0% | batch:       652 of       686\t|\tloss: 0.582536\n",
      "Training Epoch 30  95.2% | batch:       653 of       686\t|\tloss: 0.578422\n",
      "Training Epoch 30  95.3% | batch:       654 of       686\t|\tloss: 0.818891\n",
      "Training Epoch 30  95.5% | batch:       655 of       686\t|\tloss: 0.540753\n",
      "Training Epoch 30  95.6% | batch:       656 of       686\t|\tloss: 0.513787\n",
      "Training Epoch 30  95.8% | batch:       657 of       686\t|\tloss: 0.562104\n",
      "Training Epoch 30  95.9% | batch:       658 of       686\t|\tloss: 0.675064\n",
      "Training Epoch 30  96.1% | batch:       659 of       686\t|\tloss: 0.695675\n",
      "Training Epoch 30  96.2% | batch:       660 of       686\t|\tloss: 0.485337\n",
      "Training Epoch 30  96.4% | batch:       661 of       686\t|\tloss: 0.809118\n",
      "Training Epoch 30  96.5% | batch:       662 of       686\t|\tloss: 0.670787\n",
      "Training Epoch 30  96.6% | batch:       663 of       686\t|\tloss: 0.747816\n",
      "Training Epoch 30  96.8% | batch:       664 of       686\t|\tloss: 0.493979\n",
      "Training Epoch 30  96.9% | batch:       665 of       686\t|\tloss: 0.601217\n",
      "Training Epoch 30  97.1% | batch:       666 of       686\t|\tloss: 0.630034\n",
      "Training Epoch 30  97.2% | batch:       667 of       686\t|\tloss: 0.698783\n",
      "Training Epoch 30  97.4% | batch:       668 of       686\t|\tloss: 0.903649\n",
      "Training Epoch 30  97.5% | batch:       669 of       686\t|\tloss: 0.688684\n",
      "Training Epoch 30  97.7% | batch:       670 of       686\t|\tloss: 0.672976\n",
      "Training Epoch 30  97.8% | batch:       671 of       686\t|\tloss: 0.560501\n",
      "Training Epoch 30  98.0% | batch:       672 of       686\t|\tloss: 0.720399\n",
      "Training Epoch 30  98.1% | batch:       673 of       686\t|\tloss: 0.637455\n",
      "Training Epoch 30  98.3% | batch:       674 of       686\t|\tloss: 0.706623\n",
      "Training Epoch 30  98.4% | batch:       675 of       686\t|\tloss: 0.721891\n",
      "Training Epoch 30  98.5% | batch:       676 of       686\t|\tloss: 0.710945\n",
      "Training Epoch 30  98.7% | batch:       677 of       686\t|\tloss: 0.442721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 30  98.8% | batch:       678 of       686\t|\tloss: 0.765743\n",
      "Training Epoch 30  99.0% | batch:       679 of       686\t|\tloss: 0.61444\n",
      "Training Epoch 30  99.1% | batch:       680 of       686\t|\tloss: 0.570257\n",
      "Training Epoch 30  99.3% | batch:       681 of       686\t|\tloss: 0.514268\n",
      "Training Epoch 30  99.4% | batch:       682 of       686\t|\tloss: 0.566361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:13:48,133 | INFO : Epoch 30 Training Summary: epoch: 30.000000 | loss: 0.681944 | \n",
      "2023-05-25 22:13:48,135 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 26.0944561958313 seconds\n",
      "\n",
      "2023-05-25 22:13:48,138 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 23.924308451016746 seconds\n",
      "2023-05-25 22:13:48,138 | INFO : Avg batch train. time: 0.03487508520556377 seconds\n",
      "2023-05-25 22:13:48,138 | INFO : Avg sample train. time: 0.0002728126854554621 seconds\n",
      "2023-05-25 22:13:48,139 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 30  99.6% | batch:       683 of       686\t|\tloss: 0.560604\n",
      "Training Epoch 30  99.7% | batch:       684 of       686\t|\tloss: 0.616593\n",
      "Training Epoch 30  99.9% | batch:       685 of       686\t|\tloss: 0.70871\n",
      "\n",
      "Evaluating Epoch 30   0.0% | batch:         0 of       172\t|\tloss: 1.41099\n",
      "Evaluating Epoch 30   0.6% | batch:         1 of       172\t|\tloss: 1.38584\n",
      "Evaluating Epoch 30   1.2% | batch:         2 of       172\t|\tloss: 0.664972\n",
      "Evaluating Epoch 30   1.7% | batch:         3 of       172\t|\tloss: 1.94126\n",
      "Evaluating Epoch 30   2.3% | batch:         4 of       172\t|\tloss: 1.12841\n",
      "Evaluating Epoch 30   2.9% | batch:         5 of       172\t|\tloss: 1.142\n",
      "Evaluating Epoch 30   3.5% | batch:         6 of       172\t|\tloss: 1.23732\n",
      "Evaluating Epoch 30   4.1% | batch:         7 of       172\t|\tloss: 2.38596\n",
      "Evaluating Epoch 30   4.7% | batch:         8 of       172\t|\tloss: 0.714465\n",
      "Evaluating Epoch 30   5.2% | batch:         9 of       172\t|\tloss: 1.5137\n",
      "Evaluating Epoch 30   5.8% | batch:        10 of       172\t|\tloss: 1.09024\n",
      "Evaluating Epoch 30   6.4% | batch:        11 of       172\t|\tloss: 1.3157\n",
      "Evaluating Epoch 30   7.0% | batch:        12 of       172\t|\tloss: 1.19642\n",
      "Evaluating Epoch 30   7.6% | batch:        13 of       172\t|\tloss: 1.27774\n",
      "Evaluating Epoch 30   8.1% | batch:        14 of       172\t|\tloss: 1.25562\n",
      "Evaluating Epoch 30   8.7% | batch:        15 of       172\t|\tloss: 1.36375\n",
      "Evaluating Epoch 30   9.3% | batch:        16 of       172\t|\tloss: 1.89678\n",
      "Evaluating Epoch 30   9.9% | batch:        17 of       172\t|\tloss: 0.877247\n",
      "Evaluating Epoch 30  10.5% | batch:        18 of       172\t|\tloss: 21.3098\n",
      "Evaluating Epoch 30  11.0% | batch:        19 of       172\t|\tloss: 1.26533\n",
      "Evaluating Epoch 30  11.6% | batch:        20 of       172\t|\tloss: 1.59419\n",
      "Evaluating Epoch 30  12.2% | batch:        21 of       172\t|\tloss: 0.798242\n",
      "Evaluating Epoch 30  12.8% | batch:        22 of       172\t|\tloss: 6.35237\n",
      "Evaluating Epoch 30  13.4% | batch:        23 of       172\t|\tloss: 3.44795\n",
      "Evaluating Epoch 30  14.0% | batch:        24 of       172\t|\tloss: 0.801152\n",
      "Evaluating Epoch 30  14.5% | batch:        25 of       172\t|\tloss: 1.36992\n",
      "Evaluating Epoch 30  15.1% | batch:        26 of       172\t|\tloss: 8.71103\n",
      "Evaluating Epoch 30  15.7% | batch:        27 of       172\t|\tloss: 16.8697\n",
      "Evaluating Epoch 30  16.3% | batch:        28 of       172\t|\tloss: 0.212831\n",
      "Evaluating Epoch 30  16.9% | batch:        29 of       172\t|\tloss: 1.05603\n",
      "Evaluating Epoch 30  17.4% | batch:        30 of       172\t|\tloss: 1.26728\n",
      "Evaluating Epoch 30  18.0% | batch:        31 of       172\t|\tloss: 0.836745\n",
      "Evaluating Epoch 30  18.6% | batch:        32 of       172\t|\tloss: 0.273361\n",
      "Evaluating Epoch 30  19.2% | batch:        33 of       172\t|\tloss: 0.67313\n",
      "Evaluating Epoch 30  19.8% | batch:        34 of       172\t|\tloss: 0.42339\n",
      "Evaluating Epoch 30  20.3% | batch:        35 of       172\t|\tloss: 1.11771\n",
      "Evaluating Epoch 30  20.9% | batch:        36 of       172\t|\tloss: 2.84933\n",
      "Evaluating Epoch 30  21.5% | batch:        37 of       172\t|\tloss: 5.31565\n",
      "Evaluating Epoch 30  22.1% | batch:        38 of       172\t|\tloss: 4.66327\n",
      "Evaluating Epoch 30  22.7% | batch:        39 of       172\t|\tloss: 9.59632\n",
      "Evaluating Epoch 30  23.3% | batch:        40 of       172\t|\tloss: 0.276658\n",
      "Evaluating Epoch 30  23.8% | batch:        41 of       172\t|\tloss: 0.350026\n",
      "Evaluating Epoch 30  24.4% | batch:        42 of       172\t|\tloss: 0.580955\n",
      "Evaluating Epoch 30  25.0% | batch:        43 of       172\t|\tloss: 21.8359\n",
      "Evaluating Epoch 30  25.6% | batch:        44 of       172\t|\tloss: 1.29675\n",
      "Evaluating Epoch 30  26.2% | batch:        45 of       172\t|\tloss: 0.569713\n",
      "Evaluating Epoch 30  26.7% | batch:        46 of       172\t|\tloss: 0.525646\n",
      "Evaluating Epoch 30  27.3% | batch:        47 of       172\t|\tloss: 1.08437\n",
      "Evaluating Epoch 30  27.9% | batch:        48 of       172\t|\tloss: 0.591315\n",
      "Evaluating Epoch 30  28.5% | batch:        49 of       172\t|\tloss: 1.18936\n",
      "Evaluating Epoch 30  29.1% | batch:        50 of       172\t|\tloss: 0.750841\n",
      "Evaluating Epoch 30  29.7% | batch:        51 of       172\t|\tloss: 1.04132\n",
      "Evaluating Epoch 30  30.2% | batch:        52 of       172\t|\tloss: 1.04315\n",
      "Evaluating Epoch 30  30.8% | batch:        53 of       172\t|\tloss: 3.11127\n",
      "Evaluating Epoch 30  31.4% | batch:        54 of       172\t|\tloss: 0.840529\n",
      "Evaluating Epoch 30  32.0% | batch:        55 of       172\t|\tloss: 0.483751\n",
      "Evaluating Epoch 30  32.6% | batch:        56 of       172\t|\tloss: 3.45676\n",
      "Evaluating Epoch 30  33.1% | batch:        57 of       172\t|\tloss: 0.258244\n",
      "Evaluating Epoch 30  33.7% | batch:        58 of       172\t|\tloss: 2.33945\n",
      "Evaluating Epoch 30  34.3% | batch:        59 of       172\t|\tloss: 1.37793\n",
      "Evaluating Epoch 30  34.9% | batch:        60 of       172\t|\tloss: 1.28832\n",
      "Evaluating Epoch 30  35.5% | batch:        61 of       172\t|\tloss: 1.6336\n",
      "Evaluating Epoch 30  36.0% | batch:        62 of       172\t|\tloss: 0.794104\n",
      "Evaluating Epoch 30  36.6% | batch:        63 of       172\t|\tloss: 3.29212\n",
      "Evaluating Epoch 30  37.2% | batch:        64 of       172\t|\tloss: 1.04231\n",
      "Evaluating Epoch 30  37.8% | batch:        65 of       172\t|\tloss: 2.45394\n",
      "Evaluating Epoch 30  38.4% | batch:        66 of       172\t|\tloss: 1.24244\n",
      "Evaluating Epoch 30  39.0% | batch:        67 of       172\t|\tloss: 0.725862\n",
      "Evaluating Epoch 30  39.5% | batch:        68 of       172\t|\tloss: 2.51546\n",
      "Evaluating Epoch 30  40.1% | batch:        69 of       172\t|\tloss: 0.714737\n",
      "Evaluating Epoch 30  40.7% | batch:        70 of       172\t|\tloss: 1.79548\n",
      "Evaluating Epoch 30  41.3% | batch:        71 of       172\t|\tloss: 1.63587\n",
      "Evaluating Epoch 30  41.9% | batch:        72 of       172\t|\tloss: 0.961941\n",
      "Evaluating Epoch 30  42.4% | batch:        73 of       172\t|\tloss: 2.68628\n",
      "Evaluating Epoch 30  43.0% | batch:        74 of       172\t|\tloss: 0.567639\n",
      "Evaluating Epoch 30  43.6% | batch:        75 of       172\t|\tloss: 0.465634\n",
      "Evaluating Epoch 30  44.2% | batch:        76 of       172\t|\tloss: 0.583156\n",
      "Evaluating Epoch 30  44.8% | batch:        77 of       172\t|\tloss: 0.493727\n",
      "Evaluating Epoch 30  45.3% | batch:        78 of       172\t|\tloss: 0.461172\n",
      "Evaluating Epoch 30  45.9% | batch:        79 of       172\t|\tloss: 0.334837\n",
      "Evaluating Epoch 30  46.5% | batch:        80 of       172\t|\tloss: 0.453256\n",
      "Evaluating Epoch 30  47.1% | batch:        81 of       172\t|\tloss: 0.581674\n",
      "Evaluating Epoch 30  47.7% | batch:        82 of       172\t|\tloss: 0.545922\n",
      "Evaluating Epoch 30  48.3% | batch:        83 of       172\t|\tloss: 0.378304\n",
      "Evaluating Epoch 30  48.8% | batch:        84 of       172\t|\tloss: 0.841328\n",
      "Evaluating Epoch 30  49.4% | batch:        85 of       172\t|\tloss: 0.634726\n",
      "Evaluating Epoch 30  50.0% | batch:        86 of       172\t|\tloss: 0.501653\n",
      "Evaluating Epoch 30  50.6% | batch:        87 of       172\t|\tloss: 0.732743\n",
      "Evaluating Epoch 30  51.2% | batch:        88 of       172\t|\tloss: 0.684397\n",
      "Evaluating Epoch 30  51.7% | batch:        89 of       172\t|\tloss: 0.412905\n",
      "Evaluating Epoch 30  52.3% | batch:        90 of       172\t|\tloss: 0.496565\n",
      "Evaluating Epoch 30  52.9% | batch:        91 of       172\t|\tloss: 0.562938\n",
      "Evaluating Epoch 30  53.5% | batch:        92 of       172\t|\tloss: 0.501025\n",
      "Evaluating Epoch 30  54.1% | batch:        93 of       172\t|\tloss: 0.680731\n",
      "Evaluating Epoch 30  54.7% | batch:        94 of       172\t|\tloss: 0.519561\n",
      "Evaluating Epoch 30  55.2% | batch:        95 of       172\t|\tloss: 0.543859\n",
      "Evaluating Epoch 30  55.8% | batch:        96 of       172\t|\tloss: 0.554679\n",
      "Evaluating Epoch 30  56.4% | batch:        97 of       172\t|\tloss: 0.569258\n",
      "Evaluating Epoch 30  57.0% | batch:        98 of       172\t|\tloss: 0.681864\n",
      "Evaluating Epoch 30  57.6% | batch:        99 of       172\t|\tloss: 0.379808\n",
      "Evaluating Epoch 30  58.1% | batch:       100 of       172\t|\tloss: 0.550849\n",
      "Evaluating Epoch 30  58.7% | batch:       101 of       172\t|\tloss: 0.641988\n",
      "Evaluating Epoch 30  59.3% | batch:       102 of       172\t|\tloss: 0.588093\n",
      "Evaluating Epoch 30  59.9% | batch:       103 of       172\t|\tloss: 0.715003\n",
      "Evaluating Epoch 30  60.5% | batch:       104 of       172\t|\tloss: 0.604567\n",
      "Evaluating Epoch 30  61.0% | batch:       105 of       172\t|\tloss: 0.412711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 30  61.6% | batch:       106 of       172\t|\tloss: 0.427601\n",
      "Evaluating Epoch 30  62.2% | batch:       107 of       172\t|\tloss: 0.822955\n",
      "Evaluating Epoch 30  62.8% | batch:       108 of       172\t|\tloss: 0.418226\n",
      "Evaluating Epoch 30  63.4% | batch:       109 of       172\t|\tloss: 0.600233\n",
      "Evaluating Epoch 30  64.0% | batch:       110 of       172\t|\tloss: 0.648826\n",
      "Evaluating Epoch 30  64.5% | batch:       111 of       172\t|\tloss: 0.589019\n",
      "Evaluating Epoch 30  65.1% | batch:       112 of       172\t|\tloss: 0.776994\n",
      "Evaluating Epoch 30  65.7% | batch:       113 of       172\t|\tloss: 0.59079\n",
      "Evaluating Epoch 30  66.3% | batch:       114 of       172\t|\tloss: 0.619127\n",
      "Evaluating Epoch 30  66.9% | batch:       115 of       172\t|\tloss: 0.259906\n",
      "Evaluating Epoch 30  67.4% | batch:       116 of       172\t|\tloss: 0.186892\n",
      "Evaluating Epoch 30  68.0% | batch:       117 of       172\t|\tloss: 0.191307\n",
      "Evaluating Epoch 30  68.6% | batch:       118 of       172\t|\tloss: 0.164126\n",
      "Evaluating Epoch 30  69.2% | batch:       119 of       172\t|\tloss: 0.179511\n",
      "Evaluating Epoch 30  69.8% | batch:       120 of       172\t|\tloss: 0.253987\n",
      "Evaluating Epoch 30  70.3% | batch:       121 of       172\t|\tloss: 0.214788\n",
      "Evaluating Epoch 30  70.9% | batch:       122 of       172\t|\tloss: 0.203552\n",
      "Evaluating Epoch 30  71.5% | batch:       123 of       172\t|\tloss: 0.38678\n",
      "Evaluating Epoch 30  72.1% | batch:       124 of       172\t|\tloss: 0.284862\n",
      "Evaluating Epoch 30  72.7% | batch:       125 of       172\t|\tloss: 0.230996\n",
      "Evaluating Epoch 30  73.3% | batch:       126 of       172\t|\tloss: 0.205168\n",
      "Evaluating Epoch 30  73.8% | batch:       127 of       172\t|\tloss: 0.364258\n",
      "Evaluating Epoch 30  74.4% | batch:       128 of       172\t|\tloss: 0.362671\n",
      "Evaluating Epoch 30  75.0% | batch:       129 of       172\t|\tloss: 0.267309\n",
      "Evaluating Epoch 30  75.6% | batch:       130 of       172\t|\tloss: 0.281249\n",
      "Evaluating Epoch 30  76.2% | batch:       131 of       172\t|\tloss: 0.373313\n",
      "Evaluating Epoch 30  76.7% | batch:       132 of       172\t|\tloss: 0.426321\n",
      "Evaluating Epoch 30  77.3% | batch:       133 of       172\t|\tloss: 0.690066\n",
      "Evaluating Epoch 30  77.9% | batch:       134 of       172\t|\tloss: 0.506604\n",
      "Evaluating Epoch 30  78.5% | batch:       135 of       172\t|\tloss: 0.542454\n",
      "Evaluating Epoch 30  79.1% | batch:       136 of       172\t|\tloss: 0.453102\n",
      "Evaluating Epoch 30  79.7% | batch:       137 of       172\t|\tloss: 0.503918\n",
      "Evaluating Epoch 30  80.2% | batch:       138 of       172\t|\tloss: 0.37928\n",
      "Evaluating Epoch 30  80.8% | batch:       139 of       172\t|\tloss: 0.718753\n",
      "Evaluating Epoch 30  81.4% | batch:       140 of       172\t|\tloss: 0.395518\n",
      "Evaluating Epoch 30  82.0% | batch:       141 of       172\t|\tloss: 0.343954\n",
      "Evaluating Epoch 30  82.6% | batch:       142 of       172\t|\tloss: 0.387665\n",
      "Evaluating Epoch 30  83.1% | batch:       143 of       172\t|\tloss: 0.46171\n",
      "Evaluating Epoch 30  83.7% | batch:       144 of       172\t|\tloss: 0.502459\n",
      "Evaluating Epoch 30  84.3% | batch:       145 of       172\t|\tloss: 0.644224\n",
      "Evaluating Epoch 30  84.9% | batch:       146 of       172\t|\tloss: 0.445352\n",
      "Evaluating Epoch 30  85.5% | batch:       147 of       172\t|\tloss: 0.616752\n",
      "Evaluating Epoch 30  86.0% | batch:       148 of       172\t|\tloss: 0.38729\n",
      "Evaluating Epoch 30  86.6% | batch:       149 of       172\t|\tloss: 0.518218\n",
      "Evaluating Epoch 30  87.2% | batch:       150 of       172\t|\tloss: 0.194205\n",
      "Evaluating Epoch 30  87.8% | batch:       151 of       172\t|\tloss: 0.285813\n",
      "Evaluating Epoch 30  88.4% | batch:       152 of       172\t|\tloss: 0.372609\n",
      "Evaluating Epoch 30  89.0% | batch:       153 of       172\t|\tloss: 0.205144\n",
      "Evaluating Epoch 30  89.5% | batch:       154 of       172\t|\tloss: 0.239037\n",
      "Evaluating Epoch 30  90.1% | batch:       155 of       172\t|\tloss: 0.417205\n",
      "Evaluating Epoch 30  90.7% | batch:       156 of       172\t|\tloss: 0.276104\n",
      "Evaluating Epoch 30  91.3% | batch:       157 of       172\t|\tloss: 0.310424\n",
      "Evaluating Epoch 30  91.9% | batch:       158 of       172\t|\tloss: 0.163422\n",
      "Evaluating Epoch 30  92.4% | batch:       159 of       172\t|\tloss: 0.197259\n",
      "Evaluating Epoch 30  93.0% | batch:       160 of       172\t|\tloss: 0.934475\n",
      "Evaluating Epoch 30  93.6% | batch:       161 of       172\t|\tloss: 0.194746\n",
      "Evaluating Epoch 30  94.2% | batch:       162 of       172\t|\tloss: 0.288567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:13:52,052 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 3.9132566452026367 seconds\n",
      "\n",
      "2023-05-25 22:13:52,056 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 4.032936872974519 seconds\n",
      "2023-05-25 22:13:52,057 | INFO : Avg batch val. time: 0.023447307401014642 seconds\n",
      "2023-05-25 22:13:52,057 | INFO : Avg sample val. time: 0.00018367431219995986 seconds\n",
      "2023-05-25 22:13:52,058 | INFO : Epoch 30 Validation Summary: epoch: 30.000000 | loss: 1.301544 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 30  94.8% | batch:       163 of       172\t|\tloss: 0.388941\n",
      "Evaluating Epoch 30  95.3% | batch:       164 of       172\t|\tloss: 0.28844\n",
      "Evaluating Epoch 30  95.9% | batch:       165 of       172\t|\tloss: 0.31049\n",
      "Evaluating Epoch 30  96.5% | batch:       166 of       172\t|\tloss: 0.19876\n",
      "Evaluating Epoch 30  97.1% | batch:       167 of       172\t|\tloss: 0.254291\n",
      "Evaluating Epoch 30  97.7% | batch:       168 of       172\t|\tloss: 0.303571\n",
      "Evaluating Epoch 30  98.3% | batch:       169 of       172\t|\tloss: 0.126697\n",
      "Evaluating Epoch 30  98.8% | batch:       170 of       172\t|\tloss: 0.217496\n",
      "Evaluating Epoch 30  99.4% | batch:       171 of       172\t|\tloss: 0.37783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info('Starting training...')\n",
    "for epoch in tqdm(range(start_epoch + 1, config[\"epochs\"] + 1), desc='Training Epoch', leave=False):\n",
    "    mark = epoch if config['save_all'] else 'last'\n",
    "    epoch_start_time = time.time()\n",
    "    # Training\n",
    "    aggr_metrics_train = trainer.train_epoch(epoch)  # dictionary of aggregate epoch metrics\n",
    "    epoch_runtime = time.time() - epoch_start_time\n",
    "    print()\n",
    "    print_str = 'Epoch {} Training Summary: '.format(epoch)\n",
    "    for k, v in aggr_metrics_train.items():\n",
    "        tensorboard_writer.add_scalar('{}/train'.format(k), v, epoch)\n",
    "        print_str += '{}: {:8f} | '.format(k, v)\n",
    "    logger.info(print_str)\n",
    "    logger.info(\"Epoch runtime: {} hours, {} minutes, {} seconds\\n\".format(*utils.readable_time(epoch_runtime)))\n",
    "    total_epoch_time += epoch_runtime\n",
    "    avg_epoch_time = total_epoch_time / (epoch - start_epoch)\n",
    "    avg_batch_time = avg_epoch_time / len(train_loader)\n",
    "    avg_sample_time = avg_epoch_time / len(train_dataset)\n",
    "    logger.info(\"Avg epoch train. time: {} hours, {} minutes, {} seconds\".format(*utils.readable_time(avg_epoch_time)))\n",
    "    logger.info(\"Avg batch train. time: {} seconds\".format(avg_batch_time))\n",
    "    logger.info(\"Avg sample train. time: {} seconds\".format(avg_sample_time))\n",
    "\n",
    "    # evaluate if first or last epoch or at specified interval\n",
    "    if (epoch == config[\"epochs\"]) or (epoch == start_epoch + 1) or (epoch % config['val_interval'] == 0):\n",
    "        aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, tensorboard_writer, config,\n",
    "                                                              best_metrics, best_value, epoch)\n",
    "        metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "        metrics.append(list(metrics_values))\n",
    "\n",
    "    utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(mark)), epoch, model, optimizer)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    if epoch == config['lr_step'][lr_step]:\n",
    "        utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(epoch)), epoch, model, optimizer)\n",
    "        lr = lr * config['lr_factor'][lr_step]\n",
    "        if lr_step < len(config['lr_step']) - 1:  # so that this index does not get out of bounds\n",
    "            lr_step += 1\n",
    "        logger.info('Learning rate updated to: ', lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    # Difficulty scheduling\n",
    "    if config['harden'] and check_progress(epoch):\n",
    "        train_loader.dataset.update()\n",
    "        val_loader.dataset.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35492f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:13:52,079 | INFO : Exported per epoch performance metrics in '../experiments/poseErrorPred_preTrain_2023-05-25_21-58-39_T27/metrics_poseErrorPred_preTrain.xls'\n",
      "2023-05-25 22:13:52,101 | INFO : Exported performance record to 'Regression_records.xls'\n",
      "2023-05-25 22:13:52,103 | INFO : Best loss was 1.1690099435344052. Other metrics: OrderedDict([('epoch', 28), ('loss', 1.1690099435344052)])\n",
      "2023-05-25 22:13:52,104 | INFO : All Done!\n",
      "2023-05-25 22:13:52,105 | INFO : Total runtime: 0.0 hours, 14.0 minutes, 59.834444522857666 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1690099435344052\n"
     ]
    }
   ],
   "source": [
    "# Export evolution of metrics over epochs\n",
    "header = metrics_names\n",
    "metrics_filepath = os.path.join(config[\"output_dir\"], \"metrics_\" + config[\"experiment_name\"] + \".xls\")\n",
    "book = utils.export_performance_metrics(metrics_filepath, metrics, header, sheet_name=\"metrics\")\n",
    "\n",
    "# Export record metrics to a file accumulating records from all experiments\n",
    "utils.register_record(config[\"records_file\"], config[\"initial_timestamp\"], config[\"experiment_name\"],\n",
    "                      best_metrics, aggr_metrics_val, comment=config['comment'])\n",
    "\n",
    "logger.info('Best {} was {}. Other metrics: {}'.format(config['key_metric'], best_value, best_metrics))\n",
    "logger.info('All Done!')\n",
    "\n",
    "total_runtime = time.time() - total_start_time\n",
    "logger.info(\"Total runtime: {} hours, {} minutes, {} seconds\\n\".format(*utils.readable_time(total_runtime)))\n",
    "\n",
    "#return best_value\n",
    "print(best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe567bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4b23c0e",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70927512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75043eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../experiments/\"\n",
    "experiment = 'poseErrorPred_finetuned_2023-05-16_12-04-46_ia1'\n",
    "file_path = '/predictions/best_predictions.npz'\n",
    "total_path = base_path + experiment + file_path\n",
    "total_path\n",
    "#total_path = config['pred_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_path = config['pred_dir'] + '/best_predictions.npz'\n",
    "total_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['pred_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['output_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ed5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.load(total_path, allow_pickle=True)\n",
    "pred.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd526eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate(pred[\"targets\"], axis=0)\n",
    "y_pred = np.concatenate(pred[\"predictions\"], axis=0)\n",
    "IDs = np.concatenate(pred[\"IDs\"], axis=0)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256fc222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mape(y, y_pred):\n",
    "    err = y - y_pred\n",
    "    return np.mean(np.abs(err)/y)\n",
    "\n",
    "def get_mse(y, y_pred):\n",
    "    err = y - y_pred\n",
    "    return np.mean(np.square(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ce45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mse(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f604a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mape(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef8e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_output(y, y_pred, title=' '):\n",
    "    fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(14, 6))\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    \n",
    "    ax0.plot(y, color='g', label='truth')\n",
    "    ax0.set_title(title)\n",
    "    ax0.set_xlabel('Step')\n",
    "    ax0.set_ylabel('Error')\n",
    "    ax0.grid()\n",
    "    ax0.legend()\n",
    "    \n",
    "    ax1.plot(y, color='g', label='truth')\n",
    "    ax1.plot(y_pred, color='b', alpha=0.7, label='predict')\n",
    "    mse = get_mse(y, y_pred)\n",
    "    mape = get_mape(y, y_pred)\n",
    "    ax1.set_title(title+\"- mse: {:.5f} | mape: {:.5f}\".format(mse, mape))\n",
    "    ax1.set_xlabel('Step')\n",
    "    ax1.set_ylabel('Error')\n",
    "    ax1.grid()\n",
    "    ax1.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e54b0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_output(y, y_pred, title='Prediction on SenseTime Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2dffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3911d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb206dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c26533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
