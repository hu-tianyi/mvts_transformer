{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8abdfb88",
   "metadata": {},
   "source": [
    "# Pose Error Project\n",
    "### (Transformer-Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d884a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:07:31,527 | INFO : Loading packages ...\n",
      "2023-05-10 17:07:32,800 | INFO : Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-05-10 17:07:32,801 | INFO : NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Loading packages ...\")\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# 3rd party packages\n",
    "\n",
    "#from tqdm import tqdm\n",
    "# since we are using it in jupyter notebook\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Project modules\n",
    "from options import Options\n",
    "from running import setup, pipeline_factory, validate, check_progress, NEG_METRICS\n",
    "from utils import utils\n",
    "from datasets.data import data_factory, Normalizer\n",
    "from datasets.datasplit import split_dataset\n",
    "from models.ts_transformer import model_factory\n",
    "from models.loss import get_loss_module\n",
    "from optimizers import get_optimizer\n",
    "\n",
    "import parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519d8c6",
   "metadata": {},
   "source": [
    "# Setup Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e17d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting 1 - Single Stage\n",
    "# Training From Scratch\n",
    "text = \"--output_dir ../experiments/ --comment 'pm25_from_Scratch' \\\n",
    "        --name pm25_fromScratch_Regression --records_file Regression_records.xls \\\n",
    "        --data_dir ../data/regression/BeijingPM25Quality/ --data_class tsra \\\n",
    "        --epochs 100 --lr 0.001 --optimizer RAdam \\\n",
    "        --pos_encoding learnable --task regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036bf287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntext = \"--output_dir ../experiments --comment \\'finetune_for_regression\\'         --name BeijingPM25Quality_finetuned --records_file Regression_records.xls         --data_dir ../data/regression/BeijingPM25Quality/ --data_class tsra         --pattern TRAIN --val_pattern TEST  --epochs 100 --lr 0.001 --optimizer RAdam         --pos_encoding learnable --d_model 128         --load_model ../experiments/pm25_pretrained_2023-05-04_11-01-18_AhP/checkpoints/model_best.pth         --task regression --change_output --batch_size 128\"\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting 2 - Two Stages\n",
    "# Pretrain\n",
    "'''\n",
    "text = \"--output_dir ../experiments/ --comment 'pretraining_through_imputation' \\\n",
    "        --name pm25_pretrained --records_file Imputation_records.xls \\\n",
    "        --data_dir ../data/regression/BeijingPM25Quality/ --data_class tsra \\\n",
    "        --pattern TRAIN --val_ratio 0.2 --epochs 20 --lr 0.001 --optimizer RAdam \\\n",
    "        --batch_size 32 --pos_encoding learnable --d_model 128\"\n",
    "'''\n",
    "\n",
    "# Finetune\n",
    "'''\n",
    "text = \"--output_dir ../experiments --comment 'finetune_for_regression' \\\n",
    "        --name BeijingPM25Quality_finetuned --records_file Regression_records.xls \\\n",
    "        --data_dir ../data/regression/BeijingPM25Quality/ --data_class tsra \\\n",
    "        --pattern TRAIN --val_pattern TEST  --epochs 100 --lr 0.001 --optimizer RAdam \\\n",
    "        --pos_encoding learnable --d_model 128 \\\n",
    "        --load_model ../experiments/pm25_pretrained_2023-05-04_11-01-18_AhP/checkpoints/model_best.pth \\\n",
    "        --task regression --change_output --batch_size 128\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e99d546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:07:32,939 | INFO : Stored configuration file in '../experiments/pm25_fromScratch_Regression_2023-05-10_17-07-32_kDk'\n"
     ]
    }
   ],
   "source": [
    "# Process the setting string\n",
    "# Generate the config variable\n",
    "input_text = text.split()\n",
    "args = Options().parse(input_text)\n",
    "config = setup(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f6877ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_filepath': None,\n",
       " 'output_dir': '../experiments/pm25_fromScratch_Regression_2023-05-10_17-07-32_kDk',\n",
       " 'data_dir': '../data/regression/BeijingPM25Quality/',\n",
       " 'load_model': None,\n",
       " 'resume': False,\n",
       " 'change_output': False,\n",
       " 'save_all': False,\n",
       " 'experiment_name': 'pm25_fromScratch_Regression',\n",
       " 'comment': \"'pm25_from_Scratch'\",\n",
       " 'no_timestamp': False,\n",
       " 'records_file': 'Regression_records.xls',\n",
       " 'console': False,\n",
       " 'print_interval': 1,\n",
       " 'gpu': '0',\n",
       " 'n_proc': -1,\n",
       " 'num_workers': 0,\n",
       " 'seed': None,\n",
       " 'limit_size': None,\n",
       " 'test_only': None,\n",
       " 'data_class': 'tsra',\n",
       " 'labels': None,\n",
       " 'test_from': None,\n",
       " 'test_ratio': 0,\n",
       " 'val_ratio': 0.2,\n",
       " 'pattern': None,\n",
       " 'val_pattern': None,\n",
       " 'test_pattern': None,\n",
       " 'normalization': 'standardization',\n",
       " 'norm_from': None,\n",
       " 'subsample_factor': None,\n",
       " 'task': 'regression',\n",
       " 'masking_ratio': 0.15,\n",
       " 'mean_mask_length': 3,\n",
       " 'mask_mode': 'separate',\n",
       " 'mask_distribution': 'geometric',\n",
       " 'exclude_feats': None,\n",
       " 'mask_feats': [0, 1],\n",
       " 'start_hint': 0.0,\n",
       " 'end_hint': 0.0,\n",
       " 'harden': False,\n",
       " 'epochs': 100,\n",
       " 'val_interval': 2,\n",
       " 'optimizer': 'RAdam',\n",
       " 'lr': 0.001,\n",
       " 'lr_step': [1000000],\n",
       " 'lr_factor': [0.1],\n",
       " 'batch_size': 64,\n",
       " 'l2_reg': 0,\n",
       " 'global_reg': False,\n",
       " 'key_metric': 'loss',\n",
       " 'freeze': False,\n",
       " 'model': 'transformer',\n",
       " 'max_seq_len': None,\n",
       " 'data_window_len': None,\n",
       " 'd_model': 64,\n",
       " 'dim_feedforward': 256,\n",
       " 'num_heads': 8,\n",
       " 'num_layers': 3,\n",
       " 'dropout': 0.1,\n",
       " 'pos_encoding': 'learnable',\n",
       " 'activation': 'gelu',\n",
       " 'normalization_layer': 'BatchNorm',\n",
       " 'initial_timestamp': '2023-05-10_17-07-32',\n",
       " 'save_dir': '../experiments/pm25_fromScratch_Regression_2023-05-10_17-07-32_kDk/checkpoints',\n",
       " 'pred_dir': '../experiments/pm25_fromScratch_Regression_2023-05-10_17-07-32_kDk/predictions',\n",
       " 'tensorboard_dir': '../experiments/pm25_fromScratch_Regression_2023-05-10_17-07-32_kDk/tb_summaries'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236524a",
   "metadata": {},
   "source": [
    "# Setup Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "386c3baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:07:32,949 | INFO : Running:\n",
      "/home/tianyi/anaconda3/envs/transformer/lib/python3.8/site-packages/ipykernel_launcher.py -f /home/tianyi/.local/share/jupyter/runtime/kernel-efb3a0ae-2589-4ac8-8875-e056a8afd634.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add file logging besides stdout\n",
    "file_handler = logging.FileHandler(os.path.join(config['output_dir'], 'output.log'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.info('Running:\\n{}\\n'.format(' '.join(sys.argv)))  # command used to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ade90",
   "metadata": {},
   "source": [
    "# Setup Training Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72e8986f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:07:32,990 | INFO : Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if config['seed'] is not None:\n",
    "    torch.manual_seed(config['seed'])\n",
    "\n",
    "device = torch.device('cuda' if (torch.cuda.is_available() and config['gpu'] != '-1') else 'cpu')\n",
    "logger.info(\"Using device: {}\".format(device))\n",
    "if device == 'cuda':\n",
    "    logger.info(\"Device index: {}\".format(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6174d8f0",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e8acbf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:07:32,995 | INFO : Loading and preprocessing data ...\n",
      "5072it [00:10, 488.12it/s]\n"
     ]
    }
   ],
   "source": [
    " # Build data\n",
    "logger.info(\"Loading and preprocessing data ...\")\n",
    "data_class = data_factory[config['data_class']]\n",
    "my_data = data_class(config['data_dir'], \n",
    "                     pattern=config['pattern'], \n",
    "                     n_proc=config['n_proc'], \n",
    "                     limit_size=config['limit_size'], \n",
    "                     config=config)\n",
    "feat_dim = my_data.feature_df.shape[1]  # dimensionality of data features\n",
    "if config['task'] == 'classification':\n",
    "    validation_method = 'StratifiedShuffleSplit'\n",
    "    labels = my_data.labels_df.values.flatten()\n",
    "else:\n",
    "    validation_method = 'ShuffleSplit'\n",
    "    labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5312827b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_dim = my_data.feature_df.shape[1]\n",
    "feat_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b822bc4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>5300.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>1024.5</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>1023.8</td>\n",
       "      <td>-7.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>4300.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-4.7</td>\n",
       "      <td>1023.7</td>\n",
       "      <td>-7.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>4100.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>1023.1</td>\n",
       "      <td>-7.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-5.1</td>\n",
       "      <td>1022.3</td>\n",
       "      <td>-7.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5047</th>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1013.5</td>\n",
       "      <td>-16.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5047</th>\n",
       "      <td>3.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>1013.6</td>\n",
       "      <td>-15.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5047</th>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1014.2</td>\n",
       "      <td>-13.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5047</th>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1014.4</td>\n",
       "      <td>-12.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5047</th>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1014.1</td>\n",
       "      <td>-15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121152 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dim_0  dim_1   dim_2  dim_3  dim_4   dim_5  dim_6  dim_7  dim_8\n",
       "0      47.0  123.0  5300.0   13.0   -2.5  1024.5   -8.2    0.0    1.1\n",
       "0      66.0  119.0  5400.0   16.0   -3.5  1023.8   -7.3    0.0    1.0\n",
       "0      64.0  115.0  4300.0   12.0   -4.7  1023.7   -7.3    0.0    0.8\n",
       "0      56.0  111.0  4100.0    9.0   -3.6  1023.1   -7.7    0.0    1.5\n",
       "0      46.0  107.0  3600.0    7.0   -5.1  1022.3   -7.7    0.0    1.0\n",
       "...     ...    ...     ...    ...    ...     ...    ...    ...    ...\n",
       "5047    3.0   24.0   400.0   72.0   12.5  1013.5  -16.2    0.0    2.4\n",
       "5047    3.0   41.0   500.0   50.0   11.6  1013.6  -15.1    0.0    0.9\n",
       "5047    4.0   38.0   500.0   54.0   10.8  1014.2  -13.3    0.0    1.1\n",
       "5047    4.0   30.0   400.0   59.0   10.5  1014.4  -12.9    0.0    1.2\n",
       "5047    4.0   38.0   600.0   49.0    8.6  1014.1  -15.9    0.0    1.3\n",
       "\n",
       "[121152 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.feature_df\n",
    "#my_data.all_IDs[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a8224d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_data.labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "887d1316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "286032.0/11918.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4f21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d69e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import utils\n",
    "#x, y = utils.load_from_tsfile_to_dataframe(\"../data/regression/BeijingPM25Quality/BeijingPM25Quality_TRAIN.ts\")\n",
    "#y.shape\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0f7e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b87188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fde68340",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch_time = 0\n",
    "total_eval_time = 0\n",
    "\n",
    "total_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706bd0fa",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "329d7c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "test_data = my_data\n",
    "test_indices = None  # will be converted to empty list in `split_dataset`, if also test_set_ratio == 0\n",
    "val_data = my_data\n",
    "val_indices = []\n",
    "if config['test_pattern']:  # used if test data come from different files / file patterns\n",
    "    test_data = data_class(config['data_dir'], pattern=config['test_pattern'], n_proc=-1, config=config)\n",
    "    test_indices = test_data.all_IDs\n",
    "if config['test_from']:  # load test IDs directly from file, if available, otherwise use `test_set_ratio`. Can work together with `test_pattern`\n",
    "    test_indices = list(set([line.rstrip() for line in open(config['test_from']).readlines()]))\n",
    "    try:\n",
    "        test_indices = [int(ind) for ind in test_indices]  # integer indices\n",
    "    except ValueError:\n",
    "        pass  # in case indices are non-integers\n",
    "    logger.info(\"Loaded {} test IDs from file: '{}'\".format(len(test_indices), config['test_from']))\n",
    "if config['val_pattern']:  # used if val data come from different files / file patterns\n",
    "    val_data = data_class(config['data_dir'], pattern=config['val_pattern'], n_proc=-1, config=config)\n",
    "    val_indices = val_data.all_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1fa9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: currently a validation set must exist, either with `val_pattern` or `val_ratio`\n",
    "# Using a `val_pattern` means that `val_ratio` == 0 and `test_ratio` == 0\n",
    "if config['val_ratio'] > 0:\n",
    "    train_indices, val_indices, test_indices = split_dataset(data_indices=my_data.all_IDs,\n",
    "                                                             validation_method=validation_method,\n",
    "                                                             n_splits=1,\n",
    "                                                             validation_ratio=config['val_ratio'],\n",
    "                                                             test_set_ratio=config['test_ratio'],  # used only if test_indices not explicitly specified\n",
    "                                                             test_indices=test_indices,\n",
    "                                                             random_seed=1337,\n",
    "                                                             labels=labels)\n",
    "    train_indices = train_indices[0]  # `split_dataset` returns a list of indices *per fold/split*\n",
    "    val_indices = val_indices[0]  # `split_dataset` returns a list of indices *per fold/split*\n",
    "else:\n",
    "    train_indices = my_data.all_IDs\n",
    "    if test_indices is None:\n",
    "        test_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "142da4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:07:53,011 | INFO : 4038 samples may be used for training\n",
      "2023-05-10 17:07:53,012 | INFO : 1010 samples will be used for validation\n",
      "2023-05-10 17:07:53,013 | INFO : 0 samples will be used for testing\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"{} samples may be used for training\".format(len(train_indices)))\n",
    "logger.info(\"{} samples will be used for validation\".format(len(val_indices)))\n",
    "logger.info(\"{} samples will be used for testing\".format(len(test_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b579872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config['output_dir'], 'data_indices.json'), 'w') as f:\n",
    "    try:\n",
    "        json.dump({'train_indices': list(map(int, train_indices)),\n",
    "                   'val_indices': list(map(int, val_indices)),\n",
    "                   'test_indices': list(map(int, test_indices))}, f, indent=4)\n",
    "    except ValueError:  # in case indices are non-integers\n",
    "        json.dump({'train_indices': list(train_indices),\n",
    "                   'val_indices': list(val_indices),\n",
    "                   'test_indices': list(test_indices)}, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d89d2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process features\n",
    "normalizer = None\n",
    "if config['norm_from']:\n",
    "    with open(config['norm_from'], 'rb') as f:\n",
    "        norm_dict = pickle.load(f)\n",
    "    normalizer = Normalizer(**norm_dict)\n",
    "elif config['normalization'] is not None:\n",
    "    normalizer = Normalizer(config['normalization'])\n",
    "    my_data.feature_df.loc[train_indices] = normalizer.normalize(my_data.feature_df.loc[train_indices])\n",
    "    if not config['normalization'].startswith('per_sample'):\n",
    "        # get normalizing values from training set and store for future use\n",
    "        norm_dict = normalizer.__dict__\n",
    "        with open(os.path.join(config['output_dir'], 'normalization.pickle'), 'wb') as f:\n",
    "            pickle.dump(norm_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "if normalizer is not None:\n",
    "    if len(val_indices):\n",
    "        val_data.feature_df.loc[val_indices] = normalizer.normalize(val_data.feature_df.loc[val_indices])\n",
    "    if len(test_indices):\n",
    "        test_data.feature_df.loc[test_indices] = normalizer.normalize(test_data.feature_df.loc[test_indices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb5e618",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e829b2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:07:53,108 | INFO : Creating model ...\n",
      "2023-05-10 17:07:53,133 | INFO : Model:\n",
      "TSTransformerEncoderClassiregressor(\n",
      "  (project_inp): Linear(in_features=9, out_features=64, bias=True)\n",
      "  (pos_enc): LearnablePositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=1536, out_features=1, bias=True)\n",
      ")\n",
      "2023-05-10 17:07:53,133 | INFO : Total number of parameters: 153665\n",
      "2023-05-10 17:07:53,134 | INFO : Trainable parameters: 153665\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "logger.info(\"Creating model ...\")\n",
    "model = model_factory(config, my_data)\n",
    "\n",
    "if config['freeze']:\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith('output_layer'):\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "logger.info(\"Model:\\n{}\".format(model))\n",
    "logger.info(\"Total number of parameters: {}\".format(utils.count_parameters(model)))\n",
    "logger.info(\"Trainable parameters: {}\".format(utils.count_parameters(model, trainable=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb07a9",
   "metadata": {},
   "source": [
    "# Initialize optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2744d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "if config['global_reg']:\n",
    "    weight_decay = config['l2_reg']\n",
    "    output_reg = None\n",
    "else:\n",
    "    weight_decay = 0\n",
    "    output_reg = config['l2_reg']\n",
    "\n",
    "optim_class = get_optimizer(config['optimizer'])\n",
    "optimizer = optim_class(model.parameters(), lr=config['lr'], weight_decay=weight_decay)\n",
    "\n",
    "start_epoch = 0\n",
    "lr_step = 0  # current step index of `lr_step`\n",
    "lr = config['lr']  # current learning step\n",
    "# Load model and optimizer state\n",
    "if args.load_model:\n",
    "    model, optimizer, start_epoch = utils.load_model(model, config['load_model'], optimizer, config['resume'],\n",
    "                                                     config['change_output'],\n",
    "                                                     config['lr'],\n",
    "                                                     config['lr_step'],\n",
    "                                                     config['lr_factor'])\n",
    "model.to(device)\n",
    "\n",
    "loss_module = get_loss_module(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14053daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['test_only'] == 'testset':  # Only evaluate and skip training\n",
    "    dataset_class, collate_fn, runner_class = pipeline_factory(config)\n",
    "    test_dataset = dataset_class(test_data, test_indices)\n",
    "\n",
    "    test_loader = DataLoader(dataset=test_dataset,\n",
    "                             batch_size=config['batch_size'],\n",
    "                             shuffle=False,\n",
    "                             num_workers=config['num_workers'],\n",
    "                             pin_memory=True,\n",
    "                             collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "    test_evaluator = runner_class(model, test_loader, device, loss_module,\n",
    "                                        print_interval=config['print_interval'], console=config['console'])\n",
    "    aggr_metrics_test, per_batch_test = test_evaluator.evaluate(keep_all=True)\n",
    "    print_str = 'Test Summary: '\n",
    "    for k, v in aggr_metrics_test.items():\n",
    "        print_str += '{}: {:8f} | '.format(k, v)\n",
    "    logger.info(print_str)\n",
    "    #return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a07029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['test_only'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34b73e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2099 1225 2586 ... 4988  289 2292]\n",
      "[3955  933 2495 ...  860  189 3223]\n"
     ]
    }
   ],
   "source": [
    "# Initialize data generators\n",
    "if config['test_only'] != 'testset':  # Only evaluate and skip training\n",
    "    dataset_class, collate_fn, runner_class = pipeline_factory(config)\n",
    "    val_dataset = dataset_class(val_data, val_indices)\n",
    "\n",
    "    val_loader = DataLoader(dataset=val_dataset,\n",
    "                            batch_size=config['batch_size'],\n",
    "                            shuffle=False,\n",
    "                            num_workers=config['num_workers'],\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "\n",
    "    train_dataset = dataset_class(my_data, train_indices)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=config['batch_size'],\n",
    "                              shuffle=True,\n",
    "                              num_workers=config['num_workers'],\n",
    "                              pin_memory=True,\n",
    "                              collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "\n",
    "    trainer = runner_class(model, train_loader, device, loss_module, optimizer, l2_reg=output_reg,\n",
    "                                 print_interval=config['print_interval'], console=config['console'])\n",
    "    val_evaluator = runner_class(model, val_loader, device, loss_module,\n",
    "                                       print_interval=config['print_interval'], console=config['console'])\n",
    "\n",
    "    tensorboard_writer = SummaryWriter(config['tensorboard_dir'])\n",
    "\n",
    "    best_value = 1e16 if config['key_metric'] in NEG_METRICS else -1e16  # initialize with +inf or -inf depending on key metric\n",
    "    metrics = []  # (for validation) list of lists: for each epoch, stores metrics like loss, ...\n",
    "    best_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fcca71a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "torch.Size([64, 24, 9])\n",
      "tensor([[[-6.1151e-01, -2.2536e-01, -2.4848e-01, -9.8053e-01, -5.7372e-02,\n",
      "           1.8004e-01,  6.8928e-01, -7.8279e-02, -5.2800e-01],\n",
      "         [-6.1151e-01, -2.8206e-01, -2.4848e-01, -9.8053e-01, -5.7372e-02,\n",
      "           2.1704e-01,  6.8928e-01, -7.8279e-02, -6.8756e-01],\n",
      "         [-6.1151e-01, -2.8206e-01, -2.4848e-01, -9.8053e-01, -4.9087e-02,\n",
      "           2.3555e-01,  6.9620e-01, -7.8279e-02, -3.6844e-01],\n",
      "         [-6.1151e-01, -4.2382e-01, -3.2692e-01, -9.8053e-01, -3.2517e-02,\n",
      "           2.8180e-01,  7.1004e-01, -7.8279e-02, -6.0778e-01],\n",
      "         [-6.1151e-01, -4.5217e-01, -3.2692e-01, -9.8053e-01,  8.9086e-03,\n",
      "           3.1880e-01,  6.9620e-01, -7.8279e-02, -2.8867e-01],\n",
      "         [-6.1151e-01, -5.3722e-01, -3.2692e-01, -9.8053e-01,  3.3764e-02,\n",
      "           3.7431e-01,  6.9620e-01, -7.8279e-02, -4.9328e-02],\n",
      "         [-6.1151e-01, -5.3722e-01, -3.2692e-01, -9.8053e-01,  5.0334e-02,\n",
      "           4.7607e-01,  5.7164e-01, -7.8279e-02,  3.4957e-01],\n",
      "         [-6.1151e-01, -6.7898e-01, -4.8381e-01, -8.7019e-01,  5.8619e-02,\n",
      "           5.4083e-01,  4.6092e-01, -7.8279e-02,  3.4957e-01],\n",
      "         [-6.1151e-01, -8.2074e-01, -5.6225e-01, -7.4147e-01,  6.6904e-02,\n",
      "           6.1483e-01,  4.1248e-01, -7.8279e-02,  9.0802e-01],\n",
      "         [-6.1151e-01, -8.4909e-01, -5.6225e-01, -6.8631e-01,  1.0004e-01,\n",
      "           6.7959e-01,  3.2252e-01, -7.8279e-02,  1.3867e+00],\n",
      "         [-6.1151e-01, -9.6250e-01, -6.4069e-01, -5.2081e-01,  1.4976e-01,\n",
      "           6.9809e-01,  3.0868e-01, -7.8279e-02,  1.5463e+00],\n",
      "         [-6.1151e-01, -9.6250e-01, -6.4069e-01, -4.6564e-01,  1.5804e-01,\n",
      "           6.8884e-01,  2.7408e-01, -7.8279e-02,  1.7058e+00],\n",
      "         [-6.1151e-01, -9.6250e-01, -6.4069e-01, -3.5531e-01,  1.6633e-01,\n",
      "           6.4259e-01,  2.8100e-01, -7.8279e-02,  1.0676e+00],\n",
      "         [-6.1151e-01, -9.6250e-01, -6.4069e-01, -2.6336e-01,  1.9947e-01,\n",
      "           6.5184e-01,  2.4640e-01, -7.8279e-02,  1.0676e+00],\n",
      "         [-6.1151e-01, -9.0579e-01, -7.1914e-01, -2.0820e-01,  1.7461e-01,\n",
      "           6.1483e-01,  2.3256e-01, -7.8279e-02,  1.4665e+00],\n",
      "         [-6.1151e-01, -4.5217e-01, -7.1914e-01, -3.1853e-01,  1.9118e-01,\n",
      "           5.8708e-01,  2.6024e-01, -7.8279e-02,  5.0913e-01],\n",
      "         [-6.1151e-01, -4.8052e-01, -7.1914e-01, -3.5531e-01,  1.4976e-01,\n",
      "           5.6858e-01,  2.3256e-01, -7.8279e-02,  1.3069e+00],\n",
      "         [-6.1151e-01, -4.2382e-01, -7.1914e-01, -4.6564e-01,  9.1760e-02,\n",
      "           6.1483e-01,  2.3948e-01, -7.8279e-02,  5.8890e-01],\n",
      "         [-6.1151e-01, -3.6712e-01, -6.4069e-01, -5.3920e-01,  3.3764e-02,\n",
      "           6.3334e-01,  1.9796e-01, -7.8279e-02,  5.0913e-01],\n",
      "         [-6.1151e-01, -4.8052e-01, -6.4069e-01, -5.9436e-01, -4.9087e-02,\n",
      "           6.9809e-01,  2.1872e-01, -7.8279e-02,  3.0451e-02],\n",
      "         [-6.1151e-01, -3.3876e-01, -5.6225e-01, -7.0469e-01, -1.0708e-01,\n",
      "           7.2585e-01,  1.4260e-01, -7.8279e-02, -4.9328e-02],\n",
      "         [-6.1151e-01, -2.8206e-01, -5.6225e-01, -7.9664e-01, -2.2307e-01,\n",
      "           7.7210e-01,  2.5332e-01, -7.8279e-02, -6.8756e-01],\n",
      "         [-6.1151e-01, -2.6898e-02, -4.8381e-01, -9.8053e-01, -2.3136e-01,\n",
      "           7.8135e-01,  3.1560e-01, -7.8279e-02, -1.0865e+00],\n",
      "         [-6.1151e-01,  1.9991e-01, -4.0536e-01, -9.8053e-01, -1.3401e-01,\n",
      "           7.9523e-01,  1.0108e-01, -7.8279e-02, -4.4822e-01]],\n",
      "\n",
      "        [[-6.1151e-01, -2.6898e-02, -3.2692e-01, -9.8053e-01, -2.9764e-01,\n",
      "           9.5712e-01,  4.3324e-01,  6.2888e-02, -1.2911e-01],\n",
      "         [-6.1151e-01, -2.6898e-02, -2.4848e-01, -9.8053e-01, -2.4462e-01,\n",
      "           9.5712e-01,  4.0556e-01, -7.8279e-02, -8.4712e-01],\n",
      "         [-6.1151e-01, -1.1195e-01, -3.2692e-01, -9.8053e-01, -2.0650e-01,\n",
      "           9.2011e-01,  3.0868e-01, -7.8279e-02, -2.0889e-01],\n",
      "         [-6.1151e-01, -1.1195e-01, -3.2692e-01, -9.8053e-01, -1.8993e-01,\n",
      "           8.9236e-01,  2.3948e-01, -7.8279e-02,  4.2935e-01],\n",
      "         [-6.1151e-01, -1.9701e-01, -4.0536e-01, -9.8053e-01, -1.9822e-01,\n",
      "           9.0161e-01,  1.7028e-01, -7.8279e-02, -2.0889e-01],\n",
      "         [-6.1151e-01, -4.2382e-01, -5.6225e-01, -9.4375e-01, -2.2307e-01,\n",
      "           9.5712e-01,  1.9796e-01, -7.8279e-02, -6.8756e-01],\n",
      "         [-6.1151e-01, -6.5063e-01, -5.6225e-01, -8.3342e-01, -5.1305e-01,\n",
      "           9.3862e-01,  2.3256e-01, -7.8279e-02, -1.1662e+00],\n",
      "         [-6.1151e-01, -2.2536e-01, -5.6225e-01, -9.8053e-01, -2.2307e-01,\n",
      "           1.0589e+00, -2.4492e-01, -7.8279e-02,  1.1023e-01],\n",
      "         [-6.1151e-01, -2.6898e-02, -4.8381e-01, -9.8053e-01, -1.5679e-01,\n",
      "           1.1699e+00, -2.8644e-01, -7.8279e-02,  1.6260e+00],\n",
      "         [-5.4242e-01, -6.7898e-01, -6.4069e-01, -3.7370e-01, -9.8798e-02,\n",
      "           1.2254e+00, -3.4872e-01, -7.8279e-02,  1.8654e+00],\n",
      "         [-4.7332e-01, -6.5063e-01, -6.4069e-01, -3.5531e-01, -9.8798e-02,\n",
      "           1.3087e+00, -5.0096e-01, -7.8279e-02,  1.4665e+00],\n",
      "         [-5.4242e-01, -7.9239e-01, -6.4069e-01, -2.6336e-01, -5.7372e-02,\n",
      "           1.3549e+00, -6.0476e-01, -7.8279e-02,  6.6868e-01],\n",
      "         [-5.4242e-01, -9.3414e-01, -7.1914e-01, -7.9473e-02,  2.5479e-02,\n",
      "           1.3179e+00, -5.1480e-01, -7.8279e-02,  9.8780e-01],\n",
      "         [-5.4242e-01, -1.0475e+00, -7.1914e-01,  3.0860e-02,  6.6904e-02,\n",
      "           1.2531e+00, -5.5632e-01, -7.8279e-02,  1.3069e+00],\n",
      "         [-6.1151e-01, -1.1043e+00, -7.9758e-01,  1.2280e-01,  7.5189e-02,\n",
      "           1.1791e+00, -5.9092e-01, -7.8279e-02,  1.3867e+00],\n",
      "         [-6.1151e-01, -1.1043e+00, -7.9758e-01,  1.4119e-01,  6.6904e-02,\n",
      "           1.1699e+00, -7.7084e-01, -7.8279e-02,  1.3069e+00],\n",
      "         [-6.1151e-01, -1.0192e+00, -7.9758e-01,  8.6026e-02,  6.2346e-04,\n",
      "           1.1884e+00, -6.8780e-01, -7.8279e-02,  6.6868e-01],\n",
      "         [-6.1151e-01, -8.4909e-01, -7.9758e-01, -4.2696e-02, -9.0513e-02,\n",
      "           1.2161e+00, -7.9160e-01, -7.8279e-02,  4.2935e-01],\n",
      "         [-6.1151e-01, -7.9239e-01, -7.9758e-01, -9.7862e-02, -1.8993e-01,\n",
      "           1.3087e+00, -6.3244e-01, -7.8279e-02, -3.6844e-01],\n",
      "         [-6.1151e-01, -4.5217e-01, -7.1914e-01, -3.5531e-01, -2.3136e-01,\n",
      "           1.3919e+00, -5.6324e-01, -7.8279e-02, -3.6844e-01],\n",
      "         [-6.1151e-01, -3.9547e-01, -7.1914e-01, -5.2081e-01, -3.8049e-01,\n",
      "           1.4474e+00, -1.7572e-01, -7.8279e-02, -1.0865e+00],\n",
      "         [-6.1151e-01, -2.2536e-01, -6.4069e-01, -7.2308e-01, -4.4677e-01,\n",
      "           1.4659e+00,  1.8037e-02, -7.8279e-02, -1.2460e+00],\n",
      "         [-6.1151e-01, -1.4030e-01, -5.6225e-01, -8.5181e-01, -4.3849e-01,\n",
      "           1.5307e+00, -6.5003e-02, -7.8279e-02, -8.4712e-01],\n",
      "         [-6.1151e-01,  2.9805e-02, -5.6225e-01, -9.8053e-01, -5.0477e-01,\n",
      "           1.5214e+00, -1.2036e-01, -7.8279e-02, -1.0067e+00]]])\n",
      "torch.float32\n",
      "--------------------\n",
      "torch.Size([64, 1])\n",
      "tensor([31.])\n",
      "torch.float32\n",
      "--------------------\n",
      "torch.Size([64, 24])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True])\n",
      "--------------------\n",
      "[3242, 4925, 3576, 2296, 4816, 934, 3717, 602, 910, 1928, 1145, 4100, 1372, 419, 529, 4664, 1710, 1056, 859, 4373, 730, 2258, 4402, 2866, 4752, 2943, 1004, 844, 786, 3518, 2945, 3314, 1811, 1833, 1848, 1759, 2734, 2157, 873, 5036, 1153, 2736, 2357, 74, 4201, 4104, 1254, 3852, 1631, 480, 4695, 91, 1400, 1621, 4116, 809, 4143, 4918, 2009, 3489, 1954, 2701, 2936, 1495]\n"
     ]
    }
   ],
   "source": [
    "print(config[\"batch_size\"])\n",
    "for batch in train_loader:\n",
    "    X, targets, padding_masks, IDs = batch\n",
    "    print(X.shape)\n",
    "    print(X[0:2])\n",
    "    print(X.dtype)\n",
    "    print(\"-\"*20)\n",
    "    print(targets.shape)\n",
    "    print(targets[0])\n",
    "    print(targets.dtype)\n",
    "    print(\"-\"*20)\n",
    "    print(padding_masks.shape)\n",
    "    print(padding_masks[0])\n",
    "    print(\"-\"*20)\n",
    "    print(IDs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f94a773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "049fe0cf",
   "metadata": {},
   "source": [
    "# Evaluate on validation before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc3f83ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:07:53,942 | INFO : Evaluating on validation set ...\n",
      "2023-05-10 17:07:54,733 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.7899467945098877 seconds\n",
      "\n",
      "2023-05-10 17:07:54,734 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.7899467945098877 seconds\n",
      "2023-05-10 17:07:54,734 | INFO : Avg batch val. time: 0.04937167465686798 seconds\n",
      "2023-05-10 17:07:54,734 | INFO : Avg sample val. time: 0.0007821255391187007 seconds\n",
      "2023-05-10 17:07:54,735 | INFO : Epoch 0 Validation Summary: epoch: 0.000000 | loss: 32402.150743 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 0   0.0% | batch:         0 of        16\t|\tloss: 26998.7\n",
      "Evaluating Epoch 0   6.2% | batch:         1 of        16\t|\tloss: 31823.7\n",
      "Evaluating Epoch 0  12.5% | batch:         2 of        16\t|\tloss: 33461.8\n",
      "Evaluating Epoch 0  18.8% | batch:         3 of        16\t|\tloss: 27679.5\n",
      "Evaluating Epoch 0  25.0% | batch:         4 of        16\t|\tloss: 19341\n",
      "Evaluating Epoch 0  31.2% | batch:         5 of        16\t|\tloss: 35420\n",
      "Evaluating Epoch 0  37.5% | batch:         6 of        16\t|\tloss: 23197.1\n",
      "Evaluating Epoch 0  43.8% | batch:         7 of        16\t|\tloss: 43058.1\n",
      "Evaluating Epoch 0  50.0% | batch:         8 of        16\t|\tloss: 34621.2\n",
      "Evaluating Epoch 0  56.2% | batch:         9 of        16\t|\tloss: 53648\n",
      "Evaluating Epoch 0  62.5% | batch:        10 of        16\t|\tloss: 27692.5\n",
      "Evaluating Epoch 0  68.8% | batch:        11 of        16\t|\tloss: 47978.3\n",
      "Evaluating Epoch 0  75.0% | batch:        12 of        16\t|\tloss: 20585.4\n",
      "Evaluating Epoch 0  81.2% | batch:        13 of        16\t|\tloss: 26400.4\n",
      "Evaluating Epoch 0  87.5% | batch:        14 of        16\t|\tloss: 27168.2\n",
      "Evaluating Epoch 0  93.8% | batch:        15 of        16\t|\tloss: 41308.8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianyi/anaconda3/envs/transformer/lib/python3.8/site-packages/numpy/lib/npyio.py:713: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val = np.asanyarray(val)\n"
     ]
    }
   ],
   "source": [
    "aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, tensorboard_writer, config, best_metrics,\n",
    "                                                      best_value, epoch=0)\n",
    "metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "metrics.append(list(metrics_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf058db",
   "metadata": {},
   "source": [
    "# Starting training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c07b4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:07:54,749 | INFO : Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianyi/Documents/mvts_transformer/src/optimizers.py:69: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1   0.0% | batch:         0 of        64\t|\tloss: 50113.9\n",
      "Training Epoch 1   1.6% | batch:         1 of        64\t|\tloss: 30074\n",
      "Training Epoch 1   3.1% | batch:         2 of        64\t|\tloss: 32629.7\n",
      "Training Epoch 1   4.7% | batch:         3 of        64\t|\tloss: 36068.2\n",
      "Training Epoch 1   6.2% | batch:         4 of        64\t|\tloss: 24228.6\n",
      "Training Epoch 1   7.8% | batch:         5 of        64\t|\tloss: 22839\n",
      "Training Epoch 1   9.4% | batch:         6 of        64\t|\tloss: 22941\n",
      "Training Epoch 1  10.9% | batch:         7 of        64\t|\tloss: 41369\n",
      "Training Epoch 1  12.5% | batch:         8 of        64\t|\tloss: 34386.2\n",
      "Training Epoch 1  14.1% | batch:         9 of        64\t|\tloss: 33905.6\n",
      "Training Epoch 1  15.6% | batch:        10 of        64\t|\tloss: 29075.7\n",
      "Training Epoch 1  17.2% | batch:        11 of        64\t|\tloss: 31694.9\n",
      "Training Epoch 1  18.8% | batch:        12 of        64\t|\tloss: 44520.7\n",
      "Training Epoch 1  20.3% | batch:        13 of        64\t|\tloss: 39493.6\n",
      "Training Epoch 1  21.9% | batch:        14 of        64\t|\tloss: 24106.3\n",
      "Training Epoch 1  23.4% | batch:        15 of        64\t|\tloss: 31733.2\n",
      "Training Epoch 1  25.0% | batch:        16 of        64\t|\tloss: 25895.9\n",
      "Training Epoch 1  26.6% | batch:        17 of        64\t|\tloss: 45013.1\n",
      "Training Epoch 1  28.1% | batch:        18 of        64\t|\tloss: 42867.5\n",
      "Training Epoch 1  29.7% | batch:        19 of        64\t|\tloss: 31863.1\n",
      "Training Epoch 1  31.2% | batch:        20 of        64\t|\tloss: 27719.7\n",
      "Training Epoch 1  32.8% | batch:        21 of        64\t|\tloss: 30264.6\n",
      "Training Epoch 1  34.4% | batch:        22 of        64\t|\tloss: 24252.9\n",
      "Training Epoch 1  35.9% | batch:        23 of        64\t|\tloss: 28045\n",
      "Training Epoch 1  37.5% | batch:        24 of        64\t|\tloss: 22108.1\n",
      "Training Epoch 1  39.1% | batch:        25 of        64\t|\tloss: 41023.6\n",
      "Training Epoch 1  40.6% | batch:        26 of        64\t|\tloss: 34251.4\n",
      "Training Epoch 1  42.2% | batch:        27 of        64\t|\tloss: 40752.4\n",
      "Training Epoch 1  43.8% | batch:        28 of        64\t|\tloss: 41593.3\n",
      "Training Epoch 1  45.3% | batch:        29 of        64\t|\tloss: 30765.2\n",
      "Training Epoch 1  46.9% | batch:        30 of        64\t|\tloss: 38489.7\n",
      "Training Epoch 1  48.4% | batch:        31 of        64\t|\tloss: 37606.6\n",
      "Training Epoch 1  50.0% | batch:        32 of        64\t|\tloss: 27868\n",
      "Training Epoch 1  51.6% | batch:        33 of        64\t|\tloss: 33561.6\n",
      "Training Epoch 1  53.1% | batch:        34 of        64\t|\tloss: 31387.9\n",
      "Training Epoch 1  54.7% | batch:        35 of        64\t|\tloss: 18393.1\n",
      "Training Epoch 1  56.2% | batch:        36 of        64\t|\tloss: 40821.1\n",
      "Training Epoch 1  57.8% | batch:        37 of        64\t|\tloss: 39281.2\n",
      "Training Epoch 1  59.4% | batch:        38 of        64\t|\tloss: 21631\n",
      "Training Epoch 1  60.9% | batch:        39 of        64\t|\tloss: 29444\n",
      "Training Epoch 1  62.5% | batch:        40 of        64\t|\tloss: 28296.5\n",
      "Training Epoch 1  64.1% | batch:        41 of        64\t|\tloss: 22980.2\n",
      "Training Epoch 1  65.6% | batch:        42 of        64\t|\tloss: 23995.8\n",
      "Training Epoch 1  67.2% | batch:        43 of        64\t|\tloss: 44198.2\n",
      "Training Epoch 1  68.8% | batch:        44 of        64\t|\tloss: 28879.6\n",
      "Training Epoch 1  70.3% | batch:        45 of        64\t|\tloss: 21066\n",
      "Training Epoch 1  71.9% | batch:        46 of        64\t|\tloss: 20496.5\n",
      "Training Epoch 1  73.4% | batch:        47 of        64\t|\tloss: 23314.3\n",
      "Training Epoch 1  75.0% | batch:        48 of        64\t|\tloss: 23704.8\n",
      "Training Epoch 1  76.6% | batch:        49 of        64\t|\tloss: 21830.9\n",
      "Training Epoch 1  78.1% | batch:        50 of        64\t|\tloss: 26695.3\n",
      "Training Epoch 1  79.7% | batch:        51 of        64\t|\tloss: 27398.2\n",
      "Training Epoch 1  81.2% | batch:        52 of        64\t|\tloss: 24246.7\n",
      "Training Epoch 1  82.8% | batch:        53 of        64\t|\tloss: 26377.3\n",
      "Training Epoch 1  84.4% | batch:        54 of        64\t|\tloss: 26260.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:07:56,001 | INFO : Epoch 1 Training Summary: epoch: 1.000000 | loss: 30804.734507 | \n",
      "2023-05-10 17:07:56,002 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2432506084442139 seconds\n",
      "\n",
      "2023-05-10 17:07:56,002 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2432506084442139 seconds\n",
      "2023-05-10 17:07:56,003 | INFO : Avg batch train. time: 0.01942579075694084 seconds\n",
      "2023-05-10 17:07:56,003 | INFO : Avg sample train. time: 0.00030788771878261856 seconds\n",
      "2023-05-10 17:07:56,004 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  85.9% | batch:        55 of        64\t|\tloss: 30469.1\n",
      "Training Epoch 1  87.5% | batch:        56 of        64\t|\tloss: 32565.3\n",
      "Training Epoch 1  89.1% | batch:        57 of        64\t|\tloss: 28664\n",
      "Training Epoch 1  90.6% | batch:        58 of        64\t|\tloss: 21685.2\n",
      "Training Epoch 1  92.2% | batch:        59 of        64\t|\tloss: 29153.7\n",
      "Training Epoch 1  93.8% | batch:        60 of        64\t|\tloss: 18903.1\n",
      "Training Epoch 1  95.3% | batch:        61 of        64\t|\tloss: 28513.8\n",
      "Training Epoch 1  96.9% | batch:        62 of        64\t|\tloss: 44592.2\n",
      "Training Epoch 1  98.4% | batch:        63 of        64\t|\tloss: 55670.7\n",
      "\n",
      "Evaluating Epoch 1   0.0% | batch:         0 of        16\t|\tloss: 24905.7\n",
      "Evaluating Epoch 1   6.2% | batch:         1 of        16\t|\tloss: 29812.2\n",
      "Evaluating Epoch 1  12.5% | batch:         2 of        16\t|\tloss: 30719.1\n",
      "Evaluating Epoch 1  18.8% | batch:         3 of        16\t|\tloss: 25508.9\n",
      "Evaluating Epoch 1  25.0% | batch:         4 of        16\t|\tloss: 17943.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:07:56,154 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.1504673957824707 seconds\n",
      "\n",
      "2023-05-10 17:07:56,155 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.4702070951461792 seconds\n",
      "2023-05-10 17:07:56,156 | INFO : Avg batch val. time: 0.0293879434466362 seconds\n",
      "2023-05-10 17:07:56,156 | INFO : Avg sample val. time: 0.0004655515793526527 seconds\n",
      "2023-05-10 17:07:56,157 | INFO : Epoch 1 Validation Summary: epoch: 1.000000 | loss: 30052.563243 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 1  31.2% | batch:         5 of        16\t|\tloss: 32891.9\n",
      "Evaluating Epoch 1  37.5% | batch:         6 of        16\t|\tloss: 21218.3\n",
      "Evaluating Epoch 1  43.8% | batch:         7 of        16\t|\tloss: 39852.5\n",
      "Evaluating Epoch 1  50.0% | batch:         8 of        16\t|\tloss: 32064.1\n",
      "Evaluating Epoch 1  56.2% | batch:         9 of        16\t|\tloss: 50990.1\n",
      "Evaluating Epoch 1  62.5% | batch:        10 of        16\t|\tloss: 25379.8\n",
      "Evaluating Epoch 1  68.8% | batch:        11 of        16\t|\tloss: 44975.9\n",
      "Evaluating Epoch 1  75.0% | batch:        12 of        16\t|\tloss: 18885.1\n",
      "Evaluating Epoch 1  81.2% | batch:        13 of        16\t|\tloss: 24064\n",
      "Evaluating Epoch 1  87.5% | batch:        14 of        16\t|\tloss: 25017.8\n",
      "Evaluating Epoch 1  93.8% | batch:        15 of        16\t|\tloss: 38448.7\n",
      "\n",
      "Training Epoch 2   0.0% | batch:         0 of        64\t|\tloss: 28364.2\n",
      "Training Epoch 2   1.6% | batch:         1 of        64\t|\tloss: 21131.8\n",
      "Training Epoch 2   3.1% | batch:         2 of        64\t|\tloss: 19805.6\n",
      "Training Epoch 2   4.7% | batch:         3 of        64\t|\tloss: 28566\n",
      "Training Epoch 2   6.2% | batch:         4 of        64\t|\tloss: 41095.6\n",
      "Training Epoch 2   7.8% | batch:         5 of        64\t|\tloss: 28374.8\n",
      "Training Epoch 2   9.4% | batch:         6 of        64\t|\tloss: 23934.2\n",
      "Training Epoch 2  10.9% | batch:         7 of        64\t|\tloss: 33925.1\n",
      "Training Epoch 2  12.5% | batch:         8 of        64\t|\tloss: 23921.9\n",
      "Training Epoch 2  14.1% | batch:         9 of        64\t|\tloss: 29712.5\n",
      "Training Epoch 2  15.6% | batch:        10 of        64\t|\tloss: 55358.7\n",
      "Training Epoch 2  17.2% | batch:        11 of        64\t|\tloss: 21254.4\n",
      "Training Epoch 2  18.8% | batch:        12 of        64\t|\tloss: 34532.9\n",
      "Training Epoch 2  20.3% | batch:        13 of        64\t|\tloss: 30194\n",
      "Training Epoch 2  21.9% | batch:        14 of        64\t|\tloss: 45890.1\n",
      "Training Epoch 2  23.4% | batch:        15 of        64\t|\tloss: 33396.7\n",
      "Training Epoch 2  25.0% | batch:        16 of        64\t|\tloss: 41726.8\n",
      "Training Epoch 2  26.6% | batch:        17 of        64\t|\tloss: 36390.7\n",
      "Training Epoch 2  28.1% | batch:        18 of        64\t|\tloss: 33366.7\n",
      "Training Epoch 2  29.7% | batch:        19 of        64\t|\tloss: 35258.6\n",
      "Training Epoch 2  31.2% | batch:        20 of        64\t|\tloss: 15310.9\n",
      "Training Epoch 2  32.8% | batch:        21 of        64\t|\tloss: 18702.2\n",
      "Training Epoch 2  34.4% | batch:        22 of        64\t|\tloss: 22759.5\n",
      "Training Epoch 2  35.9% | batch:        23 of        64\t|\tloss: 51946.5\n",
      "Training Epoch 2  37.5% | batch:        24 of        64\t|\tloss: 21057\n",
      "Training Epoch 2  39.1% | batch:        25 of        64\t|\tloss: 18872\n",
      "Training Epoch 2  40.6% | batch:        26 of        64\t|\tloss: 30194.4\n",
      "Training Epoch 2  42.2% | batch:        27 of        64\t|\tloss: 29272.4\n",
      "Training Epoch 2  43.8% | batch:        28 of        64\t|\tloss: 27770.7\n",
      "Training Epoch 2  45.3% | batch:        29 of        64\t|\tloss: 22021.8\n",
      "Training Epoch 2  46.9% | batch:        30 of        64\t|\tloss: 36227.6\n",
      "Training Epoch 2  48.4% | batch:        31 of        64\t|\tloss: 19083.3\n",
      "Training Epoch 2  50.0% | batch:        32 of        64\t|\tloss: 40447.1\n",
      "Training Epoch 2  51.6% | batch:        33 of        64\t|\tloss: 19207.1\n",
      "Training Epoch 2  53.1% | batch:        34 of        64\t|\tloss: 27811.2\n",
      "Training Epoch 2  54.7% | batch:        35 of        64\t|\tloss: 24887.9\n",
      "Training Epoch 2  56.2% | batch:        36 of        64\t|\tloss: 31749.5\n",
      "Training Epoch 2  57.8% | batch:        37 of        64\t|\tloss: 25546.2\n",
      "Training Epoch 2  59.4% | batch:        38 of        64\t|\tloss: 22827.6\n",
      "Training Epoch 2  60.9% | batch:        39 of        64\t|\tloss: 23237.8\n",
      "Training Epoch 2  62.5% | batch:        40 of        64\t|\tloss: 15578.5\n",
      "Training Epoch 2  64.1% | batch:        41 of        64\t|\tloss: 30695.4\n",
      "Training Epoch 2  65.6% | batch:        42 of        64\t|\tloss: 20530.5\n",
      "Training Epoch 2  67.2% | batch:        43 of        64\t|\tloss: 22156.5\n",
      "Training Epoch 2  68.8% | batch:        44 of        64\t|\tloss: 27544.2\n",
      "Training Epoch 2  70.3% | batch:        45 of        64\t|\tloss: 27399.5\n",
      "Training Epoch 2  71.9% | batch:        46 of        64\t|\tloss: 17946.6\n",
      "Training Epoch 2  73.4% | batch:        47 of        64\t|\tloss: 21244.7\n",
      "Training Epoch 2  75.0% | batch:        48 of        64\t|\tloss: 15131.2\n",
      "Training Epoch 2  76.6% | batch:        49 of        64\t|\tloss: 24967.8\n",
      "Training Epoch 2  78.1% | batch:        50 of        64\t|\tloss: 38639.1\n",
      "Training Epoch 2  79.7% | batch:        51 of        64\t|\tloss: 17504.1\n",
      "Training Epoch 2  81.2% | batch:        52 of        64\t|\tloss: 25032.8\n",
      "Training Epoch 2  82.8% | batch:        53 of        64\t|\tloss: 31215.1\n",
      "Training Epoch 2  84.4% | batch:        54 of        64\t|\tloss: 29186.8\n",
      "Training Epoch 2  85.9% | batch:        55 of        64\t|\tloss: 22097.4\n",
      "Training Epoch 2  87.5% | batch:        56 of        64\t|\tloss: 43461.6\n",
      "Training Epoch 2  89.1% | batch:        57 of        64\t|\tloss: 33069.4\n",
      "Training Epoch 2  90.6% | batch:        58 of        64\t|\tloss: 13289.9\n",
      "Training Epoch 2  92.2% | batch:        59 of        64\t|\tloss: 30117.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:07:57,355 | INFO : Epoch 2 Training Summary: epoch: 2.000000 | loss: 28017.595083 | \n",
      "2023-05-10 17:07:57,356 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1838047504425049 seconds\n",
      "\n",
      "2023-05-10 17:07:57,356 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2135276794433594 seconds\n",
      "2023-05-10 17:07:57,357 | INFO : Avg batch train. time: 0.01896136999130249 seconds\n",
      "2023-05-10 17:07:57,357 | INFO : Avg sample train. time: 0.00030052691417616626 seconds\n",
      "2023-05-10 17:07:57,358 | INFO : Evaluating on validation set ...\n",
      "2023-05-10 17:07:57,503 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14493131637573242 seconds\n",
      "\n",
      "2023-05-10 17:07:57,504 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.3617818355560303 seconds\n",
      "2023-05-10 17:07:57,504 | INFO : Avg batch val. time: 0.022611364722251892 seconds\n",
      "2023-05-10 17:07:57,505 | INFO : Avg sample val. time: 0.00035819983718418836 seconds\n",
      "2023-05-10 17:07:57,505 | INFO : Epoch 2 Validation Summary: epoch: 2.000000 | loss: 26162.427104 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  93.8% | batch:        60 of        64\t|\tloss: 29061.3\n",
      "Training Epoch 2  95.3% | batch:        61 of        64\t|\tloss: 34970.2\n",
      "Training Epoch 2  96.9% | batch:        62 of        64\t|\tloss: 20438.1\n",
      "Training Epoch 2  98.4% | batch:        63 of        64\t|\tloss: 14115\n",
      "\n",
      "Evaluating Epoch 2   0.0% | batch:         0 of        16\t|\tloss: 21091.3\n",
      "Evaluating Epoch 2   6.2% | batch:         1 of        16\t|\tloss: 25648.1\n",
      "Evaluating Epoch 2  12.5% | batch:         2 of        16\t|\tloss: 26083.8\n",
      "Evaluating Epoch 2  18.8% | batch:         3 of        16\t|\tloss: 22602.4\n",
      "Evaluating Epoch 2  25.0% | batch:         4 of        16\t|\tloss: 16499.9\n",
      "Evaluating Epoch 2  31.2% | batch:         5 of        16\t|\tloss: 28497.6\n",
      "Evaluating Epoch 2  37.5% | batch:         6 of        16\t|\tloss: 19017\n",
      "Evaluating Epoch 2  43.8% | batch:         7 of        16\t|\tloss: 34353.6\n",
      "Evaluating Epoch 2  50.0% | batch:         8 of        16\t|\tloss: 26961.3\n",
      "Evaluating Epoch 2  56.2% | batch:         9 of        16\t|\tloss: 44826.9\n",
      "Evaluating Epoch 2  62.5% | batch:        10 of        16\t|\tloss: 22456.7\n",
      "Evaluating Epoch 2  68.8% | batch:        11 of        16\t|\tloss: 38884.4\n",
      "Evaluating Epoch 2  75.0% | batch:        12 of        16\t|\tloss: 16482.9\n",
      "Evaluating Epoch 2  81.2% | batch:        13 of        16\t|\tloss: 20531.3\n",
      "Evaluating Epoch 2  87.5% | batch:        14 of        16\t|\tloss: 22209.8\n",
      "Evaluating Epoch 2  93.8% | batch:        15 of        16\t|\tloss: 34213.1\n",
      "\n",
      "Training Epoch 3   0.0% | batch:         0 of        64\t|\tloss: 12351.6\n",
      "Training Epoch 3   1.6% | batch:         1 of        64\t|\tloss: 26270.8\n",
      "Training Epoch 3   3.1% | batch:         2 of        64\t|\tloss: 23703.3\n",
      "Training Epoch 3   4.7% | batch:         3 of        64\t|\tloss: 36510.4\n",
      "Training Epoch 3   6.2% | batch:         4 of        64\t|\tloss: 14573.4\n",
      "Training Epoch 3   7.8% | batch:         5 of        64\t|\tloss: 14379.7\n",
      "Training Epoch 3   9.4% | batch:         6 of        64\t|\tloss: 22151.4\n",
      "Training Epoch 3  10.9% | batch:         7 of        64\t|\tloss: 30987.9\n",
      "Training Epoch 3  12.5% | batch:         8 of        64\t|\tloss: 18208.5\n",
      "Training Epoch 3  14.1% | batch:         9 of        64\t|\tloss: 21105.9\n",
      "Training Epoch 3  15.6% | batch:        10 of        64\t|\tloss: 18479.5\n",
      "Training Epoch 3  17.2% | batch:        11 of        64\t|\tloss: 28576\n",
      "Training Epoch 3  18.8% | batch:        12 of        64\t|\tloss: 24672.3\n",
      "Training Epoch 3  20.3% | batch:        13 of        64\t|\tloss: 26738.7\n",
      "Training Epoch 3  21.9% | batch:        14 of        64\t|\tloss: 25955.9\n",
      "Training Epoch 3  23.4% | batch:        15 of        64\t|\tloss: 24767.8\n",
      "Training Epoch 3  25.0% | batch:        16 of        64\t|\tloss: 14981.8\n",
      "Training Epoch 3  26.6% | batch:        17 of        64\t|\tloss: 18032.3\n",
      "Training Epoch 3  28.1% | batch:        18 of        64\t|\tloss: 22563.8\n",
      "Training Epoch 3  29.7% | batch:        19 of        64\t|\tloss: 18488.7\n",
      "Training Epoch 3  31.2% | batch:        20 of        64\t|\tloss: 24880.5\n",
      "Training Epoch 3  32.8% | batch:        21 of        64\t|\tloss: 21144.3\n",
      "Training Epoch 3  34.4% | batch:        22 of        64\t|\tloss: 23341.3\n",
      "Training Epoch 3  35.9% | batch:        23 of        64\t|\tloss: 29590\n",
      "Training Epoch 3  37.5% | batch:        24 of        64\t|\tloss: 24200.9\n",
      "Training Epoch 3  39.1% | batch:        25 of        64\t|\tloss: 18904.8\n",
      "Training Epoch 3  40.6% | batch:        26 of        64\t|\tloss: 16872.9\n",
      "Training Epoch 3  42.2% | batch:        27 of        64\t|\tloss: 22381.3\n",
      "Training Epoch 3  43.8% | batch:        28 of        64\t|\tloss: 15740.1\n",
      "Training Epoch 3  45.3% | batch:        29 of        64\t|\tloss: 17575.6\n",
      "Training Epoch 3  46.9% | batch:        30 of        64\t|\tloss: 27584.2\n",
      "Training Epoch 3  48.4% | batch:        31 of        64\t|\tloss: 21987.5\n",
      "Training Epoch 3  50.0% | batch:        32 of        64\t|\tloss: 24829.9\n",
      "Training Epoch 3  51.6% | batch:        33 of        64\t|\tloss: 27207.9\n",
      "Training Epoch 3  53.1% | batch:        34 of        64\t|\tloss: 30572.4\n",
      "Training Epoch 3  54.7% | batch:        35 of        64\t|\tloss: 18849.3\n",
      "Training Epoch 3  56.2% | batch:        36 of        64\t|\tloss: 25252.7\n",
      "Training Epoch 3  57.8% | batch:        37 of        64\t|\tloss: 23362.6\n",
      "Training Epoch 3  59.4% | batch:        38 of        64\t|\tloss: 19630.5\n",
      "Training Epoch 3  60.9% | batch:        39 of        64\t|\tloss: 29358.2\n",
      "Training Epoch 3  62.5% | batch:        40 of        64\t|\tloss: 22116.4\n",
      "Training Epoch 3  64.1% | batch:        41 of        64\t|\tloss: 38879.6\n",
      "Training Epoch 3  65.6% | batch:        42 of        64\t|\tloss: 13481.6\n",
      "Training Epoch 3  67.2% | batch:        43 of        64\t|\tloss: 32636.3\n",
      "Training Epoch 3  68.8% | batch:        44 of        64\t|\tloss: 15508.6\n",
      "Training Epoch 3  70.3% | batch:        45 of        64\t|\tloss: 10668.7\n",
      "Training Epoch 3  71.9% | batch:        46 of        64\t|\tloss: 23293.5\n",
      "Training Epoch 3  73.4% | batch:        47 of        64\t|\tloss: 14632.4\n",
      "Training Epoch 3  75.0% | batch:        48 of        64\t|\tloss: 28137.3\n",
      "Training Epoch 3  76.6% | batch:        49 of        64\t|\tloss: 31093.2\n",
      "Training Epoch 3  78.1% | batch:        50 of        64\t|\tloss: 27307.1\n",
      "Training Epoch 3  79.7% | batch:        51 of        64\t|\tloss: 19670.4\n",
      "Training Epoch 3  81.2% | batch:        52 of        64\t|\tloss: 32019.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:07:58,788 | INFO : Epoch 3 Training Summary: epoch: 3.000000 | loss: 23169.478022 | \n",
      "2023-05-10 17:07:58,789 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2671115398406982 seconds\n",
      "\n",
      "2023-05-10 17:07:58,789 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2313889662424724 seconds\n",
      "2023-05-10 17:07:58,790 | INFO : Avg batch train. time: 0.01924045259753863 seconds\n",
      "2023-05-10 17:07:58,791 | INFO : Avg sample train. time: 0.0003049502145226529 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  82.8% | batch:        53 of        64\t|\tloss: 32995\n",
      "Training Epoch 3  84.4% | batch:        54 of        64\t|\tloss: 35305.6\n",
      "Training Epoch 3  85.9% | batch:        55 of        64\t|\tloss: 28429.9\n",
      "Training Epoch 3  87.5% | batch:        56 of        64\t|\tloss: 21980.3\n",
      "Training Epoch 3  89.1% | batch:        57 of        64\t|\tloss: 15847.1\n",
      "Training Epoch 3  90.6% | batch:        58 of        64\t|\tloss: 23481.8\n",
      "Training Epoch 3  92.2% | batch:        59 of        64\t|\tloss: 25139.1\n",
      "Training Epoch 3  93.8% | batch:        60 of        64\t|\tloss: 22216.6\n",
      "Training Epoch 3  95.3% | batch:        61 of        64\t|\tloss: 21257.4\n",
      "Training Epoch 3  96.9% | batch:        62 of        64\t|\tloss: 18481.2\n",
      "Training Epoch 3  98.4% | batch:        63 of        64\t|\tloss: 5378.05\n",
      "\n",
      "Training Epoch 4   0.0% | batch:         0 of        64\t|\tloss: 22830.8\n",
      "Training Epoch 4   1.6% | batch:         1 of        64\t|\tloss: 21045.5\n",
      "Training Epoch 4   3.1% | batch:         2 of        64\t|\tloss: 19613.1\n",
      "Training Epoch 4   4.7% | batch:         3 of        64\t|\tloss: 23644.5\n",
      "Training Epoch 4   6.2% | batch:         4 of        64\t|\tloss: 14593.3\n",
      "Training Epoch 4   7.8% | batch:         5 of        64\t|\tloss: 23926.5\n",
      "Training Epoch 4   9.4% | batch:         6 of        64\t|\tloss: 15253.5\n",
      "Training Epoch 4  10.9% | batch:         7 of        64\t|\tloss: 21453.7\n",
      "Training Epoch 4  12.5% | batch:         8 of        64\t|\tloss: 30716.6\n",
      "Training Epoch 4  14.1% | batch:         9 of        64\t|\tloss: 35974.6\n",
      "Training Epoch 4  15.6% | batch:        10 of        64\t|\tloss: 20629.7\n",
      "Training Epoch 4  17.2% | batch:        11 of        64\t|\tloss: 21969.5\n",
      "Training Epoch 4  18.8% | batch:        12 of        64\t|\tloss: 16550.7\n",
      "Training Epoch 4  20.3% | batch:        13 of        64\t|\tloss: 21530\n",
      "Training Epoch 4  21.9% | batch:        14 of        64\t|\tloss: 15001\n",
      "Training Epoch 4  23.4% | batch:        15 of        64\t|\tloss: 11976.1\n",
      "Training Epoch 4  25.0% | batch:        16 of        64\t|\tloss: 28219\n",
      "Training Epoch 4  26.6% | batch:        17 of        64\t|\tloss: 9653.5\n",
      "Training Epoch 4  28.1% | batch:        18 of        64\t|\tloss: 26025.3\n",
      "Training Epoch 4  29.7% | batch:        19 of        64\t|\tloss: 26974.1\n",
      "Training Epoch 4  31.2% | batch:        20 of        64\t|\tloss: 14723\n",
      "Training Epoch 4  32.8% | batch:        21 of        64\t|\tloss: 16063.1\n",
      "Training Epoch 4  34.4% | batch:        22 of        64\t|\tloss: 16982.4\n",
      "Training Epoch 4  35.9% | batch:        23 of        64\t|\tloss: 21741.2\n",
      "Training Epoch 4  37.5% | batch:        24 of        64\t|\tloss: 16006\n",
      "Training Epoch 4  39.1% | batch:        25 of        64\t|\tloss: 13109.7\n",
      "Training Epoch 4  40.6% | batch:        26 of        64\t|\tloss: 17047.4\n",
      "Training Epoch 4  42.2% | batch:        27 of        64\t|\tloss: 17319.6\n",
      "Training Epoch 4  43.8% | batch:        28 of        64\t|\tloss: 13151.3\n",
      "Training Epoch 4  45.3% | batch:        29 of        64\t|\tloss: 26614.2\n",
      "Training Epoch 4  46.9% | batch:        30 of        64\t|\tloss: 28416.5\n",
      "Training Epoch 4  48.4% | batch:        31 of        64\t|\tloss: 24309.6\n",
      "Training Epoch 4  50.0% | batch:        32 of        64\t|\tloss: 26656.8\n",
      "Training Epoch 4  51.6% | batch:        33 of        64\t|\tloss: 19663.2\n",
      "Training Epoch 4  53.1% | batch:        34 of        64\t|\tloss: 11570.8\n",
      "Training Epoch 4  54.7% | batch:        35 of        64\t|\tloss: 17219.3\n",
      "Training Epoch 4  56.2% | batch:        36 of        64\t|\tloss: 11834.1\n",
      "Training Epoch 4  57.8% | batch:        37 of        64\t|\tloss: 14823.1\n",
      "Training Epoch 4  59.4% | batch:        38 of        64\t|\tloss: 10016.1\n",
      "Training Epoch 4  60.9% | batch:        39 of        64\t|\tloss: 19891.3\n",
      "Training Epoch 4  62.5% | batch:        40 of        64\t|\tloss: 22967.2\n",
      "Training Epoch 4  64.1% | batch:        41 of        64\t|\tloss: 10433.4\n",
      "Training Epoch 4  65.6% | batch:        42 of        64\t|\tloss: 10182.5\n",
      "Training Epoch 4  67.2% | batch:        43 of        64\t|\tloss: 19606.8\n",
      "Training Epoch 4  68.8% | batch:        44 of        64\t|\tloss: 24580.4\n",
      "Training Epoch 4  70.3% | batch:        45 of        64\t|\tloss: 18906.3\n",
      "Training Epoch 4  71.9% | batch:        46 of        64\t|\tloss: 11902.4\n",
      "Training Epoch 4  73.4% | batch:        47 of        64\t|\tloss: 12125.9\n",
      "Training Epoch 4  75.0% | batch:        48 of        64\t|\tloss: 12333.5\n",
      "Training Epoch 4  76.6% | batch:        49 of        64\t|\tloss: 17124.4\n",
      "Training Epoch 4  78.1% | batch:        50 of        64\t|\tloss: 15459.8\n",
      "Training Epoch 4  79.7% | batch:        51 of        64\t|\tloss: 18023\n",
      "Training Epoch 4  81.2% | batch:        52 of        64\t|\tloss: 25562.8\n",
      "Training Epoch 4  82.8% | batch:        53 of        64\t|\tloss: 24425.5\n",
      "Training Epoch 4  84.4% | batch:        54 of        64\t|\tloss: 19130.5\n",
      "Training Epoch 4  85.9% | batch:        55 of        64\t|\tloss: 14275.3\n",
      "Training Epoch 4  87.5% | batch:        56 of        64\t|\tloss: 15610.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:00,010 | INFO : Epoch 4 Training Summary: epoch: 4.000000 | loss: 18600.746140 | \n",
      "2023-05-10 17:08:00,011 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.205754041671753 seconds\n",
      "\n",
      "2023-05-10 17:08:00,011 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2249802350997925 seconds\n",
      "2023-05-10 17:08:00,012 | INFO : Avg batch train. time: 0.019140316173434258 seconds\n",
      "2023-05-10 17:08:00,012 | INFO : Avg sample train. time: 0.00030336310923719477 seconds\n",
      "2023-05-10 17:08:00,013 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  89.1% | batch:        57 of        64\t|\tloss: 12966.4\n",
      "Training Epoch 4  90.6% | batch:        58 of        64\t|\tloss: 30191.2\n",
      "Training Epoch 4  92.2% | batch:        59 of        64\t|\tloss: 17286.1\n",
      "Training Epoch 4  93.8% | batch:        60 of        64\t|\tloss: 12464.2\n",
      "Training Epoch 4  95.3% | batch:        61 of        64\t|\tloss: 10280.9\n",
      "Training Epoch 4  96.9% | batch:        62 of        64\t|\tloss: 11609.1\n",
      "Training Epoch 4  98.4% | batch:        63 of        64\t|\tloss: 14965.4\n",
      "\n",
      "Evaluating Epoch 4   0.0% | batch:         0 of        16\t|\tloss: 13975.3\n",
      "Evaluating Epoch 4   6.2% | batch:         1 of        16\t|\tloss: 15806.4\n",
      "Evaluating Epoch 4  12.5% | batch:         2 of        16\t|\tloss: 16378.2\n",
      "Evaluating Epoch 4  18.8% | batch:         3 of        16\t|\tloss: 15660.2\n",
      "Evaluating Epoch 4  25.0% | batch:         4 of        16\t|\tloss: 12848.6\n",
      "Evaluating Epoch 4  31.2% | batch:         5 of        16\t|\tloss: 17534.1\n",
      "Evaluating Epoch 4  37.5% | batch:         6 of        16\t|\tloss: 15241.9\n",
      "Evaluating Epoch 4  43.8% | batch:         7 of        16\t|\tloss: 22346.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:00,173 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.15931248664855957 seconds\n",
      "\n",
      "2023-05-10 17:08:00,173 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.3111644983291626 seconds\n",
      "2023-05-10 17:08:00,174 | INFO : Avg batch val. time: 0.019447781145572662 seconds\n",
      "2023-05-10 17:08:00,174 | INFO : Avg sample val. time: 0.00030808366171204216 seconds\n",
      "2023-05-10 17:08:00,175 | INFO : Epoch 4 Validation Summary: epoch: 4.000000 | loss: 17301.372401 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 4  50.0% | batch:         8 of        16\t|\tloss: 17578.7\n",
      "Evaluating Epoch 4  56.2% | batch:         9 of        16\t|\tloss: 28756\n",
      "Evaluating Epoch 4  62.5% | batch:        10 of        16\t|\tloss: 15531.5\n",
      "Evaluating Epoch 4  68.8% | batch:        11 of        16\t|\tloss: 23555.7\n",
      "Evaluating Epoch 4  75.0% | batch:        12 of        16\t|\tloss: 10954.5\n",
      "Evaluating Epoch 4  81.2% | batch:        13 of        16\t|\tloss: 13082.1\n",
      "Evaluating Epoch 4  87.5% | batch:        14 of        16\t|\tloss: 15777.8\n",
      "Evaluating Epoch 4  93.8% | batch:        15 of        16\t|\tloss: 23052.1\n",
      "\n",
      "Training Epoch 5   0.0% | batch:         0 of        64\t|\tloss: 18019.8\n",
      "Training Epoch 5   1.6% | batch:         1 of        64\t|\tloss: 12485.2\n",
      "Training Epoch 5   3.1% | batch:         2 of        64\t|\tloss: 20748.1\n",
      "Training Epoch 5   4.7% | batch:         3 of        64\t|\tloss: 16465.6\n",
      "Training Epoch 5   6.2% | batch:         4 of        64\t|\tloss: 25910.2\n",
      "Training Epoch 5   7.8% | batch:         5 of        64\t|\tloss: 14571.1\n",
      "Training Epoch 5   9.4% | batch:         6 of        64\t|\tloss: 15101.8\n",
      "Training Epoch 5  10.9% | batch:         7 of        64\t|\tloss: 22338.5\n",
      "Training Epoch 5  12.5% | batch:         8 of        64\t|\tloss: 19129.8\n",
      "Training Epoch 5  14.1% | batch:         9 of        64\t|\tloss: 14560.1\n",
      "Training Epoch 5  15.6% | batch:        10 of        64\t|\tloss: 13743.5\n",
      "Training Epoch 5  17.2% | batch:        11 of        64\t|\tloss: 11548\n",
      "Training Epoch 5  18.8% | batch:        12 of        64\t|\tloss: 16893.6\n",
      "Training Epoch 5  20.3% | batch:        13 of        64\t|\tloss: 13462.5\n",
      "Training Epoch 5  21.9% | batch:        14 of        64\t|\tloss: 15649.5\n",
      "Training Epoch 5  23.4% | batch:        15 of        64\t|\tloss: 11856.6\n",
      "Training Epoch 5  25.0% | batch:        16 of        64\t|\tloss: 8872.55\n",
      "Training Epoch 5  26.6% | batch:        17 of        64\t|\tloss: 19049.5\n",
      "Training Epoch 5  28.1% | batch:        18 of        64\t|\tloss: 12088.7\n",
      "Training Epoch 5  29.7% | batch:        19 of        64\t|\tloss: 15609.7\n",
      "Training Epoch 5  31.2% | batch:        20 of        64\t|\tloss: 17616.3\n",
      "Training Epoch 5  32.8% | batch:        21 of        64\t|\tloss: 10325.7\n",
      "Training Epoch 5  34.4% | batch:        22 of        64\t|\tloss: 16694.1\n",
      "Training Epoch 5  35.9% | batch:        23 of        64\t|\tloss: 24394.6\n",
      "Training Epoch 5  37.5% | batch:        24 of        64\t|\tloss: 11208.5\n",
      "Training Epoch 5  39.1% | batch:        25 of        64\t|\tloss: 15072.6\n",
      "Training Epoch 5  40.6% | batch:        26 of        64\t|\tloss: 9070.35\n",
      "Training Epoch 5  42.2% | batch:        27 of        64\t|\tloss: 15544\n",
      "Training Epoch 5  43.8% | batch:        28 of        64\t|\tloss: 27486.4\n",
      "Training Epoch 5  45.3% | batch:        29 of        64\t|\tloss: 12557.1\n",
      "Training Epoch 5  46.9% | batch:        30 of        64\t|\tloss: 10529.1\n",
      "Training Epoch 5  48.4% | batch:        31 of        64\t|\tloss: 9874.4\n",
      "Training Epoch 5  50.0% | batch:        32 of        64\t|\tloss: 13284.8\n",
      "Training Epoch 5  51.6% | batch:        33 of        64\t|\tloss: 8889.77\n",
      "Training Epoch 5  53.1% | batch:        34 of        64\t|\tloss: 17208.5\n",
      "Training Epoch 5  54.7% | batch:        35 of        64\t|\tloss: 19524.1\n",
      "Training Epoch 5  56.2% | batch:        36 of        64\t|\tloss: 12873.8\n",
      "Training Epoch 5  57.8% | batch:        37 of        64\t|\tloss: 12777\n",
      "Training Epoch 5  59.4% | batch:        38 of        64\t|\tloss: 15846.6\n",
      "Training Epoch 5  60.9% | batch:        39 of        64\t|\tloss: 14560.6\n",
      "Training Epoch 5  62.5% | batch:        40 of        64\t|\tloss: 26038.6\n",
      "Training Epoch 5  64.1% | batch:        41 of        64\t|\tloss: 24951.4\n",
      "Training Epoch 5  65.6% | batch:        42 of        64\t|\tloss: 16389.4\n",
      "Training Epoch 5  67.2% | batch:        43 of        64\t|\tloss: 13976.5\n",
      "Training Epoch 5  68.8% | batch:        44 of        64\t|\tloss: 11084.5\n",
      "Training Epoch 5  70.3% | batch:        45 of        64\t|\tloss: 11383.7\n",
      "Training Epoch 5  71.9% | batch:        46 of        64\t|\tloss: 8236.7\n",
      "Training Epoch 5  73.4% | batch:        47 of        64\t|\tloss: 16188.8\n",
      "Training Epoch 5  75.0% | batch:        48 of        64\t|\tloss: 11479.7\n",
      "Training Epoch 5  76.6% | batch:        49 of        64\t|\tloss: 10174.2\n",
      "Training Epoch 5  78.1% | batch:        50 of        64\t|\tloss: 17662.7\n",
      "Training Epoch 5  79.7% | batch:        51 of        64\t|\tloss: 10178.5\n",
      "Training Epoch 5  81.2% | batch:        52 of        64\t|\tloss: 8804.04\n",
      "Training Epoch 5  82.8% | batch:        53 of        64\t|\tloss: 5535.09\n",
      "Training Epoch 5  84.4% | batch:        54 of        64\t|\tloss: 11239.9\n",
      "Training Epoch 5  85.9% | batch:        55 of        64\t|\tloss: 9459.79\n",
      "Training Epoch 5  87.5% | batch:        56 of        64\t|\tloss: 9979.62\n",
      "Training Epoch 5  89.1% | batch:        57 of        64\t|\tloss: 10932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:01,454 | INFO : Epoch 5 Training Summary: epoch: 5.000000 | loss: 14459.185277 | \n",
      "2023-05-10 17:08:01,455 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2631771564483643 seconds\n",
      "\n",
      "2023-05-10 17:08:01,455 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.232619619369507 seconds\n",
      "2023-05-10 17:08:01,456 | INFO : Avg batch train. time: 0.019259681552648546 seconds\n",
      "2023-05-10 17:08:01,456 | INFO : Avg sample train. time: 0.0003052549825085455 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  90.6% | batch:        58 of        64\t|\tloss: 14939.4\n",
      "Training Epoch 5  92.2% | batch:        59 of        64\t|\tloss: 11840.1\n",
      "Training Epoch 5  93.8% | batch:        60 of        64\t|\tloss: 9179.57\n",
      "Training Epoch 5  95.3% | batch:        61 of        64\t|\tloss: 8812.56\n",
      "Training Epoch 5  96.9% | batch:        62 of        64\t|\tloss: 19957.2\n",
      "Training Epoch 5  98.4% | batch:        63 of        64\t|\tloss: 4453.57\n",
      "\n",
      "Training Epoch 6   0.0% | batch:         0 of        64\t|\tloss: 9471.54\n",
      "Training Epoch 6   1.6% | batch:         1 of        64\t|\tloss: 9278.57\n",
      "Training Epoch 6   3.1% | batch:         2 of        64\t|\tloss: 18531.4\n",
      "Training Epoch 6   4.7% | batch:         3 of        64\t|\tloss: 16363.5\n",
      "Training Epoch 6   6.2% | batch:         4 of        64\t|\tloss: 11210.1\n",
      "Training Epoch 6   7.8% | batch:         5 of        64\t|\tloss: 14726.3\n",
      "Training Epoch 6   9.4% | batch:         6 of        64\t|\tloss: 8774.23\n",
      "Training Epoch 6  10.9% | batch:         7 of        64\t|\tloss: 15107.1\n",
      "Training Epoch 6  12.5% | batch:         8 of        64\t|\tloss: 9505.69\n",
      "Training Epoch 6  14.1% | batch:         9 of        64\t|\tloss: 10992.2\n",
      "Training Epoch 6  15.6% | batch:        10 of        64\t|\tloss: 8794.78\n",
      "Training Epoch 6  17.2% | batch:        11 of        64\t|\tloss: 11564.4\n",
      "Training Epoch 6  18.8% | batch:        12 of        64\t|\tloss: 6311.2\n",
      "Training Epoch 6  20.3% | batch:        13 of        64\t|\tloss: 15110.5\n",
      "Training Epoch 6  21.9% | batch:        14 of        64\t|\tloss: 10678.4\n",
      "Training Epoch 6  23.4% | batch:        15 of        64\t|\tloss: 8731.32\n",
      "Training Epoch 6  25.0% | batch:        16 of        64\t|\tloss: 13621.3\n",
      "Training Epoch 6  26.6% | batch:        17 of        64\t|\tloss: 12568.2\n",
      "Training Epoch 6  28.1% | batch:        18 of        64\t|\tloss: 12809.6\n",
      "Training Epoch 6  29.7% | batch:        19 of        64\t|\tloss: 5725.03\n",
      "Training Epoch 6  31.2% | batch:        20 of        64\t|\tloss: 24903.5\n",
      "Training Epoch 6  32.8% | batch:        21 of        64\t|\tloss: 13738.5\n",
      "Training Epoch 6  34.4% | batch:        22 of        64\t|\tloss: 8048.52\n",
      "Training Epoch 6  35.9% | batch:        23 of        64\t|\tloss: 11715.1\n",
      "Training Epoch 6  37.5% | batch:        24 of        64\t|\tloss: 15849.6\n",
      "Training Epoch 6  39.1% | batch:        25 of        64\t|\tloss: 9368.45\n",
      "Training Epoch 6  40.6% | batch:        26 of        64\t|\tloss: 20073.7\n",
      "Training Epoch 6  42.2% | batch:        27 of        64\t|\tloss: 7162.82\n",
      "Training Epoch 6  43.8% | batch:        28 of        64\t|\tloss: 11949.1\n",
      "Training Epoch 6  45.3% | batch:        29 of        64\t|\tloss: 9886.1\n",
      "Training Epoch 6  46.9% | batch:        30 of        64\t|\tloss: 8749.52\n",
      "Training Epoch 6  48.4% | batch:        31 of        64\t|\tloss: 11058.2\n",
      "Training Epoch 6  50.0% | batch:        32 of        64\t|\tloss: 8439.49\n",
      "Training Epoch 6  51.6% | batch:        33 of        64\t|\tloss: 7952.31\n",
      "Training Epoch 6  53.1% | batch:        34 of        64\t|\tloss: 11492.3\n",
      "Training Epoch 6  54.7% | batch:        35 of        64\t|\tloss: 9946.24\n",
      "Training Epoch 6  56.2% | batch:        36 of        64\t|\tloss: 5818.69\n",
      "Training Epoch 6  57.8% | batch:        37 of        64\t|\tloss: 11650.1\n",
      "Training Epoch 6  59.4% | batch:        38 of        64\t|\tloss: 10496\n",
      "Training Epoch 6  60.9% | batch:        39 of        64\t|\tloss: 13188.2\n",
      "Training Epoch 6  62.5% | batch:        40 of        64\t|\tloss: 6201.31\n",
      "Training Epoch 6  64.1% | batch:        41 of        64\t|\tloss: 8886.21\n",
      "Training Epoch 6  65.6% | batch:        42 of        64\t|\tloss: 5857\n",
      "Training Epoch 6  67.2% | batch:        43 of        64\t|\tloss: 9019.39\n",
      "Training Epoch 6  68.8% | batch:        44 of        64\t|\tloss: 10289.5\n",
      "Training Epoch 6  70.3% | batch:        45 of        64\t|\tloss: 14300.8\n",
      "Training Epoch 6  71.9% | batch:        46 of        64\t|\tloss: 8318.95\n",
      "Training Epoch 6  73.4% | batch:        47 of        64\t|\tloss: 11813.5\n",
      "Training Epoch 6  75.0% | batch:        48 of        64\t|\tloss: 10910.5\n",
      "Training Epoch 6  76.6% | batch:        49 of        64\t|\tloss: 6293.18\n",
      "Training Epoch 6  78.1% | batch:        50 of        64\t|\tloss: 10881.4\n",
      "Training Epoch 6  79.7% | batch:        51 of        64\t|\tloss: 10944.8\n",
      "Training Epoch 6  81.2% | batch:        52 of        64\t|\tloss: 14782.8\n",
      "Training Epoch 6  82.8% | batch:        53 of        64\t|\tloss: 11184.1\n",
      "Training Epoch 6  84.4% | batch:        54 of        64\t|\tloss: 4591.65\n",
      "Training Epoch 6  85.9% | batch:        55 of        64\t|\tloss: 12241.8\n",
      "Training Epoch 6  87.5% | batch:        56 of        64\t|\tloss: 7232.8\n",
      "Training Epoch 6  89.1% | batch:        57 of        64\t|\tloss: 10153.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:02,707 | INFO : Epoch 6 Training Summary: epoch: 6.000000 | loss: 10990.485857 | \n",
      "2023-05-10 17:08:02,708 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2385482788085938 seconds\n",
      "\n",
      "2023-05-10 17:08:02,709 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2336077292760212 seconds\n",
      "2023-05-10 17:08:02,709 | INFO : Avg batch train. time: 0.019275120769937832 seconds\n",
      "2023-05-10 17:08:02,710 | INFO : Avg sample train. time: 0.00030549968530857387 seconds\n",
      "2023-05-10 17:08:02,711 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  90.6% | batch:        58 of        64\t|\tloss: 6856.63\n",
      "Training Epoch 6  92.2% | batch:        59 of        64\t|\tloss: 12444.6\n",
      "Training Epoch 6  93.8% | batch:        60 of        64\t|\tloss: 10701.3\n",
      "Training Epoch 6  95.3% | batch:        61 of        64\t|\tloss: 14772.7\n",
      "Training Epoch 6  96.9% | batch:        62 of        64\t|\tloss: 12819.2\n",
      "Training Epoch 6  98.4% | batch:        63 of        64\t|\tloss: 6090.87\n",
      "\n",
      "Evaluating Epoch 6   0.0% | batch:         0 of        16\t|\tloss: 7990.25\n",
      "Evaluating Epoch 6   6.2% | batch:         1 of        16\t|\tloss: 8114.65\n",
      "Evaluating Epoch 6  12.5% | batch:         2 of        16\t|\tloss: 7673.26\n",
      "Evaluating Epoch 6  18.8% | batch:         3 of        16\t|\tloss: 8548.46\n",
      "Evaluating Epoch 6  25.0% | batch:         4 of        16\t|\tloss: 8259.73\n",
      "Evaluating Epoch 6  31.2% | batch:         5 of        16\t|\tloss: 9902.88\n",
      "Evaluating Epoch 6  37.5% | batch:         6 of        16\t|\tloss: 10074.6\n",
      "Evaluating Epoch 6  43.8% | batch:         7 of        16\t|\tloss: 14124.4\n",
      "Evaluating Epoch 6  50.0% | batch:         8 of        16\t|\tloss: 8872.84\n",
      "Evaluating Epoch 6  56.2% | batch:         9 of        16\t|\tloss: 18261.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:02,870 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.15873432159423828 seconds\n",
      "\n",
      "2023-05-10 17:08:02,870 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.28067846298217775 seconds\n",
      "2023-05-10 17:08:02,871 | INFO : Avg batch val. time: 0.01754240393638611 seconds\n",
      "2023-05-10 17:08:02,872 | INFO : Avg sample val. time: 0.0002778994682991859 seconds\n",
      "2023-05-10 17:08:02,873 | INFO : Epoch 6 Validation Summary: epoch: 6.000000 | loss: 9999.026640 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 6  62.5% | batch:        10 of        16\t|\tloss: 8351.82\n",
      "Evaluating Epoch 6  68.8% | batch:        11 of        16\t|\tloss: 14367.5\n",
      "Evaluating Epoch 6  75.0% | batch:        12 of        16\t|\tloss: 5763.89\n",
      "Evaluating Epoch 6  81.2% | batch:        13 of        16\t|\tloss: 6895.64\n",
      "Evaluating Epoch 6  87.5% | batch:        14 of        16\t|\tloss: 10497.2\n",
      "Evaluating Epoch 6  93.8% | batch:        15 of        16\t|\tloss: 12925.6\n",
      "\n",
      "Training Epoch 7   0.0% | batch:         0 of        64\t|\tloss: 5624.55\n",
      "Training Epoch 7   1.6% | batch:         1 of        64\t|\tloss: 15786.7\n",
      "Training Epoch 7   3.1% | batch:         2 of        64\t|\tloss: 9004.79\n",
      "Training Epoch 7   4.7% | batch:         3 of        64\t|\tloss: 7047.29\n",
      "Training Epoch 7   6.2% | batch:         4 of        64\t|\tloss: 8106.27\n",
      "Training Epoch 7   7.8% | batch:         5 of        64\t|\tloss: 9473.13\n",
      "Training Epoch 7   9.4% | batch:         6 of        64\t|\tloss: 7683.34\n",
      "Training Epoch 7  10.9% | batch:         7 of        64\t|\tloss: 4844.05\n",
      "Training Epoch 7  12.5% | batch:         8 of        64\t|\tloss: 12584.1\n",
      "Training Epoch 7  14.1% | batch:         9 of        64\t|\tloss: 6492.01\n",
      "Training Epoch 7  15.6% | batch:        10 of        64\t|\tloss: 6108.21\n",
      "Training Epoch 7  17.2% | batch:        11 of        64\t|\tloss: 12069.2\n",
      "Training Epoch 7  18.8% | batch:        12 of        64\t|\tloss: 6706.87\n",
      "Training Epoch 7  20.3% | batch:        13 of        64\t|\tloss: 7985.49\n",
      "Training Epoch 7  21.9% | batch:        14 of        64\t|\tloss: 13726.7\n",
      "Training Epoch 7  23.4% | batch:        15 of        64\t|\tloss: 7834.99\n",
      "Training Epoch 7  25.0% | batch:        16 of        64\t|\tloss: 8669.95\n",
      "Training Epoch 7  26.6% | batch:        17 of        64\t|\tloss: 9227.53\n",
      "Training Epoch 7  28.1% | batch:        18 of        64\t|\tloss: 6848.2\n",
      "Training Epoch 7  29.7% | batch:        19 of        64\t|\tloss: 10377.8\n",
      "Training Epoch 7  31.2% | batch:        20 of        64\t|\tloss: 5667.75\n",
      "Training Epoch 7  32.8% | batch:        21 of        64\t|\tloss: 11008\n",
      "Training Epoch 7  34.4% | batch:        22 of        64\t|\tloss: 11644.7\n",
      "Training Epoch 7  35.9% | batch:        23 of        64\t|\tloss: 5460.9\n",
      "Training Epoch 7  37.5% | batch:        24 of        64\t|\tloss: 8732.57\n",
      "Training Epoch 7  39.1% | batch:        25 of        64\t|\tloss: 6697.23\n",
      "Training Epoch 7  40.6% | batch:        26 of        64\t|\tloss: 8489.36\n",
      "Training Epoch 7  42.2% | batch:        27 of        64\t|\tloss: 9966.32\n",
      "Training Epoch 7  43.8% | batch:        28 of        64\t|\tloss: 11615.5\n",
      "Training Epoch 7  45.3% | batch:        29 of        64\t|\tloss: 7728.89\n",
      "Training Epoch 7  46.9% | batch:        30 of        64\t|\tloss: 5316\n",
      "Training Epoch 7  48.4% | batch:        31 of        64\t|\tloss: 7395.67\n",
      "Training Epoch 7  50.0% | batch:        32 of        64\t|\tloss: 3620.8\n",
      "Training Epoch 7  51.6% | batch:        33 of        64\t|\tloss: 5924.22\n",
      "Training Epoch 7  53.1% | batch:        34 of        64\t|\tloss: 13237.1\n",
      "Training Epoch 7  54.7% | batch:        35 of        64\t|\tloss: 6772.92\n",
      "Training Epoch 7  56.2% | batch:        36 of        64\t|\tloss: 12233.2\n",
      "Training Epoch 7  57.8% | batch:        37 of        64\t|\tloss: 5608.95\n",
      "Training Epoch 7  59.4% | batch:        38 of        64\t|\tloss: 5109.34\n",
      "Training Epoch 7  60.9% | batch:        39 of        64\t|\tloss: 6634.7\n",
      "Training Epoch 7  62.5% | batch:        40 of        64\t|\tloss: 10206.9\n",
      "Training Epoch 7  64.1% | batch:        41 of        64\t|\tloss: 16757.5\n",
      "Training Epoch 7  65.6% | batch:        42 of        64\t|\tloss: 9293.46\n",
      "Training Epoch 7  67.2% | batch:        43 of        64\t|\tloss: 11499.6\n",
      "Training Epoch 7  68.8% | batch:        44 of        64\t|\tloss: 2355.89\n",
      "Training Epoch 7  70.3% | batch:        45 of        64\t|\tloss: 11192.9\n",
      "Training Epoch 7  71.9% | batch:        46 of        64\t|\tloss: 9180.04\n",
      "Training Epoch 7  73.4% | batch:        47 of        64\t|\tloss: 4026.21\n",
      "Training Epoch 7  75.0% | batch:        48 of        64\t|\tloss: 10493\n",
      "Training Epoch 7  76.6% | batch:        49 of        64\t|\tloss: 5440.26\n",
      "Training Epoch 7  78.1% | batch:        50 of        64\t|\tloss: 4822.21\n",
      "Training Epoch 7  79.7% | batch:        51 of        64\t|\tloss: 8294.14\n",
      "Training Epoch 7  81.2% | batch:        52 of        64\t|\tloss: 4784.45\n",
      "Training Epoch 7  82.8% | batch:        53 of        64\t|\tloss: 3131.28\n",
      "Training Epoch 7  84.4% | batch:        54 of        64\t|\tloss: 7259.56\n",
      "Training Epoch 7  85.9% | batch:        55 of        64\t|\tloss: 16908.4\n",
      "Training Epoch 7  87.5% | batch:        56 of        64\t|\tloss: 9120.7\n",
      "Training Epoch 7  89.1% | batch:        57 of        64\t|\tloss: 4211.9\n",
      "Training Epoch 7  90.6% | batch:        58 of        64\t|\tloss: 10004.6\n",
      "Training Epoch 7  92.2% | batch:        59 of        64\t|\tloss: 13922.2\n",
      "Training Epoch 7  93.8% | batch:        60 of        64\t|\tloss: 3124.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:04,088 | INFO : Epoch 7 Training Summary: epoch: 7.000000 | loss: 8275.140544 | \n",
      "2023-05-10 17:08:04,089 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.20009183883667 seconds\n",
      "\n",
      "2023-05-10 17:08:04,089 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2288197449275426 seconds\n",
      "2023-05-10 17:08:04,090 | INFO : Avg batch train. time: 0.019200308514492854 seconds\n",
      "2023-05-10 17:08:04,091 | INFO : Avg sample train. time: 0.0003043139536720016 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  95.3% | batch:        61 of        64\t|\tloss: 5292.06\n",
      "Training Epoch 7  96.9% | batch:        62 of        64\t|\tloss: 5617.34\n",
      "Training Epoch 7  98.4% | batch:        63 of        64\t|\tloss: 1122.56\n",
      "\n",
      "Training Epoch 8   0.0% | batch:         0 of        64\t|\tloss: 5881.78\n",
      "Training Epoch 8   1.6% | batch:         1 of        64\t|\tloss: 7503.78\n",
      "Training Epoch 8   3.1% | batch:         2 of        64\t|\tloss: 4595.56\n",
      "Training Epoch 8   4.7% | batch:         3 of        64\t|\tloss: 6300.8\n",
      "Training Epoch 8   6.2% | batch:         4 of        64\t|\tloss: 4728.35\n",
      "Training Epoch 8   7.8% | batch:         5 of        64\t|\tloss: 3016.68\n",
      "Training Epoch 8   9.4% | batch:         6 of        64\t|\tloss: 6639.8\n",
      "Training Epoch 8  10.9% | batch:         7 of        64\t|\tloss: 7760.16\n",
      "Training Epoch 8  12.5% | batch:         8 of        64\t|\tloss: 6981.03\n",
      "Training Epoch 8  14.1% | batch:         9 of        64\t|\tloss: 7491.05\n",
      "Training Epoch 8  15.6% | batch:        10 of        64\t|\tloss: 6784.24\n",
      "Training Epoch 8  17.2% | batch:        11 of        64\t|\tloss: 9883.42\n",
      "Training Epoch 8  18.8% | batch:        12 of        64\t|\tloss: 7645.92\n",
      "Training Epoch 8  20.3% | batch:        13 of        64\t|\tloss: 3543.8\n",
      "Training Epoch 8  21.9% | batch:        14 of        64\t|\tloss: 6048.73\n",
      "Training Epoch 8  23.4% | batch:        15 of        64\t|\tloss: 8688.64\n",
      "Training Epoch 8  25.0% | batch:        16 of        64\t|\tloss: 3727.91\n",
      "Training Epoch 8  26.6% | batch:        17 of        64\t|\tloss: 6499.15\n",
      "Training Epoch 8  28.1% | batch:        18 of        64\t|\tloss: 5665.3\n",
      "Training Epoch 8  29.7% | batch:        19 of        64\t|\tloss: 9823.76\n",
      "Training Epoch 8  31.2% | batch:        20 of        64\t|\tloss: 7069.87\n",
      "Training Epoch 8  32.8% | batch:        21 of        64\t|\tloss: 4561.01\n",
      "Training Epoch 8  34.4% | batch:        22 of        64\t|\tloss: 4874.66\n",
      "Training Epoch 8  35.9% | batch:        23 of        64\t|\tloss: 4748.9\n",
      "Training Epoch 8  37.5% | batch:        24 of        64\t|\tloss: 3536.39\n",
      "Training Epoch 8  39.1% | batch:        25 of        64\t|\tloss: 6459.62\n",
      "Training Epoch 8  40.6% | batch:        26 of        64\t|\tloss: 6793.67\n",
      "Training Epoch 8  42.2% | batch:        27 of        64\t|\tloss: 1947.95\n",
      "Training Epoch 8  43.8% | batch:        28 of        64\t|\tloss: 7576.52\n",
      "Training Epoch 8  45.3% | batch:        29 of        64\t|\tloss: 11653.3\n",
      "Training Epoch 8  46.9% | batch:        30 of        64\t|\tloss: 7307.08\n",
      "Training Epoch 8  48.4% | batch:        31 of        64\t|\tloss: 8559.03\n",
      "Training Epoch 8  50.0% | batch:        32 of        64\t|\tloss: 3505.09\n",
      "Training Epoch 8  51.6% | batch:        33 of        64\t|\tloss: 5982.21\n",
      "Training Epoch 8  53.1% | batch:        34 of        64\t|\tloss: 4637.93\n",
      "Training Epoch 8  54.7% | batch:        35 of        64\t|\tloss: 2997.18\n",
      "Training Epoch 8  56.2% | batch:        36 of        64\t|\tloss: 7280.68\n",
      "Training Epoch 8  57.8% | batch:        37 of        64\t|\tloss: 3690.65\n",
      "Training Epoch 8  59.4% | batch:        38 of        64\t|\tloss: 6372.01\n",
      "Training Epoch 8  60.9% | batch:        39 of        64\t|\tloss: 4776.04\n",
      "Training Epoch 8  62.5% | batch:        40 of        64\t|\tloss: 2298.18\n",
      "Training Epoch 8  64.1% | batch:        41 of        64\t|\tloss: 4027.79\n",
      "Training Epoch 8  65.6% | batch:        42 of        64\t|\tloss: 8257.26\n",
      "Training Epoch 8  67.2% | batch:        43 of        64\t|\tloss: 2891.93\n",
      "Training Epoch 8  68.8% | batch:        44 of        64\t|\tloss: 3572.82\n",
      "Training Epoch 8  70.3% | batch:        45 of        64\t|\tloss: 5701.77\n",
      "Training Epoch 8  71.9% | batch:        46 of        64\t|\tloss: 7607.47\n",
      "Training Epoch 8  73.4% | batch:        47 of        64\t|\tloss: 8280.27\n",
      "Training Epoch 8  75.0% | batch:        48 of        64\t|\tloss: 7693.13\n",
      "Training Epoch 8  76.6% | batch:        49 of        64\t|\tloss: 3077.73\n",
      "Training Epoch 8  78.1% | batch:        50 of        64\t|\tloss: 7016.26\n",
      "Training Epoch 8  79.7% | batch:        51 of        64\t|\tloss: 4735.21\n",
      "Training Epoch 8  81.2% | batch:        52 of        64\t|\tloss: 5491.74\n",
      "Training Epoch 8  82.8% | batch:        53 of        64\t|\tloss: 5600.81\n",
      "Training Epoch 8  84.4% | batch:        54 of        64\t|\tloss: 4453.87\n",
      "Training Epoch 8  85.9% | batch:        55 of        64\t|\tloss: 6475.05\n",
      "Training Epoch 8  87.5% | batch:        56 of        64\t|\tloss: 6049.17\n",
      "Training Epoch 8  89.1% | batch:        57 of        64\t|\tloss: 2661.72\n",
      "Training Epoch 8  90.6% | batch:        58 of        64\t|\tloss: 7407.26\n",
      "Training Epoch 8  92.2% | batch:        59 of        64\t|\tloss: 2158.6\n",
      "Training Epoch 8  93.8% | batch:        60 of        64\t|\tloss: 6631.44\n",
      "Training Epoch 8  95.3% | batch:        61 of        64\t|\tloss: 4588.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:05,352 | INFO : Epoch 8 Training Summary: epoch: 8.000000 | loss: 5804.713257 | \n",
      "2023-05-10 17:08:05,353 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.252227783203125 seconds\n",
      "\n",
      "2023-05-10 17:08:05,353 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2317457497119904 seconds\n",
      "2023-05-10 17:08:05,354 | INFO : Avg batch train. time: 0.01924602733924985 seconds\n",
      "2023-05-10 17:08:05,354 | INFO : Avg sample train. time: 0.00030503857100346466 seconds\n",
      "2023-05-10 17:08:05,355 | INFO : Evaluating on validation set ...\n",
      "2023-05-10 17:08:05,505 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14940094947814941 seconds\n",
      "\n",
      "2023-05-10 17:08:05,505 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.25879887739817303 seconds\n",
      "2023-05-10 17:08:05,506 | INFO : Avg batch val. time: 0.016174929837385815 seconds\n",
      "2023-05-10 17:08:05,506 | INFO : Avg sample val. time: 0.0002562365122754188 seconds\n",
      "2023-05-10 17:08:05,507 | INFO : Epoch 8 Validation Summary: epoch: 8.000000 | loss: 6266.311123 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  96.9% | batch:        62 of        64\t|\tloss: 4600.5\n",
      "Training Epoch 8  98.4% | batch:        63 of        64\t|\tloss: 15182.6\n",
      "\n",
      "Evaluating Epoch 8   0.0% | batch:         0 of        16\t|\tloss: 4322.63\n",
      "Evaluating Epoch 8   6.2% | batch:         1 of        16\t|\tloss: 4144.39\n",
      "Evaluating Epoch 8  12.5% | batch:         2 of        16\t|\tloss: 3682.05\n",
      "Evaluating Epoch 8  18.8% | batch:         3 of        16\t|\tloss: 5065.4\n",
      "Evaluating Epoch 8  25.0% | batch:         4 of        16\t|\tloss: 4412.05\n",
      "Evaluating Epoch 8  31.2% | batch:         5 of        16\t|\tloss: 5691.94\n",
      "Evaluating Epoch 8  37.5% | batch:         6 of        16\t|\tloss: 6518.5\n",
      "Evaluating Epoch 8  43.8% | batch:         7 of        16\t|\tloss: 10115.3\n",
      "Evaluating Epoch 8  50.0% | batch:         8 of        16\t|\tloss: 5264.51\n",
      "Evaluating Epoch 8  56.2% | batch:         9 of        16\t|\tloss: 14628.7\n",
      "Evaluating Epoch 8  62.5% | batch:        10 of        16\t|\tloss: 5387.62\n",
      "Evaluating Epoch 8  68.8% | batch:        11 of        16\t|\tloss: 10817.5\n",
      "Evaluating Epoch 8  75.0% | batch:        12 of        16\t|\tloss: 2851.38\n",
      "Evaluating Epoch 8  81.2% | batch:        13 of        16\t|\tloss: 3459.92\n",
      "Evaluating Epoch 8  87.5% | batch:        14 of        16\t|\tloss: 6422.08\n",
      "Evaluating Epoch 8  93.8% | batch:        15 of        16\t|\tloss: 7816.07\n",
      "\n",
      "Training Epoch 9   0.0% | batch:         0 of        64\t|\tloss: 4232.77\n",
      "Training Epoch 9   1.6% | batch:         1 of        64\t|\tloss: 2240.04\n",
      "Training Epoch 9   3.1% | batch:         2 of        64\t|\tloss: 3613.75\n",
      "Training Epoch 9   4.7% | batch:         3 of        64\t|\tloss: 4508.52\n",
      "Training Epoch 9   6.2% | batch:         4 of        64\t|\tloss: 1547.93\n",
      "Training Epoch 9   7.8% | batch:         5 of        64\t|\tloss: 23921.8\n",
      "Training Epoch 9   9.4% | batch:         6 of        64\t|\tloss: 2467.71\n",
      "Training Epoch 9  10.9% | batch:         7 of        64\t|\tloss: 2459.18\n",
      "Training Epoch 9  12.5% | batch:         8 of        64\t|\tloss: 8808.73\n",
      "Training Epoch 9  14.1% | batch:         9 of        64\t|\tloss: 3723.68\n",
      "Training Epoch 9  15.6% | batch:        10 of        64\t|\tloss: 5081.09\n",
      "Training Epoch 9  17.2% | batch:        11 of        64\t|\tloss: 3179.02\n",
      "Training Epoch 9  18.8% | batch:        12 of        64\t|\tloss: 2526.89\n",
      "Training Epoch 9  20.3% | batch:        13 of        64\t|\tloss: 6985.89\n",
      "Training Epoch 9  21.9% | batch:        14 of        64\t|\tloss: 5311.57\n",
      "Training Epoch 9  23.4% | batch:        15 of        64\t|\tloss: 3432.23\n",
      "Training Epoch 9  25.0% | batch:        16 of        64\t|\tloss: 6158.76\n",
      "Training Epoch 9  26.6% | batch:        17 of        64\t|\tloss: 3530.85\n",
      "Training Epoch 9  28.1% | batch:        18 of        64\t|\tloss: 2811.08\n",
      "Training Epoch 9  29.7% | batch:        19 of        64\t|\tloss: 8342.82\n",
      "Training Epoch 9  31.2% | batch:        20 of        64\t|\tloss: 2834.54\n",
      "Training Epoch 9  32.8% | batch:        21 of        64\t|\tloss: 4333.36\n",
      "Training Epoch 9  34.4% | batch:        22 of        64\t|\tloss: 4081.2\n",
      "Training Epoch 9  35.9% | batch:        23 of        64\t|\tloss: 13096.5\n",
      "Training Epoch 9  37.5% | batch:        24 of        64\t|\tloss: 2980.19\n",
      "Training Epoch 9  39.1% | batch:        25 of        64\t|\tloss: 2591.52\n",
      "Training Epoch 9  40.6% | batch:        26 of        64\t|\tloss: 3856.02\n",
      "Training Epoch 9  42.2% | batch:        27 of        64\t|\tloss: 2179.55\n",
      "Training Epoch 9  43.8% | batch:        28 of        64\t|\tloss: 4177.3\n",
      "Training Epoch 9  45.3% | batch:        29 of        64\t|\tloss: 2943.84\n",
      "Training Epoch 9  46.9% | batch:        30 of        64\t|\tloss: 11793.6\n",
      "Training Epoch 9  48.4% | batch:        31 of        64\t|\tloss: 4888.46\n",
      "Training Epoch 9  50.0% | batch:        32 of        64\t|\tloss: 3640.59\n",
      "Training Epoch 9  51.6% | batch:        33 of        64\t|\tloss: 4872.63\n",
      "Training Epoch 9  53.1% | batch:        34 of        64\t|\tloss: 1353.85\n",
      "Training Epoch 9  54.7% | batch:        35 of        64\t|\tloss: 2588.71\n",
      "Training Epoch 9  56.2% | batch:        36 of        64\t|\tloss: 2478.44\n",
      "Training Epoch 9  57.8% | batch:        37 of        64\t|\tloss: 4391.82\n",
      "Training Epoch 9  59.4% | batch:        38 of        64\t|\tloss: 2421.15\n",
      "Training Epoch 9  60.9% | batch:        39 of        64\t|\tloss: 2395.42\n",
      "Training Epoch 9  62.5% | batch:        40 of        64\t|\tloss: 4205.17\n",
      "Training Epoch 9  64.1% | batch:        41 of        64\t|\tloss: 3643.63\n",
      "Training Epoch 9  65.6% | batch:        42 of        64\t|\tloss: 2862.25\n",
      "Training Epoch 9  67.2% | batch:        43 of        64\t|\tloss: 5585.06\n",
      "Training Epoch 9  68.8% | batch:        44 of        64\t|\tloss: 1725.72\n",
      "Training Epoch 9  70.3% | batch:        45 of        64\t|\tloss: 3051.24\n",
      "Training Epoch 9  71.9% | batch:        46 of        64\t|\tloss: 2507.34\n",
      "Training Epoch 9  73.4% | batch:        47 of        64\t|\tloss: 4027.32\n",
      "Training Epoch 9  75.0% | batch:        48 of        64\t|\tloss: 4172\n",
      "Training Epoch 9  76.6% | batch:        49 of        64\t|\tloss: 4881.38\n",
      "Training Epoch 9  78.1% | batch:        50 of        64\t|\tloss: 5204.81\n",
      "Training Epoch 9  79.7% | batch:        51 of        64\t|\tloss: 6188.06\n",
      "Training Epoch 9  81.2% | batch:        52 of        64\t|\tloss: 4812.99\n",
      "Training Epoch 9  82.8% | batch:        53 of        64\t|\tloss: 4447.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:06,790 | INFO : Epoch 9 Training Summary: epoch: 9.000000 | loss: 4418.673410 | \n",
      "2023-05-10 17:08:06,791 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2689800262451172 seconds\n",
      "\n",
      "2023-05-10 17:08:06,791 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2358828915490045 seconds\n",
      "2023-05-10 17:08:06,792 | INFO : Avg batch train. time: 0.019310670180453196 seconds\n",
      "2023-05-10 17:08:06,792 | INFO : Avg sample train. time: 0.0003060631232166926 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  84.4% | batch:        54 of        64\t|\tloss: 3191.85\n",
      "Training Epoch 9  85.9% | batch:        55 of        64\t|\tloss: 3048.32\n",
      "Training Epoch 9  87.5% | batch:        56 of        64\t|\tloss: 3498.94\n",
      "Training Epoch 9  89.1% | batch:        57 of        64\t|\tloss: 2223.63\n",
      "Training Epoch 9  90.6% | batch:        58 of        64\t|\tloss: 1830.36\n",
      "Training Epoch 9  92.2% | batch:        59 of        64\t|\tloss: 4924.15\n",
      "Training Epoch 9  93.8% | batch:        60 of        64\t|\tloss: 5246.02\n",
      "Training Epoch 9  95.3% | batch:        61 of        64\t|\tloss: 4784.92\n",
      "Training Epoch 9  96.9% | batch:        62 of        64\t|\tloss: 3358.77\n",
      "Training Epoch 9  98.4% | batch:        63 of        64\t|\tloss: 6256.33\n",
      "\n",
      "Training Epoch 10   0.0% | batch:         0 of        64\t|\tloss: 3331.86\n",
      "Training Epoch 10   1.6% | batch:         1 of        64\t|\tloss: 2201.05\n",
      "Training Epoch 10   3.1% | batch:         2 of        64\t|\tloss: 4546.19\n",
      "Training Epoch 10   4.7% | batch:         3 of        64\t|\tloss: 4684.92\n",
      "Training Epoch 10   6.2% | batch:         4 of        64\t|\tloss: 4311.17\n",
      "Training Epoch 10   7.8% | batch:         5 of        64\t|\tloss: 3297.36\n",
      "Training Epoch 10   9.4% | batch:         6 of        64\t|\tloss: 3961.22\n",
      "Training Epoch 10  10.9% | batch:         7 of        64\t|\tloss: 1385.15\n",
      "Training Epoch 10  12.5% | batch:         8 of        64\t|\tloss: 1923.14\n",
      "Training Epoch 10  14.1% | batch:         9 of        64\t|\tloss: 6839.42\n",
      "Training Epoch 10  15.6% | batch:        10 of        64\t|\tloss: 4017.64\n",
      "Training Epoch 10  17.2% | batch:        11 of        64\t|\tloss: 2910.64\n",
      "Training Epoch 10  18.8% | batch:        12 of        64\t|\tloss: 4282.18\n",
      "Training Epoch 10  20.3% | batch:        13 of        64\t|\tloss: 2190.02\n",
      "Training Epoch 10  21.9% | batch:        14 of        64\t|\tloss: 1970.44\n",
      "Training Epoch 10  23.4% | batch:        15 of        64\t|\tloss: 2723.15\n",
      "Training Epoch 10  25.0% | batch:        16 of        64\t|\tloss: 1595.06\n",
      "Training Epoch 10  26.6% | batch:        17 of        64\t|\tloss: 2809.07\n",
      "Training Epoch 10  28.1% | batch:        18 of        64\t|\tloss: 1753.14\n",
      "Training Epoch 10  29.7% | batch:        19 of        64\t|\tloss: 3839.26\n",
      "Training Epoch 10  31.2% | batch:        20 of        64\t|\tloss: 2414.59\n",
      "Training Epoch 10  32.8% | batch:        21 of        64\t|\tloss: 2938.18\n",
      "Training Epoch 10  34.4% | batch:        22 of        64\t|\tloss: 3325.68\n",
      "Training Epoch 10  35.9% | batch:        23 of        64\t|\tloss: 4950.27\n",
      "Training Epoch 10  37.5% | batch:        24 of        64\t|\tloss: 2622.75\n",
      "Training Epoch 10  39.1% | batch:        25 of        64\t|\tloss: 6884.41\n",
      "Training Epoch 10  40.6% | batch:        26 of        64\t|\tloss: 2040.28\n",
      "Training Epoch 10  42.2% | batch:        27 of        64\t|\tloss: 7870.08\n",
      "Training Epoch 10  43.8% | batch:        28 of        64\t|\tloss: 3036.51\n",
      "Training Epoch 10  45.3% | batch:        29 of        64\t|\tloss: 3876.84\n",
      "Training Epoch 10  46.9% | batch:        30 of        64\t|\tloss: 1774.11\n",
      "Training Epoch 10  48.4% | batch:        31 of        64\t|\tloss: 5131.92\n",
      "Training Epoch 10  50.0% | batch:        32 of        64\t|\tloss: 1832.28\n",
      "Training Epoch 10  51.6% | batch:        33 of        64\t|\tloss: 2880.02\n",
      "Training Epoch 10  53.1% | batch:        34 of        64\t|\tloss: 1709.19\n",
      "Training Epoch 10  54.7% | batch:        35 of        64\t|\tloss: 3087.66\n",
      "Training Epoch 10  56.2% | batch:        36 of        64\t|\tloss: 4754.98\n",
      "Training Epoch 10  57.8% | batch:        37 of        64\t|\tloss: 1436.58\n",
      "Training Epoch 10  59.4% | batch:        38 of        64\t|\tloss: 2315\n",
      "Training Epoch 10  60.9% | batch:        39 of        64\t|\tloss: 4843.73\n",
      "Training Epoch 10  62.5% | batch:        40 of        64\t|\tloss: 1637.62\n",
      "Training Epoch 10  64.1% | batch:        41 of        64\t|\tloss: 1527.59\n",
      "Training Epoch 10  65.6% | batch:        42 of        64\t|\tloss: 4659.39\n",
      "Training Epoch 10  67.2% | batch:        43 of        64\t|\tloss: 4878.98\n",
      "Training Epoch 10  68.8% | batch:        44 of        64\t|\tloss: 1130.03\n",
      "Training Epoch 10  70.3% | batch:        45 of        64\t|\tloss: 2176.16\n",
      "Training Epoch 10  71.9% | batch:        46 of        64\t|\tloss: 1515.33\n",
      "Training Epoch 10  73.4% | batch:        47 of        64\t|\tloss: 3437.49\n",
      "Training Epoch 10  75.0% | batch:        48 of        64\t|\tloss: 3021.92\n",
      "Training Epoch 10  76.6% | batch:        49 of        64\t|\tloss: 1845.96\n",
      "Training Epoch 10  78.1% | batch:        50 of        64\t|\tloss: 3244.32\n",
      "Training Epoch 10  79.7% | batch:        51 of        64\t|\tloss: 1944.52\n",
      "Training Epoch 10  81.2% | batch:        52 of        64\t|\tloss: 3737.67\n",
      "Training Epoch 10  82.8% | batch:        53 of        64\t|\tloss: 1734.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:08,049 | INFO : Epoch 10 Training Summary: epoch: 10.000000 | loss: 3159.527072 | \n",
      "2023-05-10 17:08:08,050 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.247354507446289 seconds\n",
      "\n",
      "2023-05-10 17:08:08,051 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2370300531387328 seconds\n",
      "2023-05-10 17:08:08,052 | INFO : Avg batch train. time: 0.0193285945802927 seconds\n",
      "2023-05-10 17:08:08,053 | INFO : Avg sample train. time: 0.0003063472147446094 seconds\n",
      "2023-05-10 17:08:08,053 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  84.4% | batch:        54 of        64\t|\tloss: 1635.59\n",
      "Training Epoch 10  85.9% | batch:        55 of        64\t|\tloss: 2053\n",
      "Training Epoch 10  87.5% | batch:        56 of        64\t|\tloss: 4738.28\n",
      "Training Epoch 10  89.1% | batch:        57 of        64\t|\tloss: 2926.91\n",
      "Training Epoch 10  90.6% | batch:        58 of        64\t|\tloss: 1594.18\n",
      "Training Epoch 10  92.2% | batch:        59 of        64\t|\tloss: 5910.13\n",
      "Training Epoch 10  93.8% | batch:        60 of        64\t|\tloss: 1513.79\n",
      "Training Epoch 10  95.3% | batch:        61 of        64\t|\tloss: 1691\n",
      "Training Epoch 10  96.9% | batch:        62 of        64\t|\tloss: 6331.29\n",
      "Training Epoch 10  98.4% | batch:        63 of        64\t|\tloss: 1741.3\n",
      "\n",
      "Evaluating Epoch 10   0.0% | batch:         0 of        16\t|\tloss: 2406.96\n",
      "Evaluating Epoch 10   6.2% | batch:         1 of        16\t|\tloss: 1937.19\n",
      "Evaluating Epoch 10  12.5% | batch:         2 of        16\t|\tloss: 1983.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:08,212 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.15876007080078125 seconds\n",
      "\n",
      "2023-05-10 17:08:08,213 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.24450761931283133 seconds\n",
      "2023-05-10 17:08:08,213 | INFO : Avg batch val. time: 0.015281726207051958 seconds\n",
      "2023-05-10 17:08:08,214 | INFO : Avg sample val. time: 0.0002420867517948825 seconds\n",
      "2023-05-10 17:08:08,214 | INFO : Epoch 10 Validation Summary: epoch: 10.000000 | loss: 3278.886711 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 10  18.8% | batch:         3 of        16\t|\tloss: 2829.83\n",
      "Evaluating Epoch 10  25.0% | batch:         4 of        16\t|\tloss: 2937.25\n",
      "Evaluating Epoch 10  31.2% | batch:         5 of        16\t|\tloss: 4827.37\n",
      "Evaluating Epoch 10  37.5% | batch:         6 of        16\t|\tloss: 4480.71\n",
      "Evaluating Epoch 10  43.8% | batch:         7 of        16\t|\tloss: 3522.4\n",
      "Evaluating Epoch 10  50.0% | batch:         8 of        16\t|\tloss: 2832.38\n",
      "Evaluating Epoch 10  56.2% | batch:         9 of        16\t|\tloss: 7117.55\n",
      "Evaluating Epoch 10  62.5% | batch:        10 of        16\t|\tloss: 2899.2\n",
      "Evaluating Epoch 10  68.8% | batch:        11 of        16\t|\tloss: 3370.4\n",
      "Evaluating Epoch 10  75.0% | batch:        12 of        16\t|\tloss: 1631.83\n",
      "Evaluating Epoch 10  81.2% | batch:        13 of        16\t|\tloss: 1798.93\n",
      "Evaluating Epoch 10  87.5% | batch:        14 of        16\t|\tloss: 3389.83\n",
      "Evaluating Epoch 10  93.8% | batch:        15 of        16\t|\tloss: 4837.33\n",
      "\n",
      "Training Epoch 11   0.0% | batch:         0 of        64\t|\tloss: 5620.23\n",
      "Training Epoch 11   1.6% | batch:         1 of        64\t|\tloss: 1937.96\n",
      "Training Epoch 11   3.1% | batch:         2 of        64\t|\tloss: 4447.82\n",
      "Training Epoch 11   4.7% | batch:         3 of        64\t|\tloss: 1403.7\n",
      "Training Epoch 11   6.2% | batch:         4 of        64\t|\tloss: 1962.11\n",
      "Training Epoch 11   7.8% | batch:         5 of        64\t|\tloss: 5582.03\n",
      "Training Epoch 11   9.4% | batch:         6 of        64\t|\tloss: 3163.39\n",
      "Training Epoch 11  10.9% | batch:         7 of        64\t|\tloss: 4251.06\n",
      "Training Epoch 11  12.5% | batch:         8 of        64\t|\tloss: 1635.15\n",
      "Training Epoch 11  14.1% | batch:         9 of        64\t|\tloss: 1837.27\n",
      "Training Epoch 11  15.6% | batch:        10 of        64\t|\tloss: 2838.45\n",
      "Training Epoch 11  17.2% | batch:        11 of        64\t|\tloss: 1551.05\n",
      "Training Epoch 11  18.8% | batch:        12 of        64\t|\tloss: 3886.03\n",
      "Training Epoch 11  20.3% | batch:        13 of        64\t|\tloss: 1289.95\n",
      "Training Epoch 11  21.9% | batch:        14 of        64\t|\tloss: 1777.4\n",
      "Training Epoch 11  23.4% | batch:        15 of        64\t|\tloss: 2642.1\n",
      "Training Epoch 11  25.0% | batch:        16 of        64\t|\tloss: 1649.78\n",
      "Training Epoch 11  26.6% | batch:        17 of        64\t|\tloss: 4368.47\n",
      "Training Epoch 11  28.1% | batch:        18 of        64\t|\tloss: 2940.69\n",
      "Training Epoch 11  29.7% | batch:        19 of        64\t|\tloss: 1746.21\n",
      "Training Epoch 11  31.2% | batch:        20 of        64\t|\tloss: 12353.6\n",
      "Training Epoch 11  32.8% | batch:        21 of        64\t|\tloss: 3136.08\n",
      "Training Epoch 11  34.4% | batch:        22 of        64\t|\tloss: 2522.02\n",
      "Training Epoch 11  35.9% | batch:        23 of        64\t|\tloss: 2081.15\n",
      "Training Epoch 11  37.5% | batch:        24 of        64\t|\tloss: 2645.64\n",
      "Training Epoch 11  39.1% | batch:        25 of        64\t|\tloss: 2149\n",
      "Training Epoch 11  40.6% | batch:        26 of        64\t|\tloss: 3386.91\n",
      "Training Epoch 11  42.2% | batch:        27 of        64\t|\tloss: 6253.47\n",
      "Training Epoch 11  43.8% | batch:        28 of        64\t|\tloss: 1985.22\n",
      "Training Epoch 11  45.3% | batch:        29 of        64\t|\tloss: 1680.85\n",
      "Training Epoch 11  46.9% | batch:        30 of        64\t|\tloss: 1904.93\n",
      "Training Epoch 11  48.4% | batch:        31 of        64\t|\tloss: 3822.11\n",
      "Training Epoch 11  50.0% | batch:        32 of        64\t|\tloss: 2285.3\n",
      "Training Epoch 11  51.6% | batch:        33 of        64\t|\tloss: 3262.4\n",
      "Training Epoch 11  53.1% | batch:        34 of        64\t|\tloss: 2947.76\n",
      "Training Epoch 11  54.7% | batch:        35 of        64\t|\tloss: 2731.05\n",
      "Training Epoch 11  56.2% | batch:        36 of        64\t|\tloss: 2003.24\n",
      "Training Epoch 11  57.8% | batch:        37 of        64\t|\tloss: 1985.02\n",
      "Training Epoch 11  59.4% | batch:        38 of        64\t|\tloss: 1900.81\n",
      "Training Epoch 11  60.9% | batch:        39 of        64\t|\tloss: 2517.39\n",
      "Training Epoch 11  62.5% | batch:        40 of        64\t|\tloss: 3070.36\n",
      "Training Epoch 11  64.1% | batch:        41 of        64\t|\tloss: 1593.08\n",
      "Training Epoch 11  65.6% | batch:        42 of        64\t|\tloss: 1667.85\n",
      "Training Epoch 11  67.2% | batch:        43 of        64\t|\tloss: 3761.49\n",
      "Training Epoch 11  68.8% | batch:        44 of        64\t|\tloss: 6041.83\n",
      "Training Epoch 11  70.3% | batch:        45 of        64\t|\tloss: 1649.87\n",
      "Training Epoch 11  71.9% | batch:        46 of        64\t|\tloss: 3759.61\n",
      "Training Epoch 11  73.4% | batch:        47 of        64\t|\tloss: 6073.43\n",
      "Training Epoch 11  75.0% | batch:        48 of        64\t|\tloss: 5747\n",
      "Training Epoch 11  76.6% | batch:        49 of        64\t|\tloss: 4530.5\n",
      "Training Epoch 11  78.1% | batch:        50 of        64\t|\tloss: 2104.86\n",
      "Training Epoch 11  79.7% | batch:        51 of        64\t|\tloss: 4029.77\n",
      "Training Epoch 11  81.2% | batch:        52 of        64\t|\tloss: 4486.01\n",
      "Training Epoch 11  82.8% | batch:        53 of        64\t|\tloss: 1553.02\n",
      "Training Epoch 11  84.4% | batch:        54 of        64\t|\tloss: 2241.4\n",
      "Training Epoch 11  85.9% | batch:        55 of        64\t|\tloss: 2684.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:09,503 | INFO : Epoch 11 Training Summary: epoch: 11.000000 | loss: 3078.561478 | \n",
      "2023-05-10 17:08:09,504 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.270965576171875 seconds\n",
      "\n",
      "2023-05-10 17:08:09,505 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2401151006872004 seconds\n",
      "2023-05-10 17:08:09,505 | INFO : Avg batch train. time: 0.019376798448237507 seconds\n",
      "2023-05-10 17:08:09,506 | INFO : Avg sample train. time: 0.0003071112185951462 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  87.5% | batch:        56 of        64\t|\tloss: 1666.98\n",
      "Training Epoch 11  89.1% | batch:        57 of        64\t|\tloss: 4695.19\n",
      "Training Epoch 11  90.6% | batch:        58 of        64\t|\tloss: 2710.36\n",
      "Training Epoch 11  92.2% | batch:        59 of        64\t|\tloss: 3136.36\n",
      "Training Epoch 11  93.8% | batch:        60 of        64\t|\tloss: 2532.86\n",
      "Training Epoch 11  95.3% | batch:        61 of        64\t|\tloss: 1991.24\n",
      "Training Epoch 11  96.9% | batch:        62 of        64\t|\tloss: 2145\n",
      "Training Epoch 11  98.4% | batch:        63 of        64\t|\tloss: 2989.04\n",
      "\n",
      "Training Epoch 12   0.0% | batch:         0 of        64\t|\tloss: 2234.76\n",
      "Training Epoch 12   1.6% | batch:         1 of        64\t|\tloss: 1620.61\n",
      "Training Epoch 12   3.1% | batch:         2 of        64\t|\tloss: 4595.29\n",
      "Training Epoch 12   4.7% | batch:         3 of        64\t|\tloss: 3361.75\n",
      "Training Epoch 12   6.2% | batch:         4 of        64\t|\tloss: 3626.27\n",
      "Training Epoch 12   7.8% | batch:         5 of        64\t|\tloss: 1147.81\n",
      "Training Epoch 12   9.4% | batch:         6 of        64\t|\tloss: 2713.02\n",
      "Training Epoch 12  10.9% | batch:         7 of        64\t|\tloss: 1320.31\n",
      "Training Epoch 12  12.5% | batch:         8 of        64\t|\tloss: 4141.25\n",
      "Training Epoch 12  14.1% | batch:         9 of        64\t|\tloss: 3683.72\n",
      "Training Epoch 12  15.6% | batch:        10 of        64\t|\tloss: 1680.65\n",
      "Training Epoch 12  17.2% | batch:        11 of        64\t|\tloss: 2443.94\n",
      "Training Epoch 12  18.8% | batch:        12 of        64\t|\tloss: 2319.95\n",
      "Training Epoch 12  20.3% | batch:        13 of        64\t|\tloss: 5261.04\n",
      "Training Epoch 12  21.9% | batch:        14 of        64\t|\tloss: 3027.1\n",
      "Training Epoch 12  23.4% | batch:        15 of        64\t|\tloss: 3424.1\n",
      "Training Epoch 12  25.0% | batch:        16 of        64\t|\tloss: 2558.17\n",
      "Training Epoch 12  26.6% | batch:        17 of        64\t|\tloss: 2818.8\n",
      "Training Epoch 12  28.1% | batch:        18 of        64\t|\tloss: 2902.24\n",
      "Training Epoch 12  29.7% | batch:        19 of        64\t|\tloss: 2081.21\n",
      "Training Epoch 12  31.2% | batch:        20 of        64\t|\tloss: 2215.64\n",
      "Training Epoch 12  32.8% | batch:        21 of        64\t|\tloss: 3815.58\n",
      "Training Epoch 12  34.4% | batch:        22 of        64\t|\tloss: 1560.7\n",
      "Training Epoch 12  35.9% | batch:        23 of        64\t|\tloss: 2791.84\n",
      "Training Epoch 12  37.5% | batch:        24 of        64\t|\tloss: 4152.59\n",
      "Training Epoch 12  39.1% | batch:        25 of        64\t|\tloss: 1569.7\n",
      "Training Epoch 12  40.6% | batch:        26 of        64\t|\tloss: 1473.89\n",
      "Training Epoch 12  42.2% | batch:        27 of        64\t|\tloss: 3504.01\n",
      "Training Epoch 12  43.8% | batch:        28 of        64\t|\tloss: 4281.34\n",
      "Training Epoch 12  45.3% | batch:        29 of        64\t|\tloss: 2446.11\n",
      "Training Epoch 12  46.9% | batch:        30 of        64\t|\tloss: 5289.04\n",
      "Training Epoch 12  48.4% | batch:        31 of        64\t|\tloss: 3101.63\n",
      "Training Epoch 12  50.0% | batch:        32 of        64\t|\tloss: 2883.71\n",
      "Training Epoch 12  51.6% | batch:        33 of        64\t|\tloss: 1900.7\n",
      "Training Epoch 12  53.1% | batch:        34 of        64\t|\tloss: 4528.89\n",
      "Training Epoch 12  54.7% | batch:        35 of        64\t|\tloss: 4040.34\n",
      "Training Epoch 12  56.2% | batch:        36 of        64\t|\tloss: 1688.76\n",
      "Training Epoch 12  57.8% | batch:        37 of        64\t|\tloss: 1076.57\n",
      "Training Epoch 12  59.4% | batch:        38 of        64\t|\tloss: 10390\n",
      "Training Epoch 12  60.9% | batch:        39 of        64\t|\tloss: 3468.74\n",
      "Training Epoch 12  62.5% | batch:        40 of        64\t|\tloss: 2080.65\n",
      "Training Epoch 12  64.1% | batch:        41 of        64\t|\tloss: 1761.68\n",
      "Training Epoch 12  65.6% | batch:        42 of        64\t|\tloss: 1483.26\n",
      "Training Epoch 12  67.2% | batch:        43 of        64\t|\tloss: 2637.45\n",
      "Training Epoch 12  68.8% | batch:        44 of        64\t|\tloss: 2675.78\n",
      "Training Epoch 12  70.3% | batch:        45 of        64\t|\tloss: 1559.28\n",
      "Training Epoch 12  71.9% | batch:        46 of        64\t|\tloss: 2054.91\n",
      "Training Epoch 12  73.4% | batch:        47 of        64\t|\tloss: 1480.22\n",
      "Training Epoch 12  75.0% | batch:        48 of        64\t|\tloss: 712.194\n",
      "Training Epoch 12  76.6% | batch:        49 of        64\t|\tloss: 1772.16\n",
      "Training Epoch 12  78.1% | batch:        50 of        64\t|\tloss: 2333.29\n",
      "Training Epoch 12  79.7% | batch:        51 of        64\t|\tloss: 5470.71\n",
      "Training Epoch 12  81.2% | batch:        52 of        64\t|\tloss: 6011.78\n",
      "Training Epoch 12  82.8% | batch:        53 of        64\t|\tloss: 1014.75\n",
      "Training Epoch 12  84.4% | batch:        54 of        64\t|\tloss: 2084.46\n",
      "Training Epoch 12  85.9% | batch:        55 of        64\t|\tloss: 1619.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:10,783 | INFO : Epoch 12 Training Summary: epoch: 12.000000 | loss: 2867.329093 | \n",
      "2023-05-10 17:08:10,784 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2682249546051025 seconds\n",
      "\n",
      "2023-05-10 17:08:10,784 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2424575885136921 seconds\n",
      "2023-05-10 17:08:10,785 | INFO : Avg batch train. time: 0.01941339982052644 seconds\n",
      "2023-05-10 17:08:10,785 | INFO : Avg sample train. time: 0.00030769132949819025 seconds\n",
      "2023-05-10 17:08:10,785 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  87.5% | batch:        56 of        64\t|\tloss: 2055.7\n",
      "Training Epoch 12  89.1% | batch:        57 of        64\t|\tloss: 1265.71\n",
      "Training Epoch 12  90.6% | batch:        58 of        64\t|\tloss: 4491.05\n",
      "Training Epoch 12  92.2% | batch:        59 of        64\t|\tloss: 3796.16\n",
      "Training Epoch 12  93.8% | batch:        60 of        64\t|\tloss: 1587.44\n",
      "Training Epoch 12  95.3% | batch:        61 of        64\t|\tloss: 4101.21\n",
      "Training Epoch 12  96.9% | batch:        62 of        64\t|\tloss: 3431.22\n",
      "Training Epoch 12  98.4% | batch:        63 of        64\t|\tloss: 3078.53\n",
      "\n",
      "Evaluating Epoch 12   0.0% | batch:         0 of        16\t|\tloss: 1403.28\n",
      "Evaluating Epoch 12   6.2% | batch:         1 of        16\t|\tloss: 1221.16\n",
      "Evaluating Epoch 12  12.5% | batch:         2 of        16\t|\tloss: 1098.52\n",
      "Evaluating Epoch 12  18.8% | batch:         3 of        16\t|\tloss: 1433.67\n",
      "Evaluating Epoch 12  25.0% | batch:         4 of        16\t|\tloss: 1694.5\n",
      "Evaluating Epoch 12  31.2% | batch:         5 of        16\t|\tloss: 2622.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:10,943 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.15769171714782715 seconds\n",
      "\n",
      "2023-05-10 17:08:10,944 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.2336556315422058 seconds\n",
      "2023-05-10 17:08:10,944 | INFO : Avg batch val. time: 0.014603476971387863 seconds\n",
      "2023-05-10 17:08:10,945 | INFO : Avg sample val. time: 0.00023134220944772852 seconds\n",
      "2023-05-10 17:08:10,946 | INFO : Epoch 12 Validation Summary: epoch: 12.000000 | loss: 2304.695645 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 12  37.5% | batch:         6 of        16\t|\tloss: 2724.12\n",
      "Evaluating Epoch 12  43.8% | batch:         7 of        16\t|\tloss: 2607.47\n",
      "Evaluating Epoch 12  50.0% | batch:         8 of        16\t|\tloss: 1954.76\n",
      "Evaluating Epoch 12  56.2% | batch:         9 of        16\t|\tloss: 7623.82\n",
      "Evaluating Epoch 12  62.5% | batch:        10 of        16\t|\tloss: 1760.95\n",
      "Evaluating Epoch 12  68.8% | batch:        11 of        16\t|\tloss: 3337.54\n",
      "Evaluating Epoch 12  75.0% | batch:        12 of        16\t|\tloss: 989.708\n",
      "Evaluating Epoch 12  81.2% | batch:        13 of        16\t|\tloss: 918.177\n",
      "Evaluating Epoch 12  87.5% | batch:        14 of        16\t|\tloss: 2280.74\n",
      "Evaluating Epoch 12  93.8% | batch:        15 of        16\t|\tloss: 3455.74\n",
      "\n",
      "Training Epoch 13   0.0% | batch:         0 of        64\t|\tloss: 2990.61\n",
      "Training Epoch 13   1.6% | batch:         1 of        64\t|\tloss: 3401.51\n",
      "Training Epoch 13   3.1% | batch:         2 of        64\t|\tloss: 2454.46\n",
      "Training Epoch 13   4.7% | batch:         3 of        64\t|\tloss: 4522.86\n",
      "Training Epoch 13   6.2% | batch:         4 of        64\t|\tloss: 1942.51\n",
      "Training Epoch 13   7.8% | batch:         5 of        64\t|\tloss: 3688.56\n",
      "Training Epoch 13   9.4% | batch:         6 of        64\t|\tloss: 2225.87\n",
      "Training Epoch 13  10.9% | batch:         7 of        64\t|\tloss: 1436.72\n",
      "Training Epoch 13  12.5% | batch:         8 of        64\t|\tloss: 1458.17\n",
      "Training Epoch 13  14.1% | batch:         9 of        64\t|\tloss: 2363.54\n",
      "Training Epoch 13  15.6% | batch:        10 of        64\t|\tloss: 2167.9\n",
      "Training Epoch 13  17.2% | batch:        11 of        64\t|\tloss: 3170.2\n",
      "Training Epoch 13  18.8% | batch:        12 of        64\t|\tloss: 2109.78\n",
      "Training Epoch 13  20.3% | batch:        13 of        64\t|\tloss: 4070.71\n",
      "Training Epoch 13  21.9% | batch:        14 of        64\t|\tloss: 1640.73\n",
      "Training Epoch 13  23.4% | batch:        15 of        64\t|\tloss: 2672\n",
      "Training Epoch 13  25.0% | batch:        16 of        64\t|\tloss: 2750.45\n",
      "Training Epoch 13  26.6% | batch:        17 of        64\t|\tloss: 3775.41\n",
      "Training Epoch 13  28.1% | batch:        18 of        64\t|\tloss: 2626.64\n",
      "Training Epoch 13  29.7% | batch:        19 of        64\t|\tloss: 2794.31\n",
      "Training Epoch 13  31.2% | batch:        20 of        64\t|\tloss: 1442.64\n",
      "Training Epoch 13  32.8% | batch:        21 of        64\t|\tloss: 1356.97\n",
      "Training Epoch 13  34.4% | batch:        22 of        64\t|\tloss: 3026.62\n",
      "Training Epoch 13  35.9% | batch:        23 of        64\t|\tloss: 4957.78\n",
      "Training Epoch 13  37.5% | batch:        24 of        64\t|\tloss: 3126.49\n",
      "Training Epoch 13  39.1% | batch:        25 of        64\t|\tloss: 3761.45\n",
      "Training Epoch 13  40.6% | batch:        26 of        64\t|\tloss: 2507.31\n",
      "Training Epoch 13  42.2% | batch:        27 of        64\t|\tloss: 2675.26\n",
      "Training Epoch 13  43.8% | batch:        28 of        64\t|\tloss: 3099.08\n",
      "Training Epoch 13  45.3% | batch:        29 of        64\t|\tloss: 2696.85\n",
      "Training Epoch 13  46.9% | batch:        30 of        64\t|\tloss: 1682.67\n",
      "Training Epoch 13  48.4% | batch:        31 of        64\t|\tloss: 1436.2\n",
      "Training Epoch 13  50.0% | batch:        32 of        64\t|\tloss: 2548.61\n",
      "Training Epoch 13  51.6% | batch:        33 of        64\t|\tloss: 3524.08\n",
      "Training Epoch 13  53.1% | batch:        34 of        64\t|\tloss: 2255.26\n",
      "Training Epoch 13  54.7% | batch:        35 of        64\t|\tloss: 2196.15\n",
      "Training Epoch 13  56.2% | batch:        36 of        64\t|\tloss: 1622.29\n",
      "Training Epoch 13  57.8% | batch:        37 of        64\t|\tloss: 1858.75\n",
      "Training Epoch 13  59.4% | batch:        38 of        64\t|\tloss: 2346.05\n",
      "Training Epoch 13  60.9% | batch:        39 of        64\t|\tloss: 3187.58\n",
      "Training Epoch 13  62.5% | batch:        40 of        64\t|\tloss: 2522.5\n",
      "Training Epoch 13  64.1% | batch:        41 of        64\t|\tloss: 3517.16\n",
      "Training Epoch 13  65.6% | batch:        42 of        64\t|\tloss: 1879.76\n",
      "Training Epoch 13  67.2% | batch:        43 of        64\t|\tloss: 1841.7\n",
      "Training Epoch 13  68.8% | batch:        44 of        64\t|\tloss: 2224.67\n",
      "Training Epoch 13  70.3% | batch:        45 of        64\t|\tloss: 6899.49\n",
      "Training Epoch 13  71.9% | batch:        46 of        64\t|\tloss: 2020.08\n",
      "Training Epoch 13  73.4% | batch:        47 of        64\t|\tloss: 874.106\n",
      "Training Epoch 13  75.0% | batch:        48 of        64\t|\tloss: 1799.8\n",
      "Training Epoch 13  76.6% | batch:        49 of        64\t|\tloss: 2507.27\n",
      "Training Epoch 13  78.1% | batch:        50 of        64\t|\tloss: 2447.9\n",
      "Training Epoch 13  79.7% | batch:        51 of        64\t|\tloss: 3926.08\n",
      "Training Epoch 13  81.2% | batch:        52 of        64\t|\tloss: 4066.01\n",
      "Training Epoch 13  82.8% | batch:        53 of        64\t|\tloss: 2009.07\n",
      "Training Epoch 13  84.4% | batch:        54 of        64\t|\tloss: 2939.57\n",
      "Training Epoch 13  85.9% | batch:        55 of        64\t|\tloss: 2480.13\n",
      "Training Epoch 13  87.5% | batch:        56 of        64\t|\tloss: 3359.38\n",
      "Training Epoch 13  89.1% | batch:        57 of        64\t|\tloss: 2288.72\n",
      "Training Epoch 13  90.6% | batch:        58 of        64\t|\tloss: 2521.21\n",
      "Training Epoch 13  92.2% | batch:        59 of        64\t|\tloss: 1783.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:12,172 | INFO : Epoch 13 Training Summary: epoch: 13.000000 | loss: 2670.147602 | \n",
      "2023-05-10 17:08:12,173 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2117857933044434 seconds\n",
      "\n",
      "2023-05-10 17:08:12,173 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2400982196514423 seconds\n",
      "2023-05-10 17:08:12,174 | INFO : Avg batch train. time: 0.019376534682053786 seconds\n",
      "2023-05-10 17:08:12,174 | INFO : Avg sample train. time: 0.00030710703805137253 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 13  93.8% | batch:        60 of        64\t|\tloss: 3365.02\n",
      "Training Epoch 13  95.3% | batch:        61 of        64\t|\tloss: 3531.89\n",
      "Training Epoch 13  96.9% | batch:        62 of        64\t|\tloss: 1774.92\n",
      "Training Epoch 13  98.4% | batch:        63 of        64\t|\tloss: 3401.5\n",
      "\n",
      "Training Epoch 14   0.0% | batch:         0 of        64\t|\tloss: 1983.05\n",
      "Training Epoch 14   1.6% | batch:         1 of        64\t|\tloss: 3008.63\n",
      "Training Epoch 14   3.1% | batch:         2 of        64\t|\tloss: 1150.79\n",
      "Training Epoch 14   4.7% | batch:         3 of        64\t|\tloss: 1412.83\n",
      "Training Epoch 14   6.2% | batch:         4 of        64\t|\tloss: 1900.54\n",
      "Training Epoch 14   7.8% | batch:         5 of        64\t|\tloss: 2435.08\n",
      "Training Epoch 14   9.4% | batch:         6 of        64\t|\tloss: 2101.27\n",
      "Training Epoch 14  10.9% | batch:         7 of        64\t|\tloss: 2367.1\n",
      "Training Epoch 14  12.5% | batch:         8 of        64\t|\tloss: 7422.16\n",
      "Training Epoch 14  14.1% | batch:         9 of        64\t|\tloss: 1440.67\n",
      "Training Epoch 14  15.6% | batch:        10 of        64\t|\tloss: 1181.94\n",
      "Training Epoch 14  17.2% | batch:        11 of        64\t|\tloss: 1643.17\n",
      "Training Epoch 14  18.8% | batch:        12 of        64\t|\tloss: 3334.29\n",
      "Training Epoch 14  20.3% | batch:        13 of        64\t|\tloss: 1476.72\n",
      "Training Epoch 14  21.9% | batch:        14 of        64\t|\tloss: 2459.5\n",
      "Training Epoch 14  23.4% | batch:        15 of        64\t|\tloss: 2336.21\n",
      "Training Epoch 14  25.0% | batch:        16 of        64\t|\tloss: 2894.9\n",
      "Training Epoch 14  26.6% | batch:        17 of        64\t|\tloss: 4262.48\n",
      "Training Epoch 14  28.1% | batch:        18 of        64\t|\tloss: 1296.91\n",
      "Training Epoch 14  29.7% | batch:        19 of        64\t|\tloss: 1675.88\n",
      "Training Epoch 14  31.2% | batch:        20 of        64\t|\tloss: 1544.02\n",
      "Training Epoch 14  32.8% | batch:        21 of        64\t|\tloss: 4411.89\n",
      "Training Epoch 14  34.4% | batch:        22 of        64\t|\tloss: 1596.46\n",
      "Training Epoch 14  35.9% | batch:        23 of        64\t|\tloss: 1787.29\n",
      "Training Epoch 14  37.5% | batch:        24 of        64\t|\tloss: 2095.23\n",
      "Training Epoch 14  39.1% | batch:        25 of        64\t|\tloss: 1445.63\n",
      "Training Epoch 14  40.6% | batch:        26 of        64\t|\tloss: 3423.55\n",
      "Training Epoch 14  42.2% | batch:        27 of        64\t|\tloss: 2484.28\n",
      "Training Epoch 14  43.8% | batch:        28 of        64\t|\tloss: 1702.41\n",
      "Training Epoch 14  45.3% | batch:        29 of        64\t|\tloss: 1192.94\n",
      "Training Epoch 14  46.9% | batch:        30 of        64\t|\tloss: 1672.69\n",
      "Training Epoch 14  48.4% | batch:        31 of        64\t|\tloss: 3149.8\n",
      "Training Epoch 14  50.0% | batch:        32 of        64\t|\tloss: 3193.5\n",
      "Training Epoch 14  51.6% | batch:        33 of        64\t|\tloss: 1600.66\n",
      "Training Epoch 14  53.1% | batch:        34 of        64\t|\tloss: 2436.02\n",
      "Training Epoch 14  54.7% | batch:        35 of        64\t|\tloss: 3246.91\n",
      "Training Epoch 14  56.2% | batch:        36 of        64\t|\tloss: 1812.37\n",
      "Training Epoch 14  57.8% | batch:        37 of        64\t|\tloss: 1742.19\n",
      "Training Epoch 14  59.4% | batch:        38 of        64\t|\tloss: 4208.43\n",
      "Training Epoch 14  60.9% | batch:        39 of        64\t|\tloss: 1473.44\n",
      "Training Epoch 14  62.5% | batch:        40 of        64\t|\tloss: 2308.89\n",
      "Training Epoch 14  64.1% | batch:        41 of        64\t|\tloss: 1860.58\n",
      "Training Epoch 14  65.6% | batch:        42 of        64\t|\tloss: 1164.59\n",
      "Training Epoch 14  67.2% | batch:        43 of        64\t|\tloss: 4085.01\n",
      "Training Epoch 14  68.8% | batch:        44 of        64\t|\tloss: 2227.42\n",
      "Training Epoch 14  70.3% | batch:        45 of        64\t|\tloss: 1874.12\n",
      "Training Epoch 14  71.9% | batch:        46 of        64\t|\tloss: 4737.91\n",
      "Training Epoch 14  73.4% | batch:        47 of        64\t|\tloss: 2286.52\n",
      "Training Epoch 14  75.0% | batch:        48 of        64\t|\tloss: 1892.59\n",
      "Training Epoch 14  76.6% | batch:        49 of        64\t|\tloss: 5927.77\n",
      "Training Epoch 14  78.1% | batch:        50 of        64\t|\tloss: 1443.46\n",
      "Training Epoch 14  79.7% | batch:        51 of        64\t|\tloss: 2624.68\n",
      "Training Epoch 14  81.2% | batch:        52 of        64\t|\tloss: 1312.18\n",
      "Training Epoch 14  82.8% | batch:        53 of        64\t|\tloss: 1339.42\n",
      "Training Epoch 14  84.4% | batch:        54 of        64\t|\tloss: 3253.78\n",
      "Training Epoch 14  85.9% | batch:        55 of        64\t|\tloss: 1156.47\n",
      "Training Epoch 14  87.5% | batch:        56 of        64\t|\tloss: 1423.78\n",
      "Training Epoch 14  89.1% | batch:        57 of        64\t|\tloss: 4984.98\n",
      "Training Epoch 14  90.6% | batch:        58 of        64\t|\tloss: 2123.4\n",
      "Training Epoch 14  92.2% | batch:        59 of        64\t|\tloss: 1650.64\n",
      "Training Epoch 14  93.8% | batch:        60 of        64\t|\tloss: 1412.51\n",
      "Training Epoch 14  95.3% | batch:        61 of        64\t|\tloss: 5101.88\n",
      "Training Epoch 14  96.9% | batch:        62 of        64\t|\tloss: 1356.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:13,401 | INFO : Epoch 14 Training Summary: epoch: 14.000000 | loss: 2405.924593 | \n",
      "2023-05-10 17:08:13,402 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2173738479614258 seconds\n",
      "\n",
      "2023-05-10 17:08:13,403 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2384750502450126 seconds\n",
      "2023-05-10 17:08:13,403 | INFO : Avg batch train. time: 0.01935117266007832 seconds\n",
      "2023-05-10 17:08:13,404 | INFO : Avg sample train. time: 0.00030670506444898776 seconds\n",
      "2023-05-10 17:08:13,404 | INFO : Evaluating on validation set ...\n",
      "2023-05-10 17:08:13,557 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.15228509902954102 seconds\n",
      "\n",
      "2023-05-10 17:08:13,557 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.22461446126302084 seconds\n",
      "2023-05-10 17:08:13,558 | INFO : Avg batch val. time: 0.014038403828938803 seconds\n",
      "2023-05-10 17:08:13,558 | INFO : Avg sample val. time: 0.00022239055570596122 seconds\n",
      "2023-05-10 17:08:13,559 | INFO : Epoch 14 Validation Summary: epoch: 14.000000 | loss: 2577.485272 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 14  98.4% | batch:        63 of        64\t|\tloss: 2628.27\n",
      "\n",
      "Evaluating Epoch 14   0.0% | batch:         0 of        16\t|\tloss: 1897.25\n",
      "Evaluating Epoch 14   6.2% | batch:         1 of        16\t|\tloss: 1565.96\n",
      "Evaluating Epoch 14  12.5% | batch:         2 of        16\t|\tloss: 1443.52\n",
      "Evaluating Epoch 14  18.8% | batch:         3 of        16\t|\tloss: 2030.74\n",
      "Evaluating Epoch 14  25.0% | batch:         4 of        16\t|\tloss: 1802.12\n",
      "Evaluating Epoch 14  31.2% | batch:         5 of        16\t|\tloss: 2611.59\n",
      "Evaluating Epoch 14  37.5% | batch:         6 of        16\t|\tloss: 3202.96\n",
      "Evaluating Epoch 14  43.8% | batch:         7 of        16\t|\tloss: 4498.42\n",
      "Evaluating Epoch 14  50.0% | batch:         8 of        16\t|\tloss: 2142.12\n",
      "Evaluating Epoch 14  56.2% | batch:         9 of        16\t|\tloss: 6021.07\n",
      "Evaluating Epoch 14  62.5% | batch:        10 of        16\t|\tloss: 2178.87\n",
      "Evaluating Epoch 14  68.8% | batch:        11 of        16\t|\tloss: 3725.21\n",
      "Evaluating Epoch 14  75.0% | batch:        12 of        16\t|\tloss: 1053.61\n",
      "Evaluating Epoch 14  81.2% | batch:        13 of        16\t|\tloss: 1058.6\n",
      "Evaluating Epoch 14  87.5% | batch:        14 of        16\t|\tloss: 2558.29\n",
      "Evaluating Epoch 14  93.8% | batch:        15 of        16\t|\tloss: 3693.56\n",
      "\n",
      "Training Epoch 15   0.0% | batch:         0 of        64\t|\tloss: 2731.29\n",
      "Training Epoch 15   1.6% | batch:         1 of        64\t|\tloss: 3071.28\n",
      "Training Epoch 15   3.1% | batch:         2 of        64\t|\tloss: 2584.27\n",
      "Training Epoch 15   4.7% | batch:         3 of        64\t|\tloss: 2130.9\n",
      "Training Epoch 15   6.2% | batch:         4 of        64\t|\tloss: 1980.58\n",
      "Training Epoch 15   7.8% | batch:         5 of        64\t|\tloss: 1361.33\n",
      "Training Epoch 15   9.4% | batch:         6 of        64\t|\tloss: 2007.06\n",
      "Training Epoch 15  10.9% | batch:         7 of        64\t|\tloss: 6160.97\n",
      "Training Epoch 15  12.5% | batch:         8 of        64\t|\tloss: 1571.71\n",
      "Training Epoch 15  14.1% | batch:         9 of        64\t|\tloss: 3078.59\n",
      "Training Epoch 15  15.6% | batch:        10 of        64\t|\tloss: 2606.51\n",
      "Training Epoch 15  17.2% | batch:        11 of        64\t|\tloss: 2018.85\n",
      "Training Epoch 15  18.8% | batch:        12 of        64\t|\tloss: 2421.15\n",
      "Training Epoch 15  20.3% | batch:        13 of        64\t|\tloss: 1849.31\n",
      "Training Epoch 15  21.9% | batch:        14 of        64\t|\tloss: 2836.92\n",
      "Training Epoch 15  23.4% | batch:        15 of        64\t|\tloss: 1470.44\n",
      "Training Epoch 15  25.0% | batch:        16 of        64\t|\tloss: 2465.17\n",
      "Training Epoch 15  26.6% | batch:        17 of        64\t|\tloss: 2361.59\n",
      "Training Epoch 15  28.1% | batch:        18 of        64\t|\tloss: 1682.1\n",
      "Training Epoch 15  29.7% | batch:        19 of        64\t|\tloss: 2046.12\n",
      "Training Epoch 15  31.2% | batch:        20 of        64\t|\tloss: 2333.45\n",
      "Training Epoch 15  32.8% | batch:        21 of        64\t|\tloss: 1260.23\n",
      "Training Epoch 15  34.4% | batch:        22 of        64\t|\tloss: 1234.65\n",
      "Training Epoch 15  35.9% | batch:        23 of        64\t|\tloss: 1812.09\n",
      "Training Epoch 15  37.5% | batch:        24 of        64\t|\tloss: 1883\n",
      "Training Epoch 15  39.1% | batch:        25 of        64\t|\tloss: 2028.86\n",
      "Training Epoch 15  40.6% | batch:        26 of        64\t|\tloss: 1858.77\n",
      "Training Epoch 15  42.2% | batch:        27 of        64\t|\tloss: 2126.88\n",
      "Training Epoch 15  43.8% | batch:        28 of        64\t|\tloss: 1528.93\n",
      "Training Epoch 15  45.3% | batch:        29 of        64\t|\tloss: 3163\n",
      "Training Epoch 15  46.9% | batch:        30 of        64\t|\tloss: 2590.03\n",
      "Training Epoch 15  48.4% | batch:        31 of        64\t|\tloss: 1359.07\n",
      "Training Epoch 15  50.0% | batch:        32 of        64\t|\tloss: 1684.47\n",
      "Training Epoch 15  51.6% | batch:        33 of        64\t|\tloss: 2269.9\n",
      "Training Epoch 15  53.1% | batch:        34 of        64\t|\tloss: 4322.43\n",
      "Training Epoch 15  54.7% | batch:        35 of        64\t|\tloss: 2019.43\n",
      "Training Epoch 15  56.2% | batch:        36 of        64\t|\tloss: 1565.09\n",
      "Training Epoch 15  57.8% | batch:        37 of        64\t|\tloss: 1963.9\n",
      "Training Epoch 15  59.4% | batch:        38 of        64\t|\tloss: 3764.6\n",
      "Training Epoch 15  60.9% | batch:        39 of        64\t|\tloss: 2654.67\n",
      "Training Epoch 15  62.5% | batch:        40 of        64\t|\tloss: 3547.92\n",
      "Training Epoch 15  64.1% | batch:        41 of        64\t|\tloss: 2229.76\n",
      "Training Epoch 15  65.6% | batch:        42 of        64\t|\tloss: 2173.03\n",
      "Training Epoch 15  67.2% | batch:        43 of        64\t|\tloss: 2122.56\n",
      "Training Epoch 15  68.8% | batch:        44 of        64\t|\tloss: 1374.25\n",
      "Training Epoch 15  70.3% | batch:        45 of        64\t|\tloss: 3128.85\n",
      "Training Epoch 15  71.9% | batch:        46 of        64\t|\tloss: 1932.97\n",
      "Training Epoch 15  73.4% | batch:        47 of        64\t|\tloss: 1449.68\n",
      "Training Epoch 15  75.0% | batch:        48 of        64\t|\tloss: 3479.22\n",
      "Training Epoch 15  76.6% | batch:        49 of        64\t|\tloss: 1856.04\n",
      "Training Epoch 15  78.1% | batch:        50 of        64\t|\tloss: 4368.4\n",
      "Training Epoch 15  79.7% | batch:        51 of        64\t|\tloss: 1995.64\n",
      "Training Epoch 15  81.2% | batch:        52 of        64\t|\tloss: 1758.74\n",
      "Training Epoch 15  82.8% | batch:        53 of        64\t|\tloss: 1623.48\n",
      "Training Epoch 15  84.4% | batch:        54 of        64\t|\tloss: 2650.53\n",
      "Training Epoch 15  85.9% | batch:        55 of        64\t|\tloss: 1724.23\n",
      "Training Epoch 15  87.5% | batch:        56 of        64\t|\tloss: 1486.53\n",
      "Training Epoch 15  89.1% | batch:        57 of        64\t|\tloss: 1147.9\n",
      "Training Epoch 15  90.6% | batch:        58 of        64\t|\tloss: 2346.33\n",
      "Training Epoch 15  92.2% | batch:        59 of        64\t|\tloss: 1761.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:14,699 | INFO : Epoch 15 Training Summary: epoch: 15.000000 | loss: 2258.791944 | \n",
      "2023-05-10 17:08:14,699 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1289503574371338 seconds\n",
      "\n",
      "2023-05-10 17:08:14,700 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2311734040578206 seconds\n",
      "2023-05-10 17:08:14,700 | INFO : Avg batch train. time: 0.019237084438403447 seconds\n",
      "2023-05-10 17:08:14,700 | INFO : Avg sample train. time: 0.0003048968311188263 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 15  93.8% | batch:        60 of        64\t|\tloss: 1885.65\n",
      "Training Epoch 15  95.3% | batch:        61 of        64\t|\tloss: 1847.52\n",
      "Training Epoch 15  96.9% | batch:        62 of        64\t|\tloss: 1927.29\n",
      "Training Epoch 15  98.4% | batch:        63 of        64\t|\tloss: 8940.65\n",
      "\n",
      "Training Epoch 16   0.0% | batch:         0 of        64\t|\tloss: 3155.23\n",
      "Training Epoch 16   1.6% | batch:         1 of        64\t|\tloss: 1716.54\n",
      "Training Epoch 16   3.1% | batch:         2 of        64\t|\tloss: 2014.96\n",
      "Training Epoch 16   4.7% | batch:         3 of        64\t|\tloss: 2263.39\n",
      "Training Epoch 16   6.2% | batch:         4 of        64\t|\tloss: 2231.97\n",
      "Training Epoch 16   7.8% | batch:         5 of        64\t|\tloss: 5650.73\n",
      "Training Epoch 16   9.4% | batch:         6 of        64\t|\tloss: 1931.04\n",
      "Training Epoch 16  10.9% | batch:         7 of        64\t|\tloss: 1937.49\n",
      "Training Epoch 16  12.5% | batch:         8 of        64\t|\tloss: 1797.15\n",
      "Training Epoch 16  14.1% | batch:         9 of        64\t|\tloss: 1632.7\n",
      "Training Epoch 16  15.6% | batch:        10 of        64\t|\tloss: 2007.01\n",
      "Training Epoch 16  17.2% | batch:        11 of        64\t|\tloss: 1754.95\n",
      "Training Epoch 16  18.8% | batch:        12 of        64\t|\tloss: 3907.2\n",
      "Training Epoch 16  20.3% | batch:        13 of        64\t|\tloss: 3195.98\n",
      "Training Epoch 16  21.9% | batch:        14 of        64\t|\tloss: 2179.98\n",
      "Training Epoch 16  23.4% | batch:        15 of        64\t|\tloss: 5016.18\n",
      "Training Epoch 16  25.0% | batch:        16 of        64\t|\tloss: 1280.38\n",
      "Training Epoch 16  26.6% | batch:        17 of        64\t|\tloss: 1243.82\n",
      "Training Epoch 16  28.1% | batch:        18 of        64\t|\tloss: 1376.35\n",
      "Training Epoch 16  29.7% | batch:        19 of        64\t|\tloss: 2266.6\n",
      "Training Epoch 16  31.2% | batch:        20 of        64\t|\tloss: 2179.47\n",
      "Training Epoch 16  32.8% | batch:        21 of        64\t|\tloss: 1415.63\n",
      "Training Epoch 16  34.4% | batch:        22 of        64\t|\tloss: 3612.03\n",
      "Training Epoch 16  35.9% | batch:        23 of        64\t|\tloss: 3987.15\n",
      "Training Epoch 16  37.5% | batch:        24 of        64\t|\tloss: 1050.38\n",
      "Training Epoch 16  39.1% | batch:        25 of        64\t|\tloss: 2204.58\n",
      "Training Epoch 16  40.6% | batch:        26 of        64\t|\tloss: 1849.97\n",
      "Training Epoch 16  42.2% | batch:        27 of        64\t|\tloss: 2893.33\n",
      "Training Epoch 16  43.8% | batch:        28 of        64\t|\tloss: 2038.21\n",
      "Training Epoch 16  45.3% | batch:        29 of        64\t|\tloss: 1238.78\n",
      "Training Epoch 16  46.9% | batch:        30 of        64\t|\tloss: 1675.91\n",
      "Training Epoch 16  48.4% | batch:        31 of        64\t|\tloss: 3704.56\n",
      "Training Epoch 16  50.0% | batch:        32 of        64\t|\tloss: 2038.75\n",
      "Training Epoch 16  51.6% | batch:        33 of        64\t|\tloss: 2692.7\n",
      "Training Epoch 16  53.1% | batch:        34 of        64\t|\tloss: 1645.4\n",
      "Training Epoch 16  54.7% | batch:        35 of        64\t|\tloss: 1180.32\n",
      "Training Epoch 16  56.2% | batch:        36 of        64\t|\tloss: 1357.04\n",
      "Training Epoch 16  57.8% | batch:        37 of        64\t|\tloss: 2747.62\n",
      "Training Epoch 16  59.4% | batch:        38 of        64\t|\tloss: 1575.57\n",
      "Training Epoch 16  60.9% | batch:        39 of        64\t|\tloss: 2726.07\n",
      "Training Epoch 16  62.5% | batch:        40 of        64\t|\tloss: 1594.8\n",
      "Training Epoch 16  64.1% | batch:        41 of        64\t|\tloss: 2046.37\n",
      "Training Epoch 16  65.6% | batch:        42 of        64\t|\tloss: 2362.74\n",
      "Training Epoch 16  67.2% | batch:        43 of        64\t|\tloss: 2348.86\n",
      "Training Epoch 16  68.8% | batch:        44 of        64\t|\tloss: 1471.25\n",
      "Training Epoch 16  70.3% | batch:        45 of        64\t|\tloss: 4270.32\n",
      "Training Epoch 16  71.9% | batch:        46 of        64\t|\tloss: 1659.34\n",
      "Training Epoch 16  73.4% | batch:        47 of        64\t|\tloss: 2670.1\n",
      "Training Epoch 16  75.0% | batch:        48 of        64\t|\tloss: 1478.6\n",
      "Training Epoch 16  76.6% | batch:        49 of        64\t|\tloss: 2226.55\n",
      "Training Epoch 16  78.1% | batch:        50 of        64\t|\tloss: 2440.44\n",
      "Training Epoch 16  79.7% | batch:        51 of        64\t|\tloss: 2400.82\n",
      "Training Epoch 16  81.2% | batch:        52 of        64\t|\tloss: 1155.81\n",
      "Training Epoch 16  82.8% | batch:        53 of        64\t|\tloss: 1102.45\n",
      "Training Epoch 16  84.4% | batch:        54 of        64\t|\tloss: 3573.84\n",
      "Training Epoch 16  85.9% | batch:        55 of        64\t|\tloss: 1929.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:15,812 | INFO : Epoch 16 Training Summary: epoch: 16.000000 | loss: 2261.107021 | \n",
      "2023-05-10 17:08:15,813 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1028921604156494 seconds\n",
      "\n",
      "2023-05-10 17:08:15,813 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.223155826330185 seconds\n",
      "2023-05-10 17:08:15,814 | INFO : Avg batch train. time: 0.01911180978640914 seconds\n",
      "2023-05-10 17:08:15,814 | INFO : Avg sample train. time: 0.00030291129923976844 seconds\n",
      "2023-05-10 17:08:15,815 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 16  87.5% | batch:        56 of        64\t|\tloss: 3630.59\n",
      "Training Epoch 16  89.1% | batch:        57 of        64\t|\tloss: 1467.02\n",
      "Training Epoch 16  90.6% | batch:        58 of        64\t|\tloss: 2510.08\n",
      "Training Epoch 16  92.2% | batch:        59 of        64\t|\tloss: 1265.41\n",
      "Training Epoch 16  93.8% | batch:        60 of        64\t|\tloss: 1616.82\n",
      "Training Epoch 16  95.3% | batch:        61 of        64\t|\tloss: 3319.08\n",
      "Training Epoch 16  96.9% | batch:        62 of        64\t|\tloss: 1652.93\n",
      "Training Epoch 16  98.4% | batch:        63 of        64\t|\tloss: 1014.39\n",
      "\n",
      "Evaluating Epoch 16   0.0% | batch:         0 of        16\t|\tloss: 1384.1\n",
      "Evaluating Epoch 16   6.2% | batch:         1 of        16\t|\tloss: 1296.69\n",
      "Evaluating Epoch 16  12.5% | batch:         2 of        16\t|\tloss: 1296.97\n",
      "Evaluating Epoch 16  18.8% | batch:         3 of        16\t|\tloss: 1822.14\n",
      "Evaluating Epoch 16  25.0% | batch:         4 of        16\t|\tloss: 1358.88\n",
      "Evaluating Epoch 16  31.2% | batch:         5 of        16\t|\tloss: 2356.48\n",
      "Evaluating Epoch 16  37.5% | batch:         6 of        16\t|\tloss: 2868.51\n",
      "Evaluating Epoch 16  43.8% | batch:         7 of        16\t|\tloss: 1962.63\n",
      "Evaluating Epoch 16  50.0% | batch:         8 of        16\t|\tloss: 1841.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:15,973 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.15804624557495117 seconds\n",
      "\n",
      "2023-05-10 17:08:15,974 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.21795763969421386 seconds\n",
      "2023-05-10 17:08:15,974 | INFO : Avg batch val. time: 0.013622352480888366 seconds\n",
      "2023-05-10 17:08:15,975 | INFO : Avg sample val. time: 0.00021579964326159788 seconds\n",
      "2023-05-10 17:08:15,975 | INFO : Epoch 16 Validation Summary: epoch: 16.000000 | loss: 2005.542713 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 16  56.2% | batch:         9 of        16\t|\tloss: 5076.9\n",
      "Evaluating Epoch 16  62.5% | batch:        10 of        16\t|\tloss: 1792.88\n",
      "Evaluating Epoch 16  68.8% | batch:        11 of        16\t|\tloss: 2409.18\n",
      "Evaluating Epoch 16  75.0% | batch:        12 of        16\t|\tloss: 905.489\n",
      "Evaluating Epoch 16  81.2% | batch:        13 of        16\t|\tloss: 1170.81\n",
      "Evaluating Epoch 16  87.5% | batch:        14 of        16\t|\tloss: 1935.12\n",
      "Evaluating Epoch 16  93.8% | batch:        15 of        16\t|\tloss: 2779.59\n",
      "\n",
      "Training Epoch 17   0.0% | batch:         0 of        64\t|\tloss: 3042.19\n",
      "Training Epoch 17   1.6% | batch:         1 of        64\t|\tloss: 2310.12\n",
      "Training Epoch 17   3.1% | batch:         2 of        64\t|\tloss: 2474.18\n",
      "Training Epoch 17   4.7% | batch:         3 of        64\t|\tloss: 1501.33\n",
      "Training Epoch 17   6.2% | batch:         4 of        64\t|\tloss: 1126.8\n",
      "Training Epoch 17   7.8% | batch:         5 of        64\t|\tloss: 1038.8\n",
      "Training Epoch 17   9.4% | batch:         6 of        64\t|\tloss: 3402.26\n",
      "Training Epoch 17  10.9% | batch:         7 of        64\t|\tloss: 3977.78\n",
      "Training Epoch 17  12.5% | batch:         8 of        64\t|\tloss: 1460.24\n",
      "Training Epoch 17  14.1% | batch:         9 of        64\t|\tloss: 1316.38\n",
      "Training Epoch 17  15.6% | batch:        10 of        64\t|\tloss: 8468.63\n",
      "Training Epoch 17  17.2% | batch:        11 of        64\t|\tloss: 4565.27\n",
      "Training Epoch 17  18.8% | batch:        12 of        64\t|\tloss: 1555.35\n",
      "Training Epoch 17  20.3% | batch:        13 of        64\t|\tloss: 1345.13\n",
      "Training Epoch 17  21.9% | batch:        14 of        64\t|\tloss: 1252.45\n",
      "Training Epoch 17  23.4% | batch:        15 of        64\t|\tloss: 1224.77\n",
      "Training Epoch 17  25.0% | batch:        16 of        64\t|\tloss: 2916.73\n",
      "Training Epoch 17  26.6% | batch:        17 of        64\t|\tloss: 6349.93\n",
      "Training Epoch 17  28.1% | batch:        18 of        64\t|\tloss: 1955.88\n",
      "Training Epoch 17  29.7% | batch:        19 of        64\t|\tloss: 2628.8\n",
      "Training Epoch 17  31.2% | batch:        20 of        64\t|\tloss: 2062.43\n",
      "Training Epoch 17  32.8% | batch:        21 of        64\t|\tloss: 2172.47\n",
      "Training Epoch 17  34.4% | batch:        22 of        64\t|\tloss: 1628.1\n",
      "Training Epoch 17  35.9% | batch:        23 of        64\t|\tloss: 2513.03\n",
      "Training Epoch 17  37.5% | batch:        24 of        64\t|\tloss: 1823.94\n",
      "Training Epoch 17  39.1% | batch:        25 of        64\t|\tloss: 2122.16\n",
      "Training Epoch 17  40.6% | batch:        26 of        64\t|\tloss: 4266.1\n",
      "Training Epoch 17  42.2% | batch:        27 of        64\t|\tloss: 1159.46\n",
      "Training Epoch 17  43.8% | batch:        28 of        64\t|\tloss: 2587.21\n",
      "Training Epoch 17  45.3% | batch:        29 of        64\t|\tloss: 1708.43\n",
      "Training Epoch 17  46.9% | batch:        30 of        64\t|\tloss: 2041.39\n",
      "Training Epoch 17  48.4% | batch:        31 of        64\t|\tloss: 1680.17\n",
      "Training Epoch 17  50.0% | batch:        32 of        64\t|\tloss: 2013.01\n",
      "Training Epoch 17  51.6% | batch:        33 of        64\t|\tloss: 2761.81\n",
      "Training Epoch 17  53.1% | batch:        34 of        64\t|\tloss: 4579.32\n",
      "Training Epoch 17  54.7% | batch:        35 of        64\t|\tloss: 3300.07\n",
      "Training Epoch 17  56.2% | batch:        36 of        64\t|\tloss: 1577.84\n",
      "Training Epoch 17  57.8% | batch:        37 of        64\t|\tloss: 1304.78\n",
      "Training Epoch 17  59.4% | batch:        38 of        64\t|\tloss: 1961.38\n",
      "Training Epoch 17  60.9% | batch:        39 of        64\t|\tloss: 2424.2\n",
      "Training Epoch 17  62.5% | batch:        40 of        64\t|\tloss: 2389.88\n",
      "Training Epoch 17  64.1% | batch:        41 of        64\t|\tloss: 1589.31\n",
      "Training Epoch 17  65.6% | batch:        42 of        64\t|\tloss: 2682.21\n",
      "Training Epoch 17  67.2% | batch:        43 of        64\t|\tloss: 3715.7\n",
      "Training Epoch 17  68.8% | batch:        44 of        64\t|\tloss: 5789.37\n",
      "Training Epoch 17  70.3% | batch:        45 of        64\t|\tloss: 1779.08\n",
      "Training Epoch 17  71.9% | batch:        46 of        64\t|\tloss: 2541.53\n",
      "Training Epoch 17  73.4% | batch:        47 of        64\t|\tloss: 2009.82\n",
      "Training Epoch 17  75.0% | batch:        48 of        64\t|\tloss: 2513.83\n",
      "Training Epoch 17  76.6% | batch:        49 of        64\t|\tloss: 2316\n",
      "Training Epoch 17  78.1% | batch:        50 of        64\t|\tloss: 2427.68\n",
      "Training Epoch 17  79.7% | batch:        51 of        64\t|\tloss: 1549.63\n",
      "Training Epoch 17  81.2% | batch:        52 of        64\t|\tloss: 6460.23\n",
      "Training Epoch 17  82.8% | batch:        53 of        64\t|\tloss: 1011.3\n",
      "Training Epoch 17  84.4% | batch:        54 of        64\t|\tloss: 1925.13\n",
      "Training Epoch 17  85.9% | batch:        55 of        64\t|\tloss: 2283.07\n",
      "Training Epoch 17  87.5% | batch:        56 of        64\t|\tloss: 2178.35\n",
      "Training Epoch 17  89.1% | batch:        57 of        64\t|\tloss: 1772.72\n",
      "Training Epoch 17  90.6% | batch:        58 of        64\t|\tloss: 1428.4\n",
      "Training Epoch 17  92.2% | batch:        59 of        64\t|\tloss: 1118.1\n",
      "Training Epoch 17  93.8% | batch:        60 of        64\t|\tloss: 1065.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:17,195 | INFO : Epoch 17 Training Summary: epoch: 17.000000 | loss: 2451.669239 | \n",
      "2023-05-10 17:08:17,195 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2054929733276367 seconds\n",
      "\n",
      "2023-05-10 17:08:17,196 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.222116834977094 seconds\n",
      "2023-05-10 17:08:17,196 | INFO : Avg batch train. time: 0.019095575546517092 seconds\n",
      "2023-05-10 17:08:17,197 | INFO : Avg sample train. time: 0.0003026539957843224 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 17  95.3% | batch:        61 of        64\t|\tloss: 2487.2\n",
      "Training Epoch 17  96.9% | batch:        62 of        64\t|\tloss: 1773.02\n",
      "Training Epoch 17  98.4% | batch:        63 of        64\t|\tloss: 2963.57\n",
      "\n",
      "Training Epoch 18   0.0% | batch:         0 of        64\t|\tloss: 1624.46\n",
      "Training Epoch 18   1.6% | batch:         1 of        64\t|\tloss: 1939.44\n",
      "Training Epoch 18   3.1% | batch:         2 of        64\t|\tloss: 3202.36\n",
      "Training Epoch 18   4.7% | batch:         3 of        64\t|\tloss: 2100.09\n",
      "Training Epoch 18   6.2% | batch:         4 of        64\t|\tloss: 1309.49\n",
      "Training Epoch 18   7.8% | batch:         5 of        64\t|\tloss: 2116.94\n",
      "Training Epoch 18   9.4% | batch:         6 of        64\t|\tloss: 1876.52\n",
      "Training Epoch 18  10.9% | batch:         7 of        64\t|\tloss: 1262.62\n",
      "Training Epoch 18  12.5% | batch:         8 of        64\t|\tloss: 1189.07\n",
      "Training Epoch 18  14.1% | batch:         9 of        64\t|\tloss: 3289.4\n",
      "Training Epoch 18  15.6% | batch:        10 of        64\t|\tloss: 4032.35\n",
      "Training Epoch 18  17.2% | batch:        11 of        64\t|\tloss: 2428.14\n",
      "Training Epoch 18  18.8% | batch:        12 of        64\t|\tloss: 1232.38\n",
      "Training Epoch 18  20.3% | batch:        13 of        64\t|\tloss: 6010.5\n",
      "Training Epoch 18  21.9% | batch:        14 of        64\t|\tloss: 2553.74\n",
      "Training Epoch 18  23.4% | batch:        15 of        64\t|\tloss: 1608.7\n",
      "Training Epoch 18  25.0% | batch:        16 of        64\t|\tloss: 1639.89\n",
      "Training Epoch 18  26.6% | batch:        17 of        64\t|\tloss: 3659.64\n",
      "Training Epoch 18  28.1% | batch:        18 of        64\t|\tloss: 5728.83\n",
      "Training Epoch 18  29.7% | batch:        19 of        64\t|\tloss: 1457.32\n",
      "Training Epoch 18  31.2% | batch:        20 of        64\t|\tloss: 1313.45\n",
      "Training Epoch 18  32.8% | batch:        21 of        64\t|\tloss: 2839.63\n",
      "Training Epoch 18  34.4% | batch:        22 of        64\t|\tloss: 4836.39\n",
      "Training Epoch 18  35.9% | batch:        23 of        64\t|\tloss: 2168.84\n",
      "Training Epoch 18  37.5% | batch:        24 of        64\t|\tloss: 3119.77\n",
      "Training Epoch 18  39.1% | batch:        25 of        64\t|\tloss: 2275.27\n",
      "Training Epoch 18  40.6% | batch:        26 of        64\t|\tloss: 3419.03\n",
      "Training Epoch 18  42.2% | batch:        27 of        64\t|\tloss: 1605.02\n",
      "Training Epoch 18  43.8% | batch:        28 of        64\t|\tloss: 3846.29\n",
      "Training Epoch 18  45.3% | batch:        29 of        64\t|\tloss: 1985.71\n",
      "Training Epoch 18  46.9% | batch:        30 of        64\t|\tloss: 1810.58\n",
      "Training Epoch 18  48.4% | batch:        31 of        64\t|\tloss: 3030.36\n",
      "Training Epoch 18  50.0% | batch:        32 of        64\t|\tloss: 1955.74\n",
      "Training Epoch 18  51.6% | batch:        33 of        64\t|\tloss: 1154.24\n",
      "Training Epoch 18  53.1% | batch:        34 of        64\t|\tloss: 886.254\n",
      "Training Epoch 18  54.7% | batch:        35 of        64\t|\tloss: 2196.53\n",
      "Training Epoch 18  56.2% | batch:        36 of        64\t|\tloss: 2082.28\n",
      "Training Epoch 18  57.8% | batch:        37 of        64\t|\tloss: 1709.33\n",
      "Training Epoch 18  59.4% | batch:        38 of        64\t|\tloss: 1699.2\n",
      "Training Epoch 18  60.9% | batch:        39 of        64\t|\tloss: 764.15\n",
      "Training Epoch 18  62.5% | batch:        40 of        64\t|\tloss: 1380.45\n",
      "Training Epoch 18  64.1% | batch:        41 of        64\t|\tloss: 3196.11\n",
      "Training Epoch 18  65.6% | batch:        42 of        64\t|\tloss: 1742.56\n",
      "Training Epoch 18  67.2% | batch:        43 of        64\t|\tloss: 3941.83\n",
      "Training Epoch 18  68.8% | batch:        44 of        64\t|\tloss: 1516.9\n",
      "Training Epoch 18  70.3% | batch:        45 of        64\t|\tloss: 1491.95\n",
      "Training Epoch 18  71.9% | batch:        46 of        64\t|\tloss: 1802.14\n",
      "Training Epoch 18  73.4% | batch:        47 of        64\t|\tloss: 1204.47\n",
      "Training Epoch 18  75.0% | batch:        48 of        64\t|\tloss: 1411.31\n",
      "Training Epoch 18  76.6% | batch:        49 of        64\t|\tloss: 4353.8\n",
      "Training Epoch 18  78.1% | batch:        50 of        64\t|\tloss: 1640.96\n",
      "Training Epoch 18  79.7% | batch:        51 of        64\t|\tloss: 2283.44\n",
      "Training Epoch 18  81.2% | batch:        52 of        64\t|\tloss: 1569.35\n",
      "Training Epoch 18  82.8% | batch:        53 of        64\t|\tloss: 1668.72\n",
      "Training Epoch 18  84.4% | batch:        54 of        64\t|\tloss: 2574.53\n",
      "Training Epoch 18  85.9% | batch:        55 of        64\t|\tloss: 2138.1\n",
      "Training Epoch 18  87.5% | batch:        56 of        64\t|\tloss: 2139.76\n",
      "Training Epoch 18  89.1% | batch:        57 of        64\t|\tloss: 1645.37\n",
      "Training Epoch 18  90.6% | batch:        58 of        64\t|\tloss: 1602.72\n",
      "Training Epoch 18  92.2% | batch:        59 of        64\t|\tloss: 1524.83\n",
      "Training Epoch 18  93.8% | batch:        60 of        64\t|\tloss: 2629.35\n",
      "Training Epoch 18  95.3% | batch:        61 of        64\t|\tloss: 1601.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:18,422 | INFO : Epoch 18 Training Summary: epoch: 18.000000 | loss: 2276.456929 | \n",
      "2023-05-10 17:08:18,423 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2166664600372314 seconds\n",
      "\n",
      "2023-05-10 17:08:18,424 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2218140363693237 seconds\n",
      "2023-05-10 17:08:18,424 | INFO : Avg batch train. time: 0.019090844318270683 seconds\n",
      "2023-05-10 17:08:18,424 | INFO : Avg sample train. time: 0.00030257900851147194 seconds\n",
      "2023-05-10 17:08:18,425 | INFO : Evaluating on validation set ...\n",
      "2023-05-10 17:08:18,571 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14586544036865234 seconds\n",
      "\n",
      "2023-05-10 17:08:18,571 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.2114038033918901 seconds\n",
      "2023-05-10 17:08:18,572 | INFO : Avg batch val. time: 0.013212737711993132 seconds\n",
      "2023-05-10 17:08:18,572 | INFO : Avg sample val. time: 0.00020931069642761397 seconds\n",
      "2023-05-10 17:08:18,573 | INFO : Epoch 18 Validation Summary: epoch: 18.000000 | loss: 2462.833408 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 18  96.9% | batch:        62 of        64\t|\tloss: 1776.59\n",
      "Training Epoch 18  98.4% | batch:        63 of        64\t|\tloss: 19561.9\n",
      "\n",
      "Evaluating Epoch 18   0.0% | batch:         0 of        16\t|\tloss: 1815.72\n",
      "Evaluating Epoch 18   6.2% | batch:         1 of        16\t|\tloss: 1660.25\n",
      "Evaluating Epoch 18  12.5% | batch:         2 of        16\t|\tloss: 1290.22\n",
      "Evaluating Epoch 18  18.8% | batch:         3 of        16\t|\tloss: 2212.26\n",
      "Evaluating Epoch 18  25.0% | batch:         4 of        16\t|\tloss: 1537.8\n",
      "Evaluating Epoch 18  31.2% | batch:         5 of        16\t|\tloss: 2387.19\n",
      "Evaluating Epoch 18  37.5% | batch:         6 of        16\t|\tloss: 3368.29\n",
      "Evaluating Epoch 18  43.8% | batch:         7 of        16\t|\tloss: 2149.09\n",
      "Evaluating Epoch 18  50.0% | batch:         8 of        16\t|\tloss: 1924.32\n",
      "Evaluating Epoch 18  56.2% | batch:         9 of        16\t|\tloss: 6858.48\n",
      "Evaluating Epoch 18  62.5% | batch:        10 of        16\t|\tloss: 2480.19\n",
      "Evaluating Epoch 18  68.8% | batch:        11 of        16\t|\tloss: 2579.5\n",
      "Evaluating Epoch 18  75.0% | batch:        12 of        16\t|\tloss: 769.351\n",
      "Evaluating Epoch 18  81.2% | batch:        13 of        16\t|\tloss: 1576.19\n",
      "Evaluating Epoch 18  87.5% | batch:        14 of        16\t|\tloss: 2600.4\n",
      "Evaluating Epoch 18  93.8% | batch:        15 of        16\t|\tloss: 4681.42\n",
      "\n",
      "Training Epoch 19   0.0% | batch:         0 of        64\t|\tloss: 2004.35\n",
      "Training Epoch 19   1.6% | batch:         1 of        64\t|\tloss: 4539.01\n",
      "Training Epoch 19   3.1% | batch:         2 of        64\t|\tloss: 1923.32\n",
      "Training Epoch 19   4.7% | batch:         3 of        64\t|\tloss: 4020.22\n",
      "Training Epoch 19   6.2% | batch:         4 of        64\t|\tloss: 1099.23\n",
      "Training Epoch 19   7.8% | batch:         5 of        64\t|\tloss: 2492.52\n",
      "Training Epoch 19   9.4% | batch:         6 of        64\t|\tloss: 1662.34\n",
      "Training Epoch 19  10.9% | batch:         7 of        64\t|\tloss: 843.245\n",
      "Training Epoch 19  12.5% | batch:         8 of        64\t|\tloss: 3330.66\n",
      "Training Epoch 19  14.1% | batch:         9 of        64\t|\tloss: 2125.21\n",
      "Training Epoch 19  15.6% | batch:        10 of        64\t|\tloss: 3094.09\n",
      "Training Epoch 19  17.2% | batch:        11 of        64\t|\tloss: 1575.44\n",
      "Training Epoch 19  18.8% | batch:        12 of        64\t|\tloss: 1953.07\n",
      "Training Epoch 19  20.3% | batch:        13 of        64\t|\tloss: 1319.21\n",
      "Training Epoch 19  21.9% | batch:        14 of        64\t|\tloss: 1274.14\n",
      "Training Epoch 19  23.4% | batch:        15 of        64\t|\tloss: 1608.49\n",
      "Training Epoch 19  25.0% | batch:        16 of        64\t|\tloss: 2842.43\n",
      "Training Epoch 19  26.6% | batch:        17 of        64\t|\tloss: 1371.64\n",
      "Training Epoch 19  28.1% | batch:        18 of        64\t|\tloss: 2072.24\n",
      "Training Epoch 19  29.7% | batch:        19 of        64\t|\tloss: 1795.61\n",
      "Training Epoch 19  31.2% | batch:        20 of        64\t|\tloss: 2352.34\n",
      "Training Epoch 19  32.8% | batch:        21 of        64\t|\tloss: 2251.8\n",
      "Training Epoch 19  34.4% | batch:        22 of        64\t|\tloss: 2762.19\n",
      "Training Epoch 19  35.9% | batch:        23 of        64\t|\tloss: 3187.39\n",
      "Training Epoch 19  37.5% | batch:        24 of        64\t|\tloss: 1215.9\n",
      "Training Epoch 19  39.1% | batch:        25 of        64\t|\tloss: 4677.32\n",
      "Training Epoch 19  40.6% | batch:        26 of        64\t|\tloss: 2343.45\n",
      "Training Epoch 19  42.2% | batch:        27 of        64\t|\tloss: 1058.91\n",
      "Training Epoch 19  43.8% | batch:        28 of        64\t|\tloss: 5585.52\n",
      "Training Epoch 19  45.3% | batch:        29 of        64\t|\tloss: 3147.35\n",
      "Training Epoch 19  46.9% | batch:        30 of        64\t|\tloss: 2159.4\n",
      "Training Epoch 19  48.4% | batch:        31 of        64\t|\tloss: 1439.96\n",
      "Training Epoch 19  50.0% | batch:        32 of        64\t|\tloss: 1336.01\n",
      "Training Epoch 19  51.6% | batch:        33 of        64\t|\tloss: 3161.84\n",
      "Training Epoch 19  53.1% | batch:        34 of        64\t|\tloss: 1225.35\n",
      "Training Epoch 19  54.7% | batch:        35 of        64\t|\tloss: 1580.77\n",
      "Training Epoch 19  56.2% | batch:        36 of        64\t|\tloss: 3130.36\n",
      "Training Epoch 19  57.8% | batch:        37 of        64\t|\tloss: 2273.38\n",
      "Training Epoch 19  59.4% | batch:        38 of        64\t|\tloss: 2091.41\n",
      "Training Epoch 19  60.9% | batch:        39 of        64\t|\tloss: 1490.75\n",
      "Training Epoch 19  62.5% | batch:        40 of        64\t|\tloss: 958.416\n",
      "Training Epoch 19  64.1% | batch:        41 of        64\t|\tloss: 2090.54\n",
      "Training Epoch 19  65.6% | batch:        42 of        64\t|\tloss: 1683.41\n",
      "Training Epoch 19  67.2% | batch:        43 of        64\t|\tloss: 2440.74\n",
      "Training Epoch 19  68.8% | batch:        44 of        64\t|\tloss: 1571.21\n",
      "Training Epoch 19  70.3% | batch:        45 of        64\t|\tloss: 1702.25\n",
      "Training Epoch 19  71.9% | batch:        46 of        64\t|\tloss: 2299.58\n",
      "Training Epoch 19  73.4% | batch:        47 of        64\t|\tloss: 2348.86\n",
      "Training Epoch 19  75.0% | batch:        48 of        64\t|\tloss: 3464.58\n",
      "Training Epoch 19  76.6% | batch:        49 of        64\t|\tloss: 2602.89\n",
      "Training Epoch 19  78.1% | batch:        50 of        64\t|\tloss: 1401.34\n",
      "Training Epoch 19  79.7% | batch:        51 of        64\t|\tloss: 971.683\n",
      "Training Epoch 19  81.2% | batch:        52 of        64\t|\tloss: 2712.01\n",
      "Training Epoch 19  82.8% | batch:        53 of        64\t|\tloss: 1715.88\n",
      "Training Epoch 19  84.4% | batch:        54 of        64\t|\tloss: 1173.16\n",
      "Training Epoch 19  85.9% | batch:        55 of        64\t|\tloss: 1627.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:19,786 | INFO : Epoch 19 Training Summary: epoch: 19.000000 | loss: 2281.156174 | \n",
      "2023-05-10 17:08:19,787 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.202955722808838 seconds\n",
      "\n",
      "2023-05-10 17:08:19,787 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2208214935503507 seconds\n",
      "2023-05-10 17:08:19,787 | INFO : Avg batch train. time: 0.01907533583672423 seconds\n",
      "2023-05-10 17:08:19,788 | INFO : Avg sample train. time: 0.0003023332079124197 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 19  87.5% | batch:        56 of        64\t|\tloss: 1361.13\n",
      "Training Epoch 19  89.1% | batch:        57 of        64\t|\tloss: 7365.03\n",
      "Training Epoch 19  90.6% | batch:        58 of        64\t|\tloss: 1169.75\n",
      "Training Epoch 19  92.2% | batch:        59 of        64\t|\tloss: 2742.39\n",
      "Training Epoch 19  93.8% | batch:        60 of        64\t|\tloss: 1497.73\n",
      "Training Epoch 19  95.3% | batch:        61 of        64\t|\tloss: 2617.41\n",
      "Training Epoch 19  96.9% | batch:        62 of        64\t|\tloss: 4788.68\n",
      "Training Epoch 19  98.4% | batch:        63 of        64\t|\tloss: 2160.06\n",
      "\n",
      "Training Epoch 20   0.0% | batch:         0 of        64\t|\tloss: 2629.26\n",
      "Training Epoch 20   1.6% | batch:         1 of        64\t|\tloss: 2755.96\n",
      "Training Epoch 20   3.1% | batch:         2 of        64\t|\tloss: 983.623\n",
      "Training Epoch 20   4.7% | batch:         3 of        64\t|\tloss: 2622.33\n",
      "Training Epoch 20   6.2% | batch:         4 of        64\t|\tloss: 2637.24\n",
      "Training Epoch 20   7.8% | batch:         5 of        64\t|\tloss: 1872.92\n",
      "Training Epoch 20   9.4% | batch:         6 of        64\t|\tloss: 2351.74\n",
      "Training Epoch 20  10.9% | batch:         7 of        64\t|\tloss: 4941.61\n",
      "Training Epoch 20  12.5% | batch:         8 of        64\t|\tloss: 2127.45\n",
      "Training Epoch 20  14.1% | batch:         9 of        64\t|\tloss: 1562.3\n",
      "Training Epoch 20  15.6% | batch:        10 of        64\t|\tloss: 993.56\n",
      "Training Epoch 20  17.2% | batch:        11 of        64\t|\tloss: 1443.68\n",
      "Training Epoch 20  18.8% | batch:        12 of        64\t|\tloss: 1694.02\n",
      "Training Epoch 20  20.3% | batch:        13 of        64\t|\tloss: 2107.3\n",
      "Training Epoch 20  21.9% | batch:        14 of        64\t|\tloss: 1141.36\n",
      "Training Epoch 20  23.4% | batch:        15 of        64\t|\tloss: 3858.03\n",
      "Training Epoch 20  25.0% | batch:        16 of        64\t|\tloss: 2633.85\n",
      "Training Epoch 20  26.6% | batch:        17 of        64\t|\tloss: 2412.65\n",
      "Training Epoch 20  28.1% | batch:        18 of        64\t|\tloss: 2913.39\n",
      "Training Epoch 20  29.7% | batch:        19 of        64\t|\tloss: 2124.19\n",
      "Training Epoch 20  31.2% | batch:        20 of        64\t|\tloss: 1301.95\n",
      "Training Epoch 20  32.8% | batch:        21 of        64\t|\tloss: 1312.92\n",
      "Training Epoch 20  34.4% | batch:        22 of        64\t|\tloss: 1304.84\n",
      "Training Epoch 20  35.9% | batch:        23 of        64\t|\tloss: 1020.42\n",
      "Training Epoch 20  37.5% | batch:        24 of        64\t|\tloss: 4148.03\n",
      "Training Epoch 20  39.1% | batch:        25 of        64\t|\tloss: 2400.68\n",
      "Training Epoch 20  40.6% | batch:        26 of        64\t|\tloss: 3114.83\n",
      "Training Epoch 20  42.2% | batch:        27 of        64\t|\tloss: 1800.02\n",
      "Training Epoch 20  43.8% | batch:        28 of        64\t|\tloss: 1538.79\n",
      "Training Epoch 20  45.3% | batch:        29 of        64\t|\tloss: 999.688\n",
      "Training Epoch 20  46.9% | batch:        30 of        64\t|\tloss: 2607.17\n",
      "Training Epoch 20  48.4% | batch:        31 of        64\t|\tloss: 1534.57\n",
      "Training Epoch 20  50.0% | batch:        32 of        64\t|\tloss: 1652.94\n",
      "Training Epoch 20  51.6% | batch:        33 of        64\t|\tloss: 948.638\n",
      "Training Epoch 20  53.1% | batch:        34 of        64\t|\tloss: 3069.18\n",
      "Training Epoch 20  54.7% | batch:        35 of        64\t|\tloss: 1567.8\n",
      "Training Epoch 20  56.2% | batch:        36 of        64\t|\tloss: 1725.08\n",
      "Training Epoch 20  57.8% | batch:        37 of        64\t|\tloss: 1791.98\n",
      "Training Epoch 20  59.4% | batch:        38 of        64\t|\tloss: 2113.58\n",
      "Training Epoch 20  60.9% | batch:        39 of        64\t|\tloss: 2636.22\n",
      "Training Epoch 20  62.5% | batch:        40 of        64\t|\tloss: 2724.04\n",
      "Training Epoch 20  64.1% | batch:        41 of        64\t|\tloss: 1772.45\n",
      "Training Epoch 20  65.6% | batch:        42 of        64\t|\tloss: 2874.76\n",
      "Training Epoch 20  67.2% | batch:        43 of        64\t|\tloss: 840.526\n",
      "Training Epoch 20  68.8% | batch:        44 of        64\t|\tloss: 1841.42\n",
      "Training Epoch 20  70.3% | batch:        45 of        64\t|\tloss: 1074\n",
      "Training Epoch 20  71.9% | batch:        46 of        64\t|\tloss: 2713.44\n",
      "Training Epoch 20  73.4% | batch:        47 of        64\t|\tloss: 1152.25\n",
      "Training Epoch 20  75.0% | batch:        48 of        64\t|\tloss: 1985.43\n",
      "Training Epoch 20  76.6% | batch:        49 of        64\t|\tloss: 4176.55\n",
      "Training Epoch 20  78.1% | batch:        50 of        64\t|\tloss: 2016.12\n",
      "Training Epoch 20  79.7% | batch:        51 of        64\t|\tloss: 2731.56\n",
      "Training Epoch 20  81.2% | batch:        52 of        64\t|\tloss: 1583.76\n",
      "Training Epoch 20  82.8% | batch:        53 of        64\t|\tloss: 1497.9\n",
      "Training Epoch 20  84.4% | batch:        54 of        64\t|\tloss: 1885.4\n",
      "Training Epoch 20  85.9% | batch:        55 of        64\t|\tloss: 1496.66\n",
      "Training Epoch 20  87.5% | batch:        56 of        64\t|\tloss: 2707.43\n",
      "Training Epoch 20  89.1% | batch:        57 of        64\t|\tloss: 1355.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:20,994 | INFO : Epoch 20 Training Summary: epoch: 20.000000 | loss: 2108.176661 | \n",
      "2023-05-10 17:08:20,994 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1968450546264648 seconds\n",
      "\n",
      "2023-05-10 17:08:20,995 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2196226716041565 seconds\n",
      "2023-05-10 17:08:20,995 | INFO : Avg batch train. time: 0.019056604243814945 seconds\n",
      "2023-05-10 17:08:20,996 | INFO : Avg sample train. time: 0.000302036322834115 seconds\n",
      "2023-05-10 17:08:20,996 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 20  90.6% | batch:        58 of        64\t|\tloss: 1686.54\n",
      "Training Epoch 20  92.2% | batch:        59 of        64\t|\tloss: 3696.59\n",
      "Training Epoch 20  93.8% | batch:        60 of        64\t|\tloss: 1961.09\n",
      "Training Epoch 20  95.3% | batch:        61 of        64\t|\tloss: 2574.31\n",
      "Training Epoch 20  96.9% | batch:        62 of        64\t|\tloss: 1667.56\n",
      "Training Epoch 20  98.4% | batch:        63 of        64\t|\tloss: 6416.58\n",
      "\n",
      "Evaluating Epoch 20   0.0% | batch:         0 of        16\t|\tloss: 1554.74\n",
      "Evaluating Epoch 20   6.2% | batch:         1 of        16\t|\tloss: 1496.43\n",
      "Evaluating Epoch 20  12.5% | batch:         2 of        16\t|\tloss: 1271.99\n",
      "Evaluating Epoch 20  18.8% | batch:         3 of        16\t|\tloss: 2406.35\n",
      "Evaluating Epoch 20  25.0% | batch:         4 of        16\t|\tloss: 1797.92\n",
      "Evaluating Epoch 20  31.2% | batch:         5 of        16\t|\tloss: 2416.29\n",
      "Evaluating Epoch 20  37.5% | batch:         6 of        16\t|\tloss: 2696.41\n",
      "Evaluating Epoch 20  43.8% | batch:         7 of        16\t|\tloss: 2516.34\n",
      "Evaluating Epoch 20  50.0% | batch:         8 of        16\t|\tloss: 1845.72\n",
      "Evaluating Epoch 20  56.2% | batch:         9 of        16\t|\tloss: 6010.7\n",
      "Evaluating Epoch 20  62.5% | batch:        10 of        16\t|\tloss: 2094.87\n",
      "Evaluating Epoch 20  68.8% | batch:        11 of        16\t|\tloss: 3645.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:21,145 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14886212348937988 seconds\n",
      "\n",
      "2023-05-10 17:08:21,146 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.20619199673334757 seconds\n",
      "2023-05-10 17:08:21,146 | INFO : Avg batch val. time: 0.012886999795834223 seconds\n",
      "2023-05-10 17:08:21,147 | INFO : Avg sample val. time: 0.0002041504918151956 seconds\n",
      "2023-05-10 17:08:21,147 | INFO : Epoch 20 Validation Summary: epoch: 20.000000 | loss: 2329.962798 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 20  75.0% | batch:        12 of        16\t|\tloss: 918.841\n",
      "Evaluating Epoch 20  81.2% | batch:        13 of        16\t|\tloss: 1234.35\n",
      "Evaluating Epoch 20  87.5% | batch:        14 of        16\t|\tloss: 1900.78\n",
      "Evaluating Epoch 20  93.8% | batch:        15 of        16\t|\tloss: 3792.33\n",
      "\n",
      "Training Epoch 21   0.0% | batch:         0 of        64\t|\tloss: 2304.27\n",
      "Training Epoch 21   1.6% | batch:         1 of        64\t|\tloss: 2371.83\n",
      "Training Epoch 21   3.1% | batch:         2 of        64\t|\tloss: 1253.94\n",
      "Training Epoch 21   4.7% | batch:         3 of        64\t|\tloss: 1458.35\n",
      "Training Epoch 21   6.2% | batch:         4 of        64\t|\tloss: 2715.47\n",
      "Training Epoch 21   7.8% | batch:         5 of        64\t|\tloss: 1648.48\n",
      "Training Epoch 21   9.4% | batch:         6 of        64\t|\tloss: 2095\n",
      "Training Epoch 21  10.9% | batch:         7 of        64\t|\tloss: 3174.08\n",
      "Training Epoch 21  12.5% | batch:         8 of        64\t|\tloss: 1556.23\n",
      "Training Epoch 21  14.1% | batch:         9 of        64\t|\tloss: 1417.83\n",
      "Training Epoch 21  15.6% | batch:        10 of        64\t|\tloss: 2302.06\n",
      "Training Epoch 21  17.2% | batch:        11 of        64\t|\tloss: 1741.93\n",
      "Training Epoch 21  18.8% | batch:        12 of        64\t|\tloss: 1409.91\n",
      "Training Epoch 21  20.3% | batch:        13 of        64\t|\tloss: 1154.2\n",
      "Training Epoch 21  21.9% | batch:        14 of        64\t|\tloss: 1098.65\n",
      "Training Epoch 21  23.4% | batch:        15 of        64\t|\tloss: 1872.07\n",
      "Training Epoch 21  25.0% | batch:        16 of        64\t|\tloss: 1966.58\n",
      "Training Epoch 21  26.6% | batch:        17 of        64\t|\tloss: 3388.1\n",
      "Training Epoch 21  28.1% | batch:        18 of        64\t|\tloss: 1143.46\n",
      "Training Epoch 21  29.7% | batch:        19 of        64\t|\tloss: 1690.8\n",
      "Training Epoch 21  31.2% | batch:        20 of        64\t|\tloss: 1734.65\n",
      "Training Epoch 21  32.8% | batch:        21 of        64\t|\tloss: 1947.85\n",
      "Training Epoch 21  34.4% | batch:        22 of        64\t|\tloss: 1925.47\n",
      "Training Epoch 21  35.9% | batch:        23 of        64\t|\tloss: 1729.63\n",
      "Training Epoch 21  37.5% | batch:        24 of        64\t|\tloss: 1605.25\n",
      "Training Epoch 21  39.1% | batch:        25 of        64\t|\tloss: 3433.04\n",
      "Training Epoch 21  40.6% | batch:        26 of        64\t|\tloss: 5754.76\n",
      "Training Epoch 21  42.2% | batch:        27 of        64\t|\tloss: 1221.22\n",
      "Training Epoch 21  43.8% | batch:        28 of        64\t|\tloss: 1041.64\n",
      "Training Epoch 21  45.3% | batch:        29 of        64\t|\tloss: 1579.05\n",
      "Training Epoch 21  46.9% | batch:        30 of        64\t|\tloss: 2545.96\n",
      "Training Epoch 21  48.4% | batch:        31 of        64\t|\tloss: 1494.38\n",
      "Training Epoch 21  50.0% | batch:        32 of        64\t|\tloss: 3491.13\n",
      "Training Epoch 21  51.6% | batch:        33 of        64\t|\tloss: 1414.43\n",
      "Training Epoch 21  53.1% | batch:        34 of        64\t|\tloss: 1144.06\n",
      "Training Epoch 21  54.7% | batch:        35 of        64\t|\tloss: 2714.03\n",
      "Training Epoch 21  56.2% | batch:        36 of        64\t|\tloss: 1463.05\n",
      "Training Epoch 21  57.8% | batch:        37 of        64\t|\tloss: 2003.72\n",
      "Training Epoch 21  59.4% | batch:        38 of        64\t|\tloss: 1737.49\n",
      "Training Epoch 21  60.9% | batch:        39 of        64\t|\tloss: 5194.85\n",
      "Training Epoch 21  62.5% | batch:        40 of        64\t|\tloss: 1480.03\n",
      "Training Epoch 21  64.1% | batch:        41 of        64\t|\tloss: 3021.89\n",
      "Training Epoch 21  65.6% | batch:        42 of        64\t|\tloss: 2191.5\n",
      "Training Epoch 21  67.2% | batch:        43 of        64\t|\tloss: 1010.15\n",
      "Training Epoch 21  68.8% | batch:        44 of        64\t|\tloss: 2265.35\n",
      "Training Epoch 21  70.3% | batch:        45 of        64\t|\tloss: 2767.63\n",
      "Training Epoch 21  71.9% | batch:        46 of        64\t|\tloss: 1378.05\n",
      "Training Epoch 21  73.4% | batch:        47 of        64\t|\tloss: 2425.17\n",
      "Training Epoch 21  75.0% | batch:        48 of        64\t|\tloss: 2829.52\n",
      "Training Epoch 21  76.6% | batch:        49 of        64\t|\tloss: 2776.69\n",
      "Training Epoch 21  78.1% | batch:        50 of        64\t|\tloss: 1476.66\n",
      "Training Epoch 21  79.7% | batch:        51 of        64\t|\tloss: 2500.98\n",
      "Training Epoch 21  81.2% | batch:        52 of        64\t|\tloss: 2472.42\n",
      "Training Epoch 21  82.8% | batch:        53 of        64\t|\tloss: 1409.1\n",
      "Training Epoch 21  84.4% | batch:        54 of        64\t|\tloss: 1552.99\n",
      "Training Epoch 21  85.9% | batch:        55 of        64\t|\tloss: 1552.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:22,293 | INFO : Epoch 21 Training Summary: epoch: 21.000000 | loss: 2130.256290 | \n",
      "2023-05-10 17:08:22,293 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1352181434631348 seconds\n",
      "\n",
      "2023-05-10 17:08:22,294 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2156034083593459 seconds\n",
      "2023-05-10 17:08:22,294 | INFO : Avg batch train. time: 0.01899380325561478 seconds\n",
      "2023-05-10 17:08:22,295 | INFO : Avg sample train. time: 0.00030104096294188853 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 21  87.5% | batch:        56 of        64\t|\tloss: 3996.44\n",
      "Training Epoch 21  89.1% | batch:        57 of        64\t|\tloss: 1791.15\n",
      "Training Epoch 21  90.6% | batch:        58 of        64\t|\tloss: 1616.61\n",
      "Training Epoch 21  92.2% | batch:        59 of        64\t|\tloss: 1862.05\n",
      "Training Epoch 21  93.8% | batch:        60 of        64\t|\tloss: 6181.41\n",
      "Training Epoch 21  95.3% | batch:        61 of        64\t|\tloss: 1115.95\n",
      "Training Epoch 21  96.9% | batch:        62 of        64\t|\tloss: 1218.06\n",
      "Training Epoch 21  98.4% | batch:        63 of        64\t|\tloss: 6130.1\n",
      "\n",
      "Training Epoch 22   0.0% | batch:         0 of        64\t|\tloss: 1858.14\n",
      "Training Epoch 22   1.6% | batch:         1 of        64\t|\tloss: 1593.21\n",
      "Training Epoch 22   3.1% | batch:         2 of        64\t|\tloss: 1294.39\n",
      "Training Epoch 22   4.7% | batch:         3 of        64\t|\tloss: 1479.05\n",
      "Training Epoch 22   6.2% | batch:         4 of        64\t|\tloss: 1144.43\n",
      "Training Epoch 22   7.8% | batch:         5 of        64\t|\tloss: 1354.07\n",
      "Training Epoch 22   9.4% | batch:         6 of        64\t|\tloss: 1497.74\n",
      "Training Epoch 22  10.9% | batch:         7 of        64\t|\tloss: 1015.31\n",
      "Training Epoch 22  12.5% | batch:         8 of        64\t|\tloss: 5821.19\n",
      "Training Epoch 22  14.1% | batch:         9 of        64\t|\tloss: 2597.93\n",
      "Training Epoch 22  15.6% | batch:        10 of        64\t|\tloss: 2993.77\n",
      "Training Epoch 22  17.2% | batch:        11 of        64\t|\tloss: 1628.07\n",
      "Training Epoch 22  18.8% | batch:        12 of        64\t|\tloss: 1773.58\n",
      "Training Epoch 22  20.3% | batch:        13 of        64\t|\tloss: 1890.19\n",
      "Training Epoch 22  21.9% | batch:        14 of        64\t|\tloss: 2893.85\n",
      "Training Epoch 22  23.4% | batch:        15 of        64\t|\tloss: 2000.45\n",
      "Training Epoch 22  25.0% | batch:        16 of        64\t|\tloss: 1585.91\n",
      "Training Epoch 22  26.6% | batch:        17 of        64\t|\tloss: 1649.04\n",
      "Training Epoch 22  28.1% | batch:        18 of        64\t|\tloss: 1378.14\n",
      "Training Epoch 22  29.7% | batch:        19 of        64\t|\tloss: 2450.92\n",
      "Training Epoch 22  31.2% | batch:        20 of        64\t|\tloss: 2259.08\n",
      "Training Epoch 22  32.8% | batch:        21 of        64\t|\tloss: 2634.31\n",
      "Training Epoch 22  34.4% | batch:        22 of        64\t|\tloss: 1174.79\n",
      "Training Epoch 22  35.9% | batch:        23 of        64\t|\tloss: 1036.64\n",
      "Training Epoch 22  37.5% | batch:        24 of        64\t|\tloss: 3212.23\n",
      "Training Epoch 22  39.1% | batch:        25 of        64\t|\tloss: 2876.4\n",
      "Training Epoch 22  40.6% | batch:        26 of        64\t|\tloss: 2711.81\n",
      "Training Epoch 22  42.2% | batch:        27 of        64\t|\tloss: 1848.95\n",
      "Training Epoch 22  43.8% | batch:        28 of        64\t|\tloss: 3123.95\n",
      "Training Epoch 22  45.3% | batch:        29 of        64\t|\tloss: 4716.52\n",
      "Training Epoch 22  46.9% | batch:        30 of        64\t|\tloss: 2968.74\n",
      "Training Epoch 22  48.4% | batch:        31 of        64\t|\tloss: 1467.28\n",
      "Training Epoch 22  50.0% | batch:        32 of        64\t|\tloss: 2064.09\n",
      "Training Epoch 22  51.6% | batch:        33 of        64\t|\tloss: 3485.11\n",
      "Training Epoch 22  53.1% | batch:        34 of        64\t|\tloss: 2275.61\n",
      "Training Epoch 22  54.7% | batch:        35 of        64\t|\tloss: 2862.32\n",
      "Training Epoch 22  56.2% | batch:        36 of        64\t|\tloss: 2558.23\n",
      "Training Epoch 22  57.8% | batch:        37 of        64\t|\tloss: 1493.07\n",
      "Training Epoch 22  59.4% | batch:        38 of        64\t|\tloss: 2678.92\n",
      "Training Epoch 22  60.9% | batch:        39 of        64\t|\tloss: 2745.13\n",
      "Training Epoch 22  62.5% | batch:        40 of        64\t|\tloss: 1534.3\n",
      "Training Epoch 22  64.1% | batch:        41 of        64\t|\tloss: 3756.18\n",
      "Training Epoch 22  65.6% | batch:        42 of        64\t|\tloss: 2366.73\n",
      "Training Epoch 22  67.2% | batch:        43 of        64\t|\tloss: 1649.42\n",
      "Training Epoch 22  68.8% | batch:        44 of        64\t|\tloss: 1774.29\n",
      "Training Epoch 22  70.3% | batch:        45 of        64\t|\tloss: 1504.82\n",
      "Training Epoch 22  71.9% | batch:        46 of        64\t|\tloss: 1546.56\n",
      "Training Epoch 22  73.4% | batch:        47 of        64\t|\tloss: 2442.88\n",
      "Training Epoch 22  75.0% | batch:        48 of        64\t|\tloss: 2214.23\n",
      "Training Epoch 22  76.6% | batch:        49 of        64\t|\tloss: 1478.74\n",
      "Training Epoch 22  78.1% | batch:        50 of        64\t|\tloss: 1182.88\n",
      "Training Epoch 22  79.7% | batch:        51 of        64\t|\tloss: 2004.32\n",
      "Training Epoch 22  81.2% | batch:        52 of        64\t|\tloss: 1791.4\n",
      "Training Epoch 22  82.8% | batch:        53 of        64\t|\tloss: 2637.74\n",
      "Training Epoch 22  84.4% | batch:        54 of        64\t|\tloss: 1694.02\n",
      "Training Epoch 22  85.9% | batch:        55 of        64\t|\tloss: 1959.93\n",
      "Training Epoch 22  87.5% | batch:        56 of        64\t|\tloss: 2457.99\n",
      "Training Epoch 22  89.1% | batch:        57 of        64\t|\tloss: 1882.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:23,503 | INFO : Epoch 22 Training Summary: epoch: 22.000000 | loss: 2157.103099 | \n",
      "2023-05-10 17:08:23,503 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1989734172821045 seconds\n",
      "\n",
      "2023-05-10 17:08:23,504 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2148474996740168 seconds\n",
      "2023-05-10 17:08:23,504 | INFO : Avg batch train. time: 0.018981992182406513 seconds\n",
      "2023-05-10 17:08:23,505 | INFO : Avg sample train. time: 0.00030085376415899375 seconds\n",
      "2023-05-10 17:08:23,505 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 22  90.6% | batch:        58 of        64\t|\tloss: 1590.43\n",
      "Training Epoch 22  92.2% | batch:        59 of        64\t|\tloss: 2400.96\n",
      "Training Epoch 22  93.8% | batch:        60 of        64\t|\tloss: 1924.44\n",
      "Training Epoch 22  95.3% | batch:        61 of        64\t|\tloss: 1691.95\n",
      "Training Epoch 22  96.9% | batch:        62 of        64\t|\tloss: 1412.91\n",
      "Training Epoch 22  98.4% | batch:        63 of        64\t|\tloss: 11816.2\n",
      "\n",
      "Evaluating Epoch 22   0.0% | batch:         0 of        16\t|\tloss: 1826.68\n",
      "Evaluating Epoch 22   6.2% | batch:         1 of        16\t|\tloss: 1827.09\n",
      "Evaluating Epoch 22  12.5% | batch:         2 of        16\t|\tloss: 1524.29\n",
      "Evaluating Epoch 22  18.8% | batch:         3 of        16\t|\tloss: 2538.27\n",
      "Evaluating Epoch 22  25.0% | batch:         4 of        16\t|\tloss: 1357.44\n",
      "Evaluating Epoch 22  31.2% | batch:         5 of        16\t|\tloss: 2533.78\n",
      "Evaluating Epoch 22  37.5% | batch:         6 of        16\t|\tloss: 2699.65\n",
      "Evaluating Epoch 22  43.8% | batch:         7 of        16\t|\tloss: 3756.23\n",
      "Evaluating Epoch 22  50.0% | batch:         8 of        16\t|\tloss: 2306.14\n",
      "Evaluating Epoch 22  56.2% | batch:         9 of        16\t|\tloss: 6379.67\n",
      "Evaluating Epoch 22  62.5% | batch:        10 of        16\t|\tloss: 2317.08\n",
      "Evaluating Epoch 22  68.8% | batch:        11 of        16\t|\tloss: 4182.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:23,652 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14637136459350586 seconds\n",
      "\n",
      "2023-05-10 17:08:23,653 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.20159040964566743 seconds\n",
      "2023-05-10 17:08:23,653 | INFO : Avg batch val. time: 0.012599400602854215 seconds\n",
      "2023-05-10 17:08:23,654 | INFO : Avg sample val. time: 0.00019959446499571033 seconds\n",
      "2023-05-10 17:08:23,654 | INFO : Epoch 22 Validation Summary: epoch: 22.000000 | loss: 2649.128287 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 22  75.0% | batch:        12 of        16\t|\tloss: 857.122\n",
      "Evaluating Epoch 22  81.2% | batch:        13 of        16\t|\tloss: 1689.56\n",
      "Evaluating Epoch 22  87.5% | batch:        14 of        16\t|\tloss: 2328.43\n",
      "Evaluating Epoch 22  93.8% | batch:        15 of        16\t|\tloss: 4714.33\n",
      "\n",
      "Training Epoch 23   0.0% | batch:         0 of        64\t|\tloss: 2266.34\n",
      "Training Epoch 23   1.6% | batch:         1 of        64\t|\tloss: 1423.22\n",
      "Training Epoch 23   3.1% | batch:         2 of        64\t|\tloss: 5124.46\n",
      "Training Epoch 23   4.7% | batch:         3 of        64\t|\tloss: 2719.15\n",
      "Training Epoch 23   6.2% | batch:         4 of        64\t|\tloss: 2871.41\n",
      "Training Epoch 23   7.8% | batch:         5 of        64\t|\tloss: 1855.21\n",
      "Training Epoch 23   9.4% | batch:         6 of        64\t|\tloss: 1980.57\n",
      "Training Epoch 23  10.9% | batch:         7 of        64\t|\tloss: 1741.93\n",
      "Training Epoch 23  12.5% | batch:         8 of        64\t|\tloss: 2317.16\n",
      "Training Epoch 23  14.1% | batch:         9 of        64\t|\tloss: 1172.54\n",
      "Training Epoch 23  15.6% | batch:        10 of        64\t|\tloss: 1006.37\n",
      "Training Epoch 23  17.2% | batch:        11 of        64\t|\tloss: 1421.94\n",
      "Training Epoch 23  18.8% | batch:        12 of        64\t|\tloss: 1217.41\n",
      "Training Epoch 23  20.3% | batch:        13 of        64\t|\tloss: 1283.28\n",
      "Training Epoch 23  21.9% | batch:        14 of        64\t|\tloss: 1765.75\n",
      "Training Epoch 23  23.4% | batch:        15 of        64\t|\tloss: 1666.26\n",
      "Training Epoch 23  25.0% | batch:        16 of        64\t|\tloss: 1656.72\n",
      "Training Epoch 23  26.6% | batch:        17 of        64\t|\tloss: 1177.67\n",
      "Training Epoch 23  28.1% | batch:        18 of        64\t|\tloss: 3754.62\n",
      "Training Epoch 23  29.7% | batch:        19 of        64\t|\tloss: 1673.55\n",
      "Training Epoch 23  31.2% | batch:        20 of        64\t|\tloss: 2156.53\n",
      "Training Epoch 23  32.8% | batch:        21 of        64\t|\tloss: 1047.95\n",
      "Training Epoch 23  34.4% | batch:        22 of        64\t|\tloss: 2085.01\n",
      "Training Epoch 23  35.9% | batch:        23 of        64\t|\tloss: 1718.74\n",
      "Training Epoch 23  37.5% | batch:        24 of        64\t|\tloss: 2674.62\n",
      "Training Epoch 23  39.1% | batch:        25 of        64\t|\tloss: 1523.11\n",
      "Training Epoch 23  40.6% | batch:        26 of        64\t|\tloss: 1453.76\n",
      "Training Epoch 23  42.2% | batch:        27 of        64\t|\tloss: 2594.26\n",
      "Training Epoch 23  43.8% | batch:        28 of        64\t|\tloss: 1645.44\n",
      "Training Epoch 23  45.3% | batch:        29 of        64\t|\tloss: 804.164\n",
      "Training Epoch 23  46.9% | batch:        30 of        64\t|\tloss: 3641.32\n",
      "Training Epoch 23  48.4% | batch:        31 of        64\t|\tloss: 3305.79\n",
      "Training Epoch 23  50.0% | batch:        32 of        64\t|\tloss: 1844.59\n",
      "Training Epoch 23  51.6% | batch:        33 of        64\t|\tloss: 1542.22\n",
      "Training Epoch 23  53.1% | batch:        34 of        64\t|\tloss: 2061.16\n",
      "Training Epoch 23  54.7% | batch:        35 of        64\t|\tloss: 2501.69\n",
      "Training Epoch 23  56.2% | batch:        36 of        64\t|\tloss: 3979.2\n",
      "Training Epoch 23  57.8% | batch:        37 of        64\t|\tloss: 2283.05\n",
      "Training Epoch 23  59.4% | batch:        38 of        64\t|\tloss: 1910.17\n",
      "Training Epoch 23  60.9% | batch:        39 of        64\t|\tloss: 1499.6\n",
      "Training Epoch 23  62.5% | batch:        40 of        64\t|\tloss: 1346.56\n",
      "Training Epoch 23  64.1% | batch:        41 of        64\t|\tloss: 865.235\n",
      "Training Epoch 23  65.6% | batch:        42 of        64\t|\tloss: 1121.74\n",
      "Training Epoch 23  67.2% | batch:        43 of        64\t|\tloss: 2107.38\n",
      "Training Epoch 23  68.8% | batch:        44 of        64\t|\tloss: 3553.4\n",
      "Training Epoch 23  70.3% | batch:        45 of        64\t|\tloss: 1800.07\n",
      "Training Epoch 23  71.9% | batch:        46 of        64\t|\tloss: 1826.78\n",
      "Training Epoch 23  73.4% | batch:        47 of        64\t|\tloss: 3932.42\n",
      "Training Epoch 23  75.0% | batch:        48 of        64\t|\tloss: 1489.23\n",
      "Training Epoch 23  76.6% | batch:        49 of        64\t|\tloss: 1057.07\n",
      "Training Epoch 23  78.1% | batch:        50 of        64\t|\tloss: 1629.47\n",
      "Training Epoch 23  79.7% | batch:        51 of        64\t|\tloss: 1735.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:24,869 | INFO : Epoch 23 Training Summary: epoch: 23.000000 | loss: 2105.147191 | \n",
      "2023-05-10 17:08:24,869 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2054443359375 seconds\n",
      "\n",
      "2023-05-10 17:08:24,870 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2144386664680813 seconds\n",
      "2023-05-10 17:08:24,870 | INFO : Avg batch train. time: 0.01897560416356377 seconds\n",
      "2023-05-10 17:08:24,870 | INFO : Avg sample train. time: 0.00030075251769888094 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 23  81.2% | batch:        52 of        64\t|\tloss: 1882.73\n",
      "Training Epoch 23  82.8% | batch:        53 of        64\t|\tloss: 3515.41\n",
      "Training Epoch 23  84.4% | batch:        54 of        64\t|\tloss: 2830.11\n",
      "Training Epoch 23  85.9% | batch:        55 of        64\t|\tloss: 5183.89\n",
      "Training Epoch 23  87.5% | batch:        56 of        64\t|\tloss: 2074.07\n",
      "Training Epoch 23  89.1% | batch:        57 of        64\t|\tloss: 1786.32\n",
      "Training Epoch 23  90.6% | batch:        58 of        64\t|\tloss: 1651.55\n",
      "Training Epoch 23  92.2% | batch:        59 of        64\t|\tloss: 2767.86\n",
      "Training Epoch 23  93.8% | batch:        60 of        64\t|\tloss: 1764.15\n",
      "Training Epoch 23  95.3% | batch:        61 of        64\t|\tloss: 2006.94\n",
      "Training Epoch 23  96.9% | batch:        62 of        64\t|\tloss: 2432.5\n",
      "Training Epoch 23  98.4% | batch:        63 of        64\t|\tloss: 997.198\n",
      "\n",
      "Training Epoch 24   0.0% | batch:         0 of        64\t|\tloss: 1590.59\n",
      "Training Epoch 24   1.6% | batch:         1 of        64\t|\tloss: 1644.55\n",
      "Training Epoch 24   3.1% | batch:         2 of        64\t|\tloss: 1917.25\n",
      "Training Epoch 24   4.7% | batch:         3 of        64\t|\tloss: 1383.29\n",
      "Training Epoch 24   6.2% | batch:         4 of        64\t|\tloss: 3028.48\n",
      "Training Epoch 24   7.8% | batch:         5 of        64\t|\tloss: 1900.56\n",
      "Training Epoch 24   9.4% | batch:         6 of        64\t|\tloss: 2280.32\n",
      "Training Epoch 24  10.9% | batch:         7 of        64\t|\tloss: 2280.38\n",
      "Training Epoch 24  12.5% | batch:         8 of        64\t|\tloss: 1611.16\n",
      "Training Epoch 24  14.1% | batch:         9 of        64\t|\tloss: 1711.11\n",
      "Training Epoch 24  15.6% | batch:        10 of        64\t|\tloss: 2199.33\n",
      "Training Epoch 24  17.2% | batch:        11 of        64\t|\tloss: 1576.79\n",
      "Training Epoch 24  18.8% | batch:        12 of        64\t|\tloss: 1568.49\n",
      "Training Epoch 24  20.3% | batch:        13 of        64\t|\tloss: 2088.11\n",
      "Training Epoch 24  21.9% | batch:        14 of        64\t|\tloss: 1272.33\n",
      "Training Epoch 24  23.4% | batch:        15 of        64\t|\tloss: 2406.32\n",
      "Training Epoch 24  25.0% | batch:        16 of        64\t|\tloss: 2331.83\n",
      "Training Epoch 24  26.6% | batch:        17 of        64\t|\tloss: 2605.08\n",
      "Training Epoch 24  28.1% | batch:        18 of        64\t|\tloss: 1721.22\n",
      "Training Epoch 24  29.7% | batch:        19 of        64\t|\tloss: 1468.19\n",
      "Training Epoch 24  31.2% | batch:        20 of        64\t|\tloss: 1691.35\n",
      "Training Epoch 24  32.8% | batch:        21 of        64\t|\tloss: 2757.06\n",
      "Training Epoch 24  34.4% | batch:        22 of        64\t|\tloss: 2497.11\n",
      "Training Epoch 24  35.9% | batch:        23 of        64\t|\tloss: 2251.17\n",
      "Training Epoch 24  37.5% | batch:        24 of        64\t|\tloss: 2485.64\n",
      "Training Epoch 24  39.1% | batch:        25 of        64\t|\tloss: 1117.02\n",
      "Training Epoch 24  40.6% | batch:        26 of        64\t|\tloss: 1348.2\n",
      "Training Epoch 24  42.2% | batch:        27 of        64\t|\tloss: 1508.86\n",
      "Training Epoch 24  43.8% | batch:        28 of        64\t|\tloss: 2679.67\n",
      "Training Epoch 24  45.3% | batch:        29 of        64\t|\tloss: 1410.59\n",
      "Training Epoch 24  46.9% | batch:        30 of        64\t|\tloss: 3249.15\n",
      "Training Epoch 24  48.4% | batch:        31 of        64\t|\tloss: 1013.76\n",
      "Training Epoch 24  50.0% | batch:        32 of        64\t|\tloss: 1782.9\n",
      "Training Epoch 24  51.6% | batch:        33 of        64\t|\tloss: 1055.18\n",
      "Training Epoch 24  53.1% | batch:        34 of        64\t|\tloss: 5367.95\n",
      "Training Epoch 24  54.7% | batch:        35 of        64\t|\tloss: 4597.76\n",
      "Training Epoch 24  56.2% | batch:        36 of        64\t|\tloss: 1814.41\n",
      "Training Epoch 24  57.8% | batch:        37 of        64\t|\tloss: 2609.72\n",
      "Training Epoch 24  59.4% | batch:        38 of        64\t|\tloss: 656.013\n",
      "Training Epoch 24  60.9% | batch:        39 of        64\t|\tloss: 1283.04\n",
      "Training Epoch 24  62.5% | batch:        40 of        64\t|\tloss: 2034.67\n",
      "Training Epoch 24  64.1% | batch:        41 of        64\t|\tloss: 1920.72\n",
      "Training Epoch 24  65.6% | batch:        42 of        64\t|\tloss: 2218.41\n",
      "Training Epoch 24  67.2% | batch:        43 of        64\t|\tloss: 819.301\n",
      "Training Epoch 24  68.8% | batch:        44 of        64\t|\tloss: 1752.11\n",
      "Training Epoch 24  70.3% | batch:        45 of        64\t|\tloss: 1504.54\n",
      "Training Epoch 24  71.9% | batch:        46 of        64\t|\tloss: 2218.66\n",
      "Training Epoch 24  73.4% | batch:        47 of        64\t|\tloss: 2467.09\n",
      "Training Epoch 24  75.0% | batch:        48 of        64\t|\tloss: 1749.95\n",
      "Training Epoch 24  76.6% | batch:        49 of        64\t|\tloss: 1666.95\n",
      "Training Epoch 24  78.1% | batch:        50 of        64\t|\tloss: 1753.89\n",
      "Training Epoch 24  79.7% | batch:        51 of        64\t|\tloss: 2147.43\n",
      "Training Epoch 24  81.2% | batch:        52 of        64\t|\tloss: 1706.89\n",
      "Training Epoch 24  82.8% | batch:        53 of        64\t|\tloss: 2141.73\n",
      "Training Epoch 24  84.4% | batch:        54 of        64\t|\tloss: 1633.17\n",
      "Training Epoch 24  85.9% | batch:        55 of        64\t|\tloss: 2171.75\n",
      "Training Epoch 24  87.5% | batch:        56 of        64\t|\tloss: 2782.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:26,040 | INFO : Epoch 24 Training Summary: epoch: 24.000000 | loss: 2028.141048 | \n",
      "2023-05-10 17:08:26,041 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1576712131500244 seconds\n",
      "\n",
      "2023-05-10 17:08:26,041 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2120733559131622 seconds\n",
      "2023-05-10 17:08:26,042 | INFO : Avg batch train. time: 0.01893864618614316 seconds\n",
      "2023-05-10 17:08:26,042 | INFO : Avg sample train. time: 0.0003001667548076182 seconds\n",
      "2023-05-10 17:08:26,043 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 24  89.1% | batch:        57 of        64\t|\tloss: 2228.41\n",
      "Training Epoch 24  90.6% | batch:        58 of        64\t|\tloss: 3770.65\n",
      "Training Epoch 24  92.2% | batch:        59 of        64\t|\tloss: 2043.18\n",
      "Training Epoch 24  93.8% | batch:        60 of        64\t|\tloss: 2340.95\n",
      "Training Epoch 24  95.3% | batch:        61 of        64\t|\tloss: 1402.44\n",
      "Training Epoch 24  96.9% | batch:        62 of        64\t|\tloss: 1501.77\n",
      "Training Epoch 24  98.4% | batch:        63 of        64\t|\tloss: 2385.44\n",
      "\n",
      "Evaluating Epoch 24   0.0% | batch:         0 of        16\t|\tloss: 1501.99\n",
      "Evaluating Epoch 24   6.2% | batch:         1 of        16\t|\tloss: 1578.13\n",
      "Evaluating Epoch 24  12.5% | batch:         2 of        16\t|\tloss: 1288.58\n",
      "Evaluating Epoch 24  18.8% | batch:         3 of        16\t|\tloss: 2444.42\n",
      "Evaluating Epoch 24  25.0% | batch:         4 of        16\t|\tloss: 1684.9\n",
      "Evaluating Epoch 24  31.2% | batch:         5 of        16\t|\tloss: 1954.62\n",
      "Evaluating Epoch 24  37.5% | batch:         6 of        16\t|\tloss: 2403.03\n",
      "Evaluating Epoch 24  43.8% | batch:         7 of        16\t|\tloss: 3619.91\n",
      "Evaluating Epoch 24  50.0% | batch:         8 of        16\t|\tloss: 1777.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:26,192 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14917278289794922 seconds\n",
      "\n",
      "2023-05-10 17:08:26,193 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.19784629344940186 seconds\n",
      "2023-05-10 17:08:26,193 | INFO : Avg batch val. time: 0.012365393340587616 seconds\n",
      "2023-05-10 17:08:26,193 | INFO : Avg sample val. time: 0.00019588741925683352 seconds\n",
      "2023-05-10 17:08:26,194 | INFO : Epoch 24 Validation Summary: epoch: 24.000000 | loss: 2573.523685 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 24  56.2% | batch:         9 of        16\t|\tloss: 9255.78\n",
      "Evaluating Epoch 24  62.5% | batch:        10 of        16\t|\tloss: 2053.93\n",
      "Evaluating Epoch 24  68.8% | batch:        11 of        16\t|\tloss: 3946.31\n",
      "Evaluating Epoch 24  75.0% | batch:        12 of        16\t|\tloss: 856.605\n",
      "Evaluating Epoch 24  81.2% | batch:        13 of        16\t|\tloss: 1249.37\n",
      "Evaluating Epoch 24  87.5% | batch:        14 of        16\t|\tloss: 2063.23\n",
      "Evaluating Epoch 24  93.8% | batch:        15 of        16\t|\tloss: 3756.36\n",
      "\n",
      "Training Epoch 25   0.0% | batch:         0 of        64\t|\tloss: 2497.56\n",
      "Training Epoch 25   1.6% | batch:         1 of        64\t|\tloss: 1964.18\n",
      "Training Epoch 25   3.1% | batch:         2 of        64\t|\tloss: 2528.79\n",
      "Training Epoch 25   4.7% | batch:         3 of        64\t|\tloss: 1012.22\n",
      "Training Epoch 25   6.2% | batch:         4 of        64\t|\tloss: 1507.1\n",
      "Training Epoch 25   7.8% | batch:         5 of        64\t|\tloss: 3818.45\n",
      "Training Epoch 25   9.4% | batch:         6 of        64\t|\tloss: 1102.76\n",
      "Training Epoch 25  10.9% | batch:         7 of        64\t|\tloss: 1140.13\n",
      "Training Epoch 25  12.5% | batch:         8 of        64\t|\tloss: 2721.02\n",
      "Training Epoch 25  14.1% | batch:         9 of        64\t|\tloss: 1355.23\n",
      "Training Epoch 25  15.6% | batch:        10 of        64\t|\tloss: 1761.53\n",
      "Training Epoch 25  17.2% | batch:        11 of        64\t|\tloss: 1748.82\n",
      "Training Epoch 25  18.8% | batch:        12 of        64\t|\tloss: 1679.03\n",
      "Training Epoch 25  20.3% | batch:        13 of        64\t|\tloss: 2830.9\n",
      "Training Epoch 25  21.9% | batch:        14 of        64\t|\tloss: 1378.02\n",
      "Training Epoch 25  23.4% | batch:        15 of        64\t|\tloss: 1573.16\n",
      "Training Epoch 25  25.0% | batch:        16 of        64\t|\tloss: 1060.43\n",
      "Training Epoch 25  26.6% | batch:        17 of        64\t|\tloss: 2121.21\n",
      "Training Epoch 25  28.1% | batch:        18 of        64\t|\tloss: 1409.72\n",
      "Training Epoch 25  29.7% | batch:        19 of        64\t|\tloss: 1654.59\n",
      "Training Epoch 25  31.2% | batch:        20 of        64\t|\tloss: 1045.84\n",
      "Training Epoch 25  32.8% | batch:        21 of        64\t|\tloss: 3698.79\n",
      "Training Epoch 25  34.4% | batch:        22 of        64\t|\tloss: 2437.51\n",
      "Training Epoch 25  35.9% | batch:        23 of        64\t|\tloss: 8158.07\n",
      "Training Epoch 25  37.5% | batch:        24 of        64\t|\tloss: 3949.82\n",
      "Training Epoch 25  39.1% | batch:        25 of        64\t|\tloss: 2608.2\n",
      "Training Epoch 25  40.6% | batch:        26 of        64\t|\tloss: 840.774\n",
      "Training Epoch 25  42.2% | batch:        27 of        64\t|\tloss: 2355.71\n",
      "Training Epoch 25  43.8% | batch:        28 of        64\t|\tloss: 1471.5\n",
      "Training Epoch 25  45.3% | batch:        29 of        64\t|\tloss: 1876.98\n",
      "Training Epoch 25  46.9% | batch:        30 of        64\t|\tloss: 3180.61\n",
      "Training Epoch 25  48.4% | batch:        31 of        64\t|\tloss: 1168.57\n",
      "Training Epoch 25  50.0% | batch:        32 of        64\t|\tloss: 944.493\n",
      "Training Epoch 25  51.6% | batch:        33 of        64\t|\tloss: 1552.61\n",
      "Training Epoch 25  53.1% | batch:        34 of        64\t|\tloss: 1603.75\n",
      "Training Epoch 25  54.7% | batch:        35 of        64\t|\tloss: 2219.64\n",
      "Training Epoch 25  56.2% | batch:        36 of        64\t|\tloss: 2137.62\n",
      "Training Epoch 25  57.8% | batch:        37 of        64\t|\tloss: 1189.36\n",
      "Training Epoch 25  59.4% | batch:        38 of        64\t|\tloss: 1261.19\n",
      "Training Epoch 25  60.9% | batch:        39 of        64\t|\tloss: 2379.81\n",
      "Training Epoch 25  62.5% | batch:        40 of        64\t|\tloss: 2332.88\n",
      "Training Epoch 25  64.1% | batch:        41 of        64\t|\tloss: 2058.46\n",
      "Training Epoch 25  65.6% | batch:        42 of        64\t|\tloss: 3773.07\n",
      "Training Epoch 25  67.2% | batch:        43 of        64\t|\tloss: 1452.41\n",
      "Training Epoch 25  68.8% | batch:        44 of        64\t|\tloss: 1876.18\n",
      "Training Epoch 25  70.3% | batch:        45 of        64\t|\tloss: 996.867\n",
      "Training Epoch 25  71.9% | batch:        46 of        64\t|\tloss: 2143.97\n",
      "Training Epoch 25  73.4% | batch:        47 of        64\t|\tloss: 2557.54\n",
      "Training Epoch 25  75.0% | batch:        48 of        64\t|\tloss: 2372.79\n",
      "Training Epoch 25  76.6% | batch:        49 of        64\t|\tloss: 955.573\n",
      "Training Epoch 25  78.1% | batch:        50 of        64\t|\tloss: 2320.67\n",
      "Training Epoch 25  79.7% | batch:        51 of        64\t|\tloss: 1459.6\n",
      "Training Epoch 25  81.2% | batch:        52 of        64\t|\tloss: 1591.01\n",
      "Training Epoch 25  82.8% | batch:        53 of        64\t|\tloss: 2505.3\n",
      "Training Epoch 25  84.4% | batch:        54 of        64\t|\tloss: 1631.79\n",
      "Training Epoch 25  85.9% | batch:        55 of        64\t|\tloss: 1711.34\n",
      "Training Epoch 25  87.5% | batch:        56 of        64\t|\tloss: 2641.43\n",
      "Training Epoch 25  89.1% | batch:        57 of        64\t|\tloss: 2378.93\n",
      "Training Epoch 25  90.6% | batch:        58 of        64\t|\tloss: 1615.43\n",
      "Training Epoch 25  92.2% | batch:        59 of        64\t|\tloss: 2275.18\n",
      "Training Epoch 25  93.8% | batch:        60 of        64\t|\tloss: 2156.5\n",
      "Training Epoch 25  95.3% | batch:        61 of        64\t|\tloss: 1266.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:27,401 | INFO : Epoch 25 Training Summary: epoch: 25.000000 | loss: 2054.266003 | \n",
      "2023-05-10 17:08:27,402 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1977405548095703 seconds\n",
      "\n",
      "2023-05-10 17:08:27,403 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2115000438690187 seconds\n",
      "2023-05-10 17:08:27,403 | INFO : Avg batch train. time: 0.018929688185453417 seconds\n",
      "2023-05-10 17:08:27,403 | INFO : Avg sample train. time: 0.00030002477559906357 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 25  96.9% | batch:        62 of        64\t|\tloss: 2324.02\n",
      "Training Epoch 25  98.4% | batch:        63 of        64\t|\tloss: 2857.85\n",
      "\n",
      "Training Epoch 26   0.0% | batch:         0 of        64\t|\tloss: 1509.81\n",
      "Training Epoch 26   1.6% | batch:         1 of        64\t|\tloss: 1822.84\n",
      "Training Epoch 26   3.1% | batch:         2 of        64\t|\tloss: 945.367\n",
      "Training Epoch 26   4.7% | batch:         3 of        64\t|\tloss: 3112.57\n",
      "Training Epoch 26   6.2% | batch:         4 of        64\t|\tloss: 3947.79\n",
      "Training Epoch 26   7.8% | batch:         5 of        64\t|\tloss: 1430.27\n",
      "Training Epoch 26   9.4% | batch:         6 of        64\t|\tloss: 1897.58\n",
      "Training Epoch 26  10.9% | batch:         7 of        64\t|\tloss: 1139.23\n",
      "Training Epoch 26  12.5% | batch:         8 of        64\t|\tloss: 2492.09\n",
      "Training Epoch 26  14.1% | batch:         9 of        64\t|\tloss: 2288.7\n",
      "Training Epoch 26  15.6% | batch:        10 of        64\t|\tloss: 2550.1\n",
      "Training Epoch 26  17.2% | batch:        11 of        64\t|\tloss: 2698.78\n",
      "Training Epoch 26  18.8% | batch:        12 of        64\t|\tloss: 1752.46\n",
      "Training Epoch 26  20.3% | batch:        13 of        64\t|\tloss: 2055.3\n",
      "Training Epoch 26  21.9% | batch:        14 of        64\t|\tloss: 965.689\n",
      "Training Epoch 26  23.4% | batch:        15 of        64\t|\tloss: 1449.24\n",
      "Training Epoch 26  25.0% | batch:        16 of        64\t|\tloss: 1659.2\n",
      "Training Epoch 26  26.6% | batch:        17 of        64\t|\tloss: 1446.22\n",
      "Training Epoch 26  28.1% | batch:        18 of        64\t|\tloss: 3498.43\n",
      "Training Epoch 26  29.7% | batch:        19 of        64\t|\tloss: 1368.9\n",
      "Training Epoch 26  31.2% | batch:        20 of        64\t|\tloss: 2256.5\n",
      "Training Epoch 26  32.8% | batch:        21 of        64\t|\tloss: 1823.9\n",
      "Training Epoch 26  34.4% | batch:        22 of        64\t|\tloss: 2308.12\n",
      "Training Epoch 26  35.9% | batch:        23 of        64\t|\tloss: 3088.57\n",
      "Training Epoch 26  37.5% | batch:        24 of        64\t|\tloss: 1339.04\n",
      "Training Epoch 26  39.1% | batch:        25 of        64\t|\tloss: 1710.02\n",
      "Training Epoch 26  40.6% | batch:        26 of        64\t|\tloss: 2055.2\n",
      "Training Epoch 26  42.2% | batch:        27 of        64\t|\tloss: 2753.7\n",
      "Training Epoch 26  43.8% | batch:        28 of        64\t|\tloss: 989.043\n",
      "Training Epoch 26  45.3% | batch:        29 of        64\t|\tloss: 2765.92\n",
      "Training Epoch 26  46.9% | batch:        30 of        64\t|\tloss: 2134.66\n",
      "Training Epoch 26  48.4% | batch:        31 of        64\t|\tloss: 2078.75\n",
      "Training Epoch 26  50.0% | batch:        32 of        64\t|\tloss: 2298.12\n",
      "Training Epoch 26  51.6% | batch:        33 of        64\t|\tloss: 4152.27\n",
      "Training Epoch 26  53.1% | batch:        34 of        64\t|\tloss: 1123.95\n",
      "Training Epoch 26  54.7% | batch:        35 of        64\t|\tloss: 1608.99\n",
      "Training Epoch 26  56.2% | batch:        36 of        64\t|\tloss: 1659.78\n",
      "Training Epoch 26  57.8% | batch:        37 of        64\t|\tloss: 1155.02\n",
      "Training Epoch 26  59.4% | batch:        38 of        64\t|\tloss: 2047.95\n",
      "Training Epoch 26  60.9% | batch:        39 of        64\t|\tloss: 3561.92\n",
      "Training Epoch 26  62.5% | batch:        40 of        64\t|\tloss: 1439.96\n",
      "Training Epoch 26  64.1% | batch:        41 of        64\t|\tloss: 2791.22\n",
      "Training Epoch 26  65.6% | batch:        42 of        64\t|\tloss: 1272.54\n",
      "Training Epoch 26  67.2% | batch:        43 of        64\t|\tloss: 850.69\n",
      "Training Epoch 26  68.8% | batch:        44 of        64\t|\tloss: 3473.87\n",
      "Training Epoch 26  70.3% | batch:        45 of        64\t|\tloss: 2395.42\n",
      "Training Epoch 26  71.9% | batch:        46 of        64\t|\tloss: 1464.51\n",
      "Training Epoch 26  73.4% | batch:        47 of        64\t|\tloss: 1011.11\n",
      "Training Epoch 26  75.0% | batch:        48 of        64\t|\tloss: 1335.56\n",
      "Training Epoch 26  76.6% | batch:        49 of        64\t|\tloss: 1620.12\n",
      "Training Epoch 26  78.1% | batch:        50 of        64\t|\tloss: 1447.46\n",
      "Training Epoch 26  79.7% | batch:        51 of        64\t|\tloss: 2407.94\n",
      "Training Epoch 26  81.2% | batch:        52 of        64\t|\tloss: 1656.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:28,601 | INFO : Epoch 26 Training Summary: epoch: 26.000000 | loss: 1989.150067 | \n",
      "2023-05-10 17:08:28,601 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1883139610290527 seconds\n",
      "\n",
      "2023-05-10 17:08:28,602 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2106082714520967 seconds\n",
      "2023-05-10 17:08:28,602 | INFO : Avg batch train. time: 0.01891575424143901 seconds\n",
      "2023-05-10 17:08:28,602 | INFO : Avg sample train. time: 0.00029980393052305516 seconds\n",
      "2023-05-10 17:08:28,603 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 26  82.8% | batch:        53 of        64\t|\tloss: 1740.7\n",
      "Training Epoch 26  84.4% | batch:        54 of        64\t|\tloss: 2253.95\n",
      "Training Epoch 26  85.9% | batch:        55 of        64\t|\tloss: 2493.33\n",
      "Training Epoch 26  87.5% | batch:        56 of        64\t|\tloss: 1656.66\n",
      "Training Epoch 26  89.1% | batch:        57 of        64\t|\tloss: 973.679\n",
      "Training Epoch 26  90.6% | batch:        58 of        64\t|\tloss: 3305.29\n",
      "Training Epoch 26  92.2% | batch:        59 of        64\t|\tloss: 1397.33\n",
      "Training Epoch 26  93.8% | batch:        60 of        64\t|\tloss: 1225.89\n",
      "Training Epoch 26  95.3% | batch:        61 of        64\t|\tloss: 2363.07\n",
      "Training Epoch 26  96.9% | batch:        62 of        64\t|\tloss: 1688.67\n",
      "Training Epoch 26  98.4% | batch:        63 of        64\t|\tloss: 3149.01\n",
      "\n",
      "Evaluating Epoch 26   0.0% | batch:         0 of        16\t|\tloss: 1496.42\n",
      "Evaluating Epoch 26   6.2% | batch:         1 of        16\t|\tloss: 1670.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:28,759 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.1564042568206787 seconds\n",
      "\n",
      "2023-05-10 17:08:28,760 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.19508349100748698 seconds\n",
      "2023-05-10 17:08:28,760 | INFO : Avg batch val. time: 0.012192718187967936 seconds\n",
      "2023-05-10 17:08:28,761 | INFO : Avg sample val. time: 0.00019315197129454156 seconds\n",
      "2023-05-10 17:08:28,761 | INFO : Epoch 26 Validation Summary: epoch: 26.000000 | loss: 2393.274161 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 26  12.5% | batch:         2 of        16\t|\tloss: 1303.99\n",
      "Evaluating Epoch 26  18.8% | batch:         3 of        16\t|\tloss: 3479\n",
      "Evaluating Epoch 26  25.0% | batch:         4 of        16\t|\tloss: 1752.01\n",
      "Evaluating Epoch 26  31.2% | batch:         5 of        16\t|\tloss: 2313.39\n",
      "Evaluating Epoch 26  37.5% | batch:         6 of        16\t|\tloss: 2983.07\n",
      "Evaluating Epoch 26  43.8% | batch:         7 of        16\t|\tloss: 2283.08\n",
      "Evaluating Epoch 26  50.0% | batch:         8 of        16\t|\tloss: 1857.02\n",
      "Evaluating Epoch 26  56.2% | batch:         9 of        16\t|\tloss: 6963.91\n",
      "Evaluating Epoch 26  62.5% | batch:        10 of        16\t|\tloss: 2007.95\n",
      "Evaluating Epoch 26  68.8% | batch:        11 of        16\t|\tloss: 2328.16\n",
      "Evaluating Epoch 26  75.0% | batch:        12 of        16\t|\tloss: 982.646\n",
      "Evaluating Epoch 26  81.2% | batch:        13 of        16\t|\tloss: 1207.62\n",
      "Evaluating Epoch 26  87.5% | batch:        14 of        16\t|\tloss: 2104.59\n",
      "Evaluating Epoch 26  93.8% | batch:        15 of        16\t|\tloss: 3885.07\n",
      "\n",
      "Training Epoch 27   0.0% | batch:         0 of        64\t|\tloss: 1121.31\n",
      "Training Epoch 27   1.6% | batch:         1 of        64\t|\tloss: 581.849\n",
      "Training Epoch 27   3.1% | batch:         2 of        64\t|\tloss: 1501.58\n",
      "Training Epoch 27   4.7% | batch:         3 of        64\t|\tloss: 2665.82\n",
      "Training Epoch 27   6.2% | batch:         4 of        64\t|\tloss: 1621.19\n",
      "Training Epoch 27   7.8% | batch:         5 of        64\t|\tloss: 2060.38\n",
      "Training Epoch 27   9.4% | batch:         6 of        64\t|\tloss: 2396.64\n",
      "Training Epoch 27  10.9% | batch:         7 of        64\t|\tloss: 1706.64\n",
      "Training Epoch 27  12.5% | batch:         8 of        64\t|\tloss: 2207.08\n",
      "Training Epoch 27  14.1% | batch:         9 of        64\t|\tloss: 1572.07\n",
      "Training Epoch 27  15.6% | batch:        10 of        64\t|\tloss: 3039.74\n",
      "Training Epoch 27  17.2% | batch:        11 of        64\t|\tloss: 2447.55\n",
      "Training Epoch 27  18.8% | batch:        12 of        64\t|\tloss: 6197.3\n",
      "Training Epoch 27  20.3% | batch:        13 of        64\t|\tloss: 2113.73\n",
      "Training Epoch 27  21.9% | batch:        14 of        64\t|\tloss: 1757.42\n",
      "Training Epoch 27  23.4% | batch:        15 of        64\t|\tloss: 3601.78\n",
      "Training Epoch 27  25.0% | batch:        16 of        64\t|\tloss: 2307.91\n",
      "Training Epoch 27  26.6% | batch:        17 of        64\t|\tloss: 1811.86\n",
      "Training Epoch 27  28.1% | batch:        18 of        64\t|\tloss: 1344.04\n",
      "Training Epoch 27  29.7% | batch:        19 of        64\t|\tloss: 1405.62\n",
      "Training Epoch 27  31.2% | batch:        20 of        64\t|\tloss: 5054.23\n",
      "Training Epoch 27  32.8% | batch:        21 of        64\t|\tloss: 1243.9\n",
      "Training Epoch 27  34.4% | batch:        22 of        64\t|\tloss: 2768.31\n",
      "Training Epoch 27  35.9% | batch:        23 of        64\t|\tloss: 1523\n",
      "Training Epoch 27  37.5% | batch:        24 of        64\t|\tloss: 1704.73\n",
      "Training Epoch 27  39.1% | batch:        25 of        64\t|\tloss: 1416.46\n",
      "Training Epoch 27  40.6% | batch:        26 of        64\t|\tloss: 1442.52\n",
      "Training Epoch 27  42.2% | batch:        27 of        64\t|\tloss: 1076.73\n",
      "Training Epoch 27  43.8% | batch:        28 of        64\t|\tloss: 2608.67\n",
      "Training Epoch 27  45.3% | batch:        29 of        64\t|\tloss: 1628.54\n",
      "Training Epoch 27  46.9% | batch:        30 of        64\t|\tloss: 1341.25\n",
      "Training Epoch 27  48.4% | batch:        31 of        64\t|\tloss: 1472.92\n",
      "Training Epoch 27  50.0% | batch:        32 of        64\t|\tloss: 947.997\n",
      "Training Epoch 27  51.6% | batch:        33 of        64\t|\tloss: 1683.86\n",
      "Training Epoch 27  53.1% | batch:        34 of        64\t|\tloss: 2080.7\n",
      "Training Epoch 27  54.7% | batch:        35 of        64\t|\tloss: 1529.72\n",
      "Training Epoch 27  56.2% | batch:        36 of        64\t|\tloss: 1335.58\n",
      "Training Epoch 27  57.8% | batch:        37 of        64\t|\tloss: 7871.86\n",
      "Training Epoch 27  59.4% | batch:        38 of        64\t|\tloss: 2073.66\n",
      "Training Epoch 27  60.9% | batch:        39 of        64\t|\tloss: 2050.23\n",
      "Training Epoch 27  62.5% | batch:        40 of        64\t|\tloss: 1042.62\n",
      "Training Epoch 27  64.1% | batch:        41 of        64\t|\tloss: 2862.8\n",
      "Training Epoch 27  65.6% | batch:        42 of        64\t|\tloss: 2001.83\n",
      "Training Epoch 27  67.2% | batch:        43 of        64\t|\tloss: 2178.98\n",
      "Training Epoch 27  68.8% | batch:        44 of        64\t|\tloss: 2910.23\n",
      "Training Epoch 27  70.3% | batch:        45 of        64\t|\tloss: 1320.11\n",
      "Training Epoch 27  71.9% | batch:        46 of        64\t|\tloss: 958.784\n",
      "Training Epoch 27  73.4% | batch:        47 of        64\t|\tloss: 2232.08\n",
      "Training Epoch 27  75.0% | batch:        48 of        64\t|\tloss: 2370.95\n",
      "Training Epoch 27  76.6% | batch:        49 of        64\t|\tloss: 3063.29\n",
      "Training Epoch 27  78.1% | batch:        50 of        64\t|\tloss: 1275.3\n",
      "Training Epoch 27  79.7% | batch:        51 of        64\t|\tloss: 3270.78\n",
      "Training Epoch 27  81.2% | batch:        52 of        64\t|\tloss: 1256.02\n",
      "Training Epoch 27  82.8% | batch:        53 of        64\t|\tloss: 5505.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:30,076 | INFO : Epoch 27 Training Summary: epoch: 27.000000 | loss: 2143.448099 | \n",
      "2023-05-10 17:08:30,077 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.3040602207183838 seconds\n",
      "\n",
      "2023-05-10 17:08:30,078 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2140694547582556 seconds\n",
      "2023-05-10 17:08:30,078 | INFO : Avg batch train. time: 0.018969835230597743 seconds\n",
      "2023-05-10 17:08:30,078 | INFO : Avg sample train. time: 0.00030066108339728964 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 27  84.4% | batch:        54 of        64\t|\tloss: 1189.37\n",
      "Training Epoch 27  85.9% | batch:        55 of        64\t|\tloss: 1218.81\n",
      "Training Epoch 27  87.5% | batch:        56 of        64\t|\tloss: 1366.07\n",
      "Training Epoch 27  89.1% | batch:        57 of        64\t|\tloss: 1791.88\n",
      "Training Epoch 27  90.6% | batch:        58 of        64\t|\tloss: 1730.04\n",
      "Training Epoch 27  92.2% | batch:        59 of        64\t|\tloss: 2556.36\n",
      "Training Epoch 27  93.8% | batch:        60 of        64\t|\tloss: 2503.62\n",
      "Training Epoch 27  95.3% | batch:        61 of        64\t|\tloss: 1751.94\n",
      "Training Epoch 27  96.9% | batch:        62 of        64\t|\tloss: 1657.3\n",
      "Training Epoch 27  98.4% | batch:        63 of        64\t|\tloss: 9676.75\n",
      "\n",
      "Training Epoch 28   0.0% | batch:         0 of        64\t|\tloss: 1018.29\n",
      "Training Epoch 28   1.6% | batch:         1 of        64\t|\tloss: 2040.89\n",
      "Training Epoch 28   3.1% | batch:         2 of        64\t|\tloss: 1958.39\n",
      "Training Epoch 28   4.7% | batch:         3 of        64\t|\tloss: 1436.86\n",
      "Training Epoch 28   6.2% | batch:         4 of        64\t|\tloss: 1579.94\n",
      "Training Epoch 28   7.8% | batch:         5 of        64\t|\tloss: 3581.18\n",
      "Training Epoch 28   9.4% | batch:         6 of        64\t|\tloss: 1385.68\n",
      "Training Epoch 28  10.9% | batch:         7 of        64\t|\tloss: 1229.02\n",
      "Training Epoch 28  12.5% | batch:         8 of        64\t|\tloss: 1906.49\n",
      "Training Epoch 28  14.1% | batch:         9 of        64\t|\tloss: 2721.53\n",
      "Training Epoch 28  15.6% | batch:        10 of        64\t|\tloss: 2487.34\n",
      "Training Epoch 28  17.2% | batch:        11 of        64\t|\tloss: 1076.42\n",
      "Training Epoch 28  18.8% | batch:        12 of        64\t|\tloss: 2201.13\n",
      "Training Epoch 28  20.3% | batch:        13 of        64\t|\tloss: 1899.83\n",
      "Training Epoch 28  21.9% | batch:        14 of        64\t|\tloss: 3116.65\n",
      "Training Epoch 28  23.4% | batch:        15 of        64\t|\tloss: 1393.99\n",
      "Training Epoch 28  25.0% | batch:        16 of        64\t|\tloss: 2175.65\n",
      "Training Epoch 28  26.6% | batch:        17 of        64\t|\tloss: 1396.31\n",
      "Training Epoch 28  28.1% | batch:        18 of        64\t|\tloss: 3451.05\n",
      "Training Epoch 28  29.7% | batch:        19 of        64\t|\tloss: 3017.53\n",
      "Training Epoch 28  31.2% | batch:        20 of        64\t|\tloss: 1203.48\n",
      "Training Epoch 28  32.8% | batch:        21 of        64\t|\tloss: 1990.56\n",
      "Training Epoch 28  34.4% | batch:        22 of        64\t|\tloss: 2095.35\n",
      "Training Epoch 28  35.9% | batch:        23 of        64\t|\tloss: 1141.19\n",
      "Training Epoch 28  37.5% | batch:        24 of        64\t|\tloss: 3295.33\n",
      "Training Epoch 28  39.1% | batch:        25 of        64\t|\tloss: 1263.75\n",
      "Training Epoch 28  40.6% | batch:        26 of        64\t|\tloss: 1412.67\n",
      "Training Epoch 28  42.2% | batch:        27 of        64\t|\tloss: 2276.8\n",
      "Training Epoch 28  43.8% | batch:        28 of        64\t|\tloss: 1373.13\n",
      "Training Epoch 28  45.3% | batch:        29 of        64\t|\tloss: 2328.17\n",
      "Training Epoch 28  46.9% | batch:        30 of        64\t|\tloss: 2046.3\n",
      "Training Epoch 28  48.4% | batch:        31 of        64\t|\tloss: 1261.94\n",
      "Training Epoch 28  50.0% | batch:        32 of        64\t|\tloss: 1283.65\n",
      "Training Epoch 28  51.6% | batch:        33 of        64\t|\tloss: 760.691\n",
      "Training Epoch 28  53.1% | batch:        34 of        64\t|\tloss: 1532.51\n",
      "Training Epoch 28  54.7% | batch:        35 of        64\t|\tloss: 2667.49\n",
      "Training Epoch 28  56.2% | batch:        36 of        64\t|\tloss: 4687.19\n",
      "Training Epoch 28  57.8% | batch:        37 of        64\t|\tloss: 1639.19\n",
      "Training Epoch 28  59.4% | batch:        38 of        64\t|\tloss: 2537\n",
      "Training Epoch 28  60.9% | batch:        39 of        64\t|\tloss: 1653.61\n",
      "Training Epoch 28  62.5% | batch:        40 of        64\t|\tloss: 3547.31\n",
      "Training Epoch 28  64.1% | batch:        41 of        64\t|\tloss: 1387.29\n",
      "Training Epoch 28  65.6% | batch:        42 of        64\t|\tloss: 2112.75\n",
      "Training Epoch 28  67.2% | batch:        43 of        64\t|\tloss: 1226.39\n",
      "Training Epoch 28  68.8% | batch:        44 of        64\t|\tloss: 1241.4\n",
      "Training Epoch 28  70.3% | batch:        45 of        64\t|\tloss: 1545\n",
      "Training Epoch 28  71.9% | batch:        46 of        64\t|\tloss: 1718.76\n",
      "Training Epoch 28  73.4% | batch:        47 of        64\t|\tloss: 1114.32\n",
      "Training Epoch 28  75.0% | batch:        48 of        64\t|\tloss: 2202.93\n",
      "Training Epoch 28  76.6% | batch:        49 of        64\t|\tloss: 2202.44\n",
      "Training Epoch 28  78.1% | batch:        50 of        64\t|\tloss: 2202.41\n",
      "Training Epoch 28  79.7% | batch:        51 of        64\t|\tloss: 2077.84\n",
      "Training Epoch 28  81.2% | batch:        52 of        64\t|\tloss: 1895.54\n",
      "Training Epoch 28  82.8% | batch:        53 of        64\t|\tloss: 2209.71\n",
      "Training Epoch 28  84.4% | batch:        54 of        64\t|\tloss: 1250.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:31,321 | INFO : Epoch 28 Training Summary: epoch: 28.000000 | loss: 2091.530544 | \n",
      "2023-05-10 17:08:31,322 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.232015609741211 seconds\n",
      "\n",
      "2023-05-10 17:08:31,323 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2147103888647897 seconds\n",
      "2023-05-10 17:08:31,323 | INFO : Avg batch train. time: 0.01897984982601234 seconds\n",
      "2023-05-10 17:08:31,323 | INFO : Avg sample train. time: 0.00030081980903040855 seconds\n",
      "2023-05-10 17:08:31,324 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 28  85.9% | batch:        55 of        64\t|\tloss: 6120.96\n",
      "Training Epoch 28  87.5% | batch:        56 of        64\t|\tloss: 1625.42\n",
      "Training Epoch 28  89.1% | batch:        57 of        64\t|\tloss: 7407.37\n",
      "Training Epoch 28  90.6% | batch:        58 of        64\t|\tloss: 2110.97\n",
      "Training Epoch 28  92.2% | batch:        59 of        64\t|\tloss: 1241.91\n",
      "Training Epoch 28  93.8% | batch:        60 of        64\t|\tloss: 1969.12\n",
      "Training Epoch 28  95.3% | batch:        61 of        64\t|\tloss: 1476.66\n",
      "Training Epoch 28  96.9% | batch:        62 of        64\t|\tloss: 2214.03\n",
      "Training Epoch 28  98.4% | batch:        63 of        64\t|\tloss: 3592.42\n",
      "\n",
      "Evaluating Epoch 28   0.0% | batch:         0 of        16\t|\tloss: 1639.25\n",
      "Evaluating Epoch 28   6.2% | batch:         1 of        16\t|\tloss: 1826.09\n",
      "Evaluating Epoch 28  12.5% | batch:         2 of        16\t|\tloss: 1371.49\n",
      "Evaluating Epoch 28  18.8% | batch:         3 of        16\t|\tloss: 2041.29\n",
      "Evaluating Epoch 28  25.0% | batch:         4 of        16\t|\tloss: 1244.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:31,482 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.15767264366149902 seconds\n",
      "\n",
      "2023-05-10 17:08:31,483 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.19274531304836273 seconds\n",
      "2023-05-10 17:08:31,483 | INFO : Avg batch val. time: 0.01204658206552267 seconds\n",
      "2023-05-10 17:08:31,483 | INFO : Avg sample val. time: 0.00019083694361224032 seconds\n",
      "2023-05-10 17:08:31,484 | INFO : Epoch 28 Validation Summary: epoch: 28.000000 | loss: 2264.863637 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 28  31.2% | batch:         5 of        16\t|\tloss: 1999.28\n",
      "Evaluating Epoch 28  37.5% | batch:         6 of        16\t|\tloss: 2665.36\n",
      "Evaluating Epoch 28  43.8% | batch:         7 of        16\t|\tloss: 2517.83\n",
      "Evaluating Epoch 28  50.0% | batch:         8 of        16\t|\tloss: 1727.71\n",
      "Evaluating Epoch 28  56.2% | batch:         9 of        16\t|\tloss: 6160.29\n",
      "Evaluating Epoch 28  62.5% | batch:        10 of        16\t|\tloss: 2147.39\n",
      "Evaluating Epoch 28  68.8% | batch:        11 of        16\t|\tloss: 3029.89\n",
      "Evaluating Epoch 28  75.0% | batch:        12 of        16\t|\tloss: 881.997\n",
      "Evaluating Epoch 28  81.2% | batch:        13 of        16\t|\tloss: 1471.35\n",
      "Evaluating Epoch 28  87.5% | batch:        14 of        16\t|\tloss: 2078.35\n",
      "Evaluating Epoch 28  93.8% | batch:        15 of        16\t|\tloss: 3764.24\n",
      "\n",
      "Training Epoch 29   0.0% | batch:         0 of        64\t|\tloss: 1328.16\n",
      "Training Epoch 29   1.6% | batch:         1 of        64\t|\tloss: 1661.59\n",
      "Training Epoch 29   3.1% | batch:         2 of        64\t|\tloss: 1483.01\n",
      "Training Epoch 29   4.7% | batch:         3 of        64\t|\tloss: 2130.63\n",
      "Training Epoch 29   6.2% | batch:         4 of        64\t|\tloss: 2029.82\n",
      "Training Epoch 29   7.8% | batch:         5 of        64\t|\tloss: 2000.94\n",
      "Training Epoch 29   9.4% | batch:         6 of        64\t|\tloss: 1438.26\n",
      "Training Epoch 29  10.9% | batch:         7 of        64\t|\tloss: 2153.81\n",
      "Training Epoch 29  12.5% | batch:         8 of        64\t|\tloss: 2429.83\n",
      "Training Epoch 29  14.1% | batch:         9 of        64\t|\tloss: 2490.92\n",
      "Training Epoch 29  15.6% | batch:        10 of        64\t|\tloss: 1890.57\n",
      "Training Epoch 29  17.2% | batch:        11 of        64\t|\tloss: 1979.64\n",
      "Training Epoch 29  18.8% | batch:        12 of        64\t|\tloss: 997.186\n",
      "Training Epoch 29  20.3% | batch:        13 of        64\t|\tloss: 2315.61\n",
      "Training Epoch 29  21.9% | batch:        14 of        64\t|\tloss: 3114.79\n",
      "Training Epoch 29  23.4% | batch:        15 of        64\t|\tloss: 1225.09\n",
      "Training Epoch 29  25.0% | batch:        16 of        64\t|\tloss: 2719.26\n",
      "Training Epoch 29  26.6% | batch:        17 of        64\t|\tloss: 2582.86\n",
      "Training Epoch 29  28.1% | batch:        18 of        64\t|\tloss: 2495.16\n",
      "Training Epoch 29  29.7% | batch:        19 of        64\t|\tloss: 2591.88\n",
      "Training Epoch 29  31.2% | batch:        20 of        64\t|\tloss: 1677.1\n",
      "Training Epoch 29  32.8% | batch:        21 of        64\t|\tloss: 2156.83\n",
      "Training Epoch 29  34.4% | batch:        22 of        64\t|\tloss: 1242.24\n",
      "Training Epoch 29  35.9% | batch:        23 of        64\t|\tloss: 3060.34\n",
      "Training Epoch 29  37.5% | batch:        24 of        64\t|\tloss: 1393.33\n",
      "Training Epoch 29  39.1% | batch:        25 of        64\t|\tloss: 1274.81\n",
      "Training Epoch 29  40.6% | batch:        26 of        64\t|\tloss: 1330.38\n",
      "Training Epoch 29  42.2% | batch:        27 of        64\t|\tloss: 1755.47\n",
      "Training Epoch 29  43.8% | batch:        28 of        64\t|\tloss: 752.294\n",
      "Training Epoch 29  45.3% | batch:        29 of        64\t|\tloss: 1376.87\n",
      "Training Epoch 29  46.9% | batch:        30 of        64\t|\tloss: 1688.8\n",
      "Training Epoch 29  48.4% | batch:        31 of        64\t|\tloss: 3330.02\n",
      "Training Epoch 29  50.0% | batch:        32 of        64\t|\tloss: 1819.58\n",
      "Training Epoch 29  51.6% | batch:        33 of        64\t|\tloss: 810.834\n",
      "Training Epoch 29  53.1% | batch:        34 of        64\t|\tloss: 1880.17\n",
      "Training Epoch 29  54.7% | batch:        35 of        64\t|\tloss: 3707.83\n",
      "Training Epoch 29  56.2% | batch:        36 of        64\t|\tloss: 1274.67\n",
      "Training Epoch 29  57.8% | batch:        37 of        64\t|\tloss: 2561.61\n",
      "Training Epoch 29  59.4% | batch:        38 of        64\t|\tloss: 2652.17\n",
      "Training Epoch 29  60.9% | batch:        39 of        64\t|\tloss: 4135.86\n",
      "Training Epoch 29  62.5% | batch:        40 of        64\t|\tloss: 1446.8\n",
      "Training Epoch 29  64.1% | batch:        41 of        64\t|\tloss: 2059.62\n",
      "Training Epoch 29  65.6% | batch:        42 of        64\t|\tloss: 1690.63\n",
      "Training Epoch 29  67.2% | batch:        43 of        64\t|\tloss: 2887.82\n",
      "Training Epoch 29  68.8% | batch:        44 of        64\t|\tloss: 943.78\n",
      "Training Epoch 29  70.3% | batch:        45 of        64\t|\tloss: 1243.84\n",
      "Training Epoch 29  71.9% | batch:        46 of        64\t|\tloss: 4020.55\n",
      "Training Epoch 29  73.4% | batch:        47 of        64\t|\tloss: 1070.94\n",
      "Training Epoch 29  75.0% | batch:        48 of        64\t|\tloss: 1615.13\n",
      "Training Epoch 29  76.6% | batch:        49 of        64\t|\tloss: 1318.09\n",
      "Training Epoch 29  78.1% | batch:        50 of        64\t|\tloss: 1268.94\n",
      "Training Epoch 29  79.7% | batch:        51 of        64\t|\tloss: 1462.56\n",
      "Training Epoch 29  81.2% | batch:        52 of        64\t|\tloss: 1201.33\n",
      "Training Epoch 29  82.8% | batch:        53 of        64\t|\tloss: 1484.25\n",
      "Training Epoch 29  84.4% | batch:        54 of        64\t|\tloss: 2360.42\n",
      "Training Epoch 29  85.9% | batch:        55 of        64\t|\tloss: 1770.55\n",
      "Training Epoch 29  87.5% | batch:        56 of        64\t|\tloss: 1377.91\n",
      "Training Epoch 29  89.1% | batch:        57 of        64\t|\tloss: 2159.55\n",
      "Training Epoch 29  90.6% | batch:        58 of        64\t|\tloss: 1169.55\n",
      "Training Epoch 29  92.2% | batch:        59 of        64\t|\tloss: 936.502\n",
      "Training Epoch 29  93.8% | batch:        60 of        64\t|\tloss: 6409.74\n",
      "Training Epoch 29  95.3% | batch:        61 of        64\t|\tloss: 2904.7\n",
      "Training Epoch 29  96.9% | batch:        62 of        64\t|\tloss: 958.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:32,646 | INFO : Epoch 29 Training Summary: epoch: 29.000000 | loss: 1979.580086 | \n",
      "2023-05-10 17:08:32,647 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1529591083526611 seconds\n",
      "\n",
      "2023-05-10 17:08:32,647 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2125810343643715 seconds\n",
      "2023-05-10 17:08:32,648 | INFO : Avg batch train. time: 0.018946578661943304 seconds\n",
      "2023-05-10 17:08:32,648 | INFO : Avg sample train. time: 0.0003002924800308003 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 29  98.4% | batch:        63 of        64\t|\tloss: 2098.72\n",
      "\n",
      "Training Epoch 30   0.0% | batch:         0 of        64\t|\tloss: 1107.03\n",
      "Training Epoch 30   1.6% | batch:         1 of        64\t|\tloss: 732.875\n",
      "Training Epoch 30   3.1% | batch:         2 of        64\t|\tloss: 1928.39\n",
      "Training Epoch 30   4.7% | batch:         3 of        64\t|\tloss: 1530.15\n",
      "Training Epoch 30   6.2% | batch:         4 of        64\t|\tloss: 1882.65\n",
      "Training Epoch 30   7.8% | batch:         5 of        64\t|\tloss: 2181.09\n",
      "Training Epoch 30   9.4% | batch:         6 of        64\t|\tloss: 1474.27\n",
      "Training Epoch 30  10.9% | batch:         7 of        64\t|\tloss: 1959.17\n",
      "Training Epoch 30  12.5% | batch:         8 of        64\t|\tloss: 1997.49\n",
      "Training Epoch 30  14.1% | batch:         9 of        64\t|\tloss: 2081.08\n",
      "Training Epoch 30  15.6% | batch:        10 of        64\t|\tloss: 1355.35\n",
      "Training Epoch 30  17.2% | batch:        11 of        64\t|\tloss: 1397.16\n",
      "Training Epoch 30  18.8% | batch:        12 of        64\t|\tloss: 1428.98\n",
      "Training Epoch 30  20.3% | batch:        13 of        64\t|\tloss: 2350.67\n",
      "Training Epoch 30  21.9% | batch:        14 of        64\t|\tloss: 1549.41\n",
      "Training Epoch 30  23.4% | batch:        15 of        64\t|\tloss: 1689.27\n",
      "Training Epoch 30  25.0% | batch:        16 of        64\t|\tloss: 5949.89\n",
      "Training Epoch 30  26.6% | batch:        17 of        64\t|\tloss: 2068.41\n",
      "Training Epoch 30  28.1% | batch:        18 of        64\t|\tloss: 1121.19\n",
      "Training Epoch 30  29.7% | batch:        19 of        64\t|\tloss: 1772.14\n",
      "Training Epoch 30  31.2% | batch:        20 of        64\t|\tloss: 2102.48\n",
      "Training Epoch 30  32.8% | batch:        21 of        64\t|\tloss: 1323.91\n",
      "Training Epoch 30  34.4% | batch:        22 of        64\t|\tloss: 1152.38\n",
      "Training Epoch 30  35.9% | batch:        23 of        64\t|\tloss: 935.016\n",
      "Training Epoch 30  37.5% | batch:        24 of        64\t|\tloss: 4139.33\n",
      "Training Epoch 30  39.1% | batch:        25 of        64\t|\tloss: 1528.63\n",
      "Training Epoch 30  40.6% | batch:        26 of        64\t|\tloss: 1608.74\n",
      "Training Epoch 30  42.2% | batch:        27 of        64\t|\tloss: 1408.99\n",
      "Training Epoch 30  43.8% | batch:        28 of        64\t|\tloss: 1591.44\n",
      "Training Epoch 30  45.3% | batch:        29 of        64\t|\tloss: 2712.92\n",
      "Training Epoch 30  46.9% | batch:        30 of        64\t|\tloss: 2282.95\n",
      "Training Epoch 30  48.4% | batch:        31 of        64\t|\tloss: 1376.49\n",
      "Training Epoch 30  50.0% | batch:        32 of        64\t|\tloss: 634.001\n",
      "Training Epoch 30  51.6% | batch:        33 of        64\t|\tloss: 1926.85\n",
      "Training Epoch 30  53.1% | batch:        34 of        64\t|\tloss: 1659.88\n",
      "Training Epoch 30  54.7% | batch:        35 of        64\t|\tloss: 1225.66\n",
      "Training Epoch 30  56.2% | batch:        36 of        64\t|\tloss: 2353.92\n",
      "Training Epoch 30  57.8% | batch:        37 of        64\t|\tloss: 1470.94\n",
      "Training Epoch 30  59.4% | batch:        38 of        64\t|\tloss: 2845.45\n",
      "Training Epoch 30  60.9% | batch:        39 of        64\t|\tloss: 1608.39\n",
      "Training Epoch 30  62.5% | batch:        40 of        64\t|\tloss: 1544.1\n",
      "Training Epoch 30  64.1% | batch:        41 of        64\t|\tloss: 1722.03\n",
      "Training Epoch 30  65.6% | batch:        42 of        64\t|\tloss: 1449.65\n",
      "Training Epoch 30  67.2% | batch:        43 of        64\t|\tloss: 1798.56\n",
      "Training Epoch 30  68.8% | batch:        44 of        64\t|\tloss: 1564.99\n",
      "Training Epoch 30  70.3% | batch:        45 of        64\t|\tloss: 1753.84\n",
      "Training Epoch 30  71.9% | batch:        46 of        64\t|\tloss: 2550.43\n",
      "Training Epoch 30  73.4% | batch:        47 of        64\t|\tloss: 1717.8\n",
      "Training Epoch 30  75.0% | batch:        48 of        64\t|\tloss: 966.835\n",
      "Training Epoch 30  76.6% | batch:        49 of        64\t|\tloss: 1707.4\n",
      "Training Epoch 30  78.1% | batch:        50 of        64\t|\tloss: 2214.02\n",
      "Training Epoch 30  79.7% | batch:        51 of        64\t|\tloss: 2080.83\n",
      "Training Epoch 30  81.2% | batch:        52 of        64\t|\tloss: 1183.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:33,895 | INFO : Epoch 30 Training Summary: epoch: 30.000000 | loss: 1869.069557 | \n",
      "2023-05-10 17:08:33,896 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.237900733947754 seconds\n",
      "\n",
      "2023-05-10 17:08:33,897 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2134250243504843 seconds\n",
      "2023-05-10 17:08:33,897 | INFO : Avg batch train. time: 0.018959766005476317 seconds\n",
      "2023-05-10 17:08:33,897 | INFO : Avg sample train. time: 0.00030050149191443394 seconds\n",
      "2023-05-10 17:08:33,898 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 30  82.8% | batch:        53 of        64\t|\tloss: 1683.91\n",
      "Training Epoch 30  84.4% | batch:        54 of        64\t|\tloss: 1700.51\n",
      "Training Epoch 30  85.9% | batch:        55 of        64\t|\tloss: 1643.79\n",
      "Training Epoch 30  87.5% | batch:        56 of        64\t|\tloss: 3178.64\n",
      "Training Epoch 30  89.1% | batch:        57 of        64\t|\tloss: 1842.14\n",
      "Training Epoch 30  90.6% | batch:        58 of        64\t|\tloss: 3330.05\n",
      "Training Epoch 30  92.2% | batch:        59 of        64\t|\tloss: 1576.14\n",
      "Training Epoch 30  93.8% | batch:        60 of        64\t|\tloss: 2796.97\n",
      "Training Epoch 30  95.3% | batch:        61 of        64\t|\tloss: 2343.4\n",
      "Training Epoch 30  96.9% | batch:        62 of        64\t|\tloss: 1942.58\n",
      "Training Epoch 30  98.4% | batch:        63 of        64\t|\tloss: 2016.43\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:34,080 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.18126535415649414 seconds\n",
      "\n",
      "2023-05-10 17:08:34,080 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.19207002134884105 seconds\n",
      "2023-05-10 17:08:34,081 | INFO : Avg batch val. time: 0.012004376334302565 seconds\n",
      "2023-05-10 17:08:34,081 | INFO : Avg sample val. time: 0.00019016833796914955 seconds\n",
      "2023-05-10 17:08:34,081 | INFO : Epoch 30 Validation Summary: epoch: 30.000000 | loss: 2153.351524 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 30   0.0% | batch:         0 of        16\t|\tloss: 1559.44\n",
      "Evaluating Epoch 30   6.2% | batch:         1 of        16\t|\tloss: 1233.75\n",
      "Evaluating Epoch 30  12.5% | batch:         2 of        16\t|\tloss: 983.651\n",
      "Evaluating Epoch 30  18.8% | batch:         3 of        16\t|\tloss: 1902.95\n",
      "Evaluating Epoch 30  25.0% | batch:         4 of        16\t|\tloss: 1838.18\n",
      "Evaluating Epoch 30  31.2% | batch:         5 of        16\t|\tloss: 2239.71\n",
      "Evaluating Epoch 30  37.5% | batch:         6 of        16\t|\tloss: 2049.29\n",
      "Evaluating Epoch 30  43.8% | batch:         7 of        16\t|\tloss: 2034.92\n",
      "Evaluating Epoch 30  50.0% | batch:         8 of        16\t|\tloss: 1630.04\n",
      "Evaluating Epoch 30  56.2% | batch:         9 of        16\t|\tloss: 7314.58\n",
      "Evaluating Epoch 30  62.5% | batch:        10 of        16\t|\tloss: 1771.8\n",
      "Evaluating Epoch 30  68.8% | batch:        11 of        16\t|\tloss: 3082.84\n",
      "Evaluating Epoch 30  75.0% | batch:        12 of        16\t|\tloss: 905.853\n",
      "Evaluating Epoch 30  81.2% | batch:        13 of        16\t|\tloss: 1084.73\n",
      "Evaluating Epoch 30  87.5% | batch:        14 of        16\t|\tloss: 2287.74\n",
      "Evaluating Epoch 30  93.8% | batch:        15 of        16\t|\tloss: 2640.79\n",
      "\n",
      "Training Epoch 31   0.0% | batch:         0 of        64\t|\tloss: 1580.92\n",
      "Training Epoch 31   1.6% | batch:         1 of        64\t|\tloss: 3204.19\n",
      "Training Epoch 31   3.1% | batch:         2 of        64\t|\tloss: 1003.25\n",
      "Training Epoch 31   4.7% | batch:         3 of        64\t|\tloss: 1879.2\n",
      "Training Epoch 31   6.2% | batch:         4 of        64\t|\tloss: 868.131\n",
      "Training Epoch 31   7.8% | batch:         5 of        64\t|\tloss: 1743.75\n",
      "Training Epoch 31   9.4% | batch:         6 of        64\t|\tloss: 1281.76\n",
      "Training Epoch 31  10.9% | batch:         7 of        64\t|\tloss: 1893.14\n",
      "Training Epoch 31  12.5% | batch:         8 of        64\t|\tloss: 2694.11\n",
      "Training Epoch 31  14.1% | batch:         9 of        64\t|\tloss: 1858.68\n",
      "Training Epoch 31  15.6% | batch:        10 of        64\t|\tloss: 922.444\n",
      "Training Epoch 31  17.2% | batch:        11 of        64\t|\tloss: 2692.06\n",
      "Training Epoch 31  18.8% | batch:        12 of        64\t|\tloss: 1191.61\n",
      "Training Epoch 31  20.3% | batch:        13 of        64\t|\tloss: 3004.04\n",
      "Training Epoch 31  21.9% | batch:        14 of        64\t|\tloss: 2695.19\n",
      "Training Epoch 31  23.4% | batch:        15 of        64\t|\tloss: 2259.25\n",
      "Training Epoch 31  25.0% | batch:        16 of        64\t|\tloss: 2693.06\n",
      "Training Epoch 31  26.6% | batch:        17 of        64\t|\tloss: 1935.81\n",
      "Training Epoch 31  28.1% | batch:        18 of        64\t|\tloss: 1970.51\n",
      "Training Epoch 31  29.7% | batch:        19 of        64\t|\tloss: 1557.09\n",
      "Training Epoch 31  31.2% | batch:        20 of        64\t|\tloss: 1990.28\n",
      "Training Epoch 31  32.8% | batch:        21 of        64\t|\tloss: 2909\n",
      "Training Epoch 31  34.4% | batch:        22 of        64\t|\tloss: 1374.7\n",
      "Training Epoch 31  35.9% | batch:        23 of        64\t|\tloss: 1440.09\n",
      "Training Epoch 31  37.5% | batch:        24 of        64\t|\tloss: 1512.27\n",
      "Training Epoch 31  39.1% | batch:        25 of        64\t|\tloss: 2489.89\n",
      "Training Epoch 31  40.6% | batch:        26 of        64\t|\tloss: 2580.23\n",
      "Training Epoch 31  42.2% | batch:        27 of        64\t|\tloss: 5534.39\n",
      "Training Epoch 31  43.8% | batch:        28 of        64\t|\tloss: 2007.99\n",
      "Training Epoch 31  45.3% | batch:        29 of        64\t|\tloss: 1364.59\n",
      "Training Epoch 31  46.9% | batch:        30 of        64\t|\tloss: 1906.62\n",
      "Training Epoch 31  48.4% | batch:        31 of        64\t|\tloss: 1813.56\n",
      "Training Epoch 31  50.0% | batch:        32 of        64\t|\tloss: 1817.71\n",
      "Training Epoch 31  51.6% | batch:        33 of        64\t|\tloss: 2245.5\n",
      "Training Epoch 31  53.1% | batch:        34 of        64\t|\tloss: 1044.5\n",
      "Training Epoch 31  54.7% | batch:        35 of        64\t|\tloss: 784.613\n",
      "Training Epoch 31  56.2% | batch:        36 of        64\t|\tloss: 1240.87\n",
      "Training Epoch 31  57.8% | batch:        37 of        64\t|\tloss: 1646.89\n",
      "Training Epoch 31  59.4% | batch:        38 of        64\t|\tloss: 1822.09\n",
      "Training Epoch 31  60.9% | batch:        39 of        64\t|\tloss: 1911.99\n",
      "Training Epoch 31  62.5% | batch:        40 of        64\t|\tloss: 2655.09\n",
      "Training Epoch 31  64.1% | batch:        41 of        64\t|\tloss: 1268.59\n",
      "Training Epoch 31  65.6% | batch:        42 of        64\t|\tloss: 1679.28\n",
      "Training Epoch 31  67.2% | batch:        43 of        64\t|\tloss: 2466.94\n",
      "Training Epoch 31  68.8% | batch:        44 of        64\t|\tloss: 2872.86\n",
      "Training Epoch 31  70.3% | batch:        45 of        64\t|\tloss: 1859.34\n",
      "Training Epoch 31  71.9% | batch:        46 of        64\t|\tloss: 2124.72\n",
      "Training Epoch 31  73.4% | batch:        47 of        64\t|\tloss: 4724.85\n",
      "Training Epoch 31  75.0% | batch:        48 of        64\t|\tloss: 1289.03\n",
      "Training Epoch 31  76.6% | batch:        49 of        64\t|\tloss: 1324.12\n",
      "Training Epoch 31  78.1% | batch:        50 of        64\t|\tloss: 935.919\n",
      "Training Epoch 31  79.7% | batch:        51 of        64\t|\tloss: 3024.39\n",
      "Training Epoch 31  81.2% | batch:        52 of        64\t|\tloss: 2083.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:35,375 | INFO : Epoch 31 Training Summary: epoch: 31.000000 | loss: 2049.291840 | \n",
      "2023-05-10 17:08:35,376 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2819807529449463 seconds\n",
      "\n",
      "2023-05-10 17:08:35,376 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2156364994664346 seconds\n",
      "2023-05-10 17:08:35,377 | INFO : Avg batch train. time: 0.01899432030416304 seconds\n",
      "2023-05-10 17:08:35,377 | INFO : Avg sample train. time: 0.00030104915786687334 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 31  82.8% | batch:        53 of        64\t|\tloss: 960.038\n",
      "Training Epoch 31  84.4% | batch:        54 of        64\t|\tloss: 1228.39\n",
      "Training Epoch 31  85.9% | batch:        55 of        64\t|\tloss: 1436.6\n",
      "Training Epoch 31  87.5% | batch:        56 of        64\t|\tloss: 3544.72\n",
      "Training Epoch 31  89.1% | batch:        57 of        64\t|\tloss: 1828.34\n",
      "Training Epoch 31  90.6% | batch:        58 of        64\t|\tloss: 2296.71\n",
      "Training Epoch 31  92.2% | batch:        59 of        64\t|\tloss: 2644.56\n",
      "Training Epoch 31  93.8% | batch:        60 of        64\t|\tloss: 2672.58\n",
      "Training Epoch 31  95.3% | batch:        61 of        64\t|\tloss: 2042.59\n",
      "Training Epoch 31  96.9% | batch:        62 of        64\t|\tloss: 2310.31\n",
      "Training Epoch 31  98.4% | batch:        63 of        64\t|\tloss: 17684.3\n",
      "\n",
      "Training Epoch 32   0.0% | batch:         0 of        64\t|\tloss: 2218.73\n",
      "Training Epoch 32   1.6% | batch:         1 of        64\t|\tloss: 2241.38\n",
      "Training Epoch 32   3.1% | batch:         2 of        64\t|\tloss: 1537.26\n",
      "Training Epoch 32   4.7% | batch:         3 of        64\t|\tloss: 915.484\n",
      "Training Epoch 32   6.2% | batch:         4 of        64\t|\tloss: 1422.44\n",
      "Training Epoch 32   7.8% | batch:         5 of        64\t|\tloss: 2572.79\n",
      "Training Epoch 32   9.4% | batch:         6 of        64\t|\tloss: 1534.54\n",
      "Training Epoch 32  10.9% | batch:         7 of        64\t|\tloss: 3671.27\n",
      "Training Epoch 32  12.5% | batch:         8 of        64\t|\tloss: 1760.14\n",
      "Training Epoch 32  14.1% | batch:         9 of        64\t|\tloss: 1095.72\n",
      "Training Epoch 32  15.6% | batch:        10 of        64\t|\tloss: 1453.11\n",
      "Training Epoch 32  17.2% | batch:        11 of        64\t|\tloss: 1312.11\n",
      "Training Epoch 32  18.8% | batch:        12 of        64\t|\tloss: 2047.9\n",
      "Training Epoch 32  20.3% | batch:        13 of        64\t|\tloss: 1093.58\n",
      "Training Epoch 32  21.9% | batch:        14 of        64\t|\tloss: 1745.48\n",
      "Training Epoch 32  23.4% | batch:        15 of        64\t|\tloss: 1939.47\n",
      "Training Epoch 32  25.0% | batch:        16 of        64\t|\tloss: 1638.26\n",
      "Training Epoch 32  26.6% | batch:        17 of        64\t|\tloss: 1641.6\n",
      "Training Epoch 32  28.1% | batch:        18 of        64\t|\tloss: 1094.06\n",
      "Training Epoch 32  29.7% | batch:        19 of        64\t|\tloss: 1622.54\n",
      "Training Epoch 32  31.2% | batch:        20 of        64\t|\tloss: 1989.94\n",
      "Training Epoch 32  32.8% | batch:        21 of        64\t|\tloss: 1093.8\n",
      "Training Epoch 32  34.4% | batch:        22 of        64\t|\tloss: 1235.58\n",
      "Training Epoch 32  35.9% | batch:        23 of        64\t|\tloss: 1295.52\n",
      "Training Epoch 32  37.5% | batch:        24 of        64\t|\tloss: 1450.4\n",
      "Training Epoch 32  39.1% | batch:        25 of        64\t|\tloss: 983.419\n",
      "Training Epoch 32  40.6% | batch:        26 of        64\t|\tloss: 1303.7\n",
      "Training Epoch 32  42.2% | batch:        27 of        64\t|\tloss: 1180.87\n",
      "Training Epoch 32  43.8% | batch:        28 of        64\t|\tloss: 1980.22\n",
      "Training Epoch 32  45.3% | batch:        29 of        64\t|\tloss: 2288.45\n",
      "Training Epoch 32  46.9% | batch:        30 of        64\t|\tloss: 1415.82\n",
      "Training Epoch 32  48.4% | batch:        31 of        64\t|\tloss: 2032.09\n",
      "Training Epoch 32  50.0% | batch:        32 of        64\t|\tloss: 1357.68\n",
      "Training Epoch 32  51.6% | batch:        33 of        64\t|\tloss: 1157.1\n",
      "Training Epoch 32  53.1% | batch:        34 of        64\t|\tloss: 3037.26\n",
      "Training Epoch 32  54.7% | batch:        35 of        64\t|\tloss: 1676.95\n",
      "Training Epoch 32  56.2% | batch:        36 of        64\t|\tloss: 2597.32\n",
      "Training Epoch 32  57.8% | batch:        37 of        64\t|\tloss: 1522.48\n",
      "Training Epoch 32  59.4% | batch:        38 of        64\t|\tloss: 3150.51\n",
      "Training Epoch 32  60.9% | batch:        39 of        64\t|\tloss: 1039.5\n",
      "Training Epoch 32  62.5% | batch:        40 of        64\t|\tloss: 2581.87\n",
      "Training Epoch 32  64.1% | batch:        41 of        64\t|\tloss: 2989.02\n",
      "Training Epoch 32  65.6% | batch:        42 of        64\t|\tloss: 1560.53\n",
      "Training Epoch 32  67.2% | batch:        43 of        64\t|\tloss: 1853.29\n",
      "Training Epoch 32  68.8% | batch:        44 of        64\t|\tloss: 679.373\n",
      "Training Epoch 32  70.3% | batch:        45 of        64\t|\tloss: 3381.9\n",
      "Training Epoch 32  71.9% | batch:        46 of        64\t|\tloss: 877.589\n",
      "Training Epoch 32  73.4% | batch:        47 of        64\t|\tloss: 3847.1\n",
      "Training Epoch 32  75.0% | batch:        48 of        64\t|\tloss: 1677.95\n",
      "Training Epoch 32  76.6% | batch:        49 of        64\t|\tloss: 2407.43\n",
      "Training Epoch 32  78.1% | batch:        50 of        64\t|\tloss: 2965.62\n",
      "Training Epoch 32  79.7% | batch:        51 of        64\t|\tloss: 1313.39\n",
      "Training Epoch 32  81.2% | batch:        52 of        64\t|\tloss: 1589.47\n",
      "Training Epoch 32  82.8% | batch:        53 of        64\t|\tloss: 2881.65\n",
      "Training Epoch 32  84.4% | batch:        54 of        64\t|\tloss: 1107.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:36,605 | INFO : Epoch 32 Training Summary: epoch: 32.000000 | loss: 1874.247888 | \n",
      "2023-05-10 17:08:36,605 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2161448001861572 seconds\n",
      "\n",
      "2023-05-10 17:08:36,606 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.215652383863926 seconds\n",
      "2023-05-10 17:08:36,606 | INFO : Avg batch train. time: 0.018994568497873843 seconds\n",
      "2023-05-10 17:08:36,607 | INFO : Avg sample train. time: 0.00030105309159582117 seconds\n",
      "2023-05-10 17:08:36,607 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 32  85.9% | batch:        55 of        64\t|\tloss: 2196.74\n",
      "Training Epoch 32  87.5% | batch:        56 of        64\t|\tloss: 3007.74\n",
      "Training Epoch 32  89.1% | batch:        57 of        64\t|\tloss: 1357.97\n",
      "Training Epoch 32  90.6% | batch:        58 of        64\t|\tloss: 1140.35\n",
      "Training Epoch 32  92.2% | batch:        59 of        64\t|\tloss: 2608.23\n",
      "Training Epoch 32  93.8% | batch:        60 of        64\t|\tloss: 816.369\n",
      "Training Epoch 32  95.3% | batch:        61 of        64\t|\tloss: 1313.9\n",
      "Training Epoch 32  96.9% | batch:        62 of        64\t|\tloss: 5347.62\n",
      "Training Epoch 32  98.4% | batch:        63 of        64\t|\tloss: 4313.64\n",
      "\n",
      "Evaluating Epoch 32   0.0% | batch:         0 of        16\t|\tloss: 1797.26\n",
      "Evaluating Epoch 32   6.2% | batch:         1 of        16\t|\tloss: 1378.11\n",
      "Evaluating Epoch 32  12.5% | batch:         2 of        16\t|\tloss: 1079.59\n",
      "Evaluating Epoch 32  18.8% | batch:         3 of        16\t|\tloss: 1870.5\n",
      "Evaluating Epoch 32  25.0% | batch:         4 of        16\t|\tloss: 1591.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:36,763 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.15586066246032715 seconds\n",
      "\n",
      "2023-05-10 17:08:36,764 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.19005839029947916 seconds\n",
      "2023-05-10 17:08:36,764 | INFO : Avg batch val. time: 0.011878649393717447 seconds\n",
      "2023-05-10 17:08:36,764 | INFO : Avg sample val. time: 0.00018817662405889025 seconds\n",
      "2023-05-10 17:08:36,765 | INFO : Epoch 32 Validation Summary: epoch: 32.000000 | loss: 2291.449482 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 32  31.2% | batch:         5 of        16\t|\tloss: 2673.68\n",
      "Evaluating Epoch 32  37.5% | batch:         6 of        16\t|\tloss: 2540.32\n",
      "Evaluating Epoch 32  43.8% | batch:         7 of        16\t|\tloss: 2599.81\n",
      "Evaluating Epoch 32  50.0% | batch:         8 of        16\t|\tloss: 1599.5\n",
      "Evaluating Epoch 32  56.2% | batch:         9 of        16\t|\tloss: 7903.43\n",
      "Evaluating Epoch 32  62.5% | batch:        10 of        16\t|\tloss: 1796.58\n",
      "Evaluating Epoch 32  68.8% | batch:        11 of        16\t|\tloss: 2417.73\n",
      "Evaluating Epoch 32  75.0% | batch:        12 of        16\t|\tloss: 713.216\n",
      "Evaluating Epoch 32  81.2% | batch:        13 of        16\t|\tloss: 1168.73\n",
      "Evaluating Epoch 32  87.5% | batch:        14 of        16\t|\tloss: 2379.19\n",
      "Evaluating Epoch 32  93.8% | batch:        15 of        16\t|\tloss: 3395.81\n",
      "\n",
      "Training Epoch 33   0.0% | batch:         0 of        64\t|\tloss: 1917.37\n",
      "Training Epoch 33   1.6% | batch:         1 of        64\t|\tloss: 2368.27\n",
      "Training Epoch 33   3.1% | batch:         2 of        64\t|\tloss: 1663.07\n",
      "Training Epoch 33   4.7% | batch:         3 of        64\t|\tloss: 2158.1\n",
      "Training Epoch 33   6.2% | batch:         4 of        64\t|\tloss: 2619.18\n",
      "Training Epoch 33   7.8% | batch:         5 of        64\t|\tloss: 1671.75\n",
      "Training Epoch 33   9.4% | batch:         6 of        64\t|\tloss: 1217.25\n",
      "Training Epoch 33  10.9% | batch:         7 of        64\t|\tloss: 3066.98\n",
      "Training Epoch 33  12.5% | batch:         8 of        64\t|\tloss: 1295.54\n",
      "Training Epoch 33  14.1% | batch:         9 of        64\t|\tloss: 1661.71\n",
      "Training Epoch 33  15.6% | batch:        10 of        64\t|\tloss: 2550.27\n",
      "Training Epoch 33  17.2% | batch:        11 of        64\t|\tloss: 2003\n",
      "Training Epoch 33  18.8% | batch:        12 of        64\t|\tloss: 5492.93\n",
      "Training Epoch 33  20.3% | batch:        13 of        64\t|\tloss: 2141.86\n",
      "Training Epoch 33  21.9% | batch:        14 of        64\t|\tloss: 2108.39\n",
      "Training Epoch 33  23.4% | batch:        15 of        64\t|\tloss: 2264.14\n",
      "Training Epoch 33  25.0% | batch:        16 of        64\t|\tloss: 944.186\n",
      "Training Epoch 33  26.6% | batch:        17 of        64\t|\tloss: 1350.83\n",
      "Training Epoch 33  28.1% | batch:        18 of        64\t|\tloss: 1519.14\n",
      "Training Epoch 33  29.7% | batch:        19 of        64\t|\tloss: 1163.34\n",
      "Training Epoch 33  31.2% | batch:        20 of        64\t|\tloss: 2837.26\n",
      "Training Epoch 33  32.8% | batch:        21 of        64\t|\tloss: 1207.29\n",
      "Training Epoch 33  34.4% | batch:        22 of        64\t|\tloss: 1824.04\n",
      "Training Epoch 33  35.9% | batch:        23 of        64\t|\tloss: 2149.4\n",
      "Training Epoch 33  37.5% | batch:        24 of        64\t|\tloss: 1318.5\n",
      "Training Epoch 33  39.1% | batch:        25 of        64\t|\tloss: 1948.38\n",
      "Training Epoch 33  40.6% | batch:        26 of        64\t|\tloss: 1481.85\n",
      "Training Epoch 33  42.2% | batch:        27 of        64\t|\tloss: 1423.31\n",
      "Training Epoch 33  43.8% | batch:        28 of        64\t|\tloss: 1235.87\n",
      "Training Epoch 33  45.3% | batch:        29 of        64\t|\tloss: 1170.51\n",
      "Training Epoch 33  46.9% | batch:        30 of        64\t|\tloss: 2799.61\n",
      "Training Epoch 33  48.4% | batch:        31 of        64\t|\tloss: 2394.39\n",
      "Training Epoch 33  50.0% | batch:        32 of        64\t|\tloss: 2366.28\n",
      "Training Epoch 33  51.6% | batch:        33 of        64\t|\tloss: 1689.62\n",
      "Training Epoch 33  53.1% | batch:        34 of        64\t|\tloss: 1302.49\n",
      "Training Epoch 33  54.7% | batch:        35 of        64\t|\tloss: 900.961\n",
      "Training Epoch 33  56.2% | batch:        36 of        64\t|\tloss: 1298.48\n",
      "Training Epoch 33  57.8% | batch:        37 of        64\t|\tloss: 1560.18\n",
      "Training Epoch 33  59.4% | batch:        38 of        64\t|\tloss: 1563.9\n",
      "Training Epoch 33  60.9% | batch:        39 of        64\t|\tloss: 2140.54\n",
      "Training Epoch 33  62.5% | batch:        40 of        64\t|\tloss: 2971.19\n",
      "Training Epoch 33  64.1% | batch:        41 of        64\t|\tloss: 1964.48\n",
      "Training Epoch 33  65.6% | batch:        42 of        64\t|\tloss: 1901.95\n",
      "Training Epoch 33  67.2% | batch:        43 of        64\t|\tloss: 1724.32\n",
      "Training Epoch 33  68.8% | batch:        44 of        64\t|\tloss: 2184.83\n",
      "Training Epoch 33  70.3% | batch:        45 of        64\t|\tloss: 3281.89\n",
      "Training Epoch 33  71.9% | batch:        46 of        64\t|\tloss: 1612.49\n",
      "Training Epoch 33  73.4% | batch:        47 of        64\t|\tloss: 1916.15\n",
      "Training Epoch 33  75.0% | batch:        48 of        64\t|\tloss: 1790.52\n",
      "Training Epoch 33  76.6% | batch:        49 of        64\t|\tloss: 988.513\n",
      "Training Epoch 33  78.1% | batch:        50 of        64\t|\tloss: 1503.44\n",
      "Training Epoch 33  79.7% | batch:        51 of        64\t|\tloss: 1945.01\n",
      "Training Epoch 33  81.2% | batch:        52 of        64\t|\tloss: 1484.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:37,914 | INFO : Epoch 33 Training Summary: epoch: 33.000000 | loss: 1854.462202 | \n",
      "2023-05-10 17:08:37,915 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1402277946472168 seconds\n",
      "\n",
      "2023-05-10 17:08:37,916 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2133667902512983 seconds\n",
      "2023-05-10 17:08:37,916 | INFO : Avg batch train. time: 0.018958856097676537 seconds\n",
      "2023-05-10 17:08:37,916 | INFO : Avg sample train. time: 0.0003004870703940808 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 33  82.8% | batch:        53 of        64\t|\tloss: 1350.3\n",
      "Training Epoch 33  84.4% | batch:        54 of        64\t|\tloss: 805.778\n",
      "Training Epoch 33  85.9% | batch:        55 of        64\t|\tloss: 1465.87\n",
      "Training Epoch 33  87.5% | batch:        56 of        64\t|\tloss: 2893.61\n",
      "Training Epoch 33  89.1% | batch:        57 of        64\t|\tloss: 1565.89\n",
      "Training Epoch 33  90.6% | batch:        58 of        64\t|\tloss: 2564.53\n",
      "Training Epoch 33  92.2% | batch:        59 of        64\t|\tloss: 1187.11\n",
      "Training Epoch 33  93.8% | batch:        60 of        64\t|\tloss: 1414.4\n",
      "Training Epoch 33  95.3% | batch:        61 of        64\t|\tloss: 1181.7\n",
      "Training Epoch 33  96.9% | batch:        62 of        64\t|\tloss: 1303.87\n",
      "Training Epoch 33  98.4% | batch:        63 of        64\t|\tloss: 2261.9\n",
      "\n",
      "Training Epoch 34   0.0% | batch:         0 of        64\t|\tloss: 882.708\n",
      "Training Epoch 34   1.6% | batch:         1 of        64\t|\tloss: 2911.93\n",
      "Training Epoch 34   3.1% | batch:         2 of        64\t|\tloss: 1881.38\n",
      "Training Epoch 34   4.7% | batch:         3 of        64\t|\tloss: 1772.45\n",
      "Training Epoch 34   6.2% | batch:         4 of        64\t|\tloss: 2712.43\n",
      "Training Epoch 34   7.8% | batch:         5 of        64\t|\tloss: 975.382\n",
      "Training Epoch 34   9.4% | batch:         6 of        64\t|\tloss: 2412.45\n",
      "Training Epoch 34  10.9% | batch:         7 of        64\t|\tloss: 2141.67\n",
      "Training Epoch 34  12.5% | batch:         8 of        64\t|\tloss: 1309.04\n",
      "Training Epoch 34  14.1% | batch:         9 of        64\t|\tloss: 2020.13\n",
      "Training Epoch 34  15.6% | batch:        10 of        64\t|\tloss: 3428.25\n",
      "Training Epoch 34  17.2% | batch:        11 of        64\t|\tloss: 1584.93\n",
      "Training Epoch 34  18.8% | batch:        12 of        64\t|\tloss: 1577.44\n",
      "Training Epoch 34  20.3% | batch:        13 of        64\t|\tloss: 3722.96\n",
      "Training Epoch 34  21.9% | batch:        14 of        64\t|\tloss: 4222.11\n",
      "Training Epoch 34  23.4% | batch:        15 of        64\t|\tloss: 1760.4\n",
      "Training Epoch 34  25.0% | batch:        16 of        64\t|\tloss: 2136.89\n",
      "Training Epoch 34  26.6% | batch:        17 of        64\t|\tloss: 1481.75\n",
      "Training Epoch 34  28.1% | batch:        18 of        64\t|\tloss: 2418.31\n",
      "Training Epoch 34  29.7% | batch:        19 of        64\t|\tloss: 1719.05\n",
      "Training Epoch 34  31.2% | batch:        20 of        64\t|\tloss: 1201.65\n",
      "Training Epoch 34  32.8% | batch:        21 of        64\t|\tloss: 1454.04\n",
      "Training Epoch 34  34.4% | batch:        22 of        64\t|\tloss: 1005.37\n",
      "Training Epoch 34  35.9% | batch:        23 of        64\t|\tloss: 1598.2\n",
      "Training Epoch 34  37.5% | batch:        24 of        64\t|\tloss: 1711.77\n",
      "Training Epoch 34  39.1% | batch:        25 of        64\t|\tloss: 1739.43\n",
      "Training Epoch 34  40.6% | batch:        26 of        64\t|\tloss: 2791.31\n",
      "Training Epoch 34  42.2% | batch:        27 of        64\t|\tloss: 2042.78\n",
      "Training Epoch 34  43.8% | batch:        28 of        64\t|\tloss: 1482.66\n",
      "Training Epoch 34  45.3% | batch:        29 of        64\t|\tloss: 872.043\n",
      "Training Epoch 34  46.9% | batch:        30 of        64\t|\tloss: 1071.78\n",
      "Training Epoch 34  48.4% | batch:        31 of        64\t|\tloss: 1669.52\n",
      "Training Epoch 34  50.0% | batch:        32 of        64\t|\tloss: 1732.59\n",
      "Training Epoch 34  51.6% | batch:        33 of        64\t|\tloss: 5343.87\n",
      "Training Epoch 34  53.1% | batch:        34 of        64\t|\tloss: 1888.79\n",
      "Training Epoch 34  54.7% | batch:        35 of        64\t|\tloss: 1199.27\n",
      "Training Epoch 34  56.2% | batch:        36 of        64\t|\tloss: 1562.66\n",
      "Training Epoch 34  57.8% | batch:        37 of        64\t|\tloss: 814.231\n",
      "Training Epoch 34  59.4% | batch:        38 of        64\t|\tloss: 1990.1\n",
      "Training Epoch 34  60.9% | batch:        39 of        64\t|\tloss: 1310.74\n",
      "Training Epoch 34  62.5% | batch:        40 of        64\t|\tloss: 1591.22\n",
      "Training Epoch 34  64.1% | batch:        41 of        64\t|\tloss: 1517.67\n",
      "Training Epoch 34  65.6% | batch:        42 of        64\t|\tloss: 1477.58\n",
      "Training Epoch 34  67.2% | batch:        43 of        64\t|\tloss: 2213.68\n",
      "Training Epoch 34  68.8% | batch:        44 of        64\t|\tloss: 1025.29\n",
      "Training Epoch 34  70.3% | batch:        45 of        64\t|\tloss: 1282.15\n",
      "Training Epoch 34  71.9% | batch:        46 of        64\t|\tloss: 2589.74\n",
      "Training Epoch 34  73.4% | batch:        47 of        64\t|\tloss: 2733.54\n",
      "Training Epoch 34  75.0% | batch:        48 of        64\t|\tloss: 1732.15\n",
      "Training Epoch 34  76.6% | batch:        49 of        64\t|\tloss: 2330.24\n",
      "Training Epoch 34  78.1% | batch:        50 of        64\t|\tloss: 1441.79\n",
      "Training Epoch 34  79.7% | batch:        51 of        64\t|\tloss: 1416.3\n",
      "Training Epoch 34  81.2% | batch:        52 of        64\t|\tloss: 1396.02\n",
      "Training Epoch 34  82.8% | batch:        53 of        64\t|\tloss: 1023.46\n",
      "Training Epoch 34  84.4% | batch:        54 of        64\t|\tloss: 1461.13\n",
      "Training Epoch 34  85.9% | batch:        55 of        64\t|\tloss: 1950.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:39,127 | INFO : Epoch 34 Training Summary: epoch: 34.000000 | loss: 1929.185505 | \n",
      "2023-05-10 17:08:39,128 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2012865543365479 seconds\n",
      "\n",
      "2023-05-10 17:08:39,128 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2130114891949821 seconds\n",
      "2023-05-10 17:08:39,128 | INFO : Avg batch train. time: 0.018953304518671596 seconds\n",
      "2023-05-10 17:08:39,129 | INFO : Avg sample train. time: 0.00030039908102897034 seconds\n",
      "2023-05-10 17:08:39,129 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 34  87.5% | batch:        56 of        64\t|\tloss: 3187.56\n",
      "Training Epoch 34  89.1% | batch:        57 of        64\t|\tloss: 1085.01\n",
      "Training Epoch 34  90.6% | batch:        58 of        64\t|\tloss: 1597.67\n",
      "Training Epoch 34  92.2% | batch:        59 of        64\t|\tloss: 1455.79\n",
      "Training Epoch 34  93.8% | batch:        60 of        64\t|\tloss: 6165.31\n",
      "Training Epoch 34  95.3% | batch:        61 of        64\t|\tloss: 1811.8\n",
      "Training Epoch 34  96.9% | batch:        62 of        64\t|\tloss: 1359.17\n",
      "Training Epoch 34  98.4% | batch:        63 of        64\t|\tloss: 3420.39\n",
      "\n",
      "Evaluating Epoch 34   0.0% | batch:         0 of        16\t|\tloss: 1591.31\n",
      "Evaluating Epoch 34   6.2% | batch:         1 of        16\t|\tloss: 1630.24\n",
      "Evaluating Epoch 34  12.5% | batch:         2 of        16\t|\tloss: 1053.04\n",
      "Evaluating Epoch 34  18.8% | batch:         3 of        16\t|\tloss: 1824.52\n",
      "Evaluating Epoch 34  25.0% | batch:         4 of        16\t|\tloss: 1456.86\n",
      "Evaluating Epoch 34  31.2% | batch:         5 of        16\t|\tloss: 2567.2\n",
      "Evaluating Epoch 34  37.5% | batch:         6 of        16\t|\tloss: 1883.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:39,288 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.15851855278015137 seconds\n",
      "\n",
      "2023-05-10 17:08:39,288 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.1883983988510935 seconds\n",
      "2023-05-10 17:08:39,289 | INFO : Avg batch val. time: 0.011774899928193343 seconds\n",
      "2023-05-10 17:08:39,289 | INFO : Avg sample val. time: 0.0001865330681693995 seconds\n",
      "2023-05-10 17:08:39,289 | INFO : Epoch 34 Validation Summary: epoch: 34.000000 | loss: 2240.074339 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 34  43.8% | batch:         7 of        16\t|\tloss: 2354.26\n",
      "Evaluating Epoch 34  50.0% | batch:         8 of        16\t|\tloss: 1641.47\n",
      "Evaluating Epoch 34  56.2% | batch:         9 of        16\t|\tloss: 8163.71\n",
      "Evaluating Epoch 34  62.5% | batch:        10 of        16\t|\tloss: 1815.06\n",
      "Evaluating Epoch 34  68.8% | batch:        11 of        16\t|\tloss: 3094.31\n",
      "Evaluating Epoch 34  75.0% | batch:        12 of        16\t|\tloss: 918.372\n",
      "Evaluating Epoch 34  81.2% | batch:        13 of        16\t|\tloss: 1119.64\n",
      "Evaluating Epoch 34  87.5% | batch:        14 of        16\t|\tloss: 1963.55\n",
      "Evaluating Epoch 34  93.8% | batch:        15 of        16\t|\tloss: 2911.1\n",
      "\n",
      "Training Epoch 35   0.0% | batch:         0 of        64\t|\tloss: 1595.36\n",
      "Training Epoch 35   1.6% | batch:         1 of        64\t|\tloss: 1740.34\n",
      "Training Epoch 35   3.1% | batch:         2 of        64\t|\tloss: 1943.97\n",
      "Training Epoch 35   4.7% | batch:         3 of        64\t|\tloss: 1548.29\n",
      "Training Epoch 35   6.2% | batch:         4 of        64\t|\tloss: 4368.04\n",
      "Training Epoch 35   7.8% | batch:         5 of        64\t|\tloss: 1475.21\n",
      "Training Epoch 35   9.4% | batch:         6 of        64\t|\tloss: 1228.81\n",
      "Training Epoch 35  10.9% | batch:         7 of        64\t|\tloss: 1267.57\n",
      "Training Epoch 35  12.5% | batch:         8 of        64\t|\tloss: 969.094\n",
      "Training Epoch 35  14.1% | batch:         9 of        64\t|\tloss: 1024.34\n",
      "Training Epoch 35  15.6% | batch:        10 of        64\t|\tloss: 1746.36\n",
      "Training Epoch 35  17.2% | batch:        11 of        64\t|\tloss: 2679.24\n",
      "Training Epoch 35  18.8% | batch:        12 of        64\t|\tloss: 1948.54\n",
      "Training Epoch 35  20.3% | batch:        13 of        64\t|\tloss: 1495.01\n",
      "Training Epoch 35  21.9% | batch:        14 of        64\t|\tloss: 1997.42\n",
      "Training Epoch 35  23.4% | batch:        15 of        64\t|\tloss: 2357.17\n",
      "Training Epoch 35  25.0% | batch:        16 of        64\t|\tloss: 1750.97\n",
      "Training Epoch 35  26.6% | batch:        17 of        64\t|\tloss: 2505.74\n",
      "Training Epoch 35  28.1% | batch:        18 of        64\t|\tloss: 1799.99\n",
      "Training Epoch 35  29.7% | batch:        19 of        64\t|\tloss: 1834.85\n",
      "Training Epoch 35  31.2% | batch:        20 of        64\t|\tloss: 929.168\n",
      "Training Epoch 35  32.8% | batch:        21 of        64\t|\tloss: 2355.73\n",
      "Training Epoch 35  34.4% | batch:        22 of        64\t|\tloss: 1494.33\n",
      "Training Epoch 35  35.9% | batch:        23 of        64\t|\tloss: 1232.71\n",
      "Training Epoch 35  37.5% | batch:        24 of        64\t|\tloss: 1086.89\n",
      "Training Epoch 35  39.1% | batch:        25 of        64\t|\tloss: 1140.02\n",
      "Training Epoch 35  40.6% | batch:        26 of        64\t|\tloss: 1400.06\n",
      "Training Epoch 35  42.2% | batch:        27 of        64\t|\tloss: 1383.74\n",
      "Training Epoch 35  43.8% | batch:        28 of        64\t|\tloss: 1281.43\n",
      "Training Epoch 35  45.3% | batch:        29 of        64\t|\tloss: 2103.12\n",
      "Training Epoch 35  46.9% | batch:        30 of        64\t|\tloss: 1312.72\n",
      "Training Epoch 35  48.4% | batch:        31 of        64\t|\tloss: 4161.23\n",
      "Training Epoch 35  50.0% | batch:        32 of        64\t|\tloss: 1554.45\n",
      "Training Epoch 35  51.6% | batch:        33 of        64\t|\tloss: 773.75\n",
      "Training Epoch 35  53.1% | batch:        34 of        64\t|\tloss: 1213.61\n",
      "Training Epoch 35  54.7% | batch:        35 of        64\t|\tloss: 1228.08\n",
      "Training Epoch 35  56.2% | batch:        36 of        64\t|\tloss: 1040.06\n",
      "Training Epoch 35  57.8% | batch:        37 of        64\t|\tloss: 1699.48\n",
      "Training Epoch 35  59.4% | batch:        38 of        64\t|\tloss: 1996.81\n",
      "Training Epoch 35  60.9% | batch:        39 of        64\t|\tloss: 1806.13\n",
      "Training Epoch 35  62.5% | batch:        40 of        64\t|\tloss: 1044.44\n",
      "Training Epoch 35  64.1% | batch:        41 of        64\t|\tloss: 1398.31\n",
      "Training Epoch 35  65.6% | batch:        42 of        64\t|\tloss: 1427.55\n",
      "Training Epoch 35  67.2% | batch:        43 of        64\t|\tloss: 1030.09\n",
      "Training Epoch 35  68.8% | batch:        44 of        64\t|\tloss: 1353.02\n",
      "Training Epoch 35  70.3% | batch:        45 of        64\t|\tloss: 1736.27\n",
      "Training Epoch 35  71.9% | batch:        46 of        64\t|\tloss: 3684.56\n",
      "Training Epoch 35  73.4% | batch:        47 of        64\t|\tloss: 1854.63\n",
      "Training Epoch 35  75.0% | batch:        48 of        64\t|\tloss: 2273.33\n",
      "Training Epoch 35  76.6% | batch:        49 of        64\t|\tloss: 1918.06\n",
      "Training Epoch 35  78.1% | batch:        50 of        64\t|\tloss: 920.803\n",
      "Training Epoch 35  79.7% | batch:        51 of        64\t|\tloss: 887.446\n",
      "Training Epoch 35  81.2% | batch:        52 of        64\t|\tloss: 741.438\n",
      "Training Epoch 35  82.8% | batch:        53 of        64\t|\tloss: 1985.05\n",
      "Training Epoch 35  84.4% | batch:        54 of        64\t|\tloss: 8745.69\n",
      "Training Epoch 35  85.9% | batch:        55 of        64\t|\tloss: 1149\n",
      "Training Epoch 35  87.5% | batch:        56 of        64\t|\tloss: 2047.61\n",
      "Training Epoch 35  89.1% | batch:        57 of        64\t|\tloss: 1331.92\n",
      "Training Epoch 35  90.6% | batch:        58 of        64\t|\tloss: 2181.64\n",
      "Training Epoch 35  92.2% | batch:        59 of        64\t|\tloss: 1377.26\n",
      "Training Epoch 35  93.8% | batch:        60 of        64\t|\tloss: 1429.17\n",
      "Training Epoch 35  95.3% | batch:        61 of        64\t|\tloss: 940.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:40,471 | INFO : Epoch 35 Training Summary: epoch: 35.000000 | loss: 1819.880744 | \n",
      "2023-05-10 17:08:40,471 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1702349185943604 seconds\n",
      "\n",
      "2023-05-10 17:08:40,472 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.211789301463536 seconds\n",
      "2023-05-10 17:08:40,472 | INFO : Avg batch train. time: 0.01893420783536775 seconds\n",
      "2023-05-10 17:08:40,472 | INFO : Avg sample train. time: 0.0003000964094758633 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 35  96.9% | batch:        62 of        64\t|\tloss: 4668.49\n",
      "Training Epoch 35  98.4% | batch:        63 of        64\t|\tloss: 2419\n",
      "\n",
      "Training Epoch 36   0.0% | batch:         0 of        64\t|\tloss: 2256.34\n",
      "Training Epoch 36   1.6% | batch:         1 of        64\t|\tloss: 818.09\n",
      "Training Epoch 36   3.1% | batch:         2 of        64\t|\tloss: 2917.6\n",
      "Training Epoch 36   4.7% | batch:         3 of        64\t|\tloss: 2646.11\n",
      "Training Epoch 36   6.2% | batch:         4 of        64\t|\tloss: 2385.06\n",
      "Training Epoch 36   7.8% | batch:         5 of        64\t|\tloss: 1111.18\n",
      "Training Epoch 36   9.4% | batch:         6 of        64\t|\tloss: 1346.29\n",
      "Training Epoch 36  10.9% | batch:         7 of        64\t|\tloss: 1068.32\n",
      "Training Epoch 36  12.5% | batch:         8 of        64\t|\tloss: 970.652\n",
      "Training Epoch 36  14.1% | batch:         9 of        64\t|\tloss: 1240.21\n",
      "Training Epoch 36  15.6% | batch:        10 of        64\t|\tloss: 1150.82\n",
      "Training Epoch 36  17.2% | batch:        11 of        64\t|\tloss: 1686.7\n",
      "Training Epoch 36  18.8% | batch:        12 of        64\t|\tloss: 1985.05\n",
      "Training Epoch 36  20.3% | batch:        13 of        64\t|\tloss: 846.112\n",
      "Training Epoch 36  21.9% | batch:        14 of        64\t|\tloss: 1396.31\n",
      "Training Epoch 36  23.4% | batch:        15 of        64\t|\tloss: 1440.23\n",
      "Training Epoch 36  25.0% | batch:        16 of        64\t|\tloss: 1149.62\n",
      "Training Epoch 36  26.6% | batch:        17 of        64\t|\tloss: 1670.3\n",
      "Training Epoch 36  28.1% | batch:        18 of        64\t|\tloss: 3355.5\n",
      "Training Epoch 36  29.7% | batch:        19 of        64\t|\tloss: 2166.24\n",
      "Training Epoch 36  31.2% | batch:        20 of        64\t|\tloss: 1217.51\n",
      "Training Epoch 36  32.8% | batch:        21 of        64\t|\tloss: 805.689\n",
      "Training Epoch 36  34.4% | batch:        22 of        64\t|\tloss: 2097.47\n",
      "Training Epoch 36  35.9% | batch:        23 of        64\t|\tloss: 3701.09\n",
      "Training Epoch 36  37.5% | batch:        24 of        64\t|\tloss: 1199.01\n",
      "Training Epoch 36  39.1% | batch:        25 of        64\t|\tloss: 1872.73\n",
      "Training Epoch 36  40.6% | batch:        26 of        64\t|\tloss: 1217.11\n",
      "Training Epoch 36  42.2% | batch:        27 of        64\t|\tloss: 1578.84\n",
      "Training Epoch 36  43.8% | batch:        28 of        64\t|\tloss: 1360.85\n",
      "Training Epoch 36  45.3% | batch:        29 of        64\t|\tloss: 2714.34\n",
      "Training Epoch 36  46.9% | batch:        30 of        64\t|\tloss: 851.444\n",
      "Training Epoch 36  48.4% | batch:        31 of        64\t|\tloss: 2565.62\n",
      "Training Epoch 36  50.0% | batch:        32 of        64\t|\tloss: 2327.93\n",
      "Training Epoch 36  51.6% | batch:        33 of        64\t|\tloss: 1220.87\n",
      "Training Epoch 36  53.1% | batch:        34 of        64\t|\tloss: 1275.68\n",
      "Training Epoch 36  54.7% | batch:        35 of        64\t|\tloss: 852.328\n",
      "Training Epoch 36  56.2% | batch:        36 of        64\t|\tloss: 1608.13\n",
      "Training Epoch 36  57.8% | batch:        37 of        64\t|\tloss: 2010.09\n",
      "Training Epoch 36  59.4% | batch:        38 of        64\t|\tloss: 1054.88\n",
      "Training Epoch 36  60.9% | batch:        39 of        64\t|\tloss: 1366.25\n",
      "Training Epoch 36  62.5% | batch:        40 of        64\t|\tloss: 1458.22\n",
      "Training Epoch 36  64.1% | batch:        41 of        64\t|\tloss: 2042.01\n",
      "Training Epoch 36  65.6% | batch:        42 of        64\t|\tloss: 1691.45\n",
      "Training Epoch 36  67.2% | batch:        43 of        64\t|\tloss: 1121.48\n",
      "Training Epoch 36  68.8% | batch:        44 of        64\t|\tloss: 1093.28\n",
      "Training Epoch 36  70.3% | batch:        45 of        64\t|\tloss: 1656.93\n",
      "Training Epoch 36  71.9% | batch:        46 of        64\t|\tloss: 1754.45\n",
      "Training Epoch 36  73.4% | batch:        47 of        64\t|\tloss: 1896.27\n",
      "Training Epoch 36  75.0% | batch:        48 of        64\t|\tloss: 3419.75\n",
      "Training Epoch 36  76.6% | batch:        49 of        64\t|\tloss: 5465.79\n",
      "Training Epoch 36  78.1% | batch:        50 of        64\t|\tloss: 2357.19\n",
      "Training Epoch 36  79.7% | batch:        51 of        64\t|\tloss: 2596.38\n",
      "Training Epoch 36  81.2% | batch:        52 of        64\t|\tloss: 1191\n",
      "Training Epoch 36  82.8% | batch:        53 of        64\t|\tloss: 1683.08\n",
      "Training Epoch 36  84.4% | batch:        54 of        64\t|\tloss: 3088.18\n",
      "Training Epoch 36  85.9% | batch:        55 of        64\t|\tloss: 1238.44\n",
      "Training Epoch 36  87.5% | batch:        56 of        64\t|\tloss: 1219.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:41,629 | INFO : Epoch 36 Training Summary: epoch: 36.000000 | loss: 1766.045926 | \n",
      "2023-05-10 17:08:41,630 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.148186206817627 seconds\n",
      "\n",
      "2023-05-10 17:08:41,630 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.210022548834483 seconds\n",
      "2023-05-10 17:08:41,631 | INFO : Avg batch train. time: 0.018906602325538795 seconds\n",
      "2023-05-10 17:08:41,631 | INFO : Avg sample train. time: 0.0002996588778688665 seconds\n",
      "2023-05-10 17:08:41,631 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 36  89.1% | batch:        57 of        64\t|\tloss: 1287.98\n",
      "Training Epoch 36  90.6% | batch:        58 of        64\t|\tloss: 1418.13\n",
      "Training Epoch 36  92.2% | batch:        59 of        64\t|\tloss: 1389.17\n",
      "Training Epoch 36  93.8% | batch:        60 of        64\t|\tloss: 2899.17\n",
      "Training Epoch 36  95.3% | batch:        61 of        64\t|\tloss: 1493.34\n",
      "Training Epoch 36  96.9% | batch:        62 of        64\t|\tloss: 907.887\n",
      "Training Epoch 36  98.4% | batch:        63 of        64\t|\tloss: 5897.93\n",
      "\n",
      "Evaluating Epoch 36   0.0% | batch:         0 of        16\t|\tloss: 1441.14\n",
      "Evaluating Epoch 36   6.2% | batch:         1 of        16\t|\tloss: 1241.08\n",
      "Evaluating Epoch 36  12.5% | batch:         2 of        16\t|\tloss: 1042.43\n",
      "Evaluating Epoch 36  18.8% | batch:         3 of        16\t|\tloss: 1595.52\n",
      "Evaluating Epoch 36  25.0% | batch:         4 of        16\t|\tloss: 1250.75\n",
      "Evaluating Epoch 36  31.2% | batch:         5 of        16\t|\tloss: 1923.57\n",
      "Evaluating Epoch 36  37.5% | batch:         6 of        16\t|\tloss: 1813.54\n",
      "Evaluating Epoch 36  43.8% | batch:         7 of        16\t|\tloss: 1718.41\n",
      "Evaluating Epoch 36  50.0% | batch:         8 of        16\t|\tloss: 1336.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:41,789 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.15702199935913086 seconds\n",
      "\n",
      "2023-05-10 17:08:41,789 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.18682957887649537 seconds\n",
      "2023-05-10 17:08:41,790 | INFO : Avg batch val. time: 0.01167684867978096 seconds\n",
      "2023-05-10 17:08:41,790 | INFO : Avg sample val. time: 0.000184979781065837 seconds\n",
      "2023-05-10 17:08:41,790 | INFO : Epoch 36 Validation Summary: epoch: 36.000000 | loss: 1861.137802 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 36  56.2% | batch:         9 of        16\t|\tloss: 5854.65\n",
      "Evaluating Epoch 36  62.5% | batch:        10 of        16\t|\tloss: 1671.67\n",
      "Evaluating Epoch 36  68.8% | batch:        11 of        16\t|\tloss: 2527.29\n",
      "Evaluating Epoch 36  75.0% | batch:        12 of        16\t|\tloss: 1118.75\n",
      "Evaluating Epoch 36  81.2% | batch:        13 of        16\t|\tloss: 1087.01\n",
      "Evaluating Epoch 36  87.5% | batch:        14 of        16\t|\tloss: 1900.09\n",
      "Evaluating Epoch 36  93.8% | batch:        15 of        16\t|\tloss: 2365.92\n",
      "\n",
      "Training Epoch 37   0.0% | batch:         0 of        64\t|\tloss: 1690.55\n",
      "Training Epoch 37   1.6% | batch:         1 of        64\t|\tloss: 978.488\n",
      "Training Epoch 37   3.1% | batch:         2 of        64\t|\tloss: 1987.55\n",
      "Training Epoch 37   4.7% | batch:         3 of        64\t|\tloss: 684.604\n",
      "Training Epoch 37   6.2% | batch:         4 of        64\t|\tloss: 1473.7\n",
      "Training Epoch 37   7.8% | batch:         5 of        64\t|\tloss: 1327.4\n",
      "Training Epoch 37   9.4% | batch:         6 of        64\t|\tloss: 1168.81\n",
      "Training Epoch 37  10.9% | batch:         7 of        64\t|\tloss: 931.633\n",
      "Training Epoch 37  12.5% | batch:         8 of        64\t|\tloss: 3327.07\n",
      "Training Epoch 37  14.1% | batch:         9 of        64\t|\tloss: 1954.52\n",
      "Training Epoch 37  15.6% | batch:        10 of        64\t|\tloss: 2615.15\n",
      "Training Epoch 37  17.2% | batch:        11 of        64\t|\tloss: 1339.09\n",
      "Training Epoch 37  18.8% | batch:        12 of        64\t|\tloss: 3738.03\n",
      "Training Epoch 37  20.3% | batch:        13 of        64\t|\tloss: 1604.91\n",
      "Training Epoch 37  21.9% | batch:        14 of        64\t|\tloss: 1343.74\n",
      "Training Epoch 37  23.4% | batch:        15 of        64\t|\tloss: 1384.31\n",
      "Training Epoch 37  25.0% | batch:        16 of        64\t|\tloss: 1160.8\n",
      "Training Epoch 37  26.6% | batch:        17 of        64\t|\tloss: 1851.29\n",
      "Training Epoch 37  28.1% | batch:        18 of        64\t|\tloss: 1519.98\n",
      "Training Epoch 37  29.7% | batch:        19 of        64\t|\tloss: 1288.34\n",
      "Training Epoch 37  31.2% | batch:        20 of        64\t|\tloss: 813.76\n",
      "Training Epoch 37  32.8% | batch:        21 of        64\t|\tloss: 2049.93\n",
      "Training Epoch 37  34.4% | batch:        22 of        64\t|\tloss: 1201.75\n",
      "Training Epoch 37  35.9% | batch:        23 of        64\t|\tloss: 3250.63\n",
      "Training Epoch 37  37.5% | batch:        24 of        64\t|\tloss: 929.604\n",
      "Training Epoch 37  39.1% | batch:        25 of        64\t|\tloss: 2049.03\n",
      "Training Epoch 37  40.6% | batch:        26 of        64\t|\tloss: 1392.31\n",
      "Training Epoch 37  42.2% | batch:        27 of        64\t|\tloss: 2579.4\n",
      "Training Epoch 37  43.8% | batch:        28 of        64\t|\tloss: 1804.45\n",
      "Training Epoch 37  45.3% | batch:        29 of        64\t|\tloss: 1120.38\n",
      "Training Epoch 37  46.9% | batch:        30 of        64\t|\tloss: 5889.38\n",
      "Training Epoch 37  48.4% | batch:        31 of        64\t|\tloss: 1035.73\n",
      "Training Epoch 37  50.0% | batch:        32 of        64\t|\tloss: 1231.45\n",
      "Training Epoch 37  51.6% | batch:        33 of        64\t|\tloss: 1013.15\n",
      "Training Epoch 37  53.1% | batch:        34 of        64\t|\tloss: 1236.74\n",
      "Training Epoch 37  54.7% | batch:        35 of        64\t|\tloss: 1342.14\n",
      "Training Epoch 37  56.2% | batch:        36 of        64\t|\tloss: 1775.05\n",
      "Training Epoch 37  57.8% | batch:        37 of        64\t|\tloss: 970.368\n",
      "Training Epoch 37  59.4% | batch:        38 of        64\t|\tloss: 1230\n",
      "Training Epoch 37  60.9% | batch:        39 of        64\t|\tloss: 1151.32\n",
      "Training Epoch 37  62.5% | batch:        40 of        64\t|\tloss: 3188.34\n",
      "Training Epoch 37  64.1% | batch:        41 of        64\t|\tloss: 1140.44\n",
      "Training Epoch 37  65.6% | batch:        42 of        64\t|\tloss: 1985.29\n",
      "Training Epoch 37  67.2% | batch:        43 of        64\t|\tloss: 1406.85\n",
      "Training Epoch 37  68.8% | batch:        44 of        64\t|\tloss: 3054.08\n",
      "Training Epoch 37  70.3% | batch:        45 of        64\t|\tloss: 1488.88\n",
      "Training Epoch 37  71.9% | batch:        46 of        64\t|\tloss: 2036.94\n",
      "Training Epoch 37  73.4% | batch:        47 of        64\t|\tloss: 2234.97\n",
      "Training Epoch 37  75.0% | batch:        48 of        64\t|\tloss: 1803.24\n",
      "Training Epoch 37  76.6% | batch:        49 of        64\t|\tloss: 1265.17\n",
      "Training Epoch 37  78.1% | batch:        50 of        64\t|\tloss: 2338.21\n",
      "Training Epoch 37  79.7% | batch:        51 of        64\t|\tloss: 1043.39\n",
      "Training Epoch 37  81.2% | batch:        52 of        64\t|\tloss: 1241.33\n",
      "Training Epoch 37  82.8% | batch:        53 of        64\t|\tloss: 1305.27\n",
      "Training Epoch 37  84.4% | batch:        54 of        64\t|\tloss: 1556.97\n",
      "Training Epoch 37  85.9% | batch:        55 of        64\t|\tloss: 1550.24\n",
      "Training Epoch 37  87.5% | batch:        56 of        64\t|\tloss: 1003.83\n",
      "Training Epoch 37  89.1% | batch:        57 of        64\t|\tloss: 1015.15\n",
      "Training Epoch 37  90.6% | batch:        58 of        64\t|\tloss: 1594.1\n",
      "Training Epoch 37  92.2% | batch:        59 of        64\t|\tloss: 1858.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:43,003 | INFO : Epoch 37 Training Summary: epoch: 37.000000 | loss: 1699.767831 | \n",
      "2023-05-10 17:08:43,004 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1982815265655518 seconds\n",
      "\n",
      "2023-05-10 17:08:43,004 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2097052239082955 seconds\n",
      "2023-05-10 17:08:43,005 | INFO : Avg batch train. time: 0.018901644123567117 seconds\n",
      "2023-05-10 17:08:43,005 | INFO : Avg sample train. time: 0.0002995802931917522 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 37  93.8% | batch:        60 of        64\t|\tloss: 1892.28\n",
      "Training Epoch 37  95.3% | batch:        61 of        64\t|\tloss: 1042.01\n",
      "Training Epoch 37  96.9% | batch:        62 of        64\t|\tloss: 2209.17\n",
      "Training Epoch 37  98.4% | batch:        63 of        64\t|\tloss: 5902.22\n",
      "\n",
      "Training Epoch 38   0.0% | batch:         0 of        64\t|\tloss: 4765.32\n",
      "Training Epoch 38   1.6% | batch:         1 of        64\t|\tloss: 1215.79\n",
      "Training Epoch 38   3.1% | batch:         2 of        64\t|\tloss: 3164.65\n",
      "Training Epoch 38   4.7% | batch:         3 of        64\t|\tloss: 1332.03\n",
      "Training Epoch 38   6.2% | batch:         4 of        64\t|\tloss: 1546.3\n",
      "Training Epoch 38   7.8% | batch:         5 of        64\t|\tloss: 1534.48\n",
      "Training Epoch 38   9.4% | batch:         6 of        64\t|\tloss: 855.134\n",
      "Training Epoch 38  10.9% | batch:         7 of        64\t|\tloss: 3142.93\n",
      "Training Epoch 38  12.5% | batch:         8 of        64\t|\tloss: 964.141\n",
      "Training Epoch 38  14.1% | batch:         9 of        64\t|\tloss: 1310.17\n",
      "Training Epoch 38  15.6% | batch:        10 of        64\t|\tloss: 1287.79\n",
      "Training Epoch 38  17.2% | batch:        11 of        64\t|\tloss: 7710.1\n",
      "Training Epoch 38  18.8% | batch:        12 of        64\t|\tloss: 1265.55\n",
      "Training Epoch 38  20.3% | batch:        13 of        64\t|\tloss: 1615.62\n",
      "Training Epoch 38  21.9% | batch:        14 of        64\t|\tloss: 2016.68\n",
      "Training Epoch 38  23.4% | batch:        15 of        64\t|\tloss: 1703.34\n",
      "Training Epoch 38  25.0% | batch:        16 of        64\t|\tloss: 1479.74\n",
      "Training Epoch 38  26.6% | batch:        17 of        64\t|\tloss: 1419.9\n",
      "Training Epoch 38  28.1% | batch:        18 of        64\t|\tloss: 998.976\n",
      "Training Epoch 38  29.7% | batch:        19 of        64\t|\tloss: 1536.98\n",
      "Training Epoch 38  31.2% | batch:        20 of        64\t|\tloss: 962.65\n",
      "Training Epoch 38  32.8% | batch:        21 of        64\t|\tloss: 906.153\n",
      "Training Epoch 38  34.4% | batch:        22 of        64\t|\tloss: 1335.17\n",
      "Training Epoch 38  35.9% | batch:        23 of        64\t|\tloss: 1158.36\n",
      "Training Epoch 38  37.5% | batch:        24 of        64\t|\tloss: 2314.92\n",
      "Training Epoch 38  39.1% | batch:        25 of        64\t|\tloss: 983.929\n",
      "Training Epoch 38  40.6% | batch:        26 of        64\t|\tloss: 2704.15\n",
      "Training Epoch 38  42.2% | batch:        27 of        64\t|\tloss: 2335.72\n",
      "Training Epoch 38  43.8% | batch:        28 of        64\t|\tloss: 1358.03\n",
      "Training Epoch 38  45.3% | batch:        29 of        64\t|\tloss: 2081.83\n",
      "Training Epoch 38  46.9% | batch:        30 of        64\t|\tloss: 822.555\n",
      "Training Epoch 38  48.4% | batch:        31 of        64\t|\tloss: 925.832\n",
      "Training Epoch 38  50.0% | batch:        32 of        64\t|\tloss: 1510.04\n",
      "Training Epoch 38  51.6% | batch:        33 of        64\t|\tloss: 6137.74\n",
      "Training Epoch 38  53.1% | batch:        34 of        64\t|\tloss: 3124.21\n",
      "Training Epoch 38  54.7% | batch:        35 of        64\t|\tloss: 1388.66\n",
      "Training Epoch 38  56.2% | batch:        36 of        64\t|\tloss: 936.72\n",
      "Training Epoch 38  57.8% | batch:        37 of        64\t|\tloss: 1580.09\n",
      "Training Epoch 38  59.4% | batch:        38 of        64\t|\tloss: 1594.17\n",
      "Training Epoch 38  60.9% | batch:        39 of        64\t|\tloss: 1890.32\n",
      "Training Epoch 38  62.5% | batch:        40 of        64\t|\tloss: 2028.27\n",
      "Training Epoch 38  64.1% | batch:        41 of        64\t|\tloss: 1565.53\n",
      "Training Epoch 38  65.6% | batch:        42 of        64\t|\tloss: 1983.89\n",
      "Training Epoch 38  67.2% | batch:        43 of        64\t|\tloss: 1556.59\n",
      "Training Epoch 38  68.8% | batch:        44 of        64\t|\tloss: 2803.98\n",
      "Training Epoch 38  70.3% | batch:        45 of        64\t|\tloss: 2514.48\n",
      "Training Epoch 38  71.9% | batch:        46 of        64\t|\tloss: 4772.75\n",
      "Training Epoch 38  73.4% | batch:        47 of        64\t|\tloss: 2062.27\n",
      "Training Epoch 38  75.0% | batch:        48 of        64\t|\tloss: 2004.38\n",
      "Training Epoch 38  76.6% | batch:        49 of        64\t|\tloss: 1555.95\n",
      "Training Epoch 38  78.1% | batch:        50 of        64\t|\tloss: 1575.62\n",
      "Training Epoch 38  79.7% | batch:        51 of        64\t|\tloss: 2731.79\n",
      "Training Epoch 38  81.2% | batch:        52 of        64\t|\tloss: 2434.92\n",
      "Training Epoch 38  82.8% | batch:        53 of        64\t|\tloss: 854.417\n",
      "Training Epoch 38  84.4% | batch:        54 of        64\t|\tloss: 1402.89\n",
      "Training Epoch 38  85.9% | batch:        55 of        64\t|\tloss: 2173.82\n",
      "Training Epoch 38  87.5% | batch:        56 of        64\t|\tloss: 1460.6\n",
      "Training Epoch 38  89.1% | batch:        57 of        64\t|\tloss: 961.438\n",
      "Training Epoch 38  90.6% | batch:        58 of        64\t|\tloss: 1314.54\n",
      "Training Epoch 38  92.2% | batch:        59 of        64\t|\tloss: 1163.09\n",
      "Training Epoch 38  93.8% | batch:        60 of        64\t|\tloss: 2299.98\n",
      "Training Epoch 38  95.3% | batch:        61 of        64\t|\tloss: 1295.75\n",
      "Training Epoch 38  96.9% | batch:        62 of        64\t|\tloss: 1140.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:44,211 | INFO : Epoch 38 Training Summary: epoch: 38.000000 | loss: 1924.775701 | \n",
      "2023-05-10 17:08:44,211 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.194990873336792 seconds\n",
      "\n",
      "2023-05-10 17:08:44,212 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2093180041564138 seconds\n",
      "2023-05-10 17:08:44,212 | INFO : Avg batch train. time: 0.018895593814943965 seconds\n",
      "2023-05-10 17:08:44,212 | INFO : Avg sample train. time: 0.00029948439924626396 seconds\n",
      "2023-05-10 17:08:44,213 | INFO : Evaluating on validation set ...\n",
      "2023-05-10 17:08:44,388 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.1754932403564453 seconds\n",
      "\n",
      "2023-05-10 17:08:44,389 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.18628975323268346 seconds\n",
      "2023-05-10 17:08:44,389 | INFO : Avg batch val. time: 0.011643109577042716 seconds\n",
      "2023-05-10 17:08:44,389 | INFO : Avg sample val. time: 0.00018444530023037966 seconds\n",
      "2023-05-10 17:08:44,390 | INFO : Epoch 38 Validation Summary: epoch: 38.000000 | loss: 1970.055105 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 38  98.4% | batch:        63 of        64\t|\tloss: 9250.59\n",
      "\n",
      "Evaluating Epoch 38   0.0% | batch:         0 of        16\t|\tloss: 1609.48\n",
      "Evaluating Epoch 38   6.2% | batch:         1 of        16\t|\tloss: 1156.88\n",
      "Evaluating Epoch 38  12.5% | batch:         2 of        16\t|\tloss: 754.468\n",
      "Evaluating Epoch 38  18.8% | batch:         3 of        16\t|\tloss: 1420.32\n",
      "Evaluating Epoch 38  25.0% | batch:         4 of        16\t|\tloss: 1458.72\n",
      "Evaluating Epoch 38  31.2% | batch:         5 of        16\t|\tloss: 2413.74\n",
      "Evaluating Epoch 38  37.5% | batch:         6 of        16\t|\tloss: 1862.03\n",
      "Evaluating Epoch 38  43.8% | batch:         7 of        16\t|\tloss: 2664.69\n",
      "Evaluating Epoch 38  50.0% | batch:         8 of        16\t|\tloss: 1418.63\n",
      "Evaluating Epoch 38  56.2% | batch:         9 of        16\t|\tloss: 6265.04\n",
      "Evaluating Epoch 38  62.5% | batch:        10 of        16\t|\tloss: 1343.66\n",
      "Evaluating Epoch 38  68.8% | batch:        11 of        16\t|\tloss: 2903.85\n",
      "Evaluating Epoch 38  75.0% | batch:        12 of        16\t|\tloss: 1079.48\n",
      "Evaluating Epoch 38  81.2% | batch:        13 of        16\t|\tloss: 1154.29\n",
      "Evaluating Epoch 38  87.5% | batch:        14 of        16\t|\tloss: 1846.53\n",
      "Evaluating Epoch 38  93.8% | batch:        15 of        16\t|\tloss: 2224.79\n",
      "\n",
      "Training Epoch 39   0.0% | batch:         0 of        64\t|\tloss: 2228.34\n",
      "Training Epoch 39   1.6% | batch:         1 of        64\t|\tloss: 1290.66\n",
      "Training Epoch 39   3.1% | batch:         2 of        64\t|\tloss: 1908.16\n",
      "Training Epoch 39   4.7% | batch:         3 of        64\t|\tloss: 2312.19\n",
      "Training Epoch 39   6.2% | batch:         4 of        64\t|\tloss: 1333.54\n",
      "Training Epoch 39   7.8% | batch:         5 of        64\t|\tloss: 1054.99\n",
      "Training Epoch 39   9.4% | batch:         6 of        64\t|\tloss: 1267.77\n",
      "Training Epoch 39  10.9% | batch:         7 of        64\t|\tloss: 1827.36\n",
      "Training Epoch 39  12.5% | batch:         8 of        64\t|\tloss: 1375.65\n",
      "Training Epoch 39  14.1% | batch:         9 of        64\t|\tloss: 1321.2\n",
      "Training Epoch 39  15.6% | batch:        10 of        64\t|\tloss: 932.316\n",
      "Training Epoch 39  17.2% | batch:        11 of        64\t|\tloss: 1303.75\n",
      "Training Epoch 39  18.8% | batch:        12 of        64\t|\tloss: 1087.13\n",
      "Training Epoch 39  20.3% | batch:        13 of        64\t|\tloss: 1272.58\n",
      "Training Epoch 39  21.9% | batch:        14 of        64\t|\tloss: 3431.16\n",
      "Training Epoch 39  23.4% | batch:        15 of        64\t|\tloss: 1519.49\n",
      "Training Epoch 39  25.0% | batch:        16 of        64\t|\tloss: 1680.25\n",
      "Training Epoch 39  26.6% | batch:        17 of        64\t|\tloss: 1319.56\n",
      "Training Epoch 39  28.1% | batch:        18 of        64\t|\tloss: 1669\n",
      "Training Epoch 39  29.7% | batch:        19 of        64\t|\tloss: 1394.6\n",
      "Training Epoch 39  31.2% | batch:        20 of        64\t|\tloss: 4384.67\n",
      "Training Epoch 39  32.8% | batch:        21 of        64\t|\tloss: 1769.31\n",
      "Training Epoch 39  34.4% | batch:        22 of        64\t|\tloss: 2548.56\n",
      "Training Epoch 39  35.9% | batch:        23 of        64\t|\tloss: 1055.76\n",
      "Training Epoch 39  37.5% | batch:        24 of        64\t|\tloss: 1313.51\n",
      "Training Epoch 39  39.1% | batch:        25 of        64\t|\tloss: 1095.63\n",
      "Training Epoch 39  40.6% | batch:        26 of        64\t|\tloss: 2465.9\n",
      "Training Epoch 39  42.2% | batch:        27 of        64\t|\tloss: 1182.42\n",
      "Training Epoch 39  43.8% | batch:        28 of        64\t|\tloss: 5286.88\n",
      "Training Epoch 39  45.3% | batch:        29 of        64\t|\tloss: 1173.48\n",
      "Training Epoch 39  46.9% | batch:        30 of        64\t|\tloss: 1854.22\n",
      "Training Epoch 39  48.4% | batch:        31 of        64\t|\tloss: 895.094\n",
      "Training Epoch 39  50.0% | batch:        32 of        64\t|\tloss: 945.061\n",
      "Training Epoch 39  51.6% | batch:        33 of        64\t|\tloss: 1880.49\n",
      "Training Epoch 39  53.1% | batch:        34 of        64\t|\tloss: 1703.31\n",
      "Training Epoch 39  54.7% | batch:        35 of        64\t|\tloss: 1735.78\n",
      "Training Epoch 39  56.2% | batch:        36 of        64\t|\tloss: 1579.66\n",
      "Training Epoch 39  57.8% | batch:        37 of        64\t|\tloss: 1014.9\n",
      "Training Epoch 39  59.4% | batch:        38 of        64\t|\tloss: 2172.64\n",
      "Training Epoch 39  60.9% | batch:        39 of        64\t|\tloss: 1815.16\n",
      "Training Epoch 39  62.5% | batch:        40 of        64\t|\tloss: 2188.26\n",
      "Training Epoch 39  64.1% | batch:        41 of        64\t|\tloss: 2744.56\n",
      "Training Epoch 39  65.6% | batch:        42 of        64\t|\tloss: 1330.96\n",
      "Training Epoch 39  67.2% | batch:        43 of        64\t|\tloss: 1564.03\n",
      "Training Epoch 39  68.8% | batch:        44 of        64\t|\tloss: 4043.51\n",
      "Training Epoch 39  70.3% | batch:        45 of        64\t|\tloss: 5119.54\n",
      "Training Epoch 39  71.9% | batch:        46 of        64\t|\tloss: 1424.66\n",
      "Training Epoch 39  73.4% | batch:        47 of        64\t|\tloss: 1340.38\n",
      "Training Epoch 39  75.0% | batch:        48 of        64\t|\tloss: 1769.7\n",
      "Training Epoch 39  76.6% | batch:        49 of        64\t|\tloss: 1270.26\n",
      "Training Epoch 39  78.1% | batch:        50 of        64\t|\tloss: 1574.92\n",
      "Training Epoch 39  79.7% | batch:        51 of        64\t|\tloss: 795.8\n",
      "Training Epoch 39  81.2% | batch:        52 of        64\t|\tloss: 1402.39\n",
      "Training Epoch 39  82.8% | batch:        53 of        64\t|\tloss: 1818.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:45,628 | INFO : Epoch 39 Training Summary: epoch: 39.000000 | loss: 1741.934017 | \n",
      "2023-05-10 17:08:45,628 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.228555679321289 seconds\n",
      "\n",
      "2023-05-10 17:08:45,629 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.20981127787859 seconds\n",
      "2023-05-10 17:08:45,629 | INFO : Avg batch train. time: 0.01890330121685297 seconds\n",
      "2023-05-10 17:08:45,629 | INFO : Avg sample train. time: 0.00029960655717647104 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 39  84.4% | batch:        54 of        64\t|\tloss: 1096.15\n",
      "Training Epoch 39  85.9% | batch:        55 of        64\t|\tloss: 1187.48\n",
      "Training Epoch 39  87.5% | batch:        56 of        64\t|\tloss: 1290.66\n",
      "Training Epoch 39  89.1% | batch:        57 of        64\t|\tloss: 933.303\n",
      "Training Epoch 39  90.6% | batch:        58 of        64\t|\tloss: 1421.88\n",
      "Training Epoch 39  92.2% | batch:        59 of        64\t|\tloss: 1611.82\n",
      "Training Epoch 39  93.8% | batch:        60 of        64\t|\tloss: 1337.64\n",
      "Training Epoch 39  95.3% | batch:        61 of        64\t|\tloss: 1891.99\n",
      "Training Epoch 39  96.9% | batch:        62 of        64\t|\tloss: 1283.12\n",
      "Training Epoch 39  98.4% | batch:        63 of        64\t|\tloss: 655.395\n",
      "\n",
      "Training Epoch 40   0.0% | batch:         0 of        64\t|\tloss: 2144.43\n",
      "Training Epoch 40   1.6% | batch:         1 of        64\t|\tloss: 973.963\n",
      "Training Epoch 40   3.1% | batch:         2 of        64\t|\tloss: 2368.87\n",
      "Training Epoch 40   4.7% | batch:         3 of        64\t|\tloss: 1138.94\n",
      "Training Epoch 40   6.2% | batch:         4 of        64\t|\tloss: 1398.56\n",
      "Training Epoch 40   7.8% | batch:         5 of        64\t|\tloss: 2813.37\n",
      "Training Epoch 40   9.4% | batch:         6 of        64\t|\tloss: 3169.87\n",
      "Training Epoch 40  10.9% | batch:         7 of        64\t|\tloss: 1959.14\n",
      "Training Epoch 40  12.5% | batch:         8 of        64\t|\tloss: 959.29\n",
      "Training Epoch 40  14.1% | batch:         9 of        64\t|\tloss: 992.302\n",
      "Training Epoch 40  15.6% | batch:        10 of        64\t|\tloss: 1401.46\n",
      "Training Epoch 40  17.2% | batch:        11 of        64\t|\tloss: 2926.31\n",
      "Training Epoch 40  18.8% | batch:        12 of        64\t|\tloss: 1262.14\n",
      "Training Epoch 40  20.3% | batch:        13 of        64\t|\tloss: 805.526\n",
      "Training Epoch 40  21.9% | batch:        14 of        64\t|\tloss: 1213.1\n",
      "Training Epoch 40  23.4% | batch:        15 of        64\t|\tloss: 1464.05\n",
      "Training Epoch 40  25.0% | batch:        16 of        64\t|\tloss: 829.238\n",
      "Training Epoch 40  26.6% | batch:        17 of        64\t|\tloss: 2366.24\n",
      "Training Epoch 40  28.1% | batch:        18 of        64\t|\tloss: 1303.15\n",
      "Training Epoch 40  29.7% | batch:        19 of        64\t|\tloss: 856.726\n",
      "Training Epoch 40  31.2% | batch:        20 of        64\t|\tloss: 2054.8\n",
      "Training Epoch 40  32.8% | batch:        21 of        64\t|\tloss: 1343.34\n",
      "Training Epoch 40  34.4% | batch:        22 of        64\t|\tloss: 1654.53\n",
      "Training Epoch 40  35.9% | batch:        23 of        64\t|\tloss: 1606.89\n",
      "Training Epoch 40  37.5% | batch:        24 of        64\t|\tloss: 1061.19\n",
      "Training Epoch 40  39.1% | batch:        25 of        64\t|\tloss: 976.772\n",
      "Training Epoch 40  40.6% | batch:        26 of        64\t|\tloss: 2254.74\n",
      "Training Epoch 40  42.2% | batch:        27 of        64\t|\tloss: 2311\n",
      "Training Epoch 40  43.8% | batch:        28 of        64\t|\tloss: 3789.95\n",
      "Training Epoch 40  45.3% | batch:        29 of        64\t|\tloss: 1033.25\n",
      "Training Epoch 40  46.9% | batch:        30 of        64\t|\tloss: 2330.69\n",
      "Training Epoch 40  48.4% | batch:        31 of        64\t|\tloss: 2541.58\n",
      "Training Epoch 40  50.0% | batch:        32 of        64\t|\tloss: 1380.94\n",
      "Training Epoch 40  51.6% | batch:        33 of        64\t|\tloss: 1432.68\n",
      "Training Epoch 40  53.1% | batch:        34 of        64\t|\tloss: 1933.41\n",
      "Training Epoch 40  54.7% | batch:        35 of        64\t|\tloss: 1468.89\n",
      "Training Epoch 40  56.2% | batch:        36 of        64\t|\tloss: 1080.14\n",
      "Training Epoch 40  57.8% | batch:        37 of        64\t|\tloss: 1369.51\n",
      "Training Epoch 40  59.4% | batch:        38 of        64\t|\tloss: 1463.51\n",
      "Training Epoch 40  60.9% | batch:        39 of        64\t|\tloss: 1226.06\n",
      "Training Epoch 40  62.5% | batch:        40 of        64\t|\tloss: 1645.85\n",
      "Training Epoch 40  64.1% | batch:        41 of        64\t|\tloss: 2678.15\n",
      "Training Epoch 40  65.6% | batch:        42 of        64\t|\tloss: 1260.82\n",
      "Training Epoch 40  67.2% | batch:        43 of        64\t|\tloss: 1559.79\n",
      "Training Epoch 40  68.8% | batch:        44 of        64\t|\tloss: 2224.57\n",
      "Training Epoch 40  70.3% | batch:        45 of        64\t|\tloss: 2303.86\n",
      "Training Epoch 40  71.9% | batch:        46 of        64\t|\tloss: 1558.77\n",
      "Training Epoch 40  73.4% | batch:        47 of        64\t|\tloss: 1802.06\n",
      "Training Epoch 40  75.0% | batch:        48 of        64\t|\tloss: 1728.58\n",
      "Training Epoch 40  76.6% | batch:        49 of        64\t|\tloss: 1776.35\n",
      "Training Epoch 40  78.1% | batch:        50 of        64\t|\tloss: 892.118\n",
      "Training Epoch 40  79.7% | batch:        51 of        64\t|\tloss: 1294.53\n",
      "Training Epoch 40  81.2% | batch:        52 of        64\t|\tloss: 1037.43\n",
      "Training Epoch 40  82.8% | batch:        53 of        64\t|\tloss: 1562.74\n",
      "Training Epoch 40  84.4% | batch:        54 of        64\t|\tloss: 1362.01\n",
      "Training Epoch 40  85.9% | batch:        55 of        64\t|\tloss: 4035.21\n",
      "Training Epoch 40  87.5% | batch:        56 of        64\t|\tloss: 1263.87\n",
      "Training Epoch 40  89.1% | batch:        57 of        64\t|\tloss: 2086.37\n",
      "Training Epoch 40  90.6% | batch:        58 of        64\t|\tloss: 820.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:46,803 | INFO : Epoch 40 Training Summary: epoch: 40.000000 | loss: 1754.314057 | \n",
      "2023-05-10 17:08:46,804 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1646759510040283 seconds\n",
      "\n",
      "2023-05-10 17:08:46,804 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.208682894706726 seconds\n",
      "2023-05-10 17:08:46,804 | INFO : Avg batch train. time: 0.018885670229792595 seconds\n",
      "2023-05-10 17:08:46,805 | INFO : Avg sample train. time: 0.0002993271160739787 seconds\n",
      "2023-05-10 17:08:46,805 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 40  92.2% | batch:        59 of        64\t|\tloss: 2044.46\n",
      "Training Epoch 40  93.8% | batch:        60 of        64\t|\tloss: 3885.58\n",
      "Training Epoch 40  95.3% | batch:        61 of        64\t|\tloss: 1393.2\n",
      "Training Epoch 40  96.9% | batch:        62 of        64\t|\tloss: 2882.83\n",
      "Training Epoch 40  98.4% | batch:        63 of        64\t|\tloss: 9875.93\n",
      "\n",
      "Evaluating Epoch 40   0.0% | batch:         0 of        16\t|\tloss: 1843.59\n",
      "Evaluating Epoch 40   6.2% | batch:         1 of        16\t|\tloss: 2003.45\n",
      "Evaluating Epoch 40  12.5% | batch:         2 of        16\t|\tloss: 1032.37\n",
      "Evaluating Epoch 40  18.8% | batch:         3 of        16\t|\tloss: 1752.49\n",
      "Evaluating Epoch 40  25.0% | batch:         4 of        16\t|\tloss: 1967.37\n",
      "Evaluating Epoch 40  31.2% | batch:         5 of        16\t|\tloss: 2693.08\n",
      "Evaluating Epoch 40  37.5% | batch:         6 of        16\t|\tloss: 2115.19\n",
      "Evaluating Epoch 40  43.8% | batch:         7 of        16\t|\tloss: 1997.47\n",
      "Evaluating Epoch 40  50.0% | batch:         8 of        16\t|\tloss: 1864.62\n",
      "Evaluating Epoch 40  56.2% | batch:         9 of        16\t|\tloss: 5601.68\n",
      "Evaluating Epoch 40  62.5% | batch:        10 of        16\t|\tloss: 2085.97\n",
      "Evaluating Epoch 40  68.8% | batch:        11 of        16\t|\tloss: 2524.36\n",
      "Evaluating Epoch 40  75.0% | batch:        12 of        16\t|\tloss: 1790.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:46,954 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.148820161819458 seconds\n",
      "\n",
      "2023-05-10 17:08:46,955 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.18458658998662775 seconds\n",
      "2023-05-10 17:08:46,955 | INFO : Avg batch val. time: 0.011536661874164234 seconds\n",
      "2023-05-10 17:08:46,955 | INFO : Avg sample val. time: 0.00018275899998676013 seconds\n",
      "2023-05-10 17:08:46,956 | INFO : Epoch 40 Validation Summary: epoch: 40.000000 | loss: 2212.030198 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 40  81.2% | batch:        13 of        16\t|\tloss: 1197.32\n",
      "Evaluating Epoch 40  87.5% | batch:        14 of        16\t|\tloss: 2270.63\n",
      "Evaluating Epoch 40  93.8% | batch:        15 of        16\t|\tloss: 2775.79\n",
      "\n",
      "Training Epoch 41   0.0% | batch:         0 of        64\t|\tloss: 788.953\n",
      "Training Epoch 41   1.6% | batch:         1 of        64\t|\tloss: 2451.19\n",
      "Training Epoch 41   3.1% | batch:         2 of        64\t|\tloss: 1208.52\n",
      "Training Epoch 41   4.7% | batch:         3 of        64\t|\tloss: 904.393\n",
      "Training Epoch 41   6.2% | batch:         4 of        64\t|\tloss: 919.466\n",
      "Training Epoch 41   7.8% | batch:         5 of        64\t|\tloss: 1461.74\n",
      "Training Epoch 41   9.4% | batch:         6 of        64\t|\tloss: 2655.87\n",
      "Training Epoch 41  10.9% | batch:         7 of        64\t|\tloss: 1040.11\n",
      "Training Epoch 41  12.5% | batch:         8 of        64\t|\tloss: 1242.02\n",
      "Training Epoch 41  14.1% | batch:         9 of        64\t|\tloss: 1610.07\n",
      "Training Epoch 41  15.6% | batch:        10 of        64\t|\tloss: 966.119\n",
      "Training Epoch 41  17.2% | batch:        11 of        64\t|\tloss: 1917.02\n",
      "Training Epoch 41  18.8% | batch:        12 of        64\t|\tloss: 1803.71\n",
      "Training Epoch 41  20.3% | batch:        13 of        64\t|\tloss: 1885.99\n",
      "Training Epoch 41  21.9% | batch:        14 of        64\t|\tloss: 1297.32\n",
      "Training Epoch 41  23.4% | batch:        15 of        64\t|\tloss: 1307.67\n",
      "Training Epoch 41  25.0% | batch:        16 of        64\t|\tloss: 1486.98\n",
      "Training Epoch 41  26.6% | batch:        17 of        64\t|\tloss: 1257.47\n",
      "Training Epoch 41  28.1% | batch:        18 of        64\t|\tloss: 992.737\n",
      "Training Epoch 41  29.7% | batch:        19 of        64\t|\tloss: 1367.33\n",
      "Training Epoch 41  31.2% | batch:        20 of        64\t|\tloss: 1329.81\n",
      "Training Epoch 41  32.8% | batch:        21 of        64\t|\tloss: 1373.48\n",
      "Training Epoch 41  34.4% | batch:        22 of        64\t|\tloss: 1440.06\n",
      "Training Epoch 41  35.9% | batch:        23 of        64\t|\tloss: 3120.91\n",
      "Training Epoch 41  37.5% | batch:        24 of        64\t|\tloss: 1658.83\n",
      "Training Epoch 41  39.1% | batch:        25 of        64\t|\tloss: 1534.45\n",
      "Training Epoch 41  40.6% | batch:        26 of        64\t|\tloss: 4152.04\n",
      "Training Epoch 41  42.2% | batch:        27 of        64\t|\tloss: 1293.37\n",
      "Training Epoch 41  43.8% | batch:        28 of        64\t|\tloss: 1057.12\n",
      "Training Epoch 41  45.3% | batch:        29 of        64\t|\tloss: 1028.11\n",
      "Training Epoch 41  46.9% | batch:        30 of        64\t|\tloss: 1030.94\n",
      "Training Epoch 41  48.4% | batch:        31 of        64\t|\tloss: 1794.9\n",
      "Training Epoch 41  50.0% | batch:        32 of        64\t|\tloss: 1867.82\n",
      "Training Epoch 41  51.6% | batch:        33 of        64\t|\tloss: 1075.97\n",
      "Training Epoch 41  53.1% | batch:        34 of        64\t|\tloss: 1076.39\n",
      "Training Epoch 41  54.7% | batch:        35 of        64\t|\tloss: 1547.59\n",
      "Training Epoch 41  56.2% | batch:        36 of        64\t|\tloss: 2132.93\n",
      "Training Epoch 41  57.8% | batch:        37 of        64\t|\tloss: 3444.46\n",
      "Training Epoch 41  59.4% | batch:        38 of        64\t|\tloss: 1526.51\n",
      "Training Epoch 41  60.9% | batch:        39 of        64\t|\tloss: 1365.41\n",
      "Training Epoch 41  62.5% | batch:        40 of        64\t|\tloss: 1557.49\n",
      "Training Epoch 41  64.1% | batch:        41 of        64\t|\tloss: 4040.15\n",
      "Training Epoch 41  65.6% | batch:        42 of        64\t|\tloss: 1251.33\n",
      "Training Epoch 41  67.2% | batch:        43 of        64\t|\tloss: 1560.15\n",
      "Training Epoch 41  68.8% | batch:        44 of        64\t|\tloss: 3305.35\n",
      "Training Epoch 41  70.3% | batch:        45 of        64\t|\tloss: 1397.47\n",
      "Training Epoch 41  71.9% | batch:        46 of        64\t|\tloss: 1751.97\n",
      "Training Epoch 41  73.4% | batch:        47 of        64\t|\tloss: 1151.86\n",
      "Training Epoch 41  75.0% | batch:        48 of        64\t|\tloss: 2031.88\n",
      "Training Epoch 41  76.6% | batch:        49 of        64\t|\tloss: 1140.59\n",
      "Training Epoch 41  78.1% | batch:        50 of        64\t|\tloss: 1327\n",
      "Training Epoch 41  79.7% | batch:        51 of        64\t|\tloss: 5970.04\n",
      "Training Epoch 41  81.2% | batch:        52 of        64\t|\tloss: 3172.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:48,179 | INFO : Epoch 41 Training Summary: epoch: 41.000000 | loss: 1813.577681 | \n",
      "2023-05-10 17:08:48,179 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2138609886169434 seconds\n",
      "\n",
      "2023-05-10 17:08:48,180 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.208809189680146 seconds\n",
      "2023-05-10 17:08:48,181 | INFO : Avg batch train. time: 0.018887643588752282 seconds\n",
      "2023-05-10 17:08:48,181 | INFO : Avg sample train. time: 0.0002993583926894864 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 41  82.8% | batch:        53 of        64\t|\tloss: 1380.02\n",
      "Training Epoch 41  84.4% | batch:        54 of        64\t|\tloss: 1932.94\n",
      "Training Epoch 41  85.9% | batch:        55 of        64\t|\tloss: 1100.89\n",
      "Training Epoch 41  87.5% | batch:        56 of        64\t|\tloss: 2156.92\n",
      "Training Epoch 41  89.1% | batch:        57 of        64\t|\tloss: 1678.6\n",
      "Training Epoch 41  90.6% | batch:        58 of        64\t|\tloss: 1552.24\n",
      "Training Epoch 41  92.2% | batch:        59 of        64\t|\tloss: 3301.17\n",
      "Training Epoch 41  93.8% | batch:        60 of        64\t|\tloss: 1520.94\n",
      "Training Epoch 41  95.3% | batch:        61 of        64\t|\tloss: 5082.72\n",
      "Training Epoch 41  96.9% | batch:        62 of        64\t|\tloss: 1477.37\n",
      "Training Epoch 41  98.4% | batch:        63 of        64\t|\tloss: 1792.38\n",
      "\n",
      "Training Epoch 42   0.0% | batch:         0 of        64\t|\tloss: 987.877\n",
      "Training Epoch 42   1.6% | batch:         1 of        64\t|\tloss: 1945.34\n",
      "Training Epoch 42   3.1% | batch:         2 of        64\t|\tloss: 1284.54\n",
      "Training Epoch 42   4.7% | batch:         3 of        64\t|\tloss: 2012.53\n",
      "Training Epoch 42   6.2% | batch:         4 of        64\t|\tloss: 2513.3\n",
      "Training Epoch 42   7.8% | batch:         5 of        64\t|\tloss: 1211.62\n",
      "Training Epoch 42   9.4% | batch:         6 of        64\t|\tloss: 1048.17\n",
      "Training Epoch 42  10.9% | batch:         7 of        64\t|\tloss: 1199.69\n",
      "Training Epoch 42  12.5% | batch:         8 of        64\t|\tloss: 588.756\n",
      "Training Epoch 42  14.1% | batch:         9 of        64\t|\tloss: 1559.18\n",
      "Training Epoch 42  15.6% | batch:        10 of        64\t|\tloss: 2706.28\n",
      "Training Epoch 42  17.2% | batch:        11 of        64\t|\tloss: 1189.43\n",
      "Training Epoch 42  18.8% | batch:        12 of        64\t|\tloss: 1993.42\n",
      "Training Epoch 42  20.3% | batch:        13 of        64\t|\tloss: 1930.87\n",
      "Training Epoch 42  21.9% | batch:        14 of        64\t|\tloss: 1790.72\n",
      "Training Epoch 42  23.4% | batch:        15 of        64\t|\tloss: 2024.77\n",
      "Training Epoch 42  25.0% | batch:        16 of        64\t|\tloss: 1136.06\n",
      "Training Epoch 42  26.6% | batch:        17 of        64\t|\tloss: 1371.93\n",
      "Training Epoch 42  28.1% | batch:        18 of        64\t|\tloss: 1068.28\n",
      "Training Epoch 42  29.7% | batch:        19 of        64\t|\tloss: 1150.49\n",
      "Training Epoch 42  31.2% | batch:        20 of        64\t|\tloss: 1191.19\n",
      "Training Epoch 42  32.8% | batch:        21 of        64\t|\tloss: 1666.8\n",
      "Training Epoch 42  34.4% | batch:        22 of        64\t|\tloss: 2130.02\n",
      "Training Epoch 42  35.9% | batch:        23 of        64\t|\tloss: 1201.28\n",
      "Training Epoch 42  37.5% | batch:        24 of        64\t|\tloss: 1530.14\n",
      "Training Epoch 42  39.1% | batch:        25 of        64\t|\tloss: 1173.05\n",
      "Training Epoch 42  40.6% | batch:        26 of        64\t|\tloss: 1798.94\n",
      "Training Epoch 42  42.2% | batch:        27 of        64\t|\tloss: 999.292\n",
      "Training Epoch 42  43.8% | batch:        28 of        64\t|\tloss: 758.044\n",
      "Training Epoch 42  45.3% | batch:        29 of        64\t|\tloss: 3662.51\n",
      "Training Epoch 42  46.9% | batch:        30 of        64\t|\tloss: 1183.24\n",
      "Training Epoch 42  48.4% | batch:        31 of        64\t|\tloss: 1256.55\n",
      "Training Epoch 42  50.0% | batch:        32 of        64\t|\tloss: 1386.05\n",
      "Training Epoch 42  51.6% | batch:        33 of        64\t|\tloss: 1718.3\n",
      "Training Epoch 42  53.1% | batch:        34 of        64\t|\tloss: 1412.05\n",
      "Training Epoch 42  54.7% | batch:        35 of        64\t|\tloss: 1115.73\n",
      "Training Epoch 42  56.2% | batch:        36 of        64\t|\tloss: 2304.78\n",
      "Training Epoch 42  57.8% | batch:        37 of        64\t|\tloss: 1076.98\n",
      "Training Epoch 42  59.4% | batch:        38 of        64\t|\tloss: 1678.43\n",
      "Training Epoch 42  60.9% | batch:        39 of        64\t|\tloss: 1510.12\n",
      "Training Epoch 42  62.5% | batch:        40 of        64\t|\tloss: 1389.29\n",
      "Training Epoch 42  64.1% | batch:        41 of        64\t|\tloss: 2167.16\n",
      "Training Epoch 42  65.6% | batch:        42 of        64\t|\tloss: 1705.9\n",
      "Training Epoch 42  67.2% | batch:        43 of        64\t|\tloss: 1686.23\n",
      "Training Epoch 42  68.8% | batch:        44 of        64\t|\tloss: 1279.43\n",
      "Training Epoch 42  70.3% | batch:        45 of        64\t|\tloss: 2385.98\n",
      "Training Epoch 42  71.9% | batch:        46 of        64\t|\tloss: 1655.45\n",
      "Training Epoch 42  73.4% | batch:        47 of        64\t|\tloss: 987.152\n",
      "Training Epoch 42  75.0% | batch:        48 of        64\t|\tloss: 1821.26\n",
      "Training Epoch 42  76.6% | batch:        49 of        64\t|\tloss: 1079.54\n",
      "Training Epoch 42  78.1% | batch:        50 of        64\t|\tloss: 1010.65\n",
      "Training Epoch 42  79.7% | batch:        51 of        64\t|\tloss: 687.444\n",
      "Training Epoch 42  81.2% | batch:        52 of        64\t|\tloss: 1616.83\n",
      "Training Epoch 42  82.8% | batch:        53 of        64\t|\tloss: 1525.76\n",
      "Training Epoch 42  84.4% | batch:        54 of        64\t|\tloss: 1905.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:49,373 | INFO : Epoch 42 Training Summary: epoch: 42.000000 | loss: 1542.256644 | \n",
      "2023-05-10 17:08:49,374 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1798756122589111 seconds\n",
      "\n",
      "2023-05-10 17:08:49,375 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2081202949796404 seconds\n",
      "2023-05-10 17:08:49,375 | INFO : Avg batch train. time: 0.01887687960905688 seconds\n",
      "2023-05-10 17:08:49,376 | INFO : Avg sample train. time: 0.0002991877897423577 seconds\n",
      "2023-05-10 17:08:49,376 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 42  85.9% | batch:        55 of        64\t|\tloss: 1770.49\n",
      "Training Epoch 42  87.5% | batch:        56 of        64\t|\tloss: 1953.16\n",
      "Training Epoch 42  89.1% | batch:        57 of        64\t|\tloss: 1510.84\n",
      "Training Epoch 42  90.6% | batch:        58 of        64\t|\tloss: 1540.41\n",
      "Training Epoch 42  92.2% | batch:        59 of        64\t|\tloss: 1449.88\n",
      "Training Epoch 42  93.8% | batch:        60 of        64\t|\tloss: 1211.77\n",
      "Training Epoch 42  95.3% | batch:        61 of        64\t|\tloss: 1267.44\n",
      "Training Epoch 42  96.9% | batch:        62 of        64\t|\tloss: 2120.03\n",
      "Training Epoch 42  98.4% | batch:        63 of        64\t|\tloss: 1203.38\n",
      "\n",
      "Evaluating Epoch 42   0.0% | batch:         0 of        16\t|\tloss: 1641.09\n",
      "Evaluating Epoch 42   6.2% | batch:         1 of        16\t|\tloss: 1227.94\n",
      "Evaluating Epoch 42  12.5% | batch:         2 of        16\t|\tloss: 1023.85\n",
      "Evaluating Epoch 42  18.8% | batch:         3 of        16\t|\tloss: 1334.51\n",
      "Evaluating Epoch 42  25.0% | batch:         4 of        16\t|\tloss: 1193.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:49,527 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.15066742897033691 seconds\n",
      "\n",
      "2023-05-10 17:08:49,528 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.18311184385548468 seconds\n",
      "2023-05-10 17:08:49,528 | INFO : Avg batch val. time: 0.011444490240967792 seconds\n",
      "2023-05-10 17:08:49,529 | INFO : Avg sample val. time: 0.00018129885530246008 seconds\n",
      "2023-05-10 17:08:49,530 | INFO : Epoch 42 Validation Summary: epoch: 42.000000 | loss: 1919.065965 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 42  31.2% | batch:         5 of        16\t|\tloss: 2265.97\n",
      "Evaluating Epoch 42  37.5% | batch:         6 of        16\t|\tloss: 1687\n",
      "Evaluating Epoch 42  43.8% | batch:         7 of        16\t|\tloss: 1947.3\n",
      "Evaluating Epoch 42  50.0% | batch:         8 of        16\t|\tloss: 1254.9\n",
      "Evaluating Epoch 42  56.2% | batch:         9 of        16\t|\tloss: 6500.33\n",
      "Evaluating Epoch 42  62.5% | batch:        10 of        16\t|\tloss: 1479.52\n",
      "Evaluating Epoch 42  68.8% | batch:        11 of        16\t|\tloss: 2374.86\n",
      "Evaluating Epoch 42  75.0% | batch:        12 of        16\t|\tloss: 1031.38\n",
      "Evaluating Epoch 42  81.2% | batch:        13 of        16\t|\tloss: 1169.9\n",
      "Evaluating Epoch 42  87.5% | batch:        14 of        16\t|\tloss: 1896.68\n",
      "Evaluating Epoch 42  93.8% | batch:        15 of        16\t|\tloss: 2888.07\n",
      "\n",
      "Training Epoch 43   0.0% | batch:         0 of        64\t|\tloss: 1717.92\n",
      "Training Epoch 43   1.6% | batch:         1 of        64\t|\tloss: 2189.6\n",
      "Training Epoch 43   3.1% | batch:         2 of        64\t|\tloss: 965.456\n",
      "Training Epoch 43   4.7% | batch:         3 of        64\t|\tloss: 2468.65\n",
      "Training Epoch 43   6.2% | batch:         4 of        64\t|\tloss: 1806.24\n",
      "Training Epoch 43   7.8% | batch:         5 of        64\t|\tloss: 1245.47\n",
      "Training Epoch 43   9.4% | batch:         6 of        64\t|\tloss: 2155.45\n",
      "Training Epoch 43  10.9% | batch:         7 of        64\t|\tloss: 1433.16\n",
      "Training Epoch 43  12.5% | batch:         8 of        64\t|\tloss: 1251.93\n",
      "Training Epoch 43  14.1% | batch:         9 of        64\t|\tloss: 4151.32\n",
      "Training Epoch 43  15.6% | batch:        10 of        64\t|\tloss: 1697.25\n",
      "Training Epoch 43  17.2% | batch:        11 of        64\t|\tloss: 1541.58\n",
      "Training Epoch 43  18.8% | batch:        12 of        64\t|\tloss: 1041.69\n",
      "Training Epoch 43  20.3% | batch:        13 of        64\t|\tloss: 1305.74\n",
      "Training Epoch 43  21.9% | batch:        14 of        64\t|\tloss: 1612.99\n",
      "Training Epoch 43  23.4% | batch:        15 of        64\t|\tloss: 1727.23\n",
      "Training Epoch 43  25.0% | batch:        16 of        64\t|\tloss: 2363.47\n",
      "Training Epoch 43  26.6% | batch:        17 of        64\t|\tloss: 1610.45\n",
      "Training Epoch 43  28.1% | batch:        18 of        64\t|\tloss: 1416.99\n",
      "Training Epoch 43  29.7% | batch:        19 of        64\t|\tloss: 1876.29\n",
      "Training Epoch 43  31.2% | batch:        20 of        64\t|\tloss: 962.468\n",
      "Training Epoch 43  32.8% | batch:        21 of        64\t|\tloss: 1100.96\n",
      "Training Epoch 43  34.4% | batch:        22 of        64\t|\tloss: 3897.15\n",
      "Training Epoch 43  35.9% | batch:        23 of        64\t|\tloss: 827.705\n",
      "Training Epoch 43  37.5% | batch:        24 of        64\t|\tloss: 3081.88\n",
      "Training Epoch 43  39.1% | batch:        25 of        64\t|\tloss: 1036.54\n",
      "Training Epoch 43  40.6% | batch:        26 of        64\t|\tloss: 1582.68\n",
      "Training Epoch 43  42.2% | batch:        27 of        64\t|\tloss: 1320.93\n",
      "Training Epoch 43  43.8% | batch:        28 of        64\t|\tloss: 4120.54\n",
      "Training Epoch 43  45.3% | batch:        29 of        64\t|\tloss: 1520.82\n",
      "Training Epoch 43  46.9% | batch:        30 of        64\t|\tloss: 970.183\n",
      "Training Epoch 43  48.4% | batch:        31 of        64\t|\tloss: 1464.28\n",
      "Training Epoch 43  50.0% | batch:        32 of        64\t|\tloss: 1371.9\n",
      "Training Epoch 43  51.6% | batch:        33 of        64\t|\tloss: 1464.7\n",
      "Training Epoch 43  53.1% | batch:        34 of        64\t|\tloss: 1889.88\n",
      "Training Epoch 43  54.7% | batch:        35 of        64\t|\tloss: 2814.2\n",
      "Training Epoch 43  56.2% | batch:        36 of        64\t|\tloss: 1050.08\n",
      "Training Epoch 43  57.8% | batch:        37 of        64\t|\tloss: 979.953\n",
      "Training Epoch 43  59.4% | batch:        38 of        64\t|\tloss: 1328.57\n",
      "Training Epoch 43  60.9% | batch:        39 of        64\t|\tloss: 1244.14\n",
      "Training Epoch 43  62.5% | batch:        40 of        64\t|\tloss: 1084.46\n",
      "Training Epoch 43  64.1% | batch:        41 of        64\t|\tloss: 1326.37\n",
      "Training Epoch 43  65.6% | batch:        42 of        64\t|\tloss: 1665.34\n",
      "Training Epoch 43  67.2% | batch:        43 of        64\t|\tloss: 1192.26\n",
      "Training Epoch 43  68.8% | batch:        44 of        64\t|\tloss: 4428.1\n",
      "Training Epoch 43  70.3% | batch:        45 of        64\t|\tloss: 1804.79\n",
      "Training Epoch 43  71.9% | batch:        46 of        64\t|\tloss: 1211.42\n",
      "Training Epoch 43  73.4% | batch:        47 of        64\t|\tloss: 1705.5\n",
      "Training Epoch 43  75.0% | batch:        48 of        64\t|\tloss: 1275.24\n",
      "Training Epoch 43  76.6% | batch:        49 of        64\t|\tloss: 1584.09\n",
      "Training Epoch 43  78.1% | batch:        50 of        64\t|\tloss: 2141.1\n",
      "Training Epoch 43  79.7% | batch:        51 of        64\t|\tloss: 2811.85\n",
      "Training Epoch 43  81.2% | batch:        52 of        64\t|\tloss: 2382.02\n",
      "Training Epoch 43  82.8% | batch:        53 of        64\t|\tloss: 1221.7\n",
      "Training Epoch 43  84.4% | batch:        54 of        64\t|\tloss: 1122.45\n",
      "Training Epoch 43  85.9% | batch:        55 of        64\t|\tloss: 1934.3\n",
      "Training Epoch 43  87.5% | batch:        56 of        64\t|\tloss: 2186.03\n",
      "Training Epoch 43  89.1% | batch:        57 of        64\t|\tloss: 1394.7\n",
      "Training Epoch 43  90.6% | batch:        58 of        64\t|\tloss: 1031.54\n",
      "Training Epoch 43  92.2% | batch:        59 of        64\t|\tloss: 1279.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:50,722 | INFO : Epoch 43 Training Summary: epoch: 43.000000 | loss: 1749.968450 | \n",
      "2023-05-10 17:08:50,722 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1825027465820312 seconds\n",
      "\n",
      "2023-05-10 17:08:50,723 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2075245380401611 seconds\n",
      "2023-05-10 17:08:50,723 | INFO : Avg batch train. time: 0.018867570906877518 seconds\n",
      "2023-05-10 17:08:50,724 | INFO : Avg sample train. time: 0.00029904025211494826 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 43  93.8% | batch:        60 of        64\t|\tloss: 1348.29\n",
      "Training Epoch 43  95.3% | batch:        61 of        64\t|\tloss: 1371.78\n",
      "Training Epoch 43  96.9% | batch:        62 of        64\t|\tloss: 3071.62\n",
      "Training Epoch 43  98.4% | batch:        63 of        64\t|\tloss: 2440.37\n",
      "\n",
      "Training Epoch 44   0.0% | batch:         0 of        64\t|\tloss: 1155.07\n",
      "Training Epoch 44   1.6% | batch:         1 of        64\t|\tloss: 781.551\n",
      "Training Epoch 44   3.1% | batch:         2 of        64\t|\tloss: 1001.61\n",
      "Training Epoch 44   4.7% | batch:         3 of        64\t|\tloss: 2621.57\n",
      "Training Epoch 44   6.2% | batch:         4 of        64\t|\tloss: 2940.41\n",
      "Training Epoch 44   7.8% | batch:         5 of        64\t|\tloss: 1956.98\n",
      "Training Epoch 44   9.4% | batch:         6 of        64\t|\tloss: 1076.38\n",
      "Training Epoch 44  10.9% | batch:         7 of        64\t|\tloss: 1800.79\n",
      "Training Epoch 44  12.5% | batch:         8 of        64\t|\tloss: 3469.19\n",
      "Training Epoch 44  14.1% | batch:         9 of        64\t|\tloss: 1854.49\n",
      "Training Epoch 44  15.6% | batch:        10 of        64\t|\tloss: 1969.39\n",
      "Training Epoch 44  17.2% | batch:        11 of        64\t|\tloss: 569.371\n",
      "Training Epoch 44  18.8% | batch:        12 of        64\t|\tloss: 1430.96\n",
      "Training Epoch 44  20.3% | batch:        13 of        64\t|\tloss: 1256.32\n",
      "Training Epoch 44  21.9% | batch:        14 of        64\t|\tloss: 1438.08\n",
      "Training Epoch 44  23.4% | batch:        15 of        64\t|\tloss: 648.307\n",
      "Training Epoch 44  25.0% | batch:        16 of        64\t|\tloss: 2412.03\n",
      "Training Epoch 44  26.6% | batch:        17 of        64\t|\tloss: 1383.31\n",
      "Training Epoch 44  28.1% | batch:        18 of        64\t|\tloss: 884.292\n",
      "Training Epoch 44  29.7% | batch:        19 of        64\t|\tloss: 849.461\n",
      "Training Epoch 44  31.2% | batch:        20 of        64\t|\tloss: 1323.82\n",
      "Training Epoch 44  32.8% | batch:        21 of        64\t|\tloss: 807.892\n",
      "Training Epoch 44  34.4% | batch:        22 of        64\t|\tloss: 1016.48\n",
      "Training Epoch 44  35.9% | batch:        23 of        64\t|\tloss: 1313.8\n",
      "Training Epoch 44  37.5% | batch:        24 of        64\t|\tloss: 980.239\n",
      "Training Epoch 44  39.1% | batch:        25 of        64\t|\tloss: 1045.14\n",
      "Training Epoch 44  40.6% | batch:        26 of        64\t|\tloss: 1326.54\n",
      "Training Epoch 44  42.2% | batch:        27 of        64\t|\tloss: 926.184\n",
      "Training Epoch 44  43.8% | batch:        28 of        64\t|\tloss: 1332.91\n",
      "Training Epoch 44  45.3% | batch:        29 of        64\t|\tloss: 1064.04\n",
      "Training Epoch 44  46.9% | batch:        30 of        64\t|\tloss: 1061.28\n",
      "Training Epoch 44  48.4% | batch:        31 of        64\t|\tloss: 1635.83\n",
      "Training Epoch 44  50.0% | batch:        32 of        64\t|\tloss: 884.338\n",
      "Training Epoch 44  51.6% | batch:        33 of        64\t|\tloss: 934.599\n",
      "Training Epoch 44  53.1% | batch:        34 of        64\t|\tloss: 1928.11\n",
      "Training Epoch 44  54.7% | batch:        35 of        64\t|\tloss: 2889.5\n",
      "Training Epoch 44  56.2% | batch:        36 of        64\t|\tloss: 1064.46\n",
      "Training Epoch 44  57.8% | batch:        37 of        64\t|\tloss: 1481.81\n",
      "Training Epoch 44  59.4% | batch:        38 of        64\t|\tloss: 1009.52\n",
      "Training Epoch 44  60.9% | batch:        39 of        64\t|\tloss: 2633.5\n",
      "Training Epoch 44  62.5% | batch:        40 of        64\t|\tloss: 1217.49\n",
      "Training Epoch 44  64.1% | batch:        41 of        64\t|\tloss: 3976.18\n",
      "Training Epoch 44  65.6% | batch:        42 of        64\t|\tloss: 1184.83\n",
      "Training Epoch 44  67.2% | batch:        43 of        64\t|\tloss: 2099.73\n",
      "Training Epoch 44  68.8% | batch:        44 of        64\t|\tloss: 2115.4\n",
      "Training Epoch 44  70.3% | batch:        45 of        64\t|\tloss: 2041.46\n",
      "Training Epoch 44  71.9% | batch:        46 of        64\t|\tloss: 1323.96\n",
      "Training Epoch 44  73.4% | batch:        47 of        64\t|\tloss: 1195.56\n",
      "Training Epoch 44  75.0% | batch:        48 of        64\t|\tloss: 3305.23\n",
      "Training Epoch 44  76.6% | batch:        49 of        64\t|\tloss: 897.912\n",
      "Training Epoch 44  78.1% | batch:        50 of        64\t|\tloss: 1464.89\n",
      "Training Epoch 44  79.7% | batch:        51 of        64\t|\tloss: 2126.51\n",
      "Training Epoch 44  81.2% | batch:        52 of        64\t|\tloss: 2482.02\n",
      "Training Epoch 44  82.8% | batch:        53 of        64\t|\tloss: 3202.32\n",
      "Training Epoch 44  84.4% | batch:        54 of        64\t|\tloss: 1145.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:51,866 | INFO : Epoch 44 Training Summary: epoch: 44.000000 | loss: 1599.221898 | \n",
      "2023-05-10 17:08:51,867 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1326353549957275 seconds\n",
      "\n",
      "2023-05-10 17:08:51,867 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2058225111527876 seconds\n",
      "2023-05-10 17:08:51,868 | INFO : Avg batch train. time: 0.018840976736762306 seconds\n",
      "2023-05-10 17:08:51,868 | INFO : Avg sample train. time: 0.0002986187496663664 seconds\n",
      "2023-05-10 17:08:51,868 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 44  85.9% | batch:        55 of        64\t|\tloss: 1360.24\n",
      "Training Epoch 44  87.5% | batch:        56 of        64\t|\tloss: 1645.33\n",
      "Training Epoch 44  89.1% | batch:        57 of        64\t|\tloss: 1061.35\n",
      "Training Epoch 44  90.6% | batch:        58 of        64\t|\tloss: 1325.45\n",
      "Training Epoch 44  92.2% | batch:        59 of        64\t|\tloss: 1335.49\n",
      "Training Epoch 44  93.8% | batch:        60 of        64\t|\tloss: 1218.1\n",
      "Training Epoch 44  95.3% | batch:        61 of        64\t|\tloss: 1331.87\n",
      "Training Epoch 44  96.9% | batch:        62 of        64\t|\tloss: 3554.02\n",
      "Training Epoch 44  98.4% | batch:        63 of        64\t|\tloss: 1390.92\n",
      "\n",
      "Evaluating Epoch 44   0.0% | batch:         0 of        16\t|\tloss: 2020.85\n",
      "Evaluating Epoch 44   6.2% | batch:         1 of        16\t|\tloss: 1776.99\n",
      "Evaluating Epoch 44  12.5% | batch:         2 of        16\t|\tloss: 1112.16\n",
      "Evaluating Epoch 44  18.8% | batch:         3 of        16\t|\tloss: 2139.57\n",
      "Evaluating Epoch 44  25.0% | batch:         4 of        16\t|\tloss: 1557.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:52,017 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.1476306915283203 seconds\n",
      "\n",
      "2023-05-10 17:08:52,017 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.1816334625085195 seconds\n",
      "2023-05-10 17:08:52,017 | INFO : Avg batch val. time: 0.011352091406782469 seconds\n",
      "2023-05-10 17:08:52,018 | INFO : Avg sample val. time: 0.00017983511139457376 seconds\n",
      "2023-05-10 17:08:52,019 | INFO : Epoch 44 Validation Summary: epoch: 44.000000 | loss: 2247.406227 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 44  31.2% | batch:         5 of        16\t|\tloss: 2271.78\n",
      "Evaluating Epoch 44  37.5% | batch:         6 of        16\t|\tloss: 1869.03\n",
      "Evaluating Epoch 44  43.8% | batch:         7 of        16\t|\tloss: 3078.34\n",
      "Evaluating Epoch 44  50.0% | batch:         8 of        16\t|\tloss: 1773.89\n",
      "Evaluating Epoch 44  56.2% | batch:         9 of        16\t|\tloss: 5108.6\n",
      "Evaluating Epoch 44  62.5% | batch:        10 of        16\t|\tloss: 2078.45\n",
      "Evaluating Epoch 44  68.8% | batch:        11 of        16\t|\tloss: 4070.71\n",
      "Evaluating Epoch 44  75.0% | batch:        12 of        16\t|\tloss: 1242.84\n",
      "Evaluating Epoch 44  81.2% | batch:        13 of        16\t|\tloss: 1247.81\n",
      "Evaluating Epoch 44  87.5% | batch:        14 of        16\t|\tloss: 2109.41\n",
      "Evaluating Epoch 44  93.8% | batch:        15 of        16\t|\tloss: 2571.36\n",
      "\n",
      "Training Epoch 45   0.0% | batch:         0 of        64\t|\tloss: 3290.43\n",
      "Training Epoch 45   1.6% | batch:         1 of        64\t|\tloss: 1220.6\n",
      "Training Epoch 45   3.1% | batch:         2 of        64\t|\tloss: 1140.65\n",
      "Training Epoch 45   4.7% | batch:         3 of        64\t|\tloss: 2060.23\n",
      "Training Epoch 45   6.2% | batch:         4 of        64\t|\tloss: 1203.5\n",
      "Training Epoch 45   7.8% | batch:         5 of        64\t|\tloss: 817.189\n",
      "Training Epoch 45   9.4% | batch:         6 of        64\t|\tloss: 846.578\n",
      "Training Epoch 45  10.9% | batch:         7 of        64\t|\tloss: 2787.22\n",
      "Training Epoch 45  12.5% | batch:         8 of        64\t|\tloss: 784.58\n",
      "Training Epoch 45  14.1% | batch:         9 of        64\t|\tloss: 2251.6\n",
      "Training Epoch 45  15.6% | batch:        10 of        64\t|\tloss: 1714.36\n",
      "Training Epoch 45  17.2% | batch:        11 of        64\t|\tloss: 1621.09\n",
      "Training Epoch 45  18.8% | batch:        12 of        64\t|\tloss: 2910.21\n",
      "Training Epoch 45  20.3% | batch:        13 of        64\t|\tloss: 1058.28\n",
      "Training Epoch 45  21.9% | batch:        14 of        64\t|\tloss: 1070.46\n",
      "Training Epoch 45  23.4% | batch:        15 of        64\t|\tloss: 2628.92\n",
      "Training Epoch 45  25.0% | batch:        16 of        64\t|\tloss: 3483.39\n",
      "Training Epoch 45  26.6% | batch:        17 of        64\t|\tloss: 1097.02\n",
      "Training Epoch 45  28.1% | batch:        18 of        64\t|\tloss: 1262.1\n",
      "Training Epoch 45  29.7% | batch:        19 of        64\t|\tloss: 983.656\n",
      "Training Epoch 45  31.2% | batch:        20 of        64\t|\tloss: 841.344\n",
      "Training Epoch 45  32.8% | batch:        21 of        64\t|\tloss: 1166.79\n",
      "Training Epoch 45  34.4% | batch:        22 of        64\t|\tloss: 2339.44\n",
      "Training Epoch 45  35.9% | batch:        23 of        64\t|\tloss: 1620.07\n",
      "Training Epoch 45  37.5% | batch:        24 of        64\t|\tloss: 2108.62\n",
      "Training Epoch 45  39.1% | batch:        25 of        64\t|\tloss: 1089.57\n",
      "Training Epoch 45  40.6% | batch:        26 of        64\t|\tloss: 2473.07\n",
      "Training Epoch 45  42.2% | batch:        27 of        64\t|\tloss: 1176.17\n",
      "Training Epoch 45  43.8% | batch:        28 of        64\t|\tloss: 1010.56\n",
      "Training Epoch 45  45.3% | batch:        29 of        64\t|\tloss: 1082.88\n",
      "Training Epoch 45  46.9% | batch:        30 of        64\t|\tloss: 1388.69\n",
      "Training Epoch 45  48.4% | batch:        31 of        64\t|\tloss: 805.687\n",
      "Training Epoch 45  50.0% | batch:        32 of        64\t|\tloss: 2506.41\n",
      "Training Epoch 45  51.6% | batch:        33 of        64\t|\tloss: 1224.85\n",
      "Training Epoch 45  53.1% | batch:        34 of        64\t|\tloss: 1296.33\n",
      "Training Epoch 45  54.7% | batch:        35 of        64\t|\tloss: 1068.64\n",
      "Training Epoch 45  56.2% | batch:        36 of        64\t|\tloss: 6323.29\n",
      "Training Epoch 45  57.8% | batch:        37 of        64\t|\tloss: 1604.57\n",
      "Training Epoch 45  59.4% | batch:        38 of        64\t|\tloss: 1737.03\n",
      "Training Epoch 45  60.9% | batch:        39 of        64\t|\tloss: 897.193\n",
      "Training Epoch 45  62.5% | batch:        40 of        64\t|\tloss: 1206.33\n",
      "Training Epoch 45  64.1% | batch:        41 of        64\t|\tloss: 1079.63\n",
      "Training Epoch 45  65.6% | batch:        42 of        64\t|\tloss: 1325.08\n",
      "Training Epoch 45  67.2% | batch:        43 of        64\t|\tloss: 2456.32\n",
      "Training Epoch 45  68.8% | batch:        44 of        64\t|\tloss: 2241.75\n",
      "Training Epoch 45  70.3% | batch:        45 of        64\t|\tloss: 1009.09\n",
      "Training Epoch 45  71.9% | batch:        46 of        64\t|\tloss: 1769.02\n",
      "Training Epoch 45  73.4% | batch:        47 of        64\t|\tloss: 2416.75\n",
      "Training Epoch 45  75.0% | batch:        48 of        64\t|\tloss: 2745.84\n",
      "Training Epoch 45  76.6% | batch:        49 of        64\t|\tloss: 1681.68\n",
      "Training Epoch 45  78.1% | batch:        50 of        64\t|\tloss: 1167.3\n",
      "Training Epoch 45  79.7% | batch:        51 of        64\t|\tloss: 2656.64\n",
      "Training Epoch 45  81.2% | batch:        52 of        64\t|\tloss: 1029.2\n",
      "Training Epoch 45  82.8% | batch:        53 of        64\t|\tloss: 1798.23\n",
      "Training Epoch 45  84.4% | batch:        54 of        64\t|\tloss: 2358.45\n",
      "Training Epoch 45  85.9% | batch:        55 of        64\t|\tloss: 1249.89\n",
      "Training Epoch 45  87.5% | batch:        56 of        64\t|\tloss: 1934.41\n",
      "Training Epoch 45  89.1% | batch:        57 of        64\t|\tloss: 1539.93\n",
      "Training Epoch 45  90.6% | batch:        58 of        64\t|\tloss: 1429.84\n",
      "Training Epoch 45  92.2% | batch:        59 of        64\t|\tloss: 923.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:53,237 | INFO : Epoch 45 Training Summary: epoch: 45.000000 | loss: 1685.867623 | \n",
      "2023-05-10 17:08:53,238 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2091875076293945 seconds\n",
      "\n",
      "2023-05-10 17:08:53,239 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2058972888522679 seconds\n",
      "2023-05-10 17:08:53,239 | INFO : Avg batch train. time: 0.018842145138316686 seconds\n",
      "2023-05-10 17:08:53,239 | INFO : Avg sample train. time: 0.0002986372681654948 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 45  93.8% | batch:        60 of        64\t|\tloss: 1049.81\n",
      "Training Epoch 45  95.3% | batch:        61 of        64\t|\tloss: 976.006\n",
      "Training Epoch 45  96.9% | batch:        62 of        64\t|\tloss: 1651.66\n",
      "Training Epoch 45  98.4% | batch:        63 of        64\t|\tloss: 7229.5\n",
      "\n",
      "Training Epoch 46   0.0% | batch:         0 of        64\t|\tloss: 1584.05\n",
      "Training Epoch 46   1.6% | batch:         1 of        64\t|\tloss: 1215.97\n",
      "Training Epoch 46   3.1% | batch:         2 of        64\t|\tloss: 1400.22\n",
      "Training Epoch 46   4.7% | batch:         3 of        64\t|\tloss: 1088.07\n",
      "Training Epoch 46   6.2% | batch:         4 of        64\t|\tloss: 3144.14\n",
      "Training Epoch 46   7.8% | batch:         5 of        64\t|\tloss: 1396.28\n",
      "Training Epoch 46   9.4% | batch:         6 of        64\t|\tloss: 1190.16\n",
      "Training Epoch 46  10.9% | batch:         7 of        64\t|\tloss: 2148.31\n",
      "Training Epoch 46  12.5% | batch:         8 of        64\t|\tloss: 946.679\n",
      "Training Epoch 46  14.1% | batch:         9 of        64\t|\tloss: 908.785\n",
      "Training Epoch 46  15.6% | batch:        10 of        64\t|\tloss: 747.896\n",
      "Training Epoch 46  17.2% | batch:        11 of        64\t|\tloss: 1416.09\n",
      "Training Epoch 46  18.8% | batch:        12 of        64\t|\tloss: 711.223\n",
      "Training Epoch 46  20.3% | batch:        13 of        64\t|\tloss: 1301.76\n",
      "Training Epoch 46  21.9% | batch:        14 of        64\t|\tloss: 1066\n",
      "Training Epoch 46  23.4% | batch:        15 of        64\t|\tloss: 1005.79\n",
      "Training Epoch 46  25.0% | batch:        16 of        64\t|\tloss: 1185.54\n",
      "Training Epoch 46  26.6% | batch:        17 of        64\t|\tloss: 1117.11\n",
      "Training Epoch 46  28.1% | batch:        18 of        64\t|\tloss: 1229.44\n",
      "Training Epoch 46  29.7% | batch:        19 of        64\t|\tloss: 1122.43\n",
      "Training Epoch 46  31.2% | batch:        20 of        64\t|\tloss: 1752.72\n",
      "Training Epoch 46  32.8% | batch:        21 of        64\t|\tloss: 3416.35\n",
      "Training Epoch 46  34.4% | batch:        22 of        64\t|\tloss: 1231.98\n",
      "Training Epoch 46  35.9% | batch:        23 of        64\t|\tloss: 1839.09\n",
      "Training Epoch 46  37.5% | batch:        24 of        64\t|\tloss: 1538.43\n",
      "Training Epoch 46  39.1% | batch:        25 of        64\t|\tloss: 1583.9\n",
      "Training Epoch 46  40.6% | batch:        26 of        64\t|\tloss: 714.756\n",
      "Training Epoch 46  42.2% | batch:        27 of        64\t|\tloss: 1349.81\n",
      "Training Epoch 46  43.8% | batch:        28 of        64\t|\tloss: 1981.99\n",
      "Training Epoch 46  45.3% | batch:        29 of        64\t|\tloss: 1312.05\n",
      "Training Epoch 46  46.9% | batch:        30 of        64\t|\tloss: 1728.43\n",
      "Training Epoch 46  48.4% | batch:        31 of        64\t|\tloss: 1417.67\n",
      "Training Epoch 46  50.0% | batch:        32 of        64\t|\tloss: 1386.99\n",
      "Training Epoch 46  51.6% | batch:        33 of        64\t|\tloss: 3238.77\n",
      "Training Epoch 46  53.1% | batch:        34 of        64\t|\tloss: 863.885\n",
      "Training Epoch 46  54.7% | batch:        35 of        64\t|\tloss: 1271.59\n",
      "Training Epoch 46  56.2% | batch:        36 of        64\t|\tloss: 3426.92\n",
      "Training Epoch 46  57.8% | batch:        37 of        64\t|\tloss: 1871.08\n",
      "Training Epoch 46  59.4% | batch:        38 of        64\t|\tloss: 1191.24\n",
      "Training Epoch 46  60.9% | batch:        39 of        64\t|\tloss: 2939.91\n",
      "Training Epoch 46  62.5% | batch:        40 of        64\t|\tloss: 1075.84\n",
      "Training Epoch 46  64.1% | batch:        41 of        64\t|\tloss: 1290.31\n",
      "Training Epoch 46  65.6% | batch:        42 of        64\t|\tloss: 2449.96\n",
      "Training Epoch 46  67.2% | batch:        43 of        64\t|\tloss: 1622.13\n",
      "Training Epoch 46  68.8% | batch:        44 of        64\t|\tloss: 2561.48\n",
      "Training Epoch 46  70.3% | batch:        45 of        64\t|\tloss: 1528.67\n",
      "Training Epoch 46  71.9% | batch:        46 of        64\t|\tloss: 1194.34\n",
      "Training Epoch 46  73.4% | batch:        47 of        64\t|\tloss: 798.439\n",
      "Training Epoch 46  75.0% | batch:        48 of        64\t|\tloss: 939.936\n",
      "Training Epoch 46  76.6% | batch:        49 of        64\t|\tloss: 1762.02\n",
      "Training Epoch 46  78.1% | batch:        50 of        64\t|\tloss: 1392.94\n",
      "Training Epoch 46  79.7% | batch:        51 of        64\t|\tloss: 1914.03\n",
      "Training Epoch 46  81.2% | batch:        52 of        64\t|\tloss: 1702.95\n",
      "Training Epoch 46  82.8% | batch:        53 of        64\t|\tloss: 2400.52\n",
      "Training Epoch 46  84.4% | batch:        54 of        64\t|\tloss: 1604.2\n",
      "Training Epoch 46  85.9% | batch:        55 of        64\t|\tloss: 1370.74\n",
      "Training Epoch 46  87.5% | batch:        56 of        64\t|\tloss: 2655.88\n",
      "Training Epoch 46  89.1% | batch:        57 of        64\t|\tloss: 1443.18\n",
      "Training Epoch 46  90.6% | batch:        58 of        64\t|\tloss: 1157.88\n",
      "Training Epoch 46  92.2% | batch:        59 of        64\t|\tloss: 1082.19\n",
      "Training Epoch 46  93.8% | batch:        60 of        64\t|\tloss: 1511.52\n",
      "Training Epoch 46  95.3% | batch:        61 of        64\t|\tloss: 4879.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:54,436 | INFO : Epoch 46 Training Summary: epoch: 46.000000 | loss: 1608.080596 | \n",
      "2023-05-10 17:08:54,437 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1885418891906738 seconds\n",
      "\n",
      "2023-05-10 17:08:54,438 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2055199975552766 seconds\n",
      "2023-05-10 17:08:54,438 | INFO : Avg batch train. time: 0.018836249961801197 seconds\n",
      "2023-05-10 17:08:54,439 | INFO : Avg sample train. time: 0.0002985438329755514 seconds\n",
      "2023-05-10 17:08:54,439 | INFO : Evaluating on validation set ...\n",
      "2023-05-10 17:08:54,586 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14632654190063477 seconds\n",
      "\n",
      "2023-05-10 17:08:54,586 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.1802211856842041 seconds\n",
      "2023-05-10 17:08:54,586 | INFO : Avg batch val. time: 0.011263824105262756 seconds\n",
      "2023-05-10 17:08:54,587 | INFO : Avg sample val. time: 0.00017843681750911295 seconds\n",
      "2023-05-10 17:08:54,587 | INFO : Epoch 46 Validation Summary: epoch: 46.000000 | loss: 2101.353972 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 46  96.9% | batch:        62 of        64\t|\tloss: 1075.82\n",
      "Training Epoch 46  98.4% | batch:        63 of        64\t|\tloss: 655.771\n",
      "\n",
      "Evaluating Epoch 46   0.0% | batch:         0 of        16\t|\tloss: 1670.71\n",
      "Evaluating Epoch 46   6.2% | batch:         1 of        16\t|\tloss: 1520.3\n",
      "Evaluating Epoch 46  12.5% | batch:         2 of        16\t|\tloss: 1022.52\n",
      "Evaluating Epoch 46  18.8% | batch:         3 of        16\t|\tloss: 2090.95\n",
      "Evaluating Epoch 46  25.0% | batch:         4 of        16\t|\tloss: 1411.26\n",
      "Evaluating Epoch 46  31.2% | batch:         5 of        16\t|\tloss: 2318.51\n",
      "Evaluating Epoch 46  37.5% | batch:         6 of        16\t|\tloss: 2281.82\n",
      "Evaluating Epoch 46  43.8% | batch:         7 of        16\t|\tloss: 2069.1\n",
      "Evaluating Epoch 46  50.0% | batch:         8 of        16\t|\tloss: 1665.41\n",
      "Evaluating Epoch 46  56.2% | batch:         9 of        16\t|\tloss: 5156.91\n",
      "Evaluating Epoch 46  62.5% | batch:        10 of        16\t|\tloss: 1998.15\n",
      "Evaluating Epoch 46  68.8% | batch:        11 of        16\t|\tloss: 3378.36\n",
      "Evaluating Epoch 46  75.0% | batch:        12 of        16\t|\tloss: 1499.58\n",
      "Evaluating Epoch 46  81.2% | batch:        13 of        16\t|\tloss: 1174.78\n",
      "Evaluating Epoch 46  87.5% | batch:        14 of        16\t|\tloss: 1759.12\n",
      "Evaluating Epoch 46  93.8% | batch:        15 of        16\t|\tloss: 2744.98\n",
      "\n",
      "Training Epoch 47   0.0% | batch:         0 of        64\t|\tloss: 1545.66\n",
      "Training Epoch 47   1.6% | batch:         1 of        64\t|\tloss: 1805.42\n",
      "Training Epoch 47   3.1% | batch:         2 of        64\t|\tloss: 1138.26\n",
      "Training Epoch 47   4.7% | batch:         3 of        64\t|\tloss: 2343.8\n",
      "Training Epoch 47   6.2% | batch:         4 of        64\t|\tloss: 1422.55\n",
      "Training Epoch 47   7.8% | batch:         5 of        64\t|\tloss: 1504.24\n",
      "Training Epoch 47   9.4% | batch:         6 of        64\t|\tloss: 2416.2\n",
      "Training Epoch 47  10.9% | batch:         7 of        64\t|\tloss: 2870.86\n",
      "Training Epoch 47  12.5% | batch:         8 of        64\t|\tloss: 1809.81\n",
      "Training Epoch 47  14.1% | batch:         9 of        64\t|\tloss: 1029.26\n",
      "Training Epoch 47  15.6% | batch:        10 of        64\t|\tloss: 968.105\n",
      "Training Epoch 47  17.2% | batch:        11 of        64\t|\tloss: 956.437\n",
      "Training Epoch 47  18.8% | batch:        12 of        64\t|\tloss: 1203.55\n",
      "Training Epoch 47  20.3% | batch:        13 of        64\t|\tloss: 939.864\n",
      "Training Epoch 47  21.9% | batch:        14 of        64\t|\tloss: 1520.13\n",
      "Training Epoch 47  23.4% | batch:        15 of        64\t|\tloss: 1770.72\n",
      "Training Epoch 47  25.0% | batch:        16 of        64\t|\tloss: 2177.44\n",
      "Training Epoch 47  26.6% | batch:        17 of        64\t|\tloss: 1246.5\n",
      "Training Epoch 47  28.1% | batch:        18 of        64\t|\tloss: 1563\n",
      "Training Epoch 47  29.7% | batch:        19 of        64\t|\tloss: 1474.6\n",
      "Training Epoch 47  31.2% | batch:        20 of        64\t|\tloss: 1669.11\n",
      "Training Epoch 47  32.8% | batch:        21 of        64\t|\tloss: 1294.91\n",
      "Training Epoch 47  34.4% | batch:        22 of        64\t|\tloss: 2548.96\n",
      "Training Epoch 47  35.9% | batch:        23 of        64\t|\tloss: 2628.07\n",
      "Training Epoch 47  37.5% | batch:        24 of        64\t|\tloss: 2413.64\n",
      "Training Epoch 47  39.1% | batch:        25 of        64\t|\tloss: 972.445\n",
      "Training Epoch 47  40.6% | batch:        26 of        64\t|\tloss: 1917.85\n",
      "Training Epoch 47  42.2% | batch:        27 of        64\t|\tloss: 729.909\n",
      "Training Epoch 47  43.8% | batch:        28 of        64\t|\tloss: 1020.56\n",
      "Training Epoch 47  45.3% | batch:        29 of        64\t|\tloss: 1047.05\n",
      "Training Epoch 47  46.9% | batch:        30 of        64\t|\tloss: 800.254\n",
      "Training Epoch 47  48.4% | batch:        31 of        64\t|\tloss: 6408.19\n",
      "Training Epoch 47  50.0% | batch:        32 of        64\t|\tloss: 1448.93\n",
      "Training Epoch 47  51.6% | batch:        33 of        64\t|\tloss: 1279.75\n",
      "Training Epoch 47  53.1% | batch:        34 of        64\t|\tloss: 2703.4\n",
      "Training Epoch 47  54.7% | batch:        35 of        64\t|\tloss: 2011.58\n",
      "Training Epoch 47  56.2% | batch:        36 of        64\t|\tloss: 905.21\n",
      "Training Epoch 47  57.8% | batch:        37 of        64\t|\tloss: 1702.63\n",
      "Training Epoch 47  59.4% | batch:        38 of        64\t|\tloss: 1282.04\n",
      "Training Epoch 47  60.9% | batch:        39 of        64\t|\tloss: 2282.06\n",
      "Training Epoch 47  62.5% | batch:        40 of        64\t|\tloss: 1399.91\n",
      "Training Epoch 47  64.1% | batch:        41 of        64\t|\tloss: 3250.85\n",
      "Training Epoch 47  65.6% | batch:        42 of        64\t|\tloss: 1492.06\n",
      "Training Epoch 47  67.2% | batch:        43 of        64\t|\tloss: 1720.71\n",
      "Training Epoch 47  68.8% | batch:        44 of        64\t|\tloss: 1254.75\n",
      "Training Epoch 47  70.3% | batch:        45 of        64\t|\tloss: 2057.43\n",
      "Training Epoch 47  71.9% | batch:        46 of        64\t|\tloss: 1174.35\n",
      "Training Epoch 47  73.4% | batch:        47 of        64\t|\tloss: 1227.13\n",
      "Training Epoch 47  75.0% | batch:        48 of        64\t|\tloss: 1658.41\n",
      "Training Epoch 47  76.6% | batch:        49 of        64\t|\tloss: 994.276\n",
      "Training Epoch 47  78.1% | batch:        50 of        64\t|\tloss: 1689.65\n",
      "Training Epoch 47  79.7% | batch:        51 of        64\t|\tloss: 1063.88\n",
      "Training Epoch 47  81.2% | batch:        52 of        64\t|\tloss: 1111.28\n",
      "Training Epoch 47  82.8% | batch:        53 of        64\t|\tloss: 1257.46\n",
      "Training Epoch 47  84.4% | batch:        54 of        64\t|\tloss: 960.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:55,808 | INFO : Epoch 47 Training Summary: epoch: 47.000000 | loss: 1696.950096 | \n",
      "2023-05-10 17:08:55,809 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2113721370697021 seconds\n",
      "\n",
      "2023-05-10 17:08:55,809 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2056445111619665 seconds\n",
      "2023-05-10 17:08:55,810 | INFO : Avg batch train. time: 0.018838195486905726 seconds\n",
      "2023-05-10 17:08:55,811 | INFO : Avg sample train. time: 0.0002985746684403087 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 47  85.9% | batch:        55 of        64\t|\tloss: 3990.22\n",
      "Training Epoch 47  87.5% | batch:        56 of        64\t|\tloss: 1350.99\n",
      "Training Epoch 47  89.1% | batch:        57 of        64\t|\tloss: 2683.37\n",
      "Training Epoch 47  90.6% | batch:        58 of        64\t|\tloss: 2574.28\n",
      "Training Epoch 47  92.2% | batch:        59 of        64\t|\tloss: 1590\n",
      "Training Epoch 47  93.8% | batch:        60 of        64\t|\tloss: 976.199\n",
      "Training Epoch 47  95.3% | batch:        61 of        64\t|\tloss: 1536.27\n",
      "Training Epoch 47  96.9% | batch:        62 of        64\t|\tloss: 1125.79\n",
      "Training Epoch 47  98.4% | batch:        63 of        64\t|\tloss: 1643.23\n",
      "\n",
      "Training Epoch 48   0.0% | batch:         0 of        64\t|\tloss: 1174.55\n",
      "Training Epoch 48   1.6% | batch:         1 of        64\t|\tloss: 1042.38\n",
      "Training Epoch 48   3.1% | batch:         2 of        64\t|\tloss: 1348.59\n",
      "Training Epoch 48   4.7% | batch:         3 of        64\t|\tloss: 1831.24\n",
      "Training Epoch 48   6.2% | batch:         4 of        64\t|\tloss: 922.301\n",
      "Training Epoch 48   7.8% | batch:         5 of        64\t|\tloss: 1318.33\n",
      "Training Epoch 48   9.4% | batch:         6 of        64\t|\tloss: 1123.75\n",
      "Training Epoch 48  10.9% | batch:         7 of        64\t|\tloss: 3471.07\n",
      "Training Epoch 48  12.5% | batch:         8 of        64\t|\tloss: 982.911\n",
      "Training Epoch 48  14.1% | batch:         9 of        64\t|\tloss: 3075.18\n",
      "Training Epoch 48  15.6% | batch:        10 of        64\t|\tloss: 2520.13\n",
      "Training Epoch 48  17.2% | batch:        11 of        64\t|\tloss: 1632.49\n",
      "Training Epoch 48  18.8% | batch:        12 of        64\t|\tloss: 597.394\n",
      "Training Epoch 48  20.3% | batch:        13 of        64\t|\tloss: 1256.28\n",
      "Training Epoch 48  21.9% | batch:        14 of        64\t|\tloss: 960.931\n",
      "Training Epoch 48  23.4% | batch:        15 of        64\t|\tloss: 4917.9\n",
      "Training Epoch 48  25.0% | batch:        16 of        64\t|\tloss: 2291.08\n",
      "Training Epoch 48  26.6% | batch:        17 of        64\t|\tloss: 1495.18\n",
      "Training Epoch 48  28.1% | batch:        18 of        64\t|\tloss: 1191.4\n",
      "Training Epoch 48  29.7% | batch:        19 of        64\t|\tloss: 1323.53\n",
      "Training Epoch 48  31.2% | batch:        20 of        64\t|\tloss: 1290.5\n",
      "Training Epoch 48  32.8% | batch:        21 of        64\t|\tloss: 1364.72\n",
      "Training Epoch 48  34.4% | batch:        22 of        64\t|\tloss: 962.911\n",
      "Training Epoch 48  35.9% | batch:        23 of        64\t|\tloss: 951.092\n",
      "Training Epoch 48  37.5% | batch:        24 of        64\t|\tloss: 1534.91\n",
      "Training Epoch 48  39.1% | batch:        25 of        64\t|\tloss: 1220.97\n",
      "Training Epoch 48  40.6% | batch:        26 of        64\t|\tloss: 1364.08\n",
      "Training Epoch 48  42.2% | batch:        27 of        64\t|\tloss: 758.099\n",
      "Training Epoch 48  43.8% | batch:        28 of        64\t|\tloss: 2264.63\n",
      "Training Epoch 48  45.3% | batch:        29 of        64\t|\tloss: 2726.88\n",
      "Training Epoch 48  46.9% | batch:        30 of        64\t|\tloss: 1704.51\n",
      "Training Epoch 48  48.4% | batch:        31 of        64\t|\tloss: 1255.76\n",
      "Training Epoch 48  50.0% | batch:        32 of        64\t|\tloss: 1703.35\n",
      "Training Epoch 48  51.6% | batch:        33 of        64\t|\tloss: 3162.72\n",
      "Training Epoch 48  53.1% | batch:        34 of        64\t|\tloss: 1150.41\n",
      "Training Epoch 48  54.7% | batch:        35 of        64\t|\tloss: 1105.7\n",
      "Training Epoch 48  56.2% | batch:        36 of        64\t|\tloss: 1313.79\n",
      "Training Epoch 48  57.8% | batch:        37 of        64\t|\tloss: 2677.85\n",
      "Training Epoch 48  59.4% | batch:        38 of        64\t|\tloss: 1498.94\n",
      "Training Epoch 48  60.9% | batch:        39 of        64\t|\tloss: 1762.18\n",
      "Training Epoch 48  62.5% | batch:        40 of        64\t|\tloss: 1022.53\n",
      "Training Epoch 48  64.1% | batch:        41 of        64\t|\tloss: 1147.2\n",
      "Training Epoch 48  65.6% | batch:        42 of        64\t|\tloss: 1363.01\n",
      "Training Epoch 48  67.2% | batch:        43 of        64\t|\tloss: 1061.63\n",
      "Training Epoch 48  68.8% | batch:        44 of        64\t|\tloss: 1731.91\n",
      "Training Epoch 48  70.3% | batch:        45 of        64\t|\tloss: 1123.92\n",
      "Training Epoch 48  71.9% | batch:        46 of        64\t|\tloss: 924.064\n",
      "Training Epoch 48  73.4% | batch:        47 of        64\t|\tloss: 1146.36\n",
      "Training Epoch 48  75.0% | batch:        48 of        64\t|\tloss: 2139.58\n",
      "Training Epoch 48  76.6% | batch:        49 of        64\t|\tloss: 3096.36\n",
      "Training Epoch 48  78.1% | batch:        50 of        64\t|\tloss: 1206.31\n",
      "Training Epoch 48  79.7% | batch:        51 of        64\t|\tloss: 1084.85\n",
      "Training Epoch 48  81.2% | batch:        52 of        64\t|\tloss: 971.234\n",
      "Training Epoch 48  82.8% | batch:        53 of        64\t|\tloss: 853.192\n",
      "Training Epoch 48  84.4% | batch:        54 of        64\t|\tloss: 940.929\n",
      "Training Epoch 48  85.9% | batch:        55 of        64\t|\tloss: 767.891\n",
      "Training Epoch 48  87.5% | batch:        56 of        64\t|\tloss: 1821.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:57,009 | INFO : Epoch 48 Training Summary: epoch: 48.000000 | loss: 1546.342704 | \n",
      "2023-05-10 17:08:57,010 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1891911029815674 seconds\n",
      "\n",
      "2023-05-10 17:08:57,010 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2053017318248749 seconds\n",
      "2023-05-10 17:08:57,011 | INFO : Avg batch train. time: 0.01883283955976367 seconds\n",
      "2023-05-10 17:08:57,012 | INFO : Avg sample train. time: 0.00029848978004578375 seconds\n",
      "2023-05-10 17:08:57,012 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 48  89.1% | batch:        57 of        64\t|\tloss: 1905.06\n",
      "Training Epoch 48  90.6% | batch:        58 of        64\t|\tloss: 1400.25\n",
      "Training Epoch 48  92.2% | batch:        59 of        64\t|\tloss: 1575.55\n",
      "Training Epoch 48  93.8% | batch:        60 of        64\t|\tloss: 1526.9\n",
      "Training Epoch 48  95.3% | batch:        61 of        64\t|\tloss: 1275.65\n",
      "Training Epoch 48  96.9% | batch:        62 of        64\t|\tloss: 1174.7\n",
      "Training Epoch 48  98.4% | batch:        63 of        64\t|\tloss: 568.153\n",
      "\n",
      "Evaluating Epoch 48   0.0% | batch:         0 of        16\t|\tloss: 1532.07\n",
      "Evaluating Epoch 48   6.2% | batch:         1 of        16\t|\tloss: 958.086\n",
      "Evaluating Epoch 48  12.5% | batch:         2 of        16\t|\tloss: 763.169\n",
      "Evaluating Epoch 48  18.8% | batch:         3 of        16\t|\tloss: 1537.21\n",
      "Evaluating Epoch 48  25.0% | batch:         4 of        16\t|\tloss: 1840.84\n",
      "Evaluating Epoch 48  31.2% | batch:         5 of        16\t|\tloss: 1694.8\n",
      "Evaluating Epoch 48  37.5% | batch:         6 of        16\t|\tloss: 1988\n",
      "Evaluating Epoch 48  43.8% | batch:         7 of        16\t|\tloss: 1626.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:57,173 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.16046953201293945 seconds\n",
      "\n",
      "2023-05-10 17:08:57,174 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.17946150669684777 seconds\n",
      "2023-05-10 17:08:57,174 | INFO : Avg batch val. time: 0.011216344168552985 seconds\n",
      "2023-05-10 17:08:57,175 | INFO : Avg sample val. time: 0.00017768466009588887 seconds\n",
      "2023-05-10 17:08:57,175 | INFO : Epoch 48 Validation Summary: epoch: 48.000000 | loss: 1705.196136 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 48  50.0% | batch:         8 of        16\t|\tloss: 1343.2\n",
      "Evaluating Epoch 48  56.2% | batch:         9 of        16\t|\tloss: 4444.76\n",
      "Evaluating Epoch 48  62.5% | batch:        10 of        16\t|\tloss: 1388.5\n",
      "Evaluating Epoch 48  68.8% | batch:        11 of        16\t|\tloss: 2459.34\n",
      "Evaluating Epoch 48  75.0% | batch:        12 of        16\t|\tloss: 1172.26\n",
      "Evaluating Epoch 48  81.2% | batch:        13 of        16\t|\tloss: 922.105\n",
      "Evaluating Epoch 48  87.5% | batch:        14 of        16\t|\tloss: 1444.27\n",
      "Evaluating Epoch 48  93.8% | batch:        15 of        16\t|\tloss: 2297.42\n",
      "\n",
      "Training Epoch 49   0.0% | batch:         0 of        64\t|\tloss: 1331.75\n",
      "Training Epoch 49   1.6% | batch:         1 of        64\t|\tloss: 1709.22\n",
      "Training Epoch 49   3.1% | batch:         2 of        64\t|\tloss: 1490.3\n",
      "Training Epoch 49   4.7% | batch:         3 of        64\t|\tloss: 925.841\n",
      "Training Epoch 49   6.2% | batch:         4 of        64\t|\tloss: 1025.76\n",
      "Training Epoch 49   7.8% | batch:         5 of        64\t|\tloss: 2474.3\n",
      "Training Epoch 49   9.4% | batch:         6 of        64\t|\tloss: 877.29\n",
      "Training Epoch 49  10.9% | batch:         7 of        64\t|\tloss: 2269.36\n",
      "Training Epoch 49  12.5% | batch:         8 of        64\t|\tloss: 933.539\n",
      "Training Epoch 49  14.1% | batch:         9 of        64\t|\tloss: 1867.95\n",
      "Training Epoch 49  15.6% | batch:        10 of        64\t|\tloss: 1254.97\n",
      "Training Epoch 49  17.2% | batch:        11 of        64\t|\tloss: 1217.04\n",
      "Training Epoch 49  18.8% | batch:        12 of        64\t|\tloss: 957.567\n",
      "Training Epoch 49  20.3% | batch:        13 of        64\t|\tloss: 1175.78\n",
      "Training Epoch 49  21.9% | batch:        14 of        64\t|\tloss: 1338.79\n",
      "Training Epoch 49  23.4% | batch:        15 of        64\t|\tloss: 560.755\n",
      "Training Epoch 49  25.0% | batch:        16 of        64\t|\tloss: 1079.65\n",
      "Training Epoch 49  26.6% | batch:        17 of        64\t|\tloss: 1476.9\n",
      "Training Epoch 49  28.1% | batch:        18 of        64\t|\tloss: 1256.48\n",
      "Training Epoch 49  29.7% | batch:        19 of        64\t|\tloss: 3712.8\n",
      "Training Epoch 49  31.2% | batch:        20 of        64\t|\tloss: 1004.95\n",
      "Training Epoch 49  32.8% | batch:        21 of        64\t|\tloss: 1832.1\n",
      "Training Epoch 49  34.4% | batch:        22 of        64\t|\tloss: 1713.6\n",
      "Training Epoch 49  35.9% | batch:        23 of        64\t|\tloss: 1113.94\n",
      "Training Epoch 49  37.5% | batch:        24 of        64\t|\tloss: 1906.16\n",
      "Training Epoch 49  39.1% | batch:        25 of        64\t|\tloss: 904.997\n",
      "Training Epoch 49  40.6% | batch:        26 of        64\t|\tloss: 1095.31\n",
      "Training Epoch 49  42.2% | batch:        27 of        64\t|\tloss: 775.646\n",
      "Training Epoch 49  43.8% | batch:        28 of        64\t|\tloss: 3125.52\n",
      "Training Epoch 49  45.3% | batch:        29 of        64\t|\tloss: 1126.7\n",
      "Training Epoch 49  46.9% | batch:        30 of        64\t|\tloss: 1631.67\n",
      "Training Epoch 49  48.4% | batch:        31 of        64\t|\tloss: 5678.94\n",
      "Training Epoch 49  50.0% | batch:        32 of        64\t|\tloss: 1374.33\n",
      "Training Epoch 49  51.6% | batch:        33 of        64\t|\tloss: 723.008\n",
      "Training Epoch 49  53.1% | batch:        34 of        64\t|\tloss: 739.015\n",
      "Training Epoch 49  54.7% | batch:        35 of        64\t|\tloss: 1465.96\n",
      "Training Epoch 49  56.2% | batch:        36 of        64\t|\tloss: 1541.1\n",
      "Training Epoch 49  57.8% | batch:        37 of        64\t|\tloss: 2085.52\n",
      "Training Epoch 49  59.4% | batch:        38 of        64\t|\tloss: 1455.26\n",
      "Training Epoch 49  60.9% | batch:        39 of        64\t|\tloss: 1822.86\n",
      "Training Epoch 49  62.5% | batch:        40 of        64\t|\tloss: 941.844\n",
      "Training Epoch 49  64.1% | batch:        41 of        64\t|\tloss: 1477.21\n",
      "Training Epoch 49  65.6% | batch:        42 of        64\t|\tloss: 1334.53\n",
      "Training Epoch 49  67.2% | batch:        43 of        64\t|\tloss: 1027.65\n",
      "Training Epoch 49  68.8% | batch:        44 of        64\t|\tloss: 1720.71\n",
      "Training Epoch 49  70.3% | batch:        45 of        64\t|\tloss: 996.868\n",
      "Training Epoch 49  71.9% | batch:        46 of        64\t|\tloss: 1236.48\n",
      "Training Epoch 49  73.4% | batch:        47 of        64\t|\tloss: 1874.4\n",
      "Training Epoch 49  75.0% | batch:        48 of        64\t|\tloss: 1099.04\n",
      "Training Epoch 49  76.6% | batch:        49 of        64\t|\tloss: 1767.18\n",
      "Training Epoch 49  78.1% | batch:        50 of        64\t|\tloss: 874.345\n",
      "Training Epoch 49  79.7% | batch:        51 of        64\t|\tloss: 1203.19\n",
      "Training Epoch 49  81.2% | batch:        52 of        64\t|\tloss: 1088.9\n",
      "Training Epoch 49  82.8% | batch:        53 of        64\t|\tloss: 1427.92\n",
      "Training Epoch 49  84.4% | batch:        54 of        64\t|\tloss: 1658.61\n",
      "Training Epoch 49  85.9% | batch:        55 of        64\t|\tloss: 1001.19\n",
      "Training Epoch 49  87.5% | batch:        56 of        64\t|\tloss: 1367.92\n",
      "Training Epoch 49  89.1% | batch:        57 of        64\t|\tloss: 1207.61\n",
      "Training Epoch 49  90.6% | batch:        58 of        64\t|\tloss: 1319.1\n",
      "Training Epoch 49  92.2% | batch:        59 of        64\t|\tloss: 1922.86\n",
      "Training Epoch 49  93.8% | batch:        60 of        64\t|\tloss: 2793.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:58,387 | INFO : Epoch 49 Training Summary: epoch: 49.000000 | loss: 1531.467569 | \n",
      "2023-05-10 17:08:58,387 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1969594955444336 seconds\n",
      "\n",
      "2023-05-10 17:08:58,388 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.205131482104866 seconds\n",
      "2023-05-10 17:08:58,388 | INFO : Avg batch train. time: 0.01883017940788853 seconds\n",
      "2023-05-10 17:08:58,388 | INFO : Avg sample train. time: 0.0002984476181537558 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 49  95.3% | batch:        61 of        64\t|\tloss: 2262.97\n",
      "Training Epoch 49  96.9% | batch:        62 of        64\t|\tloss: 2610\n",
      "Training Epoch 49  98.4% | batch:        63 of        64\t|\tloss: 3837.59\n",
      "\n",
      "Training Epoch 50   0.0% | batch:         0 of        64\t|\tloss: 1215.62\n",
      "Training Epoch 50   1.6% | batch:         1 of        64\t|\tloss: 2063.73\n",
      "Training Epoch 50   3.1% | batch:         2 of        64\t|\tloss: 715.398\n",
      "Training Epoch 50   4.7% | batch:         3 of        64\t|\tloss: 980.491\n",
      "Training Epoch 50   6.2% | batch:         4 of        64\t|\tloss: 1883.06\n",
      "Training Epoch 50   7.8% | batch:         5 of        64\t|\tloss: 1107.1\n",
      "Training Epoch 50   9.4% | batch:         6 of        64\t|\tloss: 885.552\n",
      "Training Epoch 50  10.9% | batch:         7 of        64\t|\tloss: 3775.4\n",
      "Training Epoch 50  12.5% | batch:         8 of        64\t|\tloss: 1248.58\n",
      "Training Epoch 50  14.1% | batch:         9 of        64\t|\tloss: 2036.27\n",
      "Training Epoch 50  15.6% | batch:        10 of        64\t|\tloss: 931.275\n",
      "Training Epoch 50  17.2% | batch:        11 of        64\t|\tloss: 917.928\n",
      "Training Epoch 50  18.8% | batch:        12 of        64\t|\tloss: 1499.52\n",
      "Training Epoch 50  20.3% | batch:        13 of        64\t|\tloss: 1178.73\n",
      "Training Epoch 50  21.9% | batch:        14 of        64\t|\tloss: 1471.85\n",
      "Training Epoch 50  23.4% | batch:        15 of        64\t|\tloss: 1679.45\n",
      "Training Epoch 50  25.0% | batch:        16 of        64\t|\tloss: 1183.08\n",
      "Training Epoch 50  26.6% | batch:        17 of        64\t|\tloss: 1088.74\n",
      "Training Epoch 50  28.1% | batch:        18 of        64\t|\tloss: 1238.38\n",
      "Training Epoch 50  29.7% | batch:        19 of        64\t|\tloss: 2088.65\n",
      "Training Epoch 50  31.2% | batch:        20 of        64\t|\tloss: 1770.27\n",
      "Training Epoch 50  32.8% | batch:        21 of        64\t|\tloss: 1811.46\n",
      "Training Epoch 50  34.4% | batch:        22 of        64\t|\tloss: 1607.8\n",
      "Training Epoch 50  35.9% | batch:        23 of        64\t|\tloss: 845.065\n",
      "Training Epoch 50  37.5% | batch:        24 of        64\t|\tloss: 919.083\n",
      "Training Epoch 50  39.1% | batch:        25 of        64\t|\tloss: 1515.44\n",
      "Training Epoch 50  40.6% | batch:        26 of        64\t|\tloss: 960.158\n",
      "Training Epoch 50  42.2% | batch:        27 of        64\t|\tloss: 2688.69\n",
      "Training Epoch 50  43.8% | batch:        28 of        64\t|\tloss: 940.017\n",
      "Training Epoch 50  45.3% | batch:        29 of        64\t|\tloss: 1160.11\n",
      "Training Epoch 50  46.9% | batch:        30 of        64\t|\tloss: 1046.33\n",
      "Training Epoch 50  48.4% | batch:        31 of        64\t|\tloss: 1467.51\n",
      "Training Epoch 50  50.0% | batch:        32 of        64\t|\tloss: 1745.59\n",
      "Training Epoch 50  51.6% | batch:        33 of        64\t|\tloss: 1157.6\n",
      "Training Epoch 50  53.1% | batch:        34 of        64\t|\tloss: 990.214\n",
      "Training Epoch 50  54.7% | batch:        35 of        64\t|\tloss: 1395.48\n",
      "Training Epoch 50  56.2% | batch:        36 of        64\t|\tloss: 833.261\n",
      "Training Epoch 50  57.8% | batch:        37 of        64\t|\tloss: 1021.64\n",
      "Training Epoch 50  59.4% | batch:        38 of        64\t|\tloss: 1548.22\n",
      "Training Epoch 50  60.9% | batch:        39 of        64\t|\tloss: 2924.51\n",
      "Training Epoch 50  62.5% | batch:        40 of        64\t|\tloss: 1226.96\n",
      "Training Epoch 50  64.1% | batch:        41 of        64\t|\tloss: 1547.9\n",
      "Training Epoch 50  65.6% | batch:        42 of        64\t|\tloss: 1759.25\n",
      "Training Epoch 50  67.2% | batch:        43 of        64\t|\tloss: 2161.01\n",
      "Training Epoch 50  68.8% | batch:        44 of        64\t|\tloss: 2761.18\n",
      "Training Epoch 50  70.3% | batch:        45 of        64\t|\tloss: 878.459\n",
      "Training Epoch 50  71.9% | batch:        46 of        64\t|\tloss: 776.616\n",
      "Training Epoch 50  73.4% | batch:        47 of        64\t|\tloss: 1379.11\n",
      "Training Epoch 50  75.0% | batch:        48 of        64\t|\tloss: 5137.07\n",
      "Training Epoch 50  76.6% | batch:        49 of        64\t|\tloss: 1079.21\n",
      "Training Epoch 50  78.1% | batch:        50 of        64\t|\tloss: 1095.27\n",
      "Training Epoch 50  79.7% | batch:        51 of        64\t|\tloss: 1463.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:59,588 | INFO : Epoch 50 Training Summary: epoch: 50.000000 | loss: 1497.589984 | \n",
      "2023-05-10 17:08:59,589 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1904313564300537 seconds\n",
      "\n",
      "2023-05-10 17:08:59,589 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2048374795913697 seconds\n",
      "2023-05-10 17:08:59,590 | INFO : Avg batch train. time: 0.01882558561861515 seconds\n",
      "2023-05-10 17:08:59,590 | INFO : Avg sample train. time: 0.00029837480921034415 seconds\n",
      "2023-05-10 17:08:59,590 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 50  81.2% | batch:        52 of        64\t|\tloss: 1669.41\n",
      "Training Epoch 50  82.8% | batch:        53 of        64\t|\tloss: 1203.23\n",
      "Training Epoch 50  84.4% | batch:        54 of        64\t|\tloss: 1817.63\n",
      "Training Epoch 50  85.9% | batch:        55 of        64\t|\tloss: 3276.45\n",
      "Training Epoch 50  87.5% | batch:        56 of        64\t|\tloss: 1066.14\n",
      "Training Epoch 50  89.1% | batch:        57 of        64\t|\tloss: 647.777\n",
      "Training Epoch 50  90.6% | batch:        58 of        64\t|\tloss: 918.919\n",
      "Training Epoch 50  92.2% | batch:        59 of        64\t|\tloss: 1123.16\n",
      "Training Epoch 50  93.8% | batch:        60 of        64\t|\tloss: 1608.51\n",
      "Training Epoch 50  95.3% | batch:        61 of        64\t|\tloss: 625.605\n",
      "Training Epoch 50  96.9% | batch:        62 of        64\t|\tloss: 1357.65\n",
      "Training Epoch 50  98.4% | batch:        63 of        64\t|\tloss: 3964.77\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:08:59,737 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14616775512695312 seconds\n",
      "\n",
      "2023-05-10 17:08:59,737 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.17822840478685167 seconds\n",
      "2023-05-10 17:08:59,738 | INFO : Avg batch val. time: 0.01113927529917823 seconds\n",
      "2023-05-10 17:08:59,738 | INFO : Avg sample val. time: 0.00017646376711569472 seconds\n",
      "2023-05-10 17:08:59,739 | INFO : Epoch 50 Validation Summary: epoch: 50.000000 | loss: 2349.149938 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 50   0.0% | batch:         0 of        16\t|\tloss: 2044.03\n",
      "Evaluating Epoch 50   6.2% | batch:         1 of        16\t|\tloss: 1497.09\n",
      "Evaluating Epoch 50  12.5% | batch:         2 of        16\t|\tloss: 1094.53\n",
      "Evaluating Epoch 50  18.8% | batch:         3 of        16\t|\tloss: 1854.28\n",
      "Evaluating Epoch 50  25.0% | batch:         4 of        16\t|\tloss: 2061.7\n",
      "Evaluating Epoch 50  31.2% | batch:         5 of        16\t|\tloss: 2146.48\n",
      "Evaluating Epoch 50  37.5% | batch:         6 of        16\t|\tloss: 2267.58\n",
      "Evaluating Epoch 50  43.8% | batch:         7 of        16\t|\tloss: 3343.56\n",
      "Evaluating Epoch 50  50.0% | batch:         8 of        16\t|\tloss: 1598.38\n",
      "Evaluating Epoch 50  56.2% | batch:         9 of        16\t|\tloss: 7068.36\n",
      "Evaluating Epoch 50  62.5% | batch:        10 of        16\t|\tloss: 1633.75\n",
      "Evaluating Epoch 50  68.8% | batch:        11 of        16\t|\tloss: 3823.26\n",
      "Evaluating Epoch 50  75.0% | batch:        12 of        16\t|\tloss: 1606.21\n",
      "Evaluating Epoch 50  81.2% | batch:        13 of        16\t|\tloss: 1067.86\n",
      "Evaluating Epoch 50  87.5% | batch:        14 of        16\t|\tloss: 1704.33\n",
      "Evaluating Epoch 50  93.8% | batch:        15 of        16\t|\tloss: 2894.26\n",
      "\n",
      "Training Epoch 51   0.0% | batch:         0 of        64\t|\tloss: 1118.08\n",
      "Training Epoch 51   1.6% | batch:         1 of        64\t|\tloss: 709.337\n",
      "Training Epoch 51   3.1% | batch:         2 of        64\t|\tloss: 1193.1\n",
      "Training Epoch 51   4.7% | batch:         3 of        64\t|\tloss: 1266.11\n",
      "Training Epoch 51   6.2% | batch:         4 of        64\t|\tloss: 3028.86\n",
      "Training Epoch 51   7.8% | batch:         5 of        64\t|\tloss: 2151.4\n",
      "Training Epoch 51   9.4% | batch:         6 of        64\t|\tloss: 1651.94\n",
      "Training Epoch 51  10.9% | batch:         7 of        64\t|\tloss: 1122.64\n",
      "Training Epoch 51  12.5% | batch:         8 of        64\t|\tloss: 1604.31\n",
      "Training Epoch 51  14.1% | batch:         9 of        64\t|\tloss: 1541.64\n",
      "Training Epoch 51  15.6% | batch:        10 of        64\t|\tloss: 1220.4\n",
      "Training Epoch 51  17.2% | batch:        11 of        64\t|\tloss: 1039.06\n",
      "Training Epoch 51  18.8% | batch:        12 of        64\t|\tloss: 1807.04\n",
      "Training Epoch 51  20.3% | batch:        13 of        64\t|\tloss: 1370.26\n",
      "Training Epoch 51  21.9% | batch:        14 of        64\t|\tloss: 779.748\n",
      "Training Epoch 51  23.4% | batch:        15 of        64\t|\tloss: 2001.94\n",
      "Training Epoch 51  25.0% | batch:        16 of        64\t|\tloss: 1157.94\n",
      "Training Epoch 51  26.6% | batch:        17 of        64\t|\tloss: 1799.25\n",
      "Training Epoch 51  28.1% | batch:        18 of        64\t|\tloss: 2075.85\n",
      "Training Epoch 51  29.7% | batch:        19 of        64\t|\tloss: 1125.79\n",
      "Training Epoch 51  31.2% | batch:        20 of        64\t|\tloss: 1353.54\n",
      "Training Epoch 51  32.8% | batch:        21 of        64\t|\tloss: 1279.08\n",
      "Training Epoch 51  34.4% | batch:        22 of        64\t|\tloss: 1713.72\n",
      "Training Epoch 51  35.9% | batch:        23 of        64\t|\tloss: 1170.88\n",
      "Training Epoch 51  37.5% | batch:        24 of        64\t|\tloss: 1125.89\n",
      "Training Epoch 51  39.1% | batch:        25 of        64\t|\tloss: 2549.51\n",
      "Training Epoch 51  40.6% | batch:        26 of        64\t|\tloss: 792.086\n",
      "Training Epoch 51  42.2% | batch:        27 of        64\t|\tloss: 1486.07\n",
      "Training Epoch 51  43.8% | batch:        28 of        64\t|\tloss: 1030.33\n",
      "Training Epoch 51  45.3% | batch:        29 of        64\t|\tloss: 1300.89\n",
      "Training Epoch 51  46.9% | batch:        30 of        64\t|\tloss: 2227.09\n",
      "Training Epoch 51  48.4% | batch:        31 of        64\t|\tloss: 703.811\n",
      "Training Epoch 51  50.0% | batch:        32 of        64\t|\tloss: 1167.17\n",
      "Training Epoch 51  51.6% | batch:        33 of        64\t|\tloss: 1506.13\n",
      "Training Epoch 51  53.1% | batch:        34 of        64\t|\tloss: 955.765\n",
      "Training Epoch 51  54.7% | batch:        35 of        64\t|\tloss: 1101.05\n",
      "Training Epoch 51  56.2% | batch:        36 of        64\t|\tloss: 1229.01\n",
      "Training Epoch 51  57.8% | batch:        37 of        64\t|\tloss: 3708.84\n",
      "Training Epoch 51  59.4% | batch:        38 of        64\t|\tloss: 1209.86\n",
      "Training Epoch 51  60.9% | batch:        39 of        64\t|\tloss: 1925.23\n",
      "Training Epoch 51  62.5% | batch:        40 of        64\t|\tloss: 2680.59\n",
      "Training Epoch 51  64.1% | batch:        41 of        64\t|\tloss: 1578.03\n",
      "Training Epoch 51  65.6% | batch:        42 of        64\t|\tloss: 880.697\n",
      "Training Epoch 51  67.2% | batch:        43 of        64\t|\tloss: 1833.83\n",
      "Training Epoch 51  68.8% | batch:        44 of        64\t|\tloss: 1665.69\n",
      "Training Epoch 51  70.3% | batch:        45 of        64\t|\tloss: 1277.48\n",
      "Training Epoch 51  71.9% | batch:        46 of        64\t|\tloss: 1567.68\n",
      "Training Epoch 51  73.4% | batch:        47 of        64\t|\tloss: 1186.39\n",
      "Training Epoch 51  75.0% | batch:        48 of        64\t|\tloss: 970.553\n",
      "Training Epoch 51  76.6% | batch:        49 of        64\t|\tloss: 1448.22\n",
      "Training Epoch 51  78.1% | batch:        50 of        64\t|\tloss: 3017.42\n",
      "Training Epoch 51  79.7% | batch:        51 of        64\t|\tloss: 1849.22\n",
      "Training Epoch 51  81.2% | batch:        52 of        64\t|\tloss: 1290.56\n",
      "Training Epoch 51  82.8% | batch:        53 of        64\t|\tloss: 1552.03\n",
      "Training Epoch 51  84.4% | batch:        54 of        64\t|\tloss: 1875.25\n",
      "Training Epoch 51  85.9% | batch:        55 of        64\t|\tloss: 1663.86\n",
      "Training Epoch 51  87.5% | batch:        56 of        64\t|\tloss: 2660.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:00,942 | INFO : Epoch 51 Training Summary: epoch: 51.000000 | loss: 1594.964835 | \n",
      "2023-05-10 17:09:00,942 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1937968730926514 seconds\n",
      "\n",
      "2023-05-10 17:09:00,943 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2046209971110027 seconds\n",
      "2023-05-10 17:09:00,943 | INFO : Avg batch train. time: 0.018822203079859417 seconds\n",
      "2023-05-10 17:09:00,944 | INFO : Avg sample train. time: 0.0002983211978977223 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 51  89.1% | batch:        57 of        64\t|\tloss: 1844.64\n",
      "Training Epoch 51  90.6% | batch:        58 of        64\t|\tloss: 1421.56\n",
      "Training Epoch 51  92.2% | batch:        59 of        64\t|\tloss: 1094.44\n",
      "Training Epoch 51  93.8% | batch:        60 of        64\t|\tloss: 919.875\n",
      "Training Epoch 51  95.3% | batch:        61 of        64\t|\tloss: 837.429\n",
      "Training Epoch 51  96.9% | batch:        62 of        64\t|\tloss: 5210.64\n",
      "Training Epoch 51  98.4% | batch:        63 of        64\t|\tloss: 10718.2\n",
      "\n",
      "Training Epoch 52   0.0% | batch:         0 of        64\t|\tloss: 2048.53\n",
      "Training Epoch 52   1.6% | batch:         1 of        64\t|\tloss: 1565.27\n",
      "Training Epoch 52   3.1% | batch:         2 of        64\t|\tloss: 1453.25\n",
      "Training Epoch 52   4.7% | batch:         3 of        64\t|\tloss: 1263.07\n",
      "Training Epoch 52   6.2% | batch:         4 of        64\t|\tloss: 2616.06\n",
      "Training Epoch 52   7.8% | batch:         5 of        64\t|\tloss: 1335.99\n",
      "Training Epoch 52   9.4% | batch:         6 of        64\t|\tloss: 3174.66\n",
      "Training Epoch 52  10.9% | batch:         7 of        64\t|\tloss: 1377.8\n",
      "Training Epoch 52  12.5% | batch:         8 of        64\t|\tloss: 1318.21\n",
      "Training Epoch 52  14.1% | batch:         9 of        64\t|\tloss: 1875.01\n",
      "Training Epoch 52  15.6% | batch:        10 of        64\t|\tloss: 855.952\n",
      "Training Epoch 52  17.2% | batch:        11 of        64\t|\tloss: 1233.97\n",
      "Training Epoch 52  18.8% | batch:        12 of        64\t|\tloss: 1239.73\n",
      "Training Epoch 52  20.3% | batch:        13 of        64\t|\tloss: 2875.74\n",
      "Training Epoch 52  21.9% | batch:        14 of        64\t|\tloss: 1131.67\n",
      "Training Epoch 52  23.4% | batch:        15 of        64\t|\tloss: 957.415\n",
      "Training Epoch 52  25.0% | batch:        16 of        64\t|\tloss: 931.821\n",
      "Training Epoch 52  26.6% | batch:        17 of        64\t|\tloss: 924.08\n",
      "Training Epoch 52  28.1% | batch:        18 of        64\t|\tloss: 858.865\n",
      "Training Epoch 52  29.7% | batch:        19 of        64\t|\tloss: 1255.45\n",
      "Training Epoch 52  31.2% | batch:        20 of        64\t|\tloss: 1326.3\n",
      "Training Epoch 52  32.8% | batch:        21 of        64\t|\tloss: 999.244\n",
      "Training Epoch 52  34.4% | batch:        22 of        64\t|\tloss: 2093.56\n",
      "Training Epoch 52  35.9% | batch:        23 of        64\t|\tloss: 1231.49\n",
      "Training Epoch 52  37.5% | batch:        24 of        64\t|\tloss: 950.552\n",
      "Training Epoch 52  39.1% | batch:        25 of        64\t|\tloss: 626.179\n",
      "Training Epoch 52  40.6% | batch:        26 of        64\t|\tloss: 2096.84\n",
      "Training Epoch 52  42.2% | batch:        27 of        64\t|\tloss: 1948.59\n",
      "Training Epoch 52  43.8% | batch:        28 of        64\t|\tloss: 1341.64\n",
      "Training Epoch 52  45.3% | batch:        29 of        64\t|\tloss: 1428.34\n",
      "Training Epoch 52  46.9% | batch:        30 of        64\t|\tloss: 2699.06\n",
      "Training Epoch 52  48.4% | batch:        31 of        64\t|\tloss: 1355.69\n",
      "Training Epoch 52  50.0% | batch:        32 of        64\t|\tloss: 2225.84\n",
      "Training Epoch 52  51.6% | batch:        33 of        64\t|\tloss: 1197.96\n",
      "Training Epoch 52  53.1% | batch:        34 of        64\t|\tloss: 803.34\n",
      "Training Epoch 52  54.7% | batch:        35 of        64\t|\tloss: 1039.5\n",
      "Training Epoch 52  56.2% | batch:        36 of        64\t|\tloss: 3006.05\n",
      "Training Epoch 52  57.8% | batch:        37 of        64\t|\tloss: 1396.67\n",
      "Training Epoch 52  59.4% | batch:        38 of        64\t|\tloss: 853.321\n",
      "Training Epoch 52  60.9% | batch:        39 of        64\t|\tloss: 1059.6\n",
      "Training Epoch 52  62.5% | batch:        40 of        64\t|\tloss: 1640.7\n",
      "Training Epoch 52  64.1% | batch:        41 of        64\t|\tloss: 1637.06\n",
      "Training Epoch 52  65.6% | batch:        42 of        64\t|\tloss: 819.374\n",
      "Training Epoch 52  67.2% | batch:        43 of        64\t|\tloss: 1141.38\n",
      "Training Epoch 52  68.8% | batch:        44 of        64\t|\tloss: 1040.38\n",
      "Training Epoch 52  70.3% | batch:        45 of        64\t|\tloss: 889.076\n",
      "Training Epoch 52  71.9% | batch:        46 of        64\t|\tloss: 1324.02\n",
      "Training Epoch 52  73.4% | batch:        47 of        64\t|\tloss: 2175.99\n",
      "Training Epoch 52  75.0% | batch:        48 of        64\t|\tloss: 2404.13\n",
      "Training Epoch 52  76.6% | batch:        49 of        64\t|\tloss: 1019\n",
      "Training Epoch 52  78.1% | batch:        50 of        64\t|\tloss: 1435.88\n",
      "Training Epoch 52  79.7% | batch:        51 of        64\t|\tloss: 1153.62\n",
      "Training Epoch 52  81.2% | batch:        52 of        64\t|\tloss: 1077.02\n",
      "Training Epoch 52  82.8% | batch:        53 of        64\t|\tloss: 874.186\n",
      "Training Epoch 52  84.4% | batch:        54 of        64\t|\tloss: 1309.09\n",
      "Training Epoch 52  85.9% | batch:        55 of        64\t|\tloss: 1870.97\n",
      "Training Epoch 52  87.5% | batch:        56 of        64\t|\tloss: 1804.88\n",
      "Training Epoch 52  89.1% | batch:        57 of        64\t|\tloss: 1236.22\n",
      "Training Epoch 52  90.6% | batch:        58 of        64\t|\tloss: 1579.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:02,142 | INFO : Epoch 52 Training Summary: epoch: 52.000000 | loss: 1474.827905 | \n",
      "2023-05-10 17:09:02,142 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.188840627670288 seconds\n",
      "\n",
      "2023-05-10 17:09:02,143 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.204317528467912 seconds\n",
      "2023-05-10 17:09:02,143 | INFO : Avg batch train. time: 0.018817461382311124 seconds\n",
      "2023-05-10 17:09:02,143 | INFO : Avg sample train. time: 0.0002982460446924002 seconds\n",
      "2023-05-10 17:09:02,144 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 52  92.2% | batch:        59 of        64\t|\tloss: 1144.94\n",
      "Training Epoch 52  93.8% | batch:        60 of        64\t|\tloss: 1244.77\n",
      "Training Epoch 52  95.3% | batch:        61 of        64\t|\tloss: 2100.99\n",
      "Training Epoch 52  96.9% | batch:        62 of        64\t|\tloss: 2077.67\n",
      "Training Epoch 52  98.4% | batch:        63 of        64\t|\tloss: 842.315\n",
      "\n",
      "Evaluating Epoch 52   0.0% | batch:         0 of        16\t|\tloss: 1348.13\n",
      "Evaluating Epoch 52   6.2% | batch:         1 of        16\t|\tloss: 1425\n",
      "Evaluating Epoch 52  12.5% | batch:         2 of        16\t|\tloss: 878.158\n",
      "Evaluating Epoch 52  18.8% | batch:         3 of        16\t|\tloss: 1567.76\n",
      "Evaluating Epoch 52  25.0% | batch:         4 of        16\t|\tloss: 1782.54\n",
      "Evaluating Epoch 52  31.2% | batch:         5 of        16\t|\tloss: 2373.92\n",
      "Evaluating Epoch 52  37.5% | batch:         6 of        16\t|\tloss: 2063.21\n",
      "Evaluating Epoch 52  43.8% | batch:         7 of        16\t|\tloss: 1547\n",
      "Evaluating Epoch 52  50.0% | batch:         8 of        16\t|\tloss: 1680.07\n",
      "Evaluating Epoch 52  56.2% | batch:         9 of        16\t|\tloss: 4520.31\n",
      "Evaluating Epoch 52  62.5% | batch:        10 of        16\t|\tloss: 1614.9\n",
      "Evaluating Epoch 52  68.8% | batch:        11 of        16\t|\tloss: 2430.68\n",
      "Evaluating Epoch 52  75.0% | batch:        12 of        16\t|\tloss: 1667.37\n",
      "Evaluating Epoch 52  81.2% | batch:        13 of        16\t|\tloss: 959.412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:02,292 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14795684814453125 seconds\n",
      "\n",
      "2023-05-10 17:09:02,292 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.17714727776391165 seconds\n",
      "2023-05-10 17:09:02,292 | INFO : Avg batch val. time: 0.011071704860244478 seconds\n",
      "2023-05-10 17:09:02,293 | INFO : Avg sample val. time: 0.0001753933443207046 seconds\n",
      "2023-05-10 17:09:02,293 | INFO : Epoch 52 Validation Summary: epoch: 52.000000 | loss: 1877.805840 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 52  87.5% | batch:        14 of        16\t|\tloss: 1750.56\n",
      "Evaluating Epoch 52  93.8% | batch:        15 of        16\t|\tloss: 2592.14\n",
      "\n",
      "Training Epoch 53   0.0% | batch:         0 of        64\t|\tloss: 1432.52\n",
      "Training Epoch 53   1.6% | batch:         1 of        64\t|\tloss: 1435.35\n",
      "Training Epoch 53   3.1% | batch:         2 of        64\t|\tloss: 1069.85\n",
      "Training Epoch 53   4.7% | batch:         3 of        64\t|\tloss: 4312.77\n",
      "Training Epoch 53   6.2% | batch:         4 of        64\t|\tloss: 1305.2\n",
      "Training Epoch 53   7.8% | batch:         5 of        64\t|\tloss: 2513.56\n",
      "Training Epoch 53   9.4% | batch:         6 of        64\t|\tloss: 1323.42\n",
      "Training Epoch 53  10.9% | batch:         7 of        64\t|\tloss: 2130.6\n",
      "Training Epoch 53  12.5% | batch:         8 of        64\t|\tloss: 1013.74\n",
      "Training Epoch 53  14.1% | batch:         9 of        64\t|\tloss: 3478.65\n",
      "Training Epoch 53  15.6% | batch:        10 of        64\t|\tloss: 1076.26\n",
      "Training Epoch 53  17.2% | batch:        11 of        64\t|\tloss: 1782.88\n",
      "Training Epoch 53  18.8% | batch:        12 of        64\t|\tloss: 1606.62\n",
      "Training Epoch 53  20.3% | batch:        13 of        64\t|\tloss: 1130.16\n",
      "Training Epoch 53  21.9% | batch:        14 of        64\t|\tloss: 1613.66\n",
      "Training Epoch 53  23.4% | batch:        15 of        64\t|\tloss: 1779.28\n",
      "Training Epoch 53  25.0% | batch:        16 of        64\t|\tloss: 2143.15\n",
      "Training Epoch 53  26.6% | batch:        17 of        64\t|\tloss: 2968.26\n",
      "Training Epoch 53  28.1% | batch:        18 of        64\t|\tloss: 1647.82\n",
      "Training Epoch 53  29.7% | batch:        19 of        64\t|\tloss: 2104.99\n",
      "Training Epoch 53  31.2% | batch:        20 of        64\t|\tloss: 2688.56\n",
      "Training Epoch 53  32.8% | batch:        21 of        64\t|\tloss: 1847.18\n",
      "Training Epoch 53  34.4% | batch:        22 of        64\t|\tloss: 1630.02\n",
      "Training Epoch 53  35.9% | batch:        23 of        64\t|\tloss: 1414.44\n",
      "Training Epoch 53  37.5% | batch:        24 of        64\t|\tloss: 1579.76\n",
      "Training Epoch 53  39.1% | batch:        25 of        64\t|\tloss: 1079.04\n",
      "Training Epoch 53  40.6% | batch:        26 of        64\t|\tloss: 2031.16\n",
      "Training Epoch 53  42.2% | batch:        27 of        64\t|\tloss: 895.893\n",
      "Training Epoch 53  43.8% | batch:        28 of        64\t|\tloss: 1791.4\n",
      "Training Epoch 53  45.3% | batch:        29 of        64\t|\tloss: 1043.82\n",
      "Training Epoch 53  46.9% | batch:        30 of        64\t|\tloss: 1318.12\n",
      "Training Epoch 53  48.4% | batch:        31 of        64\t|\tloss: 1019.67\n",
      "Training Epoch 53  50.0% | batch:        32 of        64\t|\tloss: 3216.98\n",
      "Training Epoch 53  51.6% | batch:        33 of        64\t|\tloss: 1946.97\n",
      "Training Epoch 53  53.1% | batch:        34 of        64\t|\tloss: 883.271\n",
      "Training Epoch 53  54.7% | batch:        35 of        64\t|\tloss: 1416.18\n",
      "Training Epoch 53  56.2% | batch:        36 of        64\t|\tloss: 2779.57\n",
      "Training Epoch 53  57.8% | batch:        37 of        64\t|\tloss: 1219.41\n",
      "Training Epoch 53  59.4% | batch:        38 of        64\t|\tloss: 908.953\n",
      "Training Epoch 53  60.9% | batch:        39 of        64\t|\tloss: 2775.72\n",
      "Training Epoch 53  62.5% | batch:        40 of        64\t|\tloss: 2457.4\n",
      "Training Epoch 53  64.1% | batch:        41 of        64\t|\tloss: 1728.36\n",
      "Training Epoch 53  65.6% | batch:        42 of        64\t|\tloss: 1825.19\n",
      "Training Epoch 53  67.2% | batch:        43 of        64\t|\tloss: 627.763\n",
      "Training Epoch 53  68.8% | batch:        44 of        64\t|\tloss: 2921.61\n",
      "Training Epoch 53  70.3% | batch:        45 of        64\t|\tloss: 1029.65\n",
      "Training Epoch 53  71.9% | batch:        46 of        64\t|\tloss: 851.807\n",
      "Training Epoch 53  73.4% | batch:        47 of        64\t|\tloss: 570.618\n",
      "Training Epoch 53  75.0% | batch:        48 of        64\t|\tloss: 1332.78\n",
      "Training Epoch 53  76.6% | batch:        49 of        64\t|\tloss: 1897.36\n",
      "Training Epoch 53  78.1% | batch:        50 of        64\t|\tloss: 876.09\n",
      "Training Epoch 53  79.7% | batch:        51 of        64\t|\tloss: 1431.84\n",
      "Training Epoch 53  81.2% | batch:        52 of        64\t|\tloss: 650.838\n",
      "Training Epoch 53  82.8% | batch:        53 of        64\t|\tloss: 1236.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:03,491 | INFO : Epoch 53 Training Summary: epoch: 53.000000 | loss: 1625.404142 | \n",
      "2023-05-10 17:09:03,492 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1883463859558105 seconds\n",
      "\n",
      "2023-05-10 17:09:03,492 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.204016186156363 seconds\n",
      "2023-05-10 17:09:03,493 | INFO : Avg batch train. time: 0.01881275290869317 seconds\n",
      "2023-05-10 17:09:03,493 | INFO : Avg sample train. time: 0.00029817141806745 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 53  84.4% | batch:        54 of        64\t|\tloss: 1963.87\n",
      "Training Epoch 53  85.9% | batch:        55 of        64\t|\tloss: 1011.23\n",
      "Training Epoch 53  87.5% | batch:        56 of        64\t|\tloss: 2288.82\n",
      "Training Epoch 53  89.1% | batch:        57 of        64\t|\tloss: 1172.9\n",
      "Training Epoch 53  90.6% | batch:        58 of        64\t|\tloss: 1772.63\n",
      "Training Epoch 53  92.2% | batch:        59 of        64\t|\tloss: 884.438\n",
      "Training Epoch 53  93.8% | batch:        60 of        64\t|\tloss: 864.896\n",
      "Training Epoch 53  95.3% | batch:        61 of        64\t|\tloss: 854.188\n",
      "Training Epoch 53  96.9% | batch:        62 of        64\t|\tloss: 920.429\n",
      "Training Epoch 53  98.4% | batch:        63 of        64\t|\tloss: 288.435\n",
      "\n",
      "Training Epoch 54   0.0% | batch:         0 of        64\t|\tloss: 863.781\n",
      "Training Epoch 54   1.6% | batch:         1 of        64\t|\tloss: 2361.98\n",
      "Training Epoch 54   3.1% | batch:         2 of        64\t|\tloss: 1124.43\n",
      "Training Epoch 54   4.7% | batch:         3 of        64\t|\tloss: 1212.36\n",
      "Training Epoch 54   6.2% | batch:         4 of        64\t|\tloss: 1055.17\n",
      "Training Epoch 54   7.8% | batch:         5 of        64\t|\tloss: 2084.32\n",
      "Training Epoch 54   9.4% | batch:         6 of        64\t|\tloss: 3484.95\n",
      "Training Epoch 54  10.9% | batch:         7 of        64\t|\tloss: 1249.64\n",
      "Training Epoch 54  12.5% | batch:         8 of        64\t|\tloss: 1030.36\n",
      "Training Epoch 54  14.1% | batch:         9 of        64\t|\tloss: 1504.97\n",
      "Training Epoch 54  15.6% | batch:        10 of        64\t|\tloss: 2156.57\n",
      "Training Epoch 54  17.2% | batch:        11 of        64\t|\tloss: 1698.36\n",
      "Training Epoch 54  18.8% | batch:        12 of        64\t|\tloss: 1356.02\n",
      "Training Epoch 54  20.3% | batch:        13 of        64\t|\tloss: 2461.45\n",
      "Training Epoch 54  21.9% | batch:        14 of        64\t|\tloss: 1004.79\n",
      "Training Epoch 54  23.4% | batch:        15 of        64\t|\tloss: 1245.48\n",
      "Training Epoch 54  25.0% | batch:        16 of        64\t|\tloss: 1221.83\n",
      "Training Epoch 54  26.6% | batch:        17 of        64\t|\tloss: 1046.56\n",
      "Training Epoch 54  28.1% | batch:        18 of        64\t|\tloss: 827.426\n",
      "Training Epoch 54  29.7% | batch:        19 of        64\t|\tloss: 1529.15\n",
      "Training Epoch 54  31.2% | batch:        20 of        64\t|\tloss: 3390.93\n",
      "Training Epoch 54  32.8% | batch:        21 of        64\t|\tloss: 1266.97\n",
      "Training Epoch 54  34.4% | batch:        22 of        64\t|\tloss: 1696.78\n",
      "Training Epoch 54  35.9% | batch:        23 of        64\t|\tloss: 1824.42\n",
      "Training Epoch 54  37.5% | batch:        24 of        64\t|\tloss: 1125.64\n",
      "Training Epoch 54  39.1% | batch:        25 of        64\t|\tloss: 1997.22\n",
      "Training Epoch 54  40.6% | batch:        26 of        64\t|\tloss: 2143.2\n",
      "Training Epoch 54  42.2% | batch:        27 of        64\t|\tloss: 971.546\n",
      "Training Epoch 54  43.8% | batch:        28 of        64\t|\tloss: 2317.22\n",
      "Training Epoch 54  45.3% | batch:        29 of        64\t|\tloss: 1757.19\n",
      "Training Epoch 54  46.9% | batch:        30 of        64\t|\tloss: 1402.48\n",
      "Training Epoch 54  48.4% | batch:        31 of        64\t|\tloss: 947.444\n",
      "Training Epoch 54  50.0% | batch:        32 of        64\t|\tloss: 2137.88\n",
      "Training Epoch 54  51.6% | batch:        33 of        64\t|\tloss: 1261.32\n",
      "Training Epoch 54  53.1% | batch:        34 of        64\t|\tloss: 940.825\n",
      "Training Epoch 54  54.7% | batch:        35 of        64\t|\tloss: 1652.1\n",
      "Training Epoch 54  56.2% | batch:        36 of        64\t|\tloss: 1012.46\n",
      "Training Epoch 54  57.8% | batch:        37 of        64\t|\tloss: 1246.26\n",
      "Training Epoch 54  59.4% | batch:        38 of        64\t|\tloss: 1740.76\n",
      "Training Epoch 54  60.9% | batch:        39 of        64\t|\tloss: 946.503\n",
      "Training Epoch 54  62.5% | batch:        40 of        64\t|\tloss: 1091.53\n",
      "Training Epoch 54  64.1% | batch:        41 of        64\t|\tloss: 1178.9\n",
      "Training Epoch 54  65.6% | batch:        42 of        64\t|\tloss: 2524.9\n",
      "Training Epoch 54  67.2% | batch:        43 of        64\t|\tloss: 1366.69\n",
      "Training Epoch 54  68.8% | batch:        44 of        64\t|\tloss: 1347.47\n",
      "Training Epoch 54  70.3% | batch:        45 of        64\t|\tloss: 1270.12\n",
      "Training Epoch 54  71.9% | batch:        46 of        64\t|\tloss: 664.172\n",
      "Training Epoch 54  73.4% | batch:        47 of        64\t|\tloss: 967.462\n",
      "Training Epoch 54  75.0% | batch:        48 of        64\t|\tloss: 1084.28\n",
      "Training Epoch 54  76.6% | batch:        49 of        64\t|\tloss: 759.455\n",
      "Training Epoch 54  78.1% | batch:        50 of        64\t|\tloss: 3414.89\n",
      "Training Epoch 54  79.7% | batch:        51 of        64\t|\tloss: 1041.58\n",
      "Training Epoch 54  81.2% | batch:        52 of        64\t|\tloss: 1166.88\n",
      "Training Epoch 54  82.8% | batch:        53 of        64\t|\tloss: 1632.67\n",
      "Training Epoch 54  84.4% | batch:        54 of        64\t|\tloss: 1550.81\n",
      "Training Epoch 54  85.9% | batch:        55 of        64\t|\tloss: 1715.22\n",
      "Training Epoch 54  87.5% | batch:        56 of        64\t|\tloss: 1075.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:04,683 | INFO : Epoch 54 Training Summary: epoch: 54.000000 | loss: 1502.984848 | \n",
      "2023-05-10 17:09:04,684 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1767351627349854 seconds\n",
      "\n",
      "2023-05-10 17:09:04,684 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.20351098201893 seconds\n",
      "2023-05-10 17:09:04,685 | INFO : Avg batch train. time: 0.018804859094045782 seconds\n",
      "2023-05-10 17:09:04,685 | INFO : Avg sample train. time: 0.0002980463056015181 seconds\n",
      "2023-05-10 17:09:04,685 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 54  89.1% | batch:        57 of        64\t|\tloss: 1264.92\n",
      "Training Epoch 54  90.6% | batch:        58 of        64\t|\tloss: 1234.19\n",
      "Training Epoch 54  92.2% | batch:        59 of        64\t|\tloss: 1182.64\n",
      "Training Epoch 54  93.8% | batch:        60 of        64\t|\tloss: 1600.21\n",
      "Training Epoch 54  95.3% | batch:        61 of        64\t|\tloss: 1033.43\n",
      "Training Epoch 54  96.9% | batch:        62 of        64\t|\tloss: 2336.15\n",
      "Training Epoch 54  98.4% | batch:        63 of        64\t|\tloss: 275.729\n",
      "\n",
      "Evaluating Epoch 54   0.0% | batch:         0 of        16\t|\tloss: 1715.94\n",
      "Evaluating Epoch 54   6.2% | batch:         1 of        16\t|\tloss: 1176.26\n",
      "Evaluating Epoch 54  12.5% | batch:         2 of        16\t|\tloss: 1142.74\n",
      "Evaluating Epoch 54  18.8% | batch:         3 of        16\t|\tloss: 1861.62\n",
      "Evaluating Epoch 54  25.0% | batch:         4 of        16\t|\tloss: 2352.24\n",
      "Evaluating Epoch 54  31.2% | batch:         5 of        16\t|\tloss: 2291.5\n",
      "Evaluating Epoch 54  37.5% | batch:         6 of        16\t|\tloss: 1878.19\n",
      "Evaluating Epoch 54  43.8% | batch:         7 of        16\t|\tloss: 2327.06\n",
      "Evaluating Epoch 54  50.0% | batch:         8 of        16\t|\tloss: 1748.35\n",
      "Evaluating Epoch 54  56.2% | batch:         9 of        16\t|\tloss: 5437.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:04,834 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.148301362991333 seconds\n",
      "\n",
      "2023-05-10 17:09:04,834 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.17615259104761585 seconds\n",
      "2023-05-10 17:09:04,835 | INFO : Avg batch val. time: 0.01100953694047599 seconds\n",
      "2023-05-10 17:09:04,835 | INFO : Avg sample val. time: 0.00017440850598773847 seconds\n",
      "2023-05-10 17:09:04,836 | INFO : Epoch 54 Validation Summary: epoch: 54.000000 | loss: 2078.929633 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 54  62.5% | batch:        10 of        16\t|\tloss: 2048.9\n",
      "Evaluating Epoch 54  68.8% | batch:        11 of        16\t|\tloss: 2378.67\n",
      "Evaluating Epoch 54  75.0% | batch:        12 of        16\t|\tloss: 1400.11\n",
      "Evaluating Epoch 54  81.2% | batch:        13 of        16\t|\tloss: 1081.77\n",
      "Evaluating Epoch 54  87.5% | batch:        14 of        16\t|\tloss: 2153.1\n",
      "Evaluating Epoch 54  93.8% | batch:        15 of        16\t|\tloss: 2322.58\n",
      "\n",
      "Training Epoch 55   0.0% | batch:         0 of        64\t|\tloss: 1716.9\n",
      "Training Epoch 55   1.6% | batch:         1 of        64\t|\tloss: 1506.96\n",
      "Training Epoch 55   3.1% | batch:         2 of        64\t|\tloss: 1816.79\n",
      "Training Epoch 55   4.7% | batch:         3 of        64\t|\tloss: 1122.49\n",
      "Training Epoch 55   6.2% | batch:         4 of        64\t|\tloss: 1026.65\n",
      "Training Epoch 55   7.8% | batch:         5 of        64\t|\tloss: 2006.43\n",
      "Training Epoch 55   9.4% | batch:         6 of        64\t|\tloss: 1663.24\n",
      "Training Epoch 55  10.9% | batch:         7 of        64\t|\tloss: 2324.63\n",
      "Training Epoch 55  12.5% | batch:         8 of        64\t|\tloss: 1366.84\n",
      "Training Epoch 55  14.1% | batch:         9 of        64\t|\tloss: 804.27\n",
      "Training Epoch 55  15.6% | batch:        10 of        64\t|\tloss: 883.598\n",
      "Training Epoch 55  17.2% | batch:        11 of        64\t|\tloss: 1765.21\n",
      "Training Epoch 55  18.8% | batch:        12 of        64\t|\tloss: 959.048\n",
      "Training Epoch 55  20.3% | batch:        13 of        64\t|\tloss: 1250.43\n",
      "Training Epoch 55  21.9% | batch:        14 of        64\t|\tloss: 1359.51\n",
      "Training Epoch 55  23.4% | batch:        15 of        64\t|\tloss: 1083.65\n",
      "Training Epoch 55  25.0% | batch:        16 of        64\t|\tloss: 1410.25\n",
      "Training Epoch 55  26.6% | batch:        17 of        64\t|\tloss: 2353.29\n",
      "Training Epoch 55  28.1% | batch:        18 of        64\t|\tloss: 754.497\n",
      "Training Epoch 55  29.7% | batch:        19 of        64\t|\tloss: 1288.99\n",
      "Training Epoch 55  31.2% | batch:        20 of        64\t|\tloss: 1072.25\n",
      "Training Epoch 55  32.8% | batch:        21 of        64\t|\tloss: 1632.92\n",
      "Training Epoch 55  34.4% | batch:        22 of        64\t|\tloss: 751.501\n",
      "Training Epoch 55  35.9% | batch:        23 of        64\t|\tloss: 2233.62\n",
      "Training Epoch 55  37.5% | batch:        24 of        64\t|\tloss: 1518.3\n",
      "Training Epoch 55  39.1% | batch:        25 of        64\t|\tloss: 3735.99\n",
      "Training Epoch 55  40.6% | batch:        26 of        64\t|\tloss: 1383.86\n",
      "Training Epoch 55  42.2% | batch:        27 of        64\t|\tloss: 1250.54\n",
      "Training Epoch 55  43.8% | batch:        28 of        64\t|\tloss: 935.638\n",
      "Training Epoch 55  45.3% | batch:        29 of        64\t|\tloss: 1060.32\n",
      "Training Epoch 55  46.9% | batch:        30 of        64\t|\tloss: 1735.94\n",
      "Training Epoch 55  48.4% | batch:        31 of        64\t|\tloss: 825.241\n",
      "Training Epoch 55  50.0% | batch:        32 of        64\t|\tloss: 958.841\n",
      "Training Epoch 55  51.6% | batch:        33 of        64\t|\tloss: 1413.93\n",
      "Training Epoch 55  53.1% | batch:        34 of        64\t|\tloss: 1464.03\n",
      "Training Epoch 55  54.7% | batch:        35 of        64\t|\tloss: 1119.25\n",
      "Training Epoch 55  56.2% | batch:        36 of        64\t|\tloss: 1092.8\n",
      "Training Epoch 55  57.8% | batch:        37 of        64\t|\tloss: 1858.11\n",
      "Training Epoch 55  59.4% | batch:        38 of        64\t|\tloss: 2310.11\n",
      "Training Epoch 55  60.9% | batch:        39 of        64\t|\tloss: 1166.87\n",
      "Training Epoch 55  62.5% | batch:        40 of        64\t|\tloss: 885.599\n",
      "Training Epoch 55  64.1% | batch:        41 of        64\t|\tloss: 1343.31\n",
      "Training Epoch 55  65.6% | batch:        42 of        64\t|\tloss: 1089.24\n",
      "Training Epoch 55  67.2% | batch:        43 of        64\t|\tloss: 888.53\n",
      "Training Epoch 55  68.8% | batch:        44 of        64\t|\tloss: 2605.16\n",
      "Training Epoch 55  70.3% | batch:        45 of        64\t|\tloss: 1193.59\n",
      "Training Epoch 55  71.9% | batch:        46 of        64\t|\tloss: 1232.33\n",
      "Training Epoch 55  73.4% | batch:        47 of        64\t|\tloss: 1405.48\n",
      "Training Epoch 55  75.0% | batch:        48 of        64\t|\tloss: 1434.31\n",
      "Training Epoch 55  76.6% | batch:        49 of        64\t|\tloss: 2199.02\n",
      "Training Epoch 55  78.1% | batch:        50 of        64\t|\tloss: 1207.54\n",
      "Training Epoch 55  79.7% | batch:        51 of        64\t|\tloss: 1581.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:06,028 | INFO : Epoch 55 Training Summary: epoch: 55.000000 | loss: 1427.024823 | \n",
      "2023-05-10 17:09:06,028 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1819589138031006 seconds\n",
      "\n",
      "2023-05-10 17:09:06,029 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2031191262331875 seconds\n",
      "2023-05-10 17:09:06,029 | INFO : Avg batch train. time: 0.018798736347393555 seconds\n",
      "2023-05-10 17:09:06,030 | INFO : Avg sample train. time: 0.00029794926355452883 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 55  81.2% | batch:        52 of        64\t|\tloss: 1035.42\n",
      "Training Epoch 55  82.8% | batch:        53 of        64\t|\tloss: 1087.99\n",
      "Training Epoch 55  84.4% | batch:        54 of        64\t|\tloss: 966.447\n",
      "Training Epoch 55  85.9% | batch:        55 of        64\t|\tloss: 2645.08\n",
      "Training Epoch 55  87.5% | batch:        56 of        64\t|\tloss: 1025.81\n",
      "Training Epoch 55  89.1% | batch:        57 of        64\t|\tloss: 970.361\n",
      "Training Epoch 55  90.6% | batch:        58 of        64\t|\tloss: 1976.85\n",
      "Training Epoch 55  92.2% | batch:        59 of        64\t|\tloss: 1154.76\n",
      "Training Epoch 55  93.8% | batch:        60 of        64\t|\tloss: 893.934\n",
      "Training Epoch 55  95.3% | batch:        61 of        64\t|\tloss: 943.078\n",
      "Training Epoch 55  96.9% | batch:        62 of        64\t|\tloss: 797.683\n",
      "Training Epoch 55  98.4% | batch:        63 of        64\t|\tloss: 15889.6\n",
      "\n",
      "Training Epoch 56   0.0% | batch:         0 of        64\t|\tloss: 940.125\n",
      "Training Epoch 56   1.6% | batch:         1 of        64\t|\tloss: 1471.19\n",
      "Training Epoch 56   3.1% | batch:         2 of        64\t|\tloss: 970.57\n",
      "Training Epoch 56   4.7% | batch:         3 of        64\t|\tloss: 942.472\n",
      "Training Epoch 56   6.2% | batch:         4 of        64\t|\tloss: 1413.07\n",
      "Training Epoch 56   7.8% | batch:         5 of        64\t|\tloss: 1179.01\n",
      "Training Epoch 56   9.4% | batch:         6 of        64\t|\tloss: 1628.97\n",
      "Training Epoch 56  10.9% | batch:         7 of        64\t|\tloss: 1195.73\n",
      "Training Epoch 56  12.5% | batch:         8 of        64\t|\tloss: 1000.34\n",
      "Training Epoch 56  14.1% | batch:         9 of        64\t|\tloss: 862.195\n",
      "Training Epoch 56  15.6% | batch:        10 of        64\t|\tloss: 2204.26\n",
      "Training Epoch 56  17.2% | batch:        11 of        64\t|\tloss: 835.835\n",
      "Training Epoch 56  18.8% | batch:        12 of        64\t|\tloss: 1389.73\n",
      "Training Epoch 56  20.3% | batch:        13 of        64\t|\tloss: 1450.29\n",
      "Training Epoch 56  21.9% | batch:        14 of        64\t|\tloss: 1241.34\n",
      "Training Epoch 56  23.4% | batch:        15 of        64\t|\tloss: 2084.96\n",
      "Training Epoch 56  25.0% | batch:        16 of        64\t|\tloss: 1761.65\n",
      "Training Epoch 56  26.6% | batch:        17 of        64\t|\tloss: 1051.49\n",
      "Training Epoch 56  28.1% | batch:        18 of        64\t|\tloss: 1172.81\n",
      "Training Epoch 56  29.7% | batch:        19 of        64\t|\tloss: 1119.12\n",
      "Training Epoch 56  31.2% | batch:        20 of        64\t|\tloss: 726.653\n",
      "Training Epoch 56  32.8% | batch:        21 of        64\t|\tloss: 2234.34\n",
      "Training Epoch 56  34.4% | batch:        22 of        64\t|\tloss: 1221.44\n",
      "Training Epoch 56  35.9% | batch:        23 of        64\t|\tloss: 902.902\n",
      "Training Epoch 56  37.5% | batch:        24 of        64\t|\tloss: 1760.91\n",
      "Training Epoch 56  39.1% | batch:        25 of        64\t|\tloss: 884.732\n",
      "Training Epoch 56  40.6% | batch:        26 of        64\t|\tloss: 2511.2\n",
      "Training Epoch 56  42.2% | batch:        27 of        64\t|\tloss: 1486\n",
      "Training Epoch 56  43.8% | batch:        28 of        64\t|\tloss: 1060.54\n",
      "Training Epoch 56  45.3% | batch:        29 of        64\t|\tloss: 645.802\n",
      "Training Epoch 56  46.9% | batch:        30 of        64\t|\tloss: 863.097\n",
      "Training Epoch 56  48.4% | batch:        31 of        64\t|\tloss: 1169.63\n",
      "Training Epoch 56  50.0% | batch:        32 of        64\t|\tloss: 1342.88\n",
      "Training Epoch 56  51.6% | batch:        33 of        64\t|\tloss: 872.64\n",
      "Training Epoch 56  53.1% | batch:        34 of        64\t|\tloss: 1838.04\n",
      "Training Epoch 56  54.7% | batch:        35 of        64\t|\tloss: 1238.05\n",
      "Training Epoch 56  56.2% | batch:        36 of        64\t|\tloss: 4194.24\n",
      "Training Epoch 56  57.8% | batch:        37 of        64\t|\tloss: 1024.44\n",
      "Training Epoch 56  59.4% | batch:        38 of        64\t|\tloss: 803.634\n",
      "Training Epoch 56  60.9% | batch:        39 of        64\t|\tloss: 1809.09\n",
      "Training Epoch 56  62.5% | batch:        40 of        64\t|\tloss: 1288.38\n",
      "Training Epoch 56  64.1% | batch:        41 of        64\t|\tloss: 1096.46\n",
      "Training Epoch 56  65.6% | batch:        42 of        64\t|\tloss: 1064.04\n",
      "Training Epoch 56  67.2% | batch:        43 of        64\t|\tloss: 2850.63\n",
      "Training Epoch 56  68.8% | batch:        44 of        64\t|\tloss: 842.25\n",
      "Training Epoch 56  70.3% | batch:        45 of        64\t|\tloss: 2958.33\n",
      "Training Epoch 56  71.9% | batch:        46 of        64\t|\tloss: 1788.56\n",
      "Training Epoch 56  73.4% | batch:        47 of        64\t|\tloss: 810.367\n",
      "Training Epoch 56  75.0% | batch:        48 of        64\t|\tloss: 962.898\n",
      "Training Epoch 56  76.6% | batch:        49 of        64\t|\tloss: 761.68\n",
      "Training Epoch 56  78.1% | batch:        50 of        64\t|\tloss: 1017.28\n",
      "Training Epoch 56  79.7% | batch:        51 of        64\t|\tloss: 5333.52\n",
      "Training Epoch 56  81.2% | batch:        52 of        64\t|\tloss: 1095.31\n",
      "Training Epoch 56  82.8% | batch:        53 of        64\t|\tloss: 995.025\n",
      "Training Epoch 56  84.4% | batch:        54 of        64\t|\tloss: 5516.03\n",
      "Training Epoch 56  85.9% | batch:        55 of        64\t|\tloss: 1178.58\n",
      "Training Epoch 56  87.5% | batch:        56 of        64\t|\tloss: 1286.88\n",
      "Training Epoch 56  89.1% | batch:        57 of        64\t|\tloss: 1073.16\n",
      "Training Epoch 56  90.6% | batch:        58 of        64\t|\tloss: 1077.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:07,179 | INFO : Epoch 56 Training Summary: epoch: 56.000000 | loss: 1508.277399 | \n",
      "2023-05-10 17:09:07,180 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1403522491455078 seconds\n",
      "\n",
      "2023-05-10 17:09:07,180 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2019982891423362 seconds\n",
      "2023-05-10 17:09:07,181 | INFO : Avg batch train. time: 0.018781223267849003 seconds\n",
      "2023-05-10 17:09:07,181 | INFO : Avg sample train. time: 0.00029767169121900353 seconds\n",
      "2023-05-10 17:09:07,181 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 56  92.2% | batch:        59 of        64\t|\tloss: 3998.12\n",
      "Training Epoch 56  93.8% | batch:        60 of        64\t|\tloss: 808.218\n",
      "Training Epoch 56  95.3% | batch:        61 of        64\t|\tloss: 967.309\n",
      "Training Epoch 56  96.9% | batch:        62 of        64\t|\tloss: 1831.91\n",
      "Training Epoch 56  98.4% | batch:        63 of        64\t|\tloss: 590.183\n",
      "\n",
      "Evaluating Epoch 56   0.0% | batch:         0 of        16\t|\tloss: 1860.03\n",
      "Evaluating Epoch 56   6.2% | batch:         1 of        16\t|\tloss: 1296.44\n",
      "Evaluating Epoch 56  12.5% | batch:         2 of        16\t|\tloss: 880.027\n",
      "Evaluating Epoch 56  18.8% | batch:         3 of        16\t|\tloss: 2005.84\n",
      "Evaluating Epoch 56  25.0% | batch:         4 of        16\t|\tloss: 2006.58\n",
      "Evaluating Epoch 56  31.2% | batch:         5 of        16\t|\tloss: 2693.83\n",
      "Evaluating Epoch 56  37.5% | batch:         6 of        16\t|\tloss: 1990.52\n",
      "Evaluating Epoch 56  43.8% | batch:         7 of        16\t|\tloss: 2408.18\n",
      "Evaluating Epoch 56  50.0% | batch:         8 of        16\t|\tloss: 1810.6\n",
      "Evaluating Epoch 56  56.2% | batch:         9 of        16\t|\tloss: 5166.42\n",
      "Evaluating Epoch 56  62.5% | batch:        10 of        16\t|\tloss: 2030.38\n",
      "Evaluating Epoch 56  68.8% | batch:        11 of        16\t|\tloss: 2706.38\n",
      "Evaluating Epoch 56  75.0% | batch:        12 of        16\t|\tloss: 1308.55\n",
      "Evaluating Epoch 56  81.2% | batch:        13 of        16\t|\tloss: 1118.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:07,329 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.1468944549560547 seconds\n",
      "\n",
      "2023-05-10 17:09:07,329 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.1751773198445638 seconds\n",
      "2023-05-10 17:09:07,329 | INFO : Avg batch val. time: 0.010948582490285238 seconds\n",
      "2023-05-10 17:09:07,330 | INFO : Avg sample val. time: 0.0001734428909352117 seconds\n",
      "2023-05-10 17:09:07,330 | INFO : Epoch 56 Validation Summary: epoch: 56.000000 | loss: 2117.479045 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 56  87.5% | batch:        14 of        16\t|\tloss: 1920.22\n",
      "Evaluating Epoch 56  93.8% | batch:        15 of        16\t|\tloss: 2833.94\n",
      "\n",
      "Training Epoch 57   0.0% | batch:         0 of        64\t|\tloss: 1065.18\n",
      "Training Epoch 57   1.6% | batch:         1 of        64\t|\tloss: 1191.67\n",
      "Training Epoch 57   3.1% | batch:         2 of        64\t|\tloss: 2070.38\n",
      "Training Epoch 57   4.7% | batch:         3 of        64\t|\tloss: 851\n",
      "Training Epoch 57   6.2% | batch:         4 of        64\t|\tloss: 1571.93\n",
      "Training Epoch 57   7.8% | batch:         5 of        64\t|\tloss: 976.881\n",
      "Training Epoch 57   9.4% | batch:         6 of        64\t|\tloss: 1029.09\n",
      "Training Epoch 57  10.9% | batch:         7 of        64\t|\tloss: 1297.75\n",
      "Training Epoch 57  12.5% | batch:         8 of        64\t|\tloss: 1306.31\n",
      "Training Epoch 57  14.1% | batch:         9 of        64\t|\tloss: 869.904\n",
      "Training Epoch 57  15.6% | batch:        10 of        64\t|\tloss: 1374.65\n",
      "Training Epoch 57  17.2% | batch:        11 of        64\t|\tloss: 1336.91\n",
      "Training Epoch 57  18.8% | batch:        12 of        64\t|\tloss: 1558.77\n",
      "Training Epoch 57  20.3% | batch:        13 of        64\t|\tloss: 544.837\n",
      "Training Epoch 57  21.9% | batch:        14 of        64\t|\tloss: 883.405\n",
      "Training Epoch 57  23.4% | batch:        15 of        64\t|\tloss: 909.815\n",
      "Training Epoch 57  25.0% | batch:        16 of        64\t|\tloss: 941.438\n",
      "Training Epoch 57  26.6% | batch:        17 of        64\t|\tloss: 1062.82\n",
      "Training Epoch 57  28.1% | batch:        18 of        64\t|\tloss: 896.216\n",
      "Training Epoch 57  29.7% | batch:        19 of        64\t|\tloss: 1006.44\n",
      "Training Epoch 57  31.2% | batch:        20 of        64\t|\tloss: 1396.81\n",
      "Training Epoch 57  32.8% | batch:        21 of        64\t|\tloss: 1275.52\n",
      "Training Epoch 57  34.4% | batch:        22 of        64\t|\tloss: 875.799\n",
      "Training Epoch 57  35.9% | batch:        23 of        64\t|\tloss: 828.095\n",
      "Training Epoch 57  37.5% | batch:        24 of        64\t|\tloss: 1328.21\n",
      "Training Epoch 57  39.1% | batch:        25 of        64\t|\tloss: 1498.73\n",
      "Training Epoch 57  40.6% | batch:        26 of        64\t|\tloss: 856.653\n",
      "Training Epoch 57  42.2% | batch:        27 of        64\t|\tloss: 782.614\n",
      "Training Epoch 57  43.8% | batch:        28 of        64\t|\tloss: 632.393\n",
      "Training Epoch 57  45.3% | batch:        29 of        64\t|\tloss: 1648.46\n",
      "Training Epoch 57  46.9% | batch:        30 of        64\t|\tloss: 1414.2\n",
      "Training Epoch 57  48.4% | batch:        31 of        64\t|\tloss: 2802.16\n",
      "Training Epoch 57  50.0% | batch:        32 of        64\t|\tloss: 1305.76\n",
      "Training Epoch 57  51.6% | batch:        33 of        64\t|\tloss: 1406.27\n",
      "Training Epoch 57  53.1% | batch:        34 of        64\t|\tloss: 1250.36\n",
      "Training Epoch 57  54.7% | batch:        35 of        64\t|\tloss: 973.024\n",
      "Training Epoch 57  56.2% | batch:        36 of        64\t|\tloss: 1044.51\n",
      "Training Epoch 57  57.8% | batch:        37 of        64\t|\tloss: 1267.16\n",
      "Training Epoch 57  59.4% | batch:        38 of        64\t|\tloss: 1961.93\n",
      "Training Epoch 57  60.9% | batch:        39 of        64\t|\tloss: 2004.07\n",
      "Training Epoch 57  62.5% | batch:        40 of        64\t|\tloss: 1176.87\n",
      "Training Epoch 57  64.1% | batch:        41 of        64\t|\tloss: 3420.25\n",
      "Training Epoch 57  65.6% | batch:        42 of        64\t|\tloss: 2138.74\n",
      "Training Epoch 57  67.2% | batch:        43 of        64\t|\tloss: 1632.75\n",
      "Training Epoch 57  68.8% | batch:        44 of        64\t|\tloss: 781.137\n",
      "Training Epoch 57  70.3% | batch:        45 of        64\t|\tloss: 1199.01\n",
      "Training Epoch 57  71.9% | batch:        46 of        64\t|\tloss: 705.352\n",
      "Training Epoch 57  73.4% | batch:        47 of        64\t|\tloss: 1313.28\n",
      "Training Epoch 57  75.0% | batch:        48 of        64\t|\tloss: 1922.55\n",
      "Training Epoch 57  76.6% | batch:        49 of        64\t|\tloss: 1101.65\n",
      "Training Epoch 57  78.1% | batch:        50 of        64\t|\tloss: 972.749\n",
      "Training Epoch 57  79.7% | batch:        51 of        64\t|\tloss: 1032.57\n",
      "Training Epoch 57  81.2% | batch:        52 of        64\t|\tloss: 719.73\n",
      "Training Epoch 57  82.8% | batch:        53 of        64\t|\tloss: 1168.91\n",
      "Training Epoch 57  84.4% | batch:        54 of        64\t|\tloss: 888.202\n",
      "Training Epoch 57  85.9% | batch:        55 of        64\t|\tloss: 983.321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:08,497 | INFO : Epoch 57 Training Summary: epoch: 57.000000 | loss: 1272.210406 | \n",
      "2023-05-10 17:09:08,498 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1578552722930908 seconds\n",
      "\n",
      "2023-05-10 17:09:08,498 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2012238502502441 seconds\n",
      "2023-05-10 17:09:08,499 | INFO : Avg batch train. time: 0.018769122660160065 seconds\n",
      "2023-05-10 17:09:08,500 | INFO : Avg sample train. time: 0.00029747990347950574 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 57  87.5% | batch:        56 of        64\t|\tloss: 3981.82\n",
      "Training Epoch 57  89.1% | batch:        57 of        64\t|\tloss: 641.246\n",
      "Training Epoch 57  90.6% | batch:        58 of        64\t|\tloss: 1084.42\n",
      "Training Epoch 57  92.2% | batch:        59 of        64\t|\tloss: 1291.9\n",
      "Training Epoch 57  93.8% | batch:        60 of        64\t|\tloss: 1140.85\n",
      "Training Epoch 57  95.3% | batch:        61 of        64\t|\tloss: 921.972\n",
      "Training Epoch 57  96.9% | batch:        62 of        64\t|\tloss: 643.781\n",
      "Training Epoch 57  98.4% | batch:        63 of        64\t|\tloss: 1145.15\n",
      "\n",
      "Training Epoch 58   0.0% | batch:         0 of        64\t|\tloss: 3135.31\n",
      "Training Epoch 58   1.6% | batch:         1 of        64\t|\tloss: 1397.61\n",
      "Training Epoch 58   3.1% | batch:         2 of        64\t|\tloss: 837.826\n",
      "Training Epoch 58   4.7% | batch:         3 of        64\t|\tloss: 915.172\n",
      "Training Epoch 58   6.2% | batch:         4 of        64\t|\tloss: 771.556\n",
      "Training Epoch 58   7.8% | batch:         5 of        64\t|\tloss: 1045.79\n",
      "Training Epoch 58   9.4% | batch:         6 of        64\t|\tloss: 1077.83\n",
      "Training Epoch 58  10.9% | batch:         7 of        64\t|\tloss: 2273.1\n",
      "Training Epoch 58  12.5% | batch:         8 of        64\t|\tloss: 703.94\n",
      "Training Epoch 58  14.1% | batch:         9 of        64\t|\tloss: 902.855\n",
      "Training Epoch 58  15.6% | batch:        10 of        64\t|\tloss: 938.716\n",
      "Training Epoch 58  17.2% | batch:        11 of        64\t|\tloss: 1027.69\n",
      "Training Epoch 58  18.8% | batch:        12 of        64\t|\tloss: 734.476\n",
      "Training Epoch 58  20.3% | batch:        13 of        64\t|\tloss: 1555.72\n",
      "Training Epoch 58  21.9% | batch:        14 of        64\t|\tloss: 1542.12\n",
      "Training Epoch 58  23.4% | batch:        15 of        64\t|\tloss: 1127.14\n",
      "Training Epoch 58  25.0% | batch:        16 of        64\t|\tloss: 1101.15\n",
      "Training Epoch 58  26.6% | batch:        17 of        64\t|\tloss: 1330.28\n",
      "Training Epoch 58  28.1% | batch:        18 of        64\t|\tloss: 734.822\n",
      "Training Epoch 58  29.7% | batch:        19 of        64\t|\tloss: 1910.15\n",
      "Training Epoch 58  31.2% | batch:        20 of        64\t|\tloss: 1533.43\n",
      "Training Epoch 58  32.8% | batch:        21 of        64\t|\tloss: 687.128\n",
      "Training Epoch 58  34.4% | batch:        22 of        64\t|\tloss: 855.4\n",
      "Training Epoch 58  35.9% | batch:        23 of        64\t|\tloss: 2273.89\n",
      "Training Epoch 58  37.5% | batch:        24 of        64\t|\tloss: 883.515\n",
      "Training Epoch 58  39.1% | batch:        25 of        64\t|\tloss: 1063.02\n",
      "Training Epoch 58  40.6% | batch:        26 of        64\t|\tloss: 1010.36\n",
      "Training Epoch 58  42.2% | batch:        27 of        64\t|\tloss: 1362.02\n",
      "Training Epoch 58  43.8% | batch:        28 of        64\t|\tloss: 1203.86\n",
      "Training Epoch 58  45.3% | batch:        29 of        64\t|\tloss: 1193.09\n",
      "Training Epoch 58  46.9% | batch:        30 of        64\t|\tloss: 1080.15\n",
      "Training Epoch 58  48.4% | batch:        31 of        64\t|\tloss: 1440.32\n",
      "Training Epoch 58  50.0% | batch:        32 of        64\t|\tloss: 1152.03\n",
      "Training Epoch 58  51.6% | batch:        33 of        64\t|\tloss: 2164.15\n",
      "Training Epoch 58  53.1% | batch:        34 of        64\t|\tloss: 2057.3\n",
      "Training Epoch 58  54.7% | batch:        35 of        64\t|\tloss: 784.577\n",
      "Training Epoch 58  56.2% | batch:        36 of        64\t|\tloss: 1187.72\n",
      "Training Epoch 58  57.8% | batch:        37 of        64\t|\tloss: 1103.98\n",
      "Training Epoch 58  59.4% | batch:        38 of        64\t|\tloss: 2127.81\n",
      "Training Epoch 58  60.9% | batch:        39 of        64\t|\tloss: 997.003\n",
      "Training Epoch 58  62.5% | batch:        40 of        64\t|\tloss: 535.387\n",
      "Training Epoch 58  64.1% | batch:        41 of        64\t|\tloss: 1371.66\n",
      "Training Epoch 58  65.6% | batch:        42 of        64\t|\tloss: 1601.27\n",
      "Training Epoch 58  67.2% | batch:        43 of        64\t|\tloss: 1059.77\n",
      "Training Epoch 58  68.8% | batch:        44 of        64\t|\tloss: 934.791\n",
      "Training Epoch 58  70.3% | batch:        45 of        64\t|\tloss: 746.502\n",
      "Training Epoch 58  71.9% | batch:        46 of        64\t|\tloss: 1209.73\n",
      "Training Epoch 58  73.4% | batch:        47 of        64\t|\tloss: 990.45\n",
      "Training Epoch 58  75.0% | batch:        48 of        64\t|\tloss: 4295.88\n",
      "Training Epoch 58  76.6% | batch:        49 of        64\t|\tloss: 1123.83\n",
      "Training Epoch 58  78.1% | batch:        50 of        64\t|\tloss: 1690.27\n",
      "Training Epoch 58  79.7% | batch:        51 of        64\t|\tloss: 1197.08\n",
      "Training Epoch 58  81.2% | batch:        52 of        64\t|\tloss: 1284.43\n",
      "Training Epoch 58  82.8% | batch:        53 of        64\t|\tloss: 1346.27\n",
      "Training Epoch 58  84.4% | batch:        54 of        64\t|\tloss: 1517.12\n",
      "Training Epoch 58  85.9% | batch:        55 of        64\t|\tloss: 1526.07\n",
      "Training Epoch 58  87.5% | batch:        56 of        64\t|\tloss: 1095.67\n",
      "Training Epoch 58  89.1% | batch:        57 of        64\t|\tloss: 1187.68\n",
      "Training Epoch 58  90.6% | batch:        58 of        64\t|\tloss: 1102.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:09,702 | INFO : Epoch 58 Training Summary: epoch: 58.000000 | loss: 1349.541694 | \n",
      "2023-05-10 17:09:09,702 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.192746877670288 seconds\n",
      "\n",
      "2023-05-10 17:09:09,703 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2010776955505897 seconds\n",
      "2023-05-10 17:09:09,703 | INFO : Avg batch train. time: 0.018766838992977965 seconds\n",
      "2023-05-10 17:09:09,704 | INFO : Avg sample train. time: 0.00029744370865542096 seconds\n",
      "2023-05-10 17:09:09,705 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 58  92.2% | batch:        59 of        64\t|\tloss: 1546.07\n",
      "Training Epoch 58  93.8% | batch:        60 of        64\t|\tloss: 2514.89\n",
      "Training Epoch 58  95.3% | batch:        61 of        64\t|\tloss: 1240.51\n",
      "Training Epoch 58  96.9% | batch:        62 of        64\t|\tloss: 846.201\n",
      "Training Epoch 58  98.4% | batch:        63 of        64\t|\tloss: 20903.1\n",
      "\n",
      "Evaluating Epoch 58   0.0% | batch:         0 of        16\t|\tloss: 2346.67\n",
      "Evaluating Epoch 58   6.2% | batch:         1 of        16\t|\tloss: 2021.68\n",
      "Evaluating Epoch 58  12.5% | batch:         2 of        16\t|\tloss: 1539.78\n",
      "Evaluating Epoch 58  18.8% | batch:         3 of        16\t|\tloss: 2310.03\n",
      "Evaluating Epoch 58  25.0% | batch:         4 of        16\t|\tloss: 1602\n",
      "Evaluating Epoch 58  31.2% | batch:         5 of        16\t|\tloss: 2855.7\n",
      "Evaluating Epoch 58  37.5% | batch:         6 of        16\t|\tloss: 2688.74\n",
      "Evaluating Epoch 58  43.8% | batch:         7 of        16\t|\tloss: 3084.51\n",
      "Evaluating Epoch 58  50.0% | batch:         8 of        16\t|\tloss: 2101.09\n",
      "Evaluating Epoch 58  56.2% | batch:         9 of        16\t|\tloss: 8754.45\n",
      "Evaluating Epoch 58  62.5% | batch:        10 of        16\t|\tloss: 2598.41\n",
      "Evaluating Epoch 58  68.8% | batch:        11 of        16\t|\tloss: 3220.46\n",
      "Evaluating Epoch 58  75.0% | batch:        12 of        16\t|\tloss: 968.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:09,855 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14937520027160645 seconds\n",
      "\n",
      "2023-05-10 17:09:09,855 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.17434499340672646 seconds\n",
      "2023-05-10 17:09:09,855 | INFO : Avg batch val. time: 0.010896562087920404 seconds\n",
      "2023-05-10 17:09:09,856 | INFO : Avg sample val. time: 0.0001726188053531945 seconds\n",
      "2023-05-10 17:09:09,857 | INFO : Epoch 58 Validation Summary: epoch: 58.000000 | loss: 2758.245452 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 58  81.2% | batch:        13 of        16\t|\tloss: 1667.59\n",
      "Evaluating Epoch 58  87.5% | batch:        14 of        16\t|\tloss: 2822.3\n",
      "Evaluating Epoch 58  93.8% | batch:        15 of        16\t|\tloss: 3771.27\n",
      "\n",
      "Training Epoch 59   0.0% | batch:         0 of        64\t|\tloss: 2026.44\n",
      "Training Epoch 59   1.6% | batch:         1 of        64\t|\tloss: 1685.9\n",
      "Training Epoch 59   3.1% | batch:         2 of        64\t|\tloss: 2273.52\n",
      "Training Epoch 59   4.7% | batch:         3 of        64\t|\tloss: 638.534\n",
      "Training Epoch 59   6.2% | batch:         4 of        64\t|\tloss: 3079.86\n",
      "Training Epoch 59   7.8% | batch:         5 of        64\t|\tloss: 1320.71\n",
      "Training Epoch 59   9.4% | batch:         6 of        64\t|\tloss: 884.173\n",
      "Training Epoch 59  10.9% | batch:         7 of        64\t|\tloss: 1317.94\n",
      "Training Epoch 59  12.5% | batch:         8 of        64\t|\tloss: 1213.19\n",
      "Training Epoch 59  14.1% | batch:         9 of        64\t|\tloss: 1831.31\n",
      "Training Epoch 59  15.6% | batch:        10 of        64\t|\tloss: 1287.81\n",
      "Training Epoch 59  17.2% | batch:        11 of        64\t|\tloss: 1297.78\n",
      "Training Epoch 59  18.8% | batch:        12 of        64\t|\tloss: 1085.06\n",
      "Training Epoch 59  20.3% | batch:        13 of        64\t|\tloss: 1570.42\n",
      "Training Epoch 59  21.9% | batch:        14 of        64\t|\tloss: 1260.67\n",
      "Training Epoch 59  23.4% | batch:        15 of        64\t|\tloss: 980.882\n",
      "Training Epoch 59  25.0% | batch:        16 of        64\t|\tloss: 1301.17\n",
      "Training Epoch 59  26.6% | batch:        17 of        64\t|\tloss: 1595.75\n",
      "Training Epoch 59  28.1% | batch:        18 of        64\t|\tloss: 1556.39\n",
      "Training Epoch 59  29.7% | batch:        19 of        64\t|\tloss: 2221.07\n",
      "Training Epoch 59  31.2% | batch:        20 of        64\t|\tloss: 1475.54\n",
      "Training Epoch 59  32.8% | batch:        21 of        64\t|\tloss: 1666.16\n",
      "Training Epoch 59  34.4% | batch:        22 of        64\t|\tloss: 973.884\n",
      "Training Epoch 59  35.9% | batch:        23 of        64\t|\tloss: 1699.09\n",
      "Training Epoch 59  37.5% | batch:        24 of        64\t|\tloss: 1096.04\n",
      "Training Epoch 59  39.1% | batch:        25 of        64\t|\tloss: 1952.5\n",
      "Training Epoch 59  40.6% | batch:        26 of        64\t|\tloss: 1566.01\n",
      "Training Epoch 59  42.2% | batch:        27 of        64\t|\tloss: 1932.49\n",
      "Training Epoch 59  43.8% | batch:        28 of        64\t|\tloss: 1482.18\n",
      "Training Epoch 59  45.3% | batch:        29 of        64\t|\tloss: 1140.74\n",
      "Training Epoch 59  46.9% | batch:        30 of        64\t|\tloss: 800.075\n",
      "Training Epoch 59  48.4% | batch:        31 of        64\t|\tloss: 1099.98\n",
      "Training Epoch 59  50.0% | batch:        32 of        64\t|\tloss: 766.616\n",
      "Training Epoch 59  51.6% | batch:        33 of        64\t|\tloss: 980.204\n",
      "Training Epoch 59  53.1% | batch:        34 of        64\t|\tloss: 3238.64\n",
      "Training Epoch 59  54.7% | batch:        35 of        64\t|\tloss: 829.57\n",
      "Training Epoch 59  56.2% | batch:        36 of        64\t|\tloss: 906.516\n",
      "Training Epoch 59  57.8% | batch:        37 of        64\t|\tloss: 743.043\n",
      "Training Epoch 59  59.4% | batch:        38 of        64\t|\tloss: 1313.82\n",
      "Training Epoch 59  60.9% | batch:        39 of        64\t|\tloss: 1200.09\n",
      "Training Epoch 59  62.5% | batch:        40 of        64\t|\tloss: 2773.84\n",
      "Training Epoch 59  64.1% | batch:        41 of        64\t|\tloss: 1418.45\n",
      "Training Epoch 59  65.6% | batch:        42 of        64\t|\tloss: 2018.12\n",
      "Training Epoch 59  67.2% | batch:        43 of        64\t|\tloss: 2934.66\n",
      "Training Epoch 59  68.8% | batch:        44 of        64\t|\tloss: 1438.97\n",
      "Training Epoch 59  70.3% | batch:        45 of        64\t|\tloss: 1972.47\n",
      "Training Epoch 59  71.9% | batch:        46 of        64\t|\tloss: 4952.18\n",
      "Training Epoch 59  73.4% | batch:        47 of        64\t|\tloss: 1302.39\n",
      "Training Epoch 59  75.0% | batch:        48 of        64\t|\tloss: 1404.69\n",
      "Training Epoch 59  76.6% | batch:        49 of        64\t|\tloss: 744.168\n",
      "Training Epoch 59  78.1% | batch:        50 of        64\t|\tloss: 922.525\n",
      "Training Epoch 59  79.7% | batch:        51 of        64\t|\tloss: 1055.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:11,068 | INFO : Epoch 59 Training Summary: epoch: 59.000000 | loss: 1484.191343 | \n",
      "2023-05-10 17:09:11,069 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1980154514312744 seconds\n",
      "\n",
      "2023-05-10 17:09:11,069 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2010257931078894 seconds\n",
      "2023-05-10 17:09:11,070 | INFO : Avg batch train. time: 0.01876602801731077 seconds\n",
      "2023-05-10 17:09:11,071 | INFO : Avg sample train. time: 0.00029743085515301866 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 59  81.2% | batch:        52 of        64\t|\tloss: 845.584\n",
      "Training Epoch 59  82.8% | batch:        53 of        64\t|\tloss: 839.265\n",
      "Training Epoch 59  84.4% | batch:        54 of        64\t|\tloss: 2150.8\n",
      "Training Epoch 59  85.9% | batch:        55 of        64\t|\tloss: 1169.13\n",
      "Training Epoch 59  87.5% | batch:        56 of        64\t|\tloss: 1333.85\n",
      "Training Epoch 59  89.1% | batch:        57 of        64\t|\tloss: 1285.48\n",
      "Training Epoch 59  90.6% | batch:        58 of        64\t|\tloss: 1131\n",
      "Training Epoch 59  92.2% | batch:        59 of        64\t|\tloss: 1225.82\n",
      "Training Epoch 59  93.8% | batch:        60 of        64\t|\tloss: 1066.56\n",
      "Training Epoch 59  95.3% | batch:        61 of        64\t|\tloss: 1329.83\n",
      "Training Epoch 59  96.9% | batch:        62 of        64\t|\tloss: 877.318\n",
      "Training Epoch 59  98.4% | batch:        63 of        64\t|\tloss: 1691.18\n",
      "\n",
      "Training Epoch 60   0.0% | batch:         0 of        64\t|\tloss: 1500.71\n",
      "Training Epoch 60   1.6% | batch:         1 of        64\t|\tloss: 910.696\n",
      "Training Epoch 60   3.1% | batch:         2 of        64\t|\tloss: 896.313\n",
      "Training Epoch 60   4.7% | batch:         3 of        64\t|\tloss: 2078.18\n",
      "Training Epoch 60   6.2% | batch:         4 of        64\t|\tloss: 2368.75\n",
      "Training Epoch 60   7.8% | batch:         5 of        64\t|\tloss: 1994.84\n",
      "Training Epoch 60   9.4% | batch:         6 of        64\t|\tloss: 809.927\n",
      "Training Epoch 60  10.9% | batch:         7 of        64\t|\tloss: 709.004\n",
      "Training Epoch 60  12.5% | batch:         8 of        64\t|\tloss: 2689.09\n",
      "Training Epoch 60  14.1% | batch:         9 of        64\t|\tloss: 1833.92\n",
      "Training Epoch 60  15.6% | batch:        10 of        64\t|\tloss: 915.692\n",
      "Training Epoch 60  17.2% | batch:        11 of        64\t|\tloss: 773.856\n",
      "Training Epoch 60  18.8% | batch:        12 of        64\t|\tloss: 1141.38\n",
      "Training Epoch 60  20.3% | batch:        13 of        64\t|\tloss: 2001.25\n",
      "Training Epoch 60  21.9% | batch:        14 of        64\t|\tloss: 1214.92\n",
      "Training Epoch 60  23.4% | batch:        15 of        64\t|\tloss: 953.049\n",
      "Training Epoch 60  25.0% | batch:        16 of        64\t|\tloss: 1187.33\n",
      "Training Epoch 60  26.6% | batch:        17 of        64\t|\tloss: 1184.65\n",
      "Training Epoch 60  28.1% | batch:        18 of        64\t|\tloss: 2334.69\n",
      "Training Epoch 60  29.7% | batch:        19 of        64\t|\tloss: 1073\n",
      "Training Epoch 60  31.2% | batch:        20 of        64\t|\tloss: 779.881\n",
      "Training Epoch 60  32.8% | batch:        21 of        64\t|\tloss: 2326.55\n",
      "Training Epoch 60  34.4% | batch:        22 of        64\t|\tloss: 1260.02\n",
      "Training Epoch 60  35.9% | batch:        23 of        64\t|\tloss: 1252.83\n",
      "Training Epoch 60  37.5% | batch:        24 of        64\t|\tloss: 696.339\n",
      "Training Epoch 60  39.1% | batch:        25 of        64\t|\tloss: 1259.44\n",
      "Training Epoch 60  40.6% | batch:        26 of        64\t|\tloss: 1581.37\n",
      "Training Epoch 60  42.2% | batch:        27 of        64\t|\tloss: 1453.76\n",
      "Training Epoch 60  43.8% | batch:        28 of        64\t|\tloss: 2672.19\n",
      "Training Epoch 60  45.3% | batch:        29 of        64\t|\tloss: 1411.28\n",
      "Training Epoch 60  46.9% | batch:        30 of        64\t|\tloss: 1374.62\n",
      "Training Epoch 60  48.4% | batch:        31 of        64\t|\tloss: 1101.28\n",
      "Training Epoch 60  50.0% | batch:        32 of        64\t|\tloss: 881.639\n",
      "Training Epoch 60  51.6% | batch:        33 of        64\t|\tloss: 1169.91\n",
      "Training Epoch 60  53.1% | batch:        34 of        64\t|\tloss: 811.498\n",
      "Training Epoch 60  54.7% | batch:        35 of        64\t|\tloss: 1369.82\n",
      "Training Epoch 60  56.2% | batch:        36 of        64\t|\tloss: 1025.71\n",
      "Training Epoch 60  57.8% | batch:        37 of        64\t|\tloss: 1048.38\n",
      "Training Epoch 60  59.4% | batch:        38 of        64\t|\tloss: 946.103\n",
      "Training Epoch 60  60.9% | batch:        39 of        64\t|\tloss: 1074.17\n",
      "Training Epoch 60  62.5% | batch:        40 of        64\t|\tloss: 1098.48\n",
      "Training Epoch 60  64.1% | batch:        41 of        64\t|\tloss: 1488.21\n",
      "Training Epoch 60  65.6% | batch:        42 of        64\t|\tloss: 1037.78\n",
      "Training Epoch 60  67.2% | batch:        43 of        64\t|\tloss: 756.254\n",
      "Training Epoch 60  68.8% | batch:        44 of        64\t|\tloss: 985.296\n",
      "Training Epoch 60  70.3% | batch:        45 of        64\t|\tloss: 770.977\n",
      "Training Epoch 60  71.9% | batch:        46 of        64\t|\tloss: 1072.17\n",
      "Training Epoch 60  73.4% | batch:        47 of        64\t|\tloss: 1048.41\n",
      "Training Epoch 60  75.0% | batch:        48 of        64\t|\tloss: 1625.04\n",
      "Training Epoch 60  76.6% | batch:        49 of        64\t|\tloss: 1020.09\n",
      "Training Epoch 60  78.1% | batch:        50 of        64\t|\tloss: 596.311\n",
      "Training Epoch 60  79.7% | batch:        51 of        64\t|\tloss: 897.819\n",
      "Training Epoch 60  81.2% | batch:        52 of        64\t|\tloss: 1102\n",
      "Training Epoch 60  82.8% | batch:        53 of        64\t|\tloss: 2172.63\n",
      "Training Epoch 60  84.4% | batch:        54 of        64\t|\tloss: 1811.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:12,261 | INFO : Epoch 60 Training Summary: epoch: 60.000000 | loss: 1349.238203 | \n",
      "2023-05-10 17:09:12,262 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1818957328796387 seconds\n",
      "\n",
      "2023-05-10 17:09:12,263 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.200706958770752 seconds\n",
      "2023-05-10 17:09:12,264 | INFO : Avg batch train. time: 0.018761046230793 seconds\n",
      "2023-05-10 17:09:12,265 | INFO : Avg sample train. time: 0.0002973518966742823 seconds\n",
      "2023-05-10 17:09:12,266 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 60  85.9% | batch:        55 of        64\t|\tloss: 897.604\n",
      "Training Epoch 60  87.5% | batch:        56 of        64\t|\tloss: 1360.48\n",
      "Training Epoch 60  89.1% | batch:        57 of        64\t|\tloss: 1195.38\n",
      "Training Epoch 60  90.6% | batch:        58 of        64\t|\tloss: 633.462\n",
      "Training Epoch 60  92.2% | batch:        59 of        64\t|\tloss: 1184.19\n",
      "Training Epoch 60  93.8% | batch:        60 of        64\t|\tloss: 1419.2\n",
      "Training Epoch 60  95.3% | batch:        61 of        64\t|\tloss: 4374.86\n",
      "Training Epoch 60  96.9% | batch:        62 of        64\t|\tloss: 1806.32\n",
      "Training Epoch 60  98.4% | batch:        63 of        64\t|\tloss: 7528.07\n",
      "\n",
      "Evaluating Epoch 60   0.0% | batch:         0 of        16\t|\tloss: 2600.06\n",
      "Evaluating Epoch 60   6.2% | batch:         1 of        16\t|\tloss: 1574.95\n",
      "Evaluating Epoch 60  12.5% | batch:         2 of        16\t|\tloss: 1005.69\n",
      "Evaluating Epoch 60  18.8% | batch:         3 of        16\t|\tloss: 2604.53\n",
      "Evaluating Epoch 60  25.0% | batch:         4 of        16\t|\tloss: 2000.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:12,417 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.15059661865234375 seconds\n",
      "\n",
      "2023-05-10 17:09:12,417 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.173602856695652 seconds\n",
      "2023-05-10 17:09:12,418 | INFO : Avg batch val. time: 0.01085017854347825 seconds\n",
      "2023-05-10 17:09:12,418 | INFO : Avg sample val. time: 0.0001718840165303485 seconds\n",
      "2023-05-10 17:09:12,419 | INFO : Epoch 60 Validation Summary: epoch: 60.000000 | loss: 2399.114337 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 60  31.2% | batch:         5 of        16\t|\tloss: 2657.52\n",
      "Evaluating Epoch 60  37.5% | batch:         6 of        16\t|\tloss: 2767.91\n",
      "Evaluating Epoch 60  43.8% | batch:         7 of        16\t|\tloss: 2711.08\n",
      "Evaluating Epoch 60  50.0% | batch:         8 of        16\t|\tloss: 1834.78\n",
      "Evaluating Epoch 60  56.2% | batch:         9 of        16\t|\tloss: 5014.79\n",
      "Evaluating Epoch 60  62.5% | batch:        10 of        16\t|\tloss: 2392.36\n",
      "Evaluating Epoch 60  68.8% | batch:        11 of        16\t|\tloss: 2789.14\n",
      "Evaluating Epoch 60  75.0% | batch:        12 of        16\t|\tloss: 1393.82\n",
      "Evaluating Epoch 60  81.2% | batch:        13 of        16\t|\tloss: 1562.03\n",
      "Evaluating Epoch 60  87.5% | batch:        14 of        16\t|\tloss: 2500.1\n",
      "Evaluating Epoch 60  93.8% | batch:        15 of        16\t|\tloss: 3137.91\n",
      "\n",
      "Training Epoch 61   0.0% | batch:         0 of        64\t|\tloss: 843.647\n",
      "Training Epoch 61   1.6% | batch:         1 of        64\t|\tloss: 930.083\n",
      "Training Epoch 61   3.1% | batch:         2 of        64\t|\tloss: 1206.84\n",
      "Training Epoch 61   4.7% | batch:         3 of        64\t|\tloss: 2901.05\n",
      "Training Epoch 61   6.2% | batch:         4 of        64\t|\tloss: 1004.24\n",
      "Training Epoch 61   7.8% | batch:         5 of        64\t|\tloss: 922.15\n",
      "Training Epoch 61   9.4% | batch:         6 of        64\t|\tloss: 1556.26\n",
      "Training Epoch 61  10.9% | batch:         7 of        64\t|\tloss: 1189.15\n",
      "Training Epoch 61  12.5% | batch:         8 of        64\t|\tloss: 2118.01\n",
      "Training Epoch 61  14.1% | batch:         9 of        64\t|\tloss: 709.388\n",
      "Training Epoch 61  15.6% | batch:        10 of        64\t|\tloss: 1975.88\n",
      "Training Epoch 61  17.2% | batch:        11 of        64\t|\tloss: 740.219\n",
      "Training Epoch 61  18.8% | batch:        12 of        64\t|\tloss: 1049.2\n",
      "Training Epoch 61  20.3% | batch:        13 of        64\t|\tloss: 1051.22\n",
      "Training Epoch 61  21.9% | batch:        14 of        64\t|\tloss: 894.317\n",
      "Training Epoch 61  23.4% | batch:        15 of        64\t|\tloss: 1368.53\n",
      "Training Epoch 61  25.0% | batch:        16 of        64\t|\tloss: 1199.81\n",
      "Training Epoch 61  26.6% | batch:        17 of        64\t|\tloss: 706.685\n",
      "Training Epoch 61  28.1% | batch:        18 of        64\t|\tloss: 898.618\n",
      "Training Epoch 61  29.7% | batch:        19 of        64\t|\tloss: 963.041\n",
      "Training Epoch 61  31.2% | batch:        20 of        64\t|\tloss: 801.611\n",
      "Training Epoch 61  32.8% | batch:        21 of        64\t|\tloss: 1207.53\n",
      "Training Epoch 61  34.4% | batch:        22 of        64\t|\tloss: 1258.44\n",
      "Training Epoch 61  35.9% | batch:        23 of        64\t|\tloss: 2058.13\n",
      "Training Epoch 61  37.5% | batch:        24 of        64\t|\tloss: 1446.08\n",
      "Training Epoch 61  39.1% | batch:        25 of        64\t|\tloss: 2591.74\n",
      "Training Epoch 61  40.6% | batch:        26 of        64\t|\tloss: 1045.46\n",
      "Training Epoch 61  42.2% | batch:        27 of        64\t|\tloss: 1820.76\n",
      "Training Epoch 61  43.8% | batch:        28 of        64\t|\tloss: 783.027\n",
      "Training Epoch 61  45.3% | batch:        29 of        64\t|\tloss: 1111.18\n",
      "Training Epoch 61  46.9% | batch:        30 of        64\t|\tloss: 1338.13\n",
      "Training Epoch 61  48.4% | batch:        31 of        64\t|\tloss: 1557.94\n",
      "Training Epoch 61  50.0% | batch:        32 of        64\t|\tloss: 1339.06\n",
      "Training Epoch 61  51.6% | batch:        33 of        64\t|\tloss: 958.236\n",
      "Training Epoch 61  53.1% | batch:        34 of        64\t|\tloss: 1353.19\n",
      "Training Epoch 61  54.7% | batch:        35 of        64\t|\tloss: 1348.31\n",
      "Training Epoch 61  56.2% | batch:        36 of        64\t|\tloss: 2043.84\n",
      "Training Epoch 61  57.8% | batch:        37 of        64\t|\tloss: 1459.38\n",
      "Training Epoch 61  59.4% | batch:        38 of        64\t|\tloss: 4094.6\n",
      "Training Epoch 61  60.9% | batch:        39 of        64\t|\tloss: 1185.97\n",
      "Training Epoch 61  62.5% | batch:        40 of        64\t|\tloss: 1061.06\n",
      "Training Epoch 61  64.1% | batch:        41 of        64\t|\tloss: 1303.4\n",
      "Training Epoch 61  65.6% | batch:        42 of        64\t|\tloss: 725.748\n",
      "Training Epoch 61  67.2% | batch:        43 of        64\t|\tloss: 1919.27\n",
      "Training Epoch 61  68.8% | batch:        44 of        64\t|\tloss: 1231.35\n",
      "Training Epoch 61  70.3% | batch:        45 of        64\t|\tloss: 1148.44\n",
      "Training Epoch 61  71.9% | batch:        46 of        64\t|\tloss: 719.015\n",
      "Training Epoch 61  73.4% | batch:        47 of        64\t|\tloss: 867.179\n",
      "Training Epoch 61  75.0% | batch:        48 of        64\t|\tloss: 2107.13\n",
      "Training Epoch 61  76.6% | batch:        49 of        64\t|\tloss: 1425.5\n",
      "Training Epoch 61  78.1% | batch:        50 of        64\t|\tloss: 1299.45\n",
      "Training Epoch 61  79.7% | batch:        51 of        64\t|\tloss: 1297.32\n",
      "Training Epoch 61  81.2% | batch:        52 of        64\t|\tloss: 2024.16\n",
      "Training Epoch 61  82.8% | batch:        53 of        64\t|\tloss: 2529.55\n",
      "Training Epoch 61  84.4% | batch:        54 of        64\t|\tloss: 752.506\n",
      "Training Epoch 61  85.9% | batch:        55 of        64\t|\tloss: 846.814\n",
      "Training Epoch 61  87.5% | batch:        56 of        64\t|\tloss: 917.823\n",
      "Training Epoch 61  89.1% | batch:        57 of        64\t|\tloss: 1880\n",
      "Training Epoch 61  90.6% | batch:        58 of        64\t|\tloss: 971.617\n",
      "Training Epoch 61  92.2% | batch:        59 of        64\t|\tloss: 1128.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:13,612 | INFO : Epoch 61 Training Summary: epoch: 61.000000 | loss: 1374.593930 | \n",
      "2023-05-10 17:09:13,613 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1831505298614502 seconds\n",
      "\n",
      "2023-05-10 17:09:13,614 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2004191484607634 seconds\n",
      "2023-05-10 17:09:13,614 | INFO : Avg batch train. time: 0.018756549194699428 seconds\n",
      "2023-05-10 17:09:13,615 | INFO : Avg sample train. time: 0.00029728062121366105 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 61  93.8% | batch:        60 of        64\t|\tloss: 1796.15\n",
      "Training Epoch 61  95.3% | batch:        61 of        64\t|\tloss: 2264.98\n",
      "Training Epoch 61  96.9% | batch:        62 of        64\t|\tloss: 1014.86\n",
      "Training Epoch 61  98.4% | batch:        63 of        64\t|\tloss: 4966.55\n",
      "\n",
      "Training Epoch 62   0.0% | batch:         0 of        64\t|\tloss: 1125.12\n",
      "Training Epoch 62   1.6% | batch:         1 of        64\t|\tloss: 1158.37\n",
      "Training Epoch 62   3.1% | batch:         2 of        64\t|\tloss: 1134.72\n",
      "Training Epoch 62   4.7% | batch:         3 of        64\t|\tloss: 855.097\n",
      "Training Epoch 62   6.2% | batch:         4 of        64\t|\tloss: 8115.56\n",
      "Training Epoch 62   7.8% | batch:         5 of        64\t|\tloss: 1225.53\n",
      "Training Epoch 62   9.4% | batch:         6 of        64\t|\tloss: 862.541\n",
      "Training Epoch 62  10.9% | batch:         7 of        64\t|\tloss: 2694.41\n",
      "Training Epoch 62  12.5% | batch:         8 of        64\t|\tloss: 1704.95\n",
      "Training Epoch 62  14.1% | batch:         9 of        64\t|\tloss: 1917.26\n",
      "Training Epoch 62  15.6% | batch:        10 of        64\t|\tloss: 711.121\n",
      "Training Epoch 62  17.2% | batch:        11 of        64\t|\tloss: 1232.58\n",
      "Training Epoch 62  18.8% | batch:        12 of        64\t|\tloss: 1327.96\n",
      "Training Epoch 62  20.3% | batch:        13 of        64\t|\tloss: 1937.47\n",
      "Training Epoch 62  21.9% | batch:        14 of        64\t|\tloss: 4011.98\n",
      "Training Epoch 62  23.4% | batch:        15 of        64\t|\tloss: 717.387\n",
      "Training Epoch 62  25.0% | batch:        16 of        64\t|\tloss: 1199.03\n",
      "Training Epoch 62  26.6% | batch:        17 of        64\t|\tloss: 990.743\n",
      "Training Epoch 62  28.1% | batch:        18 of        64\t|\tloss: 1132.39\n",
      "Training Epoch 62  29.7% | batch:        19 of        64\t|\tloss: 1098.62\n",
      "Training Epoch 62  31.2% | batch:        20 of        64\t|\tloss: 1109.75\n",
      "Training Epoch 62  32.8% | batch:        21 of        64\t|\tloss: 1000.7\n",
      "Training Epoch 62  34.4% | batch:        22 of        64\t|\tloss: 936.202\n",
      "Training Epoch 62  35.9% | batch:        23 of        64\t|\tloss: 980.546\n",
      "Training Epoch 62  37.5% | batch:        24 of        64\t|\tloss: 891.419\n",
      "Training Epoch 62  39.1% | batch:        25 of        64\t|\tloss: 1783.27\n",
      "Training Epoch 62  40.6% | batch:        26 of        64\t|\tloss: 898.601\n",
      "Training Epoch 62  42.2% | batch:        27 of        64\t|\tloss: 1050.52\n",
      "Training Epoch 62  43.8% | batch:        28 of        64\t|\tloss: 1138.87\n",
      "Training Epoch 62  45.3% | batch:        29 of        64\t|\tloss: 1781.55\n",
      "Training Epoch 62  46.9% | batch:        30 of        64\t|\tloss: 1088.79\n",
      "Training Epoch 62  48.4% | batch:        31 of        64\t|\tloss: 1886.69\n",
      "Training Epoch 62  50.0% | batch:        32 of        64\t|\tloss: 2434.83\n",
      "Training Epoch 62  51.6% | batch:        33 of        64\t|\tloss: 1317.89\n",
      "Training Epoch 62  53.1% | batch:        34 of        64\t|\tloss: 955.771\n",
      "Training Epoch 62  54.7% | batch:        35 of        64\t|\tloss: 909.047\n",
      "Training Epoch 62  56.2% | batch:        36 of        64\t|\tloss: 2855.49\n",
      "Training Epoch 62  57.8% | batch:        37 of        64\t|\tloss: 1237.28\n",
      "Training Epoch 62  59.4% | batch:        38 of        64\t|\tloss: 805.987\n",
      "Training Epoch 62  60.9% | batch:        39 of        64\t|\tloss: 888.646\n",
      "Training Epoch 62  62.5% | batch:        40 of        64\t|\tloss: 532.766\n",
      "Training Epoch 62  64.1% | batch:        41 of        64\t|\tloss: 1438.1\n",
      "Training Epoch 62  65.6% | batch:        42 of        64\t|\tloss: 825.325\n",
      "Training Epoch 62  67.2% | batch:        43 of        64\t|\tloss: 1630.46\n",
      "Training Epoch 62  68.8% | batch:        44 of        64\t|\tloss: 1237.94\n",
      "Training Epoch 62  70.3% | batch:        45 of        64\t|\tloss: 1432.98\n",
      "Training Epoch 62  71.9% | batch:        46 of        64\t|\tloss: 868.507\n",
      "Training Epoch 62  73.4% | batch:        47 of        64\t|\tloss: 531.658\n",
      "Training Epoch 62  75.0% | batch:        48 of        64\t|\tloss: 1182.74\n",
      "Training Epoch 62  76.6% | batch:        49 of        64\t|\tloss: 603.766\n",
      "Training Epoch 62  78.1% | batch:        50 of        64\t|\tloss: 2197.15\n",
      "Training Epoch 62  79.7% | batch:        51 of        64\t|\tloss: 1081.98\n",
      "Training Epoch 62  81.2% | batch:        52 of        64\t|\tloss: 2155.49\n",
      "Training Epoch 62  82.8% | batch:        53 of        64\t|\tloss: 1098.81\n",
      "Training Epoch 62  84.4% | batch:        54 of        64\t|\tloss: 1680.11\n",
      "Training Epoch 62  85.9% | batch:        55 of        64\t|\tloss: 1056.55\n",
      "Training Epoch 62  87.5% | batch:        56 of        64\t|\tloss: 728.11\n",
      "Training Epoch 62  89.1% | batch:        57 of        64\t|\tloss: 1062.02\n",
      "Training Epoch 62  90.6% | batch:        58 of        64\t|\tloss: 1419.63\n",
      "Training Epoch 62  92.2% | batch:        59 of        64\t|\tloss: 1531.14\n",
      "Training Epoch 62  93.8% | batch:        60 of        64\t|\tloss: 1168.35\n",
      "Training Epoch 62  95.3% | batch:        61 of        64\t|\tloss: 1184.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:14,811 | INFO : Epoch 62 Training Summary: epoch: 62.000000 | loss: 1415.013062 | \n",
      "2023-05-10 17:09:14,812 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1830589771270752 seconds\n",
      "\n",
      "2023-05-10 17:09:14,812 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2001391456973167 seconds\n",
      "2023-05-10 17:09:14,813 | INFO : Avg batch train. time: 0.018752174151520574 seconds\n",
      "2023-05-10 17:09:14,813 | INFO : Avg sample train. time: 0.0002972112792712523 seconds\n",
      "2023-05-10 17:09:14,814 | INFO : Evaluating on validation set ...\n",
      "2023-05-10 17:09:14,961 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.1473393440246582 seconds\n",
      "\n",
      "2023-05-10 17:09:14,962 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.17280699267531885 seconds\n",
      "2023-05-10 17:09:14,962 | INFO : Avg batch val. time: 0.010800437042207428 seconds\n",
      "2023-05-10 17:09:14,962 | INFO : Avg sample val. time: 0.00017109603235180085 seconds\n",
      "2023-05-10 17:09:14,963 | INFO : Epoch 62 Validation Summary: epoch: 62.000000 | loss: 1717.109843 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 62  96.9% | batch:        62 of        64\t|\tloss: 1145.17\n",
      "Training Epoch 62  98.4% | batch:        63 of        64\t|\tloss: 4057.1\n",
      "\n",
      "Evaluating Epoch 62   0.0% | batch:         0 of        16\t|\tloss: 1760.75\n",
      "Evaluating Epoch 62   6.2% | batch:         1 of        16\t|\tloss: 1005.48\n",
      "Evaluating Epoch 62  12.5% | batch:         2 of        16\t|\tloss: 941.299\n",
      "Evaluating Epoch 62  18.8% | batch:         3 of        16\t|\tloss: 1608.21\n",
      "Evaluating Epoch 62  25.0% | batch:         4 of        16\t|\tloss: 1474.19\n",
      "Evaluating Epoch 62  31.2% | batch:         5 of        16\t|\tloss: 1735.51\n",
      "Evaluating Epoch 62  37.5% | batch:         6 of        16\t|\tloss: 1789.72\n",
      "Evaluating Epoch 62  43.8% | batch:         7 of        16\t|\tloss: 2076.53\n",
      "Evaluating Epoch 62  50.0% | batch:         8 of        16\t|\tloss: 1293.02\n",
      "Evaluating Epoch 62  56.2% | batch:         9 of        16\t|\tloss: 4986.34\n",
      "Evaluating Epoch 62  62.5% | batch:        10 of        16\t|\tloss: 1527.68\n",
      "Evaluating Epoch 62  68.8% | batch:        11 of        16\t|\tloss: 1934.4\n",
      "Evaluating Epoch 62  75.0% | batch:        12 of        16\t|\tloss: 1117.58\n",
      "Evaluating Epoch 62  81.2% | batch:        13 of        16\t|\tloss: 1136.67\n",
      "Evaluating Epoch 62  87.5% | batch:        14 of        16\t|\tloss: 1564.69\n",
      "Evaluating Epoch 62  93.8% | batch:        15 of        16\t|\tloss: 1466.97\n",
      "\n",
      "Training Epoch 63   0.0% | batch:         0 of        64\t|\tloss: 856.487\n",
      "Training Epoch 63   1.6% | batch:         1 of        64\t|\tloss: 2134.02\n",
      "Training Epoch 63   3.1% | batch:         2 of        64\t|\tloss: 1591.3\n",
      "Training Epoch 63   4.7% | batch:         3 of        64\t|\tloss: 2741.14\n",
      "Training Epoch 63   6.2% | batch:         4 of        64\t|\tloss: 1522.87\n",
      "Training Epoch 63   7.8% | batch:         5 of        64\t|\tloss: 1100.78\n",
      "Training Epoch 63   9.4% | batch:         6 of        64\t|\tloss: 815.416\n",
      "Training Epoch 63  10.9% | batch:         7 of        64\t|\tloss: 1121.66\n",
      "Training Epoch 63  12.5% | batch:         8 of        64\t|\tloss: 1653.51\n",
      "Training Epoch 63  14.1% | batch:         9 of        64\t|\tloss: 1063.31\n",
      "Training Epoch 63  15.6% | batch:        10 of        64\t|\tloss: 828.663\n",
      "Training Epoch 63  17.2% | batch:        11 of        64\t|\tloss: 1058.02\n",
      "Training Epoch 63  18.8% | batch:        12 of        64\t|\tloss: 769.493\n",
      "Training Epoch 63  20.3% | batch:        13 of        64\t|\tloss: 2214.2\n",
      "Training Epoch 63  21.9% | batch:        14 of        64\t|\tloss: 781.248\n",
      "Training Epoch 63  23.4% | batch:        15 of        64\t|\tloss: 897.859\n",
      "Training Epoch 63  25.0% | batch:        16 of        64\t|\tloss: 679.073\n",
      "Training Epoch 63  26.6% | batch:        17 of        64\t|\tloss: 1154.16\n",
      "Training Epoch 63  28.1% | batch:        18 of        64\t|\tloss: 775.725\n",
      "Training Epoch 63  29.7% | batch:        19 of        64\t|\tloss: 1169.8\n",
      "Training Epoch 63  31.2% | batch:        20 of        64\t|\tloss: 861.813\n",
      "Training Epoch 63  32.8% | batch:        21 of        64\t|\tloss: 1092.64\n",
      "Training Epoch 63  34.4% | batch:        22 of        64\t|\tloss: 795.739\n",
      "Training Epoch 63  35.9% | batch:        23 of        64\t|\tloss: 1090.99\n",
      "Training Epoch 63  37.5% | batch:        24 of        64\t|\tloss: 763.298\n",
      "Training Epoch 63  39.1% | batch:        25 of        64\t|\tloss: 1390.93\n",
      "Training Epoch 63  40.6% | batch:        26 of        64\t|\tloss: 1016.86\n",
      "Training Epoch 63  42.2% | batch:        27 of        64\t|\tloss: 1530.44\n",
      "Training Epoch 63  43.8% | batch:        28 of        64\t|\tloss: 1706.81\n",
      "Training Epoch 63  45.3% | batch:        29 of        64\t|\tloss: 945.184\n",
      "Training Epoch 63  46.9% | batch:        30 of        64\t|\tloss: 1237.73\n",
      "Training Epoch 63  48.4% | batch:        31 of        64\t|\tloss: 1611.52\n",
      "Training Epoch 63  50.0% | batch:        32 of        64\t|\tloss: 1399.24\n",
      "Training Epoch 63  51.6% | batch:        33 of        64\t|\tloss: 1179.49\n",
      "Training Epoch 63  53.1% | batch:        34 of        64\t|\tloss: 1071.44\n",
      "Training Epoch 63  54.7% | batch:        35 of        64\t|\tloss: 1380.84\n",
      "Training Epoch 63  56.2% | batch:        36 of        64\t|\tloss: 745.827\n",
      "Training Epoch 63  57.8% | batch:        37 of        64\t|\tloss: 2593.06\n",
      "Training Epoch 63  59.4% | batch:        38 of        64\t|\tloss: 1579.75\n",
      "Training Epoch 63  60.9% | batch:        39 of        64\t|\tloss: 1556.95\n",
      "Training Epoch 63  62.5% | batch:        40 of        64\t|\tloss: 1047.14\n",
      "Training Epoch 63  64.1% | batch:        41 of        64\t|\tloss: 688.146\n",
      "Training Epoch 63  65.6% | batch:        42 of        64\t|\tloss: 1000.66\n",
      "Training Epoch 63  67.2% | batch:        43 of        64\t|\tloss: 1078.33\n",
      "Training Epoch 63  68.8% | batch:        44 of        64\t|\tloss: 1835.04\n",
      "Training Epoch 63  70.3% | batch:        45 of        64\t|\tloss: 1625.28\n",
      "Training Epoch 63  71.9% | batch:        46 of        64\t|\tloss: 706.698\n",
      "Training Epoch 63  73.4% | batch:        47 of        64\t|\tloss: 1410.52\n",
      "Training Epoch 63  75.0% | batch:        48 of        64\t|\tloss: 1014.75\n",
      "Training Epoch 63  76.6% | batch:        49 of        64\t|\tloss: 820.004\n",
      "Training Epoch 63  78.1% | batch:        50 of        64\t|\tloss: 1445.29\n",
      "Training Epoch 63  79.7% | batch:        51 of        64\t|\tloss: 2068\n",
      "Training Epoch 63  81.2% | batch:        52 of        64\t|\tloss: 849.821\n",
      "Training Epoch 63  82.8% | batch:        53 of        64\t|\tloss: 2335.24\n",
      "Training Epoch 63  84.4% | batch:        54 of        64\t|\tloss: 1035.18\n",
      "Training Epoch 63  85.9% | batch:        55 of        64\t|\tloss: 1412.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:16,162 | INFO : Epoch 63 Training Summary: epoch: 63.000000 | loss: 1306.846369 | \n",
      "2023-05-10 17:09:16,163 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1905505657196045 seconds\n",
      "\n",
      "2023-05-10 17:09:16,164 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.199986946015131 seconds\n",
      "2023-05-10 17:09:16,164 | INFO : Avg batch train. time: 0.01874979603148642 seconds\n",
      "2023-05-10 17:09:16,164 | INFO : Avg sample train. time: 0.00029717358742326175 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 63  87.5% | batch:        56 of        64\t|\tloss: 1794.03\n",
      "Training Epoch 63  89.1% | batch:        57 of        64\t|\tloss: 1313.88\n",
      "Training Epoch 63  90.6% | batch:        58 of        64\t|\tloss: 1316.14\n",
      "Training Epoch 63  92.2% | batch:        59 of        64\t|\tloss: 1808.6\n",
      "Training Epoch 63  93.8% | batch:        60 of        64\t|\tloss: 1612.6\n",
      "Training Epoch 63  95.3% | batch:        61 of        64\t|\tloss: 2410.37\n",
      "Training Epoch 63  96.9% | batch:        62 of        64\t|\tloss: 1245.23\n",
      "Training Epoch 63  98.4% | batch:        63 of        64\t|\tloss: 1077.17\n",
      "\n",
      "Training Epoch 64   0.0% | batch:         0 of        64\t|\tloss: 1279.38\n",
      "Training Epoch 64   1.6% | batch:         1 of        64\t|\tloss: 1424.67\n",
      "Training Epoch 64   3.1% | batch:         2 of        64\t|\tloss: 1414.07\n",
      "Training Epoch 64   4.7% | batch:         3 of        64\t|\tloss: 937.573\n",
      "Training Epoch 64   6.2% | batch:         4 of        64\t|\tloss: 588.748\n",
      "Training Epoch 64   7.8% | batch:         5 of        64\t|\tloss: 1315.89\n",
      "Training Epoch 64   9.4% | batch:         6 of        64\t|\tloss: 1181.64\n",
      "Training Epoch 64  10.9% | batch:         7 of        64\t|\tloss: 681.177\n",
      "Training Epoch 64  12.5% | batch:         8 of        64\t|\tloss: 715.674\n",
      "Training Epoch 64  14.1% | batch:         9 of        64\t|\tloss: 826.234\n",
      "Training Epoch 64  15.6% | batch:        10 of        64\t|\tloss: 973.911\n",
      "Training Epoch 64  17.2% | batch:        11 of        64\t|\tloss: 1569.64\n",
      "Training Epoch 64  18.8% | batch:        12 of        64\t|\tloss: 1035.78\n",
      "Training Epoch 64  20.3% | batch:        13 of        64\t|\tloss: 1550.58\n",
      "Training Epoch 64  21.9% | batch:        14 of        64\t|\tloss: 1371.43\n",
      "Training Epoch 64  23.4% | batch:        15 of        64\t|\tloss: 1327.2\n",
      "Training Epoch 64  25.0% | batch:        16 of        64\t|\tloss: 1201.26\n",
      "Training Epoch 64  26.6% | batch:        17 of        64\t|\tloss: 1159.99\n",
      "Training Epoch 64  28.1% | batch:        18 of        64\t|\tloss: 1004.15\n",
      "Training Epoch 64  29.7% | batch:        19 of        64\t|\tloss: 926.813\n",
      "Training Epoch 64  31.2% | batch:        20 of        64\t|\tloss: 1133.2\n",
      "Training Epoch 64  32.8% | batch:        21 of        64\t|\tloss: 5407.36\n",
      "Training Epoch 64  34.4% | batch:        22 of        64\t|\tloss: 1022.95\n",
      "Training Epoch 64  35.9% | batch:        23 of        64\t|\tloss: 803.285\n",
      "Training Epoch 64  37.5% | batch:        24 of        64\t|\tloss: 1432.54\n",
      "Training Epoch 64  39.1% | batch:        25 of        64\t|\tloss: 1135.1\n",
      "Training Epoch 64  40.6% | batch:        26 of        64\t|\tloss: 825.891\n",
      "Training Epoch 64  42.2% | batch:        27 of        64\t|\tloss: 1469.99\n",
      "Training Epoch 64  43.8% | batch:        28 of        64\t|\tloss: 1025.12\n",
      "Training Epoch 64  45.3% | batch:        29 of        64\t|\tloss: 1720.51\n",
      "Training Epoch 64  46.9% | batch:        30 of        64\t|\tloss: 1526.29\n",
      "Training Epoch 64  48.4% | batch:        31 of        64\t|\tloss: 2633.62\n",
      "Training Epoch 64  50.0% | batch:        32 of        64\t|\tloss: 2438.27\n",
      "Training Epoch 64  51.6% | batch:        33 of        64\t|\tloss: 1811.3\n",
      "Training Epoch 64  53.1% | batch:        34 of        64\t|\tloss: 1097.46\n",
      "Training Epoch 64  54.7% | batch:        35 of        64\t|\tloss: 799.431\n",
      "Training Epoch 64  56.2% | batch:        36 of        64\t|\tloss: 1048.81\n",
      "Training Epoch 64  57.8% | batch:        37 of        64\t|\tloss: 1909.15\n",
      "Training Epoch 64  59.4% | batch:        38 of        64\t|\tloss: 1007.3\n",
      "Training Epoch 64  60.9% | batch:        39 of        64\t|\tloss: 1118.79\n",
      "Training Epoch 64  62.5% | batch:        40 of        64\t|\tloss: 630.206\n",
      "Training Epoch 64  64.1% | batch:        41 of        64\t|\tloss: 1472.25\n",
      "Training Epoch 64  65.6% | batch:        42 of        64\t|\tloss: 937.277\n",
      "Training Epoch 64  67.2% | batch:        43 of        64\t|\tloss: 1331.66\n",
      "Training Epoch 64  68.8% | batch:        44 of        64\t|\tloss: 1098.37\n",
      "Training Epoch 64  70.3% | batch:        45 of        64\t|\tloss: 827.202\n",
      "Training Epoch 64  71.9% | batch:        46 of        64\t|\tloss: 1201\n",
      "Training Epoch 64  73.4% | batch:        47 of        64\t|\tloss: 1984.96\n",
      "Training Epoch 64  75.0% | batch:        48 of        64\t|\tloss: 2197.41\n",
      "Training Epoch 64  76.6% | batch:        49 of        64\t|\tloss: 622.935\n",
      "Training Epoch 64  78.1% | batch:        50 of        64\t|\tloss: 675.046\n",
      "Training Epoch 64  79.7% | batch:        51 of        64\t|\tloss: 2278.4\n",
      "Training Epoch 64  81.2% | batch:        52 of        64\t|\tloss: 1426.29\n",
      "Training Epoch 64  82.8% | batch:        53 of        64\t|\tloss: 1256.07\n",
      "Training Epoch 64  84.4% | batch:        54 of        64\t|\tloss: 1228.05\n",
      "Training Epoch 64  85.9% | batch:        55 of        64\t|\tloss: 634.145\n",
      "Training Epoch 64  87.5% | batch:        56 of        64\t|\tloss: 1123.78\n",
      "Training Epoch 64  89.1% | batch:        57 of        64\t|\tloss: 1162.43\n",
      "Training Epoch 64  90.6% | batch:        58 of        64\t|\tloss: 3645.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:17,405 | INFO : Epoch 64 Training Summary: epoch: 64.000000 | loss: 1346.913361 | \n",
      "2023-05-10 17:09:17,406 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2312250137329102 seconds\n",
      "\n",
      "2023-05-10 17:09:17,406 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2004750408232212 seconds\n",
      "2023-05-10 17:09:17,407 | INFO : Avg batch train. time: 0.01875742251286283 seconds\n",
      "2023-05-10 17:09:17,407 | INFO : Avg sample train. time: 0.0002972944628091187 seconds\n",
      "2023-05-10 17:09:17,408 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 64  92.2% | batch:        59 of        64\t|\tloss: 1755.91\n",
      "Training Epoch 64  93.8% | batch:        60 of        64\t|\tloss: 1402.59\n",
      "Training Epoch 64  95.3% | batch:        61 of        64\t|\tloss: 1104.08\n",
      "Training Epoch 64  96.9% | batch:        62 of        64\t|\tloss: 852.653\n",
      "Training Epoch 64  98.4% | batch:        63 of        64\t|\tloss: 3005.48\n",
      "\n",
      "Evaluating Epoch 64   0.0% | batch:         0 of        16\t|\tloss: 2159.72\n",
      "Evaluating Epoch 64   6.2% | batch:         1 of        16\t|\tloss: 2072.46\n",
      "Evaluating Epoch 64  12.5% | batch:         2 of        16\t|\tloss: 1053.65\n",
      "Evaluating Epoch 64  18.8% | batch:         3 of        16\t|\tloss: 2391.8\n",
      "Evaluating Epoch 64  25.0% | batch:         4 of        16\t|\tloss: 2168.7\n",
      "Evaluating Epoch 64  31.2% | batch:         5 of        16\t|\tloss: 2108.17\n",
      "Evaluating Epoch 64  37.5% | batch:         6 of        16\t|\tloss: 2417.04\n",
      "Evaluating Epoch 64  43.8% | batch:         7 of        16\t|\tloss: 2207.72\n",
      "Evaluating Epoch 64  50.0% | batch:         8 of        16\t|\tloss: 2148.41\n",
      "Evaluating Epoch 64  56.2% | batch:         9 of        16\t|\tloss: 4825.24\n",
      "Evaluating Epoch 64  62.5% | batch:        10 of        16\t|\tloss: 2028.71\n",
      "Evaluating Epoch 64  68.8% | batch:        11 of        16\t|\tloss: 3785.84\n",
      "Evaluating Epoch 64  75.0% | batch:        12 of        16\t|\tloss: 1839.08\n",
      "Evaluating Epoch 64  81.2% | batch:        13 of        16\t|\tloss: 1249.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:17,553 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.1448662281036377 seconds\n",
      "\n",
      "2023-05-10 17:09:17,554 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.1719852054820341 seconds\n",
      "2023-05-10 17:09:17,554 | INFO : Avg batch val. time: 0.010749075342627132 seconds\n",
      "2023-05-10 17:09:17,555 | INFO : Avg sample val. time: 0.0001702823816653803 seconds\n",
      "2023-05-10 17:09:17,555 | INFO : Epoch 64 Validation Summary: epoch: 64.000000 | loss: 2353.630368 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 64  87.5% | batch:        14 of        16\t|\tloss: 2123.14\n",
      "Evaluating Epoch 64  93.8% | batch:        15 of        16\t|\tloss: 3282.44\n",
      "\n",
      "Training Epoch 65   0.0% | batch:         0 of        64\t|\tloss: 1760.11\n",
      "Training Epoch 65   1.6% | batch:         1 of        64\t|\tloss: 576.495\n",
      "Training Epoch 65   3.1% | batch:         2 of        64\t|\tloss: 1925.58\n",
      "Training Epoch 65   4.7% | batch:         3 of        64\t|\tloss: 1651.24\n",
      "Training Epoch 65   6.2% | batch:         4 of        64\t|\tloss: 1711.9\n",
      "Training Epoch 65   7.8% | batch:         5 of        64\t|\tloss: 1599.59\n",
      "Training Epoch 65   9.4% | batch:         6 of        64\t|\tloss: 1775.06\n",
      "Training Epoch 65  10.9% | batch:         7 of        64\t|\tloss: 817.922\n",
      "Training Epoch 65  12.5% | batch:         8 of        64\t|\tloss: 1011.82\n",
      "Training Epoch 65  14.1% | batch:         9 of        64\t|\tloss: 1957.47\n",
      "Training Epoch 65  15.6% | batch:        10 of        64\t|\tloss: 1117.56\n",
      "Training Epoch 65  17.2% | batch:        11 of        64\t|\tloss: 1158.28\n",
      "Training Epoch 65  18.8% | batch:        12 of        64\t|\tloss: 764.51\n",
      "Training Epoch 65  20.3% | batch:        13 of        64\t|\tloss: 1305.42\n",
      "Training Epoch 65  21.9% | batch:        14 of        64\t|\tloss: 1608.54\n",
      "Training Epoch 65  23.4% | batch:        15 of        64\t|\tloss: 1262.6\n",
      "Training Epoch 65  25.0% | batch:        16 of        64\t|\tloss: 9215.12\n",
      "Training Epoch 65  26.6% | batch:        17 of        64\t|\tloss: 649.627\n",
      "Training Epoch 65  28.1% | batch:        18 of        64\t|\tloss: 1590.6\n",
      "Training Epoch 65  29.7% | batch:        19 of        64\t|\tloss: 2238.64\n",
      "Training Epoch 65  31.2% | batch:        20 of        64\t|\tloss: 893.086\n",
      "Training Epoch 65  32.8% | batch:        21 of        64\t|\tloss: 1153.6\n",
      "Training Epoch 65  34.4% | batch:        22 of        64\t|\tloss: 954.198\n",
      "Training Epoch 65  35.9% | batch:        23 of        64\t|\tloss: 845.169\n",
      "Training Epoch 65  37.5% | batch:        24 of        64\t|\tloss: 1078.27\n",
      "Training Epoch 65  39.1% | batch:        25 of        64\t|\tloss: 1456.29\n",
      "Training Epoch 65  40.6% | batch:        26 of        64\t|\tloss: 1175.12\n",
      "Training Epoch 65  42.2% | batch:        27 of        64\t|\tloss: 1660.78\n",
      "Training Epoch 65  43.8% | batch:        28 of        64\t|\tloss: 1014.27\n",
      "Training Epoch 65  45.3% | batch:        29 of        64\t|\tloss: 1100.79\n",
      "Training Epoch 65  46.9% | batch:        30 of        64\t|\tloss: 1082.85\n",
      "Training Epoch 65  48.4% | batch:        31 of        64\t|\tloss: 1202.48\n",
      "Training Epoch 65  50.0% | batch:        32 of        64\t|\tloss: 1061.07\n",
      "Training Epoch 65  51.6% | batch:        33 of        64\t|\tloss: 980.501\n",
      "Training Epoch 65  53.1% | batch:        34 of        64\t|\tloss: 2173.56\n",
      "Training Epoch 65  54.7% | batch:        35 of        64\t|\tloss: 3470.98\n",
      "Training Epoch 65  56.2% | batch:        36 of        64\t|\tloss: 875.946\n",
      "Training Epoch 65  57.8% | batch:        37 of        64\t|\tloss: 1181.87\n",
      "Training Epoch 65  59.4% | batch:        38 of        64\t|\tloss: 1227.29\n",
      "Training Epoch 65  60.9% | batch:        39 of        64\t|\tloss: 856.925\n",
      "Training Epoch 65  62.5% | batch:        40 of        64\t|\tloss: 750.104\n",
      "Training Epoch 65  64.1% | batch:        41 of        64\t|\tloss: 1171.52\n",
      "Training Epoch 65  65.6% | batch:        42 of        64\t|\tloss: 1105.67\n",
      "Training Epoch 65  67.2% | batch:        43 of        64\t|\tloss: 729.78\n",
      "Training Epoch 65  68.8% | batch:        44 of        64\t|\tloss: 1587.6\n",
      "Training Epoch 65  70.3% | batch:        45 of        64\t|\tloss: 959.898\n",
      "Training Epoch 65  71.9% | batch:        46 of        64\t|\tloss: 1264.68\n",
      "Training Epoch 65  73.4% | batch:        47 of        64\t|\tloss: 1165\n",
      "Training Epoch 65  75.0% | batch:        48 of        64\t|\tloss: 771.044\n",
      "Training Epoch 65  76.6% | batch:        49 of        64\t|\tloss: 1545.03\n",
      "Training Epoch 65  78.1% | batch:        50 of        64\t|\tloss: 1052.4\n",
      "Training Epoch 65  79.7% | batch:        51 of        64\t|\tloss: 995.185\n",
      "Training Epoch 65  81.2% | batch:        52 of        64\t|\tloss: 831.887\n",
      "Training Epoch 65  82.8% | batch:        53 of        64\t|\tloss: 1494.53\n",
      "Training Epoch 65  84.4% | batch:        54 of        64\t|\tloss: 857.039\n",
      "Training Epoch 65  85.9% | batch:        55 of        64\t|\tloss: 828.119\n",
      "Training Epoch 65  87.5% | batch:        56 of        64\t|\tloss: 1425.69\n",
      "Training Epoch 65  89.1% | batch:        57 of        64\t|\tloss: 965.629\n",
      "Training Epoch 65  90.6% | batch:        58 of        64\t|\tloss: 643.022\n",
      "Training Epoch 65  92.2% | batch:        59 of        64\t|\tloss: 542.242\n",
      "Training Epoch 65  93.8% | batch:        60 of        64\t|\tloss: 1072.49\n",
      "Training Epoch 65  95.3% | batch:        61 of        64\t|\tloss: 856.336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:18,842 | INFO : Epoch 65 Training Summary: epoch: 65.000000 | loss: 1356.112330 | \n",
      "2023-05-10 17:09:18,842 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2774474620819092 seconds\n",
      "\n",
      "2023-05-10 17:09:18,843 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2016592319195087 seconds\n",
      "2023-05-10 17:09:18,844 | INFO : Avg batch train. time: 0.018775925498742324 seconds\n",
      "2023-05-10 17:09:18,844 | INFO : Avg sample train. time: 0.00029758772459621314 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 65  96.9% | batch:        62 of        64\t|\tloss: 1296.02\n",
      "Training Epoch 65  98.4% | batch:        63 of        64\t|\tloss: 5420.92\n",
      "\n",
      "Training Epoch 66   0.0% | batch:         0 of        64\t|\tloss: 741.898\n",
      "Training Epoch 66   1.6% | batch:         1 of        64\t|\tloss: 1359.44\n",
      "Training Epoch 66   3.1% | batch:         2 of        64\t|\tloss: 991.34\n",
      "Training Epoch 66   4.7% | batch:         3 of        64\t|\tloss: 918.88\n",
      "Training Epoch 66   6.2% | batch:         4 of        64\t|\tloss: 1005.36\n",
      "Training Epoch 66   7.8% | batch:         5 of        64\t|\tloss: 1576.59\n",
      "Training Epoch 66   9.4% | batch:         6 of        64\t|\tloss: 1305.44\n",
      "Training Epoch 66  10.9% | batch:         7 of        64\t|\tloss: 1402.11\n",
      "Training Epoch 66  12.5% | batch:         8 of        64\t|\tloss: 3458.78\n",
      "Training Epoch 66  14.1% | batch:         9 of        64\t|\tloss: 2421.85\n",
      "Training Epoch 66  15.6% | batch:        10 of        64\t|\tloss: 683.461\n",
      "Training Epoch 66  17.2% | batch:        11 of        64\t|\tloss: 906.342\n",
      "Training Epoch 66  18.8% | batch:        12 of        64\t|\tloss: 691.749\n",
      "Training Epoch 66  20.3% | batch:        13 of        64\t|\tloss: 702.37\n",
      "Training Epoch 66  21.9% | batch:        14 of        64\t|\tloss: 886.051\n",
      "Training Epoch 66  23.4% | batch:        15 of        64\t|\tloss: 1566.54\n",
      "Training Epoch 66  25.0% | batch:        16 of        64\t|\tloss: 751.065\n",
      "Training Epoch 66  26.6% | batch:        17 of        64\t|\tloss: 1275.49\n",
      "Training Epoch 66  28.1% | batch:        18 of        64\t|\tloss: 1642.18\n",
      "Training Epoch 66  29.7% | batch:        19 of        64\t|\tloss: 727.557\n",
      "Training Epoch 66  31.2% | batch:        20 of        64\t|\tloss: 2180.54\n",
      "Training Epoch 66  32.8% | batch:        21 of        64\t|\tloss: 1240.78\n",
      "Training Epoch 66  34.4% | batch:        22 of        64\t|\tloss: 701.321\n",
      "Training Epoch 66  35.9% | batch:        23 of        64\t|\tloss: 990.396\n",
      "Training Epoch 66  37.5% | batch:        24 of        64\t|\tloss: 1024.44\n",
      "Training Epoch 66  39.1% | batch:        25 of        64\t|\tloss: 1537.61\n",
      "Training Epoch 66  40.6% | batch:        26 of        64\t|\tloss: 1221.39\n",
      "Training Epoch 66  42.2% | batch:        27 of        64\t|\tloss: 3158.22\n",
      "Training Epoch 66  43.8% | batch:        28 of        64\t|\tloss: 776.11\n",
      "Training Epoch 66  45.3% | batch:        29 of        64\t|\tloss: 713.266\n",
      "Training Epoch 66  46.9% | batch:        30 of        64\t|\tloss: 3101.81\n",
      "Training Epoch 66  48.4% | batch:        31 of        64\t|\tloss: 1587.15\n",
      "Training Epoch 66  50.0% | batch:        32 of        64\t|\tloss: 1925.02\n",
      "Training Epoch 66  51.6% | batch:        33 of        64\t|\tloss: 827.032\n",
      "Training Epoch 66  53.1% | batch:        34 of        64\t|\tloss: 765.469\n",
      "Training Epoch 66  54.7% | batch:        35 of        64\t|\tloss: 1367.93\n",
      "Training Epoch 66  56.2% | batch:        36 of        64\t|\tloss: 1189.92\n",
      "Training Epoch 66  57.8% | batch:        37 of        64\t|\tloss: 1179.12\n",
      "Training Epoch 66  59.4% | batch:        38 of        64\t|\tloss: 875.35\n",
      "Training Epoch 66  60.9% | batch:        39 of        64\t|\tloss: 1417.43\n",
      "Training Epoch 66  62.5% | batch:        40 of        64\t|\tloss: 1163.84\n",
      "Training Epoch 66  64.1% | batch:        41 of        64\t|\tloss: 855.614\n",
      "Training Epoch 66  65.6% | batch:        42 of        64\t|\tloss: 972.007\n",
      "Training Epoch 66  67.2% | batch:        43 of        64\t|\tloss: 1944.68\n",
      "Training Epoch 66  68.8% | batch:        44 of        64\t|\tloss: 660.785\n",
      "Training Epoch 66  70.3% | batch:        45 of        64\t|\tloss: 1045.56\n",
      "Training Epoch 66  71.9% | batch:        46 of        64\t|\tloss: 1239.25\n",
      "Training Epoch 66  73.4% | batch:        47 of        64\t|\tloss: 1049.98\n",
      "Training Epoch 66  75.0% | batch:        48 of        64\t|\tloss: 1686.38\n",
      "Training Epoch 66  76.6% | batch:        49 of        64\t|\tloss: 1663.04\n",
      "Training Epoch 66  78.1% | batch:        50 of        64\t|\tloss: 1051.72\n",
      "Training Epoch 66  79.7% | batch:        51 of        64\t|\tloss: 1806.97\n",
      "Training Epoch 66  81.2% | batch:        52 of        64\t|\tloss: 1449.13\n",
      "Training Epoch 66  82.8% | batch:        53 of        64\t|\tloss: 1434.68\n",
      "Training Epoch 66  84.4% | batch:        54 of        64\t|\tloss: 1324.03\n",
      "Training Epoch 66  85.9% | batch:        55 of        64\t|\tloss: 797.647\n",
      "Training Epoch 66  87.5% | batch:        56 of        64\t|\tloss: 1464.5\n",
      "Training Epoch 66  89.1% | batch:        57 of        64\t|\tloss: 682.354\n",
      "Training Epoch 66  90.6% | batch:        58 of        64\t|\tloss: 933.393\n",
      "Training Epoch 66  92.2% | batch:        59 of        64\t|\tloss: 1133.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:20,141 | INFO : Epoch 66 Training Summary: epoch: 66.000000 | loss: 1372.030227 | \n",
      "2023-05-10 17:09:20,142 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2879621982574463 seconds\n",
      "\n",
      "2023-05-10 17:09:20,142 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2029668526215986 seconds\n",
      "2023-05-10 17:09:20,143 | INFO : Avg batch train. time: 0.01879635707221248 seconds\n",
      "2023-05-10 17:09:20,144 | INFO : Avg sample train. time: 0.0002979115533981175 seconds\n",
      "2023-05-10 17:09:20,144 | INFO : Evaluating on validation set ...\n",
      "2023-05-10 17:09:20,291 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14676403999328613 seconds\n",
      "\n",
      "2023-05-10 17:09:20,292 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.1712646007537842 seconds\n",
      "2023-05-10 17:09:20,292 | INFO : Avg batch val. time: 0.010704037547111512 seconds\n",
      "2023-05-10 17:09:20,292 | INFO : Avg sample val. time: 0.00016956891163741008 seconds\n",
      "2023-05-10 17:09:20,293 | INFO : Epoch 66 Validation Summary: epoch: 66.000000 | loss: 2062.051702 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 66  93.8% | batch:        60 of        64\t|\tloss: 6470.54\n",
      "Training Epoch 66  95.3% | batch:        61 of        64\t|\tloss: 1405.18\n",
      "Training Epoch 66  96.9% | batch:        62 of        64\t|\tloss: 1414.1\n",
      "Training Epoch 66  98.4% | batch:        63 of        64\t|\tloss: 1308.57\n",
      "\n",
      "Evaluating Epoch 66   0.0% | batch:         0 of        16\t|\tloss: 1806.41\n",
      "Evaluating Epoch 66   6.2% | batch:         1 of        16\t|\tloss: 1411.21\n",
      "Evaluating Epoch 66  12.5% | batch:         2 of        16\t|\tloss: 964.026\n",
      "Evaluating Epoch 66  18.8% | batch:         3 of        16\t|\tloss: 1505.97\n",
      "Evaluating Epoch 66  25.0% | batch:         4 of        16\t|\tloss: 1556.22\n",
      "Evaluating Epoch 66  31.2% | batch:         5 of        16\t|\tloss: 1963.72\n",
      "Evaluating Epoch 66  37.5% | batch:         6 of        16\t|\tloss: 2385.82\n",
      "Evaluating Epoch 66  43.8% | batch:         7 of        16\t|\tloss: 2165.94\n",
      "Evaluating Epoch 66  50.0% | batch:         8 of        16\t|\tloss: 1398.29\n",
      "Evaluating Epoch 66  56.2% | batch:         9 of        16\t|\tloss: 6868.33\n",
      "Evaluating Epoch 66  62.5% | batch:        10 of        16\t|\tloss: 1887.8\n",
      "Evaluating Epoch 66  68.8% | batch:        11 of        16\t|\tloss: 2474.17\n",
      "Evaluating Epoch 66  75.0% | batch:        12 of        16\t|\tloss: 1032.03\n",
      "Evaluating Epoch 66  81.2% | batch:        13 of        16\t|\tloss: 1094.82\n",
      "Evaluating Epoch 66  87.5% | batch:        14 of        16\t|\tloss: 1804.29\n",
      "Evaluating Epoch 66  93.8% | batch:        15 of        16\t|\tloss: 2845.07\n",
      "\n",
      "Training Epoch 67   0.0% | batch:         0 of        64\t|\tloss: 950.861\n",
      "Training Epoch 67   1.6% | batch:         1 of        64\t|\tloss: 1038.1\n",
      "Training Epoch 67   3.1% | batch:         2 of        64\t|\tloss: 1270.2\n",
      "Training Epoch 67   4.7% | batch:         3 of        64\t|\tloss: 892.737\n",
      "Training Epoch 67   6.2% | batch:         4 of        64\t|\tloss: 1159.18\n",
      "Training Epoch 67   7.8% | batch:         5 of        64\t|\tloss: 752.231\n",
      "Training Epoch 67   9.4% | batch:         6 of        64\t|\tloss: 1062.88\n",
      "Training Epoch 67  10.9% | batch:         7 of        64\t|\tloss: 1503.43\n",
      "Training Epoch 67  12.5% | batch:         8 of        64\t|\tloss: 1311.65\n",
      "Training Epoch 67  14.1% | batch:         9 of        64\t|\tloss: 920.024\n",
      "Training Epoch 67  15.6% | batch:        10 of        64\t|\tloss: 1173.99\n",
      "Training Epoch 67  17.2% | batch:        11 of        64\t|\tloss: 487.931\n",
      "Training Epoch 67  18.8% | batch:        12 of        64\t|\tloss: 913.872\n",
      "Training Epoch 67  20.3% | batch:        13 of        64\t|\tloss: 695.348\n",
      "Training Epoch 67  21.9% | batch:        14 of        64\t|\tloss: 813.108\n",
      "Training Epoch 67  23.4% | batch:        15 of        64\t|\tloss: 752.459\n",
      "Training Epoch 67  25.0% | batch:        16 of        64\t|\tloss: 1642.29\n",
      "Training Epoch 67  26.6% | batch:        17 of        64\t|\tloss: 1844.84\n",
      "Training Epoch 67  28.1% | batch:        18 of        64\t|\tloss: 1029.02\n",
      "Training Epoch 67  29.7% | batch:        19 of        64\t|\tloss: 832.078\n",
      "Training Epoch 67  31.2% | batch:        20 of        64\t|\tloss: 1283.53\n",
      "Training Epoch 67  32.8% | batch:        21 of        64\t|\tloss: 1086.65\n",
      "Training Epoch 67  34.4% | batch:        22 of        64\t|\tloss: 1054.5\n",
      "Training Epoch 67  35.9% | batch:        23 of        64\t|\tloss: 1410.82\n",
      "Training Epoch 67  37.5% | batch:        24 of        64\t|\tloss: 741.716\n",
      "Training Epoch 67  39.1% | batch:        25 of        64\t|\tloss: 1263.74\n",
      "Training Epoch 67  40.6% | batch:        26 of        64\t|\tloss: 766.709\n",
      "Training Epoch 67  42.2% | batch:        27 of        64\t|\tloss: 985.605\n",
      "Training Epoch 67  43.8% | batch:        28 of        64\t|\tloss: 1708.44\n",
      "Training Epoch 67  45.3% | batch:        29 of        64\t|\tloss: 1161.37\n",
      "Training Epoch 67  46.9% | batch:        30 of        64\t|\tloss: 881.998\n",
      "Training Epoch 67  48.4% | batch:        31 of        64\t|\tloss: 1314.55\n",
      "Training Epoch 67  50.0% | batch:        32 of        64\t|\tloss: 2586.86\n",
      "Training Epoch 67  51.6% | batch:        33 of        64\t|\tloss: 1014.64\n",
      "Training Epoch 67  53.1% | batch:        34 of        64\t|\tloss: 1538.61\n",
      "Training Epoch 67  54.7% | batch:        35 of        64\t|\tloss: 937.69\n",
      "Training Epoch 67  56.2% | batch:        36 of        64\t|\tloss: 1661.72\n",
      "Training Epoch 67  57.8% | batch:        37 of        64\t|\tloss: 2931.06\n",
      "Training Epoch 67  59.4% | batch:        38 of        64\t|\tloss: 1806.41\n",
      "Training Epoch 67  60.9% | batch:        39 of        64\t|\tloss: 1298.83\n",
      "Training Epoch 67  62.5% | batch:        40 of        64\t|\tloss: 1683.22\n",
      "Training Epoch 67  64.1% | batch:        41 of        64\t|\tloss: 915.963\n",
      "Training Epoch 67  65.6% | batch:        42 of        64\t|\tloss: 2565.37\n",
      "Training Epoch 67  67.2% | batch:        43 of        64\t|\tloss: 1861.55\n",
      "Training Epoch 67  68.8% | batch:        44 of        64\t|\tloss: 959.993\n",
      "Training Epoch 67  70.3% | batch:        45 of        64\t|\tloss: 824.304\n",
      "Training Epoch 67  71.9% | batch:        46 of        64\t|\tloss: 1362.37\n",
      "Training Epoch 67  73.4% | batch:        47 of        64\t|\tloss: 615.821\n",
      "Training Epoch 67  75.0% | batch:        48 of        64\t|\tloss: 684.904\n",
      "Training Epoch 67  76.6% | batch:        49 of        64\t|\tloss: 1508.82\n",
      "Training Epoch 67  78.1% | batch:        50 of        64\t|\tloss: 1606.75\n",
      "Training Epoch 67  79.7% | batch:        51 of        64\t|\tloss: 1189.53\n",
      "Training Epoch 67  81.2% | batch:        52 of        64\t|\tloss: 1178.35\n",
      "Training Epoch 67  82.8% | batch:        53 of        64\t|\tloss: 863.354\n",
      "Training Epoch 67  84.4% | batch:        54 of        64\t|\tloss: 1371.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:21,507 | INFO : Epoch 67 Training Summary: epoch: 67.000000 | loss: 1322.018470 | \n",
      "2023-05-10 17:09:21,507 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.203861951828003 seconds\n",
      "\n",
      "2023-05-10 17:09:21,508 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2029802123112465 seconds\n",
      "2023-05-10 17:09:21,508 | INFO : Avg batch train. time: 0.018796565817363227 seconds\n",
      "2023-05-10 17:09:21,509 | INFO : Avg sample train. time: 0.00029791486188985797 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 67  85.9% | batch:        55 of        64\t|\tloss: 787.167\n",
      "Training Epoch 67  87.5% | batch:        56 of        64\t|\tloss: 2005.35\n",
      "Training Epoch 67  89.1% | batch:        57 of        64\t|\tloss: 1525.61\n",
      "Training Epoch 67  90.6% | batch:        58 of        64\t|\tloss: 4808.57\n",
      "Training Epoch 67  92.2% | batch:        59 of        64\t|\tloss: 1779.05\n",
      "Training Epoch 67  93.8% | batch:        60 of        64\t|\tloss: 1069.64\n",
      "Training Epoch 67  95.3% | batch:        61 of        64\t|\tloss: 1639.31\n",
      "Training Epoch 67  96.9% | batch:        62 of        64\t|\tloss: 1071.32\n",
      "Training Epoch 67  98.4% | batch:        63 of        64\t|\tloss: 11215.8\n",
      "\n",
      "Training Epoch 68   0.0% | batch:         0 of        64\t|\tloss: 685.29\n",
      "Training Epoch 68   1.6% | batch:         1 of        64\t|\tloss: 1297.75\n",
      "Training Epoch 68   3.1% | batch:         2 of        64\t|\tloss: 689.515\n",
      "Training Epoch 68   4.7% | batch:         3 of        64\t|\tloss: 881.493\n",
      "Training Epoch 68   6.2% | batch:         4 of        64\t|\tloss: 1572.82\n",
      "Training Epoch 68   7.8% | batch:         5 of        64\t|\tloss: 1202.88\n",
      "Training Epoch 68   9.4% | batch:         6 of        64\t|\tloss: 1127.35\n",
      "Training Epoch 68  10.9% | batch:         7 of        64\t|\tloss: 814.83\n",
      "Training Epoch 68  12.5% | batch:         8 of        64\t|\tloss: 1322.2\n",
      "Training Epoch 68  14.1% | batch:         9 of        64\t|\tloss: 946.543\n",
      "Training Epoch 68  15.6% | batch:        10 of        64\t|\tloss: 1649.73\n",
      "Training Epoch 68  17.2% | batch:        11 of        64\t|\tloss: 634.76\n",
      "Training Epoch 68  18.8% | batch:        12 of        64\t|\tloss: 1613.04\n",
      "Training Epoch 68  20.3% | batch:        13 of        64\t|\tloss: 3019.66\n",
      "Training Epoch 68  21.9% | batch:        14 of        64\t|\tloss: 1138.25\n",
      "Training Epoch 68  23.4% | batch:        15 of        64\t|\tloss: 983.751\n",
      "Training Epoch 68  25.0% | batch:        16 of        64\t|\tloss: 1844.54\n",
      "Training Epoch 68  26.6% | batch:        17 of        64\t|\tloss: 792.205\n",
      "Training Epoch 68  28.1% | batch:        18 of        64\t|\tloss: 1381.24\n",
      "Training Epoch 68  29.7% | batch:        19 of        64\t|\tloss: 1241.59\n",
      "Training Epoch 68  31.2% | batch:        20 of        64\t|\tloss: 723.485\n",
      "Training Epoch 68  32.8% | batch:        21 of        64\t|\tloss: 831.927\n",
      "Training Epoch 68  34.4% | batch:        22 of        64\t|\tloss: 1132.33\n",
      "Training Epoch 68  35.9% | batch:        23 of        64\t|\tloss: 806.983\n",
      "Training Epoch 68  37.5% | batch:        24 of        64\t|\tloss: 1802.08\n",
      "Training Epoch 68  39.1% | batch:        25 of        64\t|\tloss: 1898.56\n",
      "Training Epoch 68  40.6% | batch:        26 of        64\t|\tloss: 1368.17\n",
      "Training Epoch 68  42.2% | batch:        27 of        64\t|\tloss: 1666.07\n",
      "Training Epoch 68  43.8% | batch:        28 of        64\t|\tloss: 1308.42\n",
      "Training Epoch 68  45.3% | batch:        29 of        64\t|\tloss: 1655.23\n",
      "Training Epoch 68  46.9% | batch:        30 of        64\t|\tloss: 1209.69\n",
      "Training Epoch 68  48.4% | batch:        31 of        64\t|\tloss: 515.318\n",
      "Training Epoch 68  50.0% | batch:        32 of        64\t|\tloss: 1474.83\n",
      "Training Epoch 68  51.6% | batch:        33 of        64\t|\tloss: 1073.39\n",
      "Training Epoch 68  53.1% | batch:        34 of        64\t|\tloss: 840.941\n",
      "Training Epoch 68  54.7% | batch:        35 of        64\t|\tloss: 1186.24\n",
      "Training Epoch 68  56.2% | batch:        36 of        64\t|\tloss: 1593.27\n",
      "Training Epoch 68  57.8% | batch:        37 of        64\t|\tloss: 1334.19\n",
      "Training Epoch 68  59.4% | batch:        38 of        64\t|\tloss: 1016.99\n",
      "Training Epoch 68  60.9% | batch:        39 of        64\t|\tloss: 1232.93\n",
      "Training Epoch 68  62.5% | batch:        40 of        64\t|\tloss: 649.559\n",
      "Training Epoch 68  64.1% | batch:        41 of        64\t|\tloss: 885.656\n",
      "Training Epoch 68  65.6% | batch:        42 of        64\t|\tloss: 992.161\n",
      "Training Epoch 68  67.2% | batch:        43 of        64\t|\tloss: 1487.53\n",
      "Training Epoch 68  68.8% | batch:        44 of        64\t|\tloss: 1833.17\n",
      "Training Epoch 68  70.3% | batch:        45 of        64\t|\tloss: 2257.57\n",
      "Training Epoch 68  71.9% | batch:        46 of        64\t|\tloss: 967.25\n",
      "Training Epoch 68  73.4% | batch:        47 of        64\t|\tloss: 731.043\n",
      "Training Epoch 68  75.0% | batch:        48 of        64\t|\tloss: 1987.69\n",
      "Training Epoch 68  76.6% | batch:        49 of        64\t|\tloss: 1086.07\n",
      "Training Epoch 68  78.1% | batch:        50 of        64\t|\tloss: 572.493\n",
      "Training Epoch 68  79.7% | batch:        51 of        64\t|\tloss: 1124.99\n",
      "Training Epoch 68  81.2% | batch:        52 of        64\t|\tloss: 1684.63\n",
      "Training Epoch 68  82.8% | batch:        53 of        64\t|\tloss: 1771.01\n",
      "Training Epoch 68  84.4% | batch:        54 of        64\t|\tloss: 843.224\n",
      "Training Epoch 68  85.9% | batch:        55 of        64\t|\tloss: 943.035\n",
      "Training Epoch 68  87.5% | batch:        56 of        64\t|\tloss: 1088.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:22,717 | INFO : Epoch 68 Training Summary: epoch: 68.000000 | loss: 1216.585938 | \n",
      "2023-05-10 17:09:22,718 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.199005126953125 seconds\n",
      "\n",
      "2023-05-10 17:09:22,718 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.202921755173627 seconds\n",
      "2023-05-10 17:09:22,718 | INFO : Avg batch train. time: 0.018795652424587923 seconds\n",
      "2023-05-10 17:09:22,719 | INFO : Avg sample train. time: 0.00029790038513462783 seconds\n",
      "2023-05-10 17:09:22,720 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 68  89.1% | batch:        57 of        64\t|\tloss: 1234.6\n",
      "Training Epoch 68  90.6% | batch:        58 of        64\t|\tloss: 1035.98\n",
      "Training Epoch 68  92.2% | batch:        59 of        64\t|\tloss: 1196.35\n",
      "Training Epoch 68  93.8% | batch:        60 of        64\t|\tloss: 677.602\n",
      "Training Epoch 68  95.3% | batch:        61 of        64\t|\tloss: 952.273\n",
      "Training Epoch 68  96.9% | batch:        62 of        64\t|\tloss: 1195.08\n",
      "Training Epoch 68  98.4% | batch:        63 of        64\t|\tloss: 548.898\n",
      "\n",
      "Evaluating Epoch 68   0.0% | batch:         0 of        16\t|\tloss: 1610.94\n",
      "Evaluating Epoch 68   6.2% | batch:         1 of        16\t|\tloss: 949.298\n",
      "Evaluating Epoch 68  12.5% | batch:         2 of        16\t|\tloss: 772.739\n",
      "Evaluating Epoch 68  18.8% | batch:         3 of        16\t|\tloss: 1286.23\n",
      "Evaluating Epoch 68  25.0% | batch:         4 of        16\t|\tloss: 1569.36\n",
      "Evaluating Epoch 68  31.2% | batch:         5 of        16\t|\tloss: 1748.59\n",
      "Evaluating Epoch 68  37.5% | batch:         6 of        16\t|\tloss: 2702.06\n",
      "Evaluating Epoch 68  43.8% | batch:         7 of        16\t|\tloss: 2143.46\n",
      "Evaluating Epoch 68  50.0% | batch:         8 of        16\t|\tloss: 1433.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:22,869 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14899563789367676 seconds\n",
      "\n",
      "2023-05-10 17:09:22,869 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.17064601845211452 seconds\n",
      "2023-05-10 17:09:22,870 | INFO : Avg batch val. time: 0.010665376153257158 seconds\n",
      "2023-05-10 17:09:22,870 | INFO : Avg sample val. time: 0.00016895645391298467 seconds\n",
      "2023-05-10 17:09:22,871 | INFO : Epoch 68 Validation Summary: epoch: 68.000000 | loss: 1865.569458 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 68  56.2% | batch:         9 of        16\t|\tloss: 5428.48\n",
      "Evaluating Epoch 68  62.5% | batch:        10 of        16\t|\tloss: 1663.26\n",
      "Evaluating Epoch 68  68.8% | batch:        11 of        16\t|\tloss: 2038.92\n",
      "Evaluating Epoch 68  75.0% | batch:        12 of        16\t|\tloss: 1135.35\n",
      "Evaluating Epoch 68  81.2% | batch:        13 of        16\t|\tloss: 979.61\n",
      "Evaluating Epoch 68  87.5% | batch:        14 of        16\t|\tloss: 2080\n",
      "Evaluating Epoch 68  93.8% | batch:        15 of        16\t|\tloss: 2430.67\n",
      "\n",
      "Training Epoch 69   0.0% | batch:         0 of        64\t|\tloss: 780.904\n",
      "Training Epoch 69   1.6% | batch:         1 of        64\t|\tloss: 856.287\n",
      "Training Epoch 69   3.1% | batch:         2 of        64\t|\tloss: 1064.71\n",
      "Training Epoch 69   4.7% | batch:         3 of        64\t|\tloss: 1287.87\n",
      "Training Epoch 69   6.2% | batch:         4 of        64\t|\tloss: 845.576\n",
      "Training Epoch 69   7.8% | batch:         5 of        64\t|\tloss: 758.141\n",
      "Training Epoch 69   9.4% | batch:         6 of        64\t|\tloss: 1147.46\n",
      "Training Epoch 69  10.9% | batch:         7 of        64\t|\tloss: 2270.59\n",
      "Training Epoch 69  12.5% | batch:         8 of        64\t|\tloss: 560.272\n",
      "Training Epoch 69  14.1% | batch:         9 of        64\t|\tloss: 1117.9\n",
      "Training Epoch 69  15.6% | batch:        10 of        64\t|\tloss: 1303.21\n",
      "Training Epoch 69  17.2% | batch:        11 of        64\t|\tloss: 1152.91\n",
      "Training Epoch 69  18.8% | batch:        12 of        64\t|\tloss: 2319.32\n",
      "Training Epoch 69  20.3% | batch:        13 of        64\t|\tloss: 921.987\n",
      "Training Epoch 69  21.9% | batch:        14 of        64\t|\tloss: 884.366\n",
      "Training Epoch 69  23.4% | batch:        15 of        64\t|\tloss: 1066.48\n",
      "Training Epoch 69  25.0% | batch:        16 of        64\t|\tloss: 1270.23\n",
      "Training Epoch 69  26.6% | batch:        17 of        64\t|\tloss: 730.002\n",
      "Training Epoch 69  28.1% | batch:        18 of        64\t|\tloss: 945.054\n",
      "Training Epoch 69  29.7% | batch:        19 of        64\t|\tloss: 1392.9\n",
      "Training Epoch 69  31.2% | batch:        20 of        64\t|\tloss: 1272.53\n",
      "Training Epoch 69  32.8% | batch:        21 of        64\t|\tloss: 587.656\n",
      "Training Epoch 69  34.4% | batch:        22 of        64\t|\tloss: 1154.79\n",
      "Training Epoch 69  35.9% | batch:        23 of        64\t|\tloss: 781.795\n",
      "Training Epoch 69  37.5% | batch:        24 of        64\t|\tloss: 1802.3\n",
      "Training Epoch 69  39.1% | batch:        25 of        64\t|\tloss: 1185.83\n",
      "Training Epoch 69  40.6% | batch:        26 of        64\t|\tloss: 1644.97\n",
      "Training Epoch 69  42.2% | batch:        27 of        64\t|\tloss: 907.379\n",
      "Training Epoch 69  43.8% | batch:        28 of        64\t|\tloss: 649.381\n",
      "Training Epoch 69  45.3% | batch:        29 of        64\t|\tloss: 834.404\n",
      "Training Epoch 69  46.9% | batch:        30 of        64\t|\tloss: 923.126\n",
      "Training Epoch 69  48.4% | batch:        31 of        64\t|\tloss: 1186.78\n",
      "Training Epoch 69  50.0% | batch:        32 of        64\t|\tloss: 877.851\n",
      "Training Epoch 69  51.6% | batch:        33 of        64\t|\tloss: 904.228\n",
      "Training Epoch 69  53.1% | batch:        34 of        64\t|\tloss: 697.978\n",
      "Training Epoch 69  54.7% | batch:        35 of        64\t|\tloss: 1100.79\n",
      "Training Epoch 69  56.2% | batch:        36 of        64\t|\tloss: 1040.88\n",
      "Training Epoch 69  57.8% | batch:        37 of        64\t|\tloss: 1061.4\n",
      "Training Epoch 69  59.4% | batch:        38 of        64\t|\tloss: 1445.78\n",
      "Training Epoch 69  60.9% | batch:        39 of        64\t|\tloss: 1032.51\n",
      "Training Epoch 69  62.5% | batch:        40 of        64\t|\tloss: 1042.84\n",
      "Training Epoch 69  64.1% | batch:        41 of        64\t|\tloss: 2210.63\n",
      "Training Epoch 69  65.6% | batch:        42 of        64\t|\tloss: 666.473\n",
      "Training Epoch 69  67.2% | batch:        43 of        64\t|\tloss: 4201.77\n",
      "Training Epoch 69  68.8% | batch:        44 of        64\t|\tloss: 930.042\n",
      "Training Epoch 69  70.3% | batch:        45 of        64\t|\tloss: 1051.46\n",
      "Training Epoch 69  71.9% | batch:        46 of        64\t|\tloss: 765.938\n",
      "Training Epoch 69  73.4% | batch:        47 of        64\t|\tloss: 799.706\n",
      "Training Epoch 69  75.0% | batch:        48 of        64\t|\tloss: 1262.08\n",
      "Training Epoch 69  76.6% | batch:        49 of        64\t|\tloss: 762.931\n",
      "Training Epoch 69  78.1% | batch:        50 of        64\t|\tloss: 1151.84\n",
      "Training Epoch 69  79.7% | batch:        51 of        64\t|\tloss: 3240.26\n",
      "Training Epoch 69  81.2% | batch:        52 of        64\t|\tloss: 910.804\n",
      "Training Epoch 69  82.8% | batch:        53 of        64\t|\tloss: 1441.34\n",
      "Training Epoch 69  84.4% | batch:        54 of        64\t|\tloss: 1634.71\n",
      "Training Epoch 69  85.9% | batch:        55 of        64\t|\tloss: 1579.8\n",
      "Training Epoch 69  87.5% | batch:        56 of        64\t|\tloss: 1205.13\n",
      "Training Epoch 69  89.1% | batch:        57 of        64\t|\tloss: 1030.08\n",
      "Training Epoch 69  90.6% | batch:        58 of        64\t|\tloss: 1858.62\n",
      "Training Epoch 69  92.2% | batch:        59 of        64\t|\tloss: 1950.79\n",
      "Training Epoch 69  93.8% | batch:        60 of        64\t|\tloss: 1779.79\n",
      "Training Epoch 69  95.3% | batch:        61 of        64\t|\tloss: 2240.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:24,060 | INFO : Epoch 69 Training Summary: epoch: 69.000000 | loss: 1242.958198 | \n",
      "2023-05-10 17:09:24,061 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1797122955322266 seconds\n",
      "\n",
      "2023-05-10 17:09:24,061 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.202585386193317 seconds\n",
      "2023-05-10 17:09:24,062 | INFO : Avg batch train. time: 0.018790396659270577 seconds\n",
      "2023-05-10 17:09:24,062 | INFO : Avg sample train. time: 0.0002978170842479735 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 69  96.9% | batch:        62 of        64\t|\tloss: 902.793\n",
      "Training Epoch 69  98.4% | batch:        63 of        64\t|\tloss: 360.735\n",
      "\n",
      "Training Epoch 70   0.0% | batch:         0 of        64\t|\tloss: 660.776\n",
      "Training Epoch 70   1.6% | batch:         1 of        64\t|\tloss: 1168.48\n",
      "Training Epoch 70   3.1% | batch:         2 of        64\t|\tloss: 2284.5\n",
      "Training Epoch 70   4.7% | batch:         3 of        64\t|\tloss: 1758.93\n",
      "Training Epoch 70   6.2% | batch:         4 of        64\t|\tloss: 895.677\n",
      "Training Epoch 70   7.8% | batch:         5 of        64\t|\tloss: 906.562\n",
      "Training Epoch 70   9.4% | batch:         6 of        64\t|\tloss: 1349.93\n",
      "Training Epoch 70  10.9% | batch:         7 of        64\t|\tloss: 863.349\n",
      "Training Epoch 70  12.5% | batch:         8 of        64\t|\tloss: 1025.46\n",
      "Training Epoch 70  14.1% | batch:         9 of        64\t|\tloss: 1066.98\n",
      "Training Epoch 70  15.6% | batch:        10 of        64\t|\tloss: 997.898\n",
      "Training Epoch 70  17.2% | batch:        11 of        64\t|\tloss: 970.536\n",
      "Training Epoch 70  18.8% | batch:        12 of        64\t|\tloss: 1153.96\n",
      "Training Epoch 70  20.3% | batch:        13 of        64\t|\tloss: 1690.11\n",
      "Training Epoch 70  21.9% | batch:        14 of        64\t|\tloss: 842.154\n",
      "Training Epoch 70  23.4% | batch:        15 of        64\t|\tloss: 848.059\n",
      "Training Epoch 70  25.0% | batch:        16 of        64\t|\tloss: 900.529\n",
      "Training Epoch 70  26.6% | batch:        17 of        64\t|\tloss: 1273.05\n",
      "Training Epoch 70  28.1% | batch:        18 of        64\t|\tloss: 1139.55\n",
      "Training Epoch 70  29.7% | batch:        19 of        64\t|\tloss: 873.737\n",
      "Training Epoch 70  31.2% | batch:        20 of        64\t|\tloss: 2040.58\n",
      "Training Epoch 70  32.8% | batch:        21 of        64\t|\tloss: 832.2\n",
      "Training Epoch 70  34.4% | batch:        22 of        64\t|\tloss: 1695.45\n",
      "Training Epoch 70  35.9% | batch:        23 of        64\t|\tloss: 814.155\n",
      "Training Epoch 70  37.5% | batch:        24 of        64\t|\tloss: 1923.48\n",
      "Training Epoch 70  39.1% | batch:        25 of        64\t|\tloss: 2163.24\n",
      "Training Epoch 70  40.6% | batch:        26 of        64\t|\tloss: 1391.36\n",
      "Training Epoch 70  42.2% | batch:        27 of        64\t|\tloss: 1035.49\n",
      "Training Epoch 70  43.8% | batch:        28 of        64\t|\tloss: 1314.81\n",
      "Training Epoch 70  45.3% | batch:        29 of        64\t|\tloss: 1200.29\n",
      "Training Epoch 70  46.9% | batch:        30 of        64\t|\tloss: 562.826\n",
      "Training Epoch 70  48.4% | batch:        31 of        64\t|\tloss: 1371.24\n",
      "Training Epoch 70  50.0% | batch:        32 of        64\t|\tloss: 2357.58\n",
      "Training Epoch 70  51.6% | batch:        33 of        64\t|\tloss: 1092.39\n",
      "Training Epoch 70  53.1% | batch:        34 of        64\t|\tloss: 863.847\n",
      "Training Epoch 70  54.7% | batch:        35 of        64\t|\tloss: 975.156\n",
      "Training Epoch 70  56.2% | batch:        36 of        64\t|\tloss: 1413.67\n",
      "Training Epoch 70  57.8% | batch:        37 of        64\t|\tloss: 725.119\n",
      "Training Epoch 70  59.4% | batch:        38 of        64\t|\tloss: 662.151\n",
      "Training Epoch 70  60.9% | batch:        39 of        64\t|\tloss: 767.527\n",
      "Training Epoch 70  62.5% | batch:        40 of        64\t|\tloss: 1653.41\n",
      "Training Epoch 70  64.1% | batch:        41 of        64\t|\tloss: 799.137\n",
      "Training Epoch 70  65.6% | batch:        42 of        64\t|\tloss: 711.94\n",
      "Training Epoch 70  67.2% | batch:        43 of        64\t|\tloss: 1059.81\n",
      "Training Epoch 70  68.8% | batch:        44 of        64\t|\tloss: 938.276\n",
      "Training Epoch 70  70.3% | batch:        45 of        64\t|\tloss: 2171.17\n",
      "Training Epoch 70  71.9% | batch:        46 of        64\t|\tloss: 1889.59\n",
      "Training Epoch 70  73.4% | batch:        47 of        64\t|\tloss: 1171.83\n",
      "Training Epoch 70  75.0% | batch:        48 of        64\t|\tloss: 939.212\n",
      "Training Epoch 70  76.6% | batch:        49 of        64\t|\tloss: 1848.66\n",
      "Training Epoch 70  78.1% | batch:        50 of        64\t|\tloss: 720.933\n",
      "Training Epoch 70  79.7% | batch:        51 of        64\t|\tloss: 1021.29\n",
      "Training Epoch 70  81.2% | batch:        52 of        64\t|\tloss: 2555.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:25,260 | INFO : Epoch 70 Training Summary: epoch: 70.000000 | loss: 1250.845574 | \n",
      "2023-05-10 17:09:25,261 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1885485649108887 seconds\n",
      "\n",
      "2023-05-10 17:09:25,261 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2023848601749965 seconds\n",
      "2023-05-10 17:09:25,262 | INFO : Avg batch train. time: 0.01878726344023432 seconds\n",
      "2023-05-10 17:09:25,262 | INFO : Avg sample train. time: 0.0002977674245108956 seconds\n",
      "2023-05-10 17:09:25,262 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 70  82.8% | batch:        53 of        64\t|\tloss: 1685.39\n",
      "Training Epoch 70  84.4% | batch:        54 of        64\t|\tloss: 1388.38\n",
      "Training Epoch 70  85.9% | batch:        55 of        64\t|\tloss: 973.517\n",
      "Training Epoch 70  87.5% | batch:        56 of        64\t|\tloss: 1197.11\n",
      "Training Epoch 70  89.1% | batch:        57 of        64\t|\tloss: 1018.49\n",
      "Training Epoch 70  90.6% | batch:        58 of        64\t|\tloss: 715.211\n",
      "Training Epoch 70  92.2% | batch:        59 of        64\t|\tloss: 1081.93\n",
      "Training Epoch 70  93.8% | batch:        60 of        64\t|\tloss: 1649.09\n",
      "Training Epoch 70  95.3% | batch:        61 of        64\t|\tloss: 2567.12\n",
      "Training Epoch 70  96.9% | batch:        62 of        64\t|\tloss: 1212.88\n",
      "Training Epoch 70  98.4% | batch:        63 of        64\t|\tloss: 826.419\n",
      "\n",
      "Evaluating Epoch 70   0.0% | batch:         0 of        16\t|\tloss: 2342.57\n",
      "Evaluating Epoch 70   6.2% | batch:         1 of        16\t|\tloss: 1646.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:25,412 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14884138107299805 seconds\n",
      "\n",
      "2023-05-10 17:09:25,412 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.17005670392835462 seconds\n",
      "2023-05-10 17:09:25,413 | INFO : Avg batch val. time: 0.010628543995522164 seconds\n",
      "2023-05-10 17:09:25,413 | INFO : Avg sample val. time: 0.00016837297418648972 seconds\n",
      "2023-05-10 17:09:25,414 | INFO : Epoch 70 Validation Summary: epoch: 70.000000 | loss: 2322.627081 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 70  12.5% | batch:         2 of        16\t|\tloss: 1431.92\n",
      "Evaluating Epoch 70  18.8% | batch:         3 of        16\t|\tloss: 1625.83\n",
      "Evaluating Epoch 70  25.0% | batch:         4 of        16\t|\tloss: 1574.44\n",
      "Evaluating Epoch 70  31.2% | batch:         5 of        16\t|\tloss: 1838.76\n",
      "Evaluating Epoch 70  37.5% | batch:         6 of        16\t|\tloss: 2861.89\n",
      "Evaluating Epoch 70  43.8% | batch:         7 of        16\t|\tloss: 2566.36\n",
      "Evaluating Epoch 70  50.0% | batch:         8 of        16\t|\tloss: 1790.97\n",
      "Evaluating Epoch 70  56.2% | batch:         9 of        16\t|\tloss: 7532.46\n",
      "Evaluating Epoch 70  62.5% | batch:        10 of        16\t|\tloss: 1967.35\n",
      "Evaluating Epoch 70  68.8% | batch:        11 of        16\t|\tloss: 2659.33\n",
      "Evaluating Epoch 70  75.0% | batch:        12 of        16\t|\tloss: 1161.36\n",
      "Evaluating Epoch 70  81.2% | batch:        13 of        16\t|\tloss: 1207.27\n",
      "Evaluating Epoch 70  87.5% | batch:        14 of        16\t|\tloss: 2210.12\n",
      "Evaluating Epoch 70  93.8% | batch:        15 of        16\t|\tloss: 2862.92\n",
      "\n",
      "Training Epoch 71   0.0% | batch:         0 of        64\t|\tloss: 1095.4\n",
      "Training Epoch 71   1.6% | batch:         1 of        64\t|\tloss: 901.152\n",
      "Training Epoch 71   3.1% | batch:         2 of        64\t|\tloss: 1251.27\n",
      "Training Epoch 71   4.7% | batch:         3 of        64\t|\tloss: 1240.95\n",
      "Training Epoch 71   6.2% | batch:         4 of        64\t|\tloss: 711.088\n",
      "Training Epoch 71   7.8% | batch:         5 of        64\t|\tloss: 1442.93\n",
      "Training Epoch 71   9.4% | batch:         6 of        64\t|\tloss: 785.273\n",
      "Training Epoch 71  10.9% | batch:         7 of        64\t|\tloss: 3553.37\n",
      "Training Epoch 71  12.5% | batch:         8 of        64\t|\tloss: 1053.54\n",
      "Training Epoch 71  14.1% | batch:         9 of        64\t|\tloss: 1061.93\n",
      "Training Epoch 71  15.6% | batch:        10 of        64\t|\tloss: 1100.17\n",
      "Training Epoch 71  17.2% | batch:        11 of        64\t|\tloss: 2830.15\n",
      "Training Epoch 71  18.8% | batch:        12 of        64\t|\tloss: 1009.25\n",
      "Training Epoch 71  20.3% | batch:        13 of        64\t|\tloss: 1274.17\n",
      "Training Epoch 71  21.9% | batch:        14 of        64\t|\tloss: 962.953\n",
      "Training Epoch 71  23.4% | batch:        15 of        64\t|\tloss: 659.694\n",
      "Training Epoch 71  25.0% | batch:        16 of        64\t|\tloss: 651.971\n",
      "Training Epoch 71  26.6% | batch:        17 of        64\t|\tloss: 1045.71\n",
      "Training Epoch 71  28.1% | batch:        18 of        64\t|\tloss: 1398.36\n",
      "Training Epoch 71  29.7% | batch:        19 of        64\t|\tloss: 840.259\n",
      "Training Epoch 71  31.2% | batch:        20 of        64\t|\tloss: 863.068\n",
      "Training Epoch 71  32.8% | batch:        21 of        64\t|\tloss: 528.011\n",
      "Training Epoch 71  34.4% | batch:        22 of        64\t|\tloss: 757.896\n",
      "Training Epoch 71  35.9% | batch:        23 of        64\t|\tloss: 1130.22\n",
      "Training Epoch 71  37.5% | batch:        24 of        64\t|\tloss: 827.228\n",
      "Training Epoch 71  39.1% | batch:        25 of        64\t|\tloss: 1495.47\n",
      "Training Epoch 71  40.6% | batch:        26 of        64\t|\tloss: 668.802\n",
      "Training Epoch 71  42.2% | batch:        27 of        64\t|\tloss: 862.979\n",
      "Training Epoch 71  43.8% | batch:        28 of        64\t|\tloss: 1466.54\n",
      "Training Epoch 71  45.3% | batch:        29 of        64\t|\tloss: 931.756\n",
      "Training Epoch 71  46.9% | batch:        30 of        64\t|\tloss: 1790.51\n",
      "Training Epoch 71  48.4% | batch:        31 of        64\t|\tloss: 1619.57\n",
      "Training Epoch 71  50.0% | batch:        32 of        64\t|\tloss: 611.173\n",
      "Training Epoch 71  51.6% | batch:        33 of        64\t|\tloss: 1523.59\n",
      "Training Epoch 71  53.1% | batch:        34 of        64\t|\tloss: 1341.38\n",
      "Training Epoch 71  54.7% | batch:        35 of        64\t|\tloss: 644.843\n",
      "Training Epoch 71  56.2% | batch:        36 of        64\t|\tloss: 1280.97\n",
      "Training Epoch 71  57.8% | batch:        37 of        64\t|\tloss: 1234\n",
      "Training Epoch 71  59.4% | batch:        38 of        64\t|\tloss: 1064.08\n",
      "Training Epoch 71  60.9% | batch:        39 of        64\t|\tloss: 772.258\n",
      "Training Epoch 71  62.5% | batch:        40 of        64\t|\tloss: 636.525\n",
      "Training Epoch 71  64.1% | batch:        41 of        64\t|\tloss: 1024.07\n",
      "Training Epoch 71  65.6% | batch:        42 of        64\t|\tloss: 2289.03\n",
      "Training Epoch 71  67.2% | batch:        43 of        64\t|\tloss: 2448.45\n",
      "Training Epoch 71  68.8% | batch:        44 of        64\t|\tloss: 4028.12\n",
      "Training Epoch 71  70.3% | batch:        45 of        64\t|\tloss: 829.286\n",
      "Training Epoch 71  71.9% | batch:        46 of        64\t|\tloss: 967.712\n",
      "Training Epoch 71  73.4% | batch:        47 of        64\t|\tloss: 1088.11\n",
      "Training Epoch 71  75.0% | batch:        48 of        64\t|\tloss: 1976.17\n",
      "Training Epoch 71  76.6% | batch:        49 of        64\t|\tloss: 1490.34\n",
      "Training Epoch 71  78.1% | batch:        50 of        64\t|\tloss: 1825.66\n",
      "Training Epoch 71  79.7% | batch:        51 of        64\t|\tloss: 1121.17\n",
      "Training Epoch 71  81.2% | batch:        52 of        64\t|\tloss: 857.532\n",
      "Training Epoch 71  82.8% | batch:        53 of        64\t|\tloss: 1256.91\n",
      "Training Epoch 71  84.4% | batch:        54 of        64\t|\tloss: 930.316\n",
      "Training Epoch 71  85.9% | batch:        55 of        64\t|\tloss: 3575.05\n",
      "Training Epoch 71  87.5% | batch:        56 of        64\t|\tloss: 1529.52\n",
      "Training Epoch 71  89.1% | batch:        57 of        64\t|\tloss: 840.126\n",
      "Training Epoch 71  90.6% | batch:        58 of        64\t|\tloss: 857.311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:26,612 | INFO : Epoch 71 Training Summary: epoch: 71.000000 | loss: 1272.524133 | \n",
      "2023-05-10 17:09:26,613 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1890299320220947 seconds\n",
      "\n",
      "2023-05-10 17:09:26,613 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2021967625953782 seconds\n",
      "2023-05-10 17:09:26,614 | INFO : Avg batch train. time: 0.018784324415552785 seconds\n",
      "2023-05-10 17:09:26,615 | INFO : Avg sample train. time: 0.0002977208426437291 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 71  92.2% | batch:        59 of        64\t|\tloss: 910.53\n",
      "Training Epoch 71  93.8% | batch:        60 of        64\t|\tloss: 1123\n",
      "Training Epoch 71  95.3% | batch:        61 of        64\t|\tloss: 1066.51\n",
      "Training Epoch 71  96.9% | batch:        62 of        64\t|\tloss: 961.839\n",
      "Training Epoch 71  98.4% | batch:        63 of        64\t|\tloss: 3958.19\n",
      "\n",
      "Training Epoch 72   0.0% | batch:         0 of        64\t|\tloss: 703.166\n",
      "Training Epoch 72   1.6% | batch:         1 of        64\t|\tloss: 1412.66\n",
      "Training Epoch 72   3.1% | batch:         2 of        64\t|\tloss: 900.819\n",
      "Training Epoch 72   4.7% | batch:         3 of        64\t|\tloss: 736.332\n",
      "Training Epoch 72   6.2% | batch:         4 of        64\t|\tloss: 2245.88\n",
      "Training Epoch 72   7.8% | batch:         5 of        64\t|\tloss: 2584.88\n",
      "Training Epoch 72   9.4% | batch:         6 of        64\t|\tloss: 1101.74\n",
      "Training Epoch 72  10.9% | batch:         7 of        64\t|\tloss: 818.111\n",
      "Training Epoch 72  12.5% | batch:         8 of        64\t|\tloss: 816.908\n",
      "Training Epoch 72  14.1% | batch:         9 of        64\t|\tloss: 3007.46\n",
      "Training Epoch 72  15.6% | batch:        10 of        64\t|\tloss: 1142.84\n",
      "Training Epoch 72  17.2% | batch:        11 of        64\t|\tloss: 1172.89\n",
      "Training Epoch 72  18.8% | batch:        12 of        64\t|\tloss: 745.176\n",
      "Training Epoch 72  20.3% | batch:        13 of        64\t|\tloss: 910.248\n",
      "Training Epoch 72  21.9% | batch:        14 of        64\t|\tloss: 669.655\n",
      "Training Epoch 72  23.4% | batch:        15 of        64\t|\tloss: 1127.61\n",
      "Training Epoch 72  25.0% | batch:        16 of        64\t|\tloss: 1110.58\n",
      "Training Epoch 72  26.6% | batch:        17 of        64\t|\tloss: 859.456\n",
      "Training Epoch 72  28.1% | batch:        18 of        64\t|\tloss: 700.452\n",
      "Training Epoch 72  29.7% | batch:        19 of        64\t|\tloss: 3863.34\n",
      "Training Epoch 72  31.2% | batch:        20 of        64\t|\tloss: 1311.23\n",
      "Training Epoch 72  32.8% | batch:        21 of        64\t|\tloss: 1257.97\n",
      "Training Epoch 72  34.4% | batch:        22 of        64\t|\tloss: 2365.11\n",
      "Training Epoch 72  35.9% | batch:        23 of        64\t|\tloss: 862.545\n",
      "Training Epoch 72  37.5% | batch:        24 of        64\t|\tloss: 2063.24\n",
      "Training Epoch 72  39.1% | batch:        25 of        64\t|\tloss: 1673.02\n",
      "Training Epoch 72  40.6% | batch:        26 of        64\t|\tloss: 961.718\n",
      "Training Epoch 72  42.2% | batch:        27 of        64\t|\tloss: 809.622\n",
      "Training Epoch 72  43.8% | batch:        28 of        64\t|\tloss: 605.52\n",
      "Training Epoch 72  45.3% | batch:        29 of        64\t|\tloss: 735.629\n",
      "Training Epoch 72  46.9% | batch:        30 of        64\t|\tloss: 1412.87\n",
      "Training Epoch 72  48.4% | batch:        31 of        64\t|\tloss: 804.515\n",
      "Training Epoch 72  50.0% | batch:        32 of        64\t|\tloss: 598.358\n",
      "Training Epoch 72  51.6% | batch:        33 of        64\t|\tloss: 860.681\n",
      "Training Epoch 72  53.1% | batch:        34 of        64\t|\tloss: 1115.44\n",
      "Training Epoch 72  54.7% | batch:        35 of        64\t|\tloss: 1012.65\n",
      "Training Epoch 72  56.2% | batch:        36 of        64\t|\tloss: 1178.64\n",
      "Training Epoch 72  57.8% | batch:        37 of        64\t|\tloss: 900.889\n",
      "Training Epoch 72  59.4% | batch:        38 of        64\t|\tloss: 2516.59\n",
      "Training Epoch 72  60.9% | batch:        39 of        64\t|\tloss: 701.469\n",
      "Training Epoch 72  62.5% | batch:        40 of        64\t|\tloss: 845.28\n",
      "Training Epoch 72  64.1% | batch:        41 of        64\t|\tloss: 979.873\n",
      "Training Epoch 72  65.6% | batch:        42 of        64\t|\tloss: 768.317\n",
      "Training Epoch 72  67.2% | batch:        43 of        64\t|\tloss: 1128.02\n",
      "Training Epoch 72  68.8% | batch:        44 of        64\t|\tloss: 914.467\n",
      "Training Epoch 72  70.3% | batch:        45 of        64\t|\tloss: 728.805\n",
      "Training Epoch 72  71.9% | batch:        46 of        64\t|\tloss: 1128.95\n",
      "Training Epoch 72  73.4% | batch:        47 of        64\t|\tloss: 865.422\n",
      "Training Epoch 72  75.0% | batch:        48 of        64\t|\tloss: 1560.13\n",
      "Training Epoch 72  76.6% | batch:        49 of        64\t|\tloss: 1122.98\n",
      "Training Epoch 72  78.1% | batch:        50 of        64\t|\tloss: 1227.15\n",
      "Training Epoch 72  79.7% | batch:        51 of        64\t|\tloss: 1029.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:27,791 | INFO : Epoch 72 Training Summary: epoch: 72.000000 | loss: 1235.455691 | \n",
      "2023-05-10 17:09:27,791 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.167083978652954 seconds\n",
      "\n",
      "2023-05-10 17:09:27,792 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2017090850406222 seconds\n",
      "2023-05-10 17:09:27,793 | INFO : Avg batch train. time: 0.018776704453759722 seconds\n",
      "2023-05-10 17:09:27,793 | INFO : Avg sample train. time: 0.0002976000705895548 seconds\n",
      "2023-05-10 17:09:27,793 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 72  81.2% | batch:        52 of        64\t|\tloss: 1352.03\n",
      "Training Epoch 72  82.8% | batch:        53 of        64\t|\tloss: 1615.94\n",
      "Training Epoch 72  84.4% | batch:        54 of        64\t|\tloss: 1812.7\n",
      "Training Epoch 72  85.9% | batch:        55 of        64\t|\tloss: 1117.85\n",
      "Training Epoch 72  87.5% | batch:        56 of        64\t|\tloss: 821.304\n",
      "Training Epoch 72  89.1% | batch:        57 of        64\t|\tloss: 1281.55\n",
      "Training Epoch 72  90.6% | batch:        58 of        64\t|\tloss: 1770.96\n",
      "Training Epoch 72  92.2% | batch:        59 of        64\t|\tloss: 1056.43\n",
      "Training Epoch 72  93.8% | batch:        60 of        64\t|\tloss: 2291.39\n",
      "Training Epoch 72  95.3% | batch:        61 of        64\t|\tloss: 1043.54\n",
      "Training Epoch 72  96.9% | batch:        62 of        64\t|\tloss: 898.568\n",
      "Training Epoch 72  98.4% | batch:        63 of        64\t|\tloss: 2247.43\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:27,940 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.1463015079498291 seconds\n",
      "\n",
      "2023-05-10 17:09:27,941 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.16943156719207764 seconds\n",
      "2023-05-10 17:09:27,941 | INFO : Avg batch val. time: 0.010589472949504852 seconds\n",
      "2023-05-10 17:09:27,942 | INFO : Avg sample val. time: 0.00016775402692284914 seconds\n",
      "2023-05-10 17:09:27,942 | INFO : Epoch 72 Validation Summary: epoch: 72.000000 | loss: 1871.003195 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 72   0.0% | batch:         0 of        16\t|\tloss: 1653.54\n",
      "Evaluating Epoch 72   6.2% | batch:         1 of        16\t|\tloss: 881.867\n",
      "Evaluating Epoch 72  12.5% | batch:         2 of        16\t|\tloss: 953.481\n",
      "Evaluating Epoch 72  18.8% | batch:         3 of        16\t|\tloss: 1877.19\n",
      "Evaluating Epoch 72  25.0% | batch:         4 of        16\t|\tloss: 1651.27\n",
      "Evaluating Epoch 72  31.2% | batch:         5 of        16\t|\tloss: 1949.04\n",
      "Evaluating Epoch 72  37.5% | batch:         6 of        16\t|\tloss: 2032.22\n",
      "Evaluating Epoch 72  43.8% | batch:         7 of        16\t|\tloss: 2408.22\n",
      "Evaluating Epoch 72  50.0% | batch:         8 of        16\t|\tloss: 1613.37\n",
      "Evaluating Epoch 72  56.2% | batch:         9 of        16\t|\tloss: 5547.93\n",
      "Evaluating Epoch 72  62.5% | batch:        10 of        16\t|\tloss: 1520.84\n",
      "Evaluating Epoch 72  68.8% | batch:        11 of        16\t|\tloss: 1964.65\n",
      "Evaluating Epoch 72  75.0% | batch:        12 of        16\t|\tloss: 1189.63\n",
      "Evaluating Epoch 72  81.2% | batch:        13 of        16\t|\tloss: 1273.58\n",
      "Evaluating Epoch 72  87.5% | batch:        14 of        16\t|\tloss: 1481.75\n",
      "Evaluating Epoch 72  93.8% | batch:        15 of        16\t|\tloss: 1956.08\n",
      "\n",
      "Training Epoch 73   0.0% | batch:         0 of        64\t|\tloss: 936.66\n",
      "Training Epoch 73   1.6% | batch:         1 of        64\t|\tloss: 1315.98\n",
      "Training Epoch 73   3.1% | batch:         2 of        64\t|\tloss: 956.302\n",
      "Training Epoch 73   4.7% | batch:         3 of        64\t|\tloss: 1061.99\n",
      "Training Epoch 73   6.2% | batch:         4 of        64\t|\tloss: 1180.97\n",
      "Training Epoch 73   7.8% | batch:         5 of        64\t|\tloss: 932.116\n",
      "Training Epoch 73   9.4% | batch:         6 of        64\t|\tloss: 2113.47\n",
      "Training Epoch 73  10.9% | batch:         7 of        64\t|\tloss: 599.928\n",
      "Training Epoch 73  12.5% | batch:         8 of        64\t|\tloss: 1206.5\n",
      "Training Epoch 73  14.1% | batch:         9 of        64\t|\tloss: 739.642\n",
      "Training Epoch 73  15.6% | batch:        10 of        64\t|\tloss: 2294.97\n",
      "Training Epoch 73  17.2% | batch:        11 of        64\t|\tloss: 582.045\n",
      "Training Epoch 73  18.8% | batch:        12 of        64\t|\tloss: 737.714\n",
      "Training Epoch 73  20.3% | batch:        13 of        64\t|\tloss: 838.443\n",
      "Training Epoch 73  21.9% | batch:        14 of        64\t|\tloss: 871.172\n",
      "Training Epoch 73  23.4% | batch:        15 of        64\t|\tloss: 1230.53\n",
      "Training Epoch 73  25.0% | batch:        16 of        64\t|\tloss: 1263.91\n",
      "Training Epoch 73  26.6% | batch:        17 of        64\t|\tloss: 624.052\n",
      "Training Epoch 73  28.1% | batch:        18 of        64\t|\tloss: 3282.5\n",
      "Training Epoch 73  29.7% | batch:        19 of        64\t|\tloss: 1363.2\n",
      "Training Epoch 73  31.2% | batch:        20 of        64\t|\tloss: 3062.4\n",
      "Training Epoch 73  32.8% | batch:        21 of        64\t|\tloss: 965.161\n",
      "Training Epoch 73  34.4% | batch:        22 of        64\t|\tloss: 576.508\n",
      "Training Epoch 73  35.9% | batch:        23 of        64\t|\tloss: 2383.67\n",
      "Training Epoch 73  37.5% | batch:        24 of        64\t|\tloss: 1037.54\n",
      "Training Epoch 73  39.1% | batch:        25 of        64\t|\tloss: 914.65\n",
      "Training Epoch 73  40.6% | batch:        26 of        64\t|\tloss: 2456.99\n",
      "Training Epoch 73  42.2% | batch:        27 of        64\t|\tloss: 1332.98\n",
      "Training Epoch 73  43.8% | batch:        28 of        64\t|\tloss: 1388.53\n",
      "Training Epoch 73  45.3% | batch:        29 of        64\t|\tloss: 1505.45\n",
      "Training Epoch 73  46.9% | batch:        30 of        64\t|\tloss: 745.871\n",
      "Training Epoch 73  48.4% | batch:        31 of        64\t|\tloss: 666.251\n",
      "Training Epoch 73  50.0% | batch:        32 of        64\t|\tloss: 792.319\n",
      "Training Epoch 73  51.6% | batch:        33 of        64\t|\tloss: 1236.86\n",
      "Training Epoch 73  53.1% | batch:        34 of        64\t|\tloss: 919.425\n",
      "Training Epoch 73  54.7% | batch:        35 of        64\t|\tloss: 771.802\n",
      "Training Epoch 73  56.2% | batch:        36 of        64\t|\tloss: 1323.08\n",
      "Training Epoch 73  57.8% | batch:        37 of        64\t|\tloss: 1339.24\n",
      "Training Epoch 73  59.4% | batch:        38 of        64\t|\tloss: 928.928\n",
      "Training Epoch 73  60.9% | batch:        39 of        64\t|\tloss: 967.334\n",
      "Training Epoch 73  62.5% | batch:        40 of        64\t|\tloss: 653.479\n",
      "Training Epoch 73  64.1% | batch:        41 of        64\t|\tloss: 812.356\n",
      "Training Epoch 73  65.6% | batch:        42 of        64\t|\tloss: 1177.68\n",
      "Training Epoch 73  67.2% | batch:        43 of        64\t|\tloss: 684.964\n",
      "Training Epoch 73  68.8% | batch:        44 of        64\t|\tloss: 1629.66\n",
      "Training Epoch 73  70.3% | batch:        45 of        64\t|\tloss: 1033.95\n",
      "Training Epoch 73  71.9% | batch:        46 of        64\t|\tloss: 1434.68\n",
      "Training Epoch 73  73.4% | batch:        47 of        64\t|\tloss: 955.135\n",
      "Training Epoch 73  75.0% | batch:        48 of        64\t|\tloss: 949.659\n",
      "Training Epoch 73  76.6% | batch:        49 of        64\t|\tloss: 845.759\n",
      "Training Epoch 73  78.1% | batch:        50 of        64\t|\tloss: 1068.17\n",
      "Training Epoch 73  79.7% | batch:        51 of        64\t|\tloss: 1539.19\n",
      "Training Epoch 73  81.2% | batch:        52 of        64\t|\tloss: 778.874\n",
      "Training Epoch 73  82.8% | batch:        53 of        64\t|\tloss: 880.059\n",
      "Training Epoch 73  84.4% | batch:        54 of        64\t|\tloss: 711.229\n",
      "Training Epoch 73  85.9% | batch:        55 of        64\t|\tloss: 893.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:29,141 | INFO : Epoch 73 Training Summary: epoch: 73.000000 | loss: 1269.910069 | \n",
      "2023-05-10 17:09:29,142 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1895177364349365 seconds\n",
      "\n",
      "2023-05-10 17:09:29,142 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.201542080265202 seconds\n",
      "2023-05-10 17:09:29,143 | INFO : Avg batch train. time: 0.01877409500414378 seconds\n",
      "2023-05-10 17:09:29,144 | INFO : Avg sample train. time: 0.00029755871229945565 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 73  87.5% | batch:        56 of        64\t|\tloss: 587.851\n",
      "Training Epoch 73  89.1% | batch:        57 of        64\t|\tloss: 4346.33\n",
      "Training Epoch 73  90.6% | batch:        58 of        64\t|\tloss: 989.601\n",
      "Training Epoch 73  92.2% | batch:        59 of        64\t|\tloss: 1144.02\n",
      "Training Epoch 73  93.8% | batch:        60 of        64\t|\tloss: 1475.78\n",
      "Training Epoch 73  95.3% | batch:        61 of        64\t|\tloss: 2327.8\n",
      "Training Epoch 73  96.9% | batch:        62 of        64\t|\tloss: 3513.31\n",
      "Training Epoch 73  98.4% | batch:        63 of        64\t|\tloss: 2597.38\n",
      "\n",
      "Training Epoch 74   0.0% | batch:         0 of        64\t|\tloss: 1175.68\n",
      "Training Epoch 74   1.6% | batch:         1 of        64\t|\tloss: 787.503\n",
      "Training Epoch 74   3.1% | batch:         2 of        64\t|\tloss: 1489.89\n",
      "Training Epoch 74   4.7% | batch:         3 of        64\t|\tloss: 1421.82\n",
      "Training Epoch 74   6.2% | batch:         4 of        64\t|\tloss: 928.215\n",
      "Training Epoch 74   7.8% | batch:         5 of        64\t|\tloss: 1467.68\n",
      "Training Epoch 74   9.4% | batch:         6 of        64\t|\tloss: 1048.43\n",
      "Training Epoch 74  10.9% | batch:         7 of        64\t|\tloss: 756.443\n",
      "Training Epoch 74  12.5% | batch:         8 of        64\t|\tloss: 1283.88\n",
      "Training Epoch 74  14.1% | batch:         9 of        64\t|\tloss: 740.072\n",
      "Training Epoch 74  15.6% | batch:        10 of        64\t|\tloss: 1775.97\n",
      "Training Epoch 74  17.2% | batch:        11 of        64\t|\tloss: 722.971\n",
      "Training Epoch 74  18.8% | batch:        12 of        64\t|\tloss: 1035.71\n",
      "Training Epoch 74  20.3% | batch:        13 of        64\t|\tloss: 978.277\n",
      "Training Epoch 74  21.9% | batch:        14 of        64\t|\tloss: 744.489\n",
      "Training Epoch 74  23.4% | batch:        15 of        64\t|\tloss: 852.618\n",
      "Training Epoch 74  25.0% | batch:        16 of        64\t|\tloss: 845.015\n",
      "Training Epoch 74  26.6% | batch:        17 of        64\t|\tloss: 1370.76\n",
      "Training Epoch 74  28.1% | batch:        18 of        64\t|\tloss: 611.244\n",
      "Training Epoch 74  29.7% | batch:        19 of        64\t|\tloss: 793.477\n",
      "Training Epoch 74  31.2% | batch:        20 of        64\t|\tloss: 882.176\n",
      "Training Epoch 74  32.8% | batch:        21 of        64\t|\tloss: 1191.4\n",
      "Training Epoch 74  34.4% | batch:        22 of        64\t|\tloss: 700.923\n",
      "Training Epoch 74  35.9% | batch:        23 of        64\t|\tloss: 865.416\n",
      "Training Epoch 74  37.5% | batch:        24 of        64\t|\tloss: 982.926\n",
      "Training Epoch 74  39.1% | batch:        25 of        64\t|\tloss: 1309.56\n",
      "Training Epoch 74  40.6% | batch:        26 of        64\t|\tloss: 2814.16\n",
      "Training Epoch 74  42.2% | batch:        27 of        64\t|\tloss: 820.023\n",
      "Training Epoch 74  43.8% | batch:        28 of        64\t|\tloss: 1009.96\n",
      "Training Epoch 74  45.3% | batch:        29 of        64\t|\tloss: 3249.42\n",
      "Training Epoch 74  46.9% | batch:        30 of        64\t|\tloss: 688.341\n",
      "Training Epoch 74  48.4% | batch:        31 of        64\t|\tloss: 1298.1\n",
      "Training Epoch 74  50.0% | batch:        32 of        64\t|\tloss: 2615.24\n",
      "Training Epoch 74  51.6% | batch:        33 of        64\t|\tloss: 736.491\n",
      "Training Epoch 74  53.1% | batch:        34 of        64\t|\tloss: 2080.93\n",
      "Training Epoch 74  54.7% | batch:        35 of        64\t|\tloss: 1284.58\n",
      "Training Epoch 74  56.2% | batch:        36 of        64\t|\tloss: 1542.65\n",
      "Training Epoch 74  57.8% | batch:        37 of        64\t|\tloss: 1177.57\n",
      "Training Epoch 74  59.4% | batch:        38 of        64\t|\tloss: 1339.89\n",
      "Training Epoch 74  60.9% | batch:        39 of        64\t|\tloss: 847.73\n",
      "Training Epoch 74  62.5% | batch:        40 of        64\t|\tloss: 1089.2\n",
      "Training Epoch 74  64.1% | batch:        41 of        64\t|\tloss: 684.186\n",
      "Training Epoch 74  65.6% | batch:        42 of        64\t|\tloss: 1378.97\n",
      "Training Epoch 74  67.2% | batch:        43 of        64\t|\tloss: 441.307\n",
      "Training Epoch 74  68.8% | batch:        44 of        64\t|\tloss: 1102.73\n",
      "Training Epoch 74  70.3% | batch:        45 of        64\t|\tloss: 1518.4\n",
      "Training Epoch 74  71.9% | batch:        46 of        64\t|\tloss: 1176.77\n",
      "Training Epoch 74  73.4% | batch:        47 of        64\t|\tloss: 1357.49\n",
      "Training Epoch 74  75.0% | batch:        48 of        64\t|\tloss: 1445.82\n",
      "Training Epoch 74  76.6% | batch:        49 of        64\t|\tloss: 1104.46\n",
      "Training Epoch 74  78.1% | batch:        50 of        64\t|\tloss: 905.806\n",
      "Training Epoch 74  79.7% | batch:        51 of        64\t|\tloss: 1927.46\n",
      "Training Epoch 74  81.2% | batch:        52 of        64\t|\tloss: 502.052\n",
      "Training Epoch 74  82.8% | batch:        53 of        64\t|\tloss: 2302.06\n",
      "Training Epoch 74  84.4% | batch:        54 of        64\t|\tloss: 1026.57\n",
      "Training Epoch 74  85.9% | batch:        55 of        64\t|\tloss: 1044.53\n",
      "Training Epoch 74  87.5% | batch:        56 of        64\t|\tloss: 924.672\n",
      "Training Epoch 74  89.1% | batch:        57 of        64\t|\tloss: 1133.68\n",
      "Training Epoch 74  90.6% | batch:        58 of        64\t|\tloss: 1797.09\n",
      "Training Epoch 74  92.2% | batch:        59 of        64\t|\tloss: 748.337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:30,335 | INFO : Epoch 74 Training Summary: epoch: 74.000000 | loss: 1185.367171 | \n",
      "2023-05-10 17:09:30,336 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1824672222137451 seconds\n",
      "\n",
      "2023-05-10 17:09:30,336 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2012843119131553 seconds\n",
      "2023-05-10 17:09:30,337 | INFO : Avg batch train. time: 0.018770067373643052 seconds\n",
      "2023-05-10 17:09:30,337 | INFO : Avg sample train. time: 0.0002974948766501128 seconds\n",
      "2023-05-10 17:09:30,338 | INFO : Evaluating on validation set ...\n",
      "2023-05-10 17:09:30,484 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.1459355354309082 seconds\n",
      "\n",
      "2023-05-10 17:09:30,485 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.16882910483922714 seconds\n",
      "2023-05-10 17:09:30,485 | INFO : Avg batch val. time: 0.010551819052451696 seconds\n",
      "2023-05-10 17:09:30,486 | INFO : Avg sample val. time: 0.00016715752954378924 seconds\n",
      "2023-05-10 17:09:30,486 | INFO : Epoch 74 Validation Summary: epoch: 74.000000 | loss: 1933.854966 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 74  93.8% | batch:        60 of        64\t|\tloss: 907.704\n",
      "Training Epoch 74  95.3% | batch:        61 of        64\t|\tloss: 725.987\n",
      "Training Epoch 74  96.9% | batch:        62 of        64\t|\tloss: 960.19\n",
      "Training Epoch 74  98.4% | batch:        63 of        64\t|\tloss: 3372.25\n",
      "\n",
      "Evaluating Epoch 74   0.0% | batch:         0 of        16\t|\tloss: 2014.12\n",
      "Evaluating Epoch 74   6.2% | batch:         1 of        16\t|\tloss: 1149.77\n",
      "Evaluating Epoch 74  12.5% | batch:         2 of        16\t|\tloss: 1177.27\n",
      "Evaluating Epoch 74  18.8% | batch:         3 of        16\t|\tloss: 1605.61\n",
      "Evaluating Epoch 74  25.0% | batch:         4 of        16\t|\tloss: 1578.73\n",
      "Evaluating Epoch 74  31.2% | batch:         5 of        16\t|\tloss: 1907.96\n",
      "Evaluating Epoch 74  37.5% | batch:         6 of        16\t|\tloss: 2873.04\n",
      "Evaluating Epoch 74  43.8% | batch:         7 of        16\t|\tloss: 3029.55\n",
      "Evaluating Epoch 74  50.0% | batch:         8 of        16\t|\tloss: 1333.82\n",
      "Evaluating Epoch 74  56.2% | batch:         9 of        16\t|\tloss: 3656.45\n",
      "Evaluating Epoch 74  62.5% | batch:        10 of        16\t|\tloss: 1751.43\n",
      "Evaluating Epoch 74  68.8% | batch:        11 of        16\t|\tloss: 2344.32\n",
      "Evaluating Epoch 74  75.0% | batch:        12 of        16\t|\tloss: 1264.56\n",
      "Evaluating Epoch 74  81.2% | batch:        13 of        16\t|\tloss: 1163.65\n",
      "Evaluating Epoch 74  87.5% | batch:        14 of        16\t|\tloss: 1781.1\n",
      "Evaluating Epoch 74  93.8% | batch:        15 of        16\t|\tloss: 2415.73\n",
      "\n",
      "Training Epoch 75   0.0% | batch:         0 of        64\t|\tloss: 782.936\n",
      "Training Epoch 75   1.6% | batch:         1 of        64\t|\tloss: 653.23\n",
      "Training Epoch 75   3.1% | batch:         2 of        64\t|\tloss: 838.842\n",
      "Training Epoch 75   4.7% | batch:         3 of        64\t|\tloss: 580.407\n",
      "Training Epoch 75   6.2% | batch:         4 of        64\t|\tloss: 665.541\n",
      "Training Epoch 75   7.8% | batch:         5 of        64\t|\tloss: 1007.62\n",
      "Training Epoch 75   9.4% | batch:         6 of        64\t|\tloss: 1199.33\n",
      "Training Epoch 75  10.9% | batch:         7 of        64\t|\tloss: 1311.7\n",
      "Training Epoch 75  12.5% | batch:         8 of        64\t|\tloss: 1805.13\n",
      "Training Epoch 75  14.1% | batch:         9 of        64\t|\tloss: 853.165\n",
      "Training Epoch 75  15.6% | batch:        10 of        64\t|\tloss: 594.112\n",
      "Training Epoch 75  17.2% | batch:        11 of        64\t|\tloss: 1108.5\n",
      "Training Epoch 75  18.8% | batch:        12 of        64\t|\tloss: 1097.43\n",
      "Training Epoch 75  20.3% | batch:        13 of        64\t|\tloss: 1231.35\n",
      "Training Epoch 75  21.9% | batch:        14 of        64\t|\tloss: 1033.24\n",
      "Training Epoch 75  23.4% | batch:        15 of        64\t|\tloss: 547.002\n",
      "Training Epoch 75  25.0% | batch:        16 of        64\t|\tloss: 1579.45\n",
      "Training Epoch 75  26.6% | batch:        17 of        64\t|\tloss: 677.829\n",
      "Training Epoch 75  28.1% | batch:        18 of        64\t|\tloss: 3051.13\n",
      "Training Epoch 75  29.7% | batch:        19 of        64\t|\tloss: 1130.11\n",
      "Training Epoch 75  31.2% | batch:        20 of        64\t|\tloss: 1097.11\n",
      "Training Epoch 75  32.8% | batch:        21 of        64\t|\tloss: 3425.56\n",
      "Training Epoch 75  34.4% | batch:        22 of        64\t|\tloss: 951.941\n",
      "Training Epoch 75  35.9% | batch:        23 of        64\t|\tloss: 1408.57\n",
      "Training Epoch 75  37.5% | batch:        24 of        64\t|\tloss: 764.18\n",
      "Training Epoch 75  39.1% | batch:        25 of        64\t|\tloss: 1479.52\n",
      "Training Epoch 75  40.6% | batch:        26 of        64\t|\tloss: 864.596\n",
      "Training Epoch 75  42.2% | batch:        27 of        64\t|\tloss: 922.871\n",
      "Training Epoch 75  43.8% | batch:        28 of        64\t|\tloss: 609.29\n",
      "Training Epoch 75  45.3% | batch:        29 of        64\t|\tloss: 818.4\n",
      "Training Epoch 75  46.9% | batch:        30 of        64\t|\tloss: 1055.2\n",
      "Training Epoch 75  48.4% | batch:        31 of        64\t|\tloss: 787.735\n",
      "Training Epoch 75  50.0% | batch:        32 of        64\t|\tloss: 2434.57\n",
      "Training Epoch 75  51.6% | batch:        33 of        64\t|\tloss: 2014.96\n",
      "Training Epoch 75  53.1% | batch:        34 of        64\t|\tloss: 2260.37\n",
      "Training Epoch 75  54.7% | batch:        35 of        64\t|\tloss: 893.486\n",
      "Training Epoch 75  56.2% | batch:        36 of        64\t|\tloss: 1340.05\n",
      "Training Epoch 75  57.8% | batch:        37 of        64\t|\tloss: 1118.84\n",
      "Training Epoch 75  59.4% | batch:        38 of        64\t|\tloss: 690.747\n",
      "Training Epoch 75  60.9% | batch:        39 of        64\t|\tloss: 551.424\n",
      "Training Epoch 75  62.5% | batch:        40 of        64\t|\tloss: 810.52\n",
      "Training Epoch 75  64.1% | batch:        41 of        64\t|\tloss: 1162.52\n",
      "Training Epoch 75  65.6% | batch:        42 of        64\t|\tloss: 1041.7\n",
      "Training Epoch 75  67.2% | batch:        43 of        64\t|\tloss: 841.096\n",
      "Training Epoch 75  68.8% | batch:        44 of        64\t|\tloss: 745.943\n",
      "Training Epoch 75  70.3% | batch:        45 of        64\t|\tloss: 2124.04\n",
      "Training Epoch 75  71.9% | batch:        46 of        64\t|\tloss: 772.966\n",
      "Training Epoch 75  73.4% | batch:        47 of        64\t|\tloss: 645.056\n",
      "Training Epoch 75  75.0% | batch:        48 of        64\t|\tloss: 1256.77\n",
      "Training Epoch 75  76.6% | batch:        49 of        64\t|\tloss: 993.589\n",
      "Training Epoch 75  78.1% | batch:        50 of        64\t|\tloss: 678.435\n",
      "Training Epoch 75  79.7% | batch:        51 of        64\t|\tloss: 1175.63\n",
      "Training Epoch 75  81.2% | batch:        52 of        64\t|\tloss: 954.795\n",
      "Training Epoch 75  82.8% | batch:        53 of        64\t|\tloss: 617.465\n",
      "Training Epoch 75  84.4% | batch:        54 of        64\t|\tloss: 3403.29\n",
      "Training Epoch 75  85.9% | batch:        55 of        64\t|\tloss: 1547.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:31,686 | INFO : Epoch 75 Training Summary: epoch: 75.000000 | loss: 1164.756462 | \n",
      "2023-05-10 17:09:31,687 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1903066635131836 seconds\n",
      "\n",
      "2023-05-10 17:09:31,688 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2011379432678222 seconds\n",
      "2023-05-10 17:09:31,688 | INFO : Avg batch train. time: 0.018767780363559722 seconds\n",
      "2023-05-10 17:09:31,688 | INFO : Avg sample train. time: 0.00029745862884294756 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 75  87.5% | batch:        56 of        64\t|\tloss: 632.157\n",
      "Training Epoch 75  89.1% | batch:        57 of        64\t|\tloss: 904.917\n",
      "Training Epoch 75  90.6% | batch:        58 of        64\t|\tloss: 901.164\n",
      "Training Epoch 75  92.2% | batch:        59 of        64\t|\tloss: 1759.07\n",
      "Training Epoch 75  93.8% | batch:        60 of        64\t|\tloss: 1194.44\n",
      "Training Epoch 75  95.3% | batch:        61 of        64\t|\tloss: 1381.13\n",
      "Training Epoch 75  96.9% | batch:        62 of        64\t|\tloss: 554.036\n",
      "Training Epoch 75  98.4% | batch:        63 of        64\t|\tloss: 1632.02\n",
      "\n",
      "Training Epoch 76   0.0% | batch:         0 of        64\t|\tloss: 712.094\n",
      "Training Epoch 76   1.6% | batch:         1 of        64\t|\tloss: 938.248\n",
      "Training Epoch 76   3.1% | batch:         2 of        64\t|\tloss: 1447.01\n",
      "Training Epoch 76   4.7% | batch:         3 of        64\t|\tloss: 892.561\n",
      "Training Epoch 76   6.2% | batch:         4 of        64\t|\tloss: 614.244\n",
      "Training Epoch 76   7.8% | batch:         5 of        64\t|\tloss: 1071.54\n",
      "Training Epoch 76   9.4% | batch:         6 of        64\t|\tloss: 773.829\n",
      "Training Epoch 76  10.9% | batch:         7 of        64\t|\tloss: 1047.18\n",
      "Training Epoch 76  12.5% | batch:         8 of        64\t|\tloss: 1285.22\n",
      "Training Epoch 76  14.1% | batch:         9 of        64\t|\tloss: 901.435\n",
      "Training Epoch 76  15.6% | batch:        10 of        64\t|\tloss: 1552.3\n",
      "Training Epoch 76  17.2% | batch:        11 of        64\t|\tloss: 811.014\n",
      "Training Epoch 76  18.8% | batch:        12 of        64\t|\tloss: 738.904\n",
      "Training Epoch 76  20.3% | batch:        13 of        64\t|\tloss: 1007.66\n",
      "Training Epoch 76  21.9% | batch:        14 of        64\t|\tloss: 802.314\n",
      "Training Epoch 76  23.4% | batch:        15 of        64\t|\tloss: 847.406\n",
      "Training Epoch 76  25.0% | batch:        16 of        64\t|\tloss: 1242.36\n",
      "Training Epoch 76  26.6% | batch:        17 of        64\t|\tloss: 1380.47\n",
      "Training Epoch 76  28.1% | batch:        18 of        64\t|\tloss: 848.781\n",
      "Training Epoch 76  29.7% | batch:        19 of        64\t|\tloss: 504.617\n",
      "Training Epoch 76  31.2% | batch:        20 of        64\t|\tloss: 1107.96\n",
      "Training Epoch 76  32.8% | batch:        21 of        64\t|\tloss: 1103.57\n",
      "Training Epoch 76  34.4% | batch:        22 of        64\t|\tloss: 2357.35\n",
      "Training Epoch 76  35.9% | batch:        23 of        64\t|\tloss: 1532.32\n",
      "Training Epoch 76  37.5% | batch:        24 of        64\t|\tloss: 691.035\n",
      "Training Epoch 76  39.1% | batch:        25 of        64\t|\tloss: 1947.32\n",
      "Training Epoch 76  40.6% | batch:        26 of        64\t|\tloss: 914.18\n",
      "Training Epoch 76  42.2% | batch:        27 of        64\t|\tloss: 718.005\n",
      "Training Epoch 76  43.8% | batch:        28 of        64\t|\tloss: 894.744\n",
      "Training Epoch 76  45.3% | batch:        29 of        64\t|\tloss: 1172.67\n",
      "Training Epoch 76  46.9% | batch:        30 of        64\t|\tloss: 754.792\n",
      "Training Epoch 76  48.4% | batch:        31 of        64\t|\tloss: 1022.79\n",
      "Training Epoch 76  50.0% | batch:        32 of        64\t|\tloss: 1194.68\n",
      "Training Epoch 76  51.6% | batch:        33 of        64\t|\tloss: 886.14\n",
      "Training Epoch 76  53.1% | batch:        34 of        64\t|\tloss: 950.791\n",
      "Training Epoch 76  54.7% | batch:        35 of        64\t|\tloss: 1281.4\n",
      "Training Epoch 76  56.2% | batch:        36 of        64\t|\tloss: 857.186\n",
      "Training Epoch 76  57.8% | batch:        37 of        64\t|\tloss: 1003.55\n",
      "Training Epoch 76  59.4% | batch:        38 of        64\t|\tloss: 685.548\n",
      "Training Epoch 76  60.9% | batch:        39 of        64\t|\tloss: 776.268\n",
      "Training Epoch 76  62.5% | batch:        40 of        64\t|\tloss: 4191.96\n",
      "Training Epoch 76  64.1% | batch:        41 of        64\t|\tloss: 885.939\n",
      "Training Epoch 76  65.6% | batch:        42 of        64\t|\tloss: 2788\n",
      "Training Epoch 76  67.2% | batch:        43 of        64\t|\tloss: 765.785\n",
      "Training Epoch 76  68.8% | batch:        44 of        64\t|\tloss: 1913.93\n",
      "Training Epoch 76  70.3% | batch:        45 of        64\t|\tloss: 699.848\n",
      "Training Epoch 76  71.9% | batch:        46 of        64\t|\tloss: 800.969\n",
      "Training Epoch 76  73.4% | batch:        47 of        64\t|\tloss: 850.472\n",
      "Training Epoch 76  75.0% | batch:        48 of        64\t|\tloss: 951.638\n",
      "Training Epoch 76  76.6% | batch:        49 of        64\t|\tloss: 714.102\n",
      "Training Epoch 76  78.1% | batch:        50 of        64\t|\tloss: 1727.06\n",
      "Training Epoch 76  79.7% | batch:        51 of        64\t|\tloss: 1139.1\n",
      "Training Epoch 76  81.2% | batch:        52 of        64\t|\tloss: 1784.21\n",
      "Training Epoch 76  82.8% | batch:        53 of        64\t|\tloss: 1980.17\n",
      "Training Epoch 76  84.4% | batch:        54 of        64\t|\tloss: 2595.63\n",
      "Training Epoch 76  85.9% | batch:        55 of        64\t|\tloss: 916.646\n",
      "Training Epoch 76  87.5% | batch:        56 of        64\t|\tloss: 743.386\n",
      "Training Epoch 76  89.1% | batch:        57 of        64\t|\tloss: 1365.92\n",
      "Training Epoch 76  90.6% | batch:        58 of        64\t|\tloss: 974.107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:32,877 | INFO : Epoch 76 Training Summary: epoch: 76.000000 | loss: 1176.785311 | \n",
      "2023-05-10 17:09:32,877 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1791203022003174 seconds\n",
      "\n",
      "2023-05-10 17:09:32,878 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2008482374643024 seconds\n",
      "2023-05-10 17:09:32,879 | INFO : Avg batch train. time: 0.018763253710379724 seconds\n",
      "2023-05-10 17:09:32,879 | INFO : Avg sample train. time: 0.000297386883968376 seconds\n",
      "2023-05-10 17:09:32,880 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 76  92.2% | batch:        59 of        64\t|\tloss: 663.698\n",
      "Training Epoch 76  93.8% | batch:        60 of        64\t|\tloss: 1592.05\n",
      "Training Epoch 76  95.3% | batch:        61 of        64\t|\tloss: 1980.7\n",
      "Training Epoch 76  96.9% | batch:        62 of        64\t|\tloss: 815.06\n",
      "Training Epoch 76  98.4% | batch:        63 of        64\t|\tloss: 1450.34\n",
      "\n",
      "Evaluating Epoch 76   0.0% | batch:         0 of        16\t|\tloss: 1792.08\n",
      "Evaluating Epoch 76   6.2% | batch:         1 of        16\t|\tloss: 1395.36\n",
      "Evaluating Epoch 76  12.5% | batch:         2 of        16\t|\tloss: 1002.58\n",
      "Evaluating Epoch 76  18.8% | batch:         3 of        16\t|\tloss: 1490.94\n",
      "Evaluating Epoch 76  25.0% | batch:         4 of        16\t|\tloss: 1450.53\n",
      "Evaluating Epoch 76  31.2% | batch:         5 of        16\t|\tloss: 2009.33\n",
      "Evaluating Epoch 76  37.5% | batch:         6 of        16\t|\tloss: 2784\n",
      "Evaluating Epoch 76  43.8% | batch:         7 of        16\t|\tloss: 2459.52\n",
      "Evaluating Epoch 76  50.0% | batch:         8 of        16\t|\tloss: 1451.27\n",
      "Evaluating Epoch 76  56.2% | batch:         9 of        16\t|\tloss: 5937.69\n",
      "Evaluating Epoch 76  62.5% | batch:        10 of        16\t|\tloss: 1919.92\n",
      "Evaluating Epoch 76  68.8% | batch:        11 of        16\t|\tloss: 2167.99\n",
      "Evaluating Epoch 76  75.0% | batch:        12 of        16\t|\tloss: 1190.24\n",
      "Evaluating Epoch 76  81.2% | batch:        13 of        16\t|\tloss: 1247.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:33,027 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14698433876037598 seconds\n",
      "\n",
      "2023-05-10 17:09:33,028 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.16828298568725586 seconds\n",
      "2023-05-10 17:09:33,028 | INFO : Avg batch val. time: 0.010517686605453491 seconds\n",
      "2023-05-10 17:09:33,029 | INFO : Avg sample val. time: 0.0001666168175121345 seconds\n",
      "2023-05-10 17:09:33,029 | INFO : Epoch 76 Validation Summary: epoch: 76.000000 | loss: 2041.226199 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 76  87.5% | batch:        14 of        16\t|\tloss: 1775.67\n",
      "Evaluating Epoch 76  93.8% | batch:        15 of        16\t|\tloss: 2737.56\n",
      "\n",
      "Training Epoch 77   0.0% | batch:         0 of        64\t|\tloss: 875.444\n",
      "Training Epoch 77   1.6% | batch:         1 of        64\t|\tloss: 1564.1\n",
      "Training Epoch 77   3.1% | batch:         2 of        64\t|\tloss: 2517.43\n",
      "Training Epoch 77   4.7% | batch:         3 of        64\t|\tloss: 1087.61\n",
      "Training Epoch 77   6.2% | batch:         4 of        64\t|\tloss: 1060.77\n",
      "Training Epoch 77   7.8% | batch:         5 of        64\t|\tloss: 802.268\n",
      "Training Epoch 77   9.4% | batch:         6 of        64\t|\tloss: 1702.83\n",
      "Training Epoch 77  10.9% | batch:         7 of        64\t|\tloss: 838.366\n",
      "Training Epoch 77  12.5% | batch:         8 of        64\t|\tloss: 1799.07\n",
      "Training Epoch 77  14.1% | batch:         9 of        64\t|\tloss: 1846.01\n",
      "Training Epoch 77  15.6% | batch:        10 of        64\t|\tloss: 746.182\n",
      "Training Epoch 77  17.2% | batch:        11 of        64\t|\tloss: 1548.25\n",
      "Training Epoch 77  18.8% | batch:        12 of        64\t|\tloss: 730.304\n",
      "Training Epoch 77  20.3% | batch:        13 of        64\t|\tloss: 1170.94\n",
      "Training Epoch 77  21.9% | batch:        14 of        64\t|\tloss: 1068.71\n",
      "Training Epoch 77  23.4% | batch:        15 of        64\t|\tloss: 2129.24\n",
      "Training Epoch 77  25.0% | batch:        16 of        64\t|\tloss: 1014.75\n",
      "Training Epoch 77  26.6% | batch:        17 of        64\t|\tloss: 608.074\n",
      "Training Epoch 77  28.1% | batch:        18 of        64\t|\tloss: 1063.64\n",
      "Training Epoch 77  29.7% | batch:        19 of        64\t|\tloss: 1583.5\n",
      "Training Epoch 77  31.2% | batch:        20 of        64\t|\tloss: 915.255\n",
      "Training Epoch 77  32.8% | batch:        21 of        64\t|\tloss: 808.599\n",
      "Training Epoch 77  34.4% | batch:        22 of        64\t|\tloss: 745.52\n",
      "Training Epoch 77  35.9% | batch:        23 of        64\t|\tloss: 764.931\n",
      "Training Epoch 77  37.5% | batch:        24 of        64\t|\tloss: 899.313\n",
      "Training Epoch 77  39.1% | batch:        25 of        64\t|\tloss: 1436.34\n",
      "Training Epoch 77  40.6% | batch:        26 of        64\t|\tloss: 1529.42\n",
      "Training Epoch 77  42.2% | batch:        27 of        64\t|\tloss: 1012.39\n",
      "Training Epoch 77  43.8% | batch:        28 of        64\t|\tloss: 1446.11\n",
      "Training Epoch 77  45.3% | batch:        29 of        64\t|\tloss: 702.774\n",
      "Training Epoch 77  46.9% | batch:        30 of        64\t|\tloss: 1083.12\n",
      "Training Epoch 77  48.4% | batch:        31 of        64\t|\tloss: 940.306\n",
      "Training Epoch 77  50.0% | batch:        32 of        64\t|\tloss: 692.549\n",
      "Training Epoch 77  51.6% | batch:        33 of        64\t|\tloss: 625.933\n",
      "Training Epoch 77  53.1% | batch:        34 of        64\t|\tloss: 1001.91\n",
      "Training Epoch 77  54.7% | batch:        35 of        64\t|\tloss: 578.196\n",
      "Training Epoch 77  56.2% | batch:        36 of        64\t|\tloss: 1270.04\n",
      "Training Epoch 77  57.8% | batch:        37 of        64\t|\tloss: 2215\n",
      "Training Epoch 77  59.4% | batch:        38 of        64\t|\tloss: 1589.63\n",
      "Training Epoch 77  60.9% | batch:        39 of        64\t|\tloss: 914.999\n",
      "Training Epoch 77  62.5% | batch:        40 of        64\t|\tloss: 2493.69\n",
      "Training Epoch 77  64.1% | batch:        41 of        64\t|\tloss: 1043.09\n",
      "Training Epoch 77  65.6% | batch:        42 of        64\t|\tloss: 1685.98\n",
      "Training Epoch 77  67.2% | batch:        43 of        64\t|\tloss: 1248.81\n",
      "Training Epoch 77  68.8% | batch:        44 of        64\t|\tloss: 1338.97\n",
      "Training Epoch 77  70.3% | batch:        45 of        64\t|\tloss: 1162.74\n",
      "Training Epoch 77  71.9% | batch:        46 of        64\t|\tloss: 1036.84\n",
      "Training Epoch 77  73.4% | batch:        47 of        64\t|\tloss: 995.538\n",
      "Training Epoch 77  75.0% | batch:        48 of        64\t|\tloss: 711.327\n",
      "Training Epoch 77  76.6% | batch:        49 of        64\t|\tloss: 1150.56\n",
      "Training Epoch 77  78.1% | batch:        50 of        64\t|\tloss: 873.954\n",
      "Training Epoch 77  79.7% | batch:        51 of        64\t|\tloss: 1283.22\n",
      "Training Epoch 77  81.2% | batch:        52 of        64\t|\tloss: 935.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:34,224 | INFO : Epoch 77 Training Summary: epoch: 77.000000 | loss: 1137.775885 | \n",
      "2023-05-10 17:09:34,225 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1853654384613037 seconds\n",
      "\n",
      "2023-05-10 17:09:34,225 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2006471621525752 seconds\n",
      "2023-05-10 17:09:34,226 | INFO : Avg batch train. time: 0.018760111908633987 seconds\n",
      "2023-05-10 17:09:34,226 | INFO : Avg sample train. time: 0.0002973370882002415 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 77  82.8% | batch:        53 of        64\t|\tloss: 702.043\n",
      "Training Epoch 77  84.4% | batch:        54 of        64\t|\tloss: 689.977\n",
      "Training Epoch 77  85.9% | batch:        55 of        64\t|\tloss: 719.738\n",
      "Training Epoch 77  87.5% | batch:        56 of        64\t|\tloss: 620.786\n",
      "Training Epoch 77  89.1% | batch:        57 of        64\t|\tloss: 889.708\n",
      "Training Epoch 77  90.6% | batch:        58 of        64\t|\tloss: 868.433\n",
      "Training Epoch 77  92.2% | batch:        59 of        64\t|\tloss: 1601.24\n",
      "Training Epoch 77  93.8% | batch:        60 of        64\t|\tloss: 476.991\n",
      "Training Epoch 77  95.3% | batch:        61 of        64\t|\tloss: 561.376\n",
      "Training Epoch 77  96.9% | batch:        62 of        64\t|\tloss: 1612.33\n",
      "Training Epoch 77  98.4% | batch:        63 of        64\t|\tloss: 1637.9\n",
      "\n",
      "Training Epoch 78   0.0% | batch:         0 of        64\t|\tloss: 1022.08\n",
      "Training Epoch 78   1.6% | batch:         1 of        64\t|\tloss: 736.846\n",
      "Training Epoch 78   3.1% | batch:         2 of        64\t|\tloss: 949.262\n",
      "Training Epoch 78   4.7% | batch:         3 of        64\t|\tloss: 953.38\n",
      "Training Epoch 78   6.2% | batch:         4 of        64\t|\tloss: 757.206\n",
      "Training Epoch 78   7.8% | batch:         5 of        64\t|\tloss: 1174.48\n",
      "Training Epoch 78   9.4% | batch:         6 of        64\t|\tloss: 827.265\n",
      "Training Epoch 78  10.9% | batch:         7 of        64\t|\tloss: 633.785\n",
      "Training Epoch 78  12.5% | batch:         8 of        64\t|\tloss: 1329.5\n",
      "Training Epoch 78  14.1% | batch:         9 of        64\t|\tloss: 729.234\n",
      "Training Epoch 78  15.6% | batch:        10 of        64\t|\tloss: 1066.8\n",
      "Training Epoch 78  17.2% | batch:        11 of        64\t|\tloss: 1614.73\n",
      "Training Epoch 78  18.8% | batch:        12 of        64\t|\tloss: 1291.44\n",
      "Training Epoch 78  20.3% | batch:        13 of        64\t|\tloss: 781.764\n",
      "Training Epoch 78  21.9% | batch:        14 of        64\t|\tloss: 1552.5\n",
      "Training Epoch 78  23.4% | batch:        15 of        64\t|\tloss: 776.303\n",
      "Training Epoch 78  25.0% | batch:        16 of        64\t|\tloss: 904.343\n",
      "Training Epoch 78  26.6% | batch:        17 of        64\t|\tloss: 2214.31\n",
      "Training Epoch 78  28.1% | batch:        18 of        64\t|\tloss: 1294.42\n",
      "Training Epoch 78  29.7% | batch:        19 of        64\t|\tloss: 1405.3\n",
      "Training Epoch 78  31.2% | batch:        20 of        64\t|\tloss: 1563.91\n",
      "Training Epoch 78  32.8% | batch:        21 of        64\t|\tloss: 1278.82\n",
      "Training Epoch 78  34.4% | batch:        22 of        64\t|\tloss: 1165.83\n",
      "Training Epoch 78  35.9% | batch:        23 of        64\t|\tloss: 1255.27\n",
      "Training Epoch 78  37.5% | batch:        24 of        64\t|\tloss: 875.376\n",
      "Training Epoch 78  39.1% | batch:        25 of        64\t|\tloss: 1208.25\n",
      "Training Epoch 78  40.6% | batch:        26 of        64\t|\tloss: 1763.26\n",
      "Training Epoch 78  42.2% | batch:        27 of        64\t|\tloss: 925.44\n",
      "Training Epoch 78  43.8% | batch:        28 of        64\t|\tloss: 987.952\n",
      "Training Epoch 78  45.3% | batch:        29 of        64\t|\tloss: 1729.37\n",
      "Training Epoch 78  46.9% | batch:        30 of        64\t|\tloss: 1333.36\n",
      "Training Epoch 78  48.4% | batch:        31 of        64\t|\tloss: 783.702\n",
      "Training Epoch 78  50.0% | batch:        32 of        64\t|\tloss: 1553.81\n",
      "Training Epoch 78  51.6% | batch:        33 of        64\t|\tloss: 634.045\n",
      "Training Epoch 78  53.1% | batch:        34 of        64\t|\tloss: 999.35\n",
      "Training Epoch 78  54.7% | batch:        35 of        64\t|\tloss: 878.843\n",
      "Training Epoch 78  56.2% | batch:        36 of        64\t|\tloss: 1344.86\n",
      "Training Epoch 78  57.8% | batch:        37 of        64\t|\tloss: 787.338\n",
      "Training Epoch 78  59.4% | batch:        38 of        64\t|\tloss: 1035.1\n",
      "Training Epoch 78  60.9% | batch:        39 of        64\t|\tloss: 990.608\n",
      "Training Epoch 78  62.5% | batch:        40 of        64\t|\tloss: 1102.76\n",
      "Training Epoch 78  64.1% | batch:        41 of        64\t|\tloss: 1246.82\n",
      "Training Epoch 78  65.6% | batch:        42 of        64\t|\tloss: 959.381\n",
      "Training Epoch 78  67.2% | batch:        43 of        64\t|\tloss: 1432.49\n",
      "Training Epoch 78  68.8% | batch:        44 of        64\t|\tloss: 1324.9\n",
      "Training Epoch 78  70.3% | batch:        45 of        64\t|\tloss: 793.945\n",
      "Training Epoch 78  71.9% | batch:        46 of        64\t|\tloss: 1247.83\n",
      "Training Epoch 78  73.4% | batch:        47 of        64\t|\tloss: 762.585\n",
      "Training Epoch 78  75.0% | batch:        48 of        64\t|\tloss: 960.343\n",
      "Training Epoch 78  76.6% | batch:        49 of        64\t|\tloss: 838.496\n",
      "Training Epoch 78  78.1% | batch:        50 of        64\t|\tloss: 1335.21\n",
      "Training Epoch 78  79.7% | batch:        51 of        64\t|\tloss: 1690.92\n",
      "Training Epoch 78  81.2% | batch:        52 of        64\t|\tloss: 3228.1\n",
      "Training Epoch 78  82.8% | batch:        53 of        64\t|\tloss: 4009.37\n",
      "Training Epoch 78  84.4% | batch:        54 of        64\t|\tloss: 1958.8\n",
      "Training Epoch 78  85.9% | batch:        55 of        64\t|\tloss: 1719.71\n",
      "Training Epoch 78  87.5% | batch:        56 of        64\t|\tloss: 668.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:35,388 | INFO : Epoch 78 Training Summary: epoch: 78.000000 | loss: 1246.154292 | \n",
      "2023-05-10 17:09:35,388 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1521379947662354 seconds\n",
      "\n",
      "2023-05-10 17:09:35,389 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.2000252497501862 seconds\n",
      "2023-05-10 17:09:35,390 | INFO : Avg batch train. time: 0.01875039452734666 seconds\n",
      "2023-05-10 17:09:35,390 | INFO : Avg sample train. time: 0.0002971830732417499 seconds\n",
      "2023-05-10 17:09:35,391 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 78  89.1% | batch:        57 of        64\t|\tloss: 1981.43\n",
      "Training Epoch 78  90.6% | batch:        58 of        64\t|\tloss: 1044.24\n",
      "Training Epoch 78  92.2% | batch:        59 of        64\t|\tloss: 590.444\n",
      "Training Epoch 78  93.8% | batch:        60 of        64\t|\tloss: 998.072\n",
      "Training Epoch 78  95.3% | batch:        61 of        64\t|\tloss: 2349.42\n",
      "Training Epoch 78  96.9% | batch:        62 of        64\t|\tloss: 1008.24\n",
      "Training Epoch 78  98.4% | batch:        63 of        64\t|\tloss: 2804.8\n",
      "\n",
      "Evaluating Epoch 78   0.0% | batch:         0 of        16\t|\tloss: 1685.13\n",
      "Evaluating Epoch 78   6.2% | batch:         1 of        16\t|\tloss: 1230.8\n",
      "Evaluating Epoch 78  12.5% | batch:         2 of        16\t|\tloss: 795.94\n",
      "Evaluating Epoch 78  18.8% | batch:         3 of        16\t|\tloss: 1385.33\n",
      "Evaluating Epoch 78  25.0% | batch:         4 of        16\t|\tloss: 1662.87\n",
      "Evaluating Epoch 78  31.2% | batch:         5 of        16\t|\tloss: 1890.23\n",
      "Evaluating Epoch 78  37.5% | batch:         6 of        16\t|\tloss: 2583.54\n",
      "Evaluating Epoch 78  43.8% | batch:         7 of        16\t|\tloss: 2551.83\n",
      "Evaluating Epoch 78  50.0% | batch:         8 of        16\t|\tloss: 1618.67\n",
      "Evaluating Epoch 78  56.2% | batch:         9 of        16\t|\tloss: 6033.25\n",
      "Evaluating Epoch 78  62.5% | batch:        10 of        16\t|\tloss: 1421.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:35,537 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14604711532592773 seconds\n",
      "\n",
      "2023-05-10 17:09:35,538 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.16774064738576006 seconds\n",
      "2023-05-10 17:09:35,538 | INFO : Avg batch val. time: 0.010483790461610003 seconds\n",
      "2023-05-10 17:09:35,538 | INFO : Avg sample val. time: 0.00016607984889679213 seconds\n",
      "2023-05-10 17:09:35,539 | INFO : Epoch 78 Validation Summary: epoch: 78.000000 | loss: 1949.868874 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 78  68.8% | batch:        11 of        16\t|\tloss: 1779.72\n",
      "Evaluating Epoch 78  75.0% | batch:        12 of        16\t|\tloss: 1296.36\n",
      "Evaluating Epoch 78  81.2% | batch:        13 of        16\t|\tloss: 1270.79\n",
      "Evaluating Epoch 78  87.5% | batch:        14 of        16\t|\tloss: 1597.94\n",
      "Evaluating Epoch 78  93.8% | batch:        15 of        16\t|\tloss: 2518.5\n",
      "\n",
      "Training Epoch 79   0.0% | batch:         0 of        64\t|\tloss: 1210.54\n",
      "Training Epoch 79   1.6% | batch:         1 of        64\t|\tloss: 1282.43\n",
      "Training Epoch 79   3.1% | batch:         2 of        64\t|\tloss: 1515.91\n",
      "Training Epoch 79   4.7% | batch:         3 of        64\t|\tloss: 1325.13\n",
      "Training Epoch 79   6.2% | batch:         4 of        64\t|\tloss: 1849.65\n",
      "Training Epoch 79   7.8% | batch:         5 of        64\t|\tloss: 639.007\n",
      "Training Epoch 79   9.4% | batch:         6 of        64\t|\tloss: 1194.54\n",
      "Training Epoch 79  10.9% | batch:         7 of        64\t|\tloss: 1372.93\n",
      "Training Epoch 79  12.5% | batch:         8 of        64\t|\tloss: 858.014\n",
      "Training Epoch 79  14.1% | batch:         9 of        64\t|\tloss: 922.407\n",
      "Training Epoch 79  15.6% | batch:        10 of        64\t|\tloss: 543.996\n",
      "Training Epoch 79  17.2% | batch:        11 of        64\t|\tloss: 1181.15\n",
      "Training Epoch 79  18.8% | batch:        12 of        64\t|\tloss: 1221.05\n",
      "Training Epoch 79  20.3% | batch:        13 of        64\t|\tloss: 1221.62\n",
      "Training Epoch 79  21.9% | batch:        14 of        64\t|\tloss: 635.243\n",
      "Training Epoch 79  23.4% | batch:        15 of        64\t|\tloss: 657.963\n",
      "Training Epoch 79  25.0% | batch:        16 of        64\t|\tloss: 1534.03\n",
      "Training Epoch 79  26.6% | batch:        17 of        64\t|\tloss: 661.812\n",
      "Training Epoch 79  28.1% | batch:        18 of        64\t|\tloss: 1043.11\n",
      "Training Epoch 79  29.7% | batch:        19 of        64\t|\tloss: 898.435\n",
      "Training Epoch 79  31.2% | batch:        20 of        64\t|\tloss: 1911.78\n",
      "Training Epoch 79  32.8% | batch:        21 of        64\t|\tloss: 855.078\n",
      "Training Epoch 79  34.4% | batch:        22 of        64\t|\tloss: 1053.13\n",
      "Training Epoch 79  35.9% | batch:        23 of        64\t|\tloss: 864.762\n",
      "Training Epoch 79  37.5% | batch:        24 of        64\t|\tloss: 725.075\n",
      "Training Epoch 79  39.1% | batch:        25 of        64\t|\tloss: 732.638\n",
      "Training Epoch 79  40.6% | batch:        26 of        64\t|\tloss: 796.844\n",
      "Training Epoch 79  42.2% | batch:        27 of        64\t|\tloss: 921.051\n",
      "Training Epoch 79  43.8% | batch:        28 of        64\t|\tloss: 511.299\n",
      "Training Epoch 79  45.3% | batch:        29 of        64\t|\tloss: 1567.4\n",
      "Training Epoch 79  46.9% | batch:        30 of        64\t|\tloss: 1686.91\n",
      "Training Epoch 79  48.4% | batch:        31 of        64\t|\tloss: 1209.93\n",
      "Training Epoch 79  50.0% | batch:        32 of        64\t|\tloss: 1149\n",
      "Training Epoch 79  51.6% | batch:        33 of        64\t|\tloss: 1252.03\n",
      "Training Epoch 79  53.1% | batch:        34 of        64\t|\tloss: 1778.07\n",
      "Training Epoch 79  54.7% | batch:        35 of        64\t|\tloss: 835.844\n",
      "Training Epoch 79  56.2% | batch:        36 of        64\t|\tloss: 1301.95\n",
      "Training Epoch 79  57.8% | batch:        37 of        64\t|\tloss: 1485.24\n",
      "Training Epoch 79  59.4% | batch:        38 of        64\t|\tloss: 910.657\n",
      "Training Epoch 79  60.9% | batch:        39 of        64\t|\tloss: 1537.95\n",
      "Training Epoch 79  62.5% | batch:        40 of        64\t|\tloss: 1645.69\n",
      "Training Epoch 79  64.1% | batch:        41 of        64\t|\tloss: 1346.3\n",
      "Training Epoch 79  65.6% | batch:        42 of        64\t|\tloss: 1009.45\n",
      "Training Epoch 79  67.2% | batch:        43 of        64\t|\tloss: 2034.48\n",
      "Training Epoch 79  68.8% | batch:        44 of        64\t|\tloss: 1025.28\n",
      "Training Epoch 79  70.3% | batch:        45 of        64\t|\tloss: 1777.58\n",
      "Training Epoch 79  71.9% | batch:        46 of        64\t|\tloss: 1111.46\n",
      "Training Epoch 79  73.4% | batch:        47 of        64\t|\tloss: 1026.67\n",
      "Training Epoch 79  75.0% | batch:        48 of        64\t|\tloss: 1086.62\n",
      "Training Epoch 79  76.6% | batch:        49 of        64\t|\tloss: 1010.99\n",
      "Training Epoch 79  78.1% | batch:        50 of        64\t|\tloss: 753.776\n",
      "Training Epoch 79  79.7% | batch:        51 of        64\t|\tloss: 1125.54\n",
      "Training Epoch 79  81.2% | batch:        52 of        64\t|\tloss: 621.087\n",
      "Training Epoch 79  82.8% | batch:        53 of        64\t|\tloss: 820.741\n",
      "Training Epoch 79  84.4% | batch:        54 of        64\t|\tloss: 1109.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:36,690 | INFO : Epoch 79 Training Summary: epoch: 79.000000 | loss: 1157.639456 | \n",
      "2023-05-10 17:09:36,690 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1408805847167969 seconds\n",
      "\n",
      "2023-05-10 17:09:36,691 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.199276583104194 seconds\n",
      "2023-05-10 17:09:36,692 | INFO : Avg batch train. time: 0.018738696611003032 seconds\n",
      "2023-05-10 17:09:36,692 | INFO : Avg sample train. time: 0.0002969976679307068 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 79  85.9% | batch:        55 of        64\t|\tloss: 1656.73\n",
      "Training Epoch 79  87.5% | batch:        56 of        64\t|\tloss: 794.992\n",
      "Training Epoch 79  89.1% | batch:        57 of        64\t|\tloss: 959.53\n",
      "Training Epoch 79  90.6% | batch:        58 of        64\t|\tloss: 1515.37\n",
      "Training Epoch 79  92.2% | batch:        59 of        64\t|\tloss: 589.05\n",
      "Training Epoch 79  93.8% | batch:        60 of        64\t|\tloss: 2023.24\n",
      "Training Epoch 79  95.3% | batch:        61 of        64\t|\tloss: 1662.95\n",
      "Training Epoch 79  96.9% | batch:        62 of        64\t|\tloss: 1395.76\n",
      "Training Epoch 79  98.4% | batch:        63 of        64\t|\tloss: 869.834\n",
      "\n",
      "Training Epoch 80   0.0% | batch:         0 of        64\t|\tloss: 660.392\n",
      "Training Epoch 80   1.6% | batch:         1 of        64\t|\tloss: 1609.86\n",
      "Training Epoch 80   3.1% | batch:         2 of        64\t|\tloss: 670.151\n",
      "Training Epoch 80   4.7% | batch:         3 of        64\t|\tloss: 628.651\n",
      "Training Epoch 80   6.2% | batch:         4 of        64\t|\tloss: 1653.62\n",
      "Training Epoch 80   7.8% | batch:         5 of        64\t|\tloss: 583.139\n",
      "Training Epoch 80   9.4% | batch:         6 of        64\t|\tloss: 1429.65\n",
      "Training Epoch 80  10.9% | batch:         7 of        64\t|\tloss: 888.932\n",
      "Training Epoch 80  12.5% | batch:         8 of        64\t|\tloss: 946.175\n",
      "Training Epoch 80  14.1% | batch:         9 of        64\t|\tloss: 736.499\n",
      "Training Epoch 80  15.6% | batch:        10 of        64\t|\tloss: 814.723\n",
      "Training Epoch 80  17.2% | batch:        11 of        64\t|\tloss: 984.425\n",
      "Training Epoch 80  18.8% | batch:        12 of        64\t|\tloss: 1448.01\n",
      "Training Epoch 80  20.3% | batch:        13 of        64\t|\tloss: 1256.91\n",
      "Training Epoch 80  21.9% | batch:        14 of        64\t|\tloss: 2176.64\n",
      "Training Epoch 80  23.4% | batch:        15 of        64\t|\tloss: 855.336\n",
      "Training Epoch 80  25.0% | batch:        16 of        64\t|\tloss: 1086.15\n",
      "Training Epoch 80  26.6% | batch:        17 of        64\t|\tloss: 787.192\n",
      "Training Epoch 80  28.1% | batch:        18 of        64\t|\tloss: 985.809\n",
      "Training Epoch 80  29.7% | batch:        19 of        64\t|\tloss: 890.605\n",
      "Training Epoch 80  31.2% | batch:        20 of        64\t|\tloss: 683.566\n",
      "Training Epoch 80  32.8% | batch:        21 of        64\t|\tloss: 891.55\n",
      "Training Epoch 80  34.4% | batch:        22 of        64\t|\tloss: 827.735\n",
      "Training Epoch 80  35.9% | batch:        23 of        64\t|\tloss: 2393.83\n",
      "Training Epoch 80  37.5% | batch:        24 of        64\t|\tloss: 1962.78\n",
      "Training Epoch 80  39.1% | batch:        25 of        64\t|\tloss: 1165.9\n",
      "Training Epoch 80  40.6% | batch:        26 of        64\t|\tloss: 492.439\n",
      "Training Epoch 80  42.2% | batch:        27 of        64\t|\tloss: 1334.92\n",
      "Training Epoch 80  43.8% | batch:        28 of        64\t|\tloss: 807.672\n",
      "Training Epoch 80  45.3% | batch:        29 of        64\t|\tloss: 1072.78\n",
      "Training Epoch 80  46.9% | batch:        30 of        64\t|\tloss: 1080.66\n",
      "Training Epoch 80  48.4% | batch:        31 of        64\t|\tloss: 867.679\n",
      "Training Epoch 80  50.0% | batch:        32 of        64\t|\tloss: 868.469\n",
      "Training Epoch 80  51.6% | batch:        33 of        64\t|\tloss: 704.467\n",
      "Training Epoch 80  53.1% | batch:        34 of        64\t|\tloss: 709.212\n",
      "Training Epoch 80  54.7% | batch:        35 of        64\t|\tloss: 1082.41\n",
      "Training Epoch 80  56.2% | batch:        36 of        64\t|\tloss: 444.086\n",
      "Training Epoch 80  57.8% | batch:        37 of        64\t|\tloss: 1759.67\n",
      "Training Epoch 80  59.4% | batch:        38 of        64\t|\tloss: 1406.63\n",
      "Training Epoch 80  60.9% | batch:        39 of        64\t|\tloss: 553.264\n",
      "Training Epoch 80  62.5% | batch:        40 of        64\t|\tloss: 1777.31\n",
      "Training Epoch 80  64.1% | batch:        41 of        64\t|\tloss: 1750.6\n",
      "Training Epoch 80  65.6% | batch:        42 of        64\t|\tloss: 1478.39\n",
      "Training Epoch 80  67.2% | batch:        43 of        64\t|\tloss: 1441.9\n",
      "Training Epoch 80  68.8% | batch:        44 of        64\t|\tloss: 754.163\n",
      "Training Epoch 80  70.3% | batch:        45 of        64\t|\tloss: 2454.73\n",
      "Training Epoch 80  71.9% | batch:        46 of        64\t|\tloss: 1215.78\n",
      "Training Epoch 80  73.4% | batch:        47 of        64\t|\tloss: 1602.5\n",
      "Training Epoch 80  75.0% | batch:        48 of        64\t|\tloss: 964.571\n",
      "Training Epoch 80  76.6% | batch:        49 of        64\t|\tloss: 2537.72\n",
      "Training Epoch 80  78.1% | batch:        50 of        64\t|\tloss: 860.587\n",
      "Training Epoch 80  79.7% | batch:        51 of        64\t|\tloss: 788.241\n",
      "Training Epoch 80  81.2% | batch:        52 of        64\t|\tloss: 1110.27\n",
      "Training Epoch 80  82.8% | batch:        53 of        64\t|\tloss: 1733.57\n",
      "Training Epoch 80  84.4% | batch:        54 of        64\t|\tloss: 1488.52\n",
      "Training Epoch 80  85.9% | batch:        55 of        64\t|\tloss: 947.709\n",
      "Training Epoch 80  87.5% | batch:        56 of        64\t|\tloss: 812.457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:37,892 | INFO : Epoch 80 Training Summary: epoch: 80.000000 | loss: 1158.902587 | \n",
      "2023-05-10 17:09:37,893 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.191169023513794 seconds\n",
      "\n",
      "2023-05-10 17:09:37,894 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.199175238609314 seconds\n",
      "2023-05-10 17:09:37,894 | INFO : Avg batch train. time: 0.01873711310327053 seconds\n",
      "2023-05-10 17:09:37,895 | INFO : Avg sample train. time: 0.0002969725702350951 seconds\n",
      "2023-05-10 17:09:37,895 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 80  89.1% | batch:        57 of        64\t|\tloss: 1739.39\n",
      "Training Epoch 80  90.6% | batch:        58 of        64\t|\tloss: 1485.55\n",
      "Training Epoch 80  92.2% | batch:        59 of        64\t|\tloss: 969.657\n",
      "Training Epoch 80  93.8% | batch:        60 of        64\t|\tloss: 1087.18\n",
      "Training Epoch 80  95.3% | batch:        61 of        64\t|\tloss: 634.513\n",
      "Training Epoch 80  96.9% | batch:        62 of        64\t|\tloss: 1221.94\n",
      "Training Epoch 80  98.4% | batch:        63 of        64\t|\tloss: 636.434\n",
      "\n",
      "Evaluating Epoch 80   0.0% | batch:         0 of        16\t|\tloss: 1334.15\n",
      "Evaluating Epoch 80   6.2% | batch:         1 of        16\t|\tloss: 898.285\n",
      "Evaluating Epoch 80  12.5% | batch:         2 of        16\t|\tloss: 931.373\n",
      "Evaluating Epoch 80  18.8% | batch:         3 of        16\t|\tloss: 1612.83\n",
      "Evaluating Epoch 80  25.0% | batch:         4 of        16\t|\tloss: 1247.11\n",
      "Evaluating Epoch 80  31.2% | batch:         5 of        16\t|\tloss: 1937.25\n",
      "Evaluating Epoch 80  37.5% | batch:         6 of        16\t|\tloss: 2417.81\n",
      "Evaluating Epoch 80  43.8% | batch:         7 of        16\t|\tloss: 2669.32\n",
      "Evaluating Epoch 80  50.0% | batch:         8 of        16\t|\tloss: 1723.48\n",
      "Evaluating Epoch 80  56.2% | batch:         9 of        16\t|\tloss: 8234.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:38,042 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14612174034118652 seconds\n",
      "\n",
      "2023-05-10 17:09:38,042 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.16722591150374638 seconds\n",
      "2023-05-10 17:09:38,043 | INFO : Avg batch val. time: 0.010451619468984149 seconds\n",
      "2023-05-10 17:09:38,043 | INFO : Avg sample val. time: 0.0001655702094096499 seconds\n",
      "2023-05-10 17:09:38,043 | INFO : Epoch 80 Validation Summary: epoch: 80.000000 | loss: 2038.089848 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 80  62.5% | batch:        10 of        16\t|\tloss: 1528.18\n",
      "Evaluating Epoch 80  68.8% | batch:        11 of        16\t|\tloss: 2328.34\n",
      "Evaluating Epoch 80  75.0% | batch:        12 of        16\t|\tloss: 1239.26\n",
      "Evaluating Epoch 80  81.2% | batch:        13 of        16\t|\tloss: 1080.66\n",
      "Evaluating Epoch 80  87.5% | batch:        14 of        16\t|\tloss: 1167.08\n",
      "Evaluating Epoch 80  93.8% | batch:        15 of        16\t|\tloss: 2321.75\n",
      "\n",
      "Training Epoch 81   0.0% | batch:         0 of        64\t|\tloss: 972.024\n",
      "Training Epoch 81   1.6% | batch:         1 of        64\t|\tloss: 2397.12\n",
      "Training Epoch 81   3.1% | batch:         2 of        64\t|\tloss: 1091.01\n",
      "Training Epoch 81   4.7% | batch:         3 of        64\t|\tloss: 902.168\n",
      "Training Epoch 81   6.2% | batch:         4 of        64\t|\tloss: 1521.5\n",
      "Training Epoch 81   7.8% | batch:         5 of        64\t|\tloss: 2599.7\n",
      "Training Epoch 81   9.4% | batch:         6 of        64\t|\tloss: 1239.16\n",
      "Training Epoch 81  10.9% | batch:         7 of        64\t|\tloss: 743.919\n",
      "Training Epoch 81  12.5% | batch:         8 of        64\t|\tloss: 627.449\n",
      "Training Epoch 81  14.1% | batch:         9 of        64\t|\tloss: 1788.19\n",
      "Training Epoch 81  15.6% | batch:        10 of        64\t|\tloss: 567.099\n",
      "Training Epoch 81  17.2% | batch:        11 of        64\t|\tloss: 1137.92\n",
      "Training Epoch 81  18.8% | batch:        12 of        64\t|\tloss: 1489.97\n",
      "Training Epoch 81  20.3% | batch:        13 of        64\t|\tloss: 947.872\n",
      "Training Epoch 81  21.9% | batch:        14 of        64\t|\tloss: 895.135\n",
      "Training Epoch 81  23.4% | batch:        15 of        64\t|\tloss: 569.058\n",
      "Training Epoch 81  25.0% | batch:        16 of        64\t|\tloss: 1557.52\n",
      "Training Epoch 81  26.6% | batch:        17 of        64\t|\tloss: 1327.27\n",
      "Training Epoch 81  28.1% | batch:        18 of        64\t|\tloss: 607.475\n",
      "Training Epoch 81  29.7% | batch:        19 of        64\t|\tloss: 1146.15\n",
      "Training Epoch 81  31.2% | batch:        20 of        64\t|\tloss: 659.087\n",
      "Training Epoch 81  32.8% | batch:        21 of        64\t|\tloss: 1839.47\n",
      "Training Epoch 81  34.4% | batch:        22 of        64\t|\tloss: 670.193\n",
      "Training Epoch 81  35.9% | batch:        23 of        64\t|\tloss: 724.563\n",
      "Training Epoch 81  37.5% | batch:        24 of        64\t|\tloss: 2092.23\n",
      "Training Epoch 81  39.1% | batch:        25 of        64\t|\tloss: 1069.37\n",
      "Training Epoch 81  40.6% | batch:        26 of        64\t|\tloss: 1682.29\n",
      "Training Epoch 81  42.2% | batch:        27 of        64\t|\tloss: 818.782\n",
      "Training Epoch 81  43.8% | batch:        28 of        64\t|\tloss: 680.755\n",
      "Training Epoch 81  45.3% | batch:        29 of        64\t|\tloss: 1015.25\n",
      "Training Epoch 81  46.9% | batch:        30 of        64\t|\tloss: 1218.96\n",
      "Training Epoch 81  48.4% | batch:        31 of        64\t|\tloss: 2037.2\n",
      "Training Epoch 81  50.0% | batch:        32 of        64\t|\tloss: 923.279\n",
      "Training Epoch 81  51.6% | batch:        33 of        64\t|\tloss: 1285.34\n",
      "Training Epoch 81  53.1% | batch:        34 of        64\t|\tloss: 1072.52\n",
      "Training Epoch 81  54.7% | batch:        35 of        64\t|\tloss: 994.578\n",
      "Training Epoch 81  56.2% | batch:        36 of        64\t|\tloss: 671.067\n",
      "Training Epoch 81  57.8% | batch:        37 of        64\t|\tloss: 653.505\n",
      "Training Epoch 81  59.4% | batch:        38 of        64\t|\tloss: 935.404\n",
      "Training Epoch 81  60.9% | batch:        39 of        64\t|\tloss: 818.708\n",
      "Training Epoch 81  62.5% | batch:        40 of        64\t|\tloss: 2237.12\n",
      "Training Epoch 81  64.1% | batch:        41 of        64\t|\tloss: 969.675\n",
      "Training Epoch 81  65.6% | batch:        42 of        64\t|\tloss: 635.69\n",
      "Training Epoch 81  67.2% | batch:        43 of        64\t|\tloss: 975.568\n",
      "Training Epoch 81  68.8% | batch:        44 of        64\t|\tloss: 1000.65\n",
      "Training Epoch 81  70.3% | batch:        45 of        64\t|\tloss: 1291.15\n",
      "Training Epoch 81  71.9% | batch:        46 of        64\t|\tloss: 826.636\n",
      "Training Epoch 81  73.4% | batch:        47 of        64\t|\tloss: 1321.61\n",
      "Training Epoch 81  75.0% | batch:        48 of        64\t|\tloss: 1169.84\n",
      "Training Epoch 81  76.6% | batch:        49 of        64\t|\tloss: 1535.87\n",
      "Training Epoch 81  78.1% | batch:        50 of        64\t|\tloss: 1867.8\n",
      "Training Epoch 81  79.7% | batch:        51 of        64\t|\tloss: 997.204\n",
      "Training Epoch 81  81.2% | batch:        52 of        64\t|\tloss: 1741.41\n",
      "Training Epoch 81  82.8% | batch:        53 of        64\t|\tloss: 1866.78\n",
      "Training Epoch 81  84.4% | batch:        54 of        64\t|\tloss: 832.381\n",
      "Training Epoch 81  85.9% | batch:        55 of        64\t|\tloss: 1659.79\n",
      "Training Epoch 81  87.5% | batch:        56 of        64\t|\tloss: 910.939\n",
      "Training Epoch 81  89.1% | batch:        57 of        64\t|\tloss: 937.229\n",
      "Training Epoch 81  90.6% | batch:        58 of        64\t|\tloss: 1366.3\n",
      "Training Epoch 81  92.2% | batch:        59 of        64\t|\tloss: 1465.97\n",
      "Training Epoch 81  93.8% | batch:        60 of        64\t|\tloss: 1301.45\n",
      "Training Epoch 81  95.3% | batch:        61 of        64\t|\tloss: 1118.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:39,239 | INFO : Epoch 81 Training Summary: epoch: 81.000000 | loss: 1190.036233 | \n",
      "2023-05-10 17:09:39,240 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1862027645111084 seconds\n",
      "\n",
      "2023-05-10 17:09:39,240 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1990150846081016 seconds\n",
      "2023-05-10 17:09:39,241 | INFO : Avg batch train. time: 0.018734610697001587 seconds\n",
      "2023-05-10 17:09:39,241 | INFO : Avg sample train. time: 0.0002969329085210752 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 81  96.9% | batch:        62 of        64\t|\tloss: 895.651\n",
      "Training Epoch 81  98.4% | batch:        63 of        64\t|\tloss: 1800.97\n",
      "\n",
      "Training Epoch 82   0.0% | batch:         0 of        64\t|\tloss: 642.578\n",
      "Training Epoch 82   1.6% | batch:         1 of        64\t|\tloss: 1301.03\n",
      "Training Epoch 82   3.1% | batch:         2 of        64\t|\tloss: 789.065\n",
      "Training Epoch 82   4.7% | batch:         3 of        64\t|\tloss: 1320.17\n",
      "Training Epoch 82   6.2% | batch:         4 of        64\t|\tloss: 1249.4\n",
      "Training Epoch 82   7.8% | batch:         5 of        64\t|\tloss: 802.689\n",
      "Training Epoch 82   9.4% | batch:         6 of        64\t|\tloss: 575.794\n",
      "Training Epoch 82  10.9% | batch:         7 of        64\t|\tloss: 2004.14\n",
      "Training Epoch 82  12.5% | batch:         8 of        64\t|\tloss: 1200.93\n",
      "Training Epoch 82  14.1% | batch:         9 of        64\t|\tloss: 1204.21\n",
      "Training Epoch 82  15.6% | batch:        10 of        64\t|\tloss: 634.174\n",
      "Training Epoch 82  17.2% | batch:        11 of        64\t|\tloss: 1131.91\n",
      "Training Epoch 82  18.8% | batch:        12 of        64\t|\tloss: 680.686\n",
      "Training Epoch 82  20.3% | batch:        13 of        64\t|\tloss: 624.508\n",
      "Training Epoch 82  21.9% | batch:        14 of        64\t|\tloss: 4012.05\n",
      "Training Epoch 82  23.4% | batch:        15 of        64\t|\tloss: 1817.98\n",
      "Training Epoch 82  25.0% | batch:        16 of        64\t|\tloss: 1221.75\n",
      "Training Epoch 82  26.6% | batch:        17 of        64\t|\tloss: 722.062\n",
      "Training Epoch 82  28.1% | batch:        18 of        64\t|\tloss: 1794.26\n",
      "Training Epoch 82  29.7% | batch:        19 of        64\t|\tloss: 843.926\n",
      "Training Epoch 82  31.2% | batch:        20 of        64\t|\tloss: 824.153\n",
      "Training Epoch 82  32.8% | batch:        21 of        64\t|\tloss: 1120.71\n",
      "Training Epoch 82  34.4% | batch:        22 of        64\t|\tloss: 1692.26\n",
      "Training Epoch 82  35.9% | batch:        23 of        64\t|\tloss: 1689.97\n",
      "Training Epoch 82  37.5% | batch:        24 of        64\t|\tloss: 950.663\n",
      "Training Epoch 82  39.1% | batch:        25 of        64\t|\tloss: 731.81\n",
      "Training Epoch 82  40.6% | batch:        26 of        64\t|\tloss: 1828.72\n",
      "Training Epoch 82  42.2% | batch:        27 of        64\t|\tloss: 1171.53\n",
      "Training Epoch 82  43.8% | batch:        28 of        64\t|\tloss: 3181.22\n",
      "Training Epoch 82  45.3% | batch:        29 of        64\t|\tloss: 862.69\n",
      "Training Epoch 82  46.9% | batch:        30 of        64\t|\tloss: 1537.22\n",
      "Training Epoch 82  48.4% | batch:        31 of        64\t|\tloss: 753.014\n",
      "Training Epoch 82  50.0% | batch:        32 of        64\t|\tloss: 606.77\n",
      "Training Epoch 82  51.6% | batch:        33 of        64\t|\tloss: 807.03\n",
      "Training Epoch 82  53.1% | batch:        34 of        64\t|\tloss: 844.282\n",
      "Training Epoch 82  54.7% | batch:        35 of        64\t|\tloss: 825.875\n",
      "Training Epoch 82  56.2% | batch:        36 of        64\t|\tloss: 759.501\n",
      "Training Epoch 82  57.8% | batch:        37 of        64\t|\tloss: 3028.77\n",
      "Training Epoch 82  59.4% | batch:        38 of        64\t|\tloss: 2575.47\n",
      "Training Epoch 82  60.9% | batch:        39 of        64\t|\tloss: 1044.17\n",
      "Training Epoch 82  62.5% | batch:        40 of        64\t|\tloss: 997.671\n",
      "Training Epoch 82  64.1% | batch:        41 of        64\t|\tloss: 1079.08\n",
      "Training Epoch 82  65.6% | batch:        42 of        64\t|\tloss: 966.606\n",
      "Training Epoch 82  67.2% | batch:        43 of        64\t|\tloss: 547.541\n",
      "Training Epoch 82  68.8% | batch:        44 of        64\t|\tloss: 1411.05\n",
      "Training Epoch 82  70.3% | batch:        45 of        64\t|\tloss: 843.618\n",
      "Training Epoch 82  71.9% | batch:        46 of        64\t|\tloss: 1248.14\n",
      "Training Epoch 82  73.4% | batch:        47 of        64\t|\tloss: 1715.55\n",
      "Training Epoch 82  75.0% | batch:        48 of        64\t|\tloss: 901.062\n",
      "Training Epoch 82  76.6% | batch:        49 of        64\t|\tloss: 961.21\n",
      "Training Epoch 82  78.1% | batch:        50 of        64\t|\tloss: 1007.82\n",
      "Training Epoch 82  79.7% | batch:        51 of        64\t|\tloss: 1319.7\n",
      "Training Epoch 82  81.2% | batch:        52 of        64\t|\tloss: 1164.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:40,449 | INFO : Epoch 82 Training Summary: epoch: 82.000000 | loss: 1209.562668 | \n",
      "2023-05-10 17:09:40,449 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1965763568878174 seconds\n",
      "\n",
      "2023-05-10 17:09:40,450 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1989853440261469 seconds\n",
      "2023-05-10 17:09:40,451 | INFO : Avg batch train. time: 0.018734146000408545 seconds\n",
      "2023-05-10 17:09:40,451 | INFO : Avg sample train. time: 0.00029692554334476146 seconds\n",
      "2023-05-10 17:09:40,451 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 82  82.8% | batch:        53 of        64\t|\tloss: 717.58\n",
      "Training Epoch 82  84.4% | batch:        54 of        64\t|\tloss: 612.353\n",
      "Training Epoch 82  85.9% | batch:        55 of        64\t|\tloss: 719.864\n",
      "Training Epoch 82  87.5% | batch:        56 of        64\t|\tloss: 1571.62\n",
      "Training Epoch 82  89.1% | batch:        57 of        64\t|\tloss: 1260.84\n",
      "Training Epoch 82  90.6% | batch:        58 of        64\t|\tloss: 979.339\n",
      "Training Epoch 82  92.2% | batch:        59 of        64\t|\tloss: 899.689\n",
      "Training Epoch 82  93.8% | batch:        60 of        64\t|\tloss: 1527.97\n",
      "Training Epoch 82  95.3% | batch:        61 of        64\t|\tloss: 1222.82\n",
      "Training Epoch 82  96.9% | batch:        62 of        64\t|\tloss: 1151.05\n",
      "Training Epoch 82  98.4% | batch:        63 of        64\t|\tloss: 849.288\n",
      "\n",
      "Evaluating Epoch 82   0.0% | batch:         0 of        16\t|\tloss: 1556.8\n",
      "Evaluating Epoch 82   6.2% | batch:         1 of        16\t|\tloss: 1053.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:40,602 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14981436729431152 seconds\n",
      "\n",
      "2023-05-10 17:09:40,602 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.16682099187096885 seconds\n",
      "2023-05-10 17:09:40,602 | INFO : Avg batch val. time: 0.010426311991935553 seconds\n",
      "2023-05-10 17:09:40,603 | INFO : Avg sample val. time: 0.0001651692988821474 seconds\n",
      "2023-05-10 17:09:40,603 | INFO : Epoch 82 Validation Summary: epoch: 82.000000 | loss: 1882.148128 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 82  12.5% | batch:         2 of        16\t|\tloss: 764.609\n",
      "Evaluating Epoch 82  18.8% | batch:         3 of        16\t|\tloss: 1346.28\n",
      "Evaluating Epoch 82  25.0% | batch:         4 of        16\t|\tloss: 1512.55\n",
      "Evaluating Epoch 82  31.2% | batch:         5 of        16\t|\tloss: 1632.52\n",
      "Evaluating Epoch 82  37.5% | batch:         6 of        16\t|\tloss: 2176.79\n",
      "Evaluating Epoch 82  43.8% | batch:         7 of        16\t|\tloss: 2608.52\n",
      "Evaluating Epoch 82  50.0% | batch:         8 of        16\t|\tloss: 1256.42\n",
      "Evaluating Epoch 82  56.2% | batch:         9 of        16\t|\tloss: 7695.77\n",
      "Evaluating Epoch 82  62.5% | batch:        10 of        16\t|\tloss: 1418.57\n",
      "Evaluating Epoch 82  68.8% | batch:        11 of        16\t|\tloss: 1995.75\n",
      "Evaluating Epoch 82  75.0% | batch:        12 of        16\t|\tloss: 884.231\n",
      "Evaluating Epoch 82  81.2% | batch:        13 of        16\t|\tloss: 1114.37\n",
      "Evaluating Epoch 82  87.5% | batch:        14 of        16\t|\tloss: 1242.32\n",
      "Evaluating Epoch 82  93.8% | batch:        15 of        16\t|\tloss: 1847.7\n",
      "\n",
      "Training Epoch 83   0.0% | batch:         0 of        64\t|\tloss: 1261.96\n",
      "Training Epoch 83   1.6% | batch:         1 of        64\t|\tloss: 661.492\n",
      "Training Epoch 83   3.1% | batch:         2 of        64\t|\tloss: 752.346\n",
      "Training Epoch 83   4.7% | batch:         3 of        64\t|\tloss: 598.356\n",
      "Training Epoch 83   6.2% | batch:         4 of        64\t|\tloss: 1725.57\n",
      "Training Epoch 83   7.8% | batch:         5 of        64\t|\tloss: 1088.43\n",
      "Training Epoch 83   9.4% | batch:         6 of        64\t|\tloss: 510.543\n",
      "Training Epoch 83  10.9% | batch:         7 of        64\t|\tloss: 867.924\n",
      "Training Epoch 83  12.5% | batch:         8 of        64\t|\tloss: 888.836\n",
      "Training Epoch 83  14.1% | batch:         9 of        64\t|\tloss: 1202.17\n",
      "Training Epoch 83  15.6% | batch:        10 of        64\t|\tloss: 792.131\n",
      "Training Epoch 83  17.2% | batch:        11 of        64\t|\tloss: 1050.71\n",
      "Training Epoch 83  18.8% | batch:        12 of        64\t|\tloss: 980.051\n",
      "Training Epoch 83  20.3% | batch:        13 of        64\t|\tloss: 1229.17\n",
      "Training Epoch 83  21.9% | batch:        14 of        64\t|\tloss: 1130.56\n",
      "Training Epoch 83  23.4% | batch:        15 of        64\t|\tloss: 1058.16\n",
      "Training Epoch 83  25.0% | batch:        16 of        64\t|\tloss: 1432.23\n",
      "Training Epoch 83  26.6% | batch:        17 of        64\t|\tloss: 1062.07\n",
      "Training Epoch 83  28.1% | batch:        18 of        64\t|\tloss: 1188.75\n",
      "Training Epoch 83  29.7% | batch:        19 of        64\t|\tloss: 1025.97\n",
      "Training Epoch 83  31.2% | batch:        20 of        64\t|\tloss: 1748.23\n",
      "Training Epoch 83  32.8% | batch:        21 of        64\t|\tloss: 1054.44\n",
      "Training Epoch 83  34.4% | batch:        22 of        64\t|\tloss: 990.994\n",
      "Training Epoch 83  35.9% | batch:        23 of        64\t|\tloss: 3626.65\n",
      "Training Epoch 83  37.5% | batch:        24 of        64\t|\tloss: 1340.23\n",
      "Training Epoch 83  39.1% | batch:        25 of        64\t|\tloss: 671.588\n",
      "Training Epoch 83  40.6% | batch:        26 of        64\t|\tloss: 960.784\n",
      "Training Epoch 83  42.2% | batch:        27 of        64\t|\tloss: 995.576\n",
      "Training Epoch 83  43.8% | batch:        28 of        64\t|\tloss: 714.438\n",
      "Training Epoch 83  45.3% | batch:        29 of        64\t|\tloss: 969.141\n",
      "Training Epoch 83  46.9% | batch:        30 of        64\t|\tloss: 1071.87\n",
      "Training Epoch 83  48.4% | batch:        31 of        64\t|\tloss: 1168.73\n",
      "Training Epoch 83  50.0% | batch:        32 of        64\t|\tloss: 520.122\n",
      "Training Epoch 83  51.6% | batch:        33 of        64\t|\tloss: 646.431\n",
      "Training Epoch 83  53.1% | batch:        34 of        64\t|\tloss: 2489.05\n",
      "Training Epoch 83  54.7% | batch:        35 of        64\t|\tloss: 1006.32\n",
      "Training Epoch 83  56.2% | batch:        36 of        64\t|\tloss: 743.464\n",
      "Training Epoch 83  57.8% | batch:        37 of        64\t|\tloss: 856.665\n",
      "Training Epoch 83  59.4% | batch:        38 of        64\t|\tloss: 4162.29\n",
      "Training Epoch 83  60.9% | batch:        39 of        64\t|\tloss: 1043.17\n",
      "Training Epoch 83  62.5% | batch:        40 of        64\t|\tloss: 1002.69\n",
      "Training Epoch 83  64.1% | batch:        41 of        64\t|\tloss: 607.953\n",
      "Training Epoch 83  65.6% | batch:        42 of        64\t|\tloss: 900.268\n",
      "Training Epoch 83  67.2% | batch:        43 of        64\t|\tloss: 678.657\n",
      "Training Epoch 83  68.8% | batch:        44 of        64\t|\tloss: 1134.28\n",
      "Training Epoch 83  70.3% | batch:        45 of        64\t|\tloss: 2523.11\n",
      "Training Epoch 83  71.9% | batch:        46 of        64\t|\tloss: 975.136\n",
      "Training Epoch 83  73.4% | batch:        47 of        64\t|\tloss: 1315.08\n",
      "Training Epoch 83  75.0% | batch:        48 of        64\t|\tloss: 906.117\n",
      "Training Epoch 83  76.6% | batch:        49 of        64\t|\tloss: 1334.96\n",
      "Training Epoch 83  78.1% | batch:        50 of        64\t|\tloss: 586.311\n",
      "Training Epoch 83  79.7% | batch:        51 of        64\t|\tloss: 1973.95\n",
      "Training Epoch 83  81.2% | batch:        52 of        64\t|\tloss: 721.284\n",
      "Training Epoch 83  82.8% | batch:        53 of        64\t|\tloss: 583.238\n",
      "Training Epoch 83  84.4% | batch:        54 of        64\t|\tloss: 829.466\n",
      "Training Epoch 83  85.9% | batch:        55 of        64\t|\tloss: 956.841\n",
      "Training Epoch 83  87.5% | batch:        56 of        64\t|\tloss: 609.804\n",
      "Training Epoch 83  89.1% | batch:        57 of        64\t|\tloss: 625.687\n",
      "Training Epoch 83  90.6% | batch:        58 of        64\t|\tloss: 944.271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:41,794 | INFO : Epoch 83 Training Summary: epoch: 83.000000 | loss: 1150.499440 | \n",
      "2023-05-10 17:09:41,795 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1812083721160889 seconds\n",
      "\n",
      "2023-05-10 17:09:41,795 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1987711636416882 seconds\n",
      "2023-05-10 17:09:41,796 | INFO : Avg batch train. time: 0.01873079943190138 seconds\n",
      "2023-05-10 17:09:41,796 | INFO : Avg sample train. time: 0.00029687250214009117 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 83  92.2% | batch:        59 of        64\t|\tloss: 1249.69\n",
      "Training Epoch 83  93.8% | batch:        60 of        64\t|\tloss: 3168.5\n",
      "Training Epoch 83  95.3% | batch:        61 of        64\t|\tloss: 689.476\n",
      "Training Epoch 83  96.9% | batch:        62 of        64\t|\tloss: 918.22\n",
      "Training Epoch 83  98.4% | batch:        63 of        64\t|\tloss: 711.974\n",
      "\n",
      "Training Epoch 84   0.0% | batch:         0 of        64\t|\tloss: 956.98\n",
      "Training Epoch 84   1.6% | batch:         1 of        64\t|\tloss: 602.304\n",
      "Training Epoch 84   3.1% | batch:         2 of        64\t|\tloss: 982.287\n",
      "Training Epoch 84   4.7% | batch:         3 of        64\t|\tloss: 1962.23\n",
      "Training Epoch 84   6.2% | batch:         4 of        64\t|\tloss: 1237.37\n",
      "Training Epoch 84   7.8% | batch:         5 of        64\t|\tloss: 1206.21\n",
      "Training Epoch 84   9.4% | batch:         6 of        64\t|\tloss: 1258.6\n",
      "Training Epoch 84  10.9% | batch:         7 of        64\t|\tloss: 1469.78\n",
      "Training Epoch 84  12.5% | batch:         8 of        64\t|\tloss: 1071.63\n",
      "Training Epoch 84  14.1% | batch:         9 of        64\t|\tloss: 1797.37\n",
      "Training Epoch 84  15.6% | batch:        10 of        64\t|\tloss: 1372.94\n",
      "Training Epoch 84  17.2% | batch:        11 of        64\t|\tloss: 1337.5\n",
      "Training Epoch 84  18.8% | batch:        12 of        64\t|\tloss: 2389.36\n",
      "Training Epoch 84  20.3% | batch:        13 of        64\t|\tloss: 2094.34\n",
      "Training Epoch 84  21.9% | batch:        14 of        64\t|\tloss: 1250.24\n",
      "Training Epoch 84  23.4% | batch:        15 of        64\t|\tloss: 652.634\n",
      "Training Epoch 84  25.0% | batch:        16 of        64\t|\tloss: 5449.38\n",
      "Training Epoch 84  26.6% | batch:        17 of        64\t|\tloss: 879.783\n",
      "Training Epoch 84  28.1% | batch:        18 of        64\t|\tloss: 1099.5\n",
      "Training Epoch 84  29.7% | batch:        19 of        64\t|\tloss: 973.297\n",
      "Training Epoch 84  31.2% | batch:        20 of        64\t|\tloss: 1144.24\n",
      "Training Epoch 84  32.8% | batch:        21 of        64\t|\tloss: 539.08\n",
      "Training Epoch 84  34.4% | batch:        22 of        64\t|\tloss: 1697.65\n",
      "Training Epoch 84  35.9% | batch:        23 of        64\t|\tloss: 724.108\n",
      "Training Epoch 84  37.5% | batch:        24 of        64\t|\tloss: 874.887\n",
      "Training Epoch 84  39.1% | batch:        25 of        64\t|\tloss: 826.192\n",
      "Training Epoch 84  40.6% | batch:        26 of        64\t|\tloss: 916.73\n",
      "Training Epoch 84  42.2% | batch:        27 of        64\t|\tloss: 1135.02\n",
      "Training Epoch 84  43.8% | batch:        28 of        64\t|\tloss: 749.332\n",
      "Training Epoch 84  45.3% | batch:        29 of        64\t|\tloss: 2051.51\n",
      "Training Epoch 84  46.9% | batch:        30 of        64\t|\tloss: 620.955\n",
      "Training Epoch 84  48.4% | batch:        31 of        64\t|\tloss: 1069.5\n",
      "Training Epoch 84  50.0% | batch:        32 of        64\t|\tloss: 773.443\n",
      "Training Epoch 84  51.6% | batch:        33 of        64\t|\tloss: 1006.44\n",
      "Training Epoch 84  53.1% | batch:        34 of        64\t|\tloss: 1243.1\n",
      "Training Epoch 84  54.7% | batch:        35 of        64\t|\tloss: 991.578\n",
      "Training Epoch 84  56.2% | batch:        36 of        64\t|\tloss: 1010.24\n",
      "Training Epoch 84  57.8% | batch:        37 of        64\t|\tloss: 1156.53\n",
      "Training Epoch 84  59.4% | batch:        38 of        64\t|\tloss: 720.886\n",
      "Training Epoch 84  60.9% | batch:        39 of        64\t|\tloss: 2092.76\n",
      "Training Epoch 84  62.5% | batch:        40 of        64\t|\tloss: 702.311\n",
      "Training Epoch 84  64.1% | batch:        41 of        64\t|\tloss: 3553.46\n",
      "Training Epoch 84  65.6% | batch:        42 of        64\t|\tloss: 514.422\n",
      "Training Epoch 84  67.2% | batch:        43 of        64\t|\tloss: 1906.63\n",
      "Training Epoch 84  68.8% | batch:        44 of        64\t|\tloss: 828.185\n",
      "Training Epoch 84  70.3% | batch:        45 of        64\t|\tloss: 516.459\n",
      "Training Epoch 84  71.9% | batch:        46 of        64\t|\tloss: 1490.04\n",
      "Training Epoch 84  73.4% | batch:        47 of        64\t|\tloss: 615.993\n",
      "Training Epoch 84  75.0% | batch:        48 of        64\t|\tloss: 1069.77\n",
      "Training Epoch 84  76.6% | batch:        49 of        64\t|\tloss: 980.269\n",
      "Training Epoch 84  78.1% | batch:        50 of        64\t|\tloss: 735.355\n",
      "Training Epoch 84  79.7% | batch:        51 of        64\t|\tloss: 1031\n",
      "Training Epoch 84  81.2% | batch:        52 of        64\t|\tloss: 773.815\n",
      "Training Epoch 84  82.8% | batch:        53 of        64\t|\tloss: 716.045\n",
      "Training Epoch 84  84.4% | batch:        54 of        64\t|\tloss: 1485.02\n",
      "Training Epoch 84  85.9% | batch:        55 of        64\t|\tloss: 1195.91\n",
      "Training Epoch 84  87.5% | batch:        56 of        64\t|\tloss: 2094.89\n",
      "Training Epoch 84  89.1% | batch:        57 of        64\t|\tloss: 432.652\n",
      "Training Epoch 84  90.6% | batch:        58 of        64\t|\tloss: 1097.53\n",
      "Training Epoch 84  92.2% | batch:        59 of        64\t|\tloss: 706.159\n",
      "Training Epoch 84  93.8% | batch:        60 of        64\t|\tloss: 2026.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:43,003 | INFO : Epoch 84 Training Summary: epoch: 84.000000 | loss: 1246.258278 | \n",
      "2023-05-10 17:09:43,003 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1979994773864746 seconds\n",
      "\n",
      "2023-05-10 17:09:43,004 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1987619769005549 seconds\n",
      "2023-05-10 17:09:43,004 | INFO : Avg batch train. time: 0.01873065588907117 seconds\n",
      "2023-05-10 17:09:43,005 | INFO : Avg sample train. time: 0.0002968702270679928 seconds\n",
      "2023-05-10 17:09:43,005 | INFO : Evaluating on validation set ...\n",
      "2023-05-10 17:09:43,152 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14651989936828613 seconds\n",
      "\n",
      "2023-05-10 17:09:43,153 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.16635960340499878 seconds\n",
      "2023-05-10 17:09:43,153 | INFO : Avg batch val. time: 0.010397475212812424 seconds\n",
      "2023-05-10 17:09:43,154 | INFO : Avg sample val. time: 0.00016471247861881067 seconds\n",
      "2023-05-10 17:09:43,154 | INFO : Epoch 84 Validation Summary: epoch: 84.000000 | loss: 1943.657089 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 84  95.3% | batch:        61 of        64\t|\tloss: 1337.56\n",
      "Training Epoch 84  96.9% | batch:        62 of        64\t|\tloss: 1192.94\n",
      "Training Epoch 84  98.4% | batch:        63 of        64\t|\tloss: 2565.99\n",
      "\n",
      "Evaluating Epoch 84   0.0% | batch:         0 of        16\t|\tloss: 1326.51\n",
      "Evaluating Epoch 84   6.2% | batch:         1 of        16\t|\tloss: 914.531\n",
      "Evaluating Epoch 84  12.5% | batch:         2 of        16\t|\tloss: 999.607\n",
      "Evaluating Epoch 84  18.8% | batch:         3 of        16\t|\tloss: 1348\n",
      "Evaluating Epoch 84  25.0% | batch:         4 of        16\t|\tloss: 1593.18\n",
      "Evaluating Epoch 84  31.2% | batch:         5 of        16\t|\tloss: 1748.38\n",
      "Evaluating Epoch 84  37.5% | batch:         6 of        16\t|\tloss: 2272.64\n",
      "Evaluating Epoch 84  43.8% | batch:         7 of        16\t|\tloss: 1748.76\n",
      "Evaluating Epoch 84  50.0% | batch:         8 of        16\t|\tloss: 1434.78\n",
      "Evaluating Epoch 84  56.2% | batch:         9 of        16\t|\tloss: 8360.67\n",
      "Evaluating Epoch 84  62.5% | batch:        10 of        16\t|\tloss: 1678.03\n",
      "Evaluating Epoch 84  68.8% | batch:        11 of        16\t|\tloss: 2168.41\n",
      "Evaluating Epoch 84  75.0% | batch:        12 of        16\t|\tloss: 1441.79\n",
      "Evaluating Epoch 84  81.2% | batch:        13 of        16\t|\tloss: 1275.52\n",
      "Evaluating Epoch 84  87.5% | batch:        14 of        16\t|\tloss: 1135.91\n",
      "Evaluating Epoch 84  93.8% | batch:        15 of        16\t|\tloss: 1570.09\n",
      "\n",
      "Training Epoch 85   0.0% | batch:         0 of        64\t|\tloss: 3212.21\n",
      "Training Epoch 85   1.6% | batch:         1 of        64\t|\tloss: 615.578\n",
      "Training Epoch 85   3.1% | batch:         2 of        64\t|\tloss: 953.025\n",
      "Training Epoch 85   4.7% | batch:         3 of        64\t|\tloss: 844.472\n",
      "Training Epoch 85   6.2% | batch:         4 of        64\t|\tloss: 1207.44\n",
      "Training Epoch 85   7.8% | batch:         5 of        64\t|\tloss: 922.93\n",
      "Training Epoch 85   9.4% | batch:         6 of        64\t|\tloss: 1427.29\n",
      "Training Epoch 85  10.9% | batch:         7 of        64\t|\tloss: 783.307\n",
      "Training Epoch 85  12.5% | batch:         8 of        64\t|\tloss: 480.358\n",
      "Training Epoch 85  14.1% | batch:         9 of        64\t|\tloss: 940.528\n",
      "Training Epoch 85  15.6% | batch:        10 of        64\t|\tloss: 1153.11\n",
      "Training Epoch 85  17.2% | batch:        11 of        64\t|\tloss: 1099.83\n",
      "Training Epoch 85  18.8% | batch:        12 of        64\t|\tloss: 637.226\n",
      "Training Epoch 85  20.3% | batch:        13 of        64\t|\tloss: 1727.57\n",
      "Training Epoch 85  21.9% | batch:        14 of        64\t|\tloss: 997.743\n",
      "Training Epoch 85  23.4% | batch:        15 of        64\t|\tloss: 656.862\n",
      "Training Epoch 85  25.0% | batch:        16 of        64\t|\tloss: 1293.77\n",
      "Training Epoch 85  26.6% | batch:        17 of        64\t|\tloss: 967.279\n",
      "Training Epoch 85  28.1% | batch:        18 of        64\t|\tloss: 807.403\n",
      "Training Epoch 85  29.7% | batch:        19 of        64\t|\tloss: 1371.57\n",
      "Training Epoch 85  31.2% | batch:        20 of        64\t|\tloss: 866.231\n",
      "Training Epoch 85  32.8% | batch:        21 of        64\t|\tloss: 2455.52\n",
      "Training Epoch 85  34.4% | batch:        22 of        64\t|\tloss: 1134.67\n",
      "Training Epoch 85  35.9% | batch:        23 of        64\t|\tloss: 483.07\n",
      "Training Epoch 85  37.5% | batch:        24 of        64\t|\tloss: 469\n",
      "Training Epoch 85  39.1% | batch:        25 of        64\t|\tloss: 1429.05\n",
      "Training Epoch 85  40.6% | batch:        26 of        64\t|\tloss: 747.883\n",
      "Training Epoch 85  42.2% | batch:        27 of        64\t|\tloss: 722.18\n",
      "Training Epoch 85  43.8% | batch:        28 of        64\t|\tloss: 932.437\n",
      "Training Epoch 85  45.3% | batch:        29 of        64\t|\tloss: 982.1\n",
      "Training Epoch 85  46.9% | batch:        30 of        64\t|\tloss: 916.189\n",
      "Training Epoch 85  48.4% | batch:        31 of        64\t|\tloss: 889.175\n",
      "Training Epoch 85  50.0% | batch:        32 of        64\t|\tloss: 2002.2\n",
      "Training Epoch 85  51.6% | batch:        33 of        64\t|\tloss: 771.724\n",
      "Training Epoch 85  53.1% | batch:        34 of        64\t|\tloss: 1263.31\n",
      "Training Epoch 85  54.7% | batch:        35 of        64\t|\tloss: 838.538\n",
      "Training Epoch 85  56.2% | batch:        36 of        64\t|\tloss: 885.192\n",
      "Training Epoch 85  57.8% | batch:        37 of        64\t|\tloss: 1266.29\n",
      "Training Epoch 85  59.4% | batch:        38 of        64\t|\tloss: 1084.93\n",
      "Training Epoch 85  60.9% | batch:        39 of        64\t|\tloss: 813.924\n",
      "Training Epoch 85  62.5% | batch:        40 of        64\t|\tloss: 1321.56\n",
      "Training Epoch 85  64.1% | batch:        41 of        64\t|\tloss: 600.173\n",
      "Training Epoch 85  65.6% | batch:        42 of        64\t|\tloss: 779.663\n",
      "Training Epoch 85  67.2% | batch:        43 of        64\t|\tloss: 681.315\n",
      "Training Epoch 85  68.8% | batch:        44 of        64\t|\tloss: 828.785\n",
      "Training Epoch 85  70.3% | batch:        45 of        64\t|\tloss: 1520.98\n",
      "Training Epoch 85  71.9% | batch:        46 of        64\t|\tloss: 1532.33\n",
      "Training Epoch 85  73.4% | batch:        47 of        64\t|\tloss: 1107.19\n",
      "Training Epoch 85  75.0% | batch:        48 of        64\t|\tloss: 1059.1\n",
      "Training Epoch 85  76.6% | batch:        49 of        64\t|\tloss: 586.747\n",
      "Training Epoch 85  78.1% | batch:        50 of        64\t|\tloss: 719.691\n",
      "Training Epoch 85  79.7% | batch:        51 of        64\t|\tloss: 1211.83\n",
      "Training Epoch 85  81.2% | batch:        52 of        64\t|\tloss: 1508.59\n",
      "Training Epoch 85  82.8% | batch:        53 of        64\t|\tloss: 927.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:44,369 | INFO : Epoch 85 Training Summary: epoch: 85.000000 | loss: 1126.730279 | \n",
      "2023-05-10 17:09:44,370 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.2059826850891113 seconds\n",
      "\n",
      "2023-05-10 17:09:44,371 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1988469264086554 seconds\n",
      "2023-05-10 17:09:44,371 | INFO : Avg batch train. time: 0.01873198322513524 seconds\n",
      "2023-05-10 17:09:44,372 | INFO : Avg sample train. time: 0.00029689126458857243 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 85  84.4% | batch:        54 of        64\t|\tloss: 902.507\n",
      "Training Epoch 85  85.9% | batch:        55 of        64\t|\tloss: 877.936\n",
      "Training Epoch 85  87.5% | batch:        56 of        64\t|\tloss: 1117\n",
      "Training Epoch 85  89.1% | batch:        57 of        64\t|\tloss: 1337.32\n",
      "Training Epoch 85  90.6% | batch:        58 of        64\t|\tloss: 2033.37\n",
      "Training Epoch 85  92.2% | batch:        59 of        64\t|\tloss: 1084.01\n",
      "Training Epoch 85  93.8% | batch:        60 of        64\t|\tloss: 1613.7\n",
      "Training Epoch 85  95.3% | batch:        61 of        64\t|\tloss: 2034.6\n",
      "Training Epoch 85  96.9% | batch:        62 of        64\t|\tloss: 2449.68\n",
      "Training Epoch 85  98.4% | batch:        63 of        64\t|\tloss: 2139.74\n",
      "\n",
      "Training Epoch 86   0.0% | batch:         0 of        64\t|\tloss: 1046.33\n",
      "Training Epoch 86   1.6% | batch:         1 of        64\t|\tloss: 526.68\n",
      "Training Epoch 86   3.1% | batch:         2 of        64\t|\tloss: 563.371\n",
      "Training Epoch 86   4.7% | batch:         3 of        64\t|\tloss: 627.661\n",
      "Training Epoch 86   6.2% | batch:         4 of        64\t|\tloss: 669.602\n",
      "Training Epoch 86   7.8% | batch:         5 of        64\t|\tloss: 1321.75\n",
      "Training Epoch 86   9.4% | batch:         6 of        64\t|\tloss: 828.159\n",
      "Training Epoch 86  10.9% | batch:         7 of        64\t|\tloss: 3983.05\n",
      "Training Epoch 86  12.5% | batch:         8 of        64\t|\tloss: 963.438\n",
      "Training Epoch 86  14.1% | batch:         9 of        64\t|\tloss: 627.644\n",
      "Training Epoch 86  15.6% | batch:        10 of        64\t|\tloss: 1126.1\n",
      "Training Epoch 86  17.2% | batch:        11 of        64\t|\tloss: 515.365\n",
      "Training Epoch 86  18.8% | batch:        12 of        64\t|\tloss: 763.785\n",
      "Training Epoch 86  20.3% | batch:        13 of        64\t|\tloss: 1860.05\n",
      "Training Epoch 86  21.9% | batch:        14 of        64\t|\tloss: 2311.33\n",
      "Training Epoch 86  23.4% | batch:        15 of        64\t|\tloss: 892.215\n",
      "Training Epoch 86  25.0% | batch:        16 of        64\t|\tloss: 870.802\n",
      "Training Epoch 86  26.6% | batch:        17 of        64\t|\tloss: 1133.87\n",
      "Training Epoch 86  28.1% | batch:        18 of        64\t|\tloss: 701.14\n",
      "Training Epoch 86  29.7% | batch:        19 of        64\t|\tloss: 5357.48\n",
      "Training Epoch 86  31.2% | batch:        20 of        64\t|\tloss: 714.83\n",
      "Training Epoch 86  32.8% | batch:        21 of        64\t|\tloss: 1057.79\n",
      "Training Epoch 86  34.4% | batch:        22 of        64\t|\tloss: 759.756\n",
      "Training Epoch 86  35.9% | batch:        23 of        64\t|\tloss: 509.64\n",
      "Training Epoch 86  37.5% | batch:        24 of        64\t|\tloss: 5494.48\n",
      "Training Epoch 86  39.1% | batch:        25 of        64\t|\tloss: 722.147\n",
      "Training Epoch 86  40.6% | batch:        26 of        64\t|\tloss: 687.459\n",
      "Training Epoch 86  42.2% | batch:        27 of        64\t|\tloss: 1761.5\n",
      "Training Epoch 86  43.8% | batch:        28 of        64\t|\tloss: 667.897\n",
      "Training Epoch 86  45.3% | batch:        29 of        64\t|\tloss: 725.477\n",
      "Training Epoch 86  46.9% | batch:        30 of        64\t|\tloss: 1095.25\n",
      "Training Epoch 86  48.4% | batch:        31 of        64\t|\tloss: 953.244\n",
      "Training Epoch 86  50.0% | batch:        32 of        64\t|\tloss: 561.118\n",
      "Training Epoch 86  51.6% | batch:        33 of        64\t|\tloss: 1498.05\n",
      "Training Epoch 86  53.1% | batch:        34 of        64\t|\tloss: 1512.24\n",
      "Training Epoch 86  54.7% | batch:        35 of        64\t|\tloss: 848.34\n",
      "Training Epoch 86  56.2% | batch:        36 of        64\t|\tloss: 1043.57\n",
      "Training Epoch 86  57.8% | batch:        37 of        64\t|\tloss: 1075.99\n",
      "Training Epoch 86  59.4% | batch:        38 of        64\t|\tloss: 1007.97\n",
      "Training Epoch 86  60.9% | batch:        39 of        64\t|\tloss: 911.1\n",
      "Training Epoch 86  62.5% | batch:        40 of        64\t|\tloss: 775.037\n",
      "Training Epoch 86  64.1% | batch:        41 of        64\t|\tloss: 820.025\n",
      "Training Epoch 86  65.6% | batch:        42 of        64\t|\tloss: 1098.48\n",
      "Training Epoch 86  67.2% | batch:        43 of        64\t|\tloss: 813.412\n",
      "Training Epoch 86  68.8% | batch:        44 of        64\t|\tloss: 2695.58\n",
      "Training Epoch 86  70.3% | batch:        45 of        64\t|\tloss: 883.591\n",
      "Training Epoch 86  71.9% | batch:        46 of        64\t|\tloss: 1277.34\n",
      "Training Epoch 86  73.4% | batch:        47 of        64\t|\tloss: 584.204\n",
      "Training Epoch 86  75.0% | batch:        48 of        64\t|\tloss: 1338.27\n",
      "Training Epoch 86  76.6% | batch:        49 of        64\t|\tloss: 850.511\n",
      "Training Epoch 86  78.1% | batch:        50 of        64\t|\tloss: 781.886\n",
      "Training Epoch 86  79.7% | batch:        51 of        64\t|\tloss: 749.119\n",
      "Training Epoch 86  81.2% | batch:        52 of        64\t|\tloss: 1029.49\n",
      "Training Epoch 86  82.8% | batch:        53 of        64\t|\tloss: 653.85\n",
      "Training Epoch 86  84.4% | batch:        54 of        64\t|\tloss: 571.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:45,633 | INFO : Epoch 86 Training Summary: epoch: 86.000000 | loss: 1230.898231 | \n",
      "2023-05-10 17:09:45,633 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.252126932144165 seconds\n",
      "\n",
      "2023-05-10 17:09:45,634 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1994664613590684 seconds\n",
      "2023-05-10 17:09:45,634 | INFO : Avg batch train. time: 0.018741663458735444 seconds\n",
      "2023-05-10 17:09:45,635 | INFO : Avg sample train. time: 0.00029704469077738196 seconds\n",
      "2023-05-10 17:09:45,635 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 86  85.9% | batch:        55 of        64\t|\tloss: 3098.65\n",
      "Training Epoch 86  87.5% | batch:        56 of        64\t|\tloss: 1537.88\n",
      "Training Epoch 86  89.1% | batch:        57 of        64\t|\tloss: 1257.61\n",
      "Training Epoch 86  90.6% | batch:        58 of        64\t|\tloss: 1868.68\n",
      "Training Epoch 86  92.2% | batch:        59 of        64\t|\tloss: 1129.5\n",
      "Training Epoch 86  93.8% | batch:        60 of        64\t|\tloss: 1452.06\n",
      "Training Epoch 86  95.3% | batch:        61 of        64\t|\tloss: 1170.08\n",
      "Training Epoch 86  96.9% | batch:        62 of        64\t|\tloss: 840.209\n",
      "Training Epoch 86  98.4% | batch:        63 of        64\t|\tloss: 926.853\n",
      "\n",
      "Evaluating Epoch 86   0.0% | batch:         0 of        16\t|\tloss: 1714.6\n",
      "Evaluating Epoch 86   6.2% | batch:         1 of        16\t|\tloss: 814.353\n",
      "Evaluating Epoch 86  12.5% | batch:         2 of        16\t|\tloss: 750.098\n",
      "Evaluating Epoch 86  18.8% | batch:         3 of        16\t|\tloss: 1345.26\n",
      "Evaluating Epoch 86  25.0% | batch:         4 of        16\t|\tloss: 1797.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:45,783 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14753127098083496 seconds\n",
      "\n",
      "2023-05-10 17:09:45,784 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.16594119601779514 seconds\n",
      "2023-05-10 17:09:45,784 | INFO : Avg batch val. time: 0.010371324751112196 seconds\n",
      "2023-05-10 17:09:45,785 | INFO : Avg sample val. time: 0.0001642982138790051 seconds\n",
      "2023-05-10 17:09:45,785 | INFO : Epoch 86 Validation Summary: epoch: 86.000000 | loss: 1890.956807 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 86  31.2% | batch:         5 of        16\t|\tloss: 1751.97\n",
      "Evaluating Epoch 86  37.5% | batch:         6 of        16\t|\tloss: 2281.4\n",
      "Evaluating Epoch 86  43.8% | batch:         7 of        16\t|\tloss: 2066.77\n",
      "Evaluating Epoch 86  50.0% | batch:         8 of        16\t|\tloss: 1718.66\n",
      "Evaluating Epoch 86  56.2% | batch:         9 of        16\t|\tloss: 6755.5\n",
      "Evaluating Epoch 86  62.5% | batch:        10 of        16\t|\tloss: 1543.34\n",
      "Evaluating Epoch 86  68.8% | batch:        11 of        16\t|\tloss: 1828.05\n",
      "Evaluating Epoch 86  75.0% | batch:        12 of        16\t|\tloss: 1309.03\n",
      "Evaluating Epoch 86  81.2% | batch:        13 of        16\t|\tloss: 1127.75\n",
      "Evaluating Epoch 86  87.5% | batch:        14 of        16\t|\tloss: 1373.45\n",
      "Evaluating Epoch 86  93.8% | batch:        15 of        16\t|\tloss: 2129.39\n",
      "\n",
      "Training Epoch 87   0.0% | batch:         0 of        64\t|\tloss: 1610.23\n",
      "Training Epoch 87   1.6% | batch:         1 of        64\t|\tloss: 1636.32\n",
      "Training Epoch 87   3.1% | batch:         2 of        64\t|\tloss: 1575.42\n",
      "Training Epoch 87   4.7% | batch:         3 of        64\t|\tloss: 1362.76\n",
      "Training Epoch 87   6.2% | batch:         4 of        64\t|\tloss: 869.402\n",
      "Training Epoch 87   7.8% | batch:         5 of        64\t|\tloss: 1125.96\n",
      "Training Epoch 87   9.4% | batch:         6 of        64\t|\tloss: 1103.4\n",
      "Training Epoch 87  10.9% | batch:         7 of        64\t|\tloss: 905.74\n",
      "Training Epoch 87  12.5% | batch:         8 of        64\t|\tloss: 1098.23\n",
      "Training Epoch 87  14.1% | batch:         9 of        64\t|\tloss: 747.475\n",
      "Training Epoch 87  15.6% | batch:        10 of        64\t|\tloss: 789.656\n",
      "Training Epoch 87  17.2% | batch:        11 of        64\t|\tloss: 641.331\n",
      "Training Epoch 87  18.8% | batch:        12 of        64\t|\tloss: 1080.26\n",
      "Training Epoch 87  20.3% | batch:        13 of        64\t|\tloss: 1264.71\n",
      "Training Epoch 87  21.9% | batch:        14 of        64\t|\tloss: 1600.05\n",
      "Training Epoch 87  23.4% | batch:        15 of        64\t|\tloss: 1150.75\n",
      "Training Epoch 87  25.0% | batch:        16 of        64\t|\tloss: 550.434\n",
      "Training Epoch 87  26.6% | batch:        17 of        64\t|\tloss: 1092.25\n",
      "Training Epoch 87  28.1% | batch:        18 of        64\t|\tloss: 1242.35\n",
      "Training Epoch 87  29.7% | batch:        19 of        64\t|\tloss: 2090\n",
      "Training Epoch 87  31.2% | batch:        20 of        64\t|\tloss: 806.862\n",
      "Training Epoch 87  32.8% | batch:        21 of        64\t|\tloss: 953.909\n",
      "Training Epoch 87  34.4% | batch:        22 of        64\t|\tloss: 619.383\n",
      "Training Epoch 87  35.9% | batch:        23 of        64\t|\tloss: 721.501\n",
      "Training Epoch 87  37.5% | batch:        24 of        64\t|\tloss: 1242.67\n",
      "Training Epoch 87  39.1% | batch:        25 of        64\t|\tloss: 1018.83\n",
      "Training Epoch 87  40.6% | batch:        26 of        64\t|\tloss: 514.721\n",
      "Training Epoch 87  42.2% | batch:        27 of        64\t|\tloss: 1129.53\n",
      "Training Epoch 87  43.8% | batch:        28 of        64\t|\tloss: 725.136\n",
      "Training Epoch 87  45.3% | batch:        29 of        64\t|\tloss: 1046.07\n",
      "Training Epoch 87  46.9% | batch:        30 of        64\t|\tloss: 1233.42\n",
      "Training Epoch 87  48.4% | batch:        31 of        64\t|\tloss: 910.225\n",
      "Training Epoch 87  50.0% | batch:        32 of        64\t|\tloss: 534.202\n",
      "Training Epoch 87  51.6% | batch:        33 of        64\t|\tloss: 931.341\n",
      "Training Epoch 87  53.1% | batch:        34 of        64\t|\tloss: 760.045\n",
      "Training Epoch 87  54.7% | batch:        35 of        64\t|\tloss: 1007.62\n",
      "Training Epoch 87  56.2% | batch:        36 of        64\t|\tloss: 874.454\n",
      "Training Epoch 87  57.8% | batch:        37 of        64\t|\tloss: 917.543\n",
      "Training Epoch 87  59.4% | batch:        38 of        64\t|\tloss: 1257.13\n",
      "Training Epoch 87  60.9% | batch:        39 of        64\t|\tloss: 2727.81\n",
      "Training Epoch 87  62.5% | batch:        40 of        64\t|\tloss: 554.398\n",
      "Training Epoch 87  64.1% | batch:        41 of        64\t|\tloss: 2252.32\n",
      "Training Epoch 87  65.6% | batch:        42 of        64\t|\tloss: 2642.07\n",
      "Training Epoch 87  67.2% | batch:        43 of        64\t|\tloss: 1426.99\n",
      "Training Epoch 87  68.8% | batch:        44 of        64\t|\tloss: 880.934\n",
      "Training Epoch 87  70.3% | batch:        45 of        64\t|\tloss: 1061.28\n",
      "Training Epoch 87  71.9% | batch:        46 of        64\t|\tloss: 1373.8\n",
      "Training Epoch 87  73.4% | batch:        47 of        64\t|\tloss: 1160.74\n",
      "Training Epoch 87  75.0% | batch:        48 of        64\t|\tloss: 1170.93\n",
      "Training Epoch 87  76.6% | batch:        49 of        64\t|\tloss: 873.818\n",
      "Training Epoch 87  78.1% | batch:        50 of        64\t|\tloss: 1231.31\n",
      "Training Epoch 87  79.7% | batch:        51 of        64\t|\tloss: 1183.31\n",
      "Training Epoch 87  81.2% | batch:        52 of        64\t|\tloss: 728.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:46,898 | INFO : Epoch 87 Training Summary: epoch: 87.000000 | loss: 1144.665414 | \n",
      "2023-05-10 17:09:46,898 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1030902862548828 seconds\n",
      "\n",
      "2023-05-10 17:09:46,899 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1983586892314342 seconds\n",
      "2023-05-10 17:09:46,900 | INFO : Avg batch train. time: 0.01872435451924116 seconds\n",
      "2023-05-10 17:09:46,900 | INFO : Avg sample train. time: 0.0002967703539453775 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 87  82.8% | batch:        53 of        64\t|\tloss: 1701.43\n",
      "Training Epoch 87  84.4% | batch:        54 of        64\t|\tloss: 1687.87\n",
      "Training Epoch 87  85.9% | batch:        55 of        64\t|\tloss: 679.097\n",
      "Training Epoch 87  87.5% | batch:        56 of        64\t|\tloss: 1621.13\n",
      "Training Epoch 87  89.1% | batch:        57 of        64\t|\tloss: 1087.69\n",
      "Training Epoch 87  90.6% | batch:        58 of        64\t|\tloss: 1029.07\n",
      "Training Epoch 87  92.2% | batch:        59 of        64\t|\tloss: 1710.12\n",
      "Training Epoch 87  93.8% | batch:        60 of        64\t|\tloss: 587.492\n",
      "Training Epoch 87  95.3% | batch:        61 of        64\t|\tloss: 645.945\n",
      "Training Epoch 87  96.9% | batch:        62 of        64\t|\tloss: 1010.58\n",
      "Training Epoch 87  98.4% | batch:        63 of        64\t|\tloss: 4812.66\n",
      "\n",
      "Training Epoch 88   0.0% | batch:         0 of        64\t|\tloss: 1574.49\n",
      "Training Epoch 88   1.6% | batch:         1 of        64\t|\tloss: 1194.55\n",
      "Training Epoch 88   3.1% | batch:         2 of        64\t|\tloss: 969.928\n",
      "Training Epoch 88   4.7% | batch:         3 of        64\t|\tloss: 524.01\n",
      "Training Epoch 88   6.2% | batch:         4 of        64\t|\tloss: 796.159\n",
      "Training Epoch 88   7.8% | batch:         5 of        64\t|\tloss: 1149.57\n",
      "Training Epoch 88   9.4% | batch:         6 of        64\t|\tloss: 687.511\n",
      "Training Epoch 88  10.9% | batch:         7 of        64\t|\tloss: 846.727\n",
      "Training Epoch 88  12.5% | batch:         8 of        64\t|\tloss: 1113.24\n",
      "Training Epoch 88  14.1% | batch:         9 of        64\t|\tloss: 964.852\n",
      "Training Epoch 88  15.6% | batch:        10 of        64\t|\tloss: 2061.44\n",
      "Training Epoch 88  17.2% | batch:        11 of        64\t|\tloss: 989.532\n",
      "Training Epoch 88  18.8% | batch:        12 of        64\t|\tloss: 1095.82\n",
      "Training Epoch 88  20.3% | batch:        13 of        64\t|\tloss: 594.629\n",
      "Training Epoch 88  21.9% | batch:        14 of        64\t|\tloss: 777.769\n",
      "Training Epoch 88  23.4% | batch:        15 of        64\t|\tloss: 809.891\n",
      "Training Epoch 88  25.0% | batch:        16 of        64\t|\tloss: 1856.61\n",
      "Training Epoch 88  26.6% | batch:        17 of        64\t|\tloss: 1067.42\n",
      "Training Epoch 88  28.1% | batch:        18 of        64\t|\tloss: 1144.07\n",
      "Training Epoch 88  29.7% | batch:        19 of        64\t|\tloss: 1603.15\n",
      "Training Epoch 88  31.2% | batch:        20 of        64\t|\tloss: 1730.95\n",
      "Training Epoch 88  32.8% | batch:        21 of        64\t|\tloss: 948.374\n",
      "Training Epoch 88  34.4% | batch:        22 of        64\t|\tloss: 752.767\n",
      "Training Epoch 88  35.9% | batch:        23 of        64\t|\tloss: 3218.11\n",
      "Training Epoch 88  37.5% | batch:        24 of        64\t|\tloss: 600.433\n",
      "Training Epoch 88  39.1% | batch:        25 of        64\t|\tloss: 456.521\n",
      "Training Epoch 88  40.6% | batch:        26 of        64\t|\tloss: 1976.09\n",
      "Training Epoch 88  42.2% | batch:        27 of        64\t|\tloss: 770.188\n",
      "Training Epoch 88  43.8% | batch:        28 of        64\t|\tloss: 635.351\n",
      "Training Epoch 88  45.3% | batch:        29 of        64\t|\tloss: 1110.41\n",
      "Training Epoch 88  46.9% | batch:        30 of        64\t|\tloss: 498.748\n",
      "Training Epoch 88  48.4% | batch:        31 of        64\t|\tloss: 2706.61\n",
      "Training Epoch 88  50.0% | batch:        32 of        64\t|\tloss: 633.37\n",
      "Training Epoch 88  51.6% | batch:        33 of        64\t|\tloss: 1200.48\n",
      "Training Epoch 88  53.1% | batch:        34 of        64\t|\tloss: 1157.28\n",
      "Training Epoch 88  54.7% | batch:        35 of        64\t|\tloss: 923.924\n",
      "Training Epoch 88  56.2% | batch:        36 of        64\t|\tloss: 660.751\n",
      "Training Epoch 88  57.8% | batch:        37 of        64\t|\tloss: 1058.88\n",
      "Training Epoch 88  59.4% | batch:        38 of        64\t|\tloss: 1235.69\n",
      "Training Epoch 88  60.9% | batch:        39 of        64\t|\tloss: 932.648\n",
      "Training Epoch 88  62.5% | batch:        40 of        64\t|\tloss: 683.144\n",
      "Training Epoch 88  64.1% | batch:        41 of        64\t|\tloss: 1602.04\n",
      "Training Epoch 88  65.6% | batch:        42 of        64\t|\tloss: 934.115\n",
      "Training Epoch 88  67.2% | batch:        43 of        64\t|\tloss: 850.734\n",
      "Training Epoch 88  68.8% | batch:        44 of        64\t|\tloss: 646.485\n",
      "Training Epoch 88  70.3% | batch:        45 of        64\t|\tloss: 805.433\n",
      "Training Epoch 88  71.9% | batch:        46 of        64\t|\tloss: 783.477\n",
      "Training Epoch 88  73.4% | batch:        47 of        64\t|\tloss: 940.49\n",
      "Training Epoch 88  75.0% | batch:        48 of        64\t|\tloss: 546.904\n",
      "Training Epoch 88  76.6% | batch:        49 of        64\t|\tloss: 2609.56\n",
      "Training Epoch 88  78.1% | batch:        50 of        64\t|\tloss: 1687.97\n",
      "Training Epoch 88  79.7% | batch:        51 of        64\t|\tloss: 1221.52\n",
      "Training Epoch 88  81.2% | batch:        52 of        64\t|\tloss: 814.783\n",
      "Training Epoch 88  82.8% | batch:        53 of        64\t|\tloss: 943.984\n",
      "Training Epoch 88  84.4% | batch:        54 of        64\t|\tloss: 502.119\n",
      "Training Epoch 88  85.9% | batch:        55 of        64\t|\tloss: 491.892\n",
      "Training Epoch 88  87.5% | batch:        56 of        64\t|\tloss: 1754.55\n",
      "Training Epoch 88  89.1% | batch:        57 of        64\t|\tloss: 902.966\n",
      "Training Epoch 88  90.6% | batch:        58 of        64\t|\tloss: 808.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:48,074 | INFO : Epoch 88 Training Summary: epoch: 88.000000 | loss: 1127.955167 | \n",
      "2023-05-10 17:09:48,075 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1652209758758545 seconds\n",
      "\n",
      "2023-05-10 17:09:48,075 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1979821243069388 seconds\n",
      "2023-05-10 17:09:48,076 | INFO : Avg batch train. time: 0.01871847069229592 seconds\n",
      "2023-05-10 17:09:48,076 | INFO : Avg sample train. time: 0.000296677098639658 seconds\n",
      "2023-05-10 17:09:48,077 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 88  92.2% | batch:        59 of        64\t|\tloss: 1014.61\n",
      "Training Epoch 88  93.8% | batch:        60 of        64\t|\tloss: 1865.58\n",
      "Training Epoch 88  95.3% | batch:        61 of        64\t|\tloss: 1796.43\n",
      "Training Epoch 88  96.9% | batch:        62 of        64\t|\tloss: 1867.67\n",
      "Training Epoch 88  98.4% | batch:        63 of        64\t|\tloss: 672.661\n",
      "\n",
      "Evaluating Epoch 88   0.0% | batch:         0 of        16\t|\tloss: 1520.27\n",
      "Evaluating Epoch 88   6.2% | batch:         1 of        16\t|\tloss: 900.039\n",
      "Evaluating Epoch 88  12.5% | batch:         2 of        16\t|\tloss: 1147.08\n",
      "Evaluating Epoch 88  18.8% | batch:         3 of        16\t|\tloss: 1031.76\n",
      "Evaluating Epoch 88  25.0% | batch:         4 of        16\t|\tloss: 1466.56\n",
      "Evaluating Epoch 88  31.2% | batch:         5 of        16\t|\tloss: 1672.62\n",
      "Evaluating Epoch 88  37.5% | batch:         6 of        16\t|\tloss: 2272.07\n",
      "Evaluating Epoch 88  43.8% | batch:         7 of        16\t|\tloss: 1896.63\n",
      "Evaluating Epoch 88  50.0% | batch:         8 of        16\t|\tloss: 1433.29\n",
      "Evaluating Epoch 88  56.2% | batch:         9 of        16\t|\tloss: 8328.48\n",
      "Evaluating Epoch 88  62.5% | batch:        10 of        16\t|\tloss: 1655.22\n",
      "Evaluating Epoch 88  68.8% | batch:        11 of        16\t|\tloss: 1791.92\n",
      "Evaluating Epoch 88  75.0% | batch:        12 of        16\t|\tloss: 1042.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:48,230 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.1526038646697998 seconds\n",
      "\n",
      "2023-05-10 17:09:48,230 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.16565125403196915 seconds\n",
      "2023-05-10 17:09:48,231 | INFO : Avg batch val. time: 0.010353203376998072 seconds\n",
      "2023-05-10 17:09:48,231 | INFO : Avg sample val. time: 0.00016401114260591006 seconds\n",
      "2023-05-10 17:09:48,232 | INFO : Epoch 88 Validation Summary: epoch: 88.000000 | loss: 1963.863216 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 88  81.2% | batch:        13 of        16\t|\tloss: 1326.89\n",
      "Evaluating Epoch 88  87.5% | batch:        14 of        16\t|\tloss: 1590.44\n",
      "Evaluating Epoch 88  93.8% | batch:        15 of        16\t|\tloss: 2452.81\n",
      "\n",
      "Training Epoch 89   0.0% | batch:         0 of        64\t|\tloss: 667.877\n",
      "Training Epoch 89   1.6% | batch:         1 of        64\t|\tloss: 806.807\n",
      "Training Epoch 89   3.1% | batch:         2 of        64\t|\tloss: 693.204\n",
      "Training Epoch 89   4.7% | batch:         3 of        64\t|\tloss: 571.573\n",
      "Training Epoch 89   6.2% | batch:         4 of        64\t|\tloss: 952.67\n",
      "Training Epoch 89   7.8% | batch:         5 of        64\t|\tloss: 3308.02\n",
      "Training Epoch 89   9.4% | batch:         6 of        64\t|\tloss: 1630.01\n",
      "Training Epoch 89  10.9% | batch:         7 of        64\t|\tloss: 953.719\n",
      "Training Epoch 89  12.5% | batch:         8 of        64\t|\tloss: 1288.18\n",
      "Training Epoch 89  14.1% | batch:         9 of        64\t|\tloss: 2063.9\n",
      "Training Epoch 89  15.6% | batch:        10 of        64\t|\tloss: 1541.82\n",
      "Training Epoch 89  17.2% | batch:        11 of        64\t|\tloss: 1073.81\n",
      "Training Epoch 89  18.8% | batch:        12 of        64\t|\tloss: 2161.81\n",
      "Training Epoch 89  20.3% | batch:        13 of        64\t|\tloss: 737.536\n",
      "Training Epoch 89  21.9% | batch:        14 of        64\t|\tloss: 992.922\n",
      "Training Epoch 89  23.4% | batch:        15 of        64\t|\tloss: 1112.69\n",
      "Training Epoch 89  25.0% | batch:        16 of        64\t|\tloss: 857.071\n",
      "Training Epoch 89  26.6% | batch:        17 of        64\t|\tloss: 686.922\n",
      "Training Epoch 89  28.1% | batch:        18 of        64\t|\tloss: 758.504\n",
      "Training Epoch 89  29.7% | batch:        19 of        64\t|\tloss: 609.217\n",
      "Training Epoch 89  31.2% | batch:        20 of        64\t|\tloss: 917.105\n",
      "Training Epoch 89  32.8% | batch:        21 of        64\t|\tloss: 1197.11\n",
      "Training Epoch 89  34.4% | batch:        22 of        64\t|\tloss: 753.121\n",
      "Training Epoch 89  35.9% | batch:        23 of        64\t|\tloss: 568.649\n",
      "Training Epoch 89  37.5% | batch:        24 of        64\t|\tloss: 957.221\n",
      "Training Epoch 89  39.1% | batch:        25 of        64\t|\tloss: 1035.14\n",
      "Training Epoch 89  40.6% | batch:        26 of        64\t|\tloss: 577.91\n",
      "Training Epoch 89  42.2% | batch:        27 of        64\t|\tloss: 1054.91\n",
      "Training Epoch 89  43.8% | batch:        28 of        64\t|\tloss: 945.537\n",
      "Training Epoch 89  45.3% | batch:        29 of        64\t|\tloss: 900.281\n",
      "Training Epoch 89  46.9% | batch:        30 of        64\t|\tloss: 1114.79\n",
      "Training Epoch 89  48.4% | batch:        31 of        64\t|\tloss: 1100.97\n",
      "Training Epoch 89  50.0% | batch:        32 of        64\t|\tloss: 1478.12\n",
      "Training Epoch 89  51.6% | batch:        33 of        64\t|\tloss: 1206.11\n",
      "Training Epoch 89  53.1% | batch:        34 of        64\t|\tloss: 1037.56\n",
      "Training Epoch 89  54.7% | batch:        35 of        64\t|\tloss: 876.222\n",
      "Training Epoch 89  56.2% | batch:        36 of        64\t|\tloss: 979.095\n",
      "Training Epoch 89  57.8% | batch:        37 of        64\t|\tloss: 1092.36\n",
      "Training Epoch 89  59.4% | batch:        38 of        64\t|\tloss: 1523.55\n",
      "Training Epoch 89  60.9% | batch:        39 of        64\t|\tloss: 2512.63\n",
      "Training Epoch 89  62.5% | batch:        40 of        64\t|\tloss: 507.786\n",
      "Training Epoch 89  64.1% | batch:        41 of        64\t|\tloss: 718.191\n",
      "Training Epoch 89  65.6% | batch:        42 of        64\t|\tloss: 575.802\n",
      "Training Epoch 89  67.2% | batch:        43 of        64\t|\tloss: 1132.1\n",
      "Training Epoch 89  68.8% | batch:        44 of        64\t|\tloss: 772.548\n",
      "Training Epoch 89  70.3% | batch:        45 of        64\t|\tloss: 602.498\n",
      "Training Epoch 89  71.9% | batch:        46 of        64\t|\tloss: 734.725\n",
      "Training Epoch 89  73.4% | batch:        47 of        64\t|\tloss: 827.169\n",
      "Training Epoch 89  75.0% | batch:        48 of        64\t|\tloss: 940.464\n",
      "Training Epoch 89  76.6% | batch:        49 of        64\t|\tloss: 877.164\n",
      "Training Epoch 89  78.1% | batch:        50 of        64\t|\tloss: 1179.02\n",
      "Training Epoch 89  79.7% | batch:        51 of        64\t|\tloss: 818.051\n",
      "Training Epoch 89  81.2% | batch:        52 of        64\t|\tloss: 1009.46\n",
      "Training Epoch 89  82.8% | batch:        53 of        64\t|\tloss: 827.407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:49,408 | INFO : Epoch 89 Training Summary: epoch: 89.000000 | loss: 1048.364169 | \n",
      "2023-05-10 17:09:49,409 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1671929359436035 seconds\n",
      "\n",
      "2023-05-10 17:09:49,410 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1976361783702723 seconds\n",
      "2023-05-10 17:09:49,410 | INFO : Avg batch train. time: 0.018713065287035504 seconds\n",
      "2023-05-10 17:09:49,410 | INFO : Avg sample train. time: 0.00029659142604513924 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 89  84.4% | batch:        54 of        64\t|\tloss: 1860.61\n",
      "Training Epoch 89  85.9% | batch:        55 of        64\t|\tloss: 768.358\n",
      "Training Epoch 89  87.5% | batch:        56 of        64\t|\tloss: 1473.44\n",
      "Training Epoch 89  89.1% | batch:        57 of        64\t|\tloss: 670.927\n",
      "Training Epoch 89  90.6% | batch:        58 of        64\t|\tloss: 1041.12\n",
      "Training Epoch 89  92.2% | batch:        59 of        64\t|\tloss: 804.47\n",
      "Training Epoch 89  93.8% | batch:        60 of        64\t|\tloss: 993.121\n",
      "Training Epoch 89  95.3% | batch:        61 of        64\t|\tloss: 753.193\n",
      "Training Epoch 89  96.9% | batch:        62 of        64\t|\tloss: 643.452\n",
      "Training Epoch 89  98.4% | batch:        63 of        64\t|\tloss: 3386.51\n",
      "\n",
      "Training Epoch 90   0.0% | batch:         0 of        64\t|\tloss: 685.199\n",
      "Training Epoch 90   1.6% | batch:         1 of        64\t|\tloss: 1749.57\n",
      "Training Epoch 90   3.1% | batch:         2 of        64\t|\tloss: 762.678\n",
      "Training Epoch 90   4.7% | batch:         3 of        64\t|\tloss: 711.902\n",
      "Training Epoch 90   6.2% | batch:         4 of        64\t|\tloss: 781.166\n",
      "Training Epoch 90   7.8% | batch:         5 of        64\t|\tloss: 685.713\n",
      "Training Epoch 90   9.4% | batch:         6 of        64\t|\tloss: 748.994\n",
      "Training Epoch 90  10.9% | batch:         7 of        64\t|\tloss: 655.563\n",
      "Training Epoch 90  12.5% | batch:         8 of        64\t|\tloss: 1094.85\n",
      "Training Epoch 90  14.1% | batch:         9 of        64\t|\tloss: 1211.64\n",
      "Training Epoch 90  15.6% | batch:        10 of        64\t|\tloss: 718.265\n",
      "Training Epoch 90  17.2% | batch:        11 of        64\t|\tloss: 1014.34\n",
      "Training Epoch 90  18.8% | batch:        12 of        64\t|\tloss: 759.123\n",
      "Training Epoch 90  20.3% | batch:        13 of        64\t|\tloss: 2357.49\n",
      "Training Epoch 90  21.9% | batch:        14 of        64\t|\tloss: 695.578\n",
      "Training Epoch 90  23.4% | batch:        15 of        64\t|\tloss: 948.964\n",
      "Training Epoch 90  25.0% | batch:        16 of        64\t|\tloss: 1108.17\n",
      "Training Epoch 90  26.6% | batch:        17 of        64\t|\tloss: 880.743\n",
      "Training Epoch 90  28.1% | batch:        18 of        64\t|\tloss: 1771.51\n",
      "Training Epoch 90  29.7% | batch:        19 of        64\t|\tloss: 814.89\n",
      "Training Epoch 90  31.2% | batch:        20 of        64\t|\tloss: 629.947\n",
      "Training Epoch 90  32.8% | batch:        21 of        64\t|\tloss: 1931.56\n",
      "Training Epoch 90  34.4% | batch:        22 of        64\t|\tloss: 590.402\n",
      "Training Epoch 90  35.9% | batch:        23 of        64\t|\tloss: 1165.42\n",
      "Training Epoch 90  37.5% | batch:        24 of        64\t|\tloss: 833.809\n",
      "Training Epoch 90  39.1% | batch:        25 of        64\t|\tloss: 913.822\n",
      "Training Epoch 90  40.6% | batch:        26 of        64\t|\tloss: 570.08\n",
      "Training Epoch 90  42.2% | batch:        27 of        64\t|\tloss: 803.319\n",
      "Training Epoch 90  43.8% | batch:        28 of        64\t|\tloss: 960.752\n",
      "Training Epoch 90  45.3% | batch:        29 of        64\t|\tloss: 1859.85\n",
      "Training Epoch 90  46.9% | batch:        30 of        64\t|\tloss: 629.236\n",
      "Training Epoch 90  48.4% | batch:        31 of        64\t|\tloss: 1084.9\n",
      "Training Epoch 90  50.0% | batch:        32 of        64\t|\tloss: 1407.16\n",
      "Training Epoch 90  51.6% | batch:        33 of        64\t|\tloss: 1001.24\n",
      "Training Epoch 90  53.1% | batch:        34 of        64\t|\tloss: 704.39\n",
      "Training Epoch 90  54.7% | batch:        35 of        64\t|\tloss: 990.831\n",
      "Training Epoch 90  56.2% | batch:        36 of        64\t|\tloss: 1175.01\n",
      "Training Epoch 90  57.8% | batch:        37 of        64\t|\tloss: 950.661\n",
      "Training Epoch 90  59.4% | batch:        38 of        64\t|\tloss: 846.778\n",
      "Training Epoch 90  60.9% | batch:        39 of        64\t|\tloss: 752.945\n",
      "Training Epoch 90  62.5% | batch:        40 of        64\t|\tloss: 882.008\n",
      "Training Epoch 90  64.1% | batch:        41 of        64\t|\tloss: 661.562\n",
      "Training Epoch 90  65.6% | batch:        42 of        64\t|\tloss: 1389.6\n",
      "Training Epoch 90  67.2% | batch:        43 of        64\t|\tloss: 1374.79\n",
      "Training Epoch 90  68.8% | batch:        44 of        64\t|\tloss: 802.098\n",
      "Training Epoch 90  70.3% | batch:        45 of        64\t|\tloss: 709.941\n",
      "Training Epoch 90  71.9% | batch:        46 of        64\t|\tloss: 1008.4\n",
      "Training Epoch 90  73.4% | batch:        47 of        64\t|\tloss: 810.451\n",
      "Training Epoch 90  75.0% | batch:        48 of        64\t|\tloss: 1802.31\n",
      "Training Epoch 90  76.6% | batch:        49 of        64\t|\tloss: 564.138\n",
      "Training Epoch 90  78.1% | batch:        50 of        64\t|\tloss: 585.264\n",
      "Training Epoch 90  79.7% | batch:        51 of        64\t|\tloss: 1281\n",
      "Training Epoch 90  81.2% | batch:        52 of        64\t|\tloss: 795.891\n",
      "Training Epoch 90  82.8% | batch:        53 of        64\t|\tloss: 828.726\n",
      "Training Epoch 90  84.4% | batch:        54 of        64\t|\tloss: 969.338\n",
      "Training Epoch 90  85.9% | batch:        55 of        64\t|\tloss: 927.492\n",
      "Training Epoch 90  87.5% | batch:        56 of        64\t|\tloss: 707.509\n",
      "Training Epoch 90  89.1% | batch:        57 of        64\t|\tloss: 2572.31\n",
      "Training Epoch 90  90.6% | batch:        58 of        64\t|\tloss: 1620.13\n",
      "Training Epoch 90  92.2% | batch:        59 of        64\t|\tloss: 656.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:50,570 | INFO : Epoch 90 Training Summary: epoch: 90.000000 | loss: 1027.029093 | \n",
      "2023-05-10 17:09:50,570 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1505162715911865 seconds\n",
      "\n",
      "2023-05-10 17:09:50,571 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1971126238505045 seconds\n",
      "2023-05-10 17:09:50,571 | INFO : Avg batch train. time: 0.018704884747664133 seconds\n",
      "2023-05-10 17:09:50,572 | INFO : Avg sample train. time: 0.00029646176915564746 seconds\n",
      "2023-05-10 17:09:50,572 | INFO : Evaluating on validation set ...\n",
      "2023-05-10 17:09:50,720 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14790892601013184 seconds\n",
      "\n",
      "2023-05-10 17:09:50,721 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.165273757691079 seconds\n",
      "2023-05-10 17:09:50,721 | INFO : Avg batch val. time: 0.010329609855692437 seconds\n",
      "2023-05-10 17:09:50,721 | INFO : Avg sample val. time: 0.00016363738385255345 seconds\n",
      "2023-05-10 17:09:50,722 | INFO : Epoch 90 Validation Summary: epoch: 90.000000 | loss: 1787.612369 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 90  93.8% | batch:        60 of        64\t|\tloss: 834.47\n",
      "Training Epoch 90  95.3% | batch:        61 of        64\t|\tloss: 1325.97\n",
      "Training Epoch 90  96.9% | batch:        62 of        64\t|\tloss: 1330.55\n",
      "Training Epoch 90  98.4% | batch:        63 of        64\t|\tloss: 4268.47\n",
      "\n",
      "Evaluating Epoch 90   0.0% | batch:         0 of        16\t|\tloss: 1170.86\n",
      "Evaluating Epoch 90   6.2% | batch:         1 of        16\t|\tloss: 1252.99\n",
      "Evaluating Epoch 90  12.5% | batch:         2 of        16\t|\tloss: 1071.81\n",
      "Evaluating Epoch 90  18.8% | batch:         3 of        16\t|\tloss: 793.174\n",
      "Evaluating Epoch 90  25.0% | batch:         4 of        16\t|\tloss: 1228.46\n",
      "Evaluating Epoch 90  31.2% | batch:         5 of        16\t|\tloss: 1524.08\n",
      "Evaluating Epoch 90  37.5% | batch:         6 of        16\t|\tloss: 2256.88\n",
      "Evaluating Epoch 90  43.8% | batch:         7 of        16\t|\tloss: 3013.26\n",
      "Evaluating Epoch 90  50.0% | batch:         8 of        16\t|\tloss: 1185.83\n",
      "Evaluating Epoch 90  56.2% | batch:         9 of        16\t|\tloss: 4161.1\n",
      "Evaluating Epoch 90  62.5% | batch:        10 of        16\t|\tloss: 1302.47\n",
      "Evaluating Epoch 90  68.8% | batch:        11 of        16\t|\tloss: 3185.44\n",
      "Evaluating Epoch 90  75.0% | batch:        12 of        16\t|\tloss: 1734.68\n",
      "Evaluating Epoch 90  81.2% | batch:        13 of        16\t|\tloss: 1644.18\n",
      "Evaluating Epoch 90  87.5% | batch:        14 of        16\t|\tloss: 1356.54\n",
      "Evaluating Epoch 90  93.8% | batch:        15 of        16\t|\tloss: 1701.13\n",
      "\n",
      "Training Epoch 91   0.0% | batch:         0 of        64\t|\tloss: 2132.16\n",
      "Training Epoch 91   1.6% | batch:         1 of        64\t|\tloss: 826.357\n",
      "Training Epoch 91   3.1% | batch:         2 of        64\t|\tloss: 1131.82\n",
      "Training Epoch 91   4.7% | batch:         3 of        64\t|\tloss: 1477.94\n",
      "Training Epoch 91   6.2% | batch:         4 of        64\t|\tloss: 1252\n",
      "Training Epoch 91   7.8% | batch:         5 of        64\t|\tloss: 628.695\n",
      "Training Epoch 91   9.4% | batch:         6 of        64\t|\tloss: 563.762\n",
      "Training Epoch 91  10.9% | batch:         7 of        64\t|\tloss: 1002.04\n",
      "Training Epoch 91  12.5% | batch:         8 of        64\t|\tloss: 631.206\n",
      "Training Epoch 91  14.1% | batch:         9 of        64\t|\tloss: 717.339\n",
      "Training Epoch 91  15.6% | batch:        10 of        64\t|\tloss: 1636.45\n",
      "Training Epoch 91  17.2% | batch:        11 of        64\t|\tloss: 788.923\n",
      "Training Epoch 91  18.8% | batch:        12 of        64\t|\tloss: 616.638\n",
      "Training Epoch 91  20.3% | batch:        13 of        64\t|\tloss: 1052.18\n",
      "Training Epoch 91  21.9% | batch:        14 of        64\t|\tloss: 1233.21\n",
      "Training Epoch 91  23.4% | batch:        15 of        64\t|\tloss: 885.438\n",
      "Training Epoch 91  25.0% | batch:        16 of        64\t|\tloss: 1081.63\n",
      "Training Epoch 91  26.6% | batch:        17 of        64\t|\tloss: 1009.07\n",
      "Training Epoch 91  28.1% | batch:        18 of        64\t|\tloss: 1185.3\n",
      "Training Epoch 91  29.7% | batch:        19 of        64\t|\tloss: 557.786\n",
      "Training Epoch 91  31.2% | batch:        20 of        64\t|\tloss: 641.044\n",
      "Training Epoch 91  32.8% | batch:        21 of        64\t|\tloss: 833.329\n",
      "Training Epoch 91  34.4% | batch:        22 of        64\t|\tloss: 606.948\n",
      "Training Epoch 91  35.9% | batch:        23 of        64\t|\tloss: 448.163\n",
      "Training Epoch 91  37.5% | batch:        24 of        64\t|\tloss: 693.981\n",
      "Training Epoch 91  39.1% | batch:        25 of        64\t|\tloss: 1305.46\n",
      "Training Epoch 91  40.6% | batch:        26 of        64\t|\tloss: 662.668\n",
      "Training Epoch 91  42.2% | batch:        27 of        64\t|\tloss: 1356.75\n",
      "Training Epoch 91  43.8% | batch:        28 of        64\t|\tloss: 1524.86\n",
      "Training Epoch 91  45.3% | batch:        29 of        64\t|\tloss: 1241.94\n",
      "Training Epoch 91  46.9% | batch:        30 of        64\t|\tloss: 2177.54\n",
      "Training Epoch 91  48.4% | batch:        31 of        64\t|\tloss: 657.926\n",
      "Training Epoch 91  50.0% | batch:        32 of        64\t|\tloss: 1178.32\n",
      "Training Epoch 91  51.6% | batch:        33 of        64\t|\tloss: 693.046\n",
      "Training Epoch 91  53.1% | batch:        34 of        64\t|\tloss: 838.651\n",
      "Training Epoch 91  54.7% | batch:        35 of        64\t|\tloss: 698.079\n",
      "Training Epoch 91  56.2% | batch:        36 of        64\t|\tloss: 778.677\n",
      "Training Epoch 91  57.8% | batch:        37 of        64\t|\tloss: 608.111\n",
      "Training Epoch 91  59.4% | batch:        38 of        64\t|\tloss: 422.803\n",
      "Training Epoch 91  60.9% | batch:        39 of        64\t|\tloss: 2837.16\n",
      "Training Epoch 91  62.5% | batch:        40 of        64\t|\tloss: 981.664\n",
      "Training Epoch 91  64.1% | batch:        41 of        64\t|\tloss: 1462.44\n",
      "Training Epoch 91  65.6% | batch:        42 of        64\t|\tloss: 1543.48\n",
      "Training Epoch 91  67.2% | batch:        43 of        64\t|\tloss: 1097.13\n",
      "Training Epoch 91  68.8% | batch:        44 of        64\t|\tloss: 1341.55\n",
      "Training Epoch 91  70.3% | batch:        45 of        64\t|\tloss: 939.451\n",
      "Training Epoch 91  71.9% | batch:        46 of        64\t|\tloss: 746.614\n",
      "Training Epoch 91  73.4% | batch:        47 of        64\t|\tloss: 853.98\n",
      "Training Epoch 91  75.0% | batch:        48 of        64\t|\tloss: 1155.43\n",
      "Training Epoch 91  76.6% | batch:        49 of        64\t|\tloss: 951.55\n",
      "Training Epoch 91  78.1% | batch:        50 of        64\t|\tloss: 1158.17\n",
      "Training Epoch 91  79.7% | batch:        51 of        64\t|\tloss: 882.495\n",
      "Training Epoch 91  81.2% | batch:        52 of        64\t|\tloss: 785.269\n",
      "Training Epoch 91  82.8% | batch:        53 of        64\t|\tloss: 842.906\n",
      "Training Epoch 91  84.4% | batch:        54 of        64\t|\tloss: 843.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:51,933 | INFO : Epoch 91 Training Summary: epoch: 91.000000 | loss: 1065.412300 | \n",
      "2023-05-10 17:09:51,933 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.200958251953125 seconds\n",
      "\n",
      "2023-05-10 17:09:51,934 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.197154883499984 seconds\n",
      "2023-05-10 17:09:51,934 | INFO : Avg batch train. time: 0.01870554505468725 seconds\n",
      "2023-05-10 17:09:51,934 | INFO : Avg sample train. time: 0.0002964722346458603 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 91  85.9% | batch:        55 of        64\t|\tloss: 1668.72\n",
      "Training Epoch 91  87.5% | batch:        56 of        64\t|\tloss: 1654.48\n",
      "Training Epoch 91  89.1% | batch:        57 of        64\t|\tloss: 2129.3\n",
      "Training Epoch 91  90.6% | batch:        58 of        64\t|\tloss: 1256.68\n",
      "Training Epoch 91  92.2% | batch:        59 of        64\t|\tloss: 1050.04\n",
      "Training Epoch 91  93.8% | batch:        60 of        64\t|\tloss: 964.617\n",
      "Training Epoch 91  95.3% | batch:        61 of        64\t|\tloss: 1594.25\n",
      "Training Epoch 91  96.9% | batch:        62 of        64\t|\tloss: 632.122\n",
      "Training Epoch 91  98.4% | batch:        63 of        64\t|\tloss: 738.622\n",
      "\n",
      "Training Epoch 92   0.0% | batch:         0 of        64\t|\tloss: 579.616\n",
      "Training Epoch 92   1.6% | batch:         1 of        64\t|\tloss: 864.876\n",
      "Training Epoch 92   3.1% | batch:         2 of        64\t|\tloss: 1267.05\n",
      "Training Epoch 92   4.7% | batch:         3 of        64\t|\tloss: 769.288\n",
      "Training Epoch 92   6.2% | batch:         4 of        64\t|\tloss: 1194.82\n",
      "Training Epoch 92   7.8% | batch:         5 of        64\t|\tloss: 677.184\n",
      "Training Epoch 92   9.4% | batch:         6 of        64\t|\tloss: 1700.61\n",
      "Training Epoch 92  10.9% | batch:         7 of        64\t|\tloss: 1216.46\n",
      "Training Epoch 92  12.5% | batch:         8 of        64\t|\tloss: 860.908\n",
      "Training Epoch 92  14.1% | batch:         9 of        64\t|\tloss: 1091.26\n",
      "Training Epoch 92  15.6% | batch:        10 of        64\t|\tloss: 752.668\n",
      "Training Epoch 92  17.2% | batch:        11 of        64\t|\tloss: 697.529\n",
      "Training Epoch 92  18.8% | batch:        12 of        64\t|\tloss: 685.275\n",
      "Training Epoch 92  20.3% | batch:        13 of        64\t|\tloss: 833.188\n",
      "Training Epoch 92  21.9% | batch:        14 of        64\t|\tloss: 1350.64\n",
      "Training Epoch 92  23.4% | batch:        15 of        64\t|\tloss: 970.954\n",
      "Training Epoch 92  25.0% | batch:        16 of        64\t|\tloss: 2720.78\n",
      "Training Epoch 92  26.6% | batch:        17 of        64\t|\tloss: 901.751\n",
      "Training Epoch 92  28.1% | batch:        18 of        64\t|\tloss: 829.605\n",
      "Training Epoch 92  29.7% | batch:        19 of        64\t|\tloss: 994.077\n",
      "Training Epoch 92  31.2% | batch:        20 of        64\t|\tloss: 668.659\n",
      "Training Epoch 92  32.8% | batch:        21 of        64\t|\tloss: 1334.48\n",
      "Training Epoch 92  34.4% | batch:        22 of        64\t|\tloss: 563.493\n",
      "Training Epoch 92  35.9% | batch:        23 of        64\t|\tloss: 739.152\n",
      "Training Epoch 92  37.5% | batch:        24 of        64\t|\tloss: 1469.78\n",
      "Training Epoch 92  39.1% | batch:        25 of        64\t|\tloss: 1283.39\n",
      "Training Epoch 92  40.6% | batch:        26 of        64\t|\tloss: 1165.17\n",
      "Training Epoch 92  42.2% | batch:        27 of        64\t|\tloss: 632.227\n",
      "Training Epoch 92  43.8% | batch:        28 of        64\t|\tloss: 786.567\n",
      "Training Epoch 92  45.3% | batch:        29 of        64\t|\tloss: 929.237\n",
      "Training Epoch 92  46.9% | batch:        30 of        64\t|\tloss: 577.943\n",
      "Training Epoch 92  48.4% | batch:        31 of        64\t|\tloss: 448.53\n",
      "Training Epoch 92  50.0% | batch:        32 of        64\t|\tloss: 811.105\n",
      "Training Epoch 92  51.6% | batch:        33 of        64\t|\tloss: 860.021\n",
      "Training Epoch 92  53.1% | batch:        34 of        64\t|\tloss: 852.023\n",
      "Training Epoch 92  54.7% | batch:        35 of        64\t|\tloss: 1201.36\n",
      "Training Epoch 92  56.2% | batch:        36 of        64\t|\tloss: 1104.22\n",
      "Training Epoch 92  57.8% | batch:        37 of        64\t|\tloss: 1882.48\n",
      "Training Epoch 92  59.4% | batch:        38 of        64\t|\tloss: 426.915\n",
      "Training Epoch 92  60.9% | batch:        39 of        64\t|\tloss: 1914.32\n",
      "Training Epoch 92  62.5% | batch:        40 of        64\t|\tloss: 730.036\n",
      "Training Epoch 92  64.1% | batch:        41 of        64\t|\tloss: 675.329\n",
      "Training Epoch 92  65.6% | batch:        42 of        64\t|\tloss: 1047.29\n",
      "Training Epoch 92  67.2% | batch:        43 of        64\t|\tloss: 1217.32\n",
      "Training Epoch 92  68.8% | batch:        44 of        64\t|\tloss: 514.328\n",
      "Training Epoch 92  70.3% | batch:        45 of        64\t|\tloss: 749.205\n",
      "Training Epoch 92  71.9% | batch:        46 of        64\t|\tloss: 1386.36\n",
      "Training Epoch 92  73.4% | batch:        47 of        64\t|\tloss: 2213.48\n",
      "Training Epoch 92  75.0% | batch:        48 of        64\t|\tloss: 718.601\n",
      "Training Epoch 92  76.6% | batch:        49 of        64\t|\tloss: 1045.46\n",
      "Training Epoch 92  78.1% | batch:        50 of        64\t|\tloss: 1082.66\n",
      "Training Epoch 92  79.7% | batch:        51 of        64\t|\tloss: 753.017\n",
      "Training Epoch 92  81.2% | batch:        52 of        64\t|\tloss: 581.073\n",
      "Training Epoch 92  82.8% | batch:        53 of        64\t|\tloss: 2450.22\n",
      "Training Epoch 92  84.4% | batch:        54 of        64\t|\tloss: 968.648\n",
      "Training Epoch 92  85.9% | batch:        55 of        64\t|\tloss: 1056.59\n",
      "Training Epoch 92  87.5% | batch:        56 of        64\t|\tloss: 853.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:53,128 | INFO : Epoch 92 Training Summary: epoch: 92.000000 | loss: 1102.849133 | \n",
      "2023-05-10 17:09:53,128 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1844980716705322 seconds\n",
      "\n",
      "2023-05-10 17:09:53,129 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1970173094583594 seconds\n",
      "2023-05-10 17:09:53,129 | INFO : Avg batch train. time: 0.018703395460286865 seconds\n",
      "2023-05-10 17:09:53,129 | INFO : Avg sample train. time: 0.00029643816479899937 seconds\n",
      "2023-05-10 17:09:53,130 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 92  89.1% | batch:        57 of        64\t|\tloss: 6010.9\n",
      "Training Epoch 92  90.6% | batch:        58 of        64\t|\tloss: 691.636\n",
      "Training Epoch 92  92.2% | batch:        59 of        64\t|\tloss: 907.948\n",
      "Training Epoch 92  93.8% | batch:        60 of        64\t|\tloss: 1194.96\n",
      "Training Epoch 92  95.3% | batch:        61 of        64\t|\tloss: 945.169\n",
      "Training Epoch 92  96.9% | batch:        62 of        64\t|\tloss: 847.69\n",
      "Training Epoch 92  98.4% | batch:        63 of        64\t|\tloss: 3535.77\n",
      "\n",
      "Evaluating Epoch 92   0.0% | batch:         0 of        16\t|\tloss: 1515.58\n",
      "Evaluating Epoch 92   6.2% | batch:         1 of        16\t|\tloss: 1313.09\n",
      "Evaluating Epoch 92  12.5% | batch:         2 of        16\t|\tloss: 1006.53\n",
      "Evaluating Epoch 92  18.8% | batch:         3 of        16\t|\tloss: 1122.38\n",
      "Evaluating Epoch 92  25.0% | batch:         4 of        16\t|\tloss: 1428.33\n",
      "Evaluating Epoch 92  31.2% | batch:         5 of        16\t|\tloss: 1768.02\n",
      "Evaluating Epoch 92  37.5% | batch:         6 of        16\t|\tloss: 2297.58\n",
      "Evaluating Epoch 92  43.8% | batch:         7 of        16\t|\tloss: 2079.01\n",
      "Evaluating Epoch 92  50.0% | batch:         8 of        16\t|\tloss: 1502.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:53,286 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.1551814079284668 seconds\n",
      "\n",
      "2023-05-10 17:09:53,286 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.1650635004043579 seconds\n",
      "2023-05-10 17:09:53,286 | INFO : Avg batch val. time: 0.01031646877527237 seconds\n",
      "2023-05-10 17:09:53,287 | INFO : Avg sample val. time: 0.00016342920832114645 seconds\n",
      "2023-05-10 17:09:53,287 | INFO : Epoch 92 Validation Summary: epoch: 92.000000 | loss: 1888.954757 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 92  56.2% | batch:         9 of        16\t|\tloss: 6181.99\n",
      "Evaluating Epoch 92  62.5% | batch:        10 of        16\t|\tloss: 1504.67\n",
      "Evaluating Epoch 92  68.8% | batch:        11 of        16\t|\tloss: 2600.42\n",
      "Evaluating Epoch 92  75.0% | batch:        12 of        16\t|\tloss: 1679.35\n",
      "Evaluating Epoch 92  81.2% | batch:        13 of        16\t|\tloss: 1339.59\n",
      "Evaluating Epoch 92  87.5% | batch:        14 of        16\t|\tloss: 1236.9\n",
      "Evaluating Epoch 92  93.8% | batch:        15 of        16\t|\tloss: 1579.93\n",
      "\n",
      "Training Epoch 93   0.0% | batch:         0 of        64\t|\tloss: 894.387\n",
      "Training Epoch 93   1.6% | batch:         1 of        64\t|\tloss: 1130.94\n",
      "Training Epoch 93   3.1% | batch:         2 of        64\t|\tloss: 1135.68\n",
      "Training Epoch 93   4.7% | batch:         3 of        64\t|\tloss: 1655.84\n",
      "Training Epoch 93   6.2% | batch:         4 of        64\t|\tloss: 875.463\n",
      "Training Epoch 93   7.8% | batch:         5 of        64\t|\tloss: 1190.1\n",
      "Training Epoch 93   9.4% | batch:         6 of        64\t|\tloss: 1627.09\n",
      "Training Epoch 93  10.9% | batch:         7 of        64\t|\tloss: 623.913\n",
      "Training Epoch 93  12.5% | batch:         8 of        64\t|\tloss: 1009.07\n",
      "Training Epoch 93  14.1% | batch:         9 of        64\t|\tloss: 779.905\n",
      "Training Epoch 93  15.6% | batch:        10 of        64\t|\tloss: 765.009\n",
      "Training Epoch 93  17.2% | batch:        11 of        64\t|\tloss: 1022.92\n",
      "Training Epoch 93  18.8% | batch:        12 of        64\t|\tloss: 861.585\n",
      "Training Epoch 93  20.3% | batch:        13 of        64\t|\tloss: 766.661\n",
      "Training Epoch 93  21.9% | batch:        14 of        64\t|\tloss: 3014.62\n",
      "Training Epoch 93  23.4% | batch:        15 of        64\t|\tloss: 3126.97\n",
      "Training Epoch 93  25.0% | batch:        16 of        64\t|\tloss: 602.294\n",
      "Training Epoch 93  26.6% | batch:        17 of        64\t|\tloss: 584.798\n",
      "Training Epoch 93  28.1% | batch:        18 of        64\t|\tloss: 637.804\n",
      "Training Epoch 93  29.7% | batch:        19 of        64\t|\tloss: 883.493\n",
      "Training Epoch 93  31.2% | batch:        20 of        64\t|\tloss: 1621.89\n",
      "Training Epoch 93  32.8% | batch:        21 of        64\t|\tloss: 782.611\n",
      "Training Epoch 93  34.4% | batch:        22 of        64\t|\tloss: 516.101\n",
      "Training Epoch 93  35.9% | batch:        23 of        64\t|\tloss: 1984.41\n",
      "Training Epoch 93  37.5% | batch:        24 of        64\t|\tloss: 736.071\n",
      "Training Epoch 93  39.1% | batch:        25 of        64\t|\tloss: 541.914\n",
      "Training Epoch 93  40.6% | batch:        26 of        64\t|\tloss: 695.881\n",
      "Training Epoch 93  42.2% | batch:        27 of        64\t|\tloss: 925.267\n",
      "Training Epoch 93  43.8% | batch:        28 of        64\t|\tloss: 921.062\n",
      "Training Epoch 93  45.3% | batch:        29 of        64\t|\tloss: 1819.52\n",
      "Training Epoch 93  46.9% | batch:        30 of        64\t|\tloss: 634.861\n",
      "Training Epoch 93  48.4% | batch:        31 of        64\t|\tloss: 687.868\n",
      "Training Epoch 93  50.0% | batch:        32 of        64\t|\tloss: 904.308\n",
      "Training Epoch 93  51.6% | batch:        33 of        64\t|\tloss: 745.58\n",
      "Training Epoch 93  53.1% | batch:        34 of        64\t|\tloss: 709.799\n",
      "Training Epoch 93  54.7% | batch:        35 of        64\t|\tloss: 743.962\n",
      "Training Epoch 93  56.2% | batch:        36 of        64\t|\tloss: 783.028\n",
      "Training Epoch 93  57.8% | batch:        37 of        64\t|\tloss: 1815.69\n",
      "Training Epoch 93  59.4% | batch:        38 of        64\t|\tloss: 1009.8\n",
      "Training Epoch 93  60.9% | batch:        39 of        64\t|\tloss: 984.057\n",
      "Training Epoch 93  62.5% | batch:        40 of        64\t|\tloss: 846.789\n",
      "Training Epoch 93  64.1% | batch:        41 of        64\t|\tloss: 980.731\n",
      "Training Epoch 93  65.6% | batch:        42 of        64\t|\tloss: 794.938\n",
      "Training Epoch 93  67.2% | batch:        43 of        64\t|\tloss: 627.916\n",
      "Training Epoch 93  68.8% | batch:        44 of        64\t|\tloss: 2118.68\n",
      "Training Epoch 93  70.3% | batch:        45 of        64\t|\tloss: 2186.78\n",
      "Training Epoch 93  71.9% | batch:        46 of        64\t|\tloss: 1099.8\n",
      "Training Epoch 93  73.4% | batch:        47 of        64\t|\tloss: 873.198\n",
      "Training Epoch 93  75.0% | batch:        48 of        64\t|\tloss: 958.817\n",
      "Training Epoch 93  76.6% | batch:        49 of        64\t|\tloss: 1119.31\n",
      "Training Epoch 93  78.1% | batch:        50 of        64\t|\tloss: 733.341\n",
      "Training Epoch 93  79.7% | batch:        51 of        64\t|\tloss: 902.95\n",
      "Training Epoch 93  81.2% | batch:        52 of        64\t|\tloss: 1149.2\n",
      "Training Epoch 93  82.8% | batch:        53 of        64\t|\tloss: 750.062\n",
      "Training Epoch 93  84.4% | batch:        54 of        64\t|\tloss: 1429.71\n",
      "Training Epoch 93  85.9% | batch:        55 of        64\t|\tloss: 1097.33\n",
      "Training Epoch 93  87.5% | batch:        56 of        64\t|\tloss: 556.18\n",
      "Training Epoch 93  89.1% | batch:        57 of        64\t|\tloss: 1985.83\n",
      "Training Epoch 93  90.6% | batch:        58 of        64\t|\tloss: 995.985\n",
      "Training Epoch 93  92.2% | batch:        59 of        64\t|\tloss: 1549.01\n",
      "Training Epoch 93  93.8% | batch:        60 of        64\t|\tloss: 1069.73\n",
      "Training Epoch 93  95.3% | batch:        61 of        64\t|\tloss: 732.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:54,494 | INFO : Epoch 93 Training Summary: epoch: 93.000000 | loss: 1083.409315 | \n",
      "2023-05-10 17:09:54,495 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.197629690170288 seconds\n",
      "\n",
      "2023-05-10 17:09:54,495 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1970238941971973 seconds\n",
      "2023-05-10 17:09:54,496 | INFO : Avg batch train. time: 0.01870349834683121 seconds\n",
      "2023-05-10 17:09:54,496 | INFO : Avg sample train. time: 0.00029643979549212417 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 93  96.9% | batch:        62 of        64\t|\tloss: 758.405\n",
      "Training Epoch 93  98.4% | batch:        63 of        64\t|\tloss: 3056.26\n",
      "\n",
      "Training Epoch 94   0.0% | batch:         0 of        64\t|\tloss: 1038.14\n",
      "Training Epoch 94   1.6% | batch:         1 of        64\t|\tloss: 717.306\n",
      "Training Epoch 94   3.1% | batch:         2 of        64\t|\tloss: 885.66\n",
      "Training Epoch 94   4.7% | batch:         3 of        64\t|\tloss: 1733.47\n",
      "Training Epoch 94   6.2% | batch:         4 of        64\t|\tloss: 1227.33\n",
      "Training Epoch 94   7.8% | batch:         5 of        64\t|\tloss: 1149.71\n",
      "Training Epoch 94   9.4% | batch:         6 of        64\t|\tloss: 2081.29\n",
      "Training Epoch 94  10.9% | batch:         7 of        64\t|\tloss: 1431.01\n",
      "Training Epoch 94  12.5% | batch:         8 of        64\t|\tloss: 791.601\n",
      "Training Epoch 94  14.1% | batch:         9 of        64\t|\tloss: 1322.5\n",
      "Training Epoch 94  15.6% | batch:        10 of        64\t|\tloss: 733.814\n",
      "Training Epoch 94  17.2% | batch:        11 of        64\t|\tloss: 1087.72\n",
      "Training Epoch 94  18.8% | batch:        12 of        64\t|\tloss: 915.111\n",
      "Training Epoch 94  20.3% | batch:        13 of        64\t|\tloss: 2959.17\n",
      "Training Epoch 94  21.9% | batch:        14 of        64\t|\tloss: 1265.36\n",
      "Training Epoch 94  23.4% | batch:        15 of        64\t|\tloss: 643.237\n",
      "Training Epoch 94  25.0% | batch:        16 of        64\t|\tloss: 878.315\n",
      "Training Epoch 94  26.6% | batch:        17 of        64\t|\tloss: 963.008\n",
      "Training Epoch 94  28.1% | batch:        18 of        64\t|\tloss: 1125.95\n",
      "Training Epoch 94  29.7% | batch:        19 of        64\t|\tloss: 941.138\n",
      "Training Epoch 94  31.2% | batch:        20 of        64\t|\tloss: 3117.57\n",
      "Training Epoch 94  32.8% | batch:        21 of        64\t|\tloss: 576.832\n",
      "Training Epoch 94  34.4% | batch:        22 of        64\t|\tloss: 1670.45\n",
      "Training Epoch 94  35.9% | batch:        23 of        64\t|\tloss: 1024.29\n",
      "Training Epoch 94  37.5% | batch:        24 of        64\t|\tloss: 1141\n",
      "Training Epoch 94  39.1% | batch:        25 of        64\t|\tloss: 991.603\n",
      "Training Epoch 94  40.6% | batch:        26 of        64\t|\tloss: 1087.77\n",
      "Training Epoch 94  42.2% | batch:        27 of        64\t|\tloss: 862.159\n",
      "Training Epoch 94  43.8% | batch:        28 of        64\t|\tloss: 1324.03\n",
      "Training Epoch 94  45.3% | batch:        29 of        64\t|\tloss: 1778.81\n",
      "Training Epoch 94  46.9% | batch:        30 of        64\t|\tloss: 925.848\n",
      "Training Epoch 94  48.4% | batch:        31 of        64\t|\tloss: 872.315\n",
      "Training Epoch 94  50.0% | batch:        32 of        64\t|\tloss: 586.985\n",
      "Training Epoch 94  51.6% | batch:        33 of        64\t|\tloss: 1045.63\n",
      "Training Epoch 94  53.1% | batch:        34 of        64\t|\tloss: 738.799\n",
      "Training Epoch 94  54.7% | batch:        35 of        64\t|\tloss: 814.727\n",
      "Training Epoch 94  56.2% | batch:        36 of        64\t|\tloss: 675.703\n",
      "Training Epoch 94  57.8% | batch:        37 of        64\t|\tloss: 715.09\n",
      "Training Epoch 94  59.4% | batch:        38 of        64\t|\tloss: 826.073\n",
      "Training Epoch 94  60.9% | batch:        39 of        64\t|\tloss: 931.259\n",
      "Training Epoch 94  62.5% | batch:        40 of        64\t|\tloss: 1514.5\n",
      "Training Epoch 94  64.1% | batch:        41 of        64\t|\tloss: 1049.45\n",
      "Training Epoch 94  65.6% | batch:        42 of        64\t|\tloss: 671.981\n",
      "Training Epoch 94  67.2% | batch:        43 of        64\t|\tloss: 783.419\n",
      "Training Epoch 94  68.8% | batch:        44 of        64\t|\tloss: 1326.25\n",
      "Training Epoch 94  70.3% | batch:        45 of        64\t|\tloss: 703.511\n",
      "Training Epoch 94  71.9% | batch:        46 of        64\t|\tloss: 758.219\n",
      "Training Epoch 94  73.4% | batch:        47 of        64\t|\tloss: 1097.94\n",
      "Training Epoch 94  75.0% | batch:        48 of        64\t|\tloss: 967.528\n",
      "Training Epoch 94  76.6% | batch:        49 of        64\t|\tloss: 1119.31\n",
      "Training Epoch 94  78.1% | batch:        50 of        64\t|\tloss: 1137.19\n",
      "Training Epoch 94  79.7% | batch:        51 of        64\t|\tloss: 1057.58\n",
      "Training Epoch 94  81.2% | batch:        52 of        64\t|\tloss: 833.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:55,696 | INFO : Epoch 94 Training Summary: epoch: 94.000000 | loss: 1148.622862 | \n",
      "2023-05-10 17:09:55,697 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1913528442382812 seconds\n",
      "\n",
      "2023-05-10 17:09:55,698 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1969635638784855 seconds\n",
      "2023-05-10 17:09:55,698 | INFO : Avg batch train. time: 0.018702555685601335 seconds\n",
      "2023-05-10 17:09:55,699 | INFO : Avg sample train. time: 0.00029642485484856004 seconds\n",
      "2023-05-10 17:09:55,699 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 94  82.8% | batch:        53 of        64\t|\tloss: 1132.03\n",
      "Training Epoch 94  84.4% | batch:        54 of        64\t|\tloss: 2455.61\n",
      "Training Epoch 94  85.9% | batch:        55 of        64\t|\tloss: 904.122\n",
      "Training Epoch 94  87.5% | batch:        56 of        64\t|\tloss: 917.791\n",
      "Training Epoch 94  89.1% | batch:        57 of        64\t|\tloss: 2638.71\n",
      "Training Epoch 94  90.6% | batch:        58 of        64\t|\tloss: 658.729\n",
      "Training Epoch 94  92.2% | batch:        59 of        64\t|\tloss: 1547.42\n",
      "Training Epoch 94  93.8% | batch:        60 of        64\t|\tloss: 1243.24\n",
      "Training Epoch 94  95.3% | batch:        61 of        64\t|\tloss: 1374.4\n",
      "Training Epoch 94  96.9% | batch:        62 of        64\t|\tloss: 843.029\n",
      "Training Epoch 94  98.4% | batch:        63 of        64\t|\tloss: 1464.06\n",
      "\n",
      "Evaluating Epoch 94   0.0% | batch:         0 of        16\t|\tloss: 1314.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:55,847 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14739036560058594 seconds\n",
      "\n",
      "2023-05-10 17:09:55,848 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.16470282418387278 seconds\n",
      "2023-05-10 17:09:55,848 | INFO : Avg batch val. time: 0.010293926511492048 seconds\n",
      "2023-05-10 17:09:55,849 | INFO : Avg sample val. time: 0.0001630721031523493 seconds\n",
      "2023-05-10 17:09:55,850 | INFO : Epoch 94 Validation Summary: epoch: 94.000000 | loss: 1842.817362 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 94   6.2% | batch:         1 of        16\t|\tloss: 696.7\n",
      "Evaluating Epoch 94  12.5% | batch:         2 of        16\t|\tloss: 742.166\n",
      "Evaluating Epoch 94  18.8% | batch:         3 of        16\t|\tloss: 989.36\n",
      "Evaluating Epoch 94  25.0% | batch:         4 of        16\t|\tloss: 1354.29\n",
      "Evaluating Epoch 94  31.2% | batch:         5 of        16\t|\tloss: 2797.15\n",
      "Evaluating Epoch 94  37.5% | batch:         6 of        16\t|\tloss: 2135.88\n",
      "Evaluating Epoch 94  43.8% | batch:         7 of        16\t|\tloss: 2163.01\n",
      "Evaluating Epoch 94  50.0% | batch:         8 of        16\t|\tloss: 1347.35\n",
      "Evaluating Epoch 94  56.2% | batch:         9 of        16\t|\tloss: 7403.29\n",
      "Evaluating Epoch 94  62.5% | batch:        10 of        16\t|\tloss: 1381.62\n",
      "Evaluating Epoch 94  68.8% | batch:        11 of        16\t|\tloss: 2177.59\n",
      "Evaluating Epoch 94  75.0% | batch:        12 of        16\t|\tloss: 1338.64\n",
      "Evaluating Epoch 94  81.2% | batch:        13 of        16\t|\tloss: 1176.73\n",
      "Evaluating Epoch 94  87.5% | batch:        14 of        16\t|\tloss: 1065.89\n",
      "Evaluating Epoch 94  93.8% | batch:        15 of        16\t|\tloss: 1277.63\n",
      "\n",
      "Training Epoch 95   0.0% | batch:         0 of        64\t|\tloss: 895.184\n",
      "Training Epoch 95   1.6% | batch:         1 of        64\t|\tloss: 873.212\n",
      "Training Epoch 95   3.1% | batch:         2 of        64\t|\tloss: 866.859\n",
      "Training Epoch 95   4.7% | batch:         3 of        64\t|\tloss: 776.768\n",
      "Training Epoch 95   6.2% | batch:         4 of        64\t|\tloss: 1610.91\n",
      "Training Epoch 95   7.8% | batch:         5 of        64\t|\tloss: 1185.94\n",
      "Training Epoch 95   9.4% | batch:         6 of        64\t|\tloss: 741.191\n",
      "Training Epoch 95  10.9% | batch:         7 of        64\t|\tloss: 850.073\n",
      "Training Epoch 95  12.5% | batch:         8 of        64\t|\tloss: 825.83\n",
      "Training Epoch 95  14.1% | batch:         9 of        64\t|\tloss: 1012.48\n",
      "Training Epoch 95  15.6% | batch:        10 of        64\t|\tloss: 973.865\n",
      "Training Epoch 95  17.2% | batch:        11 of        64\t|\tloss: 1028.36\n",
      "Training Epoch 95  18.8% | batch:        12 of        64\t|\tloss: 1092.65\n",
      "Training Epoch 95  20.3% | batch:        13 of        64\t|\tloss: 723.651\n",
      "Training Epoch 95  21.9% | batch:        14 of        64\t|\tloss: 742.575\n",
      "Training Epoch 95  23.4% | batch:        15 of        64\t|\tloss: 856.56\n",
      "Training Epoch 95  25.0% | batch:        16 of        64\t|\tloss: 993.088\n",
      "Training Epoch 95  26.6% | batch:        17 of        64\t|\tloss: 839.359\n",
      "Training Epoch 95  28.1% | batch:        18 of        64\t|\tloss: 1919.21\n",
      "Training Epoch 95  29.7% | batch:        19 of        64\t|\tloss: 863.092\n",
      "Training Epoch 95  31.2% | batch:        20 of        64\t|\tloss: 590.091\n",
      "Training Epoch 95  32.8% | batch:        21 of        64\t|\tloss: 526.625\n",
      "Training Epoch 95  34.4% | batch:        22 of        64\t|\tloss: 1970.09\n",
      "Training Epoch 95  35.9% | batch:        23 of        64\t|\tloss: 531.286\n",
      "Training Epoch 95  37.5% | batch:        24 of        64\t|\tloss: 904.084\n",
      "Training Epoch 95  39.1% | batch:        25 of        64\t|\tloss: 1697.2\n",
      "Training Epoch 95  40.6% | batch:        26 of        64\t|\tloss: 1589.78\n",
      "Training Epoch 95  42.2% | batch:        27 of        64\t|\tloss: 1448.7\n",
      "Training Epoch 95  43.8% | batch:        28 of        64\t|\tloss: 4179.32\n",
      "Training Epoch 95  45.3% | batch:        29 of        64\t|\tloss: 473.902\n",
      "Training Epoch 95  46.9% | batch:        30 of        64\t|\tloss: 1204.02\n",
      "Training Epoch 95  48.4% | batch:        31 of        64\t|\tloss: 606.717\n",
      "Training Epoch 95  50.0% | batch:        32 of        64\t|\tloss: 647.442\n",
      "Training Epoch 95  51.6% | batch:        33 of        64\t|\tloss: 1298.61\n",
      "Training Epoch 95  53.1% | batch:        34 of        64\t|\tloss: 509.141\n",
      "Training Epoch 95  54.7% | batch:        35 of        64\t|\tloss: 886.577\n",
      "Training Epoch 95  56.2% | batch:        36 of        64\t|\tloss: 746.151\n",
      "Training Epoch 95  57.8% | batch:        37 of        64\t|\tloss: 783.094\n",
      "Training Epoch 95  59.4% | batch:        38 of        64\t|\tloss: 964.826\n",
      "Training Epoch 95  60.9% | batch:        39 of        64\t|\tloss: 803.705\n",
      "Training Epoch 95  62.5% | batch:        40 of        64\t|\tloss: 721.051\n",
      "Training Epoch 95  64.1% | batch:        41 of        64\t|\tloss: 704.479\n",
      "Training Epoch 95  65.6% | batch:        42 of        64\t|\tloss: 2065.3\n",
      "Training Epoch 95  67.2% | batch:        43 of        64\t|\tloss: 933.82\n",
      "Training Epoch 95  68.8% | batch:        44 of        64\t|\tloss: 1038.97\n",
      "Training Epoch 95  70.3% | batch:        45 of        64\t|\tloss: 626.112\n",
      "Training Epoch 95  71.9% | batch:        46 of        64\t|\tloss: 1308.82\n",
      "Training Epoch 95  73.4% | batch:        47 of        64\t|\tloss: 741.287\n",
      "Training Epoch 95  75.0% | batch:        48 of        64\t|\tloss: 968.237\n",
      "Training Epoch 95  76.6% | batch:        49 of        64\t|\tloss: 1049.83\n",
      "Training Epoch 95  78.1% | batch:        50 of        64\t|\tloss: 562.563\n",
      "Training Epoch 95  79.7% | batch:        51 of        64\t|\tloss: 633.35\n",
      "Training Epoch 95  81.2% | batch:        52 of        64\t|\tloss: 684.474\n",
      "Training Epoch 95  82.8% | batch:        53 of        64\t|\tloss: 572.277\n",
      "Training Epoch 95  84.4% | batch:        54 of        64\t|\tloss: 546.996\n",
      "Training Epoch 95  85.9% | batch:        55 of        64\t|\tloss: 658.357\n",
      "Training Epoch 95  87.5% | batch:        56 of        64\t|\tloss: 659.341\n",
      "Training Epoch 95  89.1% | batch:        57 of        64\t|\tloss: 1150.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:57,054 | INFO : Epoch 95 Training Summary: epoch: 95.000000 | loss: 1013.860115 | \n",
      "2023-05-10 17:09:57,055 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1947567462921143 seconds\n",
      "\n",
      "2023-05-10 17:09:57,055 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1969403342196816 seconds\n",
      "2023-05-10 17:09:57,056 | INFO : Avg batch train. time: 0.018702192722182525 seconds\n",
      "2023-05-10 17:09:57,056 | INFO : Avg sample train. time: 0.0002964191020851118 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 95  90.6% | batch:        58 of        64\t|\tloss: 986.201\n",
      "Training Epoch 95  92.2% | batch:        59 of        64\t|\tloss: 900.804\n",
      "Training Epoch 95  93.8% | batch:        60 of        64\t|\tloss: 1021.96\n",
      "Training Epoch 95  95.3% | batch:        61 of        64\t|\tloss: 2121.22\n",
      "Training Epoch 95  96.9% | batch:        62 of        64\t|\tloss: 720.687\n",
      "Training Epoch 95  98.4% | batch:        63 of        64\t|\tloss: 5964.31\n",
      "\n",
      "Training Epoch 96   0.0% | batch:         0 of        64\t|\tloss: 1579.41\n",
      "Training Epoch 96   1.6% | batch:         1 of        64\t|\tloss: 1037.12\n",
      "Training Epoch 96   3.1% | batch:         2 of        64\t|\tloss: 970.337\n",
      "Training Epoch 96   4.7% | batch:         3 of        64\t|\tloss: 4536.97\n",
      "Training Epoch 96   6.2% | batch:         4 of        64\t|\tloss: 1135.49\n",
      "Training Epoch 96   7.8% | batch:         5 of        64\t|\tloss: 701.035\n",
      "Training Epoch 96   9.4% | batch:         6 of        64\t|\tloss: 751.193\n",
      "Training Epoch 96  10.9% | batch:         7 of        64\t|\tloss: 1907.59\n",
      "Training Epoch 96  12.5% | batch:         8 of        64\t|\tloss: 2294.31\n",
      "Training Epoch 96  14.1% | batch:         9 of        64\t|\tloss: 640.302\n",
      "Training Epoch 96  15.6% | batch:        10 of        64\t|\tloss: 667.871\n",
      "Training Epoch 96  17.2% | batch:        11 of        64\t|\tloss: 543.888\n",
      "Training Epoch 96  18.8% | batch:        12 of        64\t|\tloss: 743.635\n",
      "Training Epoch 96  20.3% | batch:        13 of        64\t|\tloss: 613.103\n",
      "Training Epoch 96  21.9% | batch:        14 of        64\t|\tloss: 735.68\n",
      "Training Epoch 96  23.4% | batch:        15 of        64\t|\tloss: 822.318\n",
      "Training Epoch 96  25.0% | batch:        16 of        64\t|\tloss: 803.906\n",
      "Training Epoch 96  26.6% | batch:        17 of        64\t|\tloss: 1134.67\n",
      "Training Epoch 96  28.1% | batch:        18 of        64\t|\tloss: 1017.95\n",
      "Training Epoch 96  29.7% | batch:        19 of        64\t|\tloss: 277.87\n",
      "Training Epoch 96  31.2% | batch:        20 of        64\t|\tloss: 722.329\n",
      "Training Epoch 96  32.8% | batch:        21 of        64\t|\tloss: 1731.35\n",
      "Training Epoch 96  34.4% | batch:        22 of        64\t|\tloss: 1241.99\n",
      "Training Epoch 96  35.9% | batch:        23 of        64\t|\tloss: 862.208\n",
      "Training Epoch 96  37.5% | batch:        24 of        64\t|\tloss: 1285.76\n",
      "Training Epoch 96  39.1% | batch:        25 of        64\t|\tloss: 1128.39\n",
      "Training Epoch 96  40.6% | batch:        26 of        64\t|\tloss: 524.198\n",
      "Training Epoch 96  42.2% | batch:        27 of        64\t|\tloss: 714.078\n",
      "Training Epoch 96  43.8% | batch:        28 of        64\t|\tloss: 771.257\n",
      "Training Epoch 96  45.3% | batch:        29 of        64\t|\tloss: 1098.39\n",
      "Training Epoch 96  46.9% | batch:        30 of        64\t|\tloss: 1162.84\n",
      "Training Epoch 96  48.4% | batch:        31 of        64\t|\tloss: 975.083\n",
      "Training Epoch 96  50.0% | batch:        32 of        64\t|\tloss: 577.983\n",
      "Training Epoch 96  51.6% | batch:        33 of        64\t|\tloss: 589.286\n",
      "Training Epoch 96  53.1% | batch:        34 of        64\t|\tloss: 881.331\n",
      "Training Epoch 96  54.7% | batch:        35 of        64\t|\tloss: 1347.94\n",
      "Training Epoch 96  56.2% | batch:        36 of        64\t|\tloss: 965.727\n",
      "Training Epoch 96  57.8% | batch:        37 of        64\t|\tloss: 565.069\n",
      "Training Epoch 96  59.4% | batch:        38 of        64\t|\tloss: 1099.89\n",
      "Training Epoch 96  60.9% | batch:        39 of        64\t|\tloss: 1007.07\n",
      "Training Epoch 96  62.5% | batch:        40 of        64\t|\tloss: 834.074\n",
      "Training Epoch 96  64.1% | batch:        41 of        64\t|\tloss: 807.95\n",
      "Training Epoch 96  65.6% | batch:        42 of        64\t|\tloss: 1047.65\n",
      "Training Epoch 96  67.2% | batch:        43 of        64\t|\tloss: 694.223\n",
      "Training Epoch 96  68.8% | batch:        44 of        64\t|\tloss: 859.549\n",
      "Training Epoch 96  70.3% | batch:        45 of        64\t|\tloss: 3017.16\n",
      "Training Epoch 96  71.9% | batch:        46 of        64\t|\tloss: 514.16\n",
      "Training Epoch 96  73.4% | batch:        47 of        64\t|\tloss: 560.955\n",
      "Training Epoch 96  75.0% | batch:        48 of        64\t|\tloss: 1050.86\n",
      "Training Epoch 96  76.6% | batch:        49 of        64\t|\tloss: 858.937\n",
      "Training Epoch 96  78.1% | batch:        50 of        64\t|\tloss: 782.982\n",
      "Training Epoch 96  79.7% | batch:        51 of        64\t|\tloss: 1191.7\n",
      "Training Epoch 96  81.2% | batch:        52 of        64\t|\tloss: 1529.5\n",
      "Training Epoch 96  82.8% | batch:        53 of        64\t|\tloss: 939.817\n",
      "Training Epoch 96  84.4% | batch:        54 of        64\t|\tloss: 1629.22\n",
      "Training Epoch 96  85.9% | batch:        55 of        64\t|\tloss: 746.138\n",
      "Training Epoch 96  87.5% | batch:        56 of        64\t|\tloss: 638.673\n",
      "Training Epoch 96  89.1% | batch:        57 of        64\t|\tloss: 1890.77\n",
      "Training Epoch 96  90.6% | batch:        58 of        64\t|\tloss: 1180.82\n",
      "Training Epoch 96  92.2% | batch:        59 of        64\t|\tloss: 823.395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:58,250 | INFO : Epoch 96 Training Summary: epoch: 96.000000 | loss: 1052.694284 | \n",
      "2023-05-10 17:09:58,251 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1856646537780762 seconds\n",
      "\n",
      "2023-05-10 17:09:58,251 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1968228792150815 seconds\n",
      "2023-05-10 17:09:58,252 | INFO : Avg batch train. time: 0.018700357487735648 seconds\n",
      "2023-05-10 17:09:58,252 | INFO : Avg sample train. time: 0.000296390014664458 seconds\n",
      "2023-05-10 17:09:58,253 | INFO : Evaluating on validation set ...\n",
      "2023-05-10 17:09:58,400 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14739751815795898 seconds\n",
      "\n",
      "2023-05-10 17:09:58,401 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.1643567180633545 seconds\n",
      "2023-05-10 17:09:58,401 | INFO : Avg batch val. time: 0.010272294878959656 seconds\n",
      "2023-05-10 17:09:58,401 | INFO : Avg sample val. time: 0.00016272942382510348 seconds\n",
      "2023-05-10 17:09:58,402 | INFO : Epoch 96 Validation Summary: epoch: 96.000000 | loss: 2061.902177 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 96  93.8% | batch:        60 of        64\t|\tloss: 669.537\n",
      "Training Epoch 96  95.3% | batch:        61 of        64\t|\tloss: 1063.02\n",
      "Training Epoch 96  96.9% | batch:        62 of        64\t|\tloss: 799.308\n",
      "Training Epoch 96  98.4% | batch:        63 of        64\t|\tloss: 1292.93\n",
      "\n",
      "Evaluating Epoch 96   0.0% | batch:         0 of        16\t|\tloss: 1628\n",
      "Evaluating Epoch 96   6.2% | batch:         1 of        16\t|\tloss: 941.685\n",
      "Evaluating Epoch 96  12.5% | batch:         2 of        16\t|\tloss: 735.038\n",
      "Evaluating Epoch 96  18.8% | batch:         3 of        16\t|\tloss: 1664.26\n",
      "Evaluating Epoch 96  25.0% | batch:         4 of        16\t|\tloss: 1472.26\n",
      "Evaluating Epoch 96  31.2% | batch:         5 of        16\t|\tloss: 1482.15\n",
      "Evaluating Epoch 96  37.5% | batch:         6 of        16\t|\tloss: 2748.15\n",
      "Evaluating Epoch 96  43.8% | batch:         7 of        16\t|\tloss: 2251.1\n",
      "Evaluating Epoch 96  50.0% | batch:         8 of        16\t|\tloss: 1570.9\n",
      "Evaluating Epoch 96  56.2% | batch:         9 of        16\t|\tloss: 8989.35\n",
      "Evaluating Epoch 96  62.5% | batch:        10 of        16\t|\tloss: 1471.5\n",
      "Evaluating Epoch 96  68.8% | batch:        11 of        16\t|\tloss: 2602.05\n",
      "Evaluating Epoch 96  75.0% | batch:        12 of        16\t|\tloss: 1023.63\n",
      "Evaluating Epoch 96  81.2% | batch:        13 of        16\t|\tloss: 807.123\n",
      "Evaluating Epoch 96  87.5% | batch:        14 of        16\t|\tloss: 1685.38\n",
      "Evaluating Epoch 96  93.8% | batch:        15 of        16\t|\tloss: 1877.52\n",
      "\n",
      "Training Epoch 97   0.0% | batch:         0 of        64\t|\tloss: 1274.8\n",
      "Training Epoch 97   1.6% | batch:         1 of        64\t|\tloss: 747.434\n",
      "Training Epoch 97   3.1% | batch:         2 of        64\t|\tloss: 691.364\n",
      "Training Epoch 97   4.7% | batch:         3 of        64\t|\tloss: 615.465\n",
      "Training Epoch 97   6.2% | batch:         4 of        64\t|\tloss: 544.967\n",
      "Training Epoch 97   7.8% | batch:         5 of        64\t|\tloss: 777.748\n",
      "Training Epoch 97   9.4% | batch:         6 of        64\t|\tloss: 1714.09\n",
      "Training Epoch 97  10.9% | batch:         7 of        64\t|\tloss: 723.753\n",
      "Training Epoch 97  12.5% | batch:         8 of        64\t|\tloss: 691.639\n",
      "Training Epoch 97  14.1% | batch:         9 of        64\t|\tloss: 1311.6\n",
      "Training Epoch 97  15.6% | batch:        10 of        64\t|\tloss: 1016.08\n",
      "Training Epoch 97  17.2% | batch:        11 of        64\t|\tloss: 1030.09\n",
      "Training Epoch 97  18.8% | batch:        12 of        64\t|\tloss: 679.262\n",
      "Training Epoch 97  20.3% | batch:        13 of        64\t|\tloss: 633.636\n",
      "Training Epoch 97  21.9% | batch:        14 of        64\t|\tloss: 564.989\n",
      "Training Epoch 97  23.4% | batch:        15 of        64\t|\tloss: 1449.57\n",
      "Training Epoch 97  25.0% | batch:        16 of        64\t|\tloss: 1250.28\n",
      "Training Epoch 97  26.6% | batch:        17 of        64\t|\tloss: 1078.38\n",
      "Training Epoch 97  28.1% | batch:        18 of        64\t|\tloss: 3430.02\n",
      "Training Epoch 97  29.7% | batch:        19 of        64\t|\tloss: 481.625\n",
      "Training Epoch 97  31.2% | batch:        20 of        64\t|\tloss: 786.334\n",
      "Training Epoch 97  32.8% | batch:        21 of        64\t|\tloss: 1107.34\n",
      "Training Epoch 97  34.4% | batch:        22 of        64\t|\tloss: 868.393\n",
      "Training Epoch 97  35.9% | batch:        23 of        64\t|\tloss: 843.1\n",
      "Training Epoch 97  37.5% | batch:        24 of        64\t|\tloss: 767.287\n",
      "Training Epoch 97  39.1% | batch:        25 of        64\t|\tloss: 2331.63\n",
      "Training Epoch 97  40.6% | batch:        26 of        64\t|\tloss: 663.333\n",
      "Training Epoch 97  42.2% | batch:        27 of        64\t|\tloss: 1730.63\n",
      "Training Epoch 97  43.8% | batch:        28 of        64\t|\tloss: 609.784\n",
      "Training Epoch 97  45.3% | batch:        29 of        64\t|\tloss: 1890.16\n",
      "Training Epoch 97  46.9% | batch:        30 of        64\t|\tloss: 860.836\n",
      "Training Epoch 97  48.4% | batch:        31 of        64\t|\tloss: 524.165\n",
      "Training Epoch 97  50.0% | batch:        32 of        64\t|\tloss: 621.665\n",
      "Training Epoch 97  51.6% | batch:        33 of        64\t|\tloss: 1535.87\n",
      "Training Epoch 97  53.1% | batch:        34 of        64\t|\tloss: 1717.12\n",
      "Training Epoch 97  54.7% | batch:        35 of        64\t|\tloss: 639.595\n",
      "Training Epoch 97  56.2% | batch:        36 of        64\t|\tloss: 836.32\n",
      "Training Epoch 97  57.8% | batch:        37 of        64\t|\tloss: 1339.94\n",
      "Training Epoch 97  59.4% | batch:        38 of        64\t|\tloss: 532.098\n",
      "Training Epoch 97  60.9% | batch:        39 of        64\t|\tloss: 1024.08\n",
      "Training Epoch 97  62.5% | batch:        40 of        64\t|\tloss: 1579.22\n",
      "Training Epoch 97  64.1% | batch:        41 of        64\t|\tloss: 802.151\n",
      "Training Epoch 97  65.6% | batch:        42 of        64\t|\tloss: 882.48\n",
      "Training Epoch 97  67.2% | batch:        43 of        64\t|\tloss: 1366.62\n",
      "Training Epoch 97  68.8% | batch:        44 of        64\t|\tloss: 722.762\n",
      "Training Epoch 97  70.3% | batch:        45 of        64\t|\tloss: 1097.83\n",
      "Training Epoch 97  71.9% | batch:        46 of        64\t|\tloss: 681.589\n",
      "Training Epoch 97  73.4% | batch:        47 of        64\t|\tloss: 927.024\n",
      "Training Epoch 97  75.0% | batch:        48 of        64\t|\tloss: 2046.83\n",
      "Training Epoch 97  76.6% | batch:        49 of        64\t|\tloss: 700.818\n",
      "Training Epoch 97  78.1% | batch:        50 of        64\t|\tloss: 499.714\n",
      "Training Epoch 97  79.7% | batch:        51 of        64\t|\tloss: 781.863\n",
      "Training Epoch 97  81.2% | batch:        52 of        64\t|\tloss: 1969.23\n",
      "Training Epoch 97  82.8% | batch:        53 of        64\t|\tloss: 1022.9\n",
      "Training Epoch 97  84.4% | batch:        54 of        64\t|\tloss: 524.661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:09:59,596 | INFO : Epoch 97 Training Summary: epoch: 97.000000 | loss: 1051.562548 | \n",
      "2023-05-10 17:09:59,597 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1847772598266602 seconds\n",
      "\n",
      "2023-05-10 17:09:59,598 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1966986975719018 seconds\n",
      "2023-05-10 17:09:59,598 | INFO : Avg batch train. time: 0.018698417149560966 seconds\n",
      "2023-05-10 17:09:59,598 | INFO : Avg sample train. time: 0.0002963592614095844 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 97  85.9% | batch:        55 of        64\t|\tloss: 684.183\n",
      "Training Epoch 97  87.5% | batch:        56 of        64\t|\tloss: 666.15\n",
      "Training Epoch 97  89.1% | batch:        57 of        64\t|\tloss: 1184.21\n",
      "Training Epoch 97  90.6% | batch:        58 of        64\t|\tloss: 1301.68\n",
      "Training Epoch 97  92.2% | batch:        59 of        64\t|\tloss: 650.81\n",
      "Training Epoch 97  93.8% | batch:        60 of        64\t|\tloss: 903.506\n",
      "Training Epoch 97  95.3% | batch:        61 of        64\t|\tloss: 1874.07\n",
      "Training Epoch 97  96.9% | batch:        62 of        64\t|\tloss: 1123.51\n",
      "Training Epoch 97  98.4% | batch:        63 of        64\t|\tloss: 4444.93\n",
      "\n",
      "Training Epoch 98   0.0% | batch:         0 of        64\t|\tloss: 840.117\n",
      "Training Epoch 98   1.6% | batch:         1 of        64\t|\tloss: 687.852\n",
      "Training Epoch 98   3.1% | batch:         2 of        64\t|\tloss: 558.207\n",
      "Training Epoch 98   4.7% | batch:         3 of        64\t|\tloss: 992.266\n",
      "Training Epoch 98   6.2% | batch:         4 of        64\t|\tloss: 699.508\n",
      "Training Epoch 98   7.8% | batch:         5 of        64\t|\tloss: 579.894\n",
      "Training Epoch 98   9.4% | batch:         6 of        64\t|\tloss: 543.1\n",
      "Training Epoch 98  10.9% | batch:         7 of        64\t|\tloss: 1103.74\n",
      "Training Epoch 98  12.5% | batch:         8 of        64\t|\tloss: 1010.63\n",
      "Training Epoch 98  14.1% | batch:         9 of        64\t|\tloss: 590.641\n",
      "Training Epoch 98  15.6% | batch:        10 of        64\t|\tloss: 1917.99\n",
      "Training Epoch 98  17.2% | batch:        11 of        64\t|\tloss: 1431.8\n",
      "Training Epoch 98  18.8% | batch:        12 of        64\t|\tloss: 885.064\n",
      "Training Epoch 98  20.3% | batch:        13 of        64\t|\tloss: 688.387\n",
      "Training Epoch 98  21.9% | batch:        14 of        64\t|\tloss: 845.572\n",
      "Training Epoch 98  23.4% | batch:        15 of        64\t|\tloss: 1010.76\n",
      "Training Epoch 98  25.0% | batch:        16 of        64\t|\tloss: 947.517\n",
      "Training Epoch 98  26.6% | batch:        17 of        64\t|\tloss: 885.736\n",
      "Training Epoch 98  28.1% | batch:        18 of        64\t|\tloss: 845.378\n",
      "Training Epoch 98  29.7% | batch:        19 of        64\t|\tloss: 812.939\n",
      "Training Epoch 98  31.2% | batch:        20 of        64\t|\tloss: 895.943\n",
      "Training Epoch 98  32.8% | batch:        21 of        64\t|\tloss: 1297.61\n",
      "Training Epoch 98  34.4% | batch:        22 of        64\t|\tloss: 524.364\n",
      "Training Epoch 98  35.9% | batch:        23 of        64\t|\tloss: 396.996\n",
      "Training Epoch 98  37.5% | batch:        24 of        64\t|\tloss: 898.298\n",
      "Training Epoch 98  39.1% | batch:        25 of        64\t|\tloss: 529.177\n",
      "Training Epoch 98  40.6% | batch:        26 of        64\t|\tloss: 2145.7\n",
      "Training Epoch 98  42.2% | batch:        27 of        64\t|\tloss: 614.218\n",
      "Training Epoch 98  43.8% | batch:        28 of        64\t|\tloss: 1044.26\n",
      "Training Epoch 98  45.3% | batch:        29 of        64\t|\tloss: 857.447\n",
      "Training Epoch 98  46.9% | batch:        30 of        64\t|\tloss: 1632.93\n",
      "Training Epoch 98  48.4% | batch:        31 of        64\t|\tloss: 554.097\n",
      "Training Epoch 98  50.0% | batch:        32 of        64\t|\tloss: 535.983\n",
      "Training Epoch 98  51.6% | batch:        33 of        64\t|\tloss: 1204.82\n",
      "Training Epoch 98  53.1% | batch:        34 of        64\t|\tloss: 1782.74\n",
      "Training Epoch 98  54.7% | batch:        35 of        64\t|\tloss: 2266.47\n",
      "Training Epoch 98  56.2% | batch:        36 of        64\t|\tloss: 823.85\n",
      "Training Epoch 98  57.8% | batch:        37 of        64\t|\tloss: 936.669\n",
      "Training Epoch 98  59.4% | batch:        38 of        64\t|\tloss: 692.542\n",
      "Training Epoch 98  60.9% | batch:        39 of        64\t|\tloss: 988.309\n",
      "Training Epoch 98  62.5% | batch:        40 of        64\t|\tloss: 603.856\n",
      "Training Epoch 98  64.1% | batch:        41 of        64\t|\tloss: 674.7\n",
      "Training Epoch 98  65.6% | batch:        42 of        64\t|\tloss: 615.344\n",
      "Training Epoch 98  67.2% | batch:        43 of        64\t|\tloss: 1013.1\n",
      "Training Epoch 98  68.8% | batch:        44 of        64\t|\tloss: 1063.21\n",
      "Training Epoch 98  70.3% | batch:        45 of        64\t|\tloss: 1185.92\n",
      "Training Epoch 98  71.9% | batch:        46 of        64\t|\tloss: 765.542\n",
      "Training Epoch 98  73.4% | batch:        47 of        64\t|\tloss: 789.263\n",
      "Training Epoch 98  75.0% | batch:        48 of        64\t|\tloss: 869.888\n",
      "Training Epoch 98  76.6% | batch:        49 of        64\t|\tloss: 737.079\n",
      "Training Epoch 98  78.1% | batch:        50 of        64\t|\tloss: 1271.89\n",
      "Training Epoch 98  79.7% | batch:        51 of        64\t|\tloss: 760.017\n",
      "Training Epoch 98  81.2% | batch:        52 of        64\t|\tloss: 1888.37\n",
      "Training Epoch 98  82.8% | batch:        53 of        64\t|\tloss: 982.425\n",
      "Training Epoch 98  84.4% | batch:        54 of        64\t|\tloss: 2609.26\n",
      "Training Epoch 98  85.9% | batch:        55 of        64\t|\tloss: 473.363\n",
      "Training Epoch 98  87.5% | batch:        56 of        64\t|\tloss: 1894.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:10:00,799 | INFO : Epoch 98 Training Summary: epoch: 98.000000 | loss: 979.957858 | \n",
      "2023-05-10 17:10:00,800 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1913084983825684 seconds\n",
      "\n",
      "2023-05-10 17:10:00,800 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.1966436955393578 seconds\n",
      "2023-05-10 17:10:00,801 | INFO : Avg batch train. time: 0.018697557742802465 seconds\n",
      "2023-05-10 17:10:00,801 | INFO : Avg sample train. time: 0.00029634564030197073 seconds\n",
      "2023-05-10 17:10:00,801 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 98  89.1% | batch:        57 of        64\t|\tloss: 635.33\n",
      "Training Epoch 98  90.6% | batch:        58 of        64\t|\tloss: 1274.72\n",
      "Training Epoch 98  92.2% | batch:        59 of        64\t|\tloss: 403.435\n",
      "Training Epoch 98  93.8% | batch:        60 of        64\t|\tloss: 662.16\n",
      "Training Epoch 98  95.3% | batch:        61 of        64\t|\tloss: 914.903\n",
      "Training Epoch 98  96.9% | batch:        62 of        64\t|\tloss: 1125.94\n",
      "Training Epoch 98  98.4% | batch:        63 of        64\t|\tloss: 1234.02\n",
      "\n",
      "Evaluating Epoch 98   0.0% | batch:         0 of        16\t|\tloss: 2024.03\n",
      "Evaluating Epoch 98   6.2% | batch:         1 of        16\t|\tloss: 1321.32\n",
      "Evaluating Epoch 98  12.5% | batch:         2 of        16\t|\tloss: 1331.2\n",
      "Evaluating Epoch 98  18.8% | batch:         3 of        16\t|\tloss: 1604.81\n",
      "Evaluating Epoch 98  25.0% | batch:         4 of        16\t|\tloss: 1732.57\n",
      "Evaluating Epoch 98  31.2% | batch:         5 of        16\t|\tloss: 2456.94\n",
      "Evaluating Epoch 98  37.5% | batch:         6 of        16\t|\tloss: 2739.26\n",
      "Evaluating Epoch 98  43.8% | batch:         7 of        16\t|\tloss: 2596.57\n",
      "Evaluating Epoch 98  50.0% | batch:         8 of        16\t|\tloss: 1768.6\n",
      "Evaluating Epoch 98  56.2% | batch:         9 of        16\t|\tloss: 7684.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:10:00,949 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14768385887145996 seconds\n",
      "\n",
      "2023-05-10 17:10:00,950 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.1640297992556703 seconds\n",
      "2023-05-10 17:10:00,950 | INFO : Avg batch val. time: 0.010251862453479393 seconds\n",
      "2023-05-10 17:10:00,951 | INFO : Avg sample val. time: 0.0001624057418372973 seconds\n",
      "2023-05-10 17:10:00,951 | INFO : Epoch 98 Validation Summary: epoch: 98.000000 | loss: 2335.834127 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 98  62.5% | batch:        10 of        16\t|\tloss: 1903.95\n",
      "Evaluating Epoch 98  68.8% | batch:        11 of        16\t|\tloss: 2779.91\n",
      "Evaluating Epoch 98  75.0% | batch:        12 of        16\t|\tloss: 1226.95\n",
      "Evaluating Epoch 98  81.2% | batch:        13 of        16\t|\tloss: 1371.17\n",
      "Evaluating Epoch 98  87.5% | batch:        14 of        16\t|\tloss: 1763.51\n",
      "Evaluating Epoch 98  93.8% | batch:        15 of        16\t|\tloss: 3273.1\n",
      "\n",
      "Training Epoch 99   0.0% | batch:         0 of        64\t|\tloss: 1043.15\n",
      "Training Epoch 99   1.6% | batch:         1 of        64\t|\tloss: 947.342\n",
      "Training Epoch 99   3.1% | batch:         2 of        64\t|\tloss: 704.633\n",
      "Training Epoch 99   4.7% | batch:         3 of        64\t|\tloss: 708.847\n",
      "Training Epoch 99   6.2% | batch:         4 of        64\t|\tloss: 811.228\n",
      "Training Epoch 99   7.8% | batch:         5 of        64\t|\tloss: 492.169\n",
      "Training Epoch 99   9.4% | batch:         6 of        64\t|\tloss: 1236.5\n",
      "Training Epoch 99  10.9% | batch:         7 of        64\t|\tloss: 795.563\n",
      "Training Epoch 99  12.5% | batch:         8 of        64\t|\tloss: 778.156\n",
      "Training Epoch 99  14.1% | batch:         9 of        64\t|\tloss: 1930.97\n",
      "Training Epoch 99  15.6% | batch:        10 of        64\t|\tloss: 972.042\n",
      "Training Epoch 99  17.2% | batch:        11 of        64\t|\tloss: 3337.97\n",
      "Training Epoch 99  18.8% | batch:        12 of        64\t|\tloss: 1218\n",
      "Training Epoch 99  20.3% | batch:        13 of        64\t|\tloss: 653.232\n",
      "Training Epoch 99  21.9% | batch:        14 of        64\t|\tloss: 790.328\n",
      "Training Epoch 99  23.4% | batch:        15 of        64\t|\tloss: 493.183\n",
      "Training Epoch 99  25.0% | batch:        16 of        64\t|\tloss: 1121.68\n",
      "Training Epoch 99  26.6% | batch:        17 of        64\t|\tloss: 522.299\n",
      "Training Epoch 99  28.1% | batch:        18 of        64\t|\tloss: 660.639\n",
      "Training Epoch 99  29.7% | batch:        19 of        64\t|\tloss: 1340.72\n",
      "Training Epoch 99  31.2% | batch:        20 of        64\t|\tloss: 703.573\n",
      "Training Epoch 99  32.8% | batch:        21 of        64\t|\tloss: 577.275\n",
      "Training Epoch 99  34.4% | batch:        22 of        64\t|\tloss: 702.895\n",
      "Training Epoch 99  35.9% | batch:        23 of        64\t|\tloss: 1264.18\n",
      "Training Epoch 99  37.5% | batch:        24 of        64\t|\tloss: 975.486\n",
      "Training Epoch 99  39.1% | batch:        25 of        64\t|\tloss: 716.974\n",
      "Training Epoch 99  40.6% | batch:        26 of        64\t|\tloss: 1137.82\n",
      "Training Epoch 99  42.2% | batch:        27 of        64\t|\tloss: 838.325\n",
      "Training Epoch 99  43.8% | batch:        28 of        64\t|\tloss: 1665.38\n",
      "Training Epoch 99  45.3% | batch:        29 of        64\t|\tloss: 766.76\n",
      "Training Epoch 99  46.9% | batch:        30 of        64\t|\tloss: 734.485\n",
      "Training Epoch 99  48.4% | batch:        31 of        64\t|\tloss: 868.193\n",
      "Training Epoch 99  50.0% | batch:        32 of        64\t|\tloss: 1388.32\n",
      "Training Epoch 99  51.6% | batch:        33 of        64\t|\tloss: 392.279\n",
      "Training Epoch 99  53.1% | batch:        34 of        64\t|\tloss: 725.931\n",
      "Training Epoch 99  54.7% | batch:        35 of        64\t|\tloss: 962.372\n",
      "Training Epoch 99  56.2% | batch:        36 of        64\t|\tloss: 509.02\n",
      "Training Epoch 99  57.8% | batch:        37 of        64\t|\tloss: 732.964\n",
      "Training Epoch 99  59.4% | batch:        38 of        64\t|\tloss: 763.088\n",
      "Training Epoch 99  60.9% | batch:        39 of        64\t|\tloss: 641.89\n",
      "Training Epoch 99  62.5% | batch:        40 of        64\t|\tloss: 774.249\n",
      "Training Epoch 99  64.1% | batch:        41 of        64\t|\tloss: 990.374\n",
      "Training Epoch 99  65.6% | batch:        42 of        64\t|\tloss: 500.412\n",
      "Training Epoch 99  67.2% | batch:        43 of        64\t|\tloss: 598.749\n",
      "Training Epoch 99  68.8% | batch:        44 of        64\t|\tloss: 476.532\n",
      "Training Epoch 99  70.3% | batch:        45 of        64\t|\tloss: 1400.01\n",
      "Training Epoch 99  71.9% | batch:        46 of        64\t|\tloss: 546.685\n",
      "Training Epoch 99  73.4% | batch:        47 of        64\t|\tloss: 698.042\n",
      "Training Epoch 99  75.0% | batch:        48 of        64\t|\tloss: 684.216\n",
      "Training Epoch 99  76.6% | batch:        49 of        64\t|\tloss: 878.669\n",
      "Training Epoch 99  78.1% | batch:        50 of        64\t|\tloss: 691.712\n",
      "Training Epoch 99  79.7% | batch:        51 of        64\t|\tloss: 1283.91\n",
      "Training Epoch 99  81.2% | batch:        52 of        64\t|\tloss: 699.906\n",
      "Training Epoch 99  82.8% | batch:        53 of        64\t|\tloss: 554.124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:10:02,103 | INFO : Epoch 99 Training Summary: epoch: 99.000000 | loss: 954.277460 | \n",
      "2023-05-10 17:10:02,104 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1427276134490967 seconds\n",
      "\n",
      "2023-05-10 17:10:02,105 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.196099088649557 seconds\n",
      "2023-05-10 17:10:02,105 | INFO : Avg batch train. time: 0.01868904826014933 seconds\n",
      "2023-05-10 17:10:02,105 | INFO : Avg sample train. time: 0.0002962107698488254 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 99  84.4% | batch:        54 of        64\t|\tloss: 2231.53\n",
      "Training Epoch 99  85.9% | batch:        55 of        64\t|\tloss: 889.417\n",
      "Training Epoch 99  87.5% | batch:        56 of        64\t|\tloss: 1155.94\n",
      "Training Epoch 99  89.1% | batch:        57 of        64\t|\tloss: 937.909\n",
      "Training Epoch 99  90.6% | batch:        58 of        64\t|\tloss: 945.103\n",
      "Training Epoch 99  92.2% | batch:        59 of        64\t|\tloss: 1271.15\n",
      "Training Epoch 99  93.8% | batch:        60 of        64\t|\tloss: 1618.8\n",
      "Training Epoch 99  95.3% | batch:        61 of        64\t|\tloss: 1813.13\n",
      "Training Epoch 99  96.9% | batch:        62 of        64\t|\tloss: 748.303\n",
      "Training Epoch 99  98.4% | batch:        63 of        64\t|\tloss: 2071.85\n",
      "\n",
      "Training Epoch 100   0.0% | batch:         0 of        64\t|\tloss: 599.459\n",
      "Training Epoch 100   1.6% | batch:         1 of        64\t|\tloss: 1772.3\n",
      "Training Epoch 100   3.1% | batch:         2 of        64\t|\tloss: 949.542\n",
      "Training Epoch 100   4.7% | batch:         3 of        64\t|\tloss: 2118.02\n",
      "Training Epoch 100   6.2% | batch:         4 of        64\t|\tloss: 3635.6\n",
      "Training Epoch 100   7.8% | batch:         5 of        64\t|\tloss: 779.248\n",
      "Training Epoch 100   9.4% | batch:         6 of        64\t|\tloss: 686.636\n",
      "Training Epoch 100  10.9% | batch:         7 of        64\t|\tloss: 699.384\n",
      "Training Epoch 100  12.5% | batch:         8 of        64\t|\tloss: 784.902\n",
      "Training Epoch 100  14.1% | batch:         9 of        64\t|\tloss: 708.488\n",
      "Training Epoch 100  15.6% | batch:        10 of        64\t|\tloss: 510.387\n",
      "Training Epoch 100  17.2% | batch:        11 of        64\t|\tloss: 753.919\n",
      "Training Epoch 100  18.8% | batch:        12 of        64\t|\tloss: 1724.71\n",
      "Training Epoch 100  20.3% | batch:        13 of        64\t|\tloss: 689.433\n",
      "Training Epoch 100  21.9% | batch:        14 of        64\t|\tloss: 2178.47\n",
      "Training Epoch 100  23.4% | batch:        15 of        64\t|\tloss: 1132.87\n",
      "Training Epoch 100  25.0% | batch:        16 of        64\t|\tloss: 931.734\n",
      "Training Epoch 100  26.6% | batch:        17 of        64\t|\tloss: 1386.56\n",
      "Training Epoch 100  28.1% | batch:        18 of        64\t|\tloss: 1766.9\n",
      "Training Epoch 100  29.7% | batch:        19 of        64\t|\tloss: 645.543\n",
      "Training Epoch 100  31.2% | batch:        20 of        64\t|\tloss: 990.069\n",
      "Training Epoch 100  32.8% | batch:        21 of        64\t|\tloss: 677.838\n",
      "Training Epoch 100  34.4% | batch:        22 of        64\t|\tloss: 931.585\n",
      "Training Epoch 100  35.9% | batch:        23 of        64\t|\tloss: 850.455\n",
      "Training Epoch 100  37.5% | batch:        24 of        64\t|\tloss: 830.505\n",
      "Training Epoch 100  39.1% | batch:        25 of        64\t|\tloss: 1232.92\n",
      "Training Epoch 100  40.6% | batch:        26 of        64\t|\tloss: 740.006\n",
      "Training Epoch 100  42.2% | batch:        27 of        64\t|\tloss: 640.771\n",
      "Training Epoch 100  43.8% | batch:        28 of        64\t|\tloss: 860.06\n",
      "Training Epoch 100  45.3% | batch:        29 of        64\t|\tloss: 582.929\n",
      "Training Epoch 100  46.9% | batch:        30 of        64\t|\tloss: 730.58\n",
      "Training Epoch 100  48.4% | batch:        31 of        64\t|\tloss: 857.714\n",
      "Training Epoch 100  50.0% | batch:        32 of        64\t|\tloss: 1575.28\n",
      "Training Epoch 100  51.6% | batch:        33 of        64\t|\tloss: 831.975\n",
      "Training Epoch 100  53.1% | batch:        34 of        64\t|\tloss: 1160.22\n",
      "Training Epoch 100  54.7% | batch:        35 of        64\t|\tloss: 823.914\n",
      "Training Epoch 100  56.2% | batch:        36 of        64\t|\tloss: 706.045\n",
      "Training Epoch 100  57.8% | batch:        37 of        64\t|\tloss: 553.354\n",
      "Training Epoch 100  59.4% | batch:        38 of        64\t|\tloss: 879.496\n",
      "Training Epoch 100  60.9% | batch:        39 of        64\t|\tloss: 996.481\n",
      "Training Epoch 100  62.5% | batch:        40 of        64\t|\tloss: 762.052\n",
      "Training Epoch 100  64.1% | batch:        41 of        64\t|\tloss: 934.424\n",
      "Training Epoch 100  65.6% | batch:        42 of        64\t|\tloss: 1376.05\n",
      "Training Epoch 100  67.2% | batch:        43 of        64\t|\tloss: 1351.34\n",
      "Training Epoch 100  68.8% | batch:        44 of        64\t|\tloss: 845.553\n",
      "Training Epoch 100  70.3% | batch:        45 of        64\t|\tloss: 810.553\n",
      "Training Epoch 100  71.9% | batch:        46 of        64\t|\tloss: 1156.04\n",
      "Training Epoch 100  73.4% | batch:        47 of        64\t|\tloss: 810.048\n",
      "Training Epoch 100  75.0% | batch:        48 of        64\t|\tloss: 779.738\n",
      "Training Epoch 100  76.6% | batch:        49 of        64\t|\tloss: 741.942\n",
      "Training Epoch 100  78.1% | batch:        50 of        64\t|\tloss: 1818\n",
      "Training Epoch 100  79.7% | batch:        51 of        64\t|\tloss: 1190.14\n",
      "Training Epoch 100  81.2% | batch:        52 of        64\t|\tloss: 1330.35\n",
      "Training Epoch 100  82.8% | batch:        53 of        64\t|\tloss: 2008.86\n",
      "Training Epoch 100  84.4% | batch:        54 of        64\t|\tloss: 831.832\n",
      "Training Epoch 100  85.9% | batch:        55 of        64\t|\tloss: 1833.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:10:03,299 | INFO : Epoch 100 Training Summary: epoch: 100.000000 | loss: 1067.193284 | \n",
      "2023-05-10 17:10:03,300 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 1.1849725246429443 seconds\n",
      "\n",
      "2023-05-10 17:10:03,300 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 1.195987823009491 seconds\n",
      "2023-05-10 17:10:03,301 | INFO : Avg batch train. time: 0.018687309734523296 seconds\n",
      "2023-05-10 17:10:03,301 | INFO : Avg sample train. time: 0.0002961832152078977 seconds\n",
      "2023-05-10 17:10:03,302 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 100  87.5% | batch:        56 of        64\t|\tloss: 900.344\n",
      "Training Epoch 100  89.1% | batch:        57 of        64\t|\tloss: 729.436\n",
      "Training Epoch 100  90.6% | batch:        58 of        64\t|\tloss: 714.098\n",
      "Training Epoch 100  92.2% | batch:        59 of        64\t|\tloss: 839.513\n",
      "Training Epoch 100  93.8% | batch:        60 of        64\t|\tloss: 1202.13\n",
      "Training Epoch 100  95.3% | batch:        61 of        64\t|\tloss: 768.368\n",
      "Training Epoch 100  96.9% | batch:        62 of        64\t|\tloss: 1557.1\n",
      "Training Epoch 100  98.4% | batch:        63 of        64\t|\tloss: 1444.34\n",
      "\n",
      "Evaluating Epoch 100   0.0% | batch:         0 of        16\t|\tloss: 1583.32\n",
      "Evaluating Epoch 100   6.2% | batch:         1 of        16\t|\tloss: 977.297\n",
      "Evaluating Epoch 100  12.5% | batch:         2 of        16\t|\tloss: 1044.9\n",
      "Evaluating Epoch 100  18.8% | batch:         3 of        16\t|\tloss: 1356.83\n",
      "Evaluating Epoch 100  25.0% | batch:         4 of        16\t|\tloss: 1626.85\n",
      "Evaluating Epoch 100  31.2% | batch:         5 of        16\t|\tloss: 1607.53\n",
      "Evaluating Epoch 100  37.5% | batch:         6 of        16\t|\tloss: 2195.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:10:03,450 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 0.14719152450561523 seconds\n",
      "\n",
      "2023-05-10 17:10:03,451 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 0.1637059862797077 seconds\n",
      "2023-05-10 17:10:03,451 | INFO : Avg batch val. time: 0.010231624142481731 seconds\n",
      "2023-05-10 17:10:03,451 | INFO : Avg sample val. time: 0.00016208513493040367 seconds\n",
      "2023-05-10 17:10:03,452 | INFO : Epoch 100 Validation Summary: epoch: 100.000000 | loss: 1923.427684 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 100  43.8% | batch:         7 of        16\t|\tloss: 1942.74\n",
      "Evaluating Epoch 100  50.0% | batch:         8 of        16\t|\tloss: 1783.44\n",
      "Evaluating Epoch 100  56.2% | batch:         9 of        16\t|\tloss: 6761.94\n",
      "Evaluating Epoch 100  62.5% | batch:        10 of        16\t|\tloss: 1492.08\n",
      "Evaluating Epoch 100  68.8% | batch:        11 of        16\t|\tloss: 2025.48\n",
      "Evaluating Epoch 100  75.0% | batch:        12 of        16\t|\tloss: 1512.86\n",
      "Evaluating Epoch 100  81.2% | batch:        13 of        16\t|\tloss: 1370.69\n",
      "Evaluating Epoch 100  87.5% | batch:        14 of        16\t|\tloss: 1378.71\n",
      "Evaluating Epoch 100  93.8% | batch:        15 of        16\t|\tloss: 2168.78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info('Starting training...')\n",
    "for epoch in tqdm(range(start_epoch + 1, config[\"epochs\"] + 1), desc='Training Epoch', leave=False):\n",
    "    mark = epoch if config['save_all'] else 'last'\n",
    "    epoch_start_time = time.time()\n",
    "    aggr_metrics_train = trainer.train_epoch(epoch)  # dictionary of aggregate epoch metrics\n",
    "    epoch_runtime = time.time() - epoch_start_time\n",
    "    print()\n",
    "    print_str = 'Epoch {} Training Summary: '.format(epoch)\n",
    "    for k, v in aggr_metrics_train.items():\n",
    "        tensorboard_writer.add_scalar('{}/train'.format(k), v, epoch)\n",
    "        print_str += '{}: {:8f} | '.format(k, v)\n",
    "    logger.info(print_str)\n",
    "    logger.info(\"Epoch runtime: {} hours, {} minutes, {} seconds\\n\".format(*utils.readable_time(epoch_runtime)))\n",
    "    total_epoch_time += epoch_runtime\n",
    "    avg_epoch_time = total_epoch_time / (epoch - start_epoch)\n",
    "    avg_batch_time = avg_epoch_time / len(train_loader)\n",
    "    avg_sample_time = avg_epoch_time / len(train_dataset)\n",
    "    logger.info(\"Avg epoch train. time: {} hours, {} minutes, {} seconds\".format(*utils.readable_time(avg_epoch_time)))\n",
    "    logger.info(\"Avg batch train. time: {} seconds\".format(avg_batch_time))\n",
    "    logger.info(\"Avg sample train. time: {} seconds\".format(avg_sample_time))\n",
    "\n",
    "    # evaluate if first or last epoch or at specified interval\n",
    "    if (epoch == config[\"epochs\"]) or (epoch == start_epoch + 1) or (epoch % config['val_interval'] == 0):\n",
    "        aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, tensorboard_writer, config,\n",
    "                                                              best_metrics, best_value, epoch)\n",
    "        metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "        metrics.append(list(metrics_values))\n",
    "\n",
    "    utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(mark)), epoch, model, optimizer)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    if epoch == config['lr_step'][lr_step]:\n",
    "        utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(epoch)), epoch, model, optimizer)\n",
    "        lr = lr * config['lr_factor'][lr_step]\n",
    "        if lr_step < len(config['lr_step']) - 1:  # so that this index does not get out of bounds\n",
    "            lr_step += 1\n",
    "        logger.info('Learning rate updated to: ', lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    # Difficulty scheduling\n",
    "    if config['harden'] and check_progress(epoch):\n",
    "        train_loader.dataset.update()\n",
    "        val_loader.dataset.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35492f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:10:03,472 | INFO : Exported per epoch performance metrics in '../experiments/pm25_fromScratch_Regression_2023-05-10_17-07-32_kDk/metrics_pm25_fromScratch_Regression.xls'\n",
      "2023-05-10 17:10:03,475 | INFO : Exported performance record to 'Regression_records.xls'\n",
      "2023-05-10 17:10:03,476 | INFO : Best loss was 1705.1961362933168. Other metrics: OrderedDict([('epoch', 48), ('loss', 1705.1961362933168)])\n",
      "2023-05-10 17:10:03,476 | INFO : All Done!\n",
      "2023-05-10 17:10:03,477 | INFO : Total runtime: 0.0 hours, 2.0 minutes, 10.478641271591187 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1705.1961362933168\n"
     ]
    }
   ],
   "source": [
    "# Export evolution of metrics over epochs\n",
    "header = metrics_names\n",
    "metrics_filepath = os.path.join(config[\"output_dir\"], \"metrics_\" + config[\"experiment_name\"] + \".xls\")\n",
    "book = utils.export_performance_metrics(metrics_filepath, metrics, header, sheet_name=\"metrics\")\n",
    "\n",
    "# Export record metrics to a file accumulating records from all experiments\n",
    "utils.register_record(config[\"records_file\"], config[\"initial_timestamp\"], config[\"experiment_name\"],\n",
    "                      best_metrics, aggr_metrics_val, comment=config['comment'])\n",
    "\n",
    "logger.info('Best {} was {}. Other metrics: {}'.format(config['key_metric'], best_value, best_metrics))\n",
    "logger.info('All Done!')\n",
    "\n",
    "total_runtime = time.time() - total_start_time\n",
    "logger.info(\"Total runtime: {} hours, {} minutes, {} seconds\\n\".format(*utils.readable_time(total_runtime)))\n",
    "\n",
    "#return best_value\n",
    "print(best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70927512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75043eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56d5d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(config):\n",
    "\n",
    "    total_epoch_time = 0\n",
    "    total_eval_time = 0\n",
    "\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # Add file logging besides stdout\n",
    "    file_handler = logging.FileHandler(os.path.join(config['output_dir'], 'output.log'))\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    logger.info('Running:\\n{}\\n'.format(' '.join(sys.argv)))  # command used to run\n",
    "\n",
    "    if config['seed'] is not None:\n",
    "        torch.manual_seed(config['seed'])\n",
    "\n",
    "    device = torch.device('cuda' if (torch.cuda.is_available() and config['gpu'] != '-1') else 'cpu')\n",
    "    logger.info(\"Using device: {}\".format(device))\n",
    "    if device == 'cuda':\n",
    "        logger.info(\"Device index: {}\".format(torch.cuda.current_device()))\n",
    "\n",
    "    # Build data\n",
    "    logger.info(\"Loading and preprocessing data ...\")\n",
    "    data_class = data_factory[config['data_class']]\n",
    "    my_data = data_class(config['data_dir'], pattern=config['pattern'], n_proc=config['n_proc'], limit_size=config['limit_size'], config=config)\n",
    "    feat_dim = my_data.feature_df.shape[1]  # dimensionality of data features\n",
    "    if config['task'] == 'classification':\n",
    "        validation_method = 'StratifiedShuffleSplit'\n",
    "        labels = my_data.labels_df.values.flatten()\n",
    "    else:\n",
    "        validation_method = 'ShuffleSplit'\n",
    "        labels = None\n",
    "\n",
    "    # Split dataset\n",
    "    test_data = my_data\n",
    "    test_indices = None  # will be converted to empty list in `split_dataset`, if also test_set_ratio == 0\n",
    "    val_data = my_data\n",
    "    val_indices = []\n",
    "    if config['test_pattern']:  # used if test data come from different files / file patterns\n",
    "        test_data = data_class(config['data_dir'], pattern=config['test_pattern'], n_proc=-1, config=config)\n",
    "        test_indices = test_data.all_IDs\n",
    "    if config['test_from']:  # load test IDs directly from file, if available, otherwise use `test_set_ratio`. Can work together with `test_pattern`\n",
    "        test_indices = list(set([line.rstrip() for line in open(config['test_from']).readlines()]))\n",
    "        try:\n",
    "            test_indices = [int(ind) for ind in test_indices]  # integer indices\n",
    "        except ValueError:\n",
    "            pass  # in case indices are non-integers\n",
    "        logger.info(\"Loaded {} test IDs from file: '{}'\".format(len(test_indices), config['test_from']))\n",
    "    if config['val_pattern']:  # used if val data come from different files / file patterns\n",
    "        val_data = data_class(config['data_dir'], pattern=config['val_pattern'], n_proc=-1, config=config)\n",
    "        val_indices = val_data.all_IDs\n",
    "\n",
    "    # Note: currently a validation set must exist, either with `val_pattern` or `val_ratio`\n",
    "    # Using a `val_pattern` means that `val_ratio` == 0 and `test_ratio` == 0\n",
    "    if config['val_ratio'] > 0:\n",
    "        train_indices, val_indices, test_indices = split_dataset(data_indices=my_data.all_IDs,\n",
    "                                                                 validation_method=validation_method,\n",
    "                                                                 n_splits=1,\n",
    "                                                                 validation_ratio=config['val_ratio'],\n",
    "                                                                 test_set_ratio=config['test_ratio'],  # used only if test_indices not explicitly specified\n",
    "                                                                 test_indices=test_indices,\n",
    "                                                                 random_seed=1337,\n",
    "                                                                 labels=labels)\n",
    "        train_indices = train_indices[0]  # `split_dataset` returns a list of indices *per fold/split*\n",
    "        val_indices = val_indices[0]  # `split_dataset` returns a list of indices *per fold/split*\n",
    "    else:\n",
    "        train_indices = my_data.all_IDs\n",
    "        if test_indices is None:\n",
    "            test_indices = []\n",
    "\n",
    "    logger.info(\"{} samples may be used for training\".format(len(train_indices)))\n",
    "    logger.info(\"{} samples will be used for validation\".format(len(val_indices)))\n",
    "    logger.info(\"{} samples will be used for testing\".format(len(test_indices)))\n",
    "\n",
    "    with open(os.path.join(config['output_dir'], 'data_indices.json'), 'w') as f:\n",
    "        try:\n",
    "            json.dump({'train_indices': list(map(int, train_indices)),\n",
    "                       'val_indices': list(map(int, val_indices)),\n",
    "                       'test_indices': list(map(int, test_indices))}, f, indent=4)\n",
    "        except ValueError:  # in case indices are non-integers\n",
    "            json.dump({'train_indices': list(train_indices),\n",
    "                       'val_indices': list(val_indices),\n",
    "                       'test_indices': list(test_indices)}, f, indent=4)\n",
    "\n",
    "    # Pre-process features\n",
    "    normalizer = None\n",
    "    if config['norm_from']:\n",
    "        with open(config['norm_from'], 'rb') as f:\n",
    "            norm_dict = pickle.load(f)\n",
    "        normalizer = Normalizer(**norm_dict)\n",
    "    elif config['normalization'] is not None:\n",
    "        normalizer = Normalizer(config['normalization'])\n",
    "        my_data.feature_df.loc[train_indices] = normalizer.normalize(my_data.feature_df.loc[train_indices])\n",
    "        if not config['normalization'].startswith('per_sample'):\n",
    "            # get normalizing values from training set and store for future use\n",
    "            norm_dict = normalizer.__dict__\n",
    "            with open(os.path.join(config['output_dir'], 'normalization.pickle'), 'wb') as f:\n",
    "                pickle.dump(norm_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "    if normalizer is not None:\n",
    "        if len(val_indices):\n",
    "            val_data.feature_df.loc[val_indices] = normalizer.normalize(val_data.feature_df.loc[val_indices])\n",
    "        if len(test_indices):\n",
    "            test_data.feature_df.loc[test_indices] = normalizer.normalize(test_data.feature_df.loc[test_indices])\n",
    "\n",
    "    # Create model\n",
    "    logger.info(\"Creating model ...\")\n",
    "    model = model_factory(config, my_data)\n",
    "\n",
    "    if config['freeze']:\n",
    "        for name, param in model.named_parameters():\n",
    "            if name.startswith('output_layer'):\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    logger.info(\"Model:\\n{}\".format(model))\n",
    "    logger.info(\"Total number of parameters: {}\".format(utils.count_parameters(model)))\n",
    "    logger.info(\"Trainable parameters: {}\".format(utils.count_parameters(model, trainable=True)))\n",
    "\n",
    "\n",
    "    # Initialize optimizer\n",
    "\n",
    "    if config['global_reg']:\n",
    "        weight_decay = config['l2_reg']\n",
    "        output_reg = None\n",
    "    else:\n",
    "        weight_decay = 0\n",
    "        output_reg = config['l2_reg']\n",
    "\n",
    "    optim_class = get_optimizer(config['optimizer'])\n",
    "    optimizer = optim_class(model.parameters(), lr=config['lr'], weight_decay=weight_decay)\n",
    "\n",
    "    start_epoch = 0\n",
    "    lr_step = 0  # current step index of `lr_step`\n",
    "    lr = config['lr']  # current learning step\n",
    "    # Load model and optimizer state\n",
    "    if args.load_model:\n",
    "        model, optimizer, start_epoch = utils.load_model(model, config['load_model'], optimizer, config['resume'],\n",
    "                                                         config['change_output'],\n",
    "                                                         config['lr'],\n",
    "                                                         config['lr_step'],\n",
    "                                                         config['lr_factor'])\n",
    "    model.to(device)\n",
    "\n",
    "    loss_module = get_loss_module(config)\n",
    "\n",
    "    if config['test_only'] == 'testset':  # Only evaluate and skip training\n",
    "        dataset_class, collate_fn, runner_class = pipeline_factory(config)\n",
    "        test_dataset = dataset_class(test_data, test_indices)\n",
    "\n",
    "        test_loader = DataLoader(dataset=test_dataset,\n",
    "                                 batch_size=config['batch_size'],\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=config['num_workers'],\n",
    "                                 pin_memory=True,\n",
    "                                 collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "        test_evaluator = runner_class(model, test_loader, device, loss_module,\n",
    "                                            print_interval=config['print_interval'], console=config['console'])\n",
    "        aggr_metrics_test, per_batch_test = test_evaluator.evaluate(keep_all=True)\n",
    "        print_str = 'Test Summary: '\n",
    "        for k, v in aggr_metrics_test.items():\n",
    "            print_str += '{}: {:8f} | '.format(k, v)\n",
    "        logger.info(print_str)\n",
    "        return\n",
    "    \n",
    "    # Initialize data generators\n",
    "    dataset_class, collate_fn, runner_class = pipeline_factory(config)\n",
    "    val_dataset = dataset_class(val_data, val_indices)\n",
    "\n",
    "    val_loader = DataLoader(dataset=val_dataset,\n",
    "                            batch_size=config['batch_size'],\n",
    "                            shuffle=False,\n",
    "                            num_workers=config['num_workers'],\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "\n",
    "    train_dataset = dataset_class(my_data, train_indices)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=config['batch_size'],\n",
    "                              shuffle=True,\n",
    "                              num_workers=config['num_workers'],\n",
    "                              pin_memory=True,\n",
    "                              collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "\n",
    "    trainer = runner_class(model, train_loader, device, loss_module, optimizer, l2_reg=output_reg,\n",
    "                                 print_interval=config['print_interval'], console=config['console'])\n",
    "    val_evaluator = runner_class(model, val_loader, device, loss_module,\n",
    "                                       print_interval=config['print_interval'], console=config['console'])\n",
    "\n",
    "    tensorboard_writer = SummaryWriter(config['tensorboard_dir'])\n",
    "\n",
    "    best_value = 1e16 if config['key_metric'] in NEG_METRICS else -1e16  # initialize with +inf or -inf depending on key metric\n",
    "    metrics = []  # (for validation) list of lists: for each epoch, stores metrics like loss, ...\n",
    "    best_metrics = {}\n",
    "\n",
    "    # Evaluate on validation before training\n",
    "    aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, tensorboard_writer, config, best_metrics,\n",
    "                                                          best_value, epoch=0)\n",
    "    metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "    metrics.append(list(metrics_values))\n",
    "\n",
    "    logger.info('Starting training...')\n",
    "    for epoch in tqdm(range(start_epoch + 1, config[\"epochs\"] + 1), desc='Training Epoch', leave=False):\n",
    "        mark = epoch if config['save_all'] else 'last'\n",
    "        epoch_start_time = time.time()\n",
    "        aggr_metrics_train = trainer.train_epoch(epoch)  # dictionary of aggregate epoch metrics\n",
    "        epoch_runtime = time.time() - epoch_start_time\n",
    "        print()\n",
    "        print_str = 'Epoch {} Training Summary: '.format(epoch)\n",
    "        for k, v in aggr_metrics_train.items():\n",
    "            tensorboard_writer.add_scalar('{}/train'.format(k), v, epoch)\n",
    "            print_str += '{}: {:8f} | '.format(k, v)\n",
    "        logger.info(print_str)\n",
    "        logger.info(\"Epoch runtime: {} hours, {} minutes, {} seconds\\n\".format(*utils.readable_time(epoch_runtime)))\n",
    "        total_epoch_time += epoch_runtime\n",
    "        avg_epoch_time = total_epoch_time / (epoch - start_epoch)\n",
    "        avg_batch_time = avg_epoch_time / len(train_loader)\n",
    "        avg_sample_time = avg_epoch_time / len(train_dataset)\n",
    "        logger.info(\"Avg epoch train. time: {} hours, {} minutes, {} seconds\".format(*utils.readable_time(avg_epoch_time)))\n",
    "        logger.info(\"Avg batch train. time: {} seconds\".format(avg_batch_time))\n",
    "        logger.info(\"Avg sample train. time: {} seconds\".format(avg_sample_time))\n",
    "\n",
    "        # evaluate if first or last epoch or at specified interval\n",
    "        if (epoch == config[\"epochs\"]) or (epoch == start_epoch + 1) or (epoch % config['val_interval'] == 0):\n",
    "            aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, tensorboard_writer, config,\n",
    "                                                                  best_metrics, best_value, epoch)\n",
    "            metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "            metrics.append(list(metrics_values))\n",
    "\n",
    "        utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(mark)), epoch, model, optimizer)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        if epoch == config['lr_step'][lr_step]:\n",
    "            utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(epoch)), epoch, model, optimizer)\n",
    "            lr = lr * config['lr_factor'][lr_step]\n",
    "            if lr_step < len(config['lr_step']) - 1:  # so that this index does not get out of bounds\n",
    "                lr_step += 1\n",
    "            logger.info('Learning rate updated to: ', lr)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "        # Difficulty scheduling\n",
    "        if config['harden'] and check_progress(epoch):\n",
    "            train_loader.dataset.update()\n",
    "            val_loader.dataset.update()\n",
    "\n",
    "    # Export evolution of metrics over epochs\n",
    "    header = metrics_names\n",
    "    metrics_filepath = os.path.join(config[\"output_dir\"], \"metrics_\" + config[\"experiment_name\"] + \".xls\")\n",
    "    book = utils.export_performance_metrics(metrics_filepath, metrics, header, sheet_name=\"metrics\")\n",
    "\n",
    "    # Export record metrics to a file accumulating records from all experiments\n",
    "    utils.register_record(config[\"records_file\"], config[\"initial_timestamp\"], config[\"experiment_name\"],\n",
    "                          best_metrics, aggr_metrics_val, comment=config['comment'])\n",
    "\n",
    "    logger.info('Best {} was {}. Other metrics: {}'.format(config['key_metric'], best_value, best_metrics))\n",
    "    logger.info('All Done!')\n",
    "\n",
    "    total_runtime = time.time() - total_start_time\n",
    "    logger.info(\"Total runtime: {} hours, {} minutes, {} seconds\\n\".format(*utils.readable_time(total_runtime)))\n",
    "\n",
    "    return best_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ed5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6c3ff63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5*10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20e518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
