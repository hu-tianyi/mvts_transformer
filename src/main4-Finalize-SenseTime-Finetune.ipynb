{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40db25f3",
   "metadata": {},
   "source": [
    "# Pose Error Project\n",
    "### (Transformer-Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d884a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 11:10:12,536 | INFO : Loading packages ...\n",
      "2023-05-25 11:10:13,799 | INFO : Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-05-25 11:10:13,800 | INFO : NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Loading packages ...\")\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# 3rd party packages\n",
    "\n",
    "#from tqdm import tqdm\n",
    "# since we are using it in jupyter notebook\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Project modules\n",
    "from options import Options\n",
    "from running import setup, pipeline_factory, validate, check_progress, NEG_METRICS\n",
    "from utils import utils\n",
    "from datasets.data import data_factory, Normalizer\n",
    "from datasets.datasplit import split_dataset\n",
    "from models.ts_transformer import model_factory\n",
    "from models.loss import get_loss_module\n",
    "from optimizers import get_optimizer\n",
    "\n",
    "import parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871bc365",
   "metadata": {},
   "source": [
    "# Setup Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e17d66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntext = \"--output_dir ../experiments/ --comment \\'poseErrorPred_from_Scratch_smooth24\\'         --name poseErrorPred_fromScratch_Regression_Selective_4_64 --records_file Regression_records.xls         --data_dir ../data/SenseTimeV4_Selective/ --data_class pose         --epochs 25 --lr 0.0001 --optimizer RAdam --batch_size 128         --pos_encoding learnable --task regression --print_interval 1        --num_layers 4  --num_heads 8 --d_model 64 --dim_feedforward 256\"\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting 1 - Single Stage\n",
    "# Training From Scratch\n",
    "'''\n",
    "text = \"--output_dir ../experiments/ --comment 'poseErrorPred_from_Scratch_smooth24' \\\n",
    "        --name poseErrorPred_fromScratch_Regression_Selective_4_64 --records_file Regression_records.xls \\\n",
    "        --data_dir ../data/SenseTimeV4_Selective/ --data_class pose \\\n",
    "        --epochs 25 --lr 0.0001 --optimizer RAdam --batch_size 128 \\\n",
    "        --pos_encoding learnable --task regression --print_interval 1\\\n",
    "        --num_layers 4  --num_heads 8 --d_model 64 --dim_feedforward 256\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036bf287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting 2 - Two Stages\n",
    "# Pretrain\n",
    "'''\n",
    "text = \"--output_dir ../experiments/ --comment 'poseErrorPred_pretrain' \\\n",
    "        --name poseErrorPred_preTrain --records_file Regression_records.xls \\\n",
    "        --data_dir ../data/Hall_LivingRoom_Pretrain/ --data_class pose \\\n",
    "        --val_ratio 0.2 --epochs 50 --lr 0.0001 --optimizer RAdam --batch_size 128 \\\n",
    "        --pos_encoding learnable --task regression --print_interval 1\"\n",
    "'''\n",
    "# Finetune\n",
    "text = \"--output_dir ../experiments --comment 'poseErrorPred_finetune' \\\n",
    "        --name poseErrorPred_finetuned --records_file Regression_records.xls \\\n",
    "        --data_dir ../data/SenseTimeV4_Selective/ --data_class pose \\\n",
    "        --epochs 25 --lr 0.0001 --optimizer RAdam \\\n",
    "        --pos_encoding learnable --d_model 64 \\\n",
    "        --load_model ../experiments/poseErrorPred_preTrain_2023-05-25_10-45-43_HallnLivingRoom/checkpoints/model_best.pth \\\n",
    "        --task regression --change_output --batch_size 128\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e99d546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 11:10:17,587 | INFO : Stored configuration file in '../experiments/poseErrorPred_finetuned_2023-05-25_11-10-17_jyO'\n"
     ]
    }
   ],
   "source": [
    "# Process the setting string\n",
    "# Generate the config variable\n",
    "input_text = text.split()\n",
    "args = Options().parse(input_text)\n",
    "config = setup(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f6877ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_filepath': None,\n",
       " 'output_dir': '../experiments/poseErrorPred_finetuned_2023-05-25_11-10-17_jyO',\n",
       " 'data_dir': '../data/SenseTimeV4_Selective/',\n",
       " 'load_model': '../experiments/poseErrorPred_preTrain_2023-05-25_10-45-43_HallnLivingRoom/checkpoints/model_best.pth',\n",
       " 'resume': False,\n",
       " 'change_output': True,\n",
       " 'save_all': False,\n",
       " 'experiment_name': 'poseErrorPred_finetuned',\n",
       " 'comment': \"'poseErrorPred_finetune'\",\n",
       " 'no_timestamp': False,\n",
       " 'records_file': 'Regression_records.xls',\n",
       " 'console': False,\n",
       " 'print_interval': 1,\n",
       " 'gpu': '0',\n",
       " 'n_proc': -1,\n",
       " 'num_workers': 0,\n",
       " 'seed': None,\n",
       " 'limit_size': None,\n",
       " 'test_only': None,\n",
       " 'data_class': 'pose',\n",
       " 'labels': None,\n",
       " 'test_from': None,\n",
       " 'test_ratio': 0,\n",
       " 'val_ratio': 0.2,\n",
       " 'pattern': None,\n",
       " 'val_pattern': None,\n",
       " 'test_pattern': None,\n",
       " 'normalization': 'standardization',\n",
       " 'norm_from': None,\n",
       " 'subsample_factor': None,\n",
       " 'task': 'regression',\n",
       " 'masking_ratio': 0.15,\n",
       " 'mean_mask_length': 3,\n",
       " 'mask_mode': 'separate',\n",
       " 'mask_distribution': 'geometric',\n",
       " 'exclude_feats': None,\n",
       " 'mask_feats': [0, 1],\n",
       " 'start_hint': 0.0,\n",
       " 'end_hint': 0.0,\n",
       " 'harden': False,\n",
       " 'epochs': 25,\n",
       " 'val_interval': 2,\n",
       " 'optimizer': 'RAdam',\n",
       " 'lr': 0.0001,\n",
       " 'lr_step': [1000000],\n",
       " 'lr_factor': [0.1],\n",
       " 'batch_size': 128,\n",
       " 'l2_reg': 0,\n",
       " 'global_reg': False,\n",
       " 'key_metric': 'loss',\n",
       " 'freeze': False,\n",
       " 'model': 'transformer',\n",
       " 'max_seq_len': None,\n",
       " 'data_window_len': None,\n",
       " 'd_model': 64,\n",
       " 'dim_feedforward': 256,\n",
       " 'num_heads': 8,\n",
       " 'num_layers': 3,\n",
       " 'dropout': 0.1,\n",
       " 'pos_encoding': 'learnable',\n",
       " 'activation': 'gelu',\n",
       " 'normalization_layer': 'BatchNorm',\n",
       " 'initial_timestamp': '2023-05-25_11-10-17',\n",
       " 'save_dir': '../experiments/poseErrorPred_finetuned_2023-05-25_11-10-17_jyO/checkpoints',\n",
       " 'pred_dir': '../experiments/poseErrorPred_finetuned_2023-05-25_11-10-17_jyO/predictions',\n",
       " 'tensorboard_dir': '../experiments/poseErrorPred_finetuned_2023-05-25_11-10-17_jyO/tb_summaries'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639d146",
   "metadata": {},
   "source": [
    "# Setup Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "552d0b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch_time = 0\n",
    "total_eval_time = 0\n",
    "\n",
    "total_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386c3baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 11:10:20,607 | INFO : Running:\n",
      "/home/tianyi/anaconda3/envs/transformer/lib/python3.8/site-packages/ipykernel_launcher.py -f /home/tianyi/.local/share/jupyter/runtime/kernel-8a69a3c6-5009-4686-9eef-8d745fcb5b09.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add file logging besides stdout\n",
    "file_handler = logging.FileHandler(os.path.join(config['output_dir'], 'output.log'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.info('Running:\\n{}\\n'.format(' '.join(sys.argv)))  # command used to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd3027",
   "metadata": {},
   "source": [
    "# Setup Training Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07459b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 11:10:21,698 | INFO : Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if config['seed'] is not None:\n",
    "    torch.manual_seed(config['seed'])\n",
    "\n",
    "device = torch.device('cuda' if (torch.cuda.is_available() and config['gpu'] != '-1') else 'cpu')\n",
    "logger.info(\"Using device: {}\".format(device))\n",
    "if device == 'cuda':\n",
    "    logger.info(\"Device index: {}\".format(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d796b",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e8acbf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 11:10:22,965 | INFO : Loading and preprocessing data ...\n",
      "2023-05-25 11:10:22,967 | INFO : Loading 69 datasets files using 32 parallel processes ...\n"
     ]
    }
   ],
   "source": [
    " # Build data\n",
    "logger.info(\"Loading and preprocessing data ...\")\n",
    "data_class = data_factory[config['data_class']]\n",
    "my_data = data_class(config['data_dir'], \n",
    "                     pattern=config['pattern'], \n",
    "                     n_proc=config['n_proc'], \n",
    "                     limit_size=config['limit_size'], \n",
    "                     config=config)\n",
    "feat_dim = my_data.feature_df.shape[1]  # dimensionality of data features\n",
    "if config['task'] == 'classification':\n",
    "    validation_method = 'StratifiedShuffleSplit'\n",
    "    labels = my_data.labels_df.values.flatten()\n",
    "else:\n",
    "    validation_method = 'ShuffleSplit'\n",
    "    labels = None\n",
    "    \n",
    "# Modify for the pose error pred\n",
    "validation_method = 'PoseErrorTimeSplit'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706bd0fa",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "329d7c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "test_data = my_data\n",
    "test_indices = None  # will be converted to empty list in `split_dataset`, if also test_set_ratio == 0\n",
    "val_data = my_data\n",
    "val_indices = []\n",
    "if config['test_pattern']:  # used if test data come from different files / file patterns\n",
    "    test_data = data_class(config['data_dir'], pattern=config['test_pattern'], n_proc=-1, config=config)\n",
    "    test_indices = test_data.all_IDs\n",
    "if config['test_from']:  # load test IDs directly from file, if available, otherwise use `test_set_ratio`. Can work together with `test_pattern`\n",
    "    test_indices = list(set([line.rstrip() for line in open(config['test_from']).readlines()]))\n",
    "    try:\n",
    "        test_indices = [int(ind) for ind in test_indices]  # integer indices\n",
    "    except ValueError:\n",
    "        pass  # in case indices are non-integers\n",
    "    logger.info(\"Loaded {} test IDs from file: '{}'\".format(len(test_indices), config['test_from']))\n",
    "if config['val_pattern']:  # used if val data come from different files / file patterns\n",
    "    val_data = data_class(config['data_dir'], pattern=config['val_pattern'], n_proc=-1, config=config)\n",
    "    val_indices = val_data.all_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1fa9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: currently a validation set must exist, either with `val_pattern` or `val_ratio`\n",
    "# Using a `val_pattern` means that `val_ratio` == 0 and `test_ratio` == 0\n",
    "if config['val_ratio'] > 0:\n",
    "    train_indices, val_indices, test_indices = split_dataset(data_indices=my_data.all_IDs,\n",
    "                                                             validation_method=validation_method,\n",
    "                                                             n_splits=1,\n",
    "                                                             validation_ratio=config['val_ratio'],\n",
    "                                                             test_set_ratio=config['test_ratio'],  # used only if test_indices not explicitly specified\n",
    "                                                             test_indices=test_indices,\n",
    "                                                             random_seed=1337,\n",
    "                                                             labels=labels)\n",
    "    train_indices = train_indices[0]  # `split_dataset` returns a list of indices *per fold/split*\n",
    "    val_indices = val_indices[0]  # `split_dataset` returns a list of indices *per fold/split*\n",
    "else:\n",
    "    train_indices = my_data.all_IDs\n",
    "    if test_indices is None:\n",
    "        test_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "142da4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 11:11:10,608 | INFO : 88355 \t samples may be used for training\n",
      "2023-05-25 11:11:10,609 | INFO : 22125 \t samples will be used for validation\n",
      "2023-05-25 11:11:10,609 | INFO : 0 \t samples will be used for testing\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"{} \\t samples may be used for training\".format(len(train_indices)))\n",
    "logger.info(\"{} \\t samples will be used for validation\".format(len(val_indices)))\n",
    "logger.info(\"{} \\t samples will be used for testing\".format(len(test_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b579872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config['output_dir'], 'data_indices.json'), 'w') as f:\n",
    "    try:\n",
    "        json.dump({'train_indices': list(map(int, train_indices)),\n",
    "                   'val_indices': list(map(int, val_indices)),\n",
    "                   'test_indices': list(map(int, test_indices))}, f, indent=4)\n",
    "    except ValueError:  # in case indices are non-integers\n",
    "        json.dump({'train_indices': list(train_indices),\n",
    "                   'val_indices': list(val_indices),\n",
    "                   'test_indices': list(test_indices)}, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process features\n",
    "normalizer = None\n",
    "if config['norm_from']:\n",
    "    with open(config['norm_from'], 'rb') as f:\n",
    "        norm_dict = pickle.load(f)\n",
    "    normalizer = Normalizer(**norm_dict)\n",
    "elif config['normalization'] is not None:\n",
    "    normalizer = Normalizer(config['normalization'])\n",
    "    my_data.feature_df.loc[train_indices] = normalizer.normalize(my_data.feature_df.loc[train_indices])\n",
    "    if not config['normalization'].startswith('per_sample'):\n",
    "        # get normalizing values from training set and store for future use\n",
    "        norm_dict = normalizer.__dict__\n",
    "        with open(os.path.join(config['output_dir'], 'normalization.pickle'), 'wb') as f:\n",
    "            pickle.dump(norm_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "if normalizer is not None:\n",
    "    if len(val_indices):\n",
    "        val_data.feature_df.loc[val_indices] = normalizer.normalize(val_data.feature_df.loc[val_indices])\n",
    "    if len(test_indices):\n",
    "        test_data.feature_df.loc[test_indices] = normalizer.normalize(test_data.feature_df.loc[test_indices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb5e618",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e829b2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "logger.info(\"Creating model ...\")\n",
    "model = model_factory(config, my_data)\n",
    "\n",
    "if config['freeze']:\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith('output_layer'):\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "logger.info(\"Model:\\n{}\".format(model))\n",
    "logger.info(\"Total number of parameters: {}\".format(utils.count_parameters(model)))\n",
    "logger.info(\"Trainable parameters: {}\".format(utils.count_parameters(model, trainable=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb07a9",
   "metadata": {},
   "source": [
    "# Initialize optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "if config['global_reg']:\n",
    "    weight_decay = config['l2_reg']\n",
    "    output_reg = None\n",
    "else:\n",
    "    weight_decay = 0\n",
    "    output_reg = config['l2_reg']\n",
    "\n",
    "optim_class = get_optimizer(config['optimizer'])\n",
    "optimizer = optim_class(model.parameters(), lr=config['lr'], weight_decay=weight_decay)\n",
    "\n",
    "start_epoch = 0\n",
    "lr_step = 0  # current step index of `lr_step`\n",
    "lr = config['lr']  # current learning step\n",
    "# Load model and optimizer state\n",
    "if args.load_model:\n",
    "    model, optimizer, start_epoch = utils.load_model(model, config['load_model'], optimizer, config['resume'],\n",
    "                                                     config['change_output'],\n",
    "                                                     config['lr'],\n",
    "                                                     config['lr_step'],\n",
    "                                                     config['lr_factor'])\n",
    "model.to(device)\n",
    "\n",
    "loss_module = get_loss_module(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14053daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['test_only'] == 'testset':  # Only evaluate and skip training\n",
    "    dataset_class, collate_fn, runner_class = pipeline_factory(config)\n",
    "    test_dataset = dataset_class(test_data, test_indices)\n",
    "\n",
    "    test_loader = DataLoader(dataset=test_dataset,\n",
    "                             batch_size=config['batch_size'],\n",
    "                             shuffle=False,\n",
    "                             num_workers=config['num_workers'],\n",
    "                             pin_memory=True,\n",
    "                             collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "    test_evaluator = runner_class(model, test_loader, device, loss_module,\n",
    "                                        print_interval=config['print_interval'], console=config['console'])\n",
    "    aggr_metrics_test, per_batch_test = test_evaluator.evaluate(keep_all=True)\n",
    "    print_str = 'Test Summary: '\n",
    "    for k, v in aggr_metrics_test.items():\n",
    "        print_str += '{}: {:8f} | '.format(k, v)\n",
    "    logger.info(print_str)\n",
    "    #return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b73e40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize data generators\n",
    "if config['test_only'] != 'testset':  # Only evaluate and skip training\n",
    "    dataset_class, collate_fn, runner_class = pipeline_factory(config)\n",
    "    val_dataset = dataset_class(val_data, val_indices)\n",
    "\n",
    "    val_loader = DataLoader(dataset=val_dataset,\n",
    "                            batch_size=config['batch_size'],\n",
    "                            shuffle=False,\n",
    "                            num_workers=config['num_workers'],\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "\n",
    "    train_dataset = dataset_class(my_data, train_indices)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=config['batch_size'],\n",
    "                              shuffle=True,\n",
    "                              num_workers=config['num_workers'],\n",
    "                              pin_memory=True,\n",
    "                              collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "\n",
    "    trainer = runner_class(model, train_loader, device, loss_module, optimizer, l2_reg=output_reg,\n",
    "                                 print_interval=config['print_interval'], console=config['console'])\n",
    "    val_evaluator = runner_class(model, val_loader, device, loss_module,\n",
    "                                       print_interval=config['print_interval'], console=config['console'])\n",
    "\n",
    "    tensorboard_writer = SummaryWriter(config['tensorboard_dir'])\n",
    "\n",
    "    best_value = 1e16 if config['key_metric'] in NEG_METRICS else -1e16  # initialize with +inf or -inf depending on key metric\n",
    "    metrics = []  # (for validation) list of lists: for each epoch, stores metrics like loss, ...\n",
    "    best_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca71a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print(config[\"batch_size\"])\n",
    "for batch in train_loader:\n",
    "    X, targets, padding_masks, IDs = batch\n",
    "    print(X.shape)\n",
    "    print(X[0])\n",
    "    print(X.dtype)\n",
    "    print(\"-\"*20)\n",
    "    print(targets.shape)\n",
    "    print(targets[0])\n",
    "    print(targets.dtype)\n",
    "    print(\"-\"*20)\n",
    "    print(padding_masks.shape)\n",
    "    print(padding_masks[0])\n",
    "    print(\"-\"*20)\n",
    "    print(IDs)\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f94a773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "049fe0cf",
   "metadata": {},
   "source": [
    "# Evaluate on validation before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f83ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, tensorboard_writer, config, best_metrics,\n",
    "                                                      best_value, epoch=0)\n",
    "metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "metrics.append(list(metrics_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf058db",
   "metadata": {},
   "source": [
    "# Starting training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c07b4b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger.info('Starting training...')\n",
    "for epoch in tqdm(range(start_epoch + 1, config[\"epochs\"] + 1), desc='Training Epoch', leave=False):\n",
    "    mark = epoch if config['save_all'] else 'last'\n",
    "    epoch_start_time = time.time()\n",
    "    # Training\n",
    "    aggr_metrics_train = trainer.train_epoch(epoch)  # dictionary of aggregate epoch metrics\n",
    "    epoch_runtime = time.time() - epoch_start_time\n",
    "    print()\n",
    "    print_str = 'Epoch {} Training Summary: '.format(epoch)\n",
    "    for k, v in aggr_metrics_train.items():\n",
    "        tensorboard_writer.add_scalar('{}/train'.format(k), v, epoch)\n",
    "        print_str += '{}: {:8f} | '.format(k, v)\n",
    "    logger.info(print_str)\n",
    "    logger.info(\"Epoch runtime: {} hours, {} minutes, {} seconds\\n\".format(*utils.readable_time(epoch_runtime)))\n",
    "    total_epoch_time += epoch_runtime\n",
    "    avg_epoch_time = total_epoch_time / (epoch - start_epoch)\n",
    "    avg_batch_time = avg_epoch_time / len(train_loader)\n",
    "    avg_sample_time = avg_epoch_time / len(train_dataset)\n",
    "    logger.info(\"Avg epoch train. time: {} hours, {} minutes, {} seconds\".format(*utils.readable_time(avg_epoch_time)))\n",
    "    logger.info(\"Avg batch train. time: {} seconds\".format(avg_batch_time))\n",
    "    logger.info(\"Avg sample train. time: {} seconds\".format(avg_sample_time))\n",
    "\n",
    "    # evaluate if first or last epoch or at specified interval\n",
    "    if (epoch == config[\"epochs\"]) or (epoch == start_epoch + 1) or (epoch % config['val_interval'] == 0):\n",
    "        aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, tensorboard_writer, config,\n",
    "                                                              best_metrics, best_value, epoch)\n",
    "        metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "        metrics.append(list(metrics_values))\n",
    "\n",
    "    utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(mark)), epoch, model, optimizer)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    if epoch == config['lr_step'][lr_step]:\n",
    "        utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(epoch)), epoch, model, optimizer)\n",
    "        lr = lr * config['lr_factor'][lr_step]\n",
    "        if lr_step < len(config['lr_step']) - 1:  # so that this index does not get out of bounds\n",
    "            lr_step += 1\n",
    "        logger.info('Learning rate updated to: ', lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    # Difficulty scheduling\n",
    "    if config['harden'] and check_progress(epoch):\n",
    "        train_loader.dataset.update()\n",
    "        val_loader.dataset.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35492f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export evolution of metrics over epochs\n",
    "header = metrics_names\n",
    "metrics_filepath = os.path.join(config[\"output_dir\"], \"metrics_\" + config[\"experiment_name\"] + \".xls\")\n",
    "book = utils.export_performance_metrics(metrics_filepath, metrics, header, sheet_name=\"metrics\")\n",
    "\n",
    "# Export record metrics to a file accumulating records from all experiments\n",
    "utils.register_record(config[\"records_file\"], config[\"initial_timestamp\"], config[\"experiment_name\"],\n",
    "                      best_metrics, aggr_metrics_val, comment=config['comment'])\n",
    "\n",
    "logger.info('Best {} was {}. Other metrics: {}'.format(config['key_metric'], best_value, best_metrics))\n",
    "logger.info('All Done!')\n",
    "\n",
    "total_runtime = time.time() - total_start_time\n",
    "logger.info(\"Total runtime: {} hours, {} minutes, {} seconds\\n\".format(*utils.readable_time(total_runtime)))\n",
    "\n",
    "#return best_value\n",
    "print(best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe567bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4b23c0e",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70927512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75043eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../experiments/\"\n",
    "experiment = 'poseErrorPred_finetuned_2023-05-16_12-04-46_ia1'\n",
    "file_path = '/predictions/best_predictions.npz'\n",
    "total_path = base_path + experiment + file_path\n",
    "total_path\n",
    "#total_path = config['pred_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_path = config['pred_dir'] + '/best_predictions.npz'\n",
    "total_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['pred_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['output_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ed5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.load(total_path, allow_pickle=True)\n",
    "pred.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd526eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate(pred[\"targets\"], axis=0)\n",
    "y_pred = np.concatenate(pred[\"predictions\"], axis=0)\n",
    "IDs = np.concatenate(pred[\"IDs\"], axis=0)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256fc222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mape(y, y_pred):\n",
    "    err = y - y_pred\n",
    "    return np.mean(np.abs(err)/y)\n",
    "\n",
    "def get_mse(y, y_pred):\n",
    "    err = y - y_pred\n",
    "    return np.mean(np.square(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ce45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mse(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f604a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mape(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef8e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_output(y, y_pred, title=' '):\n",
    "    fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(14, 6))\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    \n",
    "    ax0.plot(y, color='g', label='truth')\n",
    "    ax0.set_title(title)\n",
    "    ax0.set_xlabel('Step')\n",
    "    ax0.set_ylabel('Error')\n",
    "    ax0.grid()\n",
    "    ax0.legend()\n",
    "    \n",
    "    ax1.plot(y, color='g', label='truth')\n",
    "    ax1.plot(y_pred, color='b', alpha=0.7, label='predict')\n",
    "    mse = get_mse(y, y_pred)\n",
    "    mape = get_mape(y, y_pred)\n",
    "    ax1.set_title(title+\"- mse: {:.5f} | mape: {:.5f}\".format(mse, mape))\n",
    "    ax1.set_xlabel('Step')\n",
    "    ax1.set_ylabel('Error')\n",
    "    ax1.grid()\n",
    "    ax1.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e54b0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_output(y, y_pred, title='Prediction on SenseTime Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2dffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3911d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb206dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c26533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
